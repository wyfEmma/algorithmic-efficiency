python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1037423020 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_jax_02-06-2024-17-34-24.log
I0206 17:34:47.143307 140225696298816 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/wmt_jax.
I0206 17:34:48.212904 140225696298816 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0206 17:34:48.213550 140225696298816 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0206 17:34:48.213687 140225696298816 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0206 17:34:48.214726 140225696298816 submission_runner.py:542] Using RNG seed 1037423020
I0206 17:34:53.957536 140225696298816 submission_runner.py:551] --- Tuning run 1/5 ---
I0206 17:34:53.957743 140225696298816 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1.
I0206 17:34:53.957917 140225696298816 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1/hparams.json.
I0206 17:34:54.152596 140225696298816 submission_runner.py:206] Initializing dataset.
I0206 17:34:54.167027 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 17:34:54.173055 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 17:34:54.327809 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 17:34:56.312499 140225696298816 submission_runner.py:213] Initializing model.
I0206 17:35:05.405957 140225696298816 submission_runner.py:255] Initializing optimizer.
I0206 17:35:06.504860 140225696298816 submission_runner.py:262] Initializing metrics bundle.
I0206 17:35:06.505070 140225696298816 submission_runner.py:280] Initializing checkpoint and logger.
I0206 17:35:06.506298 140225696298816 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1 with prefix checkpoint_
I0206 17:35:06.506452 140225696298816 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1/meta_data_0.json.
I0206 17:35:06.506678 140225696298816 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 17:35:06.506749 140225696298816 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 17:35:06.981637 140225696298816 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 17:35:07.416001 140225696298816 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1/flags_0.json.
I0206 17:35:07.427007 140225696298816 submission_runner.py:314] Starting training loop.
I0206 17:35:50.710048 140063530800896 logging_writer.py:48] [0] global_step=0, grad_norm=5.171201705932617, loss=11.02572250366211
I0206 17:35:50.730541 140225696298816 spec.py:321] Evaluating on the training split.
I0206 17:35:50.735691 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 17:35:50.739167 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 17:35:50.781632 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 17:35:58.410203 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 17:40:52.194691 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 17:40:52.202428 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 17:40:52.211737 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 17:40:52.249993 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 17:40:58.985549 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 17:45:42.555938 140225696298816 spec.py:349] Evaluating on the test split.
I0206 17:45:42.558809 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 17:45:42.562128 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 17:45:42.599737 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 17:45:45.428634 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 17:50:30.151522 140225696298816 submission_runner.py:408] Time since start: 922.72s, 	Step: 1, 	{'train/accuracy': 0.0006987399538047612, 'train/loss': 11.025596618652344, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 43.30344867706299, 'total_duration': 922.7244355678558, 'accumulated_submission_time': 43.30344867706299, 'accumulated_eval_time': 879.4209234714508, 'accumulated_logging_time': 0}
I0206 17:50:30.171493 140051097532160 logging_writer.py:48] [1] accumulated_eval_time=879.420923, accumulated_logging_time=0, accumulated_submission_time=43.303449, global_step=1, preemption_count=0, score=43.303449, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.036274, test/num_examples=3003, total_duration=922.724436, train/accuracy=0.000699, train/bleu=0.000000, train/loss=11.025597, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.047277, validation/num_examples=3000
I0206 17:51:04.637602 140051089139456 logging_writer.py:48] [100] global_step=100, grad_norm=0.38510191440582275, loss=8.936830520629883
I0206 17:51:39.138312 140051097532160 logging_writer.py:48] [200] global_step=200, grad_norm=0.1641899049282074, loss=8.6426362991333
I0206 17:52:13.688877 140051089139456 logging_writer.py:48] [300] global_step=300, grad_norm=0.1689874529838562, loss=8.394831657409668
I0206 17:52:48.258176 140051097532160 logging_writer.py:48] [400] global_step=400, grad_norm=0.25713300704956055, loss=8.00731086730957
I0206 17:53:22.845236 140051089139456 logging_writer.py:48] [500] global_step=500, grad_norm=0.26825276017189026, loss=7.63151741027832
I0206 17:53:57.416942 140051097532160 logging_writer.py:48] [600] global_step=600, grad_norm=0.4216325879096985, loss=7.345813274383545
I0206 17:54:32.035046 140051089139456 logging_writer.py:48] [700] global_step=700, grad_norm=0.5751772522926331, loss=7.205698013305664
I0206 17:55:06.631757 140051097532160 logging_writer.py:48] [800] global_step=800, grad_norm=0.7195509672164917, loss=6.941251754760742
I0206 17:55:41.238682 140051089139456 logging_writer.py:48] [900] global_step=900, grad_norm=0.6049017310142517, loss=6.743317604064941
I0206 17:56:15.825338 140051097532160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.44264116883277893, loss=6.566967010498047
I0206 17:56:50.467335 140051089139456 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5314693450927734, loss=6.4322638511657715
I0206 17:57:25.083957 140051097532160 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5997828245162964, loss=6.166048049926758
I0206 17:57:59.695937 140051089139456 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5674818754196167, loss=6.101432800292969
I0206 17:58:34.310678 140051097532160 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6836199164390564, loss=5.933964252471924
I0206 17:59:08.919555 140051089139456 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9341880083084106, loss=5.829526901245117
I0206 17:59:43.513406 140051097532160 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.7273586988449097, loss=5.7089619636535645
I0206 18:00:18.153693 140051089139456 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9930722713470459, loss=5.6615118980407715
I0206 18:00:52.818602 140051097532160 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6728308200836182, loss=5.422970771789551
I0206 18:01:27.428311 140051089139456 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9702500104904175, loss=5.404275894165039
I0206 18:02:02.066174 140051097532160 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9927054047584534, loss=5.319633960723877
I0206 18:02:36.719174 140051089139456 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9574005007743835, loss=5.080010414123535
I0206 18:03:11.339450 140051097532160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.8682920336723328, loss=5.101583003997803
I0206 18:03:45.971060 140051089139456 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.9558200240135193, loss=4.93729305267334
I0206 18:04:20.613635 140051097532160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.9841298460960388, loss=4.835195541381836
I0206 18:04:30.377501 140225696298816 spec.py:321] Evaluating on the training split.
I0206 18:04:33.357176 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:07:40.085643 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 18:07:42.779527 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:10:49.071523 140225696298816 spec.py:349] Evaluating on the test split.
I0206 18:10:51.755178 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:13:42.368225 140225696298816 submission_runner.py:408] Time since start: 2314.94s, 	Step: 2430, 	{'train/accuracy': 0.41607430577278137, 'train/loss': 3.9471724033355713, 'train/bleu': 14.572851959066698, 'validation/accuracy': 0.3996106684207916, 'validation/loss': 4.088059425354004, 'validation/bleu': 9.879816759300747, 'validation/num_examples': 3000, 'test/accuracy': 0.3871826231479645, 'test/loss': 4.267494201660156, 'test/bleu': 8.292150497981243, 'test/num_examples': 3003, 'score': 883.4170672893524, 'total_duration': 2314.9411220550537, 'accumulated_submission_time': 883.4170672893524, 'accumulated_eval_time': 1431.4115691184998, 'accumulated_logging_time': 0.03201866149902344}
I0206 18:13:42.388149 140051089139456 logging_writer.py:48] [2430] accumulated_eval_time=1431.411569, accumulated_logging_time=0.032019, accumulated_submission_time=883.417067, global_step=2430, preemption_count=0, score=883.417067, test/accuracy=0.387183, test/bleu=8.292150, test/loss=4.267494, test/num_examples=3003, total_duration=2314.941122, train/accuracy=0.416074, train/bleu=14.572852, train/loss=3.947172, validation/accuracy=0.399611, validation/bleu=9.879817, validation/loss=4.088059, validation/num_examples=3000
I0206 18:14:06.904726 140051097532160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8142932057380676, loss=4.795810222625732
I0206 18:14:41.454750 140051089139456 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7262296676635742, loss=4.650497913360596
I0206 18:15:16.076093 140051097532160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7119607329368591, loss=4.588293552398682
I0206 18:15:50.708558 140051089139456 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7597269415855408, loss=4.573563575744629
I0206 18:16:25.332229 140051097532160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9923882484436035, loss=4.4200639724731445
I0206 18:16:59.962031 140051089139456 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.6328083276748657, loss=4.43554162979126
I0206 18:17:34.618516 140051097532160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7147423028945923, loss=4.357479572296143
I0206 18:18:09.251982 140051089139456 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.7797040939331055, loss=4.3067121505737305
I0206 18:18:43.874140 140051097532160 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.6220577955245972, loss=4.2351908683776855
I0206 18:19:18.522949 140051089139456 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.640088677406311, loss=4.212076187133789
I0206 18:19:53.148794 140051097532160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6570379137992859, loss=4.082402229309082
I0206 18:20:27.797582 140051089139456 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8507725596427917, loss=4.159964561462402
I0206 18:21:02.393748 140051097532160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6578896641731262, loss=4.049694061279297
I0206 18:21:37.022253 140051089139456 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7003765106201172, loss=4.031163692474365
I0206 18:22:11.627375 140051097532160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6150919795036316, loss=3.9730076789855957
I0206 18:22:46.261873 140051089139456 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.587074339389801, loss=4.018933296203613
I0206 18:23:20.876115 140051097532160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6124882698059082, loss=3.8960256576538086
I0206 18:23:55.504515 140051089139456 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6493089199066162, loss=3.9684746265411377
I0206 18:24:30.093875 140051097532160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.629219114780426, loss=3.8649256229400635
I0206 18:25:04.714304 140051089139456 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5600475668907166, loss=3.803323268890381
I0206 18:25:39.306052 140051097532160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5288529396057129, loss=3.761192560195923
I0206 18:26:13.914244 140051089139456 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6535723209381104, loss=3.809187412261963
I0206 18:26:48.502064 140051097532160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7550334930419922, loss=3.7470505237579346
I0206 18:27:23.114857 140051089139456 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6004968881607056, loss=3.8519837856292725
I0206 18:27:42.555757 140225696298816 spec.py:321] Evaluating on the training split.
I0206 18:27:45.518803 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:30:31.378374 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 18:30:34.050996 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:33:15.230362 140225696298816 spec.py:349] Evaluating on the test split.
I0206 18:33:17.902833 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:35:49.106286 140225696298816 submission_runner.py:408] Time since start: 3641.68s, 	Step: 4858, 	{'train/accuracy': 0.5455783009529114, 'train/loss': 2.7186343669891357, 'train/bleu': 24.371344575563036, 'validation/accuracy': 0.5439361929893494, 'validation/loss': 2.697274923324585, 'validation/bleu': 20.40709098587339, 'validation/num_examples': 3000, 'test/accuracy': 0.5444541573524475, 'test/loss': 2.736154556274414, 'test/bleu': 19.09181470174716, 'test/num_examples': 3003, 'score': 1723.4967126846313, 'total_duration': 3641.679195165634, 'accumulated_submission_time': 1723.4967126846313, 'accumulated_eval_time': 1917.9620339870453, 'accumulated_logging_time': 0.062225341796875}
I0206 18:35:49.125632 140051097532160 logging_writer.py:48] [4858] accumulated_eval_time=1917.962034, accumulated_logging_time=0.062225, accumulated_submission_time=1723.496713, global_step=4858, preemption_count=0, score=1723.496713, test/accuracy=0.544454, test/bleu=19.091815, test/loss=2.736155, test/num_examples=3003, total_duration=3641.679195, train/accuracy=0.545578, train/bleu=24.371345, train/loss=2.718634, validation/accuracy=0.543936, validation/bleu=20.407091, validation/loss=2.697275, validation/num_examples=3000
I0206 18:36:03.959890 140051089139456 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.77372807264328, loss=3.752115488052368
I0206 18:36:38.456749 140051097532160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.553290843963623, loss=3.6989855766296387
I0206 18:37:13.009905 140051089139456 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5570775270462036, loss=3.6933493614196777
I0206 18:37:47.631077 140051097532160 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5058768391609192, loss=3.7463343143463135
I0206 18:38:22.255547 140051089139456 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.51153564453125, loss=3.6608564853668213
I0206 18:38:56.873560 140051097532160 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5592539310455322, loss=3.662142276763916
I0206 18:39:31.481602 140051089139456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5408886075019836, loss=3.6810147762298584
I0206 18:40:06.083142 140051097532160 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.45650339126586914, loss=3.627932548522949
I0206 18:40:40.688998 140051089139456 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.47904062271118164, loss=3.61686372756958
I0206 18:41:15.288276 140051097532160 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.4904021918773651, loss=3.588242769241333
I0206 18:41:49.883525 140051089139456 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.46978527307510376, loss=3.5569727420806885
I0206 18:42:24.454472 140051097532160 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4687871038913727, loss=3.6082100868225098
I0206 18:42:59.064193 140051089139456 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5816826820373535, loss=3.6394550800323486
I0206 18:43:33.656749 140051097532160 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6184545159339905, loss=3.6415889263153076
I0206 18:44:08.281242 140051089139456 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.4420861303806305, loss=3.6108102798461914
I0206 18:44:42.928995 140051097532160 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.590795636177063, loss=3.549562931060791
I0206 18:45:17.546991 140051089139456 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5215442776679993, loss=3.5244829654693604
I0206 18:45:52.141308 140051097532160 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.4454430043697357, loss=3.466303825378418
I0206 18:46:26.738946 140051089139456 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.4829574525356293, loss=3.430384397506714
I0206 18:47:01.378726 140051097532160 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.43395620584487915, loss=3.496316909790039
I0206 18:47:35.981372 140051089139456 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.4580950140953064, loss=3.48937726020813
I0206 18:48:10.598117 140051097532160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5815598964691162, loss=3.598828077316284
I0206 18:48:45.196261 140051089139456 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.454086571931839, loss=3.4312617778778076
I0206 18:49:19.774644 140051097532160 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.38237103819847107, loss=3.4049737453460693
I0206 18:49:49.264131 140225696298816 spec.py:321] Evaluating on the training split.
I0206 18:49:52.243309 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:52:34.335129 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 18:52:37.015904 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:55:10.876738 140225696298816 spec.py:349] Evaluating on the test split.
I0206 18:55:13.557089 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 18:57:36.715039 140225696298816 submission_runner.py:408] Time since start: 4949.29s, 	Step: 7287, 	{'train/accuracy': 0.582105815410614, 'train/loss': 2.346024751663208, 'train/bleu': 27.688640848896345, 'validation/accuracy': 0.5865395069122314, 'validation/loss': 2.3033597469329834, 'validation/bleu': 23.467709729191526, 'validation/num_examples': 3000, 'test/accuracy': 0.5892278552055359, 'test/loss': 2.3010103702545166, 'test/bleu': 22.251105801748317, 'test/num_examples': 3003, 'score': 2563.5470848083496, 'total_duration': 4949.287957668304, 'accumulated_submission_time': 2563.5470848083496, 'accumulated_eval_time': 2385.4128901958466, 'accumulated_logging_time': 0.0925908088684082}
I0206 18:57:36.736347 140051089139456 logging_writer.py:48] [7287] accumulated_eval_time=2385.412890, accumulated_logging_time=0.092591, accumulated_submission_time=2563.547085, global_step=7287, preemption_count=0, score=2563.547085, test/accuracy=0.589228, test/bleu=22.251106, test/loss=2.301010, test/num_examples=3003, total_duration=4949.287958, train/accuracy=0.582106, train/bleu=27.688641, train/loss=2.346025, validation/accuracy=0.586540, validation/bleu=23.467710, validation/loss=2.303360, validation/num_examples=3000
I0206 18:57:41.583578 140051097532160 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.3811819851398468, loss=3.3686392307281494
I0206 18:58:16.051351 140051089139456 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.43344926834106445, loss=3.46073579788208
I0206 18:58:50.555819 140051097532160 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.43360140919685364, loss=3.4162144660949707
I0206 18:59:25.112495 140051089139456 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.43483078479766846, loss=3.456373691558838
I0206 18:59:59.705937 140051097532160 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.3790756165981293, loss=3.4520528316497803
I0206 19:00:34.272186 140051089139456 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.3709361255168915, loss=3.3106255531311035
I0206 19:01:08.871913 140051097532160 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.41310206055641174, loss=3.445103406906128
I0206 19:01:43.458028 140051089139456 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.38150572776794434, loss=3.433022975921631
I0206 19:02:18.056977 140051097532160 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3523750603199005, loss=3.444673538208008
I0206 19:02:52.648926 140051089139456 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3395271599292755, loss=3.380885124206543
I0206 19:03:27.243714 140051097532160 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4152209460735321, loss=3.371548891067505
I0206 19:04:01.839661 140051089139456 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.37341973185539246, loss=3.4604623317718506
I0206 19:04:36.404543 140051097532160 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3723841607570648, loss=3.4126904010772705
I0206 19:05:10.989264 140051089139456 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.33254337310791016, loss=3.3248507976531982
I0206 19:05:45.564943 140051097532160 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3261878490447998, loss=3.3000166416168213
I0206 19:06:20.155591 140051089139456 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.41451629996299744, loss=3.404392719268799
I0206 19:06:54.721675 140051097532160 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.35787540674209595, loss=3.3362700939178467
I0206 19:07:29.311712 140051089139456 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3427949845790863, loss=3.439518451690674
I0206 19:08:03.898628 140051097532160 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3108367621898651, loss=3.3370871543884277
I0206 19:08:38.490303 140051089139456 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.3397630453109741, loss=3.4482431411743164
I0206 19:09:13.059448 140051097532160 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.29680517315864563, loss=3.3293440341949463
I0206 19:09:47.622647 140051089139456 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3626449704170227, loss=3.395277976989746
I0206 19:10:22.182643 140051097532160 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.29302477836608887, loss=3.31064510345459
I0206 19:10:56.768285 140051089139456 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3572576642036438, loss=3.333033561706543
I0206 19:11:31.353039 140051097532160 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.30427101254463196, loss=3.273519515991211
I0206 19:11:36.965454 140225696298816 spec.py:321] Evaluating on the training split.
I0206 19:11:39.928244 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:14:11.131702 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 19:14:13.807004 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:16:40.901087 140225696298816 spec.py:349] Evaluating on the test split.
I0206 19:16:43.577975 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:18:56.627989 140225696298816 submission_runner.py:408] Time since start: 6229.20s, 	Step: 9718, 	{'train/accuracy': 0.59144127368927, 'train/loss': 2.244596481323242, 'train/bleu': 27.976897526423063, 'validation/accuracy': 0.6069732308387756, 'validation/loss': 2.119935989379883, 'validation/bleu': 24.509640670153523, 'validation/num_examples': 3000, 'test/accuracy': 0.6132822036743164, 'test/loss': 2.0930325984954834, 'test/bleu': 23.29025859698584, 'test/num_examples': 3003, 'score': 3403.686936378479, 'total_duration': 6229.200881242752, 'accumulated_submission_time': 3403.686936378479, 'accumulated_eval_time': 2825.075345516205, 'accumulated_logging_time': 0.12424159049987793}
I0206 19:18:56.644077 140051089139456 logging_writer.py:48] [9718] accumulated_eval_time=2825.075346, accumulated_logging_time=0.124242, accumulated_submission_time=3403.686936, global_step=9718, preemption_count=0, score=3403.686936, test/accuracy=0.613282, test/bleu=23.290259, test/loss=2.093033, test/num_examples=3003, total_duration=6229.200881, train/accuracy=0.591441, train/bleu=27.976898, train/loss=2.244596, validation/accuracy=0.606973, validation/bleu=24.509641, validation/loss=2.119936, validation/num_examples=3000
I0206 19:19:25.249796 140051097532160 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.2867816090583801, loss=3.3017752170562744
I0206 19:19:59.737868 140051089139456 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.2872052788734436, loss=3.229933500289917
I0206 19:20:34.295013 140051097532160 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.2798093557357788, loss=3.3170785903930664
I0206 19:21:08.867972 140051089139456 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2618914842605591, loss=3.2556991577148438
I0206 19:21:43.455279 140051097532160 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.37970003485679626, loss=3.3166255950927734
I0206 19:22:18.049996 140051089139456 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.27709320187568665, loss=3.228240489959717
I0206 19:22:52.617284 140051097532160 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.2927101254463196, loss=3.2435669898986816
I0206 19:23:27.156035 140051089139456 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2769840657711029, loss=3.3140718936920166
I0206 19:24:01.728725 140051097532160 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.31958478689193726, loss=3.2099575996398926
I0206 19:24:36.306542 140051089139456 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2859167456626892, loss=3.266075611114502
I0206 19:25:10.855790 140051097532160 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2557221055030823, loss=3.2943315505981445
I0206 19:25:45.418597 140051089139456 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.269452840089798, loss=3.2941367626190186
I0206 19:26:19.971340 140051097532160 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2558205723762512, loss=3.3436989784240723
I0206 19:26:54.522630 140051089139456 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2675301432609558, loss=3.204939842224121
I0206 19:27:29.101540 140051097532160 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.27561041712760925, loss=3.274348258972168
I0206 19:28:03.689147 140051089139456 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.26955467462539673, loss=3.2409324645996094
I0206 19:28:38.273104 140051097532160 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.28594541549682617, loss=3.194338083267212
I0206 19:29:12.852895 140051089139456 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2517174184322357, loss=3.307960271835327
I0206 19:29:47.427451 140051097532160 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.27873358130455017, loss=3.3072712421417236
I0206 19:30:22.006533 140051089139456 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.25464168190956116, loss=3.1806085109710693
I0206 19:30:56.601478 140051097532160 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2508464753627777, loss=3.2797772884368896
I0206 19:31:31.178304 140051089139456 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.25307586789131165, loss=3.218045949935913
I0206 19:32:05.719224 140051097532160 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.26811692118644714, loss=3.2869346141815186
I0206 19:32:40.283779 140051089139456 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.2760368287563324, loss=3.1849656105041504
I0206 19:32:56.952574 140225696298816 spec.py:321] Evaluating on the training split.
I0206 19:32:59.915595 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:36:28.358017 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 19:36:31.034369 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:40:02.166200 140225696298816 spec.py:349] Evaluating on the test split.
I0206 19:40:04.846086 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 19:43:28.324800 140225696298816 submission_runner.py:408] Time since start: 7700.90s, 	Step: 12150, 	{'train/accuracy': 0.6025044322013855, 'train/loss': 2.141352653503418, 'train/bleu': 28.804964846373508, 'validation/accuracy': 0.6188392043113708, 'validation/loss': 2.0071840286254883, 'validation/bleu': 25.295877114636344, 'validation/num_examples': 3000, 'test/accuracy': 0.6300621628761292, 'test/loss': 1.9602621793746948, 'test/bleu': 24.743552012654888, 'test/num_examples': 3003, 'score': 4243.90465593338, 'total_duration': 7700.897699356079, 'accumulated_submission_time': 4243.90465593338, 'accumulated_eval_time': 3456.4474980831146, 'accumulated_logging_time': 0.1512916088104248}
I0206 19:43:28.342598 140051097532160 logging_writer.py:48] [12150] accumulated_eval_time=3456.447498, accumulated_logging_time=0.151292, accumulated_submission_time=4243.904656, global_step=12150, preemption_count=0, score=4243.904656, test/accuracy=0.630062, test/bleu=24.743552, test/loss=1.960262, test/num_examples=3003, total_duration=7700.897699, train/accuracy=0.602504, train/bleu=28.804965, train/loss=2.141353, validation/accuracy=0.618839, validation/bleu=25.295877, validation/loss=2.007184, validation/num_examples=3000
I0206 19:43:45.899463 140051089139456 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2449585348367691, loss=3.2372665405273438
I0206 19:44:20.383427 140051097532160 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.25761377811431885, loss=3.269665479660034
I0206 19:44:54.945875 140051089139456 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.24059900641441345, loss=3.19709849357605
I0206 19:45:29.531463 140051097532160 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.29154059290885925, loss=3.211325168609619
I0206 19:46:04.090397 140051089139456 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.25040796399116516, loss=3.1393911838531494
I0206 19:46:38.648256 140051097532160 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.25245505571365356, loss=3.1241447925567627
I0206 19:47:13.223757 140051089139456 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.22774994373321533, loss=3.214937448501587
I0206 19:47:47.796533 140051097532160 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.26934024691581726, loss=3.190678596496582
I0206 19:48:22.369756 140051089139456 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.27376240491867065, loss=3.167227268218994
I0206 19:48:56.938910 140051097532160 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.23785139620304108, loss=3.2016518115997314
I0206 19:49:31.484253 140051089139456 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.23781396448612213, loss=3.201272964477539
I0206 19:50:06.062501 140051097532160 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2540830969810486, loss=3.201423168182373
I0206 19:50:40.620842 140051089139456 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.23270100355148315, loss=3.2234153747558594
I0206 19:51:15.185673 140051097532160 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.23692838847637177, loss=3.1413090229034424
I0206 19:51:49.758525 140051089139456 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.24773883819580078, loss=3.170029640197754
I0206 19:52:24.317070 140051097532160 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.24207137525081635, loss=3.15193772315979
I0206 19:52:58.898267 140051089139456 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.236710324883461, loss=3.1907479763031006
I0206 19:53:33.448482 140051097532160 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2649395763874054, loss=3.2415335178375244
I0206 19:54:08.006401 140051089139456 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.23949876427650452, loss=3.0848422050476074
I0206 19:54:42.587809 140051097532160 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.23066385090351105, loss=3.1467432975769043
I0206 19:55:17.143286 140051089139456 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.25171738862991333, loss=3.112992286682129
I0206 19:55:51.731004 140051097532160 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2193773090839386, loss=3.1337053775787354
I0206 19:56:26.276201 140051089139456 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2381020337343216, loss=3.176713228225708
I0206 19:57:00.839959 140051097532160 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3018133342266083, loss=3.1590847969055176
I0206 19:57:28.551189 140225696298816 spec.py:321] Evaluating on the training split.
I0206 19:57:31.526684 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:00:02.395341 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 20:00:05.066751 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:02:26.169192 140225696298816 spec.py:349] Evaluating on the test split.
I0206 20:02:28.874543 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:04:50.596284 140225696298816 submission_runner.py:408] Time since start: 8983.17s, 	Step: 14582, 	{'train/accuracy': 0.6175777316093445, 'train/loss': 2.0234029293060303, 'train/bleu': 29.494188386708036, 'validation/accuracy': 0.6317466497421265, 'validation/loss': 1.9158974885940552, 'validation/bleu': 26.446092461664627, 'validation/num_examples': 3000, 'test/accuracy': 0.6397652626037598, 'test/loss': 1.8675507307052612, 'test/bleu': 25.720613656901794, 'test/num_examples': 3003, 'score': 5084.021989107132, 'total_duration': 8983.16919708252, 'accumulated_submission_time': 5084.021989107132, 'accumulated_eval_time': 3898.492534637451, 'accumulated_logging_time': 0.18063759803771973}
I0206 20:04:50.612338 140051089139456 logging_writer.py:48] [14582] accumulated_eval_time=3898.492535, accumulated_logging_time=0.180638, accumulated_submission_time=5084.021989, global_step=14582, preemption_count=0, score=5084.021989, test/accuracy=0.639765, test/bleu=25.720614, test/loss=1.867551, test/num_examples=3003, total_duration=8983.169197, train/accuracy=0.617578, train/bleu=29.494188, train/loss=2.023403, validation/accuracy=0.631747, validation/bleu=26.446092, validation/loss=1.915897, validation/num_examples=3000
I0206 20:04:57.291236 140051097532160 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.26428091526031494, loss=3.225233316421509
I0206 20:05:31.719034 140051089139456 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.250535249710083, loss=3.1262898445129395
I0206 20:06:06.268513 140051097532160 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.29223573207855225, loss=3.163512945175171
I0206 20:06:40.814578 140051089139456 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.28893059492111206, loss=3.1037039756774902
I0206 20:07:15.388788 140051097532160 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3272635340690613, loss=3.0552804470062256
I0206 20:07:49.978550 140051089139456 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.24491454660892487, loss=3.177232503890991
I0206 20:08:24.551963 140051097532160 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2476586401462555, loss=3.1544203758239746
I0206 20:08:59.122910 140051089139456 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2541877031326294, loss=3.125976800918579
I0206 20:09:33.716152 140051097532160 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.23295389115810394, loss=3.0402963161468506
I0206 20:10:08.332311 140051089139456 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.25398820638656616, loss=3.15598464012146
I0206 20:10:42.894194 140051097532160 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.24787907302379608, loss=3.1203932762145996
I0206 20:11:17.496607 140051089139456 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2490171194076538, loss=3.1051151752471924
I0206 20:11:52.067633 140051097532160 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.24576915800571442, loss=3.043318748474121
I0206 20:12:26.668787 140051089139456 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2627922296524048, loss=3.1365950107574463
I0206 20:13:01.248031 140051097532160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2640742063522339, loss=3.099513530731201
I0206 20:13:35.812921 140051089139456 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.26434072852134705, loss=3.1850929260253906
I0206 20:14:10.365846 140051097532160 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.2679435610771179, loss=3.075495481491089
I0206 20:14:44.949226 140051089139456 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2589997351169586, loss=3.061530351638794
I0206 20:15:19.516507 140051097532160 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.37872523069381714, loss=3.0167298316955566
I0206 20:15:54.056294 140051089139456 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2929265797138214, loss=3.081176996231079
I0206 20:16:28.593425 140051097532160 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.27489417791366577, loss=3.083970785140991
I0206 20:17:03.178819 140051089139456 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.33617252111434937, loss=3.070713758468628
I0206 20:17:37.750530 140051097532160 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.29751190543174744, loss=3.1092946529388428
I0206 20:18:12.317009 140051089139456 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3000238835811615, loss=3.0880305767059326
I0206 20:18:46.889938 140051097532160 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2737495005130768, loss=3.087310791015625
I0206 20:18:50.756267 140225696298816 spec.py:321] Evaluating on the training split.
I0206 20:18:53.727453 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:21:38.894900 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 20:21:41.579388 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:24:13.238666 140225696298816 spec.py:349] Evaluating on the test split.
I0206 20:24:15.922087 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:26:54.407549 140225696298816 submission_runner.py:408] Time since start: 10306.98s, 	Step: 17013, 	{'train/accuracy': 0.6190584897994995, 'train/loss': 2.003643274307251, 'train/bleu': 30.250872839385124, 'validation/accuracy': 0.6397812962532043, 'validation/loss': 1.8474228382110596, 'validation/bleu': 26.92715774325129, 'validation/num_examples': 3000, 'test/accuracy': 0.6494567394256592, 'test/loss': 1.7908827066421509, 'test/bleu': 26.089327875831792, 'test/num_examples': 3003, 'score': 5923.943639755249, 'total_duration': 10306.980433225632, 'accumulated_submission_time': 5923.943639755249, 'accumulated_eval_time': 4382.143723726273, 'accumulated_logging_time': 0.3407480716705322}
I0206 20:26:54.424077 140051089139456 logging_writer.py:48] [17013] accumulated_eval_time=4382.143724, accumulated_logging_time=0.340748, accumulated_submission_time=5923.943640, global_step=17013, preemption_count=0, score=5923.943640, test/accuracy=0.649457, test/bleu=26.089328, test/loss=1.790883, test/num_examples=3003, total_duration=10306.980433, train/accuracy=0.619058, train/bleu=30.250873, train/loss=2.003643, validation/accuracy=0.639781, validation/bleu=26.927158, validation/loss=1.847423, validation/num_examples=3000
I0206 20:27:24.739363 140051097532160 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.29774534702301025, loss=3.1142773628234863
I0206 20:27:59.239889 140051089139456 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.25445374846458435, loss=3.0458388328552246
I0206 20:28:33.816281 140051097532160 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2755354046821594, loss=3.1110658645629883
I0206 20:29:08.361626 140051089139456 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.30045413970947266, loss=3.059088706970215
I0206 20:29:42.910503 140051097532160 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3194829523563385, loss=3.0865447521209717
I0206 20:30:17.464011 140051089139456 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2922617495059967, loss=3.122107982635498
I0206 20:30:52.036733 140051097532160 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3343600928783417, loss=3.067239761352539
I0206 20:31:26.592190 140051089139456 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2850572168827057, loss=3.0589818954467773
I0206 20:32:01.144715 140051097532160 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.304946631193161, loss=3.091005325317383
I0206 20:32:35.728006 140051089139456 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.33457475900650024, loss=3.0640673637390137
I0206 20:33:10.279551 140051097532160 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3611449897289276, loss=3.113534927368164
I0206 20:33:44.844205 140051089139456 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.36137843132019043, loss=3.075615644454956
I0206 20:34:19.416199 140051097532160 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.33990177512168884, loss=3.120053291320801
I0206 20:34:53.981047 140051089139456 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2848268747329712, loss=3.092092752456665
I0206 20:35:28.568368 140051097532160 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.285633385181427, loss=3.09702205657959
I0206 20:36:03.155084 140051089139456 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3234122693538666, loss=3.0451724529266357
I0206 20:36:37.728782 140051097532160 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.28054043650627136, loss=3.041621446609497
I0206 20:37:12.297589 140051089139456 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3729829788208008, loss=3.1304683685302734
I0206 20:37:46.851863 140051097532160 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.29695698618888855, loss=2.964656114578247
I0206 20:38:21.384445 140051089139456 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4101921617984772, loss=3.0267622470855713
I0206 20:38:55.937116 140051097532160 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.29319092631340027, loss=3.0725655555725098
I0206 20:39:30.480667 140051089139456 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3215903043746948, loss=3.0467162132263184
I0206 20:40:05.058774 140051097532160 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.329699844121933, loss=3.0304229259490967
I0206 20:40:39.633210 140051089139456 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3232108950614929, loss=3.0681004524230957
I0206 20:40:54.570411 140225696298816 spec.py:321] Evaluating on the training split.
I0206 20:40:57.536371 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:43:46.094814 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 20:43:48.778600 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:46:17.940114 140225696298816 spec.py:349] Evaluating on the test split.
I0206 20:46:20.622321 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 20:48:41.723419 140225696298816 submission_runner.py:408] Time since start: 11614.30s, 	Step: 19445, 	{'train/accuracy': 0.6344646215438843, 'train/loss': 1.8915237188339233, 'train/bleu': 30.712155809537048, 'validation/accuracy': 0.6446169018745422, 'validation/loss': 1.809007167816162, 'validation/bleu': 27.5092599023563, 'validation/num_examples': 3000, 'test/accuracy': 0.6539306640625, 'test/loss': 1.760025978088379, 'test/bleu': 26.60863093886362, 'test/num_examples': 3003, 'score': 6764.000878095627, 'total_duration': 11614.296314954758, 'accumulated_submission_time': 6764.000878095627, 'accumulated_eval_time': 4849.296671628952, 'accumulated_logging_time': 0.36723852157592773}
I0206 20:48:41.744403 140051097532160 logging_writer.py:48] [19445] accumulated_eval_time=4849.296672, accumulated_logging_time=0.367239, accumulated_submission_time=6764.000878, global_step=19445, preemption_count=0, score=6764.000878, test/accuracy=0.653931, test/bleu=26.608631, test/loss=1.760026, test/num_examples=3003, total_duration=11614.296315, train/accuracy=0.634465, train/bleu=30.712156, train/loss=1.891524, validation/accuracy=0.644617, validation/bleu=27.509260, validation/loss=1.809007, validation/num_examples=3000
I0206 20:49:01.029870 140051089139456 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.32656046748161316, loss=3.050753593444824
I0206 20:49:35.478971 140051097532160 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3697986900806427, loss=3.024681568145752
I0206 20:50:09.992808 140051089139456 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.31140589714050293, loss=3.07033109664917
I0206 20:50:44.534954 140051097532160 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3559102416038513, loss=3.1442196369171143
I0206 20:51:19.103803 140051089139456 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3072797358036041, loss=3.058142900466919
I0206 20:51:53.648760 140051097532160 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.31655070185661316, loss=3.0605790615081787
I0206 20:52:28.216593 140051089139456 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3982430100440979, loss=3.013666868209839
I0206 20:53:02.750753 140051097532160 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3439774215221405, loss=3.0157201290130615
I0206 20:53:37.308157 140051089139456 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.31134334206581116, loss=3.0554306507110596
I0206 20:54:11.866483 140051097532160 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.32500314712524414, loss=3.0962657928466797
I0206 20:54:46.424298 140051089139456 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.43170052766799927, loss=3.054044485092163
I0206 20:55:20.989217 140051097532160 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.32952988147735596, loss=3.07995867729187
I0206 20:55:55.525255 140051089139456 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3800673484802246, loss=3.044799327850342
I0206 20:56:30.101917 140051097532160 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.35582709312438965, loss=3.013104200363159
I0206 20:57:04.685935 140051089139456 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3031182587146759, loss=2.9737536907196045
I0206 20:57:39.245086 140051097532160 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.32175153493881226, loss=3.0009517669677734
I0206 20:58:13.812350 140051089139456 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4225156009197235, loss=2.9900197982788086
I0206 20:58:48.346195 140051097532160 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.39885956048965454, loss=3.028301477432251
I0206 20:59:22.910634 140051089139456 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.40500131249427795, loss=3.031465768814087
I0206 20:59:57.480061 140051097532160 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.31350263953208923, loss=3.0401620864868164
I0206 21:00:32.034701 140051089139456 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3163297772407532, loss=3.051018238067627
I0206 21:01:06.610744 140051097532160 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.33028116822242737, loss=3.0150601863861084
I0206 21:01:41.162358 140051089139456 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.34845224022865295, loss=2.9955201148986816
I0206 21:02:15.708252 140051097532160 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.4009314775466919, loss=3.0232608318328857
I0206 21:02:42.033761 140225696298816 spec.py:321] Evaluating on the training split.
I0206 21:02:44.999631 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:05:43.264921 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 21:05:45.941619 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:08:27.908506 140225696298816 spec.py:349] Evaluating on the test split.
I0206 21:08:30.578883 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:11:02.357930 140225696298816 submission_runner.py:408] Time since start: 12954.93s, 	Step: 21878, 	{'train/accuracy': 0.6279143691062927, 'train/loss': 1.938279628753662, 'train/bleu': 30.117385200900088, 'validation/accuracy': 0.6497625708580017, 'validation/loss': 1.7786868810653687, 'validation/bleu': 27.39135952713508, 'validation/num_examples': 3000, 'test/accuracy': 0.6575562357902527, 'test/loss': 1.7227576971054077, 'test/bleu': 26.950158208181367, 'test/num_examples': 3003, 'score': 7604.201534986496, 'total_duration': 12954.930808782578, 'accumulated_submission_time': 7604.201534986496, 'accumulated_eval_time': 5349.620749235153, 'accumulated_logging_time': 0.3979494571685791}
I0206 21:11:02.375734 140051089139456 logging_writer.py:48] [21878] accumulated_eval_time=5349.620749, accumulated_logging_time=0.397949, accumulated_submission_time=7604.201535, global_step=21878, preemption_count=0, score=7604.201535, test/accuracy=0.657556, test/bleu=26.950158, test/loss=1.722758, test/num_examples=3003, total_duration=12954.930809, train/accuracy=0.627914, train/bleu=30.117385, train/loss=1.938280, validation/accuracy=0.649763, validation/bleu=27.391360, validation/loss=1.778687, validation/num_examples=3000
I0206 21:11:10.303371 140051097532160 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.39610525965690613, loss=3.098945379257202
I0206 21:11:44.696434 140051089139456 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3682806193828583, loss=3.0726394653320312
I0206 21:12:19.225440 140051097532160 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8854342103004456, loss=3.0581560134887695
I0206 21:12:53.785925 140051089139456 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.4196905195713043, loss=3.1172523498535156
I0206 21:13:28.329969 140051097532160 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3302445709705353, loss=2.9458296298980713
I0206 21:14:02.888344 140051089139456 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.38568970561027527, loss=3.0801234245300293
I0206 21:14:37.426405 140051097532160 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3665072023868561, loss=3.058213233947754
I0206 21:15:11.974792 140051089139456 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.32346829771995544, loss=3.0019991397857666
I0206 21:15:46.527030 140051097532160 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.30085381865501404, loss=3.057558059692383
I0206 21:16:21.099509 140051089139456 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.32499387860298157, loss=3.0075058937072754
I0206 21:16:55.668234 140051097532160 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3504445254802704, loss=3.0289621353149414
I0206 21:17:30.215233 140051089139456 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3628789484500885, loss=3.1065311431884766
I0206 21:18:04.780339 140051097532160 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3375311493873596, loss=3.046420097351074
I0206 21:18:39.380207 140051089139456 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3057137727737427, loss=2.9963390827178955
I0206 21:19:13.910593 140051097532160 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.36311641335487366, loss=3.022716999053955
I0206 21:19:48.487001 140051089139456 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3143640160560608, loss=3.0215036869049072
I0206 21:20:23.048145 140051097532160 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3913246691226959, loss=2.997879981994629
I0206 21:20:57.591564 140051089139456 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3297983407974243, loss=3.01123309135437
I0206 21:21:32.146408 140051097532160 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.35775816440582275, loss=2.9744515419006348
I0206 21:22:06.695795 140051089139456 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.3186361491680145, loss=3.033555269241333
I0206 21:22:41.257528 140051097532160 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3574672043323517, loss=3.1053385734558105
I0206 21:23:15.799379 140051089139456 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3746388554573059, loss=2.980567693710327
I0206 21:23:50.358510 140051097532160 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3209541440010071, loss=3.0500881671905518
I0206 21:24:24.900966 140051089139456 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3802979290485382, loss=3.0500545501708984
I0206 21:24:59.451779 140051097532160 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.40141168236732483, loss=3.0344767570495605
I0206 21:25:02.628398 140225696298816 spec.py:321] Evaluating on the training split.
I0206 21:25:05.594101 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:29:26.396577 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 21:29:29.080206 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:33:20.640414 140225696298816 spec.py:349] Evaluating on the test split.
I0206 21:33:23.320357 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:36:33.087527 140225696298816 submission_runner.py:408] Time since start: 14485.66s, 	Step: 24311, 	{'train/accuracy': 0.6267665028572083, 'train/loss': 1.955460786819458, 'train/bleu': 30.495337151691416, 'validation/accuracy': 0.6528747081756592, 'validation/loss': 1.7616363763809204, 'validation/bleu': 27.740078969040887, 'validation/num_examples': 3000, 'test/accuracy': 0.6625530123710632, 'test/loss': 1.7036962509155273, 'test/bleu': 27.38874092003688, 'test/num_examples': 3003, 'score': 8444.363789081573, 'total_duration': 14485.660428762436, 'accumulated_submission_time': 8444.363789081573, 'accumulated_eval_time': 6040.07980966568, 'accumulated_logging_time': 0.4274477958679199}
I0206 21:36:33.106440 140051089139456 logging_writer.py:48] [24311] accumulated_eval_time=6040.079810, accumulated_logging_time=0.427448, accumulated_submission_time=8444.363789, global_step=24311, preemption_count=0, score=8444.363789, test/accuracy=0.662553, test/bleu=27.388741, test/loss=1.703696, test/num_examples=3003, total_duration=14485.660429, train/accuracy=0.626767, train/bleu=30.495337, train/loss=1.955461, validation/accuracy=0.652875, validation/bleu=27.740079, validation/loss=1.761636, validation/num_examples=3000
I0206 21:37:04.077240 140051097532160 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.33942076563835144, loss=3.0289015769958496
I0206 21:37:38.576407 140051089139456 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.33331990242004395, loss=3.0191476345062256
I0206 21:38:13.105401 140051097532160 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.33936136960983276, loss=3.0356969833374023
I0206 21:38:47.633238 140051089139456 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3673265278339386, loss=3.0121328830718994
I0206 21:39:22.167148 140051097532160 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.376255065202713, loss=3.0302042961120605
I0206 21:39:56.706300 140051089139456 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.350363552570343, loss=2.9867587089538574
I0206 21:40:31.234474 140051097532160 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.39649564027786255, loss=2.9732754230499268
I0206 21:41:05.794697 140051089139456 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3671106994152069, loss=3.0009775161743164
I0206 21:41:40.341062 140051097532160 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3880082666873932, loss=3.015005588531494
I0206 21:42:14.901454 140051089139456 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.40503057837486267, loss=3.1098761558532715
I0206 21:42:49.459012 140051097532160 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.47513657808303833, loss=2.9897539615631104
I0206 21:43:24.002348 140051089139456 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3328758776187897, loss=2.9208691120147705
I0206 21:43:58.558179 140051097532160 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.3646116554737091, loss=2.947063684463501
I0206 21:44:33.113142 140051089139456 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3546947240829468, loss=3.0341427326202393
I0206 21:45:07.668571 140051097532160 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.35844850540161133, loss=3.1335387229919434
I0206 21:45:42.248053 140051089139456 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.34480684995651245, loss=2.9378626346588135
I0206 21:46:16.805712 140051097532160 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.35523462295532227, loss=2.943679094314575
I0206 21:46:51.357754 140051089139456 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.32188165187835693, loss=2.9618499279022217
I0206 21:47:25.921833 140051097532160 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.44603976607322693, loss=3.047274351119995
I0206 21:48:00.489891 140051089139456 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.31515857577323914, loss=2.986023426055908
I0206 21:48:35.029686 140051097532160 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.40134313702583313, loss=3.0397415161132812
I0206 21:49:09.563402 140051089139456 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3574959635734558, loss=3.0046424865722656
I0206 21:49:44.105004 140051097532160 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.42174261808395386, loss=3.0625884532928467
I0206 21:50:18.671517 140051089139456 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.37988919019699097, loss=3.0080554485321045
I0206 21:50:33.248440 140225696298816 spec.py:321] Evaluating on the training split.
I0206 21:50:36.216765 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:53:23.615896 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 21:53:26.293608 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:56:02.689440 140225696298816 spec.py:349] Evaluating on the test split.
I0206 21:56:05.370625 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 21:58:29.808655 140225696298816 submission_runner.py:408] Time since start: 15802.38s, 	Step: 26744, 	{'train/accuracy': 0.6343207955360413, 'train/loss': 1.8967632055282593, 'train/bleu': 31.050486129125893, 'validation/accuracy': 0.6534822583198547, 'validation/loss': 1.7492674589157104, 'validation/bleu': 28.098133062791273, 'validation/num_examples': 3000, 'test/accuracy': 0.6628900170326233, 'test/loss': 1.6931463479995728, 'test/bleu': 27.042964415040156, 'test/num_examples': 3003, 'score': 9284.414820194244, 'total_duration': 15802.381581544876, 'accumulated_submission_time': 9284.414820194244, 'accumulated_eval_time': 6516.639975786209, 'accumulated_logging_time': 0.45822668075561523}
I0206 21:58:29.827002 140051097532160 logging_writer.py:48] [26744] accumulated_eval_time=6516.639976, accumulated_logging_time=0.458227, accumulated_submission_time=9284.414820, global_step=26744, preemption_count=0, score=9284.414820, test/accuracy=0.662890, test/bleu=27.042964, test/loss=1.693146, test/num_examples=3003, total_duration=15802.381582, train/accuracy=0.634321, train/bleu=31.050486, train/loss=1.896763, validation/accuracy=0.653482, validation/bleu=28.098133, validation/loss=1.749267, validation/num_examples=3000
I0206 21:58:49.455712 140051089139456 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4564772844314575, loss=3.010631799697876
I0206 21:59:23.897346 140051097532160 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.32476457953453064, loss=3.0596823692321777
I0206 21:59:58.396340 140051089139456 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.34300824999809265, loss=2.9427506923675537
I0206 22:00:32.948541 140051097532160 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.36830246448516846, loss=2.962611436843872
I0206 22:01:07.518020 140051089139456 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.36932575702667236, loss=2.9610652923583984
I0206 22:01:42.067636 140051097532160 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3839508593082428, loss=3.025801658630371
I0206 22:02:16.596359 140051089139456 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3976338803768158, loss=2.91922926902771
I0206 22:02:51.136449 140051097532160 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5328803062438965, loss=3.0313634872436523
I0206 22:03:25.698433 140051089139456 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.343466579914093, loss=2.9978761672973633
I0206 22:04:00.232345 140051097532160 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3254430294036865, loss=2.9895944595336914
I0206 22:04:34.800845 140051089139456 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.3773927688598633, loss=3.02337908744812
I0206 22:05:09.338634 140051097532160 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.33951398730278015, loss=2.9872007369995117
I0206 22:05:43.894582 140051089139456 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.41107144951820374, loss=3.0083625316619873
I0206 22:06:18.432751 140051097532160 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.35774847865104675, loss=2.98960018157959
I0206 22:06:52.979427 140051089139456 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.37741971015930176, loss=2.9920032024383545
I0206 22:07:27.543952 140051097532160 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.3643139898777008, loss=2.959882974624634
I0206 22:08:02.095556 140051089139456 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3373842239379883, loss=3.0354506969451904
I0206 22:08:36.651034 140051097532160 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3582797646522522, loss=3.0259392261505127
I0206 22:09:11.217184 140051089139456 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.334963321685791, loss=3.02494478225708
I0206 22:09:45.772025 140051097532160 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.3599240779876709, loss=3.0276730060577393
I0206 22:10:20.319961 140051089139456 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.441852331161499, loss=3.0144762992858887
I0206 22:10:54.871145 140051097532160 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.34129613637924194, loss=3.0069313049316406
I0206 22:11:29.423771 140051089139456 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.339912086725235, loss=3.1225297451019287
I0206 22:12:03.990159 140051097532160 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.39161795377731323, loss=2.9726810455322266
I0206 22:12:29.979358 140225696298816 spec.py:321] Evaluating on the training split.
I0206 22:12:32.955475 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:15:13.342943 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 22:15:16.026985 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:17:55.823877 140225696298816 spec.py:349] Evaluating on the test split.
I0206 22:17:58.504079 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:20:29.847136 140225696298816 submission_runner.py:408] Time since start: 17122.42s, 	Step: 29177, 	{'train/accuracy': 0.6336384415626526, 'train/loss': 1.8958899974822998, 'train/bleu': 30.684595268987056, 'validation/accuracy': 0.6565200686454773, 'validation/loss': 1.7404468059539795, 'validation/bleu': 28.038795719325957, 'validation/num_examples': 3000, 'test/accuracy': 0.665864884853363, 'test/loss': 1.6830744743347168, 'test/bleu': 27.865501179623493, 'test/num_examples': 3003, 'score': 10124.47589802742, 'total_duration': 17122.42004466057, 'accumulated_submission_time': 10124.47589802742, 'accumulated_eval_time': 6996.507694721222, 'accumulated_logging_time': 0.4879882335662842}
I0206 22:20:29.866336 140051089139456 logging_writer.py:48] [29177] accumulated_eval_time=6996.507695, accumulated_logging_time=0.487988, accumulated_submission_time=10124.475898, global_step=29177, preemption_count=0, score=10124.475898, test/accuracy=0.665865, test/bleu=27.865501, test/loss=1.683074, test/num_examples=3003, total_duration=17122.420045, train/accuracy=0.633638, train/bleu=30.684595, train/loss=1.895890, validation/accuracy=0.656520, validation/bleu=28.038796, validation/loss=1.740447, validation/num_examples=3000
I0206 22:20:38.141243 140051097532160 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4271574020385742, loss=2.967437267303467
I0206 22:21:12.571988 140051089139456 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3583991825580597, loss=3.006251096725464
I0206 22:21:47.077131 140051097532160 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.7146084308624268, loss=3.2324330806732178
I0206 22:22:21.611881 140051089139456 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.45927396416664124, loss=3.017561197280884
I0206 22:22:56.175009 140051097532160 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4140201807022095, loss=2.9761364459991455
I0206 22:23:30.743765 140051089139456 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3280981183052063, loss=3.0392701625823975
I0206 22:24:05.302136 140051097532160 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.33681201934814453, loss=2.983100414276123
I0206 22:24:39.860636 140051089139456 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3208659291267395, loss=2.93422532081604
I0206 22:25:14.403160 140051097532160 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.31743261218070984, loss=2.9493484497070312
I0206 22:25:48.971368 140051089139456 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3536013662815094, loss=2.917856454849243
I0206 22:26:23.517942 140051097532160 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.34185662865638733, loss=3.033515453338623
I0206 22:26:58.056763 140051089139456 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3006218373775482, loss=2.9516091346740723
I0206 22:27:32.603074 140051097532160 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.3330564498901367, loss=2.9403865337371826
I0206 22:28:07.146008 140051089139456 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.34386351704597473, loss=3.0125250816345215
I0206 22:28:41.694283 140051097532160 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.31752637028694153, loss=2.9709794521331787
I0206 22:29:16.238958 140051089139456 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3775363862514496, loss=2.951460361480713
I0206 22:29:50.787531 140051097532160 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3375172019004822, loss=2.992433547973633
I0206 22:30:25.342206 140051089139456 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.32267290353775024, loss=2.874720335006714
I0206 22:30:59.890129 140051097532160 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3070148527622223, loss=2.959113359451294
I0206 22:31:34.433115 140051089139456 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3142210841178894, loss=2.939058303833008
I0206 22:32:08.977256 140051097532160 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3046954274177551, loss=2.9419941902160645
I0206 22:32:43.559713 140051089139456 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.4457380771636963, loss=2.9790239334106445
I0206 22:33:18.107942 140051097532160 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3763352036476135, loss=2.9091384410858154
I0206 22:33:52.658378 140051089139456 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3866947293281555, loss=2.964900255203247
I0206 22:34:27.192821 140051097532160 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.33769887685775757, loss=3.0288150310516357
I0206 22:34:30.028331 140225696298816 spec.py:321] Evaluating on the training split.
I0206 22:34:33.008851 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:37:13.631073 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 22:37:16.307583 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:39:44.448050 140225696298816 spec.py:349] Evaluating on the test split.
I0206 22:39:47.133573 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:42:11.314889 140225696298816 submission_runner.py:408] Time since start: 18423.89s, 	Step: 31610, 	{'train/accuracy': 0.6526271104812622, 'train/loss': 1.768532156944275, 'train/bleu': 32.05716202678791, 'validation/accuracy': 0.654350221157074, 'validation/loss': 1.736448884010315, 'validation/bleu': 28.03466477961463, 'validation/num_examples': 3000, 'test/accuracy': 0.6660043001174927, 'test/loss': 1.6788660287857056, 'test/bleu': 27.773628041650408, 'test/num_examples': 3003, 'score': 10964.547756910324, 'total_duration': 18423.887810707092, 'accumulated_submission_time': 10964.547756910324, 'accumulated_eval_time': 7457.794209480286, 'accumulated_logging_time': 0.5176031589508057}
I0206 22:42:11.335144 140051089139456 logging_writer.py:48] [31610] accumulated_eval_time=7457.794209, accumulated_logging_time=0.517603, accumulated_submission_time=10964.547757, global_step=31610, preemption_count=0, score=10964.547757, test/accuracy=0.666004, test/bleu=27.773628, test/loss=1.678866, test/num_examples=3003, total_duration=18423.887811, train/accuracy=0.652627, train/bleu=32.057162, train/loss=1.768532, validation/accuracy=0.654350, validation/bleu=28.034665, validation/loss=1.736449, validation/num_examples=3000
I0206 22:42:42.675615 140051097532160 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.35281121730804443, loss=2.9881834983825684
I0206 22:43:17.143543 140051089139456 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3992646038532257, loss=3.0173113346099854
I0206 22:43:51.699003 140051097532160 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3867289125919342, loss=3.0122058391571045
I0206 22:44:26.199447 140051089139456 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3187803328037262, loss=2.975834846496582
I0206 22:45:00.695207 140051097532160 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.35571402311325073, loss=3.04416823387146
I0206 22:45:35.198364 140051089139456 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.3305250108242035, loss=2.9540185928344727
I0206 22:46:09.743550 140051097532160 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3378365933895111, loss=2.931891679763794
I0206 22:46:44.254742 140051089139456 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.362798273563385, loss=2.963789224624634
I0206 22:47:18.793822 140051097532160 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3935371935367584, loss=2.944917678833008
I0206 22:47:53.369527 140051089139456 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.44860392808914185, loss=2.9012343883514404
I0206 22:48:27.943079 140051097532160 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3429122865200043, loss=3.0268778800964355
I0206 22:49:02.501566 140051089139456 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.37707486748695374, loss=3.023736000061035
I0206 22:49:37.044476 140051097532160 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.34528830647468567, loss=3.019596576690674
I0206 22:50:11.621103 140051089139456 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3400487005710602, loss=2.9827840328216553
I0206 22:50:46.156247 140051097532160 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.3467952311038971, loss=2.9917500019073486
I0206 22:51:20.708946 140051089139456 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.33490100502967834, loss=3.0016183853149414
I0206 22:51:55.261206 140051097532160 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4375890791416168, loss=2.978339195251465
I0206 22:52:29.806618 140051089139456 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.3203152120113373, loss=3.0426175594329834
I0206 22:53:04.331088 140051097532160 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.37545010447502136, loss=3.0030407905578613
I0206 22:53:38.846689 140051089139456 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.34984371066093445, loss=2.9948744773864746
I0206 22:54:13.401498 140051097532160 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.34215155243873596, loss=2.983966112136841
I0206 22:54:47.932315 140051089139456 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.36093810200691223, loss=2.9985246658325195
I0206 22:55:22.472491 140051097532160 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.32820528745651245, loss=2.9078714847564697
I0206 22:55:57.024763 140051089139456 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.33786067366600037, loss=3.000822067260742
I0206 22:56:11.586704 140225696298816 spec.py:321] Evaluating on the training split.
I0206 22:56:14.555269 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 22:59:04.903896 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 22:59:07.588942 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:01:46.048958 140225696298816 spec.py:349] Evaluating on the test split.
I0206 23:01:48.743808 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:04:20.495954 140225696298816 submission_runner.py:408] Time since start: 19753.07s, 	Step: 34044, 	{'train/accuracy': 0.6363418102264404, 'train/loss': 1.877790093421936, 'train/bleu': 31.210927634435524, 'validation/accuracy': 0.65668123960495, 'validation/loss': 1.7198882102966309, 'validation/bleu': 28.09483148598837, 'validation/num_examples': 3000, 'test/accuracy': 0.6665620803833008, 'test/loss': 1.669747233390808, 'test/bleu': 27.553497282281434, 'test/num_examples': 3003, 'score': 11804.70926952362, 'total_duration': 19753.06885743141, 'accumulated_submission_time': 11804.70926952362, 'accumulated_eval_time': 7946.703389883041, 'accumulated_logging_time': 0.5480039119720459}
I0206 23:04:20.515243 140051097532160 logging_writer.py:48] [34044] accumulated_eval_time=7946.703390, accumulated_logging_time=0.548004, accumulated_submission_time=11804.709270, global_step=34044, preemption_count=0, score=11804.709270, test/accuracy=0.666562, test/bleu=27.553497, test/loss=1.669747, test/num_examples=3003, total_duration=19753.068857, train/accuracy=0.636342, train/bleu=31.210928, train/loss=1.877790, validation/accuracy=0.656681, validation/bleu=28.094831, validation/loss=1.719888, validation/num_examples=3000
I0206 23:04:40.121243 140051089139456 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.36705946922302246, loss=3.0096535682678223
I0206 23:05:14.571434 140051097532160 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3412443697452545, loss=3.0199735164642334
I0206 23:05:49.081586 140051089139456 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3426613509654999, loss=2.9288666248321533
I0206 23:06:23.631139 140051097532160 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.369814395904541, loss=2.9160542488098145
I0206 23:06:58.164779 140051089139456 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.32200998067855835, loss=2.9668562412261963
I0206 23:07:32.696991 140051097532160 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.37358781695365906, loss=2.9490859508514404
I0206 23:08:07.230389 140051089139456 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3012769818305969, loss=2.9819607734680176
I0206 23:08:41.772618 140051097532160 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.38408729434013367, loss=2.9457080364227295
I0206 23:09:16.326833 140051089139456 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3854862451553345, loss=2.9963793754577637
I0206 23:09:50.850470 140051097532160 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3626769483089447, loss=2.9296250343322754
I0206 23:10:25.404869 140051089139456 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3784009516239166, loss=3.0702521800994873
I0206 23:10:59.952646 140051097532160 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.32901111245155334, loss=2.9465270042419434
I0206 23:11:34.499281 140051089139456 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.383510947227478, loss=3.0463263988494873
I0206 23:12:09.029332 140051097532160 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.37677180767059326, loss=2.9821507930755615
I0206 23:12:43.568986 140051089139456 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.35553601384162903, loss=2.9748380184173584
I0206 23:13:18.092535 140051097532160 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.33217737078666687, loss=2.928133487701416
I0206 23:13:52.603866 140051089139456 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.35956257581710815, loss=2.977728843688965
I0206 23:14:27.129568 140051097532160 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.34863370656967163, loss=2.994347333908081
I0206 23:15:01.674069 140051089139456 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.31717273592948914, loss=2.951871156692505
I0206 23:15:36.214029 140051097532160 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3790156841278076, loss=3.0173981189727783
I0206 23:16:10.738646 140051089139456 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.44720137119293213, loss=3.024442672729492
I0206 23:16:45.278608 140051097532160 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.32907310128211975, loss=2.965606451034546
I0206 23:17:19.791896 140051089139456 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.4102109670639038, loss=2.9565770626068115
I0206 23:17:54.347183 140051097532160 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.33285775780677795, loss=3.0366621017456055
I0206 23:18:20.673431 140225696298816 spec.py:321] Evaluating on the training split.
I0206 23:18:23.635008 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:21:32.626445 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 23:21:35.294811 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:25:14.124922 140225696298816 spec.py:349] Evaluating on the test split.
I0206 23:25:16.806185 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:28:13.008966 140225696298816 submission_runner.py:408] Time since start: 21185.58s, 	Step: 36478, 	{'train/accuracy': 0.6364160180091858, 'train/loss': 1.8722435235977173, 'train/bleu': 31.076741563059336, 'validation/accuracy': 0.6580327749252319, 'validation/loss': 1.717186689376831, 'validation/bleu': 28.03697177435267, 'validation/num_examples': 3000, 'test/accuracy': 0.6687583923339844, 'test/loss': 1.6572718620300293, 'test/bleu': 27.756396069290414, 'test/num_examples': 3003, 'score': 12644.779065132141, 'total_duration': 21185.581884860992, 'accumulated_submission_time': 12644.779065132141, 'accumulated_eval_time': 8539.038867473602, 'accumulated_logging_time': 0.5769636631011963}
I0206 23:28:13.028208 140051089139456 logging_writer.py:48] [36478] accumulated_eval_time=8539.038867, accumulated_logging_time=0.576964, accumulated_submission_time=12644.779065, global_step=36478, preemption_count=0, score=12644.779065, test/accuracy=0.668758, test/bleu=27.756396, test/loss=1.657272, test/num_examples=3003, total_duration=21185.581885, train/accuracy=0.636416, train/bleu=31.076742, train/loss=1.872244, validation/accuracy=0.658033, validation/bleu=28.036972, validation/loss=1.717187, validation/num_examples=3000
I0206 23:28:20.968291 140051097532160 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3320102393627167, loss=2.909360885620117
I0206 23:28:55.372156 140051089139456 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.34328505396842957, loss=3.0108087062835693
I0206 23:29:29.873072 140051097532160 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.32457202672958374, loss=2.926135540008545
I0206 23:30:04.436551 140051089139456 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.34820792078971863, loss=3.0048317909240723
I0206 23:30:38.952054 140051097532160 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3688930869102478, loss=3.0000603199005127
I0206 23:31:13.474486 140051089139456 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.45804840326309204, loss=2.946274518966675
I0206 23:31:48.014957 140051097532160 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3918023407459259, loss=2.935589075088501
I0206 23:32:22.542166 140051089139456 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4033210277557373, loss=2.9977242946624756
I0206 23:32:57.071863 140051097532160 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.40757617354393005, loss=2.949845314025879
I0206 23:33:31.602514 140051089139456 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.43425875902175903, loss=2.9695396423339844
I0206 23:34:06.131882 140051097532160 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.3410804271697998, loss=2.9843976497650146
I0206 23:34:40.694124 140051089139456 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.8645025491714478, loss=2.9863507747650146
I0206 23:35:15.225624 140051097532160 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.42322829365730286, loss=3.0122690200805664
I0206 23:35:49.764420 140051089139456 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.41915661096572876, loss=2.9626104831695557
I0206 23:36:24.320516 140051097532160 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.39540255069732666, loss=3.033766984939575
I0206 23:36:58.850892 140051089139456 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.34961748123168945, loss=2.940406560897827
I0206 23:37:33.411062 140051097532160 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3450307250022888, loss=2.920097827911377
I0206 23:38:07.949288 140051089139456 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3423970341682434, loss=2.959038019180298
I0206 23:38:42.469748 140051097532160 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.3559788465499878, loss=2.9492251873016357
I0206 23:39:17.002847 140051089139456 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.4214603304862976, loss=2.913508892059326
I0206 23:39:51.521024 140051097532160 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3770177662372589, loss=2.9377267360687256
I0206 23:40:26.047251 140051089139456 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3530614376068115, loss=3.0043396949768066
I0206 23:41:00.597718 140051097532160 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3141523003578186, loss=2.906121253967285
I0206 23:41:35.136941 140051089139456 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3762931823730469, loss=2.92781925201416
I0206 23:42:09.656481 140051097532160 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3248707950115204, loss=2.9990506172180176
I0206 23:42:13.175758 140225696298816 spec.py:321] Evaluating on the training split.
I0206 23:42:16.155449 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:45:08.547714 140225696298816 spec.py:333] Evaluating on the validation split.
I0206 23:45:11.209954 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:47:52.279320 140225696298816 spec.py:349] Evaluating on the test split.
I0206 23:47:54.943448 140225696298816 workload.py:181] Translating evaluation dataset.
I0206 23:50:18.422758 140225696298816 submission_runner.py:408] Time since start: 22511.00s, 	Step: 38912, 	{'train/accuracy': 0.6482433080673218, 'train/loss': 1.7970041036605835, 'train/bleu': 31.432741998554356, 'validation/accuracy': 0.6609093546867371, 'validation/loss': 1.703112006187439, 'validation/bleu': 28.180620886314376, 'validation/num_examples': 3000, 'test/accuracy': 0.6716634631156921, 'test/loss': 1.6385266780853271, 'test/bleu': 27.945611634666342, 'test/num_examples': 3003, 'score': 13484.840069770813, 'total_duration': 22510.995681285858, 'accumulated_submission_time': 13484.840069770813, 'accumulated_eval_time': 9024.285813570023, 'accumulated_logging_time': 0.6064877510070801}
I0206 23:50:18.442937 140051089139456 logging_writer.py:48] [38912] accumulated_eval_time=9024.285814, accumulated_logging_time=0.606488, accumulated_submission_time=13484.840070, global_step=38912, preemption_count=0, score=13484.840070, test/accuracy=0.671663, test/bleu=27.945612, test/loss=1.638527, test/num_examples=3003, total_duration=22510.995681, train/accuracy=0.648243, train/bleu=31.432742, train/loss=1.797004, validation/accuracy=0.660909, validation/bleu=28.180621, validation/loss=1.703112, validation/num_examples=3000
I0206 23:50:49.035713 140051097532160 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.31525275111198425, loss=2.8867220878601074
I0206 23:51:23.513987 140051089139456 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.34738054871559143, loss=2.9545340538024902
I0206 23:51:58.057571 140051097532160 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3451341390609741, loss=2.982342004776001
I0206 23:52:32.626697 140051089139456 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.34035828709602356, loss=3.0563111305236816
I0206 23:53:07.171230 140051097532160 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.35297495126724243, loss=2.954301118850708
I0206 23:53:41.694807 140051089139456 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3130544424057007, loss=2.923185110092163
I0206 23:54:16.236181 140051097532160 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.32689544558525085, loss=2.9467015266418457
I0206 23:54:50.768155 140051089139456 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.32536160945892334, loss=2.9488229751586914
I0206 23:55:25.324712 140051097532160 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4262058734893799, loss=2.938777208328247
I0206 23:55:59.845515 140051089139456 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.4232228100299835, loss=2.901815176010132
I0206 23:56:34.383788 140051097532160 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.352580726146698, loss=2.926185369491577
I0206 23:57:08.915673 140051089139456 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.351433128118515, loss=2.904768943786621
I0206 23:57:43.450335 140051097532160 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.32869452238082886, loss=2.9393973350524902
I0206 23:58:17.991302 140051089139456 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.39151185750961304, loss=3.0058562755584717
I0206 23:58:52.513169 140051097532160 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.3881721794605255, loss=3.0124402046203613
I0206 23:59:27.052861 140051089139456 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.34772998094558716, loss=2.9492275714874268
I0207 00:00:01.609571 140051097532160 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3313533663749695, loss=2.900494337081909
I0207 00:00:36.151288 140051089139456 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3511368930339813, loss=2.9599924087524414
I0207 00:01:10.680872 140051097532160 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3573135435581207, loss=2.992459535598755
I0207 00:01:45.188626 140051089139456 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3372434973716736, loss=3.0127127170562744
I0207 00:02:19.728164 140051097532160 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.3228769302368164, loss=2.900723695755005
I0207 00:02:54.280019 140051089139456 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.3637548089027405, loss=2.997750997543335
I0207 00:03:28.807225 140051097532160 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.33375248312950134, loss=2.88592529296875
I0207 00:04:03.329410 140051089139456 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.34065231680870056, loss=2.9600656032562256
I0207 00:04:18.586595 140225696298816 spec.py:321] Evaluating on the training split.
I0207 00:04:21.553413 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:07:09.541793 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 00:07:12.216434 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:09:44.470784 140225696298816 spec.py:349] Evaluating on the test split.
I0207 00:09:47.150014 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:12:12.947210 140225696298816 submission_runner.py:408] Time since start: 23825.52s, 	Step: 41346, 	{'train/accuracy': 0.6373471021652222, 'train/loss': 1.8696467876434326, 'train/bleu': 31.01315039572357, 'validation/accuracy': 0.6601405739784241, 'validation/loss': 1.7056689262390137, 'validation/bleu': 28.38573685693433, 'validation/num_examples': 3000, 'test/accuracy': 0.6728255152702332, 'test/loss': 1.63887357711792, 'test/bleu': 28.00362928869641, 'test/num_examples': 3003, 'score': 14324.897027015686, 'total_duration': 23825.52012705803, 'accumulated_submission_time': 14324.897027015686, 'accumulated_eval_time': 9498.646374464035, 'accumulated_logging_time': 0.6380147933959961}
I0207 00:12:12.967256 140051097532160 logging_writer.py:48] [41346] accumulated_eval_time=9498.646374, accumulated_logging_time=0.638015, accumulated_submission_time=14324.897027, global_step=41346, preemption_count=0, score=14324.897027, test/accuracy=0.672826, test/bleu=28.003629, test/loss=1.638874, test/num_examples=3003, total_duration=23825.520127, train/accuracy=0.637347, train/bleu=31.013150, train/loss=1.869647, validation/accuracy=0.660141, validation/bleu=28.385737, validation/loss=1.705669, validation/num_examples=3000
I0207 00:12:31.926336 140051089139456 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.3617677092552185, loss=2.959761381149292
I0207 00:13:06.296231 140051097532160 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3232823312282562, loss=2.835897922515869
I0207 00:13:40.794737 140051089139456 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.40405961871147156, loss=3.000500202178955
I0207 00:14:15.325796 140051097532160 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3614712953567505, loss=2.8914847373962402
I0207 00:14:49.825854 140051089139456 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3436778783798218, loss=3.0052261352539062
I0207 00:15:24.390578 140051097532160 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3274221420288086, loss=3.0411434173583984
I0207 00:15:58.926239 140051089139456 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.33277225494384766, loss=2.9406163692474365
I0207 00:16:33.453779 140051097532160 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.35489320755004883, loss=3.0483577251434326
I0207 00:17:07.985131 140051089139456 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.35373541712760925, loss=2.966660976409912
I0207 00:17:42.522297 140051097532160 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3583766222000122, loss=3.0182037353515625
I0207 00:18:17.072639 140051089139456 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.36650052666664124, loss=2.9174582958221436
I0207 00:18:51.595141 140051097532160 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3757500648498535, loss=2.9403467178344727
I0207 00:19:26.133906 140051089139456 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3468141257762909, loss=2.9773433208465576
I0207 00:20:00.681470 140051097532160 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3400503098964691, loss=2.912177562713623
I0207 00:20:35.234469 140051089139456 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3875981271266937, loss=2.969205141067505
I0207 00:21:09.744375 140051097532160 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.33177557587623596, loss=2.944420576095581
I0207 00:21:44.270329 140051089139456 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.33986344933509827, loss=2.9741427898406982
I0207 00:22:18.806371 140051097532160 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.330650269985199, loss=2.950202226638794
I0207 00:22:53.349386 140051089139456 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.34091976284980774, loss=2.9050424098968506
I0207 00:23:27.891053 140051097532160 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3188938796520233, loss=2.9341328144073486
I0207 00:24:02.423354 140051089139456 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3548195958137512, loss=2.9360885620117188
I0207 00:24:36.938437 140051097532160 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.32588401436805725, loss=2.9128527641296387
I0207 00:25:11.438359 140051089139456 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.5393248200416565, loss=2.9413869380950928
I0207 00:25:45.985572 140051097532160 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.40393051505088806, loss=2.9293105602264404
I0207 00:26:12.984041 140225696298816 spec.py:321] Evaluating on the training split.
I0207 00:26:15.957795 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:29:13.582380 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 00:29:16.261216 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:32:00.474073 140225696298816 spec.py:349] Evaluating on the test split.
I0207 00:32:03.158321 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:34:23.679230 140225696298816 submission_runner.py:408] Time since start: 25156.25s, 	Step: 43780, 	{'train/accuracy': 0.6370298266410828, 'train/loss': 1.8723466396331787, 'train/bleu': 31.06694249891112, 'validation/accuracy': 0.6600537896156311, 'validation/loss': 1.697940468788147, 'validation/bleu': 28.26108878119694, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.6407963037490845, 'test/bleu': 27.696134166557446, 'test/num_examples': 3003, 'score': 15164.826464653015, 'total_duration': 25156.252128362656, 'accumulated_submission_time': 15164.826464653015, 'accumulated_eval_time': 9989.341492176056, 'accumulated_logging_time': 0.668973445892334}
I0207 00:34:23.699406 140051089139456 logging_writer.py:48] [43780] accumulated_eval_time=9989.341492, accumulated_logging_time=0.668973, accumulated_submission_time=15164.826465, global_step=43780, preemption_count=0, score=15164.826465, test/accuracy=0.671338, test/bleu=27.696134, test/loss=1.640796, test/num_examples=3003, total_duration=25156.252128, train/accuracy=0.637030, train/bleu=31.066942, train/loss=1.872347, validation/accuracy=0.660054, validation/bleu=28.261089, validation/loss=1.697940, validation/num_examples=3000
I0207 00:34:30.921869 140051097532160 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3381626307964325, loss=2.9959521293640137
I0207 00:35:05.340395 140051089139456 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3741976022720337, loss=2.896686553955078
I0207 00:35:39.867161 140051097532160 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.33923640847206116, loss=2.92842698097229
I0207 00:36:14.411010 140051089139456 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.36470064520835876, loss=2.9993512630462646
I0207 00:36:48.964252 140051097532160 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.35893842577934265, loss=2.8633108139038086
I0207 00:37:23.480447 140051089139456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.39927130937576294, loss=2.9861080646514893
I0207 00:37:58.002850 140051097532160 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.33541902899742126, loss=2.92429518699646
I0207 00:38:32.550185 140051089139456 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.32836440205574036, loss=2.9610321521759033
I0207 00:39:07.109884 140051097532160 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.3506910800933838, loss=2.9487879276275635
I0207 00:39:41.665395 140051089139456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.33229586482048035, loss=2.9778671264648438
I0207 00:40:16.202339 140051097532160 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.33212119340896606, loss=2.9168779850006104
I0207 00:40:50.737547 140051089139456 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.37897634506225586, loss=2.889808177947998
I0207 00:41:25.272433 140051097532160 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3508775532245636, loss=2.911936044692993
I0207 00:41:59.779174 140051089139456 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.36930498480796814, loss=2.970911979675293
I0207 00:42:34.324678 140051097532160 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3478598892688751, loss=2.935234308242798
I0207 00:43:08.830235 140051089139456 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.34505563974380493, loss=2.9161884784698486
I0207 00:43:43.376588 140051097532160 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3211207985877991, loss=2.9689178466796875
I0207 00:44:17.918958 140051089139456 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.37320655584335327, loss=2.9331328868865967
I0207 00:44:52.461326 140051097532160 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3634892702102661, loss=2.9326019287109375
I0207 00:45:27.020295 140051089139456 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.36803945899009705, loss=2.977060317993164
I0207 00:46:01.557886 140051097532160 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.34497329592704773, loss=2.941394329071045
I0207 00:46:36.060540 140051089139456 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3507508933544159, loss=2.966773748397827
I0207 00:47:10.603376 140051097532160 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3754139244556427, loss=2.9403839111328125
I0207 00:47:45.135637 140051089139456 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3893856406211853, loss=2.9949474334716797
I0207 00:48:19.675243 140051097532160 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.4015156626701355, loss=2.9472572803497314
I0207 00:48:23.885740 140225696298816 spec.py:321] Evaluating on the training split.
I0207 00:48:26.843468 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:51:27.621397 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 00:51:30.290757 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:53:57.943359 140225696298816 spec.py:349] Evaluating on the test split.
I0207 00:54:00.618484 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 00:56:23.997555 140225696298816 submission_runner.py:408] Time since start: 26476.57s, 	Step: 46214, 	{'train/accuracy': 0.6392388343811035, 'train/loss': 1.8472505807876587, 'train/bleu': 31.986452000965535, 'validation/accuracy': 0.6632279753684998, 'validation/loss': 1.6884485483169556, 'validation/bleu': 28.6184400125844, 'validation/num_examples': 3000, 'test/accuracy': 0.6738249063491821, 'test/loss': 1.6290220022201538, 'test/bleu': 28.05211040280357, 'test/num_examples': 3003, 'score': 16004.92531490326, 'total_duration': 26476.57047533989, 'accumulated_submission_time': 16004.92531490326, 'accumulated_eval_time': 10469.453252315521, 'accumulated_logging_time': 0.7002456188201904}
I0207 00:56:24.017968 140051089139456 logging_writer.py:48] [46214] accumulated_eval_time=10469.453252, accumulated_logging_time=0.700246, accumulated_submission_time=16004.925315, global_step=46214, preemption_count=0, score=16004.925315, test/accuracy=0.673825, test/bleu=28.052110, test/loss=1.629022, test/num_examples=3003, total_duration=26476.570475, train/accuracy=0.639239, train/bleu=31.986452, train/loss=1.847251, validation/accuracy=0.663228, validation/bleu=28.618440, validation/loss=1.688449, validation/num_examples=3000
I0207 00:56:53.947122 140051097532160 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.40916964411735535, loss=2.8865723609924316
I0207 00:57:28.439137 140051089139456 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3405293822288513, loss=2.8676064014434814
I0207 00:58:02.985743 140051097532160 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4046655595302582, loss=3.059717893600464
I0207 00:58:37.501268 140051089139456 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3665701448917389, loss=2.9514737129211426
I0207 00:59:12.063169 140051097532160 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.44683924317359924, loss=2.957353115081787
I0207 00:59:46.590093 140051089139456 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3597584664821625, loss=2.926880359649658
I0207 01:00:21.137431 140051097532160 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.3404105305671692, loss=2.8924081325531006
I0207 01:00:55.673570 140051089139456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3712347149848938, loss=2.860224962234497
I0207 01:01:30.209562 140051097532160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3381860554218292, loss=2.9433863162994385
I0207 01:02:04.738420 140051089139456 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.33882105350494385, loss=2.977217435836792
I0207 01:02:39.279473 140051097532160 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.31807175278663635, loss=2.8802454471588135
I0207 01:03:13.818073 140051089139456 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.36414194107055664, loss=2.9813060760498047
I0207 01:03:48.336465 140051097532160 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.39268094301223755, loss=2.9310061931610107
I0207 01:04:22.876250 140051089139456 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.33157482743263245, loss=2.869978666305542
I0207 01:04:57.414326 140051097532160 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.38190627098083496, loss=2.9125330448150635
I0207 01:05:31.915287 140051089139456 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3407631814479828, loss=2.9430601596832275
I0207 01:06:06.460070 140051097532160 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.39146676659584045, loss=2.94423770904541
I0207 01:06:40.981266 140051089139456 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.4387412965297699, loss=2.9267616271972656
I0207 01:07:15.523488 140051097532160 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.36757925152778625, loss=2.952939033508301
I0207 01:07:50.062777 140051089139456 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3156391978263855, loss=2.9862020015716553
I0207 01:08:24.602904 140051097532160 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3551385700702667, loss=2.88136625289917
I0207 01:08:59.156672 140051089139456 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.3485202193260193, loss=2.886474609375
I0207 01:09:33.684403 140051097532160 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.35581421852111816, loss=2.9456934928894043
I0207 01:10:08.235994 140051089139456 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3455536663532257, loss=2.9715189933776855
I0207 01:10:24.181650 140225696298816 spec.py:321] Evaluating on the training split.
I0207 01:10:27.154611 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:13:23.628435 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 01:13:26.321007 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:16:04.105957 140225696298816 spec.py:349] Evaluating on the test split.
I0207 01:16:06.779415 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:18:26.650370 140225696298816 submission_runner.py:408] Time since start: 27799.22s, 	Step: 48648, 	{'train/accuracy': 0.6418137550354004, 'train/loss': 1.8364354372024536, 'train/bleu': 31.435799419526706, 'validation/accuracy': 0.662161648273468, 'validation/loss': 1.6864773035049438, 'validation/bleu': 28.773273751970383, 'validation/num_examples': 3000, 'test/accuracy': 0.6732555031776428, 'test/loss': 1.6297379732131958, 'test/bleu': 28.00563946047731, 'test/num_examples': 3003, 'score': 16845.00081062317, 'total_duration': 27799.22328066826, 'accumulated_submission_time': 16845.00081062317, 'accumulated_eval_time': 10951.921907424927, 'accumulated_logging_time': 0.731576681137085}
I0207 01:18:26.671222 140051097532160 logging_writer.py:48] [48648] accumulated_eval_time=10951.921907, accumulated_logging_time=0.731577, accumulated_submission_time=16845.000811, global_step=48648, preemption_count=0, score=16845.000811, test/accuracy=0.673256, test/bleu=28.005639, test/loss=1.629738, test/num_examples=3003, total_duration=27799.223281, train/accuracy=0.641814, train/bleu=31.435799, train/loss=1.836435, validation/accuracy=0.662162, validation/bleu=28.773274, validation/loss=1.686477, validation/num_examples=3000
I0207 01:18:44.932966 140051089139456 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.31958526372909546, loss=2.9374916553497314
I0207 01:19:19.363458 140051097532160 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3489329218864441, loss=2.891906499862671
I0207 01:19:53.885466 140051089139456 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3314189016819, loss=2.9234657287597656
I0207 01:20:28.444160 140051097532160 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.33821719884872437, loss=2.8678879737854004
I0207 01:21:02.969530 140051089139456 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.31970614194869995, loss=2.9626529216766357
I0207 01:21:37.483296 140051097532160 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3220321536064148, loss=2.8424875736236572
I0207 01:22:11.998035 140051089139456 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3868294656276703, loss=2.9456429481506348
I0207 01:22:46.498886 140051097532160 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3248640298843384, loss=2.9061548709869385
I0207 01:23:21.013656 140051089139456 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3268834054470062, loss=2.9581425189971924
I0207 01:23:55.561617 140051097532160 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.37421557307243347, loss=2.9212169647216797
I0207 01:24:30.104917 140051089139456 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3543721139431, loss=2.935842752456665
I0207 01:25:04.639166 140051097532160 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.3461395800113678, loss=2.9374516010284424
I0207 01:25:39.185630 140051089139456 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.35101231932640076, loss=2.988049030303955
I0207 01:26:13.724925 140051097532160 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.35617056488990784, loss=2.9113991260528564
I0207 01:26:48.233319 140051089139456 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.35395729541778564, loss=2.936772346496582
I0207 01:27:22.767791 140051097532160 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.32213521003723145, loss=2.8779404163360596
I0207 01:27:57.312918 140051089139456 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3945372700691223, loss=2.8875834941864014
I0207 01:28:31.842527 140051097532160 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3299146294593811, loss=2.954636573791504
I0207 01:29:06.367636 140051089139456 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3843618631362915, loss=2.8989269733428955
I0207 01:29:40.894040 140051097532160 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3486791253089905, loss=2.8576154708862305
I0207 01:30:15.418180 140051089139456 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.39129525423049927, loss=2.9656403064727783
I0207 01:30:49.940252 140051097532160 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.40725523233413696, loss=2.899630308151245
I0207 01:31:24.483781 140051089139456 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3458271622657776, loss=2.989910840988159
I0207 01:31:58.999778 140051097532160 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3620828688144684, loss=2.893159866333008
I0207 01:32:26.709726 140225696298816 spec.py:321] Evaluating on the training split.
I0207 01:32:29.699090 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:35:05.266196 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 01:35:07.938112 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:37:38.745027 140225696298816 spec.py:349] Evaluating on the test split.
I0207 01:37:41.423866 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:40:00.543117 140225696298816 submission_runner.py:408] Time since start: 29093.12s, 	Step: 51082, 	{'train/accuracy': 0.6506011486053467, 'train/loss': 1.7793463468551636, 'train/bleu': 32.20130396977178, 'validation/accuracy': 0.6655961871147156, 'validation/loss': 1.6724234819412231, 'validation/bleu': 28.732523018514083, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679845809937, 'test/loss': 1.6110336780548096, 'test/bleu': 28.39498522119112, 'test/num_examples': 3003, 'score': 17684.95172381401, 'total_duration': 29093.11604142189, 'accumulated_submission_time': 17684.95172381401, 'accumulated_eval_time': 11405.755249738693, 'accumulated_logging_time': 0.7633512020111084}
I0207 01:40:00.564949 140051089139456 logging_writer.py:48] [51082] accumulated_eval_time=11405.755250, accumulated_logging_time=0.763351, accumulated_submission_time=17684.951724, global_step=51082, preemption_count=0, score=17684.951724, test/accuracy=0.675568, test/bleu=28.394985, test/loss=1.611034, test/num_examples=3003, total_duration=29093.116041, train/accuracy=0.650601, train/bleu=32.201304, train/loss=1.779346, validation/accuracy=0.665596, validation/bleu=28.732523, validation/loss=1.672423, validation/num_examples=3000
I0207 01:40:07.114932 140051097532160 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.35711342096328735, loss=2.9425485134124756
I0207 01:40:41.531402 140051089139456 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.37564319372177124, loss=2.9818177223205566
I0207 01:41:16.022159 140051097532160 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.3867473006248474, loss=2.9260082244873047
I0207 01:41:50.544610 140051089139456 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3380221724510193, loss=2.9039766788482666
I0207 01:42:25.058683 140051097532160 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.37187421321868896, loss=2.8942062854766846
I0207 01:42:59.626578 140051089139456 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.33173611760139465, loss=2.925759792327881
I0207 01:43:34.181061 140051097532160 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.39009806513786316, loss=2.889042854309082
I0207 01:44:08.714949 140051089139456 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3637957274913788, loss=3.0039215087890625
I0207 01:44:43.242364 140051097532160 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3653171956539154, loss=2.8731067180633545
I0207 01:45:17.775805 140051089139456 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3342207372188568, loss=2.842254638671875
I0207 01:45:52.309415 140051097532160 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3499120771884918, loss=2.9331138134002686
I0207 01:46:26.852963 140051089139456 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3450978100299835, loss=2.940239191055298
I0207 01:47:01.374735 140051097532160 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.31960830092430115, loss=2.892317056655884
I0207 01:47:35.934582 140051089139456 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.35234710574150085, loss=2.93757963180542
I0207 01:48:10.481849 140051097532160 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3282335698604584, loss=2.897642135620117
I0207 01:48:45.046941 140051089139456 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3645752966403961, loss=2.8806114196777344
I0207 01:49:19.591553 140051097532160 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3481510281562805, loss=2.9087700843811035
I0207 01:49:54.120121 140051089139456 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.34510403871536255, loss=2.961332082748413
I0207 01:50:28.662803 140051097532160 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3715877830982208, loss=2.9603302478790283
I0207 01:51:03.219195 140051089139456 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.37978678941726685, loss=2.927703380584717
I0207 01:51:37.762490 140051097532160 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.34631502628326416, loss=2.910743474960327
I0207 01:52:12.263664 140051089139456 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3537856340408325, loss=2.8955981731414795
I0207 01:52:46.802435 140051097532160 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3543928861618042, loss=2.9050886631011963
I0207 01:53:21.343784 140051089139456 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3314642608165741, loss=2.8969764709472656
I0207 01:53:55.850617 140051097532160 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.37312495708465576, loss=2.937175989151001
I0207 01:54:00.743534 140225696298816 spec.py:321] Evaluating on the training split.
I0207 01:54:03.709687 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:56:56.352739 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 01:56:59.020137 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 01:59:28.457360 140225696298816 spec.py:349] Evaluating on the test split.
I0207 01:59:31.132424 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:01:52.122757 140225696298816 submission_runner.py:408] Time since start: 30404.70s, 	Step: 53516, 	{'train/accuracy': 0.6455166935920715, 'train/loss': 1.8138216733932495, 'train/bleu': 31.56522640495758, 'validation/accuracy': 0.6665757298469543, 'validation/loss': 1.6703606843948364, 'validation/bleu': 28.52078753000625, 'validation/num_examples': 3000, 'test/accuracy': 0.6770902276039124, 'test/loss': 1.6028481721878052, 'test/bleu': 28.199818862678665, 'test/num_examples': 3003, 'score': 18525.0443277359, 'total_duration': 30404.6956615448, 'accumulated_submission_time': 18525.0443277359, 'accumulated_eval_time': 11877.134405851364, 'accumulated_logging_time': 0.7947971820831299}
I0207 02:01:52.146043 140051089139456 logging_writer.py:48] [53516] accumulated_eval_time=11877.134406, accumulated_logging_time=0.794797, accumulated_submission_time=18525.044328, global_step=53516, preemption_count=0, score=18525.044328, test/accuracy=0.677090, test/bleu=28.199819, test/loss=1.602848, test/num_examples=3003, total_duration=30404.695662, train/accuracy=0.645517, train/bleu=31.565226, train/loss=1.813822, validation/accuracy=0.666576, validation/bleu=28.520788, validation/loss=1.670361, validation/num_examples=3000
I0207 02:02:21.375104 140051097532160 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.3596523404121399, loss=2.9664506912231445
I0207 02:02:55.839997 140051089139456 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3810146749019623, loss=2.9774653911590576
I0207 02:03:30.332837 140051097532160 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.33687520027160645, loss=2.9108967781066895
I0207 02:04:04.842840 140051089139456 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.362805038690567, loss=2.874253511428833
I0207 02:04:39.367316 140051097532160 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.36239302158355713, loss=2.912121534347534
I0207 02:05:13.899425 140051089139456 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.34829509258270264, loss=2.8424320220947266
I0207 02:05:48.454752 140051097532160 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.3810165822505951, loss=2.9092094898223877
I0207 02:06:22.960931 140051089139456 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.35152873396873474, loss=2.895709991455078
I0207 02:06:57.486535 140051097532160 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.36374324560165405, loss=2.8815460205078125
I0207 02:07:32.011344 140051089139456 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3893653154373169, loss=2.8750369548797607
I0207 02:08:06.539859 140051097532160 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.35521557927131653, loss=2.944927453994751
I0207 02:08:41.057716 140051089139456 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3305179178714752, loss=2.909532070159912
I0207 02:09:15.599761 140051097532160 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3984377980232239, loss=2.9068102836608887
I0207 02:09:50.137320 140051089139456 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3562217056751251, loss=2.959911584854126
I0207 02:10:24.665551 140051097532160 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3283448815345764, loss=2.893800735473633
I0207 02:10:59.199295 140051089139456 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.35749194025993347, loss=2.97481632232666
I0207 02:11:33.748494 140051097532160 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.38757967948913574, loss=2.9170289039611816
I0207 02:12:08.291292 140051089139456 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3465655446052551, loss=2.9142704010009766
I0207 02:12:42.839540 140051097532160 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.32724785804748535, loss=2.8810617923736572
I0207 02:13:17.370978 140051089139456 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.42205485701560974, loss=2.8901491165161133
I0207 02:13:51.888103 140051097532160 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3556492328643799, loss=2.9238059520721436
I0207 02:14:26.439292 140051089139456 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3428329527378082, loss=2.8932714462280273
I0207 02:15:00.962544 140051097532160 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3993156850337982, loss=2.9611380100250244
I0207 02:15:35.462728 140051089139456 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.3603745996952057, loss=2.9159176349639893
I0207 02:15:52.445759 140225696298816 spec.py:321] Evaluating on the training split.
I0207 02:15:55.409427 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:19:29.399516 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 02:19:32.067954 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:22:57.946856 140225696298816 spec.py:349] Evaluating on the test split.
I0207 02:23:00.609770 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:26:15.886792 140225696298816 submission_runner.py:408] Time since start: 31868.46s, 	Step: 55951, 	{'train/accuracy': 0.6445412635803223, 'train/loss': 1.8259145021438599, 'train/bleu': 32.05229143406017, 'validation/accuracy': 0.6659185886383057, 'validation/loss': 1.664124608039856, 'validation/bleu': 28.706732866328736, 'validation/num_examples': 3000, 'test/accuracy': 0.680553138256073, 'test/loss': 1.5927239656448364, 'test/bleu': 28.54534410724413, 'test/num_examples': 3003, 'score': 19365.257098674774, 'total_duration': 31868.459720373154, 'accumulated_submission_time': 19365.257098674774, 'accumulated_eval_time': 12500.575400590897, 'accumulated_logging_time': 0.8282985687255859}
I0207 02:26:15.908791 140051097532160 logging_writer.py:48] [55951] accumulated_eval_time=12500.575401, accumulated_logging_time=0.828299, accumulated_submission_time=19365.257099, global_step=55951, preemption_count=0, score=19365.257099, test/accuracy=0.680553, test/bleu=28.545344, test/loss=1.592724, test/num_examples=3003, total_duration=31868.459720, train/accuracy=0.644541, train/bleu=32.052291, train/loss=1.825915, validation/accuracy=0.665919, validation/bleu=28.706733, validation/loss=1.664125, validation/num_examples=3000
I0207 02:26:33.116677 140051089139456 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.35942530632019043, loss=2.938490629196167
I0207 02:27:07.546366 140051097532160 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3472634553909302, loss=2.8024709224700928
I0207 02:27:42.064073 140051089139456 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.4334281384944916, loss=2.876988649368286
I0207 02:28:16.576793 140051097532160 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.325442373752594, loss=2.956745147705078
I0207 02:28:51.092981 140051089139456 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3474827706813812, loss=2.892561435699463
I0207 02:29:25.627403 140051097532160 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.37910449504852295, loss=2.9199023246765137
I0207 02:30:00.136203 140051089139456 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.354146808385849, loss=2.8381152153015137
I0207 02:30:34.654076 140051097532160 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.43788057565689087, loss=2.949890613555908
I0207 02:31:09.205516 140051089139456 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.35596248507499695, loss=2.8965208530426025
I0207 02:31:43.722233 140051097532160 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.34334883093833923, loss=2.9032909870147705
I0207 02:32:18.275591 140051089139456 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.36080223321914673, loss=2.8832974433898926
I0207 02:32:52.831339 140051097532160 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.36760738492012024, loss=2.9804792404174805
I0207 02:33:27.352896 140051089139456 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3715732991695404, loss=2.8448293209075928
I0207 02:34:01.911439 140051097532160 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.31864693760871887, loss=2.9127261638641357
I0207 02:34:36.450159 140051089139456 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3465732932090759, loss=2.8989250659942627
I0207 02:35:10.948566 140051097532160 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.37213054299354553, loss=2.9413084983825684
I0207 02:35:45.495519 140051089139456 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.37894535064697266, loss=2.907320261001587
I0207 02:36:20.001410 140051097532160 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3525428771972656, loss=2.860124111175537
I0207 02:36:54.523631 140051089139456 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.43232306838035583, loss=2.8646059036254883
I0207 02:37:29.050786 140051097532160 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3574524521827698, loss=2.885143756866455
I0207 02:38:03.595325 140051089139456 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.39233872294425964, loss=2.8472466468811035
I0207 02:38:38.104205 140051097532160 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3305096924304962, loss=2.910618543624878
I0207 02:39:12.624897 140051089139456 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3600641191005707, loss=2.8704757690429688
I0207 02:39:47.165430 140051097532160 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3537270426750183, loss=2.880021810531616
I0207 02:40:15.888135 140225696298816 spec.py:321] Evaluating on the training split.
I0207 02:40:18.860477 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:44:18.804046 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 02:44:21.481162 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:47:00.380459 140225696298816 spec.py:349] Evaluating on the test split.
I0207 02:47:03.069391 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 02:49:23.116855 140225696298816 submission_runner.py:408] Time since start: 33255.69s, 	Step: 58385, 	{'train/accuracy': 0.6512637138366699, 'train/loss': 1.7649765014648438, 'train/bleu': 31.62150612676097, 'validation/accuracy': 0.6669105291366577, 'validation/loss': 1.6533396244049072, 'validation/bleu': 28.550633559064593, 'validation/num_examples': 3000, 'test/accuracy': 0.6782174110412598, 'test/loss': 1.5912344455718994, 'test/bleu': 28.35522166002915, 'test/num_examples': 3003, 'score': 20205.149728775024, 'total_duration': 33255.68974637985, 'accumulated_submission_time': 20205.149728775024, 'accumulated_eval_time': 13047.804039001465, 'accumulated_logging_time': 0.8610539436340332}
I0207 02:49:23.139832 140051089139456 logging_writer.py:48] [58385] accumulated_eval_time=13047.804039, accumulated_logging_time=0.861054, accumulated_submission_time=20205.149729, global_step=58385, preemption_count=0, score=20205.149729, test/accuracy=0.678217, test/bleu=28.355222, test/loss=1.591234, test/num_examples=3003, total_duration=33255.689746, train/accuracy=0.651264, train/bleu=31.621506, train/loss=1.764977, validation/accuracy=0.666911, validation/bleu=28.550634, validation/loss=1.653340, validation/num_examples=3000
I0207 02:49:28.661032 140051097532160 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.34508395195007324, loss=2.881211519241333
I0207 02:50:03.060809 140051089139456 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.34845101833343506, loss=2.9314234256744385
I0207 02:50:37.528245 140051097532160 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.34223830699920654, loss=2.9099972248077393
I0207 02:51:12.020362 140051089139456 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.36091306805610657, loss=2.9286556243896484
I0207 02:51:46.543514 140051097532160 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.335333913564682, loss=2.9365479946136475
I0207 02:52:21.070981 140051089139456 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.4099878966808319, loss=2.8870699405670166
I0207 02:52:55.618167 140051097532160 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3354700803756714, loss=2.8412163257598877
I0207 02:53:30.148201 140051089139456 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.33693960309028625, loss=2.917367935180664
I0207 02:54:04.671118 140051097532160 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.40797099471092224, loss=2.901876449584961
I0207 02:54:39.206070 140051089139456 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.36918774247169495, loss=2.995673179626465
I0207 02:55:13.741971 140051097532160 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.35418787598609924, loss=2.895565986633301
I0207 02:55:48.238324 140051089139456 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.31091633439064026, loss=2.8886873722076416
I0207 02:56:22.750523 140051097532160 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3413446545600891, loss=2.8379664421081543
I0207 02:56:57.287118 140051089139456 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3732183277606964, loss=2.917819023132324
I0207 02:57:31.793521 140051097532160 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.3599404990673065, loss=2.8745310306549072
I0207 02:58:06.339097 140051089139456 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.34272515773773193, loss=2.8542933464050293
I0207 02:58:40.850124 140051097532160 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.34316152334213257, loss=2.8796257972717285
I0207 02:59:15.391679 140051089139456 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.4234335720539093, loss=2.9203083515167236
I0207 02:59:49.900357 140051097532160 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3562253713607788, loss=2.87974214553833
I0207 03:00:24.402927 140051089139456 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.344558984041214, loss=2.924758195877075
I0207 03:00:58.907901 140051097532160 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.36956706643104553, loss=2.937772035598755
I0207 03:01:33.443212 140051089139456 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.364731103181839, loss=2.9177064895629883
I0207 03:02:07.954736 140051097532160 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.31591469049453735, loss=2.8984193801879883
I0207 03:02:42.483479 140051089139456 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3589741289615631, loss=2.846604824066162
I0207 03:03:17.030952 140051097532160 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.41011229157447815, loss=2.9911811351776123
I0207 03:03:23.298201 140225696298816 spec.py:321] Evaluating on the training split.
I0207 03:03:26.263361 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:06:10.158446 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 03:06:12.832078 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:08:43.839734 140225696298816 spec.py:349] Evaluating on the test split.
I0207 03:08:46.509673 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:11:10.350474 140225696298816 submission_runner.py:408] Time since start: 34562.92s, 	Step: 60820, 	{'train/accuracy': 0.6503833532333374, 'train/loss': 1.7853611707687378, 'train/bleu': 31.359232455254705, 'validation/accuracy': 0.6674684882164001, 'validation/loss': 1.655722975730896, 'validation/bleu': 28.792026358666575, 'validation/num_examples': 3000, 'test/accuracy': 0.6813201308250427, 'test/loss': 1.5846421718597412, 'test/bleu': 28.693769513099046, 'test/num_examples': 3003, 'score': 21045.221281528473, 'total_duration': 34562.92338228226, 'accumulated_submission_time': 21045.221281528473, 'accumulated_eval_time': 13514.856243610382, 'accumulated_logging_time': 0.8937373161315918}
I0207 03:11:10.373930 140051089139456 logging_writer.py:48] [60820] accumulated_eval_time=13514.856244, accumulated_logging_time=0.893737, accumulated_submission_time=21045.221282, global_step=60820, preemption_count=0, score=21045.221282, test/accuracy=0.681320, test/bleu=28.693770, test/loss=1.584642, test/num_examples=3003, total_duration=34562.923382, train/accuracy=0.650383, train/bleu=31.359232, train/loss=1.785361, validation/accuracy=0.667468, validation/bleu=28.792026, validation/loss=1.655723, validation/num_examples=3000
I0207 03:11:38.235323 140051097532160 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.33910390734672546, loss=2.9212827682495117
I0207 03:12:12.698850 140051089139456 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.35266777873039246, loss=2.905709743499756
I0207 03:12:47.205415 140051097532160 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.34508565068244934, loss=2.880580186843872
I0207 03:13:21.724904 140051089139456 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.35057878494262695, loss=2.886646032333374
I0207 03:13:56.226627 140051097532160 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.3472428619861603, loss=2.9094204902648926
I0207 03:14:30.726070 140051089139456 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.34386470913887024, loss=2.843308448791504
I0207 03:15:05.264736 140051097532160 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3581337034702301, loss=2.840122938156128
I0207 03:15:39.763087 140051089139456 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.35905706882476807, loss=2.897512912750244
I0207 03:16:14.256310 140051097532160 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3953903615474701, loss=2.8682501316070557
I0207 03:16:48.770323 140051089139456 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3357459604740143, loss=2.9241385459899902
I0207 03:17:23.289003 140051097532160 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.34948256611824036, loss=2.925870656967163
I0207 03:17:57.808509 140051089139456 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.396258682012558, loss=2.931777000427246
I0207 03:18:32.320908 140051097532160 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3343382477760315, loss=2.8506035804748535
I0207 03:19:06.850538 140051089139456 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.34610193967819214, loss=2.9195587635040283
I0207 03:19:41.356838 140051097532160 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.34922996163368225, loss=2.915226936340332
I0207 03:20:15.856395 140051089139456 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3475111424922943, loss=2.8673319816589355
I0207 03:20:50.372482 140051097532160 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3361181914806366, loss=2.8703150749206543
I0207 03:21:24.869738 140051089139456 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.34592384099960327, loss=2.839834213256836
I0207 03:21:59.368678 140051097532160 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3584788739681244, loss=2.884014844894409
I0207 03:22:33.925253 140051089139456 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.35264575481414795, loss=2.9683303833007812
I0207 03:23:08.454505 140051097532160 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.34836962819099426, loss=2.907050609588623
I0207 03:23:42.996446 140051089139456 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3643679916858673, loss=2.919109344482422
I0207 03:24:17.550285 140051097532160 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3631746768951416, loss=2.8709475994110107
I0207 03:24:52.060188 140051089139456 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.35594281554222107, loss=2.837963581085205
I0207 03:25:10.435908 140225696298816 spec.py:321] Evaluating on the training split.
I0207 03:25:13.403161 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:27:56.225121 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 03:27:58.903209 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:30:28.002009 140225696298816 spec.py:349] Evaluating on the test split.
I0207 03:30:30.670381 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:32:56.140429 140225696298816 submission_runner.py:408] Time since start: 35868.71s, 	Step: 63255, 	{'train/accuracy': 0.6639273166656494, 'train/loss': 1.6947698593139648, 'train/bleu': 32.86530003174533, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.639392375946045, 'validation/bleu': 29.130301380967907, 'validation/num_examples': 3000, 'test/accuracy': 0.6831212639808655, 'test/loss': 1.5691425800323486, 'test/bleu': 28.78299893054562, 'test/num_examples': 3003, 'score': 21885.19492340088, 'total_duration': 35868.71333575249, 'accumulated_submission_time': 21885.19492340088, 'accumulated_eval_time': 13980.560697555542, 'accumulated_logging_time': 0.9289267063140869}
I0207 03:32:56.164851 140051097532160 logging_writer.py:48] [63255] accumulated_eval_time=13980.560698, accumulated_logging_time=0.928927, accumulated_submission_time=21885.194923, global_step=63255, preemption_count=0, score=21885.194923, test/accuracy=0.683121, test/bleu=28.782999, test/loss=1.569143, test/num_examples=3003, total_duration=35868.713336, train/accuracy=0.663927, train/bleu=32.865300, train/loss=1.694770, validation/accuracy=0.668783, validation/bleu=29.130301, validation/loss=1.639392, validation/num_examples=3000
I0207 03:33:12.003293 140051089139456 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.37948328256607056, loss=2.8475847244262695
I0207 03:33:46.415189 140051097532160 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3864080011844635, loss=2.918579339981079
I0207 03:34:20.911322 140051089139456 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3516678810119629, loss=2.9125282764434814
I0207 03:34:55.441951 140051097532160 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3884291350841522, loss=2.907595157623291
I0207 03:35:29.970899 140051089139456 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.34611976146698, loss=2.8406026363372803
I0207 03:36:04.466792 140051097532160 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3624282479286194, loss=2.8914976119995117
I0207 03:36:38.934194 140051089139456 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.34575486183166504, loss=2.8860342502593994
I0207 03:37:13.456326 140051097532160 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3516233265399933, loss=2.9070072174072266
I0207 03:37:47.993090 140051089139456 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.37791579961776733, loss=2.8832859992980957
I0207 03:38:22.512255 140051097532160 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.33713653683662415, loss=2.912689685821533
I0207 03:38:57.031918 140051089139456 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.35162532329559326, loss=2.906813383102417
I0207 03:39:31.555415 140051097532160 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.37220579385757446, loss=2.9164388179779053
I0207 03:40:06.053224 140051089139456 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3364666700363159, loss=2.931098461151123
I0207 03:40:40.594812 140051097532160 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3886554539203644, loss=2.909329891204834
I0207 03:41:15.152128 140051089139456 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.4091065526008606, loss=2.91318678855896
I0207 03:41:49.702614 140051097532160 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3371395766735077, loss=2.8801441192626953
I0207 03:42:24.236926 140051089139456 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3926706910133362, loss=2.8674893379211426
I0207 03:42:58.756352 140051097532160 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.35258689522743225, loss=2.8730058670043945
I0207 03:43:33.273845 140051089139456 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.32415398955345154, loss=2.8562517166137695
I0207 03:44:07.791574 140051097532160 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3642933964729309, loss=2.901477813720703
I0207 03:44:42.349426 140051089139456 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.32823124527931213, loss=2.893238067626953
I0207 03:45:16.879013 140051097532160 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3539346158504486, loss=2.921225070953369
I0207 03:45:51.394452 140051089139456 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3515951335430145, loss=2.898338556289673
I0207 03:46:25.915448 140051097532160 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3562925159931183, loss=2.893833637237549
I0207 03:46:56.345568 140225696298816 spec.py:321] Evaluating on the training split.
I0207 03:46:59.303540 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:50:11.696314 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 03:50:14.373799 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:52:46.436169 140225696298816 spec.py:349] Evaluating on the test split.
I0207 03:52:49.112128 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 03:55:23.833136 140225696298816 submission_runner.py:408] Time since start: 37216.41s, 	Step: 65690, 	{'train/accuracy': 0.6493473649024963, 'train/loss': 1.7821170091629028, 'train/bleu': 32.37563384288451, 'validation/accuracy': 0.6715725660324097, 'validation/loss': 1.6345800161361694, 'validation/bleu': 29.14645826795015, 'validation/num_examples': 3000, 'test/accuracy': 0.6841439008712769, 'test/loss': 1.5664318799972534, 'test/bleu': 28.850066271740804, 'test/num_examples': 3003, 'score': 22725.288113832474, 'total_duration': 37216.40605187416, 'accumulated_submission_time': 22725.288113832474, 'accumulated_eval_time': 14488.048207998276, 'accumulated_logging_time': 0.964684009552002}
I0207 03:55:23.857300 140051089139456 logging_writer.py:48] [65690] accumulated_eval_time=14488.048208, accumulated_logging_time=0.964684, accumulated_submission_time=22725.288114, global_step=65690, preemption_count=0, score=22725.288114, test/accuracy=0.684144, test/bleu=28.850066, test/loss=1.566432, test/num_examples=3003, total_duration=37216.406052, train/accuracy=0.649347, train/bleu=32.375634, train/loss=1.782117, validation/accuracy=0.671573, validation/bleu=29.146458, validation/loss=1.634580, validation/num_examples=3000
I0207 03:55:27.658466 140051097532160 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3331138789653778, loss=2.8285534381866455
I0207 03:56:02.048636 140051089139456 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.38522499799728394, loss=2.9178380966186523
I0207 03:56:36.496641 140051097532160 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.36719802021980286, loss=2.8723912239074707
I0207 03:57:11.007231 140051089139456 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.38324907422065735, loss=2.89021372795105
I0207 03:57:45.497653 140051097532160 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3885733187198639, loss=2.839616060256958
I0207 03:58:20.000055 140051089139456 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3538888096809387, loss=2.8431906700134277
I0207 03:58:54.552315 140051097532160 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.339606374502182, loss=2.816441297531128
I0207 03:59:29.040407 140051089139456 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3496555685997009, loss=2.798232078552246
I0207 04:00:03.557805 140051097532160 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.39730823040008545, loss=2.8718912601470947
I0207 04:00:38.098192 140051089139456 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.36104637384414673, loss=2.948206901550293
I0207 04:01:12.631313 140051097532160 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.39416155219078064, loss=2.838777780532837
I0207 04:01:47.128939 140051089139456 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.37469640374183655, loss=2.8801817893981934
I0207 04:02:21.652246 140051097532160 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.33694344758987427, loss=2.8340346813201904
I0207 04:02:56.196229 140051089139456 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.34995201230049133, loss=2.9266717433929443
I0207 04:03:30.744807 140051097532160 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.33730098605155945, loss=2.8959219455718994
I0207 04:04:05.255008 140051089139456 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.3863748013973236, loss=2.8510096073150635
I0207 04:04:39.773241 140051097532160 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.36235174536705017, loss=2.8071835041046143
I0207 04:05:14.273855 140051089139456 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.35935837030410767, loss=2.9050354957580566
I0207 04:05:48.797424 140051097532160 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.33600541949272156, loss=2.8000965118408203
I0207 04:06:23.335229 140051089139456 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3835681676864624, loss=2.7660789489746094
I0207 04:06:57.862538 140051097532160 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3582777976989746, loss=2.859225273132324
I0207 04:07:32.378290 140051089139456 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3473350405693054, loss=2.8801512718200684
I0207 04:08:06.939723 140051097532160 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.4164901673793793, loss=2.917198896408081
I0207 04:08:41.464446 140051089139456 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.36984768509864807, loss=2.8199212551116943
I0207 04:09:15.964284 140051097532160 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.385824590921402, loss=2.8019425868988037
I0207 04:09:23.975075 140225696298816 spec.py:321] Evaluating on the training split.
I0207 04:09:26.945044 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:12:27.587294 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 04:12:30.261797 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:15:09.339523 140225696298816 spec.py:349] Evaluating on the test split.
I0207 04:15:12.017718 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:17:32.623047 140225696298816 submission_runner.py:408] Time since start: 38545.20s, 	Step: 68125, 	{'train/accuracy': 0.6522968411445618, 'train/loss': 1.7712243795394897, 'train/bleu': 32.26716561149852, 'validation/accuracy': 0.6721181273460388, 'validation/loss': 1.6278233528137207, 'validation/bleu': 29.2856531151115, 'validation/num_examples': 3000, 'test/accuracy': 0.6857939958572388, 'test/loss': 1.5533937215805054, 'test/bleu': 29.1256183217246, 'test/num_examples': 3003, 'score': 23565.320879220963, 'total_duration': 38545.195959568024, 'accumulated_submission_time': 23565.320879220963, 'accumulated_eval_time': 14976.696114301682, 'accumulated_logging_time': 0.9985537528991699}
I0207 04:17:32.646699 140051089139456 logging_writer.py:48] [68125] accumulated_eval_time=14976.696114, accumulated_logging_time=0.998554, accumulated_submission_time=23565.320879, global_step=68125, preemption_count=0, score=23565.320879, test/accuracy=0.685794, test/bleu=29.125618, test/loss=1.553394, test/num_examples=3003, total_duration=38545.195960, train/accuracy=0.652297, train/bleu=32.267166, train/loss=1.771224, validation/accuracy=0.672118, validation/bleu=29.285653, validation/loss=1.627823, validation/num_examples=3000
I0207 04:17:58.856873 140051097532160 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.38510167598724365, loss=2.9256176948547363
I0207 04:18:33.298794 140051089139456 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3252929449081421, loss=2.9192192554473877
I0207 04:19:07.797377 140051097532160 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.3477039635181427, loss=2.889787435531616
I0207 04:19:42.279274 140051089139456 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3535591661930084, loss=2.8707377910614014
I0207 04:20:16.807184 140051097532160 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3633834719657898, loss=2.91009259223938
I0207 04:20:51.311071 140051089139456 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.36677950620651245, loss=2.869410276412964
I0207 04:21:25.802242 140051097532160 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3785395920276642, loss=2.936896562576294
I0207 04:22:00.332265 140051089139456 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.34426453709602356, loss=2.8558201789855957
I0207 04:22:34.881813 140051097532160 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.36127281188964844, loss=2.874793529510498
I0207 04:23:09.410788 140051089139456 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3598729074001312, loss=2.90118408203125
I0207 04:23:43.918490 140051097532160 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.36036762595176697, loss=2.894150495529175
I0207 04:24:18.420646 140051089139456 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.36729341745376587, loss=2.806767702102661
I0207 04:24:52.972447 140051097532160 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3305444121360779, loss=2.8298163414001465
I0207 04:25:27.526985 140051089139456 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.35133880376815796, loss=2.827042818069458
I0207 04:26:02.033764 140051097532160 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.35219863057136536, loss=2.854748487472534
I0207 04:26:36.598718 140051089139456 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.34536147117614746, loss=2.87749981880188
I0207 04:27:11.132965 140051097532160 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3608006238937378, loss=2.7823662757873535
I0207 04:27:45.671791 140051089139456 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.379255473613739, loss=2.8450372219085693
I0207 04:28:20.218045 140051097532160 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3516566753387451, loss=2.808004379272461
I0207 04:28:54.738449 140051089139456 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3541242480278015, loss=2.846052408218384
I0207 04:29:29.265533 140051097532160 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.338697224855423, loss=2.80745267868042
I0207 04:30:03.775299 140051089139456 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3628773093223572, loss=2.7902262210845947
I0207 04:30:38.293958 140051097532160 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.4092732071876526, loss=2.865736484527588
I0207 04:31:12.803177 140051089139456 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.34018033742904663, loss=2.853688955307007
I0207 04:31:32.911161 140225696298816 spec.py:321] Evaluating on the training split.
I0207 04:31:35.880220 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:34:22.415293 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 04:34:25.093262 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:36:50.210122 140225696298816 spec.py:349] Evaluating on the test split.
I0207 04:36:52.879272 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:39:18.320272 140225696298816 submission_runner.py:408] Time since start: 39850.89s, 	Step: 70560, 	{'train/accuracy': 0.6611246466636658, 'train/loss': 1.709555745124817, 'train/bleu': 32.33671503844988, 'validation/accuracy': 0.6735812425613403, 'validation/loss': 1.61667001247406, 'validation/bleu': 29.112008037255638, 'validation/num_examples': 3000, 'test/accuracy': 0.6852827072143555, 'test/loss': 1.5494496822357178, 'test/bleu': 28.897881717129575, 'test/num_examples': 3003, 'score': 24405.445092201233, 'total_duration': 39850.8931684494, 'accumulated_submission_time': 24405.445092201233, 'accumulated_eval_time': 15442.105145931244, 'accumulated_logging_time': 1.085925817489624}
I0207 04:39:18.344855 140051097532160 logging_writer.py:48] [70560] accumulated_eval_time=15442.105146, accumulated_logging_time=1.085926, accumulated_submission_time=24405.445092, global_step=70560, preemption_count=0, score=24405.445092, test/accuracy=0.685283, test/bleu=28.897882, test/loss=1.549450, test/num_examples=3003, total_duration=39850.893168, train/accuracy=0.661125, train/bleu=32.336715, train/loss=1.709556, validation/accuracy=0.673581, validation/bleu=29.112008, validation/loss=1.616670, validation/num_examples=3000
I0207 04:39:32.481748 140051089139456 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3574713170528412, loss=2.89591908454895
I0207 04:40:06.894622 140051097532160 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.33832621574401855, loss=2.81121826171875
I0207 04:40:41.378059 140051089139456 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.35063785314559937, loss=2.9101006984710693
I0207 04:41:15.892719 140051097532160 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.35050487518310547, loss=2.8459882736206055
I0207 04:41:50.412719 140051089139456 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.38060981035232544, loss=2.9265902042388916
I0207 04:42:24.943457 140051097532160 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.370209664106369, loss=2.884432077407837
I0207 04:42:59.471290 140051089139456 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.34318479895591736, loss=2.853531837463379
I0207 04:43:34.001265 140051097532160 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.34451669454574585, loss=2.8897016048431396
I0207 04:44:08.523351 140051089139456 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.40754878520965576, loss=2.8776586055755615
I0207 04:44:43.026150 140051097532160 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.4051525592803955, loss=2.901090145111084
I0207 04:45:17.581364 140051089139456 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.36054468154907227, loss=2.8252642154693604
I0207 04:45:52.130907 140051097532160 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.36287960410118103, loss=2.8978750705718994
I0207 04:46:26.657067 140051089139456 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.36157211661338806, loss=2.8836162090301514
I0207 04:47:01.211592 140051097532160 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.35870781540870667, loss=2.829859495162964
I0207 04:47:35.730458 140051089139456 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3536941111087799, loss=2.867208242416382
I0207 04:48:10.272454 140051097532160 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.384531170129776, loss=2.8060672283172607
I0207 04:48:44.778579 140051089139456 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3738023638725281, loss=2.891096353530884
I0207 04:49:19.296111 140051097532160 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3400856852531433, loss=2.8304290771484375
I0207 04:49:53.791677 140051089139456 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3568456470966339, loss=2.83042573928833
I0207 04:50:28.318479 140051097532160 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3500743806362152, loss=2.8157591819763184
I0207 04:51:02.830802 140051089139456 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.34230875968933105, loss=2.803473711013794
I0207 04:51:37.347883 140051097532160 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.349532812833786, loss=2.764270305633545
I0207 04:52:11.883898 140051089139456 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.3591764569282532, loss=2.8630433082580566
I0207 04:52:46.438366 140051097532160 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3774997889995575, loss=2.766455888748169
I0207 04:53:18.619626 140225696298816 spec.py:321] Evaluating on the training split.
I0207 04:53:21.587383 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:56:00.639017 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 04:56:03.316473 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 04:58:39.769424 140225696298816 spec.py:349] Evaluating on the test split.
I0207 04:58:42.452880 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:01:04.808650 140225696298816 submission_runner.py:408] Time since start: 41157.38s, 	Step: 72995, 	{'train/accuracy': 0.6553438901901245, 'train/loss': 1.7596919536590576, 'train/bleu': 32.79811780395135, 'validation/accuracy': 0.6746723651885986, 'validation/loss': 1.609011173248291, 'validation/bleu': 29.66018880236669, 'validation/num_examples': 3000, 'test/accuracy': 0.6878392100334167, 'test/loss': 1.5402271747589111, 'test/bleu': 29.309156302699094, 'test/num_examples': 3003, 'score': 25245.630289554596, 'total_duration': 41157.38154554367, 'accumulated_submission_time': 25245.630289554596, 'accumulated_eval_time': 15908.294090032578, 'accumulated_logging_time': 1.1230123043060303}
I0207 05:01:04.833079 140051089139456 logging_writer.py:48] [72995] accumulated_eval_time=15908.294090, accumulated_logging_time=1.123012, accumulated_submission_time=25245.630290, global_step=72995, preemption_count=0, score=25245.630290, test/accuracy=0.687839, test/bleu=29.309156, test/loss=1.540227, test/num_examples=3003, total_duration=41157.381546, train/accuracy=0.655344, train/bleu=32.798118, train/loss=1.759692, validation/accuracy=0.674672, validation/bleu=29.660189, validation/loss=1.609011, validation/num_examples=3000
I0207 05:01:06.929532 140051097532160 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.36928591132164, loss=2.8903863430023193
I0207 05:01:41.310607 140051089139456 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.39462825655937195, loss=2.7918171882629395
I0207 05:02:15.767813 140051097532160 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3886863887310028, loss=2.833122491836548
I0207 05:02:50.318522 140051089139456 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.3468163311481476, loss=2.8074748516082764
I0207 05:03:24.856092 140051097532160 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.37998732924461365, loss=2.842949151992798
I0207 05:03:59.387410 140051089139456 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3581696152687073, loss=2.7981386184692383
I0207 05:04:33.941137 140051097532160 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.39866945147514343, loss=2.8369948863983154
I0207 05:05:08.451022 140051089139456 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3771113455295563, loss=2.860912799835205
I0207 05:05:42.988221 140051097532160 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.37281838059425354, loss=2.8584775924682617
I0207 05:06:17.529820 140051089139456 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.4295125901699066, loss=2.8067684173583984
I0207 05:06:52.080900 140051097532160 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.46367302536964417, loss=2.844437599182129
I0207 05:07:26.626805 140051089139456 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3683985471725464, loss=2.8050060272216797
I0207 05:08:01.178123 140051097532160 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.4011348485946655, loss=2.878814458847046
I0207 05:08:35.705193 140051089139456 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.37106022238731384, loss=2.8024234771728516
I0207 05:09:10.238570 140051097532160 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3814242482185364, loss=2.8488707542419434
I0207 05:09:44.768536 140051089139456 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.38013389706611633, loss=2.891775131225586
I0207 05:10:19.293946 140051097532160 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3869456946849823, loss=2.8151462078094482
I0207 05:10:53.811227 140051089139456 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3545919954776764, loss=2.8077616691589355
I0207 05:11:28.320223 140051097532160 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.38178372383117676, loss=2.805478811264038
I0207 05:12:02.821704 140051089139456 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.3656420111656189, loss=2.82781982421875
I0207 05:12:37.355385 140051097532160 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.35319048166275024, loss=2.8465545177459717
I0207 05:13:11.859471 140051089139456 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3903782367706299, loss=2.8462791442871094
I0207 05:13:46.381017 140051097532160 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3559419512748718, loss=2.8863325119018555
I0207 05:14:20.915932 140051089139456 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.39911752939224243, loss=2.752143621444702
I0207 05:14:55.481876 140051097532160 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.34561577439308167, loss=2.7879953384399414
I0207 05:15:04.871858 140225696298816 spec.py:321] Evaluating on the training split.
I0207 05:15:07.848275 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:18:28.679754 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 05:18:31.350995 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:21:08.233521 140225696298816 spec.py:349] Evaluating on the test split.
I0207 05:21:10.910916 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:23:55.152435 140225696298816 submission_runner.py:408] Time since start: 42527.73s, 	Step: 75429, 	{'train/accuracy': 0.6763533353805542, 'train/loss': 1.624076247215271, 'train/bleu': 33.937369926587024, 'validation/accuracy': 0.6763462424278259, 'validation/loss': 1.6011841297149658, 'validation/bleu': 29.693910913014506, 'validation/num_examples': 3000, 'test/accuracy': 0.689012885093689, 'test/loss': 1.5321506261825562, 'test/bleu': 29.094085815300794, 'test/num_examples': 3003, 'score': 26085.57997250557, 'total_duration': 42527.72531962395, 'accumulated_submission_time': 26085.57997250557, 'accumulated_eval_time': 16438.57457280159, 'accumulated_logging_time': 1.1592369079589844}
I0207 05:23:55.177386 140051089139456 logging_writer.py:48] [75429] accumulated_eval_time=16438.574573, accumulated_logging_time=1.159237, accumulated_submission_time=26085.579973, global_step=75429, preemption_count=0, score=26085.579973, test/accuracy=0.689013, test/bleu=29.094086, test/loss=1.532151, test/num_examples=3003, total_duration=42527.725320, train/accuracy=0.676353, train/bleu=33.937370, train/loss=1.624076, validation/accuracy=0.676346, validation/bleu=29.693911, validation/loss=1.601184, validation/num_examples=3000
I0207 05:24:19.944989 140051097532160 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.34620401263237, loss=2.768547534942627
I0207 05:24:54.360625 140051089139456 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3464191257953644, loss=2.8045778274536133
I0207 05:25:28.840809 140051097532160 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3797507882118225, loss=2.8615429401397705
I0207 05:26:03.347004 140051089139456 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3835669457912445, loss=2.9069225788116455
I0207 05:26:37.861170 140051097532160 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.36306941509246826, loss=2.8797779083251953
I0207 05:27:12.381952 140051089139456 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.33127835392951965, loss=2.8537912368774414
I0207 05:27:46.877428 140051097532160 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.37444180250167847, loss=2.828176736831665
I0207 05:28:21.383312 140051089139456 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.3608464002609253, loss=2.841679811477661
I0207 05:28:55.895728 140051097532160 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3724879324436188, loss=2.82705020904541
I0207 05:29:30.428800 140051089139456 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3740755319595337, loss=2.8475940227508545
I0207 05:30:04.937022 140051097532160 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.34173643589019775, loss=2.8366827964782715
I0207 05:30:39.462430 140051089139456 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3698269724845886, loss=2.7509777545928955
I0207 05:31:13.985939 140051097532160 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3934885859489441, loss=2.9178285598754883
I0207 05:31:48.488587 140051089139456 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3857487738132477, loss=2.933464527130127
I0207 05:32:23.005216 140051097532160 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.33135101199150085, loss=2.7863821983337402
I0207 05:32:57.535100 140051089139456 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.35530078411102295, loss=2.7609403133392334
I0207 05:33:32.058103 140051097532160 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3796285390853882, loss=2.8511807918548584
I0207 05:34:06.583068 140051089139456 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.35480666160583496, loss=2.8066492080688477
I0207 05:34:41.129146 140051097532160 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3737843632698059, loss=2.8666248321533203
I0207 05:35:15.646494 140051089139456 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.37193208932876587, loss=2.8174688816070557
I0207 05:35:50.170062 140051097532160 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.36836203932762146, loss=2.8098366260528564
I0207 05:36:24.734432 140051089139456 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3629924952983856, loss=2.824547052383423
I0207 05:36:59.286547 140051097532160 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3680666387081146, loss=2.75839900970459
I0207 05:37:33.798639 140051089139456 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3599362075328827, loss=2.8206496238708496
I0207 05:37:55.263195 140225696298816 spec.py:321] Evaluating on the training split.
I0207 05:37:58.231669 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:41:07.120285 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 05:41:09.786847 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:43:34.254164 140225696298816 spec.py:349] Evaluating on the test split.
I0207 05:43:36.945282 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 05:46:00.121664 140225696298816 submission_runner.py:408] Time since start: 43852.69s, 	Step: 77864, 	{'train/accuracy': 0.6609399318695068, 'train/loss': 1.7068854570388794, 'train/bleu': 32.66516373016686, 'validation/accuracy': 0.6770405769348145, 'validation/loss': 1.597065806388855, 'validation/bleu': 29.539697532026157, 'validation/num_examples': 3000, 'test/accuracy': 0.6887107491493225, 'test/loss': 1.527599573135376, 'test/bleu': 29.207579730088618, 'test/num_examples': 3003, 'score': 26925.578989982605, 'total_duration': 43852.69458556175, 'accumulated_submission_time': 26925.578989982605, 'accumulated_eval_time': 16923.432988643646, 'accumulated_logging_time': 1.1936464309692383}
I0207 05:46:00.147264 140051097532160 logging_writer.py:48] [77864] accumulated_eval_time=16923.432989, accumulated_logging_time=1.193646, accumulated_submission_time=26925.578990, global_step=77864, preemption_count=0, score=26925.578990, test/accuracy=0.688711, test/bleu=29.207580, test/loss=1.527600, test/num_examples=3003, total_duration=43852.694586, train/accuracy=0.660940, train/bleu=32.665164, train/loss=1.706885, validation/accuracy=0.677041, validation/bleu=29.539698, validation/loss=1.597066, validation/num_examples=3000
I0207 05:46:12.898495 140051089139456 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.38632211089134216, loss=2.8522543907165527
I0207 05:46:47.345844 140051097532160 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3919583857059479, loss=2.895573616027832
I0207 05:47:21.842617 140051089139456 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.4187830984592438, loss=2.8647308349609375
I0207 05:47:56.358272 140051097532160 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.39803269505500793, loss=2.9212639331817627
I0207 05:48:30.871917 140051089139456 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3718213737010956, loss=2.810234308242798
I0207 05:49:05.413379 140051097532160 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.37681952118873596, loss=2.726870059967041
I0207 05:49:39.956256 140051089139456 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3585602343082428, loss=2.7814385890960693
I0207 05:50:14.464782 140051097532160 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3611706495285034, loss=2.8392133712768555
I0207 05:50:49.005384 140051089139456 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.38109439611434937, loss=2.875113010406494
I0207 05:51:23.529946 140051097532160 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3743530213832855, loss=2.798975706100464
I0207 05:51:58.073448 140051089139456 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3338066041469574, loss=2.778944730758667
I0207 05:52:32.592706 140051097532160 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3563762307167053, loss=2.810042381286621
I0207 05:53:07.119949 140051089139456 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.34769606590270996, loss=2.847682237625122
I0207 05:53:41.609284 140051097532160 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.35593000054359436, loss=2.79577374458313
I0207 05:54:16.131394 140051089139456 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3567099869251251, loss=2.811452627182007
I0207 05:54:50.654191 140051097532160 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4306732714176178, loss=2.890204668045044
I0207 05:55:25.194828 140051089139456 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.37910401821136475, loss=2.822722911834717
I0207 05:55:59.731975 140051097532160 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3497597277164459, loss=2.8253471851348877
I0207 05:56:34.270527 140051089139456 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.36962661147117615, loss=2.8633129596710205
I0207 05:57:08.814801 140051097532160 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3796391785144806, loss=2.804482936859131
I0207 05:57:43.328111 140051089139456 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.36429303884506226, loss=2.7255773544311523
I0207 05:58:17.858144 140051097532160 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.39165669679641724, loss=2.856099843978882
I0207 05:58:52.384331 140051089139456 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.38595741987228394, loss=2.846088171005249
I0207 05:59:26.905430 140051097532160 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3585531711578369, loss=2.7478907108306885
I0207 06:00:00.448902 140225696298816 spec.py:321] Evaluating on the training split.
I0207 06:00:03.427670 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:03:18.520881 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 06:03:21.191739 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:05:58.854067 140225696298816 spec.py:349] Evaluating on the test split.
I0207 06:06:01.525473 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:08:32.563949 140225696298816 submission_runner.py:408] Time since start: 45205.14s, 	Step: 80299, 	{'train/accuracy': 0.6609601974487305, 'train/loss': 1.7184412479400635, 'train/bleu': 32.26415699149423, 'validation/accuracy': 0.6776357293128967, 'validation/loss': 1.5888279676437378, 'validation/bleu': 29.55108559881877, 'validation/num_examples': 3000, 'test/accuracy': 0.6894195675849915, 'test/loss': 1.5198761224746704, 'test/bleu': 29.1376581392568, 'test/num_examples': 3003, 'score': 27765.792411088943, 'total_duration': 45205.13687705994, 'accumulated_submission_time': 27765.792411088943, 'accumulated_eval_time': 17435.547990322113, 'accumulated_logging_time': 1.2304267883300781}
I0207 06:08:32.589219 140051089139456 logging_writer.py:48] [80299] accumulated_eval_time=17435.547990, accumulated_logging_time=1.230427, accumulated_submission_time=27765.792411, global_step=80299, preemption_count=0, score=27765.792411, test/accuracy=0.689420, test/bleu=29.137658, test/loss=1.519876, test/num_examples=3003, total_duration=45205.136877, train/accuracy=0.660960, train/bleu=32.264157, train/loss=1.718441, validation/accuracy=0.677636, validation/bleu=29.551086, validation/loss=1.588828, validation/num_examples=3000
I0207 06:08:33.292852 140051097532160 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.38543349504470825, loss=2.8389227390289307
I0207 06:09:07.696851 140051089139456 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3603259027004242, loss=2.798495054244995
I0207 06:09:42.148341 140051097532160 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3586007058620453, loss=2.810948133468628
I0207 06:10:16.640178 140051089139456 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3919053077697754, loss=2.7931740283966064
I0207 06:10:51.154193 140051097532160 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.3937091529369354, loss=2.8274409770965576
I0207 06:11:25.688213 140051089139456 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.37092381715774536, loss=2.904203414916992
I0207 06:12:00.234094 140051097532160 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3871278166770935, loss=2.813589334487915
I0207 06:12:34.763823 140051089139456 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.39868679642677307, loss=2.8007209300994873
I0207 06:13:09.301382 140051097532160 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.4039195477962494, loss=2.8570353984832764
I0207 06:13:43.817605 140051089139456 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.37792137265205383, loss=2.794309139251709
I0207 06:14:18.352997 140051097532160 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3700340986251831, loss=2.8000502586364746
I0207 06:14:52.858925 140051089139456 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3692585229873657, loss=2.827108144760132
I0207 06:15:27.379005 140051097532160 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.36957186460494995, loss=2.846628427505493
I0207 06:16:01.915737 140051089139456 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3990456759929657, loss=2.867309093475342
I0207 06:16:36.424070 140051097532160 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3618850111961365, loss=2.8091962337493896
I0207 06:17:10.969915 140051089139456 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.4373045265674591, loss=2.8340795040130615
I0207 06:17:45.496294 140051097532160 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.38736769556999207, loss=2.8523025512695312
I0207 06:18:20.021530 140051089139456 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3798059821128845, loss=2.86287522315979
I0207 06:18:54.533188 140051097532160 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.40441009402275085, loss=2.839374542236328
I0207 06:19:29.075244 140051089139456 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3956301510334015, loss=2.8959991931915283
I0207 06:20:03.649149 140051097532160 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.4146239459514618, loss=2.8502273559570312
I0207 06:20:38.211981 140051089139456 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.3769393265247345, loss=2.8230249881744385
I0207 06:21:12.761446 140051097532160 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.40254732966423035, loss=2.7796409130096436
I0207 06:21:47.304022 140051089139456 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4169348180294037, loss=2.8567605018615723
I0207 06:22:21.864264 140051097532160 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3968762159347534, loss=2.857512950897217
I0207 06:22:32.654566 140225696298816 spec.py:321] Evaluating on the training split.
I0207 06:22:35.615884 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:25:13.543173 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 06:25:16.217637 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:27:45.066375 140225696298816 spec.py:349] Evaluating on the test split.
I0207 06:27:47.741823 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:30:09.068234 140225696298816 submission_runner.py:408] Time since start: 46501.64s, 	Step: 82733, 	{'train/accuracy': 0.6679980754852295, 'train/loss': 1.673125147819519, 'train/bleu': 33.5625012966058, 'validation/accuracy': 0.6785408854484558, 'validation/loss': 1.5870471000671387, 'validation/bleu': 29.696808866105183, 'validation/num_examples': 3000, 'test/accuracy': 0.6937772631645203, 'test/loss': 1.509999394416809, 'test/bleu': 29.743239862901277, 'test/num_examples': 3003, 'score': 28605.771606206894, 'total_duration': 46501.641139268875, 'accumulated_submission_time': 28605.771606206894, 'accumulated_eval_time': 17891.961584091187, 'accumulated_logging_time': 1.264892816543579}
I0207 06:30:09.093670 140051089139456 logging_writer.py:48] [82733] accumulated_eval_time=17891.961584, accumulated_logging_time=1.264893, accumulated_submission_time=28605.771606, global_step=82733, preemption_count=0, score=28605.771606, test/accuracy=0.693777, test/bleu=29.743240, test/loss=1.509999, test/num_examples=3003, total_duration=46501.641139, train/accuracy=0.667998, train/bleu=33.562501, train/loss=1.673125, validation/accuracy=0.678541, validation/bleu=29.696809, validation/loss=1.587047, validation/num_examples=3000
I0207 06:30:32.539759 140051097532160 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.36607781052589417, loss=2.841294288635254
I0207 06:31:06.972477 140051089139456 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.3839310109615326, loss=2.802327871322632
I0207 06:31:41.498188 140051097532160 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3744613826274872, loss=2.7451252937316895
I0207 06:32:16.003644 140051089139456 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3850412368774414, loss=2.8600378036499023
I0207 06:32:50.516149 140051097532160 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3832745850086212, loss=2.862921953201294
I0207 06:33:25.054377 140051089139456 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.36576440930366516, loss=2.804025650024414
I0207 06:33:59.580714 140051097532160 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.40160641074180603, loss=2.8576557636260986
I0207 06:34:34.130131 140051089139456 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3911159634590149, loss=2.8496150970458984
I0207 06:35:08.659161 140051097532160 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3903926908969879, loss=2.7913331985473633
I0207 06:35:43.211522 140051089139456 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.390367329120636, loss=2.789898633956909
I0207 06:36:17.730112 140051097532160 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.39151304960250854, loss=2.7561752796173096
I0207 06:36:52.280194 140051089139456 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.38375023007392883, loss=2.8324947357177734
I0207 06:37:26.808121 140051097532160 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4196912944316864, loss=2.7890191078186035
I0207 06:38:01.364916 140051089139456 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.38895633816719055, loss=2.8219258785247803
I0207 06:38:35.889885 140051097532160 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.38420140743255615, loss=2.8395180702209473
I0207 06:39:10.400066 140051089139456 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4002697467803955, loss=2.8256733417510986
I0207 06:39:44.929083 140051097532160 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.40834230184555054, loss=2.879336357116699
I0207 06:40:19.468145 140051089139456 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.4169987142086029, loss=2.8141417503356934
I0207 06:40:54.024379 140051097532160 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3831028342247009, loss=2.789577007293701
I0207 06:41:28.570548 140051089139456 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3649020791053772, loss=2.7546300888061523
I0207 06:42:03.088170 140051097532160 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3887123465538025, loss=2.7868454456329346
I0207 06:42:37.648134 140051089139456 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4192620515823364, loss=2.7819912433624268
I0207 06:43:12.201726 140051097532160 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.4087780714035034, loss=2.8042075634002686
I0207 06:43:46.757681 140051089139456 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.39234212040901184, loss=2.8685007095336914
I0207 06:44:09.272999 140225696298816 spec.py:321] Evaluating on the training split.
I0207 06:44:12.242847 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:46:56.372615 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 06:46:59.051712 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:49:54.409260 140225696298816 spec.py:349] Evaluating on the test split.
I0207 06:49:57.080461 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 06:52:34.976138 140225696298816 submission_runner.py:408] Time since start: 47847.55s, 	Step: 85167, 	{'train/accuracy': 0.6636251211166382, 'train/loss': 1.700329303741455, 'train/bleu': 32.7424648321825, 'validation/accuracy': 0.6798551678657532, 'validation/loss': 1.578163981437683, 'validation/bleu': 29.852257792933663, 'validation/num_examples': 3000, 'test/accuracy': 0.6938701868057251, 'test/loss': 1.5030529499053955, 'test/bleu': 29.60024070059604, 'test/num_examples': 3003, 'score': 29445.863482236862, 'total_duration': 47847.549050569534, 'accumulated_submission_time': 29445.863482236862, 'accumulated_eval_time': 18397.664662599564, 'accumulated_logging_time': 1.301595687866211}
I0207 06:52:35.002894 140051097532160 logging_writer.py:48] [85167] accumulated_eval_time=18397.664663, accumulated_logging_time=1.301596, accumulated_submission_time=29445.863482, global_step=85167, preemption_count=0, score=29445.863482, test/accuracy=0.693870, test/bleu=29.600241, test/loss=1.503053, test/num_examples=3003, total_duration=47847.549051, train/accuracy=0.663625, train/bleu=32.742465, train/loss=1.700329, validation/accuracy=0.679855, validation/bleu=29.852258, validation/loss=1.578164, validation/num_examples=3000
I0207 06:52:46.735785 140051089139456 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3997066020965576, loss=2.753296375274658
I0207 06:53:21.109410 140051097532160 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.40778660774230957, loss=2.8072402477264404
I0207 06:53:55.608215 140051089139456 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3994896709918976, loss=2.750821828842163
I0207 06:54:30.169956 140051097532160 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.43928444385528564, loss=2.7917356491088867
I0207 06:55:04.687234 140051089139456 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3976193368434906, loss=2.76094913482666
I0207 06:55:39.226855 140051097532160 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.40001729130744934, loss=2.8283472061157227
I0207 06:56:13.750749 140051089139456 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4143598675727844, loss=2.8857195377349854
I0207 06:56:48.283973 140051097532160 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.39746373891830444, loss=2.8089113235473633
I0207 06:57:22.835140 140051089139456 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.5170257687568665, loss=2.811688184738159
I0207 06:57:57.371649 140051097532160 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.41274163126945496, loss=2.7533810138702393
I0207 06:58:31.926820 140051089139456 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.39514490962028503, loss=2.8168392181396484
I0207 06:59:06.471989 140051097532160 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.3982403576374054, loss=2.799238681793213
I0207 06:59:41.004976 140051089139456 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3899812400341034, loss=2.84566068649292
I0207 07:00:15.531911 140051097532160 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3933551013469696, loss=2.821566104888916
I0207 07:00:50.081387 140051089139456 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.40295034646987915, loss=2.84734845161438
I0207 07:01:24.635531 140051097532160 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3978913128376007, loss=2.796323299407959
I0207 07:01:59.167750 140051089139456 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.41523370146751404, loss=2.814466953277588
I0207 07:02:33.732239 140051097532160 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3967682421207428, loss=2.7231667041778564
I0207 07:03:08.270889 140051089139456 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.41806429624557495, loss=2.8688178062438965
I0207 07:03:42.820050 140051097532160 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4117019772529602, loss=2.794440984725952
I0207 07:04:17.332137 140051089139456 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.4086565375328064, loss=2.811600923538208
I0207 07:04:51.861795 140051097532160 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.38286343216896057, loss=2.802720308303833
I0207 07:05:26.405716 140051089139456 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.37348970770835876, loss=2.758631467819214
I0207 07:06:00.948013 140051097532160 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4007713496685028, loss=2.79396390914917
I0207 07:06:35.483743 140051089139456 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.3977240324020386, loss=2.798574447631836
I0207 07:06:35.490819 140225696298816 spec.py:321] Evaluating on the training split.
I0207 07:06:38.184690 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:09:33.894600 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 07:09:36.567231 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:12:09.519905 140225696298816 spec.py:349] Evaluating on the test split.
I0207 07:12:12.192503 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:14:37.231224 140225696298816 submission_runner.py:408] Time since start: 49169.80s, 	Step: 87601, 	{'train/accuracy': 0.7056272029876709, 'train/loss': 1.4632694721221924, 'train/bleu': 35.86558291442635, 'validation/accuracy': 0.6798055768013, 'validation/loss': 1.5734652280807495, 'validation/bleu': 29.873149366350557, 'validation/num_examples': 3000, 'test/accuracy': 0.6945558190345764, 'test/loss': 1.498718023300171, 'test/bleu': 29.48370587895101, 'test/num_examples': 3003, 'score': 30286.26316356659, 'total_duration': 49169.80411338806, 'accumulated_submission_time': 30286.26316356659, 'accumulated_eval_time': 18879.404970645905, 'accumulated_logging_time': 1.3394997119903564}
I0207 07:14:37.257709 140051097532160 logging_writer.py:48] [87601] accumulated_eval_time=18879.404971, accumulated_logging_time=1.339500, accumulated_submission_time=30286.263164, global_step=87601, preemption_count=0, score=30286.263164, test/accuracy=0.694556, test/bleu=29.483706, test/loss=1.498718, test/num_examples=3003, total_duration=49169.804113, train/accuracy=0.705627, train/bleu=35.865583, train/loss=1.463269, validation/accuracy=0.679806, validation/bleu=29.873149, validation/loss=1.573465, validation/num_examples=3000
I0207 07:15:11.676309 140051089139456 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3891768753528595, loss=2.7626278400421143
I0207 07:15:46.123375 140051097532160 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3890385627746582, loss=2.7641167640686035
I0207 07:16:20.648016 140051089139456 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.39709800481796265, loss=2.755932331085205
I0207 07:16:55.199367 140051097532160 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.4113350808620453, loss=2.7885074615478516
I0207 07:17:29.734057 140051089139456 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3892052173614502, loss=2.7483386993408203
I0207 07:18:04.275182 140051097532160 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3705274164676666, loss=2.8373265266418457
I0207 07:18:38.804619 140051089139456 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.41851428151130676, loss=2.799997568130493
I0207 07:19:13.337789 140051097532160 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.38969308137893677, loss=2.800468683242798
I0207 07:19:47.912654 140051089139456 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3999699056148529, loss=2.792102336883545
I0207 07:20:22.448905 140051097532160 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.38934123516082764, loss=2.7730798721313477
I0207 07:20:56.988589 140051089139456 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.39730310440063477, loss=2.786736488342285
I0207 07:21:31.551787 140051097532160 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4310370981693268, loss=2.8919677734375
I0207 07:22:06.073100 140051089139456 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4207567572593689, loss=2.7622289657592773
I0207 07:22:40.617421 140051097532160 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3801558315753937, loss=2.748861074447632
I0207 07:23:15.145988 140051089139456 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.380562424659729, loss=2.831007957458496
I0207 07:23:49.667877 140051097532160 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4133818447589874, loss=2.848890781402588
I0207 07:24:24.202656 140051089139456 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.421440988779068, loss=2.819300413131714
I0207 07:24:58.716390 140051097532160 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.4212241768836975, loss=2.756263017654419
I0207 07:25:33.261205 140051089139456 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.39110246300697327, loss=2.749103307723999
I0207 07:26:07.807205 140051097532160 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.42949002981185913, loss=2.7452023029327393
I0207 07:26:42.311579 140051089139456 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.41381701827049255, loss=2.7680513858795166
I0207 07:27:16.859130 140051097532160 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.408863365650177, loss=2.780121088027954
I0207 07:27:51.416989 140051089139456 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4006785750389099, loss=2.8127329349517822
I0207 07:28:25.926379 140051097532160 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.38892221450805664, loss=2.803598642349243
I0207 07:28:37.373281 140225696298816 spec.py:321] Evaluating on the training split.
I0207 07:28:40.340074 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:31:45.423298 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 07:31:48.085633 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:34:18.587939 140225696298816 spec.py:349] Evaluating on the test split.
I0207 07:34:21.262904 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:36:38.798638 140225696298816 submission_runner.py:408] Time since start: 50491.37s, 	Step: 90035, 	{'train/accuracy': 0.6675459742546082, 'train/loss': 1.6747876405715942, 'train/bleu': 33.33141123987844, 'validation/accuracy': 0.6829301714897156, 'validation/loss': 1.564455509185791, 'validation/bleu': 29.92197180523269, 'validation/num_examples': 3000, 'test/accuracy': 0.6961826682090759, 'test/loss': 1.489499807357788, 'test/bleu': 29.9603429821149, 'test/num_examples': 3003, 'score': 31126.28993988037, 'total_duration': 50491.37156367302, 'accumulated_submission_time': 31126.28993988037, 'accumulated_eval_time': 19360.830275535583, 'accumulated_logging_time': 1.3778259754180908}
I0207 07:36:38.826004 140051089139456 logging_writer.py:48] [90035] accumulated_eval_time=19360.830276, accumulated_logging_time=1.377826, accumulated_submission_time=31126.289940, global_step=90035, preemption_count=0, score=31126.289940, test/accuracy=0.696183, test/bleu=29.960343, test/loss=1.489500, test/num_examples=3003, total_duration=50491.371564, train/accuracy=0.667546, train/bleu=33.331411, train/loss=1.674788, validation/accuracy=0.682930, validation/bleu=29.921972, validation/loss=1.564456, validation/num_examples=3000
I0207 07:37:01.527013 140051097532160 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4142262637615204, loss=2.7963485717773438
I0207 07:37:35.973878 140051089139456 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.39934754371643066, loss=2.6913490295410156
I0207 07:38:10.480784 140051097532160 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.43756043910980225, loss=2.8080995082855225
I0207 07:38:45.017701 140051089139456 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.43306028842926025, loss=2.778881072998047
I0207 07:39:19.559728 140051097532160 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3895902931690216, loss=2.671828031539917
I0207 07:39:54.086092 140051089139456 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.40877196192741394, loss=2.794381618499756
I0207 07:40:28.610338 140051097532160 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4236086905002594, loss=2.7363860607147217
I0207 07:41:03.147544 140051089139456 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4181351661682129, loss=2.7412500381469727
I0207 07:41:37.675393 140051097532160 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4359549880027771, loss=2.8194077014923096
I0207 07:42:12.200286 140051089139456 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.39323869347572327, loss=2.7744765281677246
I0207 07:42:46.759690 140051097532160 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.4029192328453064, loss=2.769343614578247
I0207 07:43:21.299531 140051089139456 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.40480807423591614, loss=2.8614883422851562
I0207 07:43:55.832084 140051097532160 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.41217881441116333, loss=2.7369065284729004
I0207 07:44:30.335006 140051089139456 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4006938338279724, loss=2.7519068717956543
I0207 07:45:04.891772 140051097532160 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4273518919944763, loss=2.822082042694092
I0207 07:45:39.427720 140051089139456 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4414483606815338, loss=2.7971808910369873
I0207 07:46:13.950144 140051097532160 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4616852402687073, loss=2.7987401485443115
I0207 07:46:48.429085 140051089139456 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4240531623363495, loss=2.866135597229004
I0207 07:47:22.985845 140051097532160 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.40542176365852356, loss=2.7375643253326416
I0207 07:47:57.534229 140051089139456 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4310101270675659, loss=2.768476724624634
I0207 07:48:32.062836 140051097532160 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.43928492069244385, loss=2.7097158432006836
I0207 07:49:06.592853 140051089139456 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.4449971318244934, loss=2.769615650177002
I0207 07:49:41.112462 140051097532160 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4582065939903259, loss=2.7522530555725098
I0207 07:50:15.642595 140051089139456 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4515889286994934, loss=2.8297786712646484
I0207 07:50:38.838456 140225696298816 spec.py:321] Evaluating on the training split.
I0207 07:50:41.804959 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:53:40.359961 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 07:53:43.043404 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:56:18.579279 140225696298816 spec.py:349] Evaluating on the test split.
I0207 07:56:21.258988 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 07:58:38.368187 140225696298816 submission_runner.py:408] Time since start: 51810.94s, 	Step: 92469, 	{'train/accuracy': 0.6658973097801208, 'train/loss': 1.6872364282608032, 'train/bleu': 33.660730787562834, 'validation/accuracy': 0.6835625171661377, 'validation/loss': 1.5563279390335083, 'validation/bleu': 30.26361366275628, 'validation/num_examples': 3000, 'test/accuracy': 0.6978908777236938, 'test/loss': 1.4811248779296875, 'test/bleu': 30.035932898851115, 'test/num_examples': 3003, 'score': 31966.213314533234, 'total_duration': 51810.94109606743, 'accumulated_submission_time': 31966.213314533234, 'accumulated_eval_time': 19840.359940052032, 'accumulated_logging_time': 1.4165542125701904}
I0207 07:58:38.395468 140051097532160 logging_writer.py:48] [92469] accumulated_eval_time=19840.359940, accumulated_logging_time=1.416554, accumulated_submission_time=31966.213315, global_step=92469, preemption_count=0, score=31966.213315, test/accuracy=0.697891, test/bleu=30.035933, test/loss=1.481125, test/num_examples=3003, total_duration=51810.941096, train/accuracy=0.665897, train/bleu=33.660731, train/loss=1.687236, validation/accuracy=0.683563, validation/bleu=30.263614, validation/loss=1.556328, validation/num_examples=3000
I0207 07:58:49.432604 140051089139456 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.437504380941391, loss=2.809253215789795
I0207 07:59:23.857835 140051097532160 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.4055774509906769, loss=2.7957234382629395
I0207 07:59:58.355537 140051089139456 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4173264503479004, loss=2.769087553024292
I0207 08:00:32.877313 140051097532160 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.40750861167907715, loss=2.7054779529571533
I0207 08:01:07.420110 140051089139456 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.43702179193496704, loss=2.7192223072052
I0207 08:01:41.948710 140051097532160 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4142983853816986, loss=2.7695322036743164
I0207 08:02:16.494231 140051089139456 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4149320423603058, loss=2.7344212532043457
I0207 08:02:51.043342 140051097532160 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.44084370136260986, loss=2.697340250015259
I0207 08:03:25.600854 140051089139456 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4244530498981476, loss=2.712850332260132
I0207 08:04:00.136474 140051097532160 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4509005844593048, loss=2.762209415435791
I0207 08:04:34.663125 140051089139456 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.44187280535697937, loss=2.810429096221924
I0207 08:05:09.186450 140051097532160 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.4380517303943634, loss=2.7621712684631348
I0207 08:05:43.738627 140051089139456 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.45136481523513794, loss=2.7603096961975098
I0207 08:06:18.281697 140051097532160 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4435303211212158, loss=2.755349636077881
I0207 08:06:52.815133 140051089139456 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.43784430623054504, loss=2.8100152015686035
I0207 08:07:27.346590 140051097532160 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4603896737098694, loss=2.72684907913208
I0207 08:08:01.851061 140051089139456 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.43571704626083374, loss=2.7695350646972656
I0207 08:08:36.399206 140051097532160 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.44601744413375854, loss=2.7438766956329346
I0207 08:09:10.933571 140051089139456 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.43888387084007263, loss=2.801647424697876
I0207 08:09:45.459435 140051097532160 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4392029345035553, loss=2.756627321243286
I0207 08:10:20.011178 140051089139456 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.42376697063446045, loss=2.7906696796417236
I0207 08:10:54.545853 140051097532160 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.42056038975715637, loss=2.757107734680176
I0207 08:11:29.075476 140051089139456 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4372580051422119, loss=2.7510178089141846
I0207 08:12:03.599251 140051097532160 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4537976384162903, loss=2.6975207328796387
I0207 08:12:38.162539 140051089139456 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4260163903236389, loss=2.724257707595825
I0207 08:12:38.577087 140225696298816 spec.py:321] Evaluating on the training split.
I0207 08:12:41.542720 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:15:37.160873 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 08:15:39.837560 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:18:16.394833 140225696298816 spec.py:349] Evaluating on the test split.
I0207 08:18:19.079614 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:20:42.313247 140225696298816 submission_runner.py:408] Time since start: 53134.89s, 	Step: 94903, 	{'train/accuracy': 0.6817435026168823, 'train/loss': 1.5854727029800415, 'train/bleu': 33.982398172121265, 'validation/accuracy': 0.6839964985847473, 'validation/loss': 1.5531708002090454, 'validation/bleu': 30.10177069772297, 'validation/num_examples': 3000, 'test/accuracy': 0.7008076310157776, 'test/loss': 1.4715487957000732, 'test/bleu': 29.92625119054191, 'test/num_examples': 3003, 'score': 32806.30698490143, 'total_duration': 53134.88615298271, 'accumulated_submission_time': 32806.30698490143, 'accumulated_eval_time': 20324.09602546692, 'accumulated_logging_time': 1.453615665435791}
I0207 08:20:42.341955 140051097532160 logging_writer.py:48] [94903] accumulated_eval_time=20324.096025, accumulated_logging_time=1.453616, accumulated_submission_time=32806.306985, global_step=94903, preemption_count=0, score=32806.306985, test/accuracy=0.700808, test/bleu=29.926251, test/loss=1.471549, test/num_examples=3003, total_duration=53134.886153, train/accuracy=0.681744, train/bleu=33.982398, train/loss=1.585473, validation/accuracy=0.683996, validation/bleu=30.101771, validation/loss=1.553171, validation/num_examples=3000
I0207 08:21:16.102842 140051089139456 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.4556211233139038, loss=2.757753610610962
I0207 08:21:50.582296 140051097532160 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.44691407680511475, loss=2.7336416244506836
I0207 08:22:25.132437 140051089139456 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.44001489877700806, loss=2.7839879989624023
I0207 08:22:59.697823 140051097532160 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.4728401303291321, loss=2.7782318592071533
I0207 08:23:34.232266 140051089139456 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.4467540681362152, loss=2.664659261703491
I0207 08:24:08.776938 140051097532160 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4456523060798645, loss=2.806142807006836
I0207 08:24:43.320801 140051089139456 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4426155984401703, loss=2.752382755279541
I0207 08:25:17.873606 140051097532160 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4260932505130768, loss=2.7108707427978516
I0207 08:25:52.435473 140051089139456 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4171176850795746, loss=2.706760883331299
I0207 08:26:26.932169 140051097532160 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.448128342628479, loss=2.7196223735809326
I0207 08:27:01.462706 140051089139456 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.4614834487438202, loss=2.765458345413208
I0207 08:27:35.986642 140051097532160 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.48023512959480286, loss=2.7617921829223633
I0207 08:28:10.525646 140051089139456 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.43188712000846863, loss=2.724672555923462
I0207 08:28:45.042452 140051097532160 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.44984447956085205, loss=2.812770366668701
I0207 08:29:19.615729 140051089139456 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.6351433992385864, loss=2.7402844429016113
I0207 08:29:54.176311 140051097532160 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.4429611265659332, loss=2.775552988052368
I0207 08:30:28.710977 140051089139456 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.4701510965824127, loss=2.7911770343780518
I0207 08:31:03.259193 140051097532160 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.4557618498802185, loss=2.7023866176605225
I0207 08:31:37.783049 140051089139456 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.4597128927707672, loss=2.811401128768921
I0207 08:32:12.335376 140051097532160 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.46325168013572693, loss=2.739062786102295
I0207 08:32:46.892168 140051089139456 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4669587016105652, loss=2.7877328395843506
I0207 08:33:21.442461 140051097532160 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.4803067743778229, loss=2.680421829223633
I0207 08:33:55.950588 140051089139456 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.44024336338043213, loss=2.7473905086517334
I0207 08:34:30.514463 140051097532160 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.45449763536453247, loss=2.7975666522979736
I0207 08:34:42.646906 140225696298816 spec.py:321] Evaluating on the training split.
I0207 08:34:45.612271 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:37:37.928385 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 08:37:40.607533 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:40:03.228285 140225696298816 spec.py:349] Evaluating on the test split.
I0207 08:40:05.909352 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:42:23.994388 140225696298816 submission_runner.py:408] Time since start: 54436.57s, 	Step: 97337, 	{'train/accuracy': 0.6754931807518005, 'train/loss': 1.6232917308807373, 'train/bleu': 33.42584230300187, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.5403165817260742, 'validation/bleu': 30.324988297327963, 'validation/num_examples': 3000, 'test/accuracy': 0.7019813060760498, 'test/loss': 1.4636123180389404, 'test/bleu': 30.23504562070128, 'test/num_examples': 3003, 'score': 33646.52457332611, 'total_duration': 54436.56728100777, 'accumulated_submission_time': 33646.52457332611, 'accumulated_eval_time': 20785.443420886993, 'accumulated_logging_time': 1.492318868637085}
I0207 08:42:24.024146 140051089139456 logging_writer.py:48] [97337] accumulated_eval_time=20785.443421, accumulated_logging_time=1.492319, accumulated_submission_time=33646.524573, global_step=97337, preemption_count=0, score=33646.524573, test/accuracy=0.701981, test/bleu=30.235046, test/loss=1.463612, test/num_examples=3003, total_duration=54436.567281, train/accuracy=0.675493, train/bleu=33.425842, train/loss=1.623292, validation/accuracy=0.686873, validation/bleu=30.324988, validation/loss=1.540317, validation/num_examples=3000
I0207 08:42:46.068939 140051097532160 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.47762173414230347, loss=2.713169813156128
I0207 08:43:20.531766 140051089139456 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.48397335410118103, loss=2.8127291202545166
I0207 08:43:55.047733 140051097532160 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4586462378501892, loss=2.7278571128845215
I0207 08:44:29.594296 140051089139456 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4548107981681824, loss=2.7366738319396973
I0207 08:45:04.133424 140051097532160 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.4429895877838135, loss=2.701355218887329
I0207 08:45:38.653431 140051089139456 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.46817469596862793, loss=2.7727155685424805
I0207 08:46:13.211695 140051097532160 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.4768420457839966, loss=2.783507823944092
I0207 08:46:47.773914 140051089139456 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.47138309478759766, loss=2.7148098945617676
I0207 08:47:22.299275 140051097532160 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.47609686851501465, loss=2.7756664752960205
I0207 08:47:56.841698 140051089139456 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.476578027009964, loss=2.7509756088256836
I0207 08:48:31.394954 140051097532160 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5077097415924072, loss=2.734947443008423
I0207 08:49:05.917146 140051089139456 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.4720359146595001, loss=2.7099485397338867
I0207 08:49:40.444231 140051097532160 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5125374794006348, loss=2.726316452026367
I0207 08:50:14.965672 140051089139456 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.4551234841346741, loss=2.670830726623535
I0207 08:50:49.512024 140051097532160 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5090228319168091, loss=2.815697193145752
I0207 08:51:24.041068 140051089139456 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.44339853525161743, loss=2.70200252532959
I0207 08:51:58.578513 140051097532160 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.470096230506897, loss=2.7222797870635986
I0207 08:52:33.102348 140051089139456 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.4792890250682831, loss=2.779902935028076
I0207 08:53:07.649871 140051097532160 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.43753981590270996, loss=2.6838560104370117
I0207 08:53:42.203856 140051089139456 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.4873090088367462, loss=2.743314743041992
I0207 08:54:16.748763 140051097532160 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.44657087326049805, loss=2.7186994552612305
I0207 08:54:51.258725 140051089139456 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.4684385359287262, loss=2.748743772506714
I0207 08:55:25.789191 140051097532160 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.4875963032245636, loss=2.6917989253997803
I0207 08:56:00.327682 140051089139456 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.45651471614837646, loss=2.7287676334381104
I0207 08:56:24.256080 140225696298816 spec.py:321] Evaluating on the training split.
I0207 08:56:27.227994 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 08:59:23.781777 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 08:59:26.461986 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:02:00.854361 140225696298816 spec.py:349] Evaluating on the test split.
I0207 09:02:03.539641 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:04:23.192171 140225696298816 submission_runner.py:408] Time since start: 55755.77s, 	Step: 99771, 	{'train/accuracy': 0.6739282011985779, 'train/loss': 1.63223135471344, 'train/bleu': 33.990486539139006, 'validation/accuracy': 0.6875302195549011, 'validation/loss': 1.536251187324524, 'validation/bleu': 30.240875818928536, 'validation/num_examples': 3000, 'test/accuracy': 0.7032363414764404, 'test/loss': 1.4567337036132812, 'test/bleu': 30.214177626963963, 'test/num_examples': 3003, 'score': 34486.669437885284, 'total_duration': 55755.76510024071, 'accumulated_submission_time': 34486.669437885284, 'accumulated_eval_time': 21264.37947511673, 'accumulated_logging_time': 1.5324418544769287}
I0207 09:04:23.221453 140051097532160 logging_writer.py:48] [99771] accumulated_eval_time=21264.379475, accumulated_logging_time=1.532442, accumulated_submission_time=34486.669438, global_step=99771, preemption_count=0, score=34486.669438, test/accuracy=0.703236, test/bleu=30.214178, test/loss=1.456734, test/num_examples=3003, total_duration=55755.765100, train/accuracy=0.673928, train/bleu=33.990487, train/loss=1.632231, validation/accuracy=0.687530, validation/bleu=30.240876, validation/loss=1.536251, validation/num_examples=3000
I0207 09:04:33.553013 140051089139456 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.4508552849292755, loss=2.7196884155273438
I0207 09:05:07.969805 140051097532160 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.4969172775745392, loss=2.7700226306915283
I0207 09:05:42.481736 140051089139456 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.47066619992256165, loss=2.77700138092041
I0207 09:06:17.022145 140051097532160 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.4708614647388458, loss=2.7272276878356934
I0207 09:06:51.557351 140051089139456 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5027704238891602, loss=2.7145373821258545
I0207 09:07:26.113034 140051097532160 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.4473104774951935, loss=2.7213733196258545
I0207 09:08:00.646207 140051089139456 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.4702881872653961, loss=2.7383861541748047
I0207 09:08:35.194354 140051097532160 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.49912071228027344, loss=2.771291494369507
I0207 09:09:09.742816 140051089139456 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.4897986948490143, loss=2.715758800506592
I0207 09:09:44.279672 140051097532160 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.48638981580734253, loss=2.7625606060028076
I0207 09:10:18.819871 140051089139456 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5156886577606201, loss=2.6802866458892822
I0207 09:10:53.360518 140051097532160 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.4905584156513214, loss=2.6979503631591797
I0207 09:11:27.900601 140051089139456 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.48564907908439636, loss=2.7308292388916016
I0207 09:12:02.447908 140051097532160 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5013279914855957, loss=2.6840593814849854
I0207 09:12:37.020785 140051089139456 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5157970190048218, loss=2.7478644847869873
I0207 09:13:11.579254 140051097532160 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.4874200224876404, loss=2.7901527881622314
I0207 09:13:46.125099 140051089139456 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.4701153039932251, loss=2.7217509746551514
I0207 09:14:20.681770 140051097532160 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.53071129322052, loss=2.764432668685913
I0207 09:14:55.220762 140051089139456 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5201570391654968, loss=2.808342933654785
I0207 09:15:29.758330 140051097532160 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.49378448724746704, loss=2.7264492511749268
I0207 09:16:04.308993 140051089139456 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.49326151609420776, loss=2.739797353744507
I0207 09:16:38.838024 140051097532160 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5261531472206116, loss=2.7507681846618652
I0207 09:17:13.391907 140051089139456 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.4671260416507721, loss=2.7042531967163086
I0207 09:17:47.942251 140051097532160 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.4789574146270752, loss=2.651357889175415
I0207 09:18:22.454028 140051089139456 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.47979050874710083, loss=2.721771717071533
I0207 09:18:23.217518 140225696298816 spec.py:321] Evaluating on the training split.
I0207 09:18:26.187388 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:21:12.832751 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 09:21:15.518923 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:23:36.463637 140225696298816 spec.py:349] Evaluating on the test split.
I0207 09:23:39.150486 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:25:55.126820 140225696298816 submission_runner.py:408] Time since start: 57047.70s, 	Step: 102204, 	{'train/accuracy': 0.6823273301124573, 'train/loss': 1.5801806449890137, 'train/bleu': 34.581900656888564, 'validation/accuracy': 0.6878277659416199, 'validation/loss': 1.5308395624160767, 'validation/bleu': 30.43020782921961, 'validation/num_examples': 3000, 'test/accuracy': 0.7022369503974915, 'test/loss': 1.456139326095581, 'test/bleu': 30.21770641738397, 'test/num_examples': 3003, 'score': 35326.57743191719, 'total_duration': 57047.699733018875, 'accumulated_submission_time': 35326.57743191719, 'accumulated_eval_time': 21716.28870844841, 'accumulated_logging_time': 1.5729172229766846}
I0207 09:25:55.156244 140051097532160 logging_writer.py:48] [102204] accumulated_eval_time=21716.288708, accumulated_logging_time=1.572917, accumulated_submission_time=35326.577432, global_step=102204, preemption_count=0, score=35326.577432, test/accuracy=0.702237, test/bleu=30.217706, test/loss=1.456139, test/num_examples=3003, total_duration=57047.699733, train/accuracy=0.682327, train/bleu=34.581901, train/loss=1.580181, validation/accuracy=0.687828, validation/bleu=30.430208, validation/loss=1.530840, validation/num_examples=3000
I0207 09:26:28.584911 140051089139456 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5330740213394165, loss=2.7090227603912354
I0207 09:27:03.054922 140051097532160 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.4651661217212677, loss=2.6584067344665527
I0207 09:27:37.582746 140051089139456 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.49577420949935913, loss=2.7493276596069336
I0207 09:28:12.105988 140051097532160 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.49657756090164185, loss=2.7168984413146973
I0207 09:28:46.623753 140051089139456 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5006041526794434, loss=2.8169126510620117
I0207 09:29:21.144566 140051097532160 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.49590471386909485, loss=2.768334150314331
I0207 09:29:55.666714 140051089139456 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5013595819473267, loss=2.691772937774658
I0207 09:30:30.183161 140051097532160 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.48589617013931274, loss=2.750467300415039
I0207 09:31:04.748919 140051089139456 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5306181907653809, loss=2.75011944770813
I0207 09:31:39.292325 140051097532160 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5120052099227905, loss=2.706991195678711
I0207 09:32:13.841219 140051089139456 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5350912809371948, loss=2.7429332733154297
I0207 09:32:48.362779 140051097532160 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5076996684074402, loss=2.7045655250549316
I0207 09:33:22.886760 140051089139456 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5198885202407837, loss=2.6984493732452393
I0207 09:33:57.421987 140051097532160 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.527362048625946, loss=2.722414493560791
I0207 09:34:31.965782 140051089139456 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5126938223838806, loss=2.7184760570526123
I0207 09:35:06.499864 140051097532160 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5056604743003845, loss=2.650033712387085
I0207 09:35:41.042975 140051089139456 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5316129922866821, loss=2.7331337928771973
I0207 09:36:15.572023 140051097532160 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5172448754310608, loss=2.691547155380249
I0207 09:36:50.108405 140051089139456 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5204128623008728, loss=2.742279291152954
I0207 09:37:24.645570 140051097532160 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.526240348815918, loss=2.7120778560638428
I0207 09:37:59.166929 140051089139456 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5175777673721313, loss=2.7253196239471436
I0207 09:38:33.700878 140051097532160 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5047919154167175, loss=2.724411964416504
I0207 09:39:08.248189 140051089139456 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5156801342964172, loss=2.767547607421875
I0207 09:39:42.798299 140051097532160 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.4983714818954468, loss=2.674039602279663
I0207 09:39:55.287237 140225696298816 spec.py:321] Evaluating on the training split.
I0207 09:39:58.264561 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:43:09.625244 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 09:43:12.294623 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:45:48.683010 140225696298816 spec.py:349] Evaluating on the test split.
I0207 09:45:51.355383 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 09:48:10.232509 140225696298816 submission_runner.py:408] Time since start: 58382.81s, 	Step: 104638, 	{'train/accuracy': 0.6786729693412781, 'train/loss': 1.605922818183899, 'train/bleu': 34.14329271426784, 'validation/accuracy': 0.688373327255249, 'validation/loss': 1.5237150192260742, 'validation/bleu': 30.576941500675026, 'validation/num_examples': 3000, 'test/accuracy': 0.7040613889694214, 'test/loss': 1.4440556764602661, 'test/bleu': 30.680197257595136, 'test/num_examples': 3003, 'score': 36166.62100124359, 'total_duration': 58382.8054254055, 'accumulated_submission_time': 36166.62100124359, 'accumulated_eval_time': 22211.2339220047, 'accumulated_logging_time': 1.612227439880371}
I0207 09:48:10.262169 140051089139456 logging_writer.py:48] [104638] accumulated_eval_time=22211.233922, accumulated_logging_time=1.612227, accumulated_submission_time=36166.621001, global_step=104638, preemption_count=0, score=36166.621001, test/accuracy=0.704061, test/bleu=30.680197, test/loss=1.444056, test/num_examples=3003, total_duration=58382.805425, train/accuracy=0.678673, train/bleu=34.143293, train/loss=1.605923, validation/accuracy=0.688373, validation/bleu=30.576942, validation/loss=1.523715, validation/num_examples=3000
I0207 09:48:31.950025 140051097532160 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.521870493888855, loss=2.7195539474487305
I0207 09:49:06.410323 140051089139456 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.524016797542572, loss=2.7146248817443848
I0207 09:49:40.932394 140051097532160 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5456579923629761, loss=2.700876235961914
I0207 09:50:15.464923 140051089139456 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5190566182136536, loss=2.7043614387512207
I0207 09:50:50.009207 140051097532160 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5147536396980286, loss=2.6703054904937744
I0207 09:51:24.535051 140051089139456 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.5332725048065186, loss=2.744594097137451
I0207 09:51:59.078431 140051097532160 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5154016017913818, loss=2.715329885482788
I0207 09:52:33.642612 140051089139456 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5206993222236633, loss=2.6754653453826904
I0207 09:53:08.178573 140051097532160 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5255925059318542, loss=2.6926846504211426
I0207 09:53:42.728466 140051089139456 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.513590395450592, loss=2.702218532562256
I0207 09:54:17.270493 140051097532160 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5261048674583435, loss=2.6863605976104736
I0207 09:54:51.781686 140051089139456 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5409940481185913, loss=2.7121150493621826
I0207 09:55:26.291325 140051097532160 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5559062957763672, loss=2.6686720848083496
I0207 09:56:00.823841 140051089139456 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.51630699634552, loss=2.629087209701538
I0207 09:56:35.356902 140051097532160 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.522260308265686, loss=2.6632115840911865
I0207 09:57:09.899876 140051089139456 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5559446811676025, loss=2.737745523452759
I0207 09:57:44.437259 140051097532160 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.548704981803894, loss=2.7414252758026123
I0207 09:58:18.964221 140051089139456 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5445077419281006, loss=2.707195520401001
I0207 09:58:53.502123 140051097532160 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.5214257836341858, loss=2.680849552154541
I0207 09:59:28.035263 140051089139456 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.5196952223777771, loss=2.726128339767456
I0207 10:00:02.562249 140051097532160 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5479556322097778, loss=2.732680320739746
I0207 10:00:37.108493 140051089139456 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5400943160057068, loss=2.704071521759033
I0207 10:01:11.651614 140051097532160 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5343618392944336, loss=2.6594486236572266
I0207 10:01:46.181370 140051089139456 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5431878566741943, loss=2.748138189315796
I0207 10:02:10.425531 140225696298816 spec.py:321] Evaluating on the training split.
I0207 10:02:13.405751 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:05:07.391452 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 10:05:10.072597 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:07:34.363621 140225696298816 spec.py:349] Evaluating on the test split.
I0207 10:07:37.045686 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:09:56.909946 140225696298816 submission_runner.py:408] Time since start: 59689.48s, 	Step: 107072, 	{'train/accuracy': 0.6972777843475342, 'train/loss': 1.5013537406921387, 'train/bleu': 35.75124263193658, 'validation/accuracy': 0.6902828216552734, 'validation/loss': 1.5222079753875732, 'validation/bleu': 30.507136574926953, 'validation/num_examples': 3000, 'test/accuracy': 0.7058277130126953, 'test/loss': 1.44207763671875, 'test/bleu': 30.487959006414762, 'test/num_examples': 3003, 'score': 37006.69671726227, 'total_duration': 59689.48287606239, 'accumulated_submission_time': 37006.69671726227, 'accumulated_eval_time': 22677.718291044235, 'accumulated_logging_time': 1.6531658172607422}
I0207 10:09:56.939023 140051097532160 logging_writer.py:48] [107072] accumulated_eval_time=22677.718291, accumulated_logging_time=1.653166, accumulated_submission_time=37006.696717, global_step=107072, preemption_count=0, score=37006.696717, test/accuracy=0.705828, test/bleu=30.487959, test/loss=1.442078, test/num_examples=3003, total_duration=59689.482876, train/accuracy=0.697278, train/bleu=35.751243, train/loss=1.501354, validation/accuracy=0.690283, validation/bleu=30.507137, validation/loss=1.522208, validation/num_examples=3000
I0207 10:10:06.939802 140051089139456 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5773162841796875, loss=2.714653491973877
I0207 10:10:41.342009 140051097532160 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.5391282439231873, loss=2.714829683303833
I0207 10:11:15.835456 140051089139456 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5469886064529419, loss=2.68135666847229
I0207 10:11:50.387986 140051097532160 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.5252876877784729, loss=2.699232578277588
I0207 10:12:24.902897 140051089139456 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.5693450570106506, loss=2.6248412132263184
I0207 10:12:59.453557 140051097532160 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5501127243041992, loss=2.6537599563598633
I0207 10:13:34.031015 140051089139456 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.5420969724655151, loss=2.6466612815856934
I0207 10:14:08.558565 140051097532160 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.5546776652336121, loss=2.6784141063690186
I0207 10:14:43.095286 140051089139456 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.5628548264503479, loss=2.6295037269592285
I0207 10:15:17.653245 140051097532160 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5655196905136108, loss=2.6892504692077637
I0207 10:15:52.185014 140051089139456 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5648713111877441, loss=2.68841814994812
I0207 10:16:26.739390 140051097532160 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.5800292491912842, loss=2.6859989166259766
I0207 10:17:01.305472 140051089139456 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.5602565407752991, loss=2.723325729370117
I0207 10:17:35.838334 140051097532160 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.5276884436607361, loss=2.6608123779296875
I0207 10:18:10.399011 140051089139456 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.5656985640525818, loss=2.675004720687866
I0207 10:18:44.944190 140051097532160 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.5600093007087708, loss=2.7117652893066406
I0207 10:19:19.475141 140051089139456 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.5853474736213684, loss=2.716085433959961
I0207 10:19:54.025522 140051097532160 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5669766068458557, loss=2.6554672718048096
I0207 10:20:28.543223 140051089139456 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.5677695274353027, loss=2.6412875652313232
I0207 10:21:03.055428 140051097532160 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.5880787372589111, loss=2.7007856369018555
I0207 10:21:37.593628 140051089139456 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.5581747889518738, loss=2.628750801086426
I0207 10:22:12.141034 140051097532160 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.5950593948364258, loss=2.7083160877227783
I0207 10:22:46.694127 140051089139456 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.592801034450531, loss=2.6524643898010254
I0207 10:23:21.252022 140051097532160 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.5738068222999573, loss=2.653843641281128
I0207 10:23:55.787470 140051089139456 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.5428880453109741, loss=2.729769229888916
I0207 10:23:57.239116 140225696298816 spec.py:321] Evaluating on the training split.
I0207 10:24:00.212974 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:27:15.181100 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 10:27:17.856728 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:29:43.683295 140225696298816 spec.py:349] Evaluating on the test split.
I0207 10:29:46.365227 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:32:06.077902 140225696298816 submission_runner.py:408] Time since start: 61018.65s, 	Step: 109506, 	{'train/accuracy': 0.6877254843711853, 'train/loss': 1.5565861463546753, 'train/bleu': 35.05868995744967, 'validation/accuracy': 0.6905431747436523, 'validation/loss': 1.5144131183624268, 'validation/bleu': 30.726323142927132, 'validation/num_examples': 3000, 'test/accuracy': 0.7075126767158508, 'test/loss': 1.4303762912750244, 'test/bleu': 30.782647298833037, 'test/num_examples': 3003, 'score': 37846.90900039673, 'total_duration': 61018.65080690384, 'accumulated_submission_time': 37846.90900039673, 'accumulated_eval_time': 23166.557002544403, 'accumulated_logging_time': 1.693382978439331}
I0207 10:32:06.108769 140051097532160 logging_writer.py:48] [109506] accumulated_eval_time=23166.557003, accumulated_logging_time=1.693383, accumulated_submission_time=37846.909000, global_step=109506, preemption_count=0, score=37846.909000, test/accuracy=0.707513, test/bleu=30.782647, test/loss=1.430376, test/num_examples=3003, total_duration=61018.650807, train/accuracy=0.687725, train/bleu=35.058690, train/loss=1.556586, validation/accuracy=0.690543, validation/bleu=30.726323, validation/loss=1.514413, validation/num_examples=3000
I0207 10:32:38.784447 140051089139456 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.5861870646476746, loss=2.626377582550049
I0207 10:33:13.282246 140051097532160 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.5813795328140259, loss=2.695509910583496
I0207 10:33:47.804244 140051089139456 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.5663244724273682, loss=2.608820915222168
I0207 10:34:22.340943 140051097532160 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.593049168586731, loss=2.706447124481201
I0207 10:34:56.893714 140051089139456 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.5597707629203796, loss=2.6723506450653076
I0207 10:35:31.429304 140051097532160 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.5869386792182922, loss=2.6896750926971436
I0207 10:36:05.942894 140051089139456 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.5613435506820679, loss=2.709529161453247
I0207 10:36:40.494489 140051097532160 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.580578088760376, loss=2.663083791732788
I0207 10:37:15.038828 140051089139456 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6062576770782471, loss=2.754258155822754
I0207 10:37:49.569567 140051097532160 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.5844220519065857, loss=2.65804386138916
I0207 10:38:24.128448 140051089139456 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.5693972706794739, loss=2.6161985397338867
I0207 10:38:58.731223 140051097532160 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6116000413894653, loss=2.72184681892395
I0207 10:39:33.279422 140051089139456 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.56437748670578, loss=2.7098569869995117
I0207 10:40:07.803679 140051097532160 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.5880643129348755, loss=2.735257863998413
I0207 10:40:42.346740 140051089139456 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6013827919960022, loss=2.6712698936462402
I0207 10:41:16.916932 140051097532160 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6350969076156616, loss=2.6946728229522705
I0207 10:41:51.461650 140051089139456 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.5852473974227905, loss=2.6286113262176514
I0207 10:42:26.042780 140051097532160 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.5823315978050232, loss=2.6876347064971924
I0207 10:43:00.615747 140051089139456 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.5978615880012512, loss=2.707312822341919
I0207 10:43:35.157075 140051097532160 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6024085283279419, loss=2.7068417072296143
I0207 10:44:09.684149 140051089139456 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6144939064979553, loss=2.6779210567474365
I0207 10:44:44.212510 140051097532160 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.594071090221405, loss=2.6583914756774902
I0207 10:45:18.741500 140051089139456 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6064585447311401, loss=2.652632713317871
I0207 10:45:53.292262 140051097532160 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6150318384170532, loss=2.7143702507019043
I0207 10:46:06.141179 140225696298816 spec.py:321] Evaluating on the training split.
I0207 10:46:09.119647 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:48:55.731323 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 10:48:58.410917 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:51:32.398126 140225696298816 spec.py:349] Evaluating on the test split.
I0207 10:51:35.085104 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 10:53:58.386859 140225696298816 submission_runner.py:408] Time since start: 62330.96s, 	Step: 111939, 	{'train/accuracy': 0.6905927658081055, 'train/loss': 1.540831446647644, 'train/bleu': 34.9471526156188, 'validation/accuracy': 0.6911631226539612, 'validation/loss': 1.5094802379608154, 'validation/bleu': 30.61271867582907, 'validation/num_examples': 3000, 'test/accuracy': 0.707129180431366, 'test/loss': 1.4275809526443481, 'test/bleu': 30.68408721481186, 'test/num_examples': 3003, 'score': 38686.852848529816, 'total_duration': 62330.95977449417, 'accumulated_submission_time': 38686.852848529816, 'accumulated_eval_time': 23638.802623033524, 'accumulated_logging_time': 1.736116647720337}
I0207 10:53:58.417551 140051089139456 logging_writer.py:48] [111939] accumulated_eval_time=23638.802623, accumulated_logging_time=1.736117, accumulated_submission_time=38686.852849, global_step=111939, preemption_count=0, score=38686.852849, test/accuracy=0.707129, test/bleu=30.684087, test/loss=1.427581, test/num_examples=3003, total_duration=62330.959774, train/accuracy=0.690593, train/bleu=34.947153, train/loss=1.540831, validation/accuracy=0.691163, validation/bleu=30.612719, validation/loss=1.509480, validation/num_examples=3000
I0207 10:54:19.793729 140051097532160 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6323229670524597, loss=2.70241379737854
I0207 10:54:54.241707 140051089139456 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6377853155136108, loss=2.625584125518799
I0207 10:55:28.776706 140051097532160 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6182472109794617, loss=2.6310250759124756
I0207 10:56:03.312785 140051089139456 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.5986020565032959, loss=2.639299154281616
I0207 10:56:37.852587 140051097532160 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.590169370174408, loss=2.6312777996063232
I0207 10:57:12.377501 140051089139456 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6129797101020813, loss=2.661816120147705
I0207 10:57:46.936883 140051097532160 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.6145234704017639, loss=2.675104856491089
I0207 10:58:21.476798 140051089139456 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6114168167114258, loss=2.6043450832366943
I0207 10:58:56.029525 140051097532160 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.5963382720947266, loss=2.6422324180603027
I0207 10:59:30.597997 140051089139456 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6108905673027039, loss=2.638101577758789
I0207 11:00:05.177286 140051097532160 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.5908547639846802, loss=2.5794479846954346
I0207 11:00:39.763095 140051089139456 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6190311908721924, loss=2.587146282196045
I0207 11:01:14.300239 140051097532160 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.5795372724533081, loss=2.584764003753662
I0207 11:01:48.822777 140051089139456 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.5962074995040894, loss=2.66886568069458
I0207 11:02:23.387272 140051097532160 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.641097366809845, loss=2.6485037803649902
I0207 11:02:57.959614 140051089139456 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.635733962059021, loss=2.652491331100464
I0207 11:03:32.499495 140051097532160 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6078951358795166, loss=2.634187936782837
I0207 11:04:07.053504 140051089139456 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.6271229982376099, loss=2.64656138420105
I0207 11:04:41.614087 140051097532160 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.6599618792533875, loss=2.6272034645080566
I0207 11:05:16.178900 140051089139456 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.620943009853363, loss=2.610715389251709
I0207 11:05:50.732527 140051097532160 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6127349138259888, loss=2.6357004642486572
I0207 11:06:25.291791 140051089139456 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6221210360527039, loss=2.6368191242218018
I0207 11:06:59.844512 140051097532160 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6244847178459167, loss=2.667902708053589
I0207 11:07:34.405852 140051089139456 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6476967930793762, loss=2.663907051086426
I0207 11:07:58.641211 140225696298816 spec.py:321] Evaluating on the training split.
I0207 11:08:01.613019 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:10:52.609541 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 11:10:55.287716 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:13:25.621923 140225696298816 spec.py:349] Evaluating on the test split.
I0207 11:13:28.300280 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:15:44.811242 140225696298816 submission_runner.py:408] Time since start: 63637.38s, 	Step: 114372, 	{'train/accuracy': 0.7004918456077576, 'train/loss': 1.4847887754440308, 'train/bleu': 36.07395854939219, 'validation/accuracy': 0.6930230259895325, 'validation/loss': 1.5063766241073608, 'validation/bleu': 30.78058822928789, 'validation/num_examples': 3000, 'test/accuracy': 0.708628237247467, 'test/loss': 1.4222077131271362, 'test/bleu': 30.82562319625663, 'test/num_examples': 3003, 'score': 39526.988307476044, 'total_duration': 63637.38416719437, 'accumulated_submission_time': 39526.988307476044, 'accumulated_eval_time': 24104.972608327866, 'accumulated_logging_time': 1.7767961025238037}
I0207 11:15:44.841508 140051097532160 logging_writer.py:48] [114372] accumulated_eval_time=24104.972608, accumulated_logging_time=1.776796, accumulated_submission_time=39526.988307, global_step=114372, preemption_count=0, score=39526.988307, test/accuracy=0.708628, test/bleu=30.825623, test/loss=1.422208, test/num_examples=3003, total_duration=63637.384167, train/accuracy=0.700492, train/bleu=36.073959, train/loss=1.484789, validation/accuracy=0.693023, validation/bleu=30.780588, validation/loss=1.506377, validation/num_examples=3000
I0207 11:15:54.827841 140051089139456 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.657676100730896, loss=2.6478207111358643
I0207 11:16:29.261836 140051097532160 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6305356025695801, loss=2.7186641693115234
I0207 11:17:03.764157 140051089139456 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.6357977390289307, loss=2.634122610092163
I0207 11:17:38.274586 140051097532160 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.643854558467865, loss=2.5979769229888916
I0207 11:18:12.834645 140051089139456 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.631226122379303, loss=2.6222684383392334
I0207 11:18:47.364646 140051097532160 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.6382873058319092, loss=2.6393258571624756
I0207 11:19:21.926583 140051089139456 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.6236562728881836, loss=2.6118545532226562
I0207 11:19:56.495120 140051097532160 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6332106590270996, loss=2.602625608444214
I0207 11:20:31.052785 140051089139456 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.6271249651908875, loss=2.6586055755615234
I0207 11:21:05.611839 140051097532160 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.6344467401504517, loss=2.6701138019561768
I0207 11:21:40.154574 140051089139456 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.6726683974266052, loss=2.6624183654785156
I0207 11:22:14.659039 140051097532160 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.6362806558609009, loss=2.6874468326568604
I0207 11:22:49.221943 140051089139456 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.6393671631813049, loss=2.6600115299224854
I0207 11:23:23.753674 140051097532160 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.6490976810455322, loss=2.672389507293701
I0207 11:23:58.322718 140051089139456 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6491664052009583, loss=2.6363413333892822
I0207 11:24:32.872193 140051097532160 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.6294411420822144, loss=2.6464438438415527
I0207 11:25:07.370156 140051089139456 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.6775684952735901, loss=2.6517953872680664
I0207 11:25:41.925559 140051097532160 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6395487785339355, loss=2.612253189086914
I0207 11:26:16.456321 140051089139456 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.6549245119094849, loss=2.614459991455078
I0207 11:26:50.990594 140051097532160 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.638835072517395, loss=2.63468861579895
I0207 11:27:25.552023 140051089139456 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.6758112907409668, loss=2.559812068939209
I0207 11:28:00.114955 140051097532160 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6343034505844116, loss=2.6303484439849854
I0207 11:28:34.661788 140051089139456 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.6790726184844971, loss=2.6529757976531982
I0207 11:29:09.208679 140051097532160 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.6358240842819214, loss=2.5764174461364746
I0207 11:29:43.768750 140051089139456 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.6597468852996826, loss=2.625011682510376
I0207 11:29:44.875609 140225696298816 spec.py:321] Evaluating on the training split.
I0207 11:29:47.855677 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:32:49.351805 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 11:32:52.035410 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:35:18.843127 140225696298816 spec.py:349] Evaluating on the test split.
I0207 11:35:21.534411 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:37:37.096599 140225696298816 submission_runner.py:408] Time since start: 64949.67s, 	Step: 116805, 	{'train/accuracy': 0.6977561116218567, 'train/loss': 1.5053915977478027, 'train/bleu': 35.92062263456214, 'validation/accuracy': 0.6923906803131104, 'validation/loss': 1.5051144361495972, 'validation/bleu': 30.750445632725896, 'validation/num_examples': 3000, 'test/accuracy': 0.709511399269104, 'test/loss': 1.4227913618087769, 'test/bleu': 30.89250024411417, 'test/num_examples': 3003, 'score': 40366.93438744545, 'total_duration': 64949.66950559616, 'accumulated_submission_time': 40366.93438744545, 'accumulated_eval_time': 24577.193524599075, 'accumulated_logging_time': 1.8184754848480225}
I0207 11:37:37.128249 140051097532160 logging_writer.py:48] [116805] accumulated_eval_time=24577.193525, accumulated_logging_time=1.818475, accumulated_submission_time=40366.934387, global_step=116805, preemption_count=0, score=40366.934387, test/accuracy=0.709511, test/bleu=30.892500, test/loss=1.422791, test/num_examples=3003, total_duration=64949.669506, train/accuracy=0.697756, train/bleu=35.920623, train/loss=1.505392, validation/accuracy=0.692391, validation/bleu=30.750446, validation/loss=1.505114, validation/num_examples=3000
I0207 11:38:10.170635 140051089139456 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.6529588103294373, loss=2.618849515914917
I0207 11:38:44.648709 140051097532160 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.690907895565033, loss=2.660515069961548
I0207 11:39:19.178845 140051089139456 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.672748327255249, loss=2.6386687755584717
I0207 11:39:53.705824 140051097532160 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.6991690993309021, loss=2.630927324295044
I0207 11:40:28.247216 140051089139456 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.6461209058761597, loss=2.6599483489990234
I0207 11:41:02.771204 140051097532160 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.6644579768180847, loss=2.665252923965454
I0207 11:41:37.299627 140051089139456 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6708515882492065, loss=2.689039707183838
I0207 11:42:11.851246 140051097532160 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.6525563597679138, loss=2.579282522201538
I0207 11:42:46.386000 140051089139456 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.6452760100364685, loss=2.5887129306793213
I0207 11:43:20.923121 140051097532160 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.6377007961273193, loss=2.612837791442871
I0207 11:43:55.447036 140051089139456 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.6725938320159912, loss=2.6169345378875732
I0207 11:44:29.959109 140051097532160 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.6412981152534485, loss=2.545173406600952
I0207 11:45:04.480349 140051089139456 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.691112220287323, loss=2.6381189823150635
I0207 11:45:39.006041 140051097532160 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.682032585144043, loss=2.594902276992798
I0207 11:46:13.556573 140051089139456 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.6475033164024353, loss=2.6487512588500977
I0207 11:46:48.091152 140051097532160 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.6864310503005981, loss=2.626645803451538
I0207 11:47:22.641379 140051089139456 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6457895040512085, loss=2.635449171066284
I0207 11:47:57.192203 140051097532160 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.6448855996131897, loss=2.6013200283050537
I0207 11:48:31.733488 140051089139456 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.6495040059089661, loss=2.674482583999634
I0207 11:49:06.299278 140051097532160 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.674782395362854, loss=2.6011807918548584
I0207 11:49:40.859952 140051089139456 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.676110565662384, loss=2.5798752307891846
I0207 11:50:15.419204 140051097532160 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.6886612176895142, loss=2.674837112426758
I0207 11:50:49.963111 140051089139456 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.6723477244377136, loss=2.6079275608062744
I0207 11:51:24.513402 140051097532160 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.6950052380561829, loss=2.7010884284973145
I0207 11:51:37.359794 140225696298816 spec.py:321] Evaluating on the training split.
I0207 11:51:40.335516 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:54:40.259540 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 11:54:42.938923 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:57:10.004232 140225696298816 spec.py:349] Evaluating on the test split.
I0207 11:57:12.695913 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 11:59:34.440574 140225696298816 submission_runner.py:408] Time since start: 66267.01s, 	Step: 119239, 	{'train/accuracy': 0.7087553143501282, 'train/loss': 1.4513678550720215, 'train/bleu': 36.904724728202694, 'validation/accuracy': 0.6937917470932007, 'validation/loss': 1.5004736185073853, 'validation/bleu': 31.007339831048924, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.4181479215621948, 'test/bleu': 30.699474406255312, 'test/num_examples': 3003, 'score': 41207.07846236229, 'total_duration': 66267.013463974, 'accumulated_submission_time': 41207.07846236229, 'accumulated_eval_time': 25054.274214982986, 'accumulated_logging_time': 1.8607072830200195}
I0207 11:59:34.471753 140051089139456 logging_writer.py:48] [119239] accumulated_eval_time=25054.274215, accumulated_logging_time=1.860707, accumulated_submission_time=41207.078462, global_step=119239, preemption_count=0, score=41207.078462, test/accuracy=0.709918, test/bleu=30.699474, test/loss=1.418148, test/num_examples=3003, total_duration=66267.013464, train/accuracy=0.708755, train/bleu=36.904725, train/loss=1.451368, validation/accuracy=0.693792, validation/bleu=31.007340, validation/loss=1.500474, validation/num_examples=3000
I0207 11:59:55.825237 140051097532160 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.6739965677261353, loss=2.603576183319092
I0207 12:00:30.253737 140051089139456 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.6880154013633728, loss=2.63862943649292
I0207 12:01:04.751254 140051097532160 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.6708542108535767, loss=2.6291441917419434
I0207 12:01:39.286711 140051089139456 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.6602882146835327, loss=2.630495309829712
I0207 12:02:13.805675 140051097532160 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.6564398407936096, loss=2.6376535892486572
I0207 12:02:48.362991 140051089139456 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.686908483505249, loss=2.5624074935913086
I0207 12:03:22.931818 140051097532160 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.6714250445365906, loss=2.624734401702881
I0207 12:03:57.499741 140051089139456 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.6844337582588196, loss=2.602613925933838
I0207 12:04:32.041288 140051097532160 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.686063289642334, loss=2.639380931854248
I0207 12:05:06.609115 140051089139456 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.6920471787452698, loss=2.594106674194336
I0207 12:05:41.141656 140051097532160 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.704213559627533, loss=2.6610467433929443
I0207 12:06:15.699182 140051089139456 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.6860247850418091, loss=2.659654140472412
I0207 12:06:50.228838 140051097532160 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.6723640561103821, loss=2.6240572929382324
I0207 12:07:24.783834 140051089139456 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.6829601526260376, loss=2.678722381591797
I0207 12:07:59.333187 140051097532160 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.6844714283943176, loss=2.599471092224121
I0207 12:08:33.897582 140051089139456 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7044480443000793, loss=2.605909585952759
I0207 12:09:08.441278 140051097532160 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7200672626495361, loss=2.6297526359558105
I0207 12:09:42.982971 140051089139456 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7024426460266113, loss=2.5620880126953125
I0207 12:10:17.521611 140051097532160 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7088770270347595, loss=2.5896778106689453
I0207 12:10:52.050259 140051089139456 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.6858606338500977, loss=2.6247315406799316
I0207 12:11:26.596372 140051097532160 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.6849691867828369, loss=2.5449440479278564
I0207 12:12:01.145028 140051089139456 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.710462749004364, loss=2.597670316696167
I0207 12:12:35.700664 140051097532160 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.699748158454895, loss=2.6090943813323975
I0207 12:13:10.244410 140051089139456 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.6812388300895691, loss=2.5229129791259766
I0207 12:13:34.495718 140225696298816 spec.py:321] Evaluating on the training split.
I0207 12:13:37.475189 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:16:34.521594 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 12:16:37.210178 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:19:05.418260 140225696298816 spec.py:349] Evaluating on the test split.
I0207 12:19:08.116419 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:21:23.035688 140225696298816 submission_runner.py:408] Time since start: 67575.61s, 	Step: 121672, 	{'train/accuracy': 0.7050728797912598, 'train/loss': 1.4681651592254639, 'train/bleu': 36.167227486174745, 'validation/accuracy': 0.6942753195762634, 'validation/loss': 1.498553991317749, 'validation/bleu': 30.98735750993361, 'validation/num_examples': 3000, 'test/accuracy': 0.7114868760108948, 'test/loss': 1.4147039651870728, 'test/bleu': 30.876806630830238, 'test/num_examples': 3003, 'score': 42047.01487851143, 'total_duration': 67575.60860204697, 'accumulated_submission_time': 42047.01487851143, 'accumulated_eval_time': 25522.814121484756, 'accumulated_logging_time': 1.9024908542633057}
I0207 12:21:23.066635 140051097532160 logging_writer.py:48] [121672] accumulated_eval_time=25522.814121, accumulated_logging_time=1.902491, accumulated_submission_time=42047.014879, global_step=121672, preemption_count=0, score=42047.014879, test/accuracy=0.711487, test/bleu=30.876807, test/loss=1.414704, test/num_examples=3003, total_duration=67575.608602, train/accuracy=0.705073, train/bleu=36.167227, train/loss=1.468165, validation/accuracy=0.694275, validation/bleu=30.987358, validation/loss=1.498554, validation/num_examples=3000
I0207 12:21:33.110803 140051089139456 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.6973122358322144, loss=2.6310386657714844
I0207 12:22:07.557544 140051097532160 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7313238978385925, loss=2.673306941986084
I0207 12:22:42.105190 140051089139456 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.6884087324142456, loss=2.633295774459839
I0207 12:23:16.662949 140051097532160 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.690553605556488, loss=2.5795958042144775
I0207 12:23:51.221119 140051089139456 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7305984497070312, loss=2.5381839275360107
I0207 12:24:25.780939 140051097532160 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.6862499713897705, loss=2.633375406265259
I0207 12:25:00.328142 140051089139456 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7069116234779358, loss=2.5793511867523193
I0207 12:25:34.870076 140051097532160 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.7327781319618225, loss=2.563971996307373
I0207 12:26:09.422586 140051089139456 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.6726527214050293, loss=2.5717391967773438
I0207 12:26:43.970127 140051097532160 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.6864825487136841, loss=2.603792905807495
I0207 12:27:18.552041 140051089139456 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7027944326400757, loss=2.613999128341675
I0207 12:27:53.107261 140051097532160 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7021063566207886, loss=2.6393039226531982
I0207 12:28:27.654673 140051089139456 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7271344661712646, loss=2.5847156047821045
I0207 12:29:02.218672 140051097532160 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7330290079116821, loss=2.656614303588867
I0207 12:29:36.781498 140051089139456 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.7083380818367004, loss=2.6110053062438965
I0207 12:30:11.360867 140051097532160 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7091616988182068, loss=2.5521581172943115
I0207 12:30:45.936631 140051089139456 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7198694348335266, loss=2.6075403690338135
I0207 12:31:20.496034 140051097532160 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.7233878970146179, loss=2.5996735095977783
I0207 12:31:55.043201 140051089139456 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.7500420808792114, loss=2.572167158126831
I0207 12:32:29.612675 140051097532160 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7129775285720825, loss=2.6490020751953125
I0207 12:33:04.173411 140051089139456 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7187926769256592, loss=2.599327802658081
I0207 12:33:38.730862 140051097532160 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.6805658340454102, loss=2.5752599239349365
I0207 12:34:13.293641 140051089139456 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7193605303764343, loss=2.5951831340789795
I0207 12:34:47.841451 140051097532160 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.6976693272590637, loss=2.540911912918091
I0207 12:35:22.386868 140051089139456 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.756580650806427, loss=2.563894271850586
I0207 12:35:23.146682 140225696298816 spec.py:321] Evaluating on the training split.
I0207 12:35:26.119212 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:38:26.481117 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 12:38:29.150798 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:40:55.009898 140225696298816 spec.py:349] Evaluating on the test split.
I0207 12:40:57.695095 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 12:43:09.437099 140225696298816 submission_runner.py:408] Time since start: 68882.01s, 	Step: 124104, 	{'train/accuracy': 0.7041686773300171, 'train/loss': 1.4648628234863281, 'train/bleu': 36.28302984249575, 'validation/accuracy': 0.6946101188659668, 'validation/loss': 1.495320200920105, 'validation/bleu': 30.891498149474604, 'validation/num_examples': 3000, 'test/accuracy': 0.7113357782363892, 'test/loss': 1.4104185104370117, 'test/bleu': 30.965012996729225, 'test/num_examples': 3003, 'score': 42886.95123958588, 'total_duration': 68882.01002573967, 'accumulated_submission_time': 42886.95123958588, 'accumulated_eval_time': 25989.104485034943, 'accumulated_logging_time': 1.9998183250427246}
I0207 12:43:09.469564 140051097532160 logging_writer.py:48] [124104] accumulated_eval_time=25989.104485, accumulated_logging_time=1.999818, accumulated_submission_time=42886.951240, global_step=124104, preemption_count=0, score=42886.951240, test/accuracy=0.711336, test/bleu=30.965013, test/loss=1.410419, test/num_examples=3003, total_duration=68882.010026, train/accuracy=0.704169, train/bleu=36.283030, train/loss=1.464863, validation/accuracy=0.694610, validation/bleu=30.891498, validation/loss=1.495320, validation/num_examples=3000
I0207 12:43:42.857046 140051089139456 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7009513974189758, loss=2.5893023014068604
I0207 12:44:17.352598 140051097532160 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7407253980636597, loss=2.554492950439453
I0207 12:44:51.893050 140051089139456 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7415489554405212, loss=2.623568058013916
I0207 12:45:26.417360 140051097532160 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7094422578811646, loss=2.6269729137420654
I0207 12:46:00.937062 140051089139456 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.6983222961425781, loss=2.6159119606018066
I0207 12:46:35.468476 140051097532160 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7237215638160706, loss=2.5939700603485107
I0207 12:47:10.024702 140051089139456 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7458479404449463, loss=2.625483274459839
I0207 12:47:44.565313 140051097532160 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.6977002620697021, loss=2.6089694499969482
I0207 12:48:19.145283 140051089139456 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.6851500272750854, loss=2.6069281101226807
I0207 12:48:53.677608 140051097532160 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7337724566459656, loss=2.584737539291382
I0207 12:49:28.247300 140051089139456 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.7160907983779907, loss=2.549089193344116
I0207 12:50:02.798723 140051097532160 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.6998620629310608, loss=2.5817058086395264
I0207 12:50:37.344057 140051089139456 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.7236822247505188, loss=2.664994478225708
I0207 12:51:11.907489 140051097532160 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7244582176208496, loss=2.606231689453125
I0207 12:51:46.473088 140051089139456 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.716350257396698, loss=2.6291239261627197
I0207 12:52:21.019617 140051097532160 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7170125842094421, loss=2.575009346008301
I0207 12:52:55.570660 140051089139456 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.703920841217041, loss=2.6168243885040283
I0207 12:53:30.115289 140051097532160 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7251071333885193, loss=2.592714309692383
I0207 12:54:04.671306 140051089139456 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7204409241676331, loss=2.59682035446167
I0207 12:54:39.197066 140051097532160 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.6910526752471924, loss=2.6141176223754883
I0207 12:55:13.746784 140051089139456 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.7366061210632324, loss=2.558809280395508
I0207 12:55:48.296554 140051097532160 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7461549043655396, loss=2.6873481273651123
I0207 12:56:22.856595 140051089139456 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.7339336276054382, loss=2.6066172122955322
I0207 12:56:57.404270 140051097532160 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7043538689613342, loss=2.5871362686157227
I0207 12:57:09.570238 140225696298816 spec.py:321] Evaluating on the training split.
I0207 12:57:12.546289 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:00:06.638051 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 13:00:09.314258 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:02:46.655780 140225696298816 spec.py:349] Evaluating on the test split.
I0207 13:02:49.326773 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:05:05.882388 140225696298816 submission_runner.py:408] Time since start: 70198.46s, 	Step: 126537, 	{'train/accuracy': 0.7092657685279846, 'train/loss': 1.449809193611145, 'train/bleu': 36.99707375984359, 'validation/accuracy': 0.693990170955658, 'validation/loss': 1.496898889541626, 'validation/bleu': 30.973520390291938, 'validation/num_examples': 3000, 'test/accuracy': 0.7120446562767029, 'test/loss': 1.4122170209884644, 'test/bleu': 31.058713552731536, 'test/num_examples': 3003, 'score': 43726.96578860283, 'total_duration': 70198.45528745651, 'accumulated_submission_time': 43726.96578860283, 'accumulated_eval_time': 26465.41655921936, 'accumulated_logging_time': 2.0420727729797363}
I0207 13:05:05.913334 140051089139456 logging_writer.py:48] [126537] accumulated_eval_time=26465.416559, accumulated_logging_time=2.042073, accumulated_submission_time=43726.965789, global_step=126537, preemption_count=0, score=43726.965789, test/accuracy=0.712045, test/bleu=31.058714, test/loss=1.412217, test/num_examples=3003, total_duration=70198.455287, train/accuracy=0.709266, train/bleu=36.997074, train/loss=1.449809, validation/accuracy=0.693990, validation/bleu=30.973520, validation/loss=1.496899, validation/num_examples=3000
I0207 13:05:27.935246 140051097532160 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.7044247984886169, loss=2.5299489498138428
I0207 13:06:02.375501 140051089139456 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7175963521003723, loss=2.52301025390625
I0207 13:06:36.892447 140051097532160 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.7509532570838928, loss=2.581852436065674
I0207 13:07:11.442728 140051089139456 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.700844407081604, loss=2.5416665077209473
I0207 13:07:45.994399 140051097532160 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7251836061477661, loss=2.53473162651062
I0207 13:08:20.521045 140051089139456 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.7263774871826172, loss=2.5605170726776123
I0207 13:08:55.064519 140051097532160 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.6980978846549988, loss=2.546055316925049
I0207 13:09:29.592152 140051089139456 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7139922976493835, loss=2.607943534851074
I0207 13:10:04.122547 140051097532160 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.754643440246582, loss=2.535024642944336
I0207 13:10:38.668682 140051089139456 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.7249082922935486, loss=2.6349523067474365
I0207 13:11:13.190610 140051097532160 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7249496579170227, loss=2.575395107269287
I0207 13:11:47.740694 140051089139456 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.7267903089523315, loss=2.60296893119812
I0207 13:12:22.290728 140051097532160 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.7361112833023071, loss=2.574746608734131
I0207 13:12:56.858399 140051089139456 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.7185920476913452, loss=2.5611562728881836
I0207 13:13:31.395241 140051097532160 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7203961610794067, loss=2.5974340438842773
I0207 13:14:05.935455 140051089139456 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.7430495619773865, loss=2.6214935779571533
I0207 13:14:40.489456 140051097532160 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.6968648433685303, loss=2.6096882820129395
I0207 13:15:15.039647 140051089139456 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7130118608474731, loss=2.6266963481903076
I0207 13:15:49.596343 140051097532160 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.72141033411026, loss=2.569096326828003
I0207 13:16:24.150279 140051089139456 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.723522424697876, loss=2.583083391189575
I0207 13:16:58.719227 140051097532160 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7256460785865784, loss=2.6088192462921143
I0207 13:17:33.267261 140051089139456 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.7293705344200134, loss=2.6008036136627197
I0207 13:18:07.826586 140051097532160 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.7286292910575867, loss=2.610623598098755
I0207 13:18:42.385844 140051089139456 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.6925602555274963, loss=2.5920064449310303
I0207 13:19:05.960528 140225696298816 spec.py:321] Evaluating on the training split.
I0207 13:19:08.937575 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:21:58.382086 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 13:22:01.054716 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:24:24.183833 140225696298816 spec.py:349] Evaluating on the test split.
I0207 13:24:26.870904 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:26:46.625718 140225696298816 submission_runner.py:408] Time since start: 71499.20s, 	Step: 128970, 	{'train/accuracy': 0.7105950117111206, 'train/loss': 1.435489535331726, 'train/bleu': 37.02532880316543, 'validation/accuracy': 0.6939777731895447, 'validation/loss': 1.4965522289276123, 'validation/bleu': 31.032989289148937, 'validation/num_examples': 3000, 'test/accuracy': 0.7122886776924133, 'test/loss': 1.4110324382781982, 'test/bleu': 30.867242538049812, 'test/num_examples': 3003, 'score': 44566.927010297775, 'total_duration': 71499.1986413002, 'accumulated_submission_time': 44566.927010297775, 'accumulated_eval_time': 26926.081700086594, 'accumulated_logging_time': 2.082228422164917}
I0207 13:26:46.657413 140051097532160 logging_writer.py:48] [128970] accumulated_eval_time=26926.081700, accumulated_logging_time=2.082228, accumulated_submission_time=44566.927010, global_step=128970, preemption_count=0, score=44566.927010, test/accuracy=0.712289, test/bleu=30.867243, test/loss=1.411032, test/num_examples=3003, total_duration=71499.198641, train/accuracy=0.710595, train/bleu=37.025329, train/loss=1.435490, validation/accuracy=0.693978, validation/bleu=31.032989, validation/loss=1.496552, validation/num_examples=3000
I0207 13:26:57.343054 140051089139456 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.7043309211730957, loss=2.5967860221862793
I0207 13:27:31.762322 140051097532160 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7227213382720947, loss=2.5487759113311768
I0207 13:28:06.237416 140051089139456 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.7111285328865051, loss=2.590703010559082
I0207 13:28:40.787031 140051097532160 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7057814598083496, loss=2.583620309829712
I0207 13:29:15.337657 140051089139456 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.7295340299606323, loss=2.5612359046936035
I0207 13:29:49.886094 140051097532160 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.701585590839386, loss=2.5513975620269775
I0207 13:30:24.417691 140051089139456 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7011417746543884, loss=2.5628840923309326
I0207 13:30:58.954143 140051097532160 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.717918336391449, loss=2.6004836559295654
I0207 13:31:33.526304 140051089139456 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7195572257041931, loss=2.5511388778686523
I0207 13:32:08.068394 140051097532160 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7134260535240173, loss=2.6261072158813477
I0207 13:32:42.631572 140051089139456 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7404059767723083, loss=2.5305325984954834
I0207 13:33:17.180735 140051097532160 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7244907021522522, loss=2.599726676940918
I0207 13:33:51.730201 140051089139456 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.7316238880157471, loss=2.608616828918457
I0207 13:34:26.303358 140051097532160 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.7044088244438171, loss=2.6391713619232178
I0207 13:35:00.862359 140051089139456 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7065278887748718, loss=2.5938711166381836
I0207 13:35:35.415440 140051097532160 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7441197037696838, loss=2.5941340923309326
I0207 13:36:09.980222 140051089139456 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.710324764251709, loss=2.587894916534424
I0207 13:36:44.537333 140051097532160 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7243642210960388, loss=2.5740766525268555
I0207 13:37:19.100426 140051089139456 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7198612689971924, loss=2.6283161640167236
I0207 13:37:53.643779 140051097532160 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.6948870420455933, loss=2.5509657859802246
I0207 13:38:28.201391 140051089139456 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7256913781166077, loss=2.5427451133728027
I0207 13:39:02.755764 140051097532160 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7117926478385925, loss=2.613377332687378
I0207 13:39:37.305525 140051089139456 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.7265132665634155, loss=2.618680477142334
I0207 13:40:11.859787 140051097532160 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.754710853099823, loss=2.6463851928710938
I0207 13:40:46.427741 140051089139456 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.7130411863327026, loss=2.5711915493011475
I0207 13:40:46.843364 140225696298816 spec.py:321] Evaluating on the training split.
I0207 13:40:49.807546 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:43:41.355311 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 13:43:44.036292 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:46:15.278482 140225696298816 spec.py:349] Evaluating on the test split.
I0207 13:46:17.965555 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 13:48:33.185348 140225696298816 submission_runner.py:408] Time since start: 72805.76s, 	Step: 131403, 	{'train/accuracy': 0.705418586730957, 'train/loss': 1.4653115272521973, 'train/bleu': 36.74133977241949, 'validation/accuracy': 0.6943249106407166, 'validation/loss': 1.496314287185669, 'validation/bleu': 30.95072385384553, 'validation/num_examples': 3000, 'test/accuracy': 0.7123119235038757, 'test/loss': 1.4104419946670532, 'test/bleu': 30.963210714876272, 'test/num_examples': 3003, 'score': 45407.02690410614, 'total_duration': 72805.75824856758, 'accumulated_submission_time': 45407.02690410614, 'accumulated_eval_time': 27392.42360830307, 'accumulated_logging_time': 2.123875141143799}
I0207 13:48:33.218427 140051097532160 logging_writer.py:48] [131403] accumulated_eval_time=27392.423608, accumulated_logging_time=2.123875, accumulated_submission_time=45407.026904, global_step=131403, preemption_count=0, score=45407.026904, test/accuracy=0.712312, test/bleu=30.963211, test/loss=1.410442, test/num_examples=3003, total_duration=72805.758249, train/accuracy=0.705419, train/bleu=36.741340, train/loss=1.465312, validation/accuracy=0.694325, validation/bleu=30.950724, validation/loss=1.496314, validation/num_examples=3000
I0207 13:49:06.943355 140051089139456 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7207009196281433, loss=2.5752391815185547
I0207 13:49:41.391349 140051097532160 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7319161891937256, loss=2.5869524478912354
I0207 13:50:15.939506 140051089139456 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.7350637912750244, loss=2.6044681072235107
I0207 13:50:50.495923 140051097532160 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7153911590576172, loss=2.602928400039673
I0207 13:51:25.033710 140051089139456 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.7140504717826843, loss=2.5317962169647217
I0207 13:51:59.563618 140051097532160 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7297699451446533, loss=2.615724563598633
I0207 13:52:34.132452 140051089139456 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.706646740436554, loss=2.524512767791748
I0207 13:53:08.690216 140051097532160 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.7107191681861877, loss=2.5941383838653564
I0207 13:53:43.239446 140051089139456 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.7324047684669495, loss=2.5891454219818115
I0207 13:54:17.784249 140051097532160 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.7631850838661194, loss=2.631664276123047
I0207 13:54:52.317065 140051089139456 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.701483428478241, loss=2.590583562850952
I0207 13:55:26.870491 140051097532160 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7269987463951111, loss=2.564854621887207
I0207 13:56:01.429071 140051089139456 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7327249050140381, loss=2.6126697063446045
I0207 13:56:35.996714 140051097532160 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7391003370285034, loss=2.6331663131713867
I0207 13:57:10.546885 140051089139456 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.7103296518325806, loss=2.6506166458129883
I0207 13:57:45.091752 140051097532160 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.6980224251747131, loss=2.617025136947632
I0207 13:58:19.606588 140051089139456 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.71815425157547, loss=2.5595319271087646
I0207 13:58:54.145864 140051097532160 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.7102925777435303, loss=2.5731894969940186
I0207 13:59:28.680345 140051089139456 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7179195284843445, loss=2.6467325687408447
I0207 13:59:39.456759 140225696298816 spec.py:321] Evaluating on the training split.
I0207 13:59:42.432834 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:02:43.375986 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 14:02:46.036443 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:05:18.118537 140225696298816 spec.py:349] Evaluating on the test split.
I0207 14:05:20.804318 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:07:36.838581 140225696298816 submission_runner.py:408] Time since start: 73949.41s, 	Step: 133333, 	{'train/accuracy': 0.7103955149650574, 'train/loss': 1.443301796913147, 'train/bleu': 36.525395183981935, 'validation/accuracy': 0.6940893530845642, 'validation/loss': 1.4960108995437622, 'validation/bleu': 30.98135216301977, 'validation/num_examples': 3000, 'test/accuracy': 0.7121259570121765, 'test/loss': 1.4103668928146362, 'test/bleu': 30.99405716021917, 'test/num_examples': 3003, 'score': 46073.19484710693, 'total_duration': 73949.41149926186, 'accumulated_submission_time': 46073.19484710693, 'accumulated_eval_time': 27869.805367946625, 'accumulated_logging_time': 2.1672544479370117}
I0207 14:07:36.871037 140051097532160 logging_writer.py:48] [133333] accumulated_eval_time=27869.805368, accumulated_logging_time=2.167254, accumulated_submission_time=46073.194847, global_step=133333, preemption_count=0, score=46073.194847, test/accuracy=0.712126, test/bleu=30.994057, test/loss=1.410367, test/num_examples=3003, total_duration=73949.411499, train/accuracy=0.710396, train/bleu=36.525395, train/loss=1.443302, validation/accuracy=0.694089, validation/bleu=30.981352, validation/loss=1.496011, validation/num_examples=3000
I0207 14:07:36.902306 140051089139456 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46073.194847
I0207 14:07:38.081550 140225696298816 checkpoints.py:490] Saving checkpoint at step: 133333
I0207 14:07:42.100657 140225696298816 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1/checkpoint_133333
I0207 14:07:42.105750 140225696298816 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_1/checkpoint_133333.
I0207 14:07:42.146543 140225696298816 submission_runner.py:583] Tuning trial 1/5
I0207 14:07:42.146691 140225696298816 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0207 14:07:42.152153 140225696298816 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006987399538047612, 'train/loss': 11.025596618652344, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 43.30344867706299, 'total_duration': 922.7244355678558, 'accumulated_submission_time': 43.30344867706299, 'accumulated_eval_time': 879.4209234714508, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2430, {'train/accuracy': 0.41607430577278137, 'train/loss': 3.9471724033355713, 'train/bleu': 14.572851959066698, 'validation/accuracy': 0.3996106684207916, 'validation/loss': 4.088059425354004, 'validation/bleu': 9.879816759300747, 'validation/num_examples': 3000, 'test/accuracy': 0.3871826231479645, 'test/loss': 4.267494201660156, 'test/bleu': 8.292150497981243, 'test/num_examples': 3003, 'score': 883.4170672893524, 'total_duration': 2314.9411220550537, 'accumulated_submission_time': 883.4170672893524, 'accumulated_eval_time': 1431.4115691184998, 'accumulated_logging_time': 0.03201866149902344, 'global_step': 2430, 'preemption_count': 0}), (4858, {'train/accuracy': 0.5455783009529114, 'train/loss': 2.7186343669891357, 'train/bleu': 24.371344575563036, 'validation/accuracy': 0.5439361929893494, 'validation/loss': 2.697274923324585, 'validation/bleu': 20.40709098587339, 'validation/num_examples': 3000, 'test/accuracy': 0.5444541573524475, 'test/loss': 2.736154556274414, 'test/bleu': 19.09181470174716, 'test/num_examples': 3003, 'score': 1723.4967126846313, 'total_duration': 3641.679195165634, 'accumulated_submission_time': 1723.4967126846313, 'accumulated_eval_time': 1917.9620339870453, 'accumulated_logging_time': 0.062225341796875, 'global_step': 4858, 'preemption_count': 0}), (7287, {'train/accuracy': 0.582105815410614, 'train/loss': 2.346024751663208, 'train/bleu': 27.688640848896345, 'validation/accuracy': 0.5865395069122314, 'validation/loss': 2.3033597469329834, 'validation/bleu': 23.467709729191526, 'validation/num_examples': 3000, 'test/accuracy': 0.5892278552055359, 'test/loss': 2.3010103702545166, 'test/bleu': 22.251105801748317, 'test/num_examples': 3003, 'score': 2563.5470848083496, 'total_duration': 4949.287957668304, 'accumulated_submission_time': 2563.5470848083496, 'accumulated_eval_time': 2385.4128901958466, 'accumulated_logging_time': 0.0925908088684082, 'global_step': 7287, 'preemption_count': 0}), (9718, {'train/accuracy': 0.59144127368927, 'train/loss': 2.244596481323242, 'train/bleu': 27.976897526423063, 'validation/accuracy': 0.6069732308387756, 'validation/loss': 2.119935989379883, 'validation/bleu': 24.509640670153523, 'validation/num_examples': 3000, 'test/accuracy': 0.6132822036743164, 'test/loss': 2.0930325984954834, 'test/bleu': 23.29025859698584, 'test/num_examples': 3003, 'score': 3403.686936378479, 'total_duration': 6229.200881242752, 'accumulated_submission_time': 3403.686936378479, 'accumulated_eval_time': 2825.075345516205, 'accumulated_logging_time': 0.12424159049987793, 'global_step': 9718, 'preemption_count': 0}), (12150, {'train/accuracy': 0.6025044322013855, 'train/loss': 2.141352653503418, 'train/bleu': 28.804964846373508, 'validation/accuracy': 0.6188392043113708, 'validation/loss': 2.0071840286254883, 'validation/bleu': 25.295877114636344, 'validation/num_examples': 3000, 'test/accuracy': 0.6300621628761292, 'test/loss': 1.9602621793746948, 'test/bleu': 24.743552012654888, 'test/num_examples': 3003, 'score': 4243.90465593338, 'total_duration': 7700.897699356079, 'accumulated_submission_time': 4243.90465593338, 'accumulated_eval_time': 3456.4474980831146, 'accumulated_logging_time': 0.1512916088104248, 'global_step': 12150, 'preemption_count': 0}), (14582, {'train/accuracy': 0.6175777316093445, 'train/loss': 2.0234029293060303, 'train/bleu': 29.494188386708036, 'validation/accuracy': 0.6317466497421265, 'validation/loss': 1.9158974885940552, 'validation/bleu': 26.446092461664627, 'validation/num_examples': 3000, 'test/accuracy': 0.6397652626037598, 'test/loss': 1.8675507307052612, 'test/bleu': 25.720613656901794, 'test/num_examples': 3003, 'score': 5084.021989107132, 'total_duration': 8983.16919708252, 'accumulated_submission_time': 5084.021989107132, 'accumulated_eval_time': 3898.492534637451, 'accumulated_logging_time': 0.18063759803771973, 'global_step': 14582, 'preemption_count': 0}), (17013, {'train/accuracy': 0.6190584897994995, 'train/loss': 2.003643274307251, 'train/bleu': 30.250872839385124, 'validation/accuracy': 0.6397812962532043, 'validation/loss': 1.8474228382110596, 'validation/bleu': 26.92715774325129, 'validation/num_examples': 3000, 'test/accuracy': 0.6494567394256592, 'test/loss': 1.7908827066421509, 'test/bleu': 26.089327875831792, 'test/num_examples': 3003, 'score': 5923.943639755249, 'total_duration': 10306.980433225632, 'accumulated_submission_time': 5923.943639755249, 'accumulated_eval_time': 4382.143723726273, 'accumulated_logging_time': 0.3407480716705322, 'global_step': 17013, 'preemption_count': 0}), (19445, {'train/accuracy': 0.6344646215438843, 'train/loss': 1.8915237188339233, 'train/bleu': 30.712155809537048, 'validation/accuracy': 0.6446169018745422, 'validation/loss': 1.809007167816162, 'validation/bleu': 27.5092599023563, 'validation/num_examples': 3000, 'test/accuracy': 0.6539306640625, 'test/loss': 1.760025978088379, 'test/bleu': 26.60863093886362, 'test/num_examples': 3003, 'score': 6764.000878095627, 'total_duration': 11614.296314954758, 'accumulated_submission_time': 6764.000878095627, 'accumulated_eval_time': 4849.296671628952, 'accumulated_logging_time': 0.36723852157592773, 'global_step': 19445, 'preemption_count': 0}), (21878, {'train/accuracy': 0.6279143691062927, 'train/loss': 1.938279628753662, 'train/bleu': 30.117385200900088, 'validation/accuracy': 0.6497625708580017, 'validation/loss': 1.7786868810653687, 'validation/bleu': 27.39135952713508, 'validation/num_examples': 3000, 'test/accuracy': 0.6575562357902527, 'test/loss': 1.7227576971054077, 'test/bleu': 26.950158208181367, 'test/num_examples': 3003, 'score': 7604.201534986496, 'total_duration': 12954.930808782578, 'accumulated_submission_time': 7604.201534986496, 'accumulated_eval_time': 5349.620749235153, 'accumulated_logging_time': 0.3979494571685791, 'global_step': 21878, 'preemption_count': 0}), (24311, {'train/accuracy': 0.6267665028572083, 'train/loss': 1.955460786819458, 'train/bleu': 30.495337151691416, 'validation/accuracy': 0.6528747081756592, 'validation/loss': 1.7616363763809204, 'validation/bleu': 27.740078969040887, 'validation/num_examples': 3000, 'test/accuracy': 0.6625530123710632, 'test/loss': 1.7036962509155273, 'test/bleu': 27.38874092003688, 'test/num_examples': 3003, 'score': 8444.363789081573, 'total_duration': 14485.660428762436, 'accumulated_submission_time': 8444.363789081573, 'accumulated_eval_time': 6040.07980966568, 'accumulated_logging_time': 0.4274477958679199, 'global_step': 24311, 'preemption_count': 0}), (26744, {'train/accuracy': 0.6343207955360413, 'train/loss': 1.8967632055282593, 'train/bleu': 31.050486129125893, 'validation/accuracy': 0.6534822583198547, 'validation/loss': 1.7492674589157104, 'validation/bleu': 28.098133062791273, 'validation/num_examples': 3000, 'test/accuracy': 0.6628900170326233, 'test/loss': 1.6931463479995728, 'test/bleu': 27.042964415040156, 'test/num_examples': 3003, 'score': 9284.414820194244, 'total_duration': 15802.381581544876, 'accumulated_submission_time': 9284.414820194244, 'accumulated_eval_time': 6516.639975786209, 'accumulated_logging_time': 0.45822668075561523, 'global_step': 26744, 'preemption_count': 0}), (29177, {'train/accuracy': 0.6336384415626526, 'train/loss': 1.8958899974822998, 'train/bleu': 30.684595268987056, 'validation/accuracy': 0.6565200686454773, 'validation/loss': 1.7404468059539795, 'validation/bleu': 28.038795719325957, 'validation/num_examples': 3000, 'test/accuracy': 0.665864884853363, 'test/loss': 1.6830744743347168, 'test/bleu': 27.865501179623493, 'test/num_examples': 3003, 'score': 10124.47589802742, 'total_duration': 17122.42004466057, 'accumulated_submission_time': 10124.47589802742, 'accumulated_eval_time': 6996.507694721222, 'accumulated_logging_time': 0.4879882335662842, 'global_step': 29177, 'preemption_count': 0}), (31610, {'train/accuracy': 0.6526271104812622, 'train/loss': 1.768532156944275, 'train/bleu': 32.05716202678791, 'validation/accuracy': 0.654350221157074, 'validation/loss': 1.736448884010315, 'validation/bleu': 28.03466477961463, 'validation/num_examples': 3000, 'test/accuracy': 0.6660043001174927, 'test/loss': 1.6788660287857056, 'test/bleu': 27.773628041650408, 'test/num_examples': 3003, 'score': 10964.547756910324, 'total_duration': 18423.887810707092, 'accumulated_submission_time': 10964.547756910324, 'accumulated_eval_time': 7457.794209480286, 'accumulated_logging_time': 0.5176031589508057, 'global_step': 31610, 'preemption_count': 0}), (34044, {'train/accuracy': 0.6363418102264404, 'train/loss': 1.877790093421936, 'train/bleu': 31.210927634435524, 'validation/accuracy': 0.65668123960495, 'validation/loss': 1.7198882102966309, 'validation/bleu': 28.09483148598837, 'validation/num_examples': 3000, 'test/accuracy': 0.6665620803833008, 'test/loss': 1.669747233390808, 'test/bleu': 27.553497282281434, 'test/num_examples': 3003, 'score': 11804.70926952362, 'total_duration': 19753.06885743141, 'accumulated_submission_time': 11804.70926952362, 'accumulated_eval_time': 7946.703389883041, 'accumulated_logging_time': 0.5480039119720459, 'global_step': 34044, 'preemption_count': 0}), (36478, {'train/accuracy': 0.6364160180091858, 'train/loss': 1.8722435235977173, 'train/bleu': 31.076741563059336, 'validation/accuracy': 0.6580327749252319, 'validation/loss': 1.717186689376831, 'validation/bleu': 28.03697177435267, 'validation/num_examples': 3000, 'test/accuracy': 0.6687583923339844, 'test/loss': 1.6572718620300293, 'test/bleu': 27.756396069290414, 'test/num_examples': 3003, 'score': 12644.779065132141, 'total_duration': 21185.581884860992, 'accumulated_submission_time': 12644.779065132141, 'accumulated_eval_time': 8539.038867473602, 'accumulated_logging_time': 0.5769636631011963, 'global_step': 36478, 'preemption_count': 0}), (38912, {'train/accuracy': 0.6482433080673218, 'train/loss': 1.7970041036605835, 'train/bleu': 31.432741998554356, 'validation/accuracy': 0.6609093546867371, 'validation/loss': 1.703112006187439, 'validation/bleu': 28.180620886314376, 'validation/num_examples': 3000, 'test/accuracy': 0.6716634631156921, 'test/loss': 1.6385266780853271, 'test/bleu': 27.945611634666342, 'test/num_examples': 3003, 'score': 13484.840069770813, 'total_duration': 22510.995681285858, 'accumulated_submission_time': 13484.840069770813, 'accumulated_eval_time': 9024.285813570023, 'accumulated_logging_time': 0.6064877510070801, 'global_step': 38912, 'preemption_count': 0}), (41346, {'train/accuracy': 0.6373471021652222, 'train/loss': 1.8696467876434326, 'train/bleu': 31.01315039572357, 'validation/accuracy': 0.6601405739784241, 'validation/loss': 1.7056689262390137, 'validation/bleu': 28.38573685693433, 'validation/num_examples': 3000, 'test/accuracy': 0.6728255152702332, 'test/loss': 1.63887357711792, 'test/bleu': 28.00362928869641, 'test/num_examples': 3003, 'score': 14324.897027015686, 'total_duration': 23825.52012705803, 'accumulated_submission_time': 14324.897027015686, 'accumulated_eval_time': 9498.646374464035, 'accumulated_logging_time': 0.6380147933959961, 'global_step': 41346, 'preemption_count': 0}), (43780, {'train/accuracy': 0.6370298266410828, 'train/loss': 1.8723466396331787, 'train/bleu': 31.06694249891112, 'validation/accuracy': 0.6600537896156311, 'validation/loss': 1.697940468788147, 'validation/bleu': 28.26108878119694, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.6407963037490845, 'test/bleu': 27.696134166557446, 'test/num_examples': 3003, 'score': 15164.826464653015, 'total_duration': 25156.252128362656, 'accumulated_submission_time': 15164.826464653015, 'accumulated_eval_time': 9989.341492176056, 'accumulated_logging_time': 0.668973445892334, 'global_step': 43780, 'preemption_count': 0}), (46214, {'train/accuracy': 0.6392388343811035, 'train/loss': 1.8472505807876587, 'train/bleu': 31.986452000965535, 'validation/accuracy': 0.6632279753684998, 'validation/loss': 1.6884485483169556, 'validation/bleu': 28.6184400125844, 'validation/num_examples': 3000, 'test/accuracy': 0.6738249063491821, 'test/loss': 1.6290220022201538, 'test/bleu': 28.05211040280357, 'test/num_examples': 3003, 'score': 16004.92531490326, 'total_duration': 26476.57047533989, 'accumulated_submission_time': 16004.92531490326, 'accumulated_eval_time': 10469.453252315521, 'accumulated_logging_time': 0.7002456188201904, 'global_step': 46214, 'preemption_count': 0}), (48648, {'train/accuracy': 0.6418137550354004, 'train/loss': 1.8364354372024536, 'train/bleu': 31.435799419526706, 'validation/accuracy': 0.662161648273468, 'validation/loss': 1.6864773035049438, 'validation/bleu': 28.773273751970383, 'validation/num_examples': 3000, 'test/accuracy': 0.6732555031776428, 'test/loss': 1.6297379732131958, 'test/bleu': 28.00563946047731, 'test/num_examples': 3003, 'score': 16845.00081062317, 'total_duration': 27799.22328066826, 'accumulated_submission_time': 16845.00081062317, 'accumulated_eval_time': 10951.921907424927, 'accumulated_logging_time': 0.731576681137085, 'global_step': 48648, 'preemption_count': 0}), (51082, {'train/accuracy': 0.6506011486053467, 'train/loss': 1.7793463468551636, 'train/bleu': 32.20130396977178, 'validation/accuracy': 0.6655961871147156, 'validation/loss': 1.6724234819412231, 'validation/bleu': 28.732523018514083, 'validation/num_examples': 3000, 'test/accuracy': 0.6755679845809937, 'test/loss': 1.6110336780548096, 'test/bleu': 28.39498522119112, 'test/num_examples': 3003, 'score': 17684.95172381401, 'total_duration': 29093.11604142189, 'accumulated_submission_time': 17684.95172381401, 'accumulated_eval_time': 11405.755249738693, 'accumulated_logging_time': 0.7633512020111084, 'global_step': 51082, 'preemption_count': 0}), (53516, {'train/accuracy': 0.6455166935920715, 'train/loss': 1.8138216733932495, 'train/bleu': 31.56522640495758, 'validation/accuracy': 0.6665757298469543, 'validation/loss': 1.6703606843948364, 'validation/bleu': 28.52078753000625, 'validation/num_examples': 3000, 'test/accuracy': 0.6770902276039124, 'test/loss': 1.6028481721878052, 'test/bleu': 28.199818862678665, 'test/num_examples': 3003, 'score': 18525.0443277359, 'total_duration': 30404.6956615448, 'accumulated_submission_time': 18525.0443277359, 'accumulated_eval_time': 11877.134405851364, 'accumulated_logging_time': 0.7947971820831299, 'global_step': 53516, 'preemption_count': 0}), (55951, {'train/accuracy': 0.6445412635803223, 'train/loss': 1.8259145021438599, 'train/bleu': 32.05229143406017, 'validation/accuracy': 0.6659185886383057, 'validation/loss': 1.664124608039856, 'validation/bleu': 28.706732866328736, 'validation/num_examples': 3000, 'test/accuracy': 0.680553138256073, 'test/loss': 1.5927239656448364, 'test/bleu': 28.54534410724413, 'test/num_examples': 3003, 'score': 19365.257098674774, 'total_duration': 31868.459720373154, 'accumulated_submission_time': 19365.257098674774, 'accumulated_eval_time': 12500.575400590897, 'accumulated_logging_time': 0.8282985687255859, 'global_step': 55951, 'preemption_count': 0}), (58385, {'train/accuracy': 0.6512637138366699, 'train/loss': 1.7649765014648438, 'train/bleu': 31.62150612676097, 'validation/accuracy': 0.6669105291366577, 'validation/loss': 1.6533396244049072, 'validation/bleu': 28.550633559064593, 'validation/num_examples': 3000, 'test/accuracy': 0.6782174110412598, 'test/loss': 1.5912344455718994, 'test/bleu': 28.35522166002915, 'test/num_examples': 3003, 'score': 20205.149728775024, 'total_duration': 33255.68974637985, 'accumulated_submission_time': 20205.149728775024, 'accumulated_eval_time': 13047.804039001465, 'accumulated_logging_time': 0.8610539436340332, 'global_step': 58385, 'preemption_count': 0}), (60820, {'train/accuracy': 0.6503833532333374, 'train/loss': 1.7853611707687378, 'train/bleu': 31.359232455254705, 'validation/accuracy': 0.6674684882164001, 'validation/loss': 1.655722975730896, 'validation/bleu': 28.792026358666575, 'validation/num_examples': 3000, 'test/accuracy': 0.6813201308250427, 'test/loss': 1.5846421718597412, 'test/bleu': 28.693769513099046, 'test/num_examples': 3003, 'score': 21045.221281528473, 'total_duration': 34562.92338228226, 'accumulated_submission_time': 21045.221281528473, 'accumulated_eval_time': 13514.856243610382, 'accumulated_logging_time': 0.8937373161315918, 'global_step': 60820, 'preemption_count': 0}), (63255, {'train/accuracy': 0.6639273166656494, 'train/loss': 1.6947698593139648, 'train/bleu': 32.86530003174533, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.639392375946045, 'validation/bleu': 29.130301380967907, 'validation/num_examples': 3000, 'test/accuracy': 0.6831212639808655, 'test/loss': 1.5691425800323486, 'test/bleu': 28.78299893054562, 'test/num_examples': 3003, 'score': 21885.19492340088, 'total_duration': 35868.71333575249, 'accumulated_submission_time': 21885.19492340088, 'accumulated_eval_time': 13980.560697555542, 'accumulated_logging_time': 0.9289267063140869, 'global_step': 63255, 'preemption_count': 0}), (65690, {'train/accuracy': 0.6493473649024963, 'train/loss': 1.7821170091629028, 'train/bleu': 32.37563384288451, 'validation/accuracy': 0.6715725660324097, 'validation/loss': 1.6345800161361694, 'validation/bleu': 29.14645826795015, 'validation/num_examples': 3000, 'test/accuracy': 0.6841439008712769, 'test/loss': 1.5664318799972534, 'test/bleu': 28.850066271740804, 'test/num_examples': 3003, 'score': 22725.288113832474, 'total_duration': 37216.40605187416, 'accumulated_submission_time': 22725.288113832474, 'accumulated_eval_time': 14488.048207998276, 'accumulated_logging_time': 0.964684009552002, 'global_step': 65690, 'preemption_count': 0}), (68125, {'train/accuracy': 0.6522968411445618, 'train/loss': 1.7712243795394897, 'train/bleu': 32.26716561149852, 'validation/accuracy': 0.6721181273460388, 'validation/loss': 1.6278233528137207, 'validation/bleu': 29.2856531151115, 'validation/num_examples': 3000, 'test/accuracy': 0.6857939958572388, 'test/loss': 1.5533937215805054, 'test/bleu': 29.1256183217246, 'test/num_examples': 3003, 'score': 23565.320879220963, 'total_duration': 38545.195959568024, 'accumulated_submission_time': 23565.320879220963, 'accumulated_eval_time': 14976.696114301682, 'accumulated_logging_time': 0.9985537528991699, 'global_step': 68125, 'preemption_count': 0}), (70560, {'train/accuracy': 0.6611246466636658, 'train/loss': 1.709555745124817, 'train/bleu': 32.33671503844988, 'validation/accuracy': 0.6735812425613403, 'validation/loss': 1.61667001247406, 'validation/bleu': 29.112008037255638, 'validation/num_examples': 3000, 'test/accuracy': 0.6852827072143555, 'test/loss': 1.5494496822357178, 'test/bleu': 28.897881717129575, 'test/num_examples': 3003, 'score': 24405.445092201233, 'total_duration': 39850.8931684494, 'accumulated_submission_time': 24405.445092201233, 'accumulated_eval_time': 15442.105145931244, 'accumulated_logging_time': 1.085925817489624, 'global_step': 70560, 'preemption_count': 0}), (72995, {'train/accuracy': 0.6553438901901245, 'train/loss': 1.7596919536590576, 'train/bleu': 32.79811780395135, 'validation/accuracy': 0.6746723651885986, 'validation/loss': 1.609011173248291, 'validation/bleu': 29.66018880236669, 'validation/num_examples': 3000, 'test/accuracy': 0.6878392100334167, 'test/loss': 1.5402271747589111, 'test/bleu': 29.309156302699094, 'test/num_examples': 3003, 'score': 25245.630289554596, 'total_duration': 41157.38154554367, 'accumulated_submission_time': 25245.630289554596, 'accumulated_eval_time': 15908.294090032578, 'accumulated_logging_time': 1.1230123043060303, 'global_step': 72995, 'preemption_count': 0}), (75429, {'train/accuracy': 0.6763533353805542, 'train/loss': 1.624076247215271, 'train/bleu': 33.937369926587024, 'validation/accuracy': 0.6763462424278259, 'validation/loss': 1.6011841297149658, 'validation/bleu': 29.693910913014506, 'validation/num_examples': 3000, 'test/accuracy': 0.689012885093689, 'test/loss': 1.5321506261825562, 'test/bleu': 29.094085815300794, 'test/num_examples': 3003, 'score': 26085.57997250557, 'total_duration': 42527.72531962395, 'accumulated_submission_time': 26085.57997250557, 'accumulated_eval_time': 16438.57457280159, 'accumulated_logging_time': 1.1592369079589844, 'global_step': 75429, 'preemption_count': 0}), (77864, {'train/accuracy': 0.6609399318695068, 'train/loss': 1.7068854570388794, 'train/bleu': 32.66516373016686, 'validation/accuracy': 0.6770405769348145, 'validation/loss': 1.597065806388855, 'validation/bleu': 29.539697532026157, 'validation/num_examples': 3000, 'test/accuracy': 0.6887107491493225, 'test/loss': 1.527599573135376, 'test/bleu': 29.207579730088618, 'test/num_examples': 3003, 'score': 26925.578989982605, 'total_duration': 43852.69458556175, 'accumulated_submission_time': 26925.578989982605, 'accumulated_eval_time': 16923.432988643646, 'accumulated_logging_time': 1.1936464309692383, 'global_step': 77864, 'preemption_count': 0}), (80299, {'train/accuracy': 0.6609601974487305, 'train/loss': 1.7184412479400635, 'train/bleu': 32.26415699149423, 'validation/accuracy': 0.6776357293128967, 'validation/loss': 1.5888279676437378, 'validation/bleu': 29.55108559881877, 'validation/num_examples': 3000, 'test/accuracy': 0.6894195675849915, 'test/loss': 1.5198761224746704, 'test/bleu': 29.1376581392568, 'test/num_examples': 3003, 'score': 27765.792411088943, 'total_duration': 45205.13687705994, 'accumulated_submission_time': 27765.792411088943, 'accumulated_eval_time': 17435.547990322113, 'accumulated_logging_time': 1.2304267883300781, 'global_step': 80299, 'preemption_count': 0}), (82733, {'train/accuracy': 0.6679980754852295, 'train/loss': 1.673125147819519, 'train/bleu': 33.5625012966058, 'validation/accuracy': 0.6785408854484558, 'validation/loss': 1.5870471000671387, 'validation/bleu': 29.696808866105183, 'validation/num_examples': 3000, 'test/accuracy': 0.6937772631645203, 'test/loss': 1.509999394416809, 'test/bleu': 29.743239862901277, 'test/num_examples': 3003, 'score': 28605.771606206894, 'total_duration': 46501.641139268875, 'accumulated_submission_time': 28605.771606206894, 'accumulated_eval_time': 17891.961584091187, 'accumulated_logging_time': 1.264892816543579, 'global_step': 82733, 'preemption_count': 0}), (85167, {'train/accuracy': 0.6636251211166382, 'train/loss': 1.700329303741455, 'train/bleu': 32.7424648321825, 'validation/accuracy': 0.6798551678657532, 'validation/loss': 1.578163981437683, 'validation/bleu': 29.852257792933663, 'validation/num_examples': 3000, 'test/accuracy': 0.6938701868057251, 'test/loss': 1.5030529499053955, 'test/bleu': 29.60024070059604, 'test/num_examples': 3003, 'score': 29445.863482236862, 'total_duration': 47847.549050569534, 'accumulated_submission_time': 29445.863482236862, 'accumulated_eval_time': 18397.664662599564, 'accumulated_logging_time': 1.301595687866211, 'global_step': 85167, 'preemption_count': 0}), (87601, {'train/accuracy': 0.7056272029876709, 'train/loss': 1.4632694721221924, 'train/bleu': 35.86558291442635, 'validation/accuracy': 0.6798055768013, 'validation/loss': 1.5734652280807495, 'validation/bleu': 29.873149366350557, 'validation/num_examples': 3000, 'test/accuracy': 0.6945558190345764, 'test/loss': 1.498718023300171, 'test/bleu': 29.48370587895101, 'test/num_examples': 3003, 'score': 30286.26316356659, 'total_duration': 49169.80411338806, 'accumulated_submission_time': 30286.26316356659, 'accumulated_eval_time': 18879.404970645905, 'accumulated_logging_time': 1.3394997119903564, 'global_step': 87601, 'preemption_count': 0}), (90035, {'train/accuracy': 0.6675459742546082, 'train/loss': 1.6747876405715942, 'train/bleu': 33.33141123987844, 'validation/accuracy': 0.6829301714897156, 'validation/loss': 1.564455509185791, 'validation/bleu': 29.92197180523269, 'validation/num_examples': 3000, 'test/accuracy': 0.6961826682090759, 'test/loss': 1.489499807357788, 'test/bleu': 29.9603429821149, 'test/num_examples': 3003, 'score': 31126.28993988037, 'total_duration': 50491.37156367302, 'accumulated_submission_time': 31126.28993988037, 'accumulated_eval_time': 19360.830275535583, 'accumulated_logging_time': 1.3778259754180908, 'global_step': 90035, 'preemption_count': 0}), (92469, {'train/accuracy': 0.6658973097801208, 'train/loss': 1.6872364282608032, 'train/bleu': 33.660730787562834, 'validation/accuracy': 0.6835625171661377, 'validation/loss': 1.5563279390335083, 'validation/bleu': 30.26361366275628, 'validation/num_examples': 3000, 'test/accuracy': 0.6978908777236938, 'test/loss': 1.4811248779296875, 'test/bleu': 30.035932898851115, 'test/num_examples': 3003, 'score': 31966.213314533234, 'total_duration': 51810.94109606743, 'accumulated_submission_time': 31966.213314533234, 'accumulated_eval_time': 19840.359940052032, 'accumulated_logging_time': 1.4165542125701904, 'global_step': 92469, 'preemption_count': 0}), (94903, {'train/accuracy': 0.6817435026168823, 'train/loss': 1.5854727029800415, 'train/bleu': 33.982398172121265, 'validation/accuracy': 0.6839964985847473, 'validation/loss': 1.5531708002090454, 'validation/bleu': 30.10177069772297, 'validation/num_examples': 3000, 'test/accuracy': 0.7008076310157776, 'test/loss': 1.4715487957000732, 'test/bleu': 29.92625119054191, 'test/num_examples': 3003, 'score': 32806.30698490143, 'total_duration': 53134.88615298271, 'accumulated_submission_time': 32806.30698490143, 'accumulated_eval_time': 20324.09602546692, 'accumulated_logging_time': 1.453615665435791, 'global_step': 94903, 'preemption_count': 0}), (97337, {'train/accuracy': 0.6754931807518005, 'train/loss': 1.6232917308807373, 'train/bleu': 33.42584230300187, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.5403165817260742, 'validation/bleu': 30.324988297327963, 'validation/num_examples': 3000, 'test/accuracy': 0.7019813060760498, 'test/loss': 1.4636123180389404, 'test/bleu': 30.23504562070128, 'test/num_examples': 3003, 'score': 33646.52457332611, 'total_duration': 54436.56728100777, 'accumulated_submission_time': 33646.52457332611, 'accumulated_eval_time': 20785.443420886993, 'accumulated_logging_time': 1.492318868637085, 'global_step': 97337, 'preemption_count': 0}), (99771, {'train/accuracy': 0.6739282011985779, 'train/loss': 1.63223135471344, 'train/bleu': 33.990486539139006, 'validation/accuracy': 0.6875302195549011, 'validation/loss': 1.536251187324524, 'validation/bleu': 30.240875818928536, 'validation/num_examples': 3000, 'test/accuracy': 0.7032363414764404, 'test/loss': 1.4567337036132812, 'test/bleu': 30.214177626963963, 'test/num_examples': 3003, 'score': 34486.669437885284, 'total_duration': 55755.76510024071, 'accumulated_submission_time': 34486.669437885284, 'accumulated_eval_time': 21264.37947511673, 'accumulated_logging_time': 1.5324418544769287, 'global_step': 99771, 'preemption_count': 0}), (102204, {'train/accuracy': 0.6823273301124573, 'train/loss': 1.5801806449890137, 'train/bleu': 34.581900656888564, 'validation/accuracy': 0.6878277659416199, 'validation/loss': 1.5308395624160767, 'validation/bleu': 30.43020782921961, 'validation/num_examples': 3000, 'test/accuracy': 0.7022369503974915, 'test/loss': 1.456139326095581, 'test/bleu': 30.21770641738397, 'test/num_examples': 3003, 'score': 35326.57743191719, 'total_duration': 57047.699733018875, 'accumulated_submission_time': 35326.57743191719, 'accumulated_eval_time': 21716.28870844841, 'accumulated_logging_time': 1.5729172229766846, 'global_step': 102204, 'preemption_count': 0}), (104638, {'train/accuracy': 0.6786729693412781, 'train/loss': 1.605922818183899, 'train/bleu': 34.14329271426784, 'validation/accuracy': 0.688373327255249, 'validation/loss': 1.5237150192260742, 'validation/bleu': 30.576941500675026, 'validation/num_examples': 3000, 'test/accuracy': 0.7040613889694214, 'test/loss': 1.4440556764602661, 'test/bleu': 30.680197257595136, 'test/num_examples': 3003, 'score': 36166.62100124359, 'total_duration': 58382.8054254055, 'accumulated_submission_time': 36166.62100124359, 'accumulated_eval_time': 22211.2339220047, 'accumulated_logging_time': 1.612227439880371, 'global_step': 104638, 'preemption_count': 0}), (107072, {'train/accuracy': 0.6972777843475342, 'train/loss': 1.5013537406921387, 'train/bleu': 35.75124263193658, 'validation/accuracy': 0.6902828216552734, 'validation/loss': 1.5222079753875732, 'validation/bleu': 30.507136574926953, 'validation/num_examples': 3000, 'test/accuracy': 0.7058277130126953, 'test/loss': 1.44207763671875, 'test/bleu': 30.487959006414762, 'test/num_examples': 3003, 'score': 37006.69671726227, 'total_duration': 59689.48287606239, 'accumulated_submission_time': 37006.69671726227, 'accumulated_eval_time': 22677.718291044235, 'accumulated_logging_time': 1.6531658172607422, 'global_step': 107072, 'preemption_count': 0}), (109506, {'train/accuracy': 0.6877254843711853, 'train/loss': 1.5565861463546753, 'train/bleu': 35.05868995744967, 'validation/accuracy': 0.6905431747436523, 'validation/loss': 1.5144131183624268, 'validation/bleu': 30.726323142927132, 'validation/num_examples': 3000, 'test/accuracy': 0.7075126767158508, 'test/loss': 1.4303762912750244, 'test/bleu': 30.782647298833037, 'test/num_examples': 3003, 'score': 37846.90900039673, 'total_duration': 61018.65080690384, 'accumulated_submission_time': 37846.90900039673, 'accumulated_eval_time': 23166.557002544403, 'accumulated_logging_time': 1.693382978439331, 'global_step': 109506, 'preemption_count': 0}), (111939, {'train/accuracy': 0.6905927658081055, 'train/loss': 1.540831446647644, 'train/bleu': 34.9471526156188, 'validation/accuracy': 0.6911631226539612, 'validation/loss': 1.5094802379608154, 'validation/bleu': 30.61271867582907, 'validation/num_examples': 3000, 'test/accuracy': 0.707129180431366, 'test/loss': 1.4275809526443481, 'test/bleu': 30.68408721481186, 'test/num_examples': 3003, 'score': 38686.852848529816, 'total_duration': 62330.95977449417, 'accumulated_submission_time': 38686.852848529816, 'accumulated_eval_time': 23638.802623033524, 'accumulated_logging_time': 1.736116647720337, 'global_step': 111939, 'preemption_count': 0}), (114372, {'train/accuracy': 0.7004918456077576, 'train/loss': 1.4847887754440308, 'train/bleu': 36.07395854939219, 'validation/accuracy': 0.6930230259895325, 'validation/loss': 1.5063766241073608, 'validation/bleu': 30.78058822928789, 'validation/num_examples': 3000, 'test/accuracy': 0.708628237247467, 'test/loss': 1.4222077131271362, 'test/bleu': 30.82562319625663, 'test/num_examples': 3003, 'score': 39526.988307476044, 'total_duration': 63637.38416719437, 'accumulated_submission_time': 39526.988307476044, 'accumulated_eval_time': 24104.972608327866, 'accumulated_logging_time': 1.7767961025238037, 'global_step': 114372, 'preemption_count': 0}), (116805, {'train/accuracy': 0.6977561116218567, 'train/loss': 1.5053915977478027, 'train/bleu': 35.92062263456214, 'validation/accuracy': 0.6923906803131104, 'validation/loss': 1.5051144361495972, 'validation/bleu': 30.750445632725896, 'validation/num_examples': 3000, 'test/accuracy': 0.709511399269104, 'test/loss': 1.4227913618087769, 'test/bleu': 30.89250024411417, 'test/num_examples': 3003, 'score': 40366.93438744545, 'total_duration': 64949.66950559616, 'accumulated_submission_time': 40366.93438744545, 'accumulated_eval_time': 24577.193524599075, 'accumulated_logging_time': 1.8184754848480225, 'global_step': 116805, 'preemption_count': 0}), (119239, {'train/accuracy': 0.7087553143501282, 'train/loss': 1.4513678550720215, 'train/bleu': 36.904724728202694, 'validation/accuracy': 0.6937917470932007, 'validation/loss': 1.5004736185073853, 'validation/bleu': 31.007339831048924, 'validation/num_examples': 3000, 'test/accuracy': 0.7099180817604065, 'test/loss': 1.4181479215621948, 'test/bleu': 30.699474406255312, 'test/num_examples': 3003, 'score': 41207.07846236229, 'total_duration': 66267.013463974, 'accumulated_submission_time': 41207.07846236229, 'accumulated_eval_time': 25054.274214982986, 'accumulated_logging_time': 1.8607072830200195, 'global_step': 119239, 'preemption_count': 0}), (121672, {'train/accuracy': 0.7050728797912598, 'train/loss': 1.4681651592254639, 'train/bleu': 36.167227486174745, 'validation/accuracy': 0.6942753195762634, 'validation/loss': 1.498553991317749, 'validation/bleu': 30.98735750993361, 'validation/num_examples': 3000, 'test/accuracy': 0.7114868760108948, 'test/loss': 1.4147039651870728, 'test/bleu': 30.876806630830238, 'test/num_examples': 3003, 'score': 42047.01487851143, 'total_duration': 67575.60860204697, 'accumulated_submission_time': 42047.01487851143, 'accumulated_eval_time': 25522.814121484756, 'accumulated_logging_time': 1.9024908542633057, 'global_step': 121672, 'preemption_count': 0}), (124104, {'train/accuracy': 0.7041686773300171, 'train/loss': 1.4648628234863281, 'train/bleu': 36.28302984249575, 'validation/accuracy': 0.6946101188659668, 'validation/loss': 1.495320200920105, 'validation/bleu': 30.891498149474604, 'validation/num_examples': 3000, 'test/accuracy': 0.7113357782363892, 'test/loss': 1.4104185104370117, 'test/bleu': 30.965012996729225, 'test/num_examples': 3003, 'score': 42886.95123958588, 'total_duration': 68882.01002573967, 'accumulated_submission_time': 42886.95123958588, 'accumulated_eval_time': 25989.104485034943, 'accumulated_logging_time': 1.9998183250427246, 'global_step': 124104, 'preemption_count': 0}), (126537, {'train/accuracy': 0.7092657685279846, 'train/loss': 1.449809193611145, 'train/bleu': 36.99707375984359, 'validation/accuracy': 0.693990170955658, 'validation/loss': 1.496898889541626, 'validation/bleu': 30.973520390291938, 'validation/num_examples': 3000, 'test/accuracy': 0.7120446562767029, 'test/loss': 1.4122170209884644, 'test/bleu': 31.058713552731536, 'test/num_examples': 3003, 'score': 43726.96578860283, 'total_duration': 70198.45528745651, 'accumulated_submission_time': 43726.96578860283, 'accumulated_eval_time': 26465.41655921936, 'accumulated_logging_time': 2.0420727729797363, 'global_step': 126537, 'preemption_count': 0}), (128970, {'train/accuracy': 0.7105950117111206, 'train/loss': 1.435489535331726, 'train/bleu': 37.02532880316543, 'validation/accuracy': 0.6939777731895447, 'validation/loss': 1.4965522289276123, 'validation/bleu': 31.032989289148937, 'validation/num_examples': 3000, 'test/accuracy': 0.7122886776924133, 'test/loss': 1.4110324382781982, 'test/bleu': 30.867242538049812, 'test/num_examples': 3003, 'score': 44566.927010297775, 'total_duration': 71499.1986413002, 'accumulated_submission_time': 44566.927010297775, 'accumulated_eval_time': 26926.081700086594, 'accumulated_logging_time': 2.082228422164917, 'global_step': 128970, 'preemption_count': 0}), (131403, {'train/accuracy': 0.705418586730957, 'train/loss': 1.4653115272521973, 'train/bleu': 36.74133977241949, 'validation/accuracy': 0.6943249106407166, 'validation/loss': 1.496314287185669, 'validation/bleu': 30.95072385384553, 'validation/num_examples': 3000, 'test/accuracy': 0.7123119235038757, 'test/loss': 1.4104419946670532, 'test/bleu': 30.963210714876272, 'test/num_examples': 3003, 'score': 45407.02690410614, 'total_duration': 72805.75824856758, 'accumulated_submission_time': 45407.02690410614, 'accumulated_eval_time': 27392.42360830307, 'accumulated_logging_time': 2.123875141143799, 'global_step': 131403, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7103955149650574, 'train/loss': 1.443301796913147, 'train/bleu': 36.525395183981935, 'validation/accuracy': 0.6940893530845642, 'validation/loss': 1.4960108995437622, 'validation/bleu': 30.98135216301977, 'validation/num_examples': 3000, 'test/accuracy': 0.7121259570121765, 'test/loss': 1.4103668928146362, 'test/bleu': 30.99405716021917, 'test/num_examples': 3003, 'score': 46073.19484710693, 'total_duration': 73949.41149926186, 'accumulated_submission_time': 46073.19484710693, 'accumulated_eval_time': 27869.805367946625, 'accumulated_logging_time': 2.1672544479370117, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0207 14:07:42.152359 140225696298816 submission_runner.py:586] Timing: 46073.19484710693
I0207 14:07:42.152408 140225696298816 submission_runner.py:588] Total number of evals: 56
I0207 14:07:42.152446 140225696298816 submission_runner.py:589] ====================
I0207 14:07:42.152505 140225696298816 submission_runner.py:542] Using RNG seed 1037423020
I0207 14:07:42.154177 140225696298816 submission_runner.py:551] --- Tuning run 2/5 ---
I0207 14:07:42.154291 140225696298816 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2.
I0207 14:07:42.154504 140225696298816 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2/hparams.json.
I0207 14:07:42.155262 140225696298816 submission_runner.py:206] Initializing dataset.
I0207 14:07:42.157571 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 14:07:42.160515 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0207 14:07:42.196233 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 14:07:42.774216 140225696298816 submission_runner.py:213] Initializing model.
I0207 14:07:49.057305 140225696298816 submission_runner.py:255] Initializing optimizer.
I0207 14:07:49.815298 140225696298816 submission_runner.py:262] Initializing metrics bundle.
I0207 14:07:49.815479 140225696298816 submission_runner.py:280] Initializing checkpoint and logger.
I0207 14:07:49.816198 140225696298816 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2 with prefix checkpoint_
I0207 14:07:49.816311 140225696298816 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2/meta_data_0.json.
I0207 14:07:49.816520 140225696298816 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0207 14:07:49.816579 140225696298816 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0207 14:07:50.355033 140225696298816 logger_utils.py:220] Unable to record git information. Continuing without it.
I0207 14:07:50.875485 140225696298816 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2/flags_0.json.
I0207 14:07:50.882176 140225696298816 submission_runner.py:314] Starting training loop.
I0207 14:08:17.040530 140050996819712 logging_writer.py:48] [0] global_step=0, grad_norm=4.6562819480896, loss=11.028264045715332
I0207 14:08:17.049052 140225696298816 spec.py:321] Evaluating on the training split.
I0207 14:08:19.718013 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:13:02.726424 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 14:13:05.393970 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:17:47.883177 140225696298816 spec.py:349] Evaluating on the test split.
I0207 14:17:50.556388 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:22:33.531050 140225696298816 submission_runner.py:408] Time since start: 882.65s, 	Step: 1, 	{'train/accuracy': 0.0005942517309449613, 'train/loss': 11.024877548217773, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.1668484210968, 'total_duration': 882.6488099098206, 'accumulated_submission_time': 26.1668484210968, 'accumulated_eval_time': 856.4819233417511, 'accumulated_logging_time': 0}
I0207 14:22:33.540135 140051005212416 logging_writer.py:48] [1] accumulated_eval_time=856.481923, accumulated_logging_time=0, accumulated_submission_time=26.166848, global_step=1, preemption_count=0, score=26.166848, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.036274, test/num_examples=3003, total_duration=882.648810, train/accuracy=0.000594, train/bleu=0.000000, train/loss=11.024878, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.047277, validation/num_examples=3000
I0207 14:23:08.033532 140050996819712 logging_writer.py:48] [100] global_step=100, grad_norm=0.24970856308937073, loss=9.060127258300781
I0207 14:23:42.513028 140051005212416 logging_writer.py:48] [200] global_step=200, grad_norm=0.2567581236362457, loss=8.737282752990723
I0207 14:24:17.082259 140050996819712 logging_writer.py:48] [300] global_step=300, grad_norm=0.47856849431991577, loss=8.342386245727539
I0207 14:24:51.679609 140051005212416 logging_writer.py:48] [400] global_step=400, grad_norm=0.5125924944877625, loss=8.048563003540039
I0207 14:25:26.282498 140050996819712 logging_writer.py:48] [500] global_step=500, grad_norm=0.9300322532653809, loss=7.814052581787109
I0207 14:26:00.891277 140051005212416 logging_writer.py:48] [600] global_step=600, grad_norm=0.8229386210441589, loss=7.59480094909668
I0207 14:26:35.517569 140050996819712 logging_writer.py:48] [700] global_step=700, grad_norm=0.6472436189651489, loss=7.480161666870117
I0207 14:27:10.121240 140051005212416 logging_writer.py:48] [800] global_step=800, grad_norm=0.7528484463691711, loss=7.268680095672607
I0207 14:27:44.734848 140050996819712 logging_writer.py:48] [900] global_step=900, grad_norm=0.5833507180213928, loss=7.114068031311035
I0207 14:28:19.348927 140051005212416 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.6050288081169128, loss=6.9822998046875
I0207 14:28:53.955061 140050996819712 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6585420966148376, loss=6.89065408706665
I0207 14:29:28.571228 140051005212416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5036805272102356, loss=6.6712727546691895
I0207 14:30:03.183503 140050996819712 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6235783696174622, loss=6.647675514221191
I0207 14:30:37.817980 140051005212416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6442097425460815, loss=6.515190124511719
I0207 14:31:12.426438 140050996819712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.6198375225067139, loss=6.424623489379883
I0207 14:31:47.056132 140051005212416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5220593810081482, loss=6.331459999084473
I0207 14:32:21.687842 140050996819712 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.6285537481307983, loss=6.301540374755859
I0207 14:32:56.332507 140051005212416 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.636553943157196, loss=6.0978264808654785
I0207 14:33:30.970700 140050996819712 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.5143371820449829, loss=6.057608604431152
I0207 14:34:05.601452 140051005212416 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7185231447219849, loss=6.013935089111328
I0207 14:34:40.206262 140050996819712 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6577823162078857, loss=5.783000469207764
I0207 14:35:14.803480 140051005212416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6154706478118896, loss=5.789316177368164
I0207 14:35:49.423698 140050996819712 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5548875331878662, loss=5.6362152099609375
I0207 14:36:24.048272 140051005212416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.638176441192627, loss=5.581938743591309
I0207 14:36:33.818810 140225696298816 spec.py:321] Evaluating on the training split.
I0207 14:36:36.790935 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:39:54.133941 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 14:39:56.826283 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:42:52.088729 140225696298816 spec.py:349] Evaluating on the test split.
I0207 14:42:54.774671 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 14:45:37.854837 140225696298816 submission_runner.py:408] Time since start: 2266.97s, 	Step: 2430, 	{'train/accuracy': 0.4335348904132843, 'train/loss': 3.881221294403076, 'train/bleu': 15.486008673345255, 'validation/accuracy': 0.41708099842071533, 'validation/loss': 4.0259270668029785, 'validation/bleu': 10.90630801345595, 'validation/num_examples': 3000, 'test/accuracy': 0.4070304036140442, 'test/loss': 4.187845706939697, 'test/bleu': 9.262918884334642, 'test/num_examples': 3003, 'score': 866.3570308685303, 'total_duration': 2266.9725909233093, 'accumulated_submission_time': 866.3570308685303, 'accumulated_eval_time': 1400.5178980827332, 'accumulated_logging_time': 0.019860267639160156}
I0207 14:45:37.869785 140050996819712 logging_writer.py:48] [2430] accumulated_eval_time=1400.517898, accumulated_logging_time=0.019860, accumulated_submission_time=866.357031, global_step=2430, preemption_count=0, score=866.357031, test/accuracy=0.407030, test/bleu=9.262919, test/loss=4.187846, test/num_examples=3003, total_duration=2266.972591, train/accuracy=0.433535, train/bleu=15.486009, train/loss=3.881221, validation/accuracy=0.417081, validation/bleu=10.906308, validation/loss=4.025927, validation/num_examples=3000
I0207 14:46:02.338569 140051005212416 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4917915463447571, loss=5.518531799316406
I0207 14:46:36.875524 140050996819712 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.60801762342453, loss=5.403322696685791
I0207 14:47:11.485021 140051005212416 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7420243620872498, loss=5.3986639976501465
I0207 14:47:46.102521 140050996819712 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7009760737419128, loss=5.358185768127441
I0207 14:48:20.725449 140051005212416 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5825669169425964, loss=5.213800430297852
I0207 14:48:55.354383 140050996819712 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5221297144889832, loss=5.22990083694458
I0207 14:49:29.975553 140051005212416 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6726710200309753, loss=5.2311272621154785
I0207 14:50:04.569917 140050996819712 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.524958074092865, loss=5.140591621398926
I0207 14:50:39.190314 140051005212416 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5225136280059814, loss=5.077333927154541
I0207 14:51:13.815543 140050996819712 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.529639720916748, loss=5.078787803649902
I0207 14:51:48.439556 140051005212416 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4840763211250305, loss=4.9529547691345215
I0207 14:52:23.089281 140050996819712 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.48212674260139465, loss=5.015174388885498
I0207 14:52:57.709144 140051005212416 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4292738437652588, loss=4.919651031494141
I0207 14:53:32.322604 140050996819712 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5885478258132935, loss=4.942451477050781
I0207 14:54:06.942481 140051005212416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.4308968484401703, loss=4.874710559844971
I0207 14:54:41.571171 140050996819712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.44397440552711487, loss=4.918001174926758
I0207 14:55:16.197239 140051005212416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.5577715635299683, loss=4.836021900177002
I0207 14:55:50.812619 140050996819712 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4231303334236145, loss=4.864247798919678
I0207 14:56:25.447997 140051005212416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.3732105493545532, loss=4.780600070953369
I0207 14:57:00.065830 140050996819712 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.42991903424263, loss=4.747775554656982
I0207 14:57:34.677244 140051005212416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.38282909989356995, loss=4.695132732391357
I0207 14:58:09.297575 140050996819712 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.43577486276626587, loss=4.752388000488281
I0207 14:58:43.910092 140051005212416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.360990434885025, loss=4.682271480560303
I0207 14:59:18.518225 140050996819712 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.40406695008277893, loss=4.777834892272949
I0207 14:59:37.971688 140225696298816 spec.py:321] Evaluating on the training split.
I0207 14:59:40.938282 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:02:20.993066 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 15:02:23.699616 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:05:06.516263 140225696298816 spec.py:349] Evaluating on the test split.
I0207 15:05:09.193994 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:07:40.281209 140225696298816 submission_runner.py:408] Time since start: 3589.40s, 	Step: 4858, 	{'train/accuracy': 0.5460978150367737, 'train/loss': 2.815052032470703, 'train/bleu': 23.83795839443816, 'validation/accuracy': 0.5495033860206604, 'validation/loss': 2.779167413711548, 'validation/bleu': 20.46226812080449, 'validation/num_examples': 3000, 'test/accuracy': 0.5491255521774292, 'test/loss': 2.8148353099823, 'test/bleu': 18.97626936557872, 'test/num_examples': 3003, 'score': 1706.3713157176971, 'total_duration': 3589.398926973343, 'accumulated_submission_time': 1706.3713157176971, 'accumulated_eval_time': 1882.8273282051086, 'accumulated_logging_time': 0.04439544677734375}
I0207 15:07:40.297090 140051005212416 logging_writer.py:48] [4858] accumulated_eval_time=1882.827328, accumulated_logging_time=0.044395, accumulated_submission_time=1706.371316, global_step=4858, preemption_count=0, score=1706.371316, test/accuracy=0.549126, test/bleu=18.976269, test/loss=2.814835, test/num_examples=3003, total_duration=3589.398927, train/accuracy=0.546098, train/bleu=23.837958, train/loss=2.815052, validation/accuracy=0.549503, validation/bleu=20.462268, validation/loss=2.779167, validation/num_examples=3000
I0207 15:07:55.119948 140050996819712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.4385088086128235, loss=4.695936679840088
I0207 15:08:29.618028 140051005212416 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.35837000608444214, loss=4.6541523933410645
I0207 15:09:04.181663 140050996819712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3725665509700775, loss=4.654788494110107
I0207 15:09:38.779695 140051005212416 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.38871657848358154, loss=4.700090408325195
I0207 15:10:13.360026 140050996819712 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.3449119031429291, loss=4.621642589569092
I0207 15:10:47.980952 140051005212416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.32819193601608276, loss=4.6060333251953125
I0207 15:11:22.562351 140050996819712 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.37002354860305786, loss=4.639671325683594
I0207 15:11:57.124221 140051005212416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.37807440757751465, loss=4.593719482421875
I0207 15:12:31.716912 140050996819712 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.36134058237075806, loss=4.586395740509033
I0207 15:13:06.301347 140051005212416 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.3372497260570526, loss=4.557416915893555
I0207 15:13:40.912501 140050996819712 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.3365563750267029, loss=4.541069507598877
I0207 15:14:15.518027 140051005212416 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.31709638237953186, loss=4.5775957107543945
I0207 15:14:50.114870 140050996819712 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.3389853537082672, loss=4.609501838684082
I0207 15:15:24.689815 140051005212416 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.31921684741973877, loss=4.597827434539795
I0207 15:15:59.266377 140050996819712 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.28950604796409607, loss=4.572294235229492
I0207 15:16:33.863351 140051005212416 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.3098192811012268, loss=4.522346496582031
I0207 15:17:08.459902 140050996819712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2948518693447113, loss=4.496591091156006
I0207 15:17:43.041044 140051005212416 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2973702847957611, loss=4.461077690124512
I0207 15:18:17.629693 140050996819712 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.27773675322532654, loss=4.420794486999512
I0207 15:18:52.215917 140051005212416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.27275216579437256, loss=4.471306324005127
I0207 15:19:26.766094 140050996819712 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.2550467550754547, loss=4.4585747718811035
I0207 15:20:01.366823 140051005212416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3340652585029602, loss=4.5501227378845215
I0207 15:20:35.922178 140050996819712 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.2569602131843567, loss=4.408557415008545
I0207 15:21:10.489327 140051005212416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.26265522837638855, loss=4.384848594665527
I0207 15:21:40.623700 140225696298816 spec.py:321] Evaluating on the training split.
I0207 15:21:43.595715 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:24:14.285193 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 15:24:16.969327 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:26:51.960919 140225696298816 spec.py:349] Evaluating on the test split.
I0207 15:26:54.648635 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:29:17.666666 140225696298816 submission_runner.py:408] Time since start: 4886.78s, 	Step: 7289, 	{'train/accuracy': 0.583653450012207, 'train/loss': 2.4412691593170166, 'train/bleu': 27.028060189117536, 'validation/accuracy': 0.5920695066452026, 'validation/loss': 2.3615870475769043, 'validation/bleu': 23.554423470160216, 'validation/num_examples': 3000, 'test/accuracy': 0.5970367789268494, 'test/loss': 2.35249662399292, 'test/bleu': 22.367772014175003, 'test/num_examples': 3003, 'score': 2546.6103324890137, 'total_duration': 4886.784422636032, 'accumulated_submission_time': 2546.6103324890137, 'accumulated_eval_time': 2339.8702476024628, 'accumulated_logging_time': 0.07002735137939453}
I0207 15:29:17.682549 140050996819712 logging_writer.py:48] [7289] accumulated_eval_time=2339.870248, accumulated_logging_time=0.070027, accumulated_submission_time=2546.610332, global_step=7289, preemption_count=0, score=2546.610332, test/accuracy=0.597037, test/bleu=22.367772, test/loss=2.352497, test/num_examples=3003, total_duration=4886.784423, train/accuracy=0.583653, train/bleu=27.028060, train/loss=2.441269, validation/accuracy=0.592070, validation/bleu=23.554423, validation/loss=2.361587, validation/num_examples=3000
I0207 15:29:21.849848 140051005212416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.25145798921585083, loss=4.354990005493164
I0207 15:29:56.287832 140050996819712 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.2399531751871109, loss=4.428795337677002
I0207 15:30:30.792365 140051005212416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.22994209825992584, loss=4.387822151184082
I0207 15:31:05.365706 140050996819712 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.23130455613136292, loss=4.413659572601318
I0207 15:31:39.931273 140051005212416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.22411246597766876, loss=4.405704498291016
I0207 15:32:14.506994 140050996819712 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.21740464866161346, loss=4.288664817810059
I0207 15:32:49.093138 140051005212416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.22643516957759857, loss=4.400507926940918
I0207 15:33:23.715389 140050996819712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.21689556539058685, loss=4.399693965911865
I0207 15:33:58.300478 140051005212416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.2027364820241928, loss=4.399587154388428
I0207 15:34:32.875602 140050996819712 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.21883182227611542, loss=4.348370552062988
I0207 15:35:07.432879 140051005212416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.2139025777578354, loss=4.332186222076416
I0207 15:35:41.998813 140050996819712 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.20977163314819336, loss=4.413060188293457
I0207 15:36:16.582819 140051005212416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20173242688179016, loss=4.361966609954834
I0207 15:36:51.139544 140050996819712 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.24417591094970703, loss=4.306021213531494
I0207 15:37:25.707136 140051005212416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.20623905956745148, loss=4.272007942199707
I0207 15:38:00.281433 140050996819712 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.23524358868598938, loss=4.362331390380859
I0207 15:38:34.870318 140051005212416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.21520325541496277, loss=4.293332576751709
I0207 15:39:09.445814 140050996819712 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.21641185879707336, loss=4.397752285003662
I0207 15:39:44.008356 140051005212416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.2140720933675766, loss=4.306321144104004
I0207 15:40:18.564705 140050996819712 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.2364559918642044, loss=4.401750087738037
I0207 15:40:53.159629 140051005212416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.2093663066625595, loss=4.297727584838867
I0207 15:41:27.728894 140050996819712 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.2208344042301178, loss=4.350484848022461
I0207 15:42:02.286762 140051005212416 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.18286997079849243, loss=4.273015975952148
I0207 15:42:36.879298 140050996819712 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.20184384286403656, loss=4.2957000732421875
I0207 15:43:11.461167 140051005212416 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.1789652705192566, loss=4.240712642669678
I0207 15:43:17.745743 140225696298816 spec.py:321] Evaluating on the training split.
I0207 15:43:20.709194 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:45:56.857318 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 15:45:59.538734 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:48:39.373573 140225696298816 spec.py:349] Evaluating on the test split.
I0207 15:48:42.053906 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 15:51:07.650186 140225696298816 submission_runner.py:408] Time since start: 6196.77s, 	Step: 9720, 	{'train/accuracy': 0.6012474894523621, 'train/loss': 2.2867743968963623, 'train/bleu': 29.023278515494034, 'validation/accuracy': 0.6191863417625427, 'validation/loss': 2.1540048122406006, 'validation/bleu': 25.267400374292624, 'validation/num_examples': 3000, 'test/accuracy': 0.6241241097450256, 'test/loss': 2.124663829803467, 'test/bleu': 24.386627330019042, 'test/num_examples': 3003, 'score': 3386.586151123047, 'total_duration': 6196.767947673798, 'accumulated_submission_time': 3386.586151123047, 'accumulated_eval_time': 2809.7746393680573, 'accumulated_logging_time': 0.09573030471801758}
I0207 15:51:07.666157 140050996819712 logging_writer.py:48] [9720] accumulated_eval_time=2809.774639, accumulated_logging_time=0.095730, accumulated_submission_time=3386.586151, global_step=9720, preemption_count=0, score=3386.586151, test/accuracy=0.624124, test/bleu=24.386627, test/loss=2.124664, test/num_examples=3003, total_duration=6196.767948, train/accuracy=0.601247, train/bleu=29.023279, train/loss=2.286774, validation/accuracy=0.619186, validation/bleu=25.267400, validation/loss=2.154005, validation/num_examples=3000
I0207 15:51:35.585725 140051005212416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.21417763829231262, loss=4.270071983337402
I0207 15:52:10.044354 140050996819712 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.17594631016254425, loss=4.2024030685424805
I0207 15:52:44.637901 140051005212416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.17668059468269348, loss=4.274723052978516
I0207 15:53:19.210402 140050996819712 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.18948639929294586, loss=4.226156711578369
I0207 15:53:53.745753 140051005212416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.21253924071788788, loss=4.2802348136901855
I0207 15:54:28.301143 140050996819712 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.18496909737586975, loss=4.193814754486084
I0207 15:55:02.875546 140051005212416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17299334704875946, loss=4.210544586181641
I0207 15:55:37.439996 140050996819712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.172958642244339, loss=4.277957439422607
I0207 15:56:12.020511 140051005212416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17536801099777222, loss=4.181811809539795
I0207 15:56:46.583376 140050996819712 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.1762787401676178, loss=4.2299370765686035
I0207 15:57:21.152344 140051005212416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.18459095060825348, loss=4.260550022125244
I0207 15:57:55.727172 140050996819712 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1800147294998169, loss=4.254443645477295
I0207 15:58:30.307967 140051005212416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17849200963974, loss=4.292294025421143
I0207 15:59:04.861131 140050996819712 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2001112848520279, loss=4.183112621307373
I0207 15:59:39.408043 140051005212416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.18834049999713898, loss=4.237778186798096
I0207 16:00:13.986155 140050996819712 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.24342700839042664, loss=4.2109527587890625
I0207 16:00:48.550318 140051005212416 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.16738319396972656, loss=4.163950443267822
I0207 16:01:23.109225 140050996819712 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1663023829460144, loss=4.263938903808594
I0207 16:01:57.680264 140051005212416 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1728169023990631, loss=4.27131462097168
I0207 16:02:32.270337 140050996819712 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.1645558476448059, loss=4.154402732849121
I0207 16:03:06.812655 140051005212416 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.18624688684940338, loss=4.243898868560791
I0207 16:03:41.410547 140050996819712 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.18752670288085938, loss=4.191048622131348
I0207 16:04:15.985905 140051005212416 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1672724336385727, loss=4.251229286193848
I0207 16:04:50.578819 140050996819712 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.16999733448028564, loss=4.155500411987305
I0207 16:05:07.943035 140225696298816 spec.py:321] Evaluating on the training split.
I0207 16:05:10.906635 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:08:16.040332 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 16:08:18.725570 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:11:27.316145 140225696298816 spec.py:349] Evaluating on the test split.
I0207 16:11:30.003565 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:14:42.017896 140225696298816 submission_runner.py:408] Time since start: 7611.14s, 	Step: 12152, 	{'train/accuracy': 0.6117793917655945, 'train/loss': 2.2029600143432617, 'train/bleu': 29.49963286982012, 'validation/accuracy': 0.6326642036437988, 'validation/loss': 2.0467686653137207, 'validation/bleu': 26.453320841038845, 'validation/num_examples': 3000, 'test/accuracy': 0.6393585801124573, 'test/loss': 2.005385637283325, 'test/bleu': 25.511380252988697, 'test/num_examples': 3003, 'score': 4226.775480031967, 'total_duration': 7611.135657072067, 'accumulated_submission_time': 4226.775480031967, 'accumulated_eval_time': 3383.849452972412, 'accumulated_logging_time': 0.12136363983154297}
I0207 16:14:42.034142 140051005212416 logging_writer.py:48] [12152] accumulated_eval_time=3383.849453, accumulated_logging_time=0.121364, accumulated_submission_time=4226.775480, global_step=12152, preemption_count=0, score=4226.775480, test/accuracy=0.639359, test/bleu=25.511380, test/loss=2.005386, test/num_examples=3003, total_duration=7611.135657, train/accuracy=0.611779, train/bleu=29.499633, train/loss=2.202960, validation/accuracy=0.632664, validation/bleu=26.453321, validation/loss=2.046769, validation/num_examples=3000
I0207 16:14:58.911097 140050996819712 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.21005332469940186, loss=4.2077131271362305
I0207 16:15:33.387311 140051005212416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.17951656877994537, loss=4.229153633117676
I0207 16:16:07.911922 140050996819712 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1720251441001892, loss=4.173382759094238
I0207 16:16:42.475732 140051005212416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.18931405246257782, loss=4.18109130859375
I0207 16:17:17.050407 140050996819712 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.17104864120483398, loss=4.119153022766113
I0207 16:17:51.596513 140051005212416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1605168730020523, loss=4.104725360870361
I0207 16:18:26.141640 140050996819712 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.16361412405967712, loss=4.18727970123291
I0207 16:19:00.699952 140051005212416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.17719954252243042, loss=4.164757251739502
I0207 16:19:35.258074 140050996819712 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2283964455127716, loss=4.144458770751953
I0207 16:20:09.860705 140051005212416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.17565618455410004, loss=4.1707563400268555
I0207 16:20:44.451603 140050996819712 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.17626206576824188, loss=4.165788650512695
I0207 16:21:19.003117 140051005212416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.1576659083366394, loss=4.164897918701172
I0207 16:21:53.586504 140050996819712 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.1579621583223343, loss=4.191622257232666
I0207 16:22:28.185891 140051005212416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.15746332705020905, loss=4.123079299926758
I0207 16:23:02.750574 140050996819712 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.19495469331741333, loss=4.146139144897461
I0207 16:23:37.348971 140051005212416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18312311172485352, loss=4.134444713592529
I0207 16:24:12.094914 140050996819712 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.17996443808078766, loss=4.158988952636719
I0207 16:24:46.692125 140051005212416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1760077178478241, loss=4.201552391052246
I0207 16:25:21.281999 140050996819712 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.17153619229793549, loss=4.0717620849609375
I0207 16:25:55.854593 140051005212416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.1658019870519638, loss=4.129161834716797
I0207 16:26:30.427677 140050996819712 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.19636079668998718, loss=4.102090358734131
I0207 16:27:05.010182 140051005212416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.20489360392093658, loss=4.1144938468933105
I0207 16:27:39.579155 140050996819712 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17452049255371094, loss=4.155298233032227
I0207 16:28:14.153999 140051005212416 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1691821664571762, loss=4.1372294425964355
I0207 16:28:42.212237 140225696298816 spec.py:321] Evaluating on the training split.
I0207 16:28:45.184237 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:33:15.307711 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 16:33:17.994252 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:35:56.523194 140225696298816 spec.py:349] Evaluating on the test split.
I0207 16:35:59.199554 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:38:29.760933 140225696298816 submission_runner.py:408] Time since start: 9038.88s, 	Step: 14583, 	{'train/accuracy': 0.6235041618347168, 'train/loss': 2.090024471282959, 'train/bleu': 30.151688486651715, 'validation/accuracy': 0.642856240272522, 'validation/loss': 1.947042465209961, 'validation/bleu': 26.987183390760876, 'validation/num_examples': 3000, 'test/accuracy': 0.6523270010948181, 'test/loss': 1.8937307596206665, 'test/bleu': 26.47813059844154, 'test/num_examples': 3003, 'score': 5066.86608338356, 'total_duration': 9038.87869143486, 'accumulated_submission_time': 5066.86608338356, 'accumulated_eval_time': 3971.3980989456177, 'accumulated_logging_time': 0.1472148895263672}
I0207 16:38:29.777652 140050996819712 logging_writer.py:48] [14583] accumulated_eval_time=3971.398099, accumulated_logging_time=0.147215, accumulated_submission_time=5066.866083, global_step=14583, preemption_count=0, score=5066.866083, test/accuracy=0.652327, test/bleu=26.478131, test/loss=1.893731, test/num_examples=3003, total_duration=9038.878691, train/accuracy=0.623504, train/bleu=30.151688, train/loss=2.090024, validation/accuracy=0.642856, validation/bleu=26.987183, validation/loss=1.947042, validation/num_examples=3000
I0207 16:38:35.999995 140051005212416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.16011552512645721, loss=4.20463752746582
I0207 16:39:10.405885 140050996819712 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15501858294010162, loss=4.10943603515625
I0207 16:39:44.928498 140051005212416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.16599401831626892, loss=4.136430740356445
I0207 16:40:19.480375 140050996819712 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.16730542480945587, loss=4.091845989227295
I0207 16:40:54.054557 140051005212416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18438856303691864, loss=4.049107551574707
I0207 16:41:28.631678 140050996819712 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1883803904056549, loss=4.161539077758789
I0207 16:42:03.229940 140051005212416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.16953261196613312, loss=4.135013103485107
I0207 16:42:37.792054 140050996819712 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2034161239862442, loss=4.1168646812438965
I0207 16:43:12.352930 140051005212416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.15195196866989136, loss=4.047160625457764
I0207 16:43:46.913449 140050996819712 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.1651621162891388, loss=4.136499404907227
I0207 16:44:21.503849 140051005212416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.17470580339431763, loss=4.116612434387207
I0207 16:44:56.078126 140050996819712 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.17914405465126038, loss=4.096091270446777
I0207 16:45:30.652665 140051005212416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.16440290212631226, loss=4.045111656188965
I0207 16:46:05.228995 140050996819712 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.15753881633281708, loss=4.120019912719727
I0207 16:46:39.819864 140051005212416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.16569767892360687, loss=4.088521480560303
I0207 16:47:14.372517 140050996819712 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.16983774304389954, loss=4.15862512588501
I0207 16:47:48.943755 140051005212416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.16392876207828522, loss=4.060858726501465
I0207 16:48:23.517252 140050996819712 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.23734645545482635, loss=4.063591003417969
I0207 16:48:58.099872 140051005212416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.1638614684343338, loss=4.0181379318237305
I0207 16:49:32.681413 140050996819712 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.16814956068992615, loss=4.0674519538879395
I0207 16:50:07.297068 140051005212416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.18735824525356293, loss=4.069995403289795
I0207 16:50:41.866191 140050996819712 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.17039023339748383, loss=4.06986141204834
I0207 16:51:16.442693 140051005212416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.1826155185699463, loss=4.095457077026367
I0207 16:51:51.042685 140050996819712 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15989793837070465, loss=4.072461128234863
I0207 16:52:25.627694 140051005212416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17133855819702148, loss=4.073659420013428
I0207 16:52:29.853165 140225696298816 spec.py:321] Evaluating on the training split.
I0207 16:52:32.817454 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:55:21.003060 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 16:55:23.682183 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 16:57:51.640318 140225696298816 spec.py:349] Evaluating on the test split.
I0207 16:57:54.327313 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:00:08.455262 140225696298816 submission_runner.py:408] Time since start: 10337.57s, 	Step: 17014, 	{'train/accuracy': 0.6282114386558533, 'train/loss': 2.0542614459991455, 'train/bleu': 30.50936542867486, 'validation/accuracy': 0.6478034853935242, 'validation/loss': 1.9196758270263672, 'validation/bleu': 27.620231439534933, 'validation/num_examples': 3000, 'test/accuracy': 0.6583580374717712, 'test/loss': 1.854627013206482, 'test/bleu': 26.803849032849374, 'test/num_examples': 3003, 'score': 5906.852504491806, 'total_duration': 10337.573017835617, 'accumulated_submission_time': 5906.852504491806, 'accumulated_eval_time': 4430.000137805939, 'accumulated_logging_time': 0.17507600784301758}
I0207 17:00:08.472205 140050996819712 logging_writer.py:48] [17014] accumulated_eval_time=4430.000138, accumulated_logging_time=0.175076, accumulated_submission_time=5906.852504, global_step=17014, preemption_count=0, score=5906.852504, test/accuracy=0.658358, test/bleu=26.803849, test/loss=1.854627, test/num_examples=3003, total_duration=10337.573018, train/accuracy=0.628211, train/bleu=30.509365, train/loss=2.054261, validation/accuracy=0.647803, validation/bleu=27.620231, validation/loss=1.919676, validation/num_examples=3000
I0207 17:00:38.420050 140051005212416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2239704579114914, loss=4.106982231140137
I0207 17:01:12.948119 140050996819712 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.17839296162128448, loss=4.043001651763916
I0207 17:01:47.542815 140051005212416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.17969383299350739, loss=4.103585243225098
I0207 17:02:22.092638 140050996819712 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.1666194647550583, loss=4.053983688354492
I0207 17:02:56.669145 140051005212416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.16288037598133087, loss=4.069441795349121
I0207 17:03:31.233613 140050996819712 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15677523612976074, loss=4.103671550750732
I0207 17:04:05.802716 140051005212416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.19505228102207184, loss=4.057827472686768
I0207 17:04:40.388928 140050996819712 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.14796222746372223, loss=4.04428768157959
I0207 17:05:14.943202 140051005212416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.17276518046855927, loss=4.0785441398620605
I0207 17:05:49.518220 140050996819712 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.18592007458209991, loss=4.059007167816162
I0207 17:06:24.099364 140051005212416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.21885649859905243, loss=4.094707012176514
I0207 17:06:58.668663 140050996819712 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.19737786054611206, loss=4.060818672180176
I0207 17:07:33.222336 140051005212416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.2800985276699066, loss=4.097874164581299
I0207 17:08:07.799446 140050996819712 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.16060058772563934, loss=4.081085681915283
I0207 17:08:42.364516 140051005212416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.17793378233909607, loss=4.084847450256348
I0207 17:09:16.910028 140050996819712 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.16498175263404846, loss=4.038475513458252
I0207 17:09:51.467727 140051005212416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.15347759425640106, loss=4.024473190307617
I0207 17:10:26.077123 140050996819712 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.20284268260002136, loss=4.118766784667969
I0207 17:11:00.669044 140051005212416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.22456997632980347, loss=3.965545654296875
I0207 17:11:35.233418 140050996819712 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.15381738543510437, loss=4.017024517059326
I0207 17:12:09.808700 140051005212416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.1541462391614914, loss=4.052066326141357
I0207 17:12:44.396647 140050996819712 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2663481533527374, loss=4.040751934051514
I0207 17:13:18.965122 140051005212416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.17494134604930878, loss=4.029644966125488
I0207 17:13:53.548043 140050996819712 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1727372109889984, loss=4.061278343200684
I0207 17:14:08.491925 140225696298816 spec.py:321] Evaluating on the training split.
I0207 17:14:11.467357 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:17:01.176730 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 17:17:03.860071 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:19:39.278021 140225696298816 spec.py:349] Evaluating on the test split.
I0207 17:19:41.961610 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:22:09.666289 140225696298816 submission_runner.py:408] Time since start: 11658.78s, 	Step: 19445, 	{'train/accuracy': 0.6460630893707275, 'train/loss': 1.9484455585479736, 'train/bleu': 31.32886972123233, 'validation/accuracy': 0.652415931224823, 'validation/loss': 1.8769932985305786, 'validation/bleu': 27.88024937862478, 'validation/num_examples': 3000, 'test/accuracy': 0.6621463298797607, 'test/loss': 1.8195322751998901, 'test/bleu': 27.046071671892715, 'test/num_examples': 3003, 'score': 6746.784756422043, 'total_duration': 11658.784047603607, 'accumulated_submission_time': 6746.784756422043, 'accumulated_eval_time': 4911.174456119537, 'accumulated_logging_time': 0.20150542259216309}
I0207 17:22:09.683691 140051005212416 logging_writer.py:48] [19445] accumulated_eval_time=4911.174456, accumulated_logging_time=0.201505, accumulated_submission_time=6746.784756, global_step=19445, preemption_count=0, score=6746.784756, test/accuracy=0.662146, test/bleu=27.046072, test/loss=1.819532, test/num_examples=3003, total_duration=11658.784048, train/accuracy=0.646063, train/bleu=31.328870, train/loss=1.948446, validation/accuracy=0.652416, validation/bleu=27.880249, validation/loss=1.876993, validation/num_examples=3000
I0207 17:22:29.004743 140050996819712 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.1617310792207718, loss=4.041250705718994
I0207 17:23:03.470360 140051005212416 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17755469679832458, loss=4.017232894897461
I0207 17:23:38.033289 140050996819712 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.15903109312057495, loss=4.0534348487854
I0207 17:24:12.600455 140051005212416 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.18357738852500916, loss=4.120497226715088
I0207 17:24:47.169952 140050996819712 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.21766148507595062, loss=4.048386096954346
I0207 17:25:21.741159 140051005212416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.19097241759300232, loss=4.049172878265381
I0207 17:25:56.312410 140050996819712 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.26440367102622986, loss=4.004498481750488
I0207 17:26:30.889624 140051005212416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.16152572631835938, loss=4.0146989822387695
I0207 17:27:05.477808 140050996819712 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.19847969710826874, loss=4.03717565536499
I0207 17:27:40.039266 140051005212416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.15613676607608795, loss=4.078212738037109
I0207 17:28:14.595338 140050996819712 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.2266611009836197, loss=4.037351131439209
I0207 17:28:49.181011 140051005212416 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.16723601520061493, loss=4.066018104553223
I0207 17:29:23.758210 140050996819712 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1629171073436737, loss=4.030489921569824
I0207 17:29:58.335578 140051005212416 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.21103009581565857, loss=4.0079474449157715
I0207 17:30:32.883047 140050996819712 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.16629467904567719, loss=3.9711437225341797
I0207 17:31:07.451842 140051005212416 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.2110656350851059, loss=3.986368179321289
I0207 17:31:42.034759 140050996819712 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.18690341711044312, loss=3.987156629562378
I0207 17:32:16.595115 140051005212416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.16742660105228424, loss=4.002620220184326
I0207 17:32:51.181976 140050996819712 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.2588752210140228, loss=4.0192999839782715
I0207 17:33:25.749212 140051005212416 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.1595185399055481, loss=4.023148059844971
I0207 17:34:00.307927 140050996819712 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.1697828471660614, loss=4.028851509094238
I0207 17:34:34.884925 140051005212416 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.19187487661838531, loss=4.000587463378906
I0207 17:35:09.498052 140050996819712 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.1788811981678009, loss=3.974386215209961
I0207 17:35:44.061864 140051005212416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.22300000488758087, loss=4.011404514312744
I0207 17:36:09.699538 140225696298816 spec.py:321] Evaluating on the training split.
I0207 17:36:12.663951 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:38:51.815346 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 17:38:54.492600 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:41:24.585119 140225696298816 spec.py:349] Evaluating on the test split.
I0207 17:41:27.266976 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 17:43:49.169100 140225696298816 submission_runner.py:408] Time since start: 12958.29s, 	Step: 21876, 	{'train/accuracy': 0.6394797563552856, 'train/loss': 1.9793699979782104, 'train/bleu': 31.291298722145083, 'validation/accuracy': 0.6581319570541382, 'validation/loss': 1.8350783586502075, 'validation/bleu': 28.01187089919093, 'validation/num_examples': 3000, 'test/accuracy': 0.6679449081420898, 'test/loss': 1.7722675800323486, 'test/bleu': 27.51437928438289, 'test/num_examples': 3003, 'score': 7586.71210360527, 'total_duration': 12958.286858320236, 'accumulated_submission_time': 7586.71210360527, 'accumulated_eval_time': 5370.643972635269, 'accumulated_logging_time': 0.23011422157287598}
I0207 17:43:49.186483 140050996819712 logging_writer.py:48] [21876] accumulated_eval_time=5370.643973, accumulated_logging_time=0.230114, accumulated_submission_time=7586.712104, global_step=21876, preemption_count=0, score=7586.712104, test/accuracy=0.667945, test/bleu=27.514379, test/loss=1.772268, test/num_examples=3003, total_duration=12958.286858, train/accuracy=0.639480, train/bleu=31.291299, train/loss=1.979370, validation/accuracy=0.658132, validation/bleu=28.011871, validation/loss=1.835078, validation/num_examples=3000
I0207 17:43:57.824331 140051005212416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.21864478290081024, loss=4.075962066650391
I0207 17:44:32.273489 140050996819712 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.17627187073230743, loss=4.049107551574707
I0207 17:45:06.778179 140051005212416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1737840473651886, loss=4.042076110839844
I0207 17:45:41.355590 140050996819712 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.28757959604263306, loss=4.084540367126465
I0207 17:46:15.916951 140051005212416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.17595407366752625, loss=3.9377293586730957
I0207 17:46:50.488265 140050996819712 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.1851901113986969, loss=4.05633544921875
I0207 17:47:25.072708 140051005212416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.15978677570819855, loss=4.036181926727295
I0207 17:47:59.656455 140050996819712 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.17236903309822083, loss=3.985210418701172
I0207 17:48:34.221051 140051005212416 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.22103433310985565, loss=4.035254001617432
I0207 17:49:08.826726 140050996819712 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.17548444867134094, loss=3.9877378940582275
I0207 17:49:43.384493 140051005212416 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.2185848504304886, loss=4.008193016052246
I0207 17:50:17.945510 140050996819712 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.18406891822814941, loss=4.069850444793701
I0207 17:50:52.495074 140051005212416 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.1698416769504547, loss=4.022644996643066
I0207 17:51:27.041999 140050996819712 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.16837690770626068, loss=3.979626178741455
I0207 17:52:01.629338 140051005212416 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1824207454919815, loss=3.99949049949646
I0207 17:52:36.230439 140050996819712 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.20624758303165436, loss=4.0021843910217285
I0207 17:53:10.827759 140051005212416 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.16886691749095917, loss=3.983348846435547
I0207 17:53:45.411077 140050996819712 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2351604551076889, loss=3.994192123413086
I0207 17:54:20.010945 140051005212416 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.17489273846149445, loss=3.953240394592285
I0207 17:54:54.581134 140050996819712 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.17544031143188477, loss=4.009987831115723
I0207 17:55:29.155809 140051005212416 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.18476997315883636, loss=4.069125175476074
I0207 17:56:03.716389 140050996819712 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.1907181292772293, loss=3.9614369869232178
I0207 17:56:38.279148 140051005212416 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2923824191093445, loss=4.0248870849609375
I0207 17:57:12.831697 140050996819712 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2981303036212921, loss=4.029533863067627
I0207 17:57:47.407718 140051005212416 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.2016228437423706, loss=4.013284683227539
I0207 17:57:49.202258 140225696298816 spec.py:321] Evaluating on the training split.
I0207 17:57:52.175786 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:01:20.900547 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 18:01:23.577497 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:05:21.159944 140225696298816 spec.py:349] Evaluating on the test split.
I0207 18:05:23.833654 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:09:26.700640 140225696298816 submission_runner.py:408] Time since start: 14495.82s, 	Step: 24307, 	{'train/accuracy': 0.6397490501403809, 'train/loss': 1.9547765254974365, 'train/bleu': 30.85258301009002, 'validation/accuracy': 0.6600165963172913, 'validation/loss': 1.805528998374939, 'validation/bleu': 28.20852868398465, 'validation/num_examples': 3000, 'test/accuracy': 0.672105073928833, 'test/loss': 1.7438435554504395, 'test/bleu': 27.47958904209218, 'test/num_examples': 3003, 'score': 8426.640635728836, 'total_duration': 14495.81839632988, 'accumulated_submission_time': 8426.640635728836, 'accumulated_eval_time': 6068.142309427261, 'accumulated_logging_time': 0.2572338581085205}
I0207 18:09:26.718539 140050996819712 logging_writer.py:48] [24307] accumulated_eval_time=6068.142309, accumulated_logging_time=0.257234, accumulated_submission_time=8426.640636, global_step=24307, preemption_count=0, score=8426.640636, test/accuracy=0.672105, test/bleu=27.479589, test/loss=1.743844, test/num_examples=3003, total_duration=14495.818396, train/accuracy=0.639749, train/bleu=30.852583, train/loss=1.954777, validation/accuracy=0.660017, validation/bleu=28.208529, validation/loss=1.805529, validation/num_examples=3000
I0207 18:09:59.132493 140051005212416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.20736061036586761, loss=3.99454927444458
I0207 18:10:33.685868 140050996819712 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.19380849599838257, loss=4.00246000289917
I0207 18:11:08.279481 140051005212416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.2078525573015213, loss=4.030732154846191
I0207 18:11:42.862730 140050996819712 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.18445497751235962, loss=3.995079755783081
I0207 18:12:17.419081 140051005212416 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.17930540442466736, loss=4.00324821472168
I0207 18:12:52.003922 140050996819712 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.20435230433940887, loss=3.956235408782959
I0207 18:13:26.571445 140051005212416 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.18533426523208618, loss=3.948580026626587
I0207 18:14:01.139588 140050996819712 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.18465137481689453, loss=3.979632616043091
I0207 18:14:35.724050 140051005212416 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2089410275220871, loss=3.9932477474212646
I0207 18:15:10.281620 140050996819712 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2547197937965393, loss=4.0674052238464355
I0207 18:15:44.863125 140051005212416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.1779063642024994, loss=3.964552879333496
I0207 18:16:19.466673 140050996819712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.23584209382534027, loss=3.902623176574707
I0207 18:16:54.042791 140051005212416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2002442181110382, loss=3.9284565448760986
I0207 18:17:28.632332 140050996819712 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.2688232362270355, loss=4.007994174957275
I0207 18:18:03.226891 140051005212416 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.17953884601593018, loss=4.088260173797607
I0207 18:18:37.798716 140050996819712 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.20887045562267303, loss=3.920219898223877
I0207 18:19:12.380686 140051005212416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4442167580127716, loss=3.9406542778015137
I0207 18:19:46.969159 140050996819712 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.176712766289711, loss=3.9393086433410645
I0207 18:20:21.552850 140051005212416 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.17829127609729767, loss=4.003882884979248
I0207 18:20:56.122924 140050996819712 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.17857138812541962, loss=3.9631383419036865
I0207 18:21:30.668347 140051005212416 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.18987832963466644, loss=4.004236698150635
I0207 18:22:05.222253 140050996819712 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1756790727376938, loss=3.968136787414551
I0207 18:22:39.826363 140051005212416 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19777877628803253, loss=4.02070426940918
I0207 18:23:14.419785 140050996819712 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.1968599557876587, loss=3.974518299102783
I0207 18:23:26.946669 140225696298816 spec.py:321] Evaluating on the training split.
I0207 18:23:29.916036 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:28:08.756978 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 18:28:11.421680 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:31:55.323764 140225696298816 spec.py:349] Evaluating on the test split.
I0207 18:31:57.991860 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:35:01.526283 140225696298816 submission_runner.py:408] Time since start: 16030.64s, 	Step: 26738, 	{'train/accuracy': 0.6513376832008362, 'train/loss': 1.8955870866775513, 'train/bleu': 31.97804613904664, 'validation/accuracy': 0.6653606295585632, 'validation/loss': 1.7924036979675293, 'validation/bleu': 28.451939799484045, 'validation/num_examples': 3000, 'test/accuracy': 0.6745221018791199, 'test/loss': 1.7301757335662842, 'test/bleu': 28.200531420440008, 'test/num_examples': 3003, 'score': 9266.780019283295, 'total_duration': 16030.644038438797, 'accumulated_submission_time': 9266.780019283295, 'accumulated_eval_time': 6762.721871137619, 'accumulated_logging_time': 0.28625965118408203}
I0207 18:35:01.545326 140051005212416 logging_writer.py:48] [26738] accumulated_eval_time=6762.721871, accumulated_logging_time=0.286260, accumulated_submission_time=9266.780019, global_step=26738, preemption_count=0, score=9266.780019, test/accuracy=0.674522, test/bleu=28.200531, test/loss=1.730176, test/num_examples=3003, total_duration=16030.644038, train/accuracy=0.651338, train/bleu=31.978046, train/loss=1.895587, validation/accuracy=0.665361, validation/bleu=28.451940, validation/loss=1.792404, validation/num_examples=3000
I0207 18:35:23.213755 140050996819712 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.250260591506958, loss=3.9772849082946777
I0207 18:35:57.681359 140051005212416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.2854922115802765, loss=4.023392200469971
I0207 18:36:32.229430 140050996819712 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.17256109416484833, loss=3.9143059253692627
I0207 18:37:06.801232 140051005212416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.2047509104013443, loss=3.932358503341675
I0207 18:37:41.377259 140050996819712 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.18646542727947235, loss=3.936383008956909
I0207 18:38:15.963081 140051005212416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2493176907300949, loss=3.9866676330566406
I0207 18:38:50.525735 140050996819712 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.1922643631696701, loss=3.8941400051116943
I0207 18:39:25.124428 140051005212416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2351081520318985, loss=3.9994916915893555
I0207 18:39:59.695102 140050996819712 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.24171285331249237, loss=3.96696400642395
I0207 18:40:34.278943 140051005212416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.18084365129470825, loss=3.9515626430511475
I0207 18:41:08.852440 140050996819712 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2156832069158554, loss=3.9845762252807617
I0207 18:41:43.414097 140051005212416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.18905124068260193, loss=3.9589884281158447
I0207 18:42:18.016596 140050996819712 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3054163157939911, loss=3.98066782951355
I0207 18:42:52.596652 140051005212416 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.1951877474784851, loss=3.9550299644470215
I0207 18:43:27.159679 140050996819712 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2067188322544098, loss=3.9588911533355713
I0207 18:44:01.732025 140051005212416 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.20162664353847504, loss=3.9336111545562744
I0207 18:44:36.313085 140050996819712 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.23470619320869446, loss=3.9918644428253174
I0207 18:45:10.872762 140051005212416 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.2647562325000763, loss=3.9947731494903564
I0207 18:45:45.447415 140050996819712 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.20243825018405914, loss=3.986839532852173
I0207 18:46:20.013202 140051005212416 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2481151521205902, loss=3.99418306350708
I0207 18:46:54.582735 140050996819712 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1968873292207718, loss=3.9757027626037598
I0207 18:47:29.159813 140051005212416 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.18985401093959808, loss=3.977046251296997
I0207 18:48:03.733087 140050996819712 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.20327062904834747, loss=4.0707807540893555
I0207 18:48:38.281967 140051005212416 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.20834682881832123, loss=3.94330096244812
I0207 18:49:01.859011 140225696298816 spec.py:321] Evaluating on the training split.
I0207 18:49:04.836508 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:51:46.160856 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 18:51:48.828335 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:54:29.653496 140225696298816 spec.py:349] Evaluating on the test split.
I0207 18:54:32.329099 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 18:56:55.525926 140225696298816 submission_runner.py:408] Time since start: 17344.64s, 	Step: 29170, 	{'train/accuracy': 0.6469125747680664, 'train/loss': 1.9222787618637085, 'train/bleu': 32.02554933303594, 'validation/accuracy': 0.6672824621200562, 'validation/loss': 1.7752902507781982, 'validation/bleu': 28.697036519419203, 'validation/num_examples': 3000, 'test/accuracy': 0.6792516708374023, 'test/loss': 1.7129313945770264, 'test/bleu': 28.453572109704602, 'test/num_examples': 3003, 'score': 10107.006126642227, 'total_duration': 17344.64367866516, 'accumulated_submission_time': 10107.006126642227, 'accumulated_eval_time': 7236.388736963272, 'accumulated_logging_time': 0.3159620761871338}
I0207 18:56:55.545273 140050996819712 logging_writer.py:48] [29170] accumulated_eval_time=7236.388737, accumulated_logging_time=0.315962, accumulated_submission_time=10107.006127, global_step=29170, preemption_count=0, score=10107.006127, test/accuracy=0.679252, test/bleu=28.453572, test/loss=1.712931, test/num_examples=3003, total_duration=17344.643679, train/accuracy=0.646913, train/bleu=32.025549, train/loss=1.922279, validation/accuracy=0.667282, validation/bleu=28.697037, validation/loss=1.775290, validation/num_examples=3000
I0207 18:57:06.223895 140051005212416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4289686381816864, loss=3.939378023147583
I0207 18:57:40.638386 140050996819712 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.22600828111171722, loss=3.9667704105377197
I0207 18:58:15.187219 140051005212416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.19185853004455566, loss=3.956920862197876
I0207 18:58:49.767203 140050996819712 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.19597597420215607, loss=3.924144983291626
I0207 18:59:24.341084 140051005212416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19518792629241943, loss=3.9311227798461914
I0207 18:59:58.921321 140050996819712 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2056565284729004, loss=3.9913971424102783
I0207 19:00:33.511717 140051005212416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.21729601919651031, loss=3.956390142440796
I0207 19:01:08.069336 140050996819712 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.23865282535552979, loss=3.9132049083709717
I0207 19:01:42.644477 140051005212416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.22397486865520477, loss=3.9321746826171875
I0207 19:02:17.196290 140050996819712 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19587606191635132, loss=3.8952136039733887
I0207 19:02:51.766309 140051005212416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.30584704875946045, loss=3.9982950687408447
I0207 19:03:26.326343 140050996819712 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3994186818599701, loss=3.925076961517334
I0207 19:04:00.882260 140051005212416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.19055034220218658, loss=3.913149833679199
I0207 19:04:35.445376 140050996819712 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2752120792865753, loss=3.9736528396606445
I0207 19:05:10.013097 140051005212416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.21347524225711823, loss=3.9407849311828613
I0207 19:05:44.615597 140050996819712 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.2068636119365692, loss=3.9177403450012207
I0207 19:06:19.200929 140051005212416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.2901822030544281, loss=3.9573981761932373
I0207 19:06:53.771081 140050996819712 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.2351548969745636, loss=3.8522260189056396
I0207 19:07:28.358470 140051005212416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.2350904494524002, loss=3.921663999557495
I0207 19:08:02.921865 140050996819712 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3025124967098236, loss=3.9113974571228027
I0207 19:08:37.498631 140051005212416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.23539920151233673, loss=3.9046430587768555
I0207 19:09:12.069365 140050996819712 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3295905292034149, loss=3.944239377975464
I0207 19:09:46.659857 140051005212416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.19157660007476807, loss=3.8759872913360596
I0207 19:10:21.242974 140050996819712 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3948100805282593, loss=3.924738883972168
I0207 19:10:55.831221 140051005212416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.21837474405765533, loss=3.9711015224456787
I0207 19:10:55.837950 140225696298816 spec.py:321] Evaluating on the training split.
I0207 19:10:58.527138 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:13:43.983325 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 19:13:46.653865 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:16:23.026772 140225696298816 spec.py:349] Evaluating on the test split.
I0207 19:16:25.711842 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:18:45.371436 140225696298816 submission_runner.py:408] Time since start: 18654.49s, 	Step: 31601, 	{'train/accuracy': 0.6717249155044556, 'train/loss': 1.7598832845687866, 'train/bleu': 33.27960037029534, 'validation/accuracy': 0.6671584844589233, 'validation/loss': 1.7688673734664917, 'validation/bleu': 29.136006467228604, 'validation/num_examples': 3000, 'test/accuracy': 0.679449200630188, 'test/loss': 1.6971747875213623, 'test/bleu': 28.461407126008957, 'test/num_examples': 3003, 'score': 10947.211053609848, 'total_duration': 18654.48917913437, 'accumulated_submission_time': 10947.211053609848, 'accumulated_eval_time': 7705.9221296310425, 'accumulated_logging_time': 0.3449394702911377}
I0207 19:18:45.391295 140050996819712 logging_writer.py:48] [31601] accumulated_eval_time=7705.922130, accumulated_logging_time=0.344939, accumulated_submission_time=10947.211054, global_step=31601, preemption_count=0, score=10947.211054, test/accuracy=0.679449, test/bleu=28.461407, test/loss=1.697175, test/num_examples=3003, total_duration=18654.489179, train/accuracy=0.671725, train/bleu=33.279600, train/loss=1.759883, validation/accuracy=0.667158, validation/bleu=29.136006, validation/loss=1.768867, validation/num_examples=3000
I0207 19:19:19.860374 140051005212416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.20572268962860107, loss=3.938089609146118
I0207 19:19:54.388011 140050996819712 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.20864716172218323, loss=3.9614925384521484
I0207 19:20:28.946320 140051005212416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.26489755511283875, loss=3.964512586593628
I0207 19:21:03.523828 140050996819712 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.21515721082687378, loss=3.9376721382141113
I0207 19:21:38.082698 140051005212416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.2206863909959793, loss=3.9807047843933105
I0207 19:22:12.656532 140050996819712 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.20752012729644775, loss=3.9160139560699463
I0207 19:22:47.247137 140051005212416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.21947415173053741, loss=3.8904848098754883
I0207 19:23:21.820155 140050996819712 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2811065912246704, loss=3.915585517883301
I0207 19:23:56.401950 140051005212416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2096584290266037, loss=3.899996042251587
I0207 19:24:30.975162 140050996819712 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.22299736738204956, loss=3.8732962608337402
I0207 19:25:05.555390 140051005212416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.26605185866355896, loss=3.966334104537964
I0207 19:25:40.134868 140050996819712 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.21103598177433014, loss=3.968388795852661
I0207 19:26:14.710630 140051005212416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.21723638474941254, loss=3.9591031074523926
I0207 19:26:49.282665 140050996819712 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2107858806848526, loss=3.936506509780884
I0207 19:27:23.871718 140051005212416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.22429172694683075, loss=3.944999933242798
I0207 19:27:58.438459 140050996819712 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.2075054794549942, loss=3.9428043365478516
I0207 19:28:33.012542 140051005212416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3114483058452606, loss=3.937591075897217
I0207 19:29:07.610265 140050996819712 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.23731312155723572, loss=3.9863674640655518
I0207 19:29:42.184058 140051005212416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.21046647429466248, loss=3.9472389221191406
I0207 19:30:16.770628 140050996819712 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.25179773569107056, loss=3.927990436553955
I0207 19:30:51.369883 140051005212416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.21978673338890076, loss=3.9274752140045166
I0207 19:31:25.930147 140050996819712 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.20912159979343414, loss=3.9466958045959473
I0207 19:32:00.511294 140051005212416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.227418452501297, loss=3.86664080619812
I0207 19:32:35.115591 140050996819712 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.29187676310539246, loss=3.9468801021575928
I0207 19:32:45.561548 140225696298816 spec.py:321] Evaluating on the training split.
I0207 19:32:48.540636 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:35:47.587878 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 19:35:50.268875 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:38:34.392711 140225696298816 spec.py:349] Evaluating on the test split.
I0207 19:38:37.084944 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:41:19.404762 140225696298816 submission_runner.py:408] Time since start: 20008.52s, 	Step: 34032, 	{'train/accuracy': 0.6559919714927673, 'train/loss': 1.8558120727539062, 'train/bleu': 32.682312093461405, 'validation/accuracy': 0.6694275140762329, 'validation/loss': 1.746963381767273, 'validation/bleu': 29.291129779033984, 'validation/num_examples': 3000, 'test/accuracy': 0.6810644268989563, 'test/loss': 1.6823828220367432, 'test/bleu': 28.599915373846635, 'test/num_examples': 3003, 'score': 11787.294679403305, 'total_duration': 20008.522481679916, 'accumulated_submission_time': 11787.294679403305, 'accumulated_eval_time': 8219.765256166458, 'accumulated_logging_time': 0.3742711544036865}
I0207 19:41:19.424389 140051005212416 logging_writer.py:48] [34032] accumulated_eval_time=8219.765256, accumulated_logging_time=0.374271, accumulated_submission_time=11787.294679, global_step=34032, preemption_count=0, score=11787.294679, test/accuracy=0.681064, test/bleu=28.599915, test/loss=1.682383, test/num_examples=3003, total_duration=20008.522482, train/accuracy=0.655992, train/bleu=32.682312, train/loss=1.855812, validation/accuracy=0.669428, validation/bleu=29.291130, validation/loss=1.746963, validation/num_examples=3000
I0207 19:41:43.207243 140050996819712 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.23098640143871307, loss=3.946394205093384
I0207 19:42:17.723806 140051005212416 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.2645382285118103, loss=3.9597463607788086
I0207 19:42:52.329510 140050996819712 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2316230684518814, loss=3.889831781387329
I0207 19:43:26.914882 140051005212416 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20178453624248505, loss=3.8732471466064453
I0207 19:44:01.515828 140050996819712 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.23428459465503693, loss=3.923565149307251
I0207 19:44:36.117363 140051005212416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.1278652548789978, loss=7.855119705200195
I0207 19:45:10.649108 140050996819712 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3754718601703644, loss=5.905570030212402
I0207 19:45:45.206464 140051005212416 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.34252825379371643, loss=5.604113578796387
I0207 19:46:19.769055 140050996819712 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.40060126781463623, loss=5.523948669433594
I0207 19:46:54.332361 140051005212416 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.1821861267089844, loss=4.789105415344238
I0207 19:47:28.950690 140050996819712 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.22661170363426208, loss=4.06375789642334
I0207 19:48:03.533429 140051005212416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.19646070897579193, loss=3.928410291671753
I0207 19:48:38.133447 140050996819712 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.21893487870693207, loss=3.9980623722076416
I0207 19:49:12.725919 140051005212416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.21299605071544647, loss=3.9360015392303467
I0207 19:49:47.334676 140050996819712 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2101738154888153, loss=3.9255950450897217
I0207 19:50:21.903024 140051005212416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.21561957895755768, loss=3.882049560546875
I0207 19:50:56.486479 140050996819712 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.20425856113433838, loss=3.9221489429473877
I0207 19:51:31.058894 140051005212416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.2217329740524292, loss=3.933471441268921
I0207 19:52:05.635820 140050996819712 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.21545957028865814, loss=3.892286539077759
I0207 19:52:40.238840 140051005212416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.19853633642196655, loss=3.9522743225097656
I0207 19:53:14.865387 140050996819712 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2311497926712036, loss=3.9628283977508545
I0207 19:53:49.462482 140051005212416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.247664675116539, loss=3.9075801372528076
I0207 19:54:24.022339 140050996819712 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2271370142698288, loss=3.906926155090332
I0207 19:54:58.585361 140051005212416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.21492598950862885, loss=3.9742512702941895
I0207 19:55:19.749478 140225696298816 spec.py:321] Evaluating on the training split.
I0207 19:55:22.725462 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 19:58:39.302516 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 19:58:41.974467 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:02:30.936072 140225696298816 spec.py:349] Evaluating on the test split.
I0207 20:02:33.609793 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:05:34.470884 140225696298816 submission_runner.py:408] Time since start: 21463.59s, 	Step: 36463, 	{'train/accuracy': 0.6533215045928955, 'train/loss': 1.8728480339050293, 'train/bleu': 32.04000187532934, 'validation/accuracy': 0.6712626218795776, 'validation/loss': 1.741132140159607, 'validation/bleu': 28.41640521392595, 'validation/num_examples': 3000, 'test/accuracy': 0.682819128036499, 'test/loss': 1.669180154800415, 'test/bleu': 29.095199098370255, 'test/num_examples': 3003, 'score': 12627.530505895615, 'total_duration': 21463.58862900734, 'accumulated_submission_time': 12627.530505895615, 'accumulated_eval_time': 8834.486620664597, 'accumulated_logging_time': 0.40384364128112793}
I0207 20:05:34.490958 140050996819712 logging_writer.py:48] [36463] accumulated_eval_time=8834.486621, accumulated_logging_time=0.403844, accumulated_submission_time=12627.530506, global_step=36463, preemption_count=0, score=12627.530506, test/accuracy=0.682819, test/bleu=29.095199, test/loss=1.669180, test/num_examples=3003, total_duration=21463.588629, train/accuracy=0.653322, train/bleu=32.040002, train/loss=1.872848, validation/accuracy=0.671263, validation/bleu=28.416405, validation/loss=1.741132, validation/num_examples=3000
I0207 20:05:47.575967 140051005212416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2101924568414688, loss=3.8612098693847656
I0207 20:06:22.035441 140050996819712 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.22599156200885773, loss=3.9492177963256836
I0207 20:06:56.586579 140051005212416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.24321888387203217, loss=3.8858940601348877
I0207 20:07:31.156223 140050996819712 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.22031840682029724, loss=3.942471981048584
I0207 20:08:05.723759 140051005212416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.23158656060695648, loss=3.946457862854004
I0207 20:08:40.308456 140050996819712 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2799569368362427, loss=3.8983287811279297
I0207 20:09:14.891571 140051005212416 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.21955151855945587, loss=3.8929591178894043
I0207 20:09:49.458870 140050996819712 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2788994610309601, loss=3.943071126937866
I0207 20:10:24.029126 140051005212416 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2989702820777893, loss=3.903714179992676
I0207 20:10:58.593211 140050996819712 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.2889977693557739, loss=3.928821563720703
I0207 20:11:33.181267 140051005212416 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.22380153834819794, loss=3.9377360343933105
I0207 20:12:07.764635 140050996819712 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2739070653915405, loss=3.9115986824035645
I0207 20:12:42.369575 140051005212416 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.22787970304489136, loss=3.94926118850708
I0207 20:13:16.936399 140050996819712 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.1999036818742752, loss=3.876295566558838
I0207 20:13:51.495975 140051005212416 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.25914180278778076, loss=3.9598324298858643
I0207 20:14:26.084664 140050996819712 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.21594947576522827, loss=3.9059998989105225
I0207 20:15:00.636011 140051005212416 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.212154820561409, loss=3.8789143562316895
I0207 20:15:35.220592 140050996819712 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.23411035537719727, loss=3.918844223022461
I0207 20:16:09.811761 140051005212416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21368514001369476, loss=3.9095144271850586
I0207 20:16:44.404280 140050996819712 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.27446267008781433, loss=3.881192207336426
I0207 20:17:18.968143 140051005212416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.23223340511322021, loss=3.8939366340637207
I0207 20:17:53.547120 140050996819712 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2112911343574524, loss=3.955033302307129
I0207 20:18:28.117499 140051005212416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2257584035396576, loss=3.8742754459381104
I0207 20:19:02.687553 140050996819712 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.24507302045822144, loss=3.889057159423828
I0207 20:19:34.561604 140225696298816 spec.py:321] Evaluating on the training split.
I0207 20:19:37.525104 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:22:30.151603 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 20:22:32.847605 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:25:01.374949 140225696298816 spec.py:349] Evaluating on the test split.
I0207 20:25:04.057689 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:27:26.297497 140225696298816 submission_runner.py:408] Time since start: 22775.42s, 	Step: 38894, 	{'train/accuracy': 0.6603764891624451, 'train/loss': 1.8058222532272339, 'train/bleu': 32.47630884958098, 'validation/accuracy': 0.6701342463493347, 'validation/loss': 1.730035662651062, 'validation/bleu': 29.079701147488368, 'validation/num_examples': 3000, 'test/accuracy': 0.6835280060768127, 'test/loss': 1.6568653583526611, 'test/bleu': 28.82156068715825, 'test/num_examples': 3003, 'score': 13467.513338565826, 'total_duration': 22775.415242671967, 'accumulated_submission_time': 13467.513338565826, 'accumulated_eval_time': 9306.22245168686, 'accumulated_logging_time': 0.4338409900665283}
I0207 20:27:26.318063 140051005212416 logging_writer.py:48] [38894] accumulated_eval_time=9306.222452, accumulated_logging_time=0.433841, accumulated_submission_time=13467.513339, global_step=38894, preemption_count=0, score=13467.513339, test/accuracy=0.683528, test/bleu=28.821561, test/loss=1.656865, test/num_examples=3003, total_duration=22775.415243, train/accuracy=0.660376, train/bleu=32.476309, train/loss=1.805822, validation/accuracy=0.670134, validation/bleu=29.079701, validation/loss=1.730036, validation/num_examples=3000
I0207 20:27:28.762918 140050996819712 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.2174340933561325, loss=3.9448986053466797
I0207 20:28:03.222641 140051005212416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2449890673160553, loss=3.8554530143737793
I0207 20:28:37.761219 140050996819712 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.21253588795661926, loss=3.9098048210144043
I0207 20:29:12.323034 140051005212416 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.23734162747859955, loss=3.935443639755249
I0207 20:29:46.907490 140050996819712 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3101068437099457, loss=3.9901719093322754
I0207 20:30:21.476942 140051005212416 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.24153675138950348, loss=3.904541015625
I0207 20:30:56.054275 140050996819712 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.22835201025009155, loss=3.8855066299438477
I0207 20:31:30.630216 140051005212416 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.24072875082492828, loss=3.8923447132110596
I0207 20:32:05.218860 140050996819712 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.22945120930671692, loss=3.902866840362549
I0207 20:32:39.801852 140051005212416 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.2802189886569977, loss=3.8946805000305176
I0207 20:33:14.401073 140050996819712 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.24222688376903534, loss=3.8514106273651123
I0207 20:33:48.993375 140051005212416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.24094988405704498, loss=3.8776955604553223
I0207 20:34:23.575627 140050996819712 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.25304749608039856, loss=3.8642430305480957
I0207 20:34:58.153283 140051005212416 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2233457863330841, loss=3.8861794471740723
I0207 20:35:32.722919 140050996819712 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.23683643341064453, loss=3.943883180618286
I0207 20:36:07.288158 140051005212416 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2928067147731781, loss=3.9546265602111816
I0207 20:36:41.873379 140050996819712 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.315530925989151, loss=3.9006667137145996
I0207 20:37:16.453782 140051005212416 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.25142422318458557, loss=3.856480360031128
I0207 20:37:51.029647 140050996819712 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.2599756717681885, loss=3.9020333290100098
I0207 20:38:25.602736 140051005212416 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.2360975444316864, loss=3.9258172512054443
I0207 20:39:00.170708 140050996819712 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.2428494244813919, loss=3.978713035583496
I0207 20:39:34.721662 140051005212416 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2909913659095764, loss=3.8700621128082275
I0207 20:40:09.306547 140050996819712 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.23899908363819122, loss=3.956000328063965
I0207 20:40:43.897185 140051005212416 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.2074674516916275, loss=3.8602044582366943
I0207 20:41:18.486357 140050996819712 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2402448058128357, loss=3.9239094257354736
I0207 20:41:26.512596 140225696298816 spec.py:321] Evaluating on the training split.
I0207 20:41:29.482375 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:44:22.835625 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 20:44:25.515035 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:47:08.454906 140225696298816 spec.py:349] Evaluating on the test split.
I0207 20:47:11.135703 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 20:49:52.596802 140225696298816 submission_runner.py:408] Time since start: 24121.71s, 	Step: 41325, 	{'train/accuracy': 0.6599708795547485, 'train/loss': 1.8201743364334106, 'train/bleu': 32.381106417776536, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.712181806564331, 'validation/bleu': 29.184639902057, 'validation/num_examples': 3000, 'test/accuracy': 0.6861310005187988, 'test/loss': 1.6359333992004395, 'test/bleu': 29.236897732583365, 'test/num_examples': 3003, 'score': 14307.620192050934, 'total_duration': 24121.71455836296, 'accumulated_submission_time': 14307.620192050934, 'accumulated_eval_time': 9812.306602954865, 'accumulated_logging_time': 0.464618444442749}
I0207 20:49:52.617191 140051005212416 logging_writer.py:48] [41325] accumulated_eval_time=9812.306603, accumulated_logging_time=0.464618, accumulated_submission_time=14307.620192, global_step=41325, preemption_count=0, score=14307.620192, test/accuracy=0.686131, test/bleu=29.236898, test/loss=1.635933, test/num_examples=3003, total_duration=24121.714558, train/accuracy=0.659971, train/bleu=32.381106, train/loss=1.820174, validation/accuracy=0.672874, validation/bleu=29.184640, validation/loss=1.712182, validation/num_examples=3000
I0207 20:50:18.800128 140050996819712 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.28746601939201355, loss=3.9160139560699463
I0207 20:50:53.290833 140051005212416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2164500653743744, loss=3.8039450645446777
I0207 20:51:27.867253 140050996819712 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.24768370389938354, loss=3.9506890773773193
I0207 20:52:02.437064 140051005212416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2330581694841385, loss=3.8551204204559326
I0207 20:52:37.005382 140050996819712 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2999415397644043, loss=3.9570369720458984
I0207 20:53:11.561641 140051005212416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.2331000119447708, loss=3.9793026447296143
I0207 20:53:46.139776 140050996819712 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.23890051245689392, loss=3.88938307762146
I0207 20:54:20.728512 140051005212416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2536599934101105, loss=3.981353998184204
I0207 20:54:55.307669 140050996819712 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.22734256088733673, loss=3.9199957847595215
I0207 20:55:29.886470 140051005212416 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.24198542535305023, loss=3.9591407775878906
I0207 20:56:04.461591 140050996819712 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2646811306476593, loss=3.879302740097046
I0207 20:56:39.030545 140051005212416 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2764478325843811, loss=3.889746904373169
I0207 20:57:13.615258 140050996819712 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.25532740354537964, loss=3.906628131866455
I0207 20:57:48.193434 140051005212416 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.24622420966625214, loss=3.863234043121338
I0207 20:58:22.770495 140050996819712 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.2520886957645416, loss=3.912785053253174
I0207 20:58:57.339525 140051005212416 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2369379699230194, loss=3.8949875831604004
I0207 20:59:31.941723 140050996819712 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.23889108002185822, loss=3.9248857498168945
I0207 21:00:06.525186 140051005212416 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.30650460720062256, loss=3.895709991455078
I0207 21:00:41.085795 140050996819712 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.23289266228675842, loss=3.855283260345459
I0207 21:01:15.668345 140051005212416 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2221342921257019, loss=3.8832366466522217
I0207 21:01:50.251792 140050996819712 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.26498106122016907, loss=3.8712666034698486
I0207 21:02:24.809801 140051005212416 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2995139956474304, loss=3.862745761871338
I0207 21:02:59.396567 140050996819712 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.29154908657073975, loss=3.881549119949341
I0207 21:03:33.990406 140051005212416 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2487080693244934, loss=3.875669240951538
I0207 21:03:52.734477 140225696298816 spec.py:321] Evaluating on the training split.
I0207 21:03:55.706462 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:07:17.936734 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 21:07:20.621652 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:10:41.153054 140225696298816 spec.py:349] Evaluating on the test split.
I0207 21:10:43.819679 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:13:24.107917 140225696298816 submission_runner.py:408] Time since start: 25533.23s, 	Step: 43756, 	{'train/accuracy': 0.6520457863807678, 'train/loss': 1.86770761013031, 'train/bleu': 32.20933811417118, 'validation/accuracy': 0.6751186847686768, 'validation/loss': 1.7147032022476196, 'validation/bleu': 29.250316554246652, 'validation/num_examples': 3000, 'test/accuracy': 0.6876997351646423, 'test/loss': 1.6349014043807983, 'test/bleu': 29.037590233653013, 'test/num_examples': 3003, 'score': 15147.648758888245, 'total_duration': 25533.225674152374, 'accumulated_submission_time': 15147.648758888245, 'accumulated_eval_time': 10383.679992437363, 'accumulated_logging_time': 0.4959902763366699}
I0207 21:13:24.128399 140050996819712 logging_writer.py:48] [43756] accumulated_eval_time=10383.679992, accumulated_logging_time=0.495990, accumulated_submission_time=15147.648759, global_step=43756, preemption_count=0, score=15147.648759, test/accuracy=0.687700, test/bleu=29.037590, test/loss=1.634901, test/num_examples=3003, total_duration=25533.225674, train/accuracy=0.652046, train/bleu=32.209338, train/loss=1.867708, validation/accuracy=0.675119, validation/bleu=29.250317, validation/loss=1.714703, validation/num_examples=3000
I0207 21:13:39.659616 140051005212416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.28245455026626587, loss=3.9377543926239014
I0207 21:14:14.105320 140050996819712 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.03697964921593666, loss=7.984884738922119
I0207 21:14:48.549170 140051005212416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3696148097515106, loss=7.299768447875977
I0207 21:15:23.095703 140050996819712 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.8531296253204346, loss=5.980447769165039
I0207 21:15:57.625371 140051005212416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.5308032631874084, loss=5.545891761779785
I0207 21:16:32.187484 140050996819712 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.42867544293403625, loss=5.543580055236816
I0207 21:17:06.735625 140051005212416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.39949020743370056, loss=5.477238178253174
I0207 21:17:41.290608 140050996819712 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.739738404750824, loss=5.446036338806152
I0207 21:18:15.855561 140051005212416 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.5603816509246826, loss=5.4472808837890625
I0207 21:18:50.386765 140050996819712 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.4155772626399994, loss=5.635222434997559
I0207 21:19:24.870981 140051005212416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.6656765341758728, loss=5.470024108886719
I0207 21:19:59.407357 140050996819712 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.9482922554016113, loss=5.405342102050781
I0207 21:20:33.924944 140051005212416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.9275464415550232, loss=5.132809638977051
I0207 21:21:08.496495 140050996819712 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3224719166755676, loss=4.008214473724365
I0207 21:21:43.081460 140051005212416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.22982807457447052, loss=3.9385201930999756
I0207 21:22:17.656233 140050996819712 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.22918470203876495, loss=3.8996331691741943
I0207 21:22:52.245017 140051005212416 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.23513491451740265, loss=3.932713747024536
I0207 21:23:26.817745 140050996819712 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.24408981204032898, loss=3.9002525806427
I0207 21:24:01.376743 140051005212416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.21874041855335236, loss=3.8872122764587402
I0207 21:24:35.940624 140050996819712 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.23368361592292786, loss=3.929201126098633
I0207 21:25:10.517039 140051005212416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2964335083961487, loss=3.8954360485076904
I0207 21:25:45.098093 140050996819712 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2407379448413849, loss=3.9107468128204346
I0207 21:26:19.666951 140051005212416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.24357494711875916, loss=3.88984751701355
I0207 21:26:54.212228 140050996819712 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22835572063922882, loss=3.934671401977539
I0207 21:27:24.350708 140225696298816 spec.py:321] Evaluating on the training split.
I0207 21:27:27.330873 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:30:19.302717 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 21:30:21.999441 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:32:56.893033 140225696298816 spec.py:349] Evaluating on the test split.
I0207 21:32:59.579291 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:35:23.592849 140225696298816 submission_runner.py:408] Time since start: 26852.71s, 	Step: 46189, 	{'train/accuracy': 0.6639376878738403, 'train/loss': 1.7865409851074219, 'train/bleu': 32.48354802392069, 'validation/accuracy': 0.6739283800125122, 'validation/loss': 1.713200569152832, 'validation/bleu': 29.521032934089877, 'validation/num_examples': 3000, 'test/accuracy': 0.6856428980827332, 'test/loss': 1.6498336791992188, 'test/bleu': 29.08447388429547, 'test/num_examples': 3003, 'score': 15987.783904314041, 'total_duration': 26852.710567712784, 'accumulated_submission_time': 15987.783904314041, 'accumulated_eval_time': 10862.922046422958, 'accumulated_logging_time': 0.5261423587799072}
I0207 21:35:23.615006 140051005212416 logging_writer.py:48] [46189] accumulated_eval_time=10862.922046, accumulated_logging_time=0.526142, accumulated_submission_time=15987.783904, global_step=46189, preemption_count=0, score=15987.783904, test/accuracy=0.685643, test/bleu=29.084474, test/loss=1.649834, test/num_examples=3003, total_duration=26852.710568, train/accuracy=0.663938, train/bleu=32.483548, train/loss=1.786541, validation/accuracy=0.673928, validation/bleu=29.521033, validation/loss=1.713201, validation/num_examples=3000
I0207 21:35:27.770402 140050996819712 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.28408193588256836, loss=3.901524782180786
I0207 21:36:02.226162 140051005212416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3311431109905243, loss=3.8409881591796875
I0207 21:36:36.719071 140050996819712 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.24860860407352448, loss=3.832120180130005
I0207 21:37:11.268522 140051005212416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.24101419746875763, loss=3.988161325454712
I0207 21:37:45.822613 140050996819712 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2882416248321533, loss=3.8981387615203857
I0207 21:38:20.367246 140051005212416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2708723843097687, loss=3.899759292602539
I0207 21:38:54.929439 140050996819712 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.24125809967517853, loss=3.886678695678711
I0207 21:39:29.506122 140051005212416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2737136483192444, loss=3.853278875350952
I0207 21:40:04.073806 140050996819712 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.23494896292686462, loss=3.832458972930908
I0207 21:40:38.662122 140051005212416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.22370436787605286, loss=3.900120496749878
I0207 21:41:13.239778 140050996819712 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2751120924949646, loss=3.9210681915283203
I0207 21:41:47.801594 140051005212416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2388044148683548, loss=3.844114303588867
I0207 21:42:22.382994 140050996819712 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.23347221314907074, loss=3.932497262954712
I0207 21:42:56.964895 140051005212416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2700099050998688, loss=3.8846497535705566
I0207 21:43:31.543613 140050996819712 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2706993520259857, loss=3.8347437381744385
I0207 21:44:06.137027 140051005212416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.31139278411865234, loss=3.867490530014038
I0207 21:44:40.705685 140050996819712 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.34706515073776245, loss=3.8974192142486572
I0207 21:45:15.280472 140051005212416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2939334809780121, loss=3.8889670372009277
I0207 21:45:49.886323 140050996819712 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.26237860321998596, loss=3.8759517669677734
I0207 21:46:24.450543 140051005212416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.24350020289421082, loss=3.897035837173462
I0207 21:46:59.044165 140050996819712 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2485782653093338, loss=3.932194948196411
I0207 21:47:33.606281 140051005212416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2356782853603363, loss=3.836246967315674
I0207 21:48:08.162178 140050996819712 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.308653324842453, loss=3.843642473220825
I0207 21:48:42.717005 140051005212416 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.23965942859649658, loss=3.886688470840454
I0207 21:49:17.302246 140050996819712 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.30426302552223206, loss=3.922461986541748
I0207 21:49:23.599639 140225696298816 spec.py:321] Evaluating on the training split.
I0207 21:49:26.578463 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:52:14.801008 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 21:52:17.476569 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:54:54.600655 140225696298816 spec.py:349] Evaluating on the test split.
I0207 21:54:57.283553 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 21:57:17.317802 140225696298816 submission_runner.py:408] Time since start: 28166.44s, 	Step: 48620, 	{'train/accuracy': 0.6577057242393494, 'train/loss': 1.8336910009384155, 'train/bleu': 32.11022349497067, 'validation/accuracy': 0.6760362386703491, 'validation/loss': 1.7174301147460938, 'validation/bleu': 29.529682871449634, 'validation/num_examples': 3000, 'test/accuracy': 0.6883621215820312, 'test/loss': 1.642948031425476, 'test/bleu': 29.228574269408423, 'test/num_examples': 3003, 'score': 16827.680357694626, 'total_duration': 28166.43555521965, 'accumulated_submission_time': 16827.680357694626, 'accumulated_eval_time': 11336.640148878098, 'accumulated_logging_time': 0.5586118698120117}
I0207 21:57:17.340033 140051005212416 logging_writer.py:48] [48620] accumulated_eval_time=11336.640149, accumulated_logging_time=0.558612, accumulated_submission_time=16827.680358, global_step=48620, preemption_count=0, score=16827.680358, test/accuracy=0.688362, test/bleu=29.228574, test/loss=1.642948, test/num_examples=3003, total_duration=28166.435555, train/accuracy=0.657706, train/bleu=32.110223, train/loss=1.833691, validation/accuracy=0.676036, validation/bleu=29.529683, validation/loss=1.717430, validation/num_examples=3000
I0207 21:57:45.211753 140050996819712 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2899274528026581, loss=3.8848633766174316
I0207 21:58:19.719229 140051005212416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.24876858294010162, loss=3.8488564491271973
I0207 21:58:54.284055 140050996819712 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.23433762788772583, loss=3.865361213684082
I0207 21:59:28.842801 140051005212416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3414131999015808, loss=3.830813407897949
I0207 22:00:03.443708 140050996819712 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.24560654163360596, loss=3.912461042404175
I0207 22:00:38.051180 140051005212416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2595129907131195, loss=3.805417537689209
I0207 22:01:12.635306 140050996819712 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.27789396047592163, loss=3.892666816711426
I0207 22:01:47.245353 140051005212416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.24661222100257874, loss=3.861297369003296
I0207 22:02:21.811926 140050996819712 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.23107147216796875, loss=3.897739887237549
I0207 22:02:56.382840 140051005212416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.26178640127182007, loss=3.871774911880493
I0207 22:03:30.978967 140050996819712 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2848986089229584, loss=3.875814199447632
I0207 22:04:05.579849 140051005212416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.2675875723361969, loss=3.8876214027404785
I0207 22:04:40.138407 140050996819712 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.25757554173469543, loss=3.9251463413238525
I0207 22:05:14.709835 140051005212416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.25139182806015015, loss=3.85465669631958
I0207 22:05:49.269681 140050996819712 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3054822087287903, loss=3.8803186416625977
I0207 22:06:23.835510 140051005212416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2621081471443176, loss=3.851181983947754
I0207 22:06:58.393764 140050996819712 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2384754717350006, loss=3.859985589981079
I0207 22:07:32.974138 140051005212416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.24549366533756256, loss=3.9171981811523438
I0207 22:08:07.534041 140050996819712 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.23150454461574554, loss=3.8663077354431152
I0207 22:08:42.121670 140051005212416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.23230230808258057, loss=3.8290178775787354
I0207 22:09:16.700153 140050996819712 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.23112793266773224, loss=3.9285237789154053
I0207 22:09:51.283052 140051005212416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.24977804720401764, loss=3.8661105632781982
I0207 22:10:25.844640 140050996819712 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3039417862892151, loss=3.939455032348633
I0207 22:11:00.418201 140051005212416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.30750295519828796, loss=3.86314058303833
I0207 22:11:17.432268 140225696298816 spec.py:321] Evaluating on the training split.
I0207 22:11:20.410618 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:14:13.730154 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 22:14:16.413666 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:16:46.645111 140225696298816 spec.py:349] Evaluating on the test split.
I0207 22:16:49.330011 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:19:03.462338 140225696298816 submission_runner.py:408] Time since start: 29472.58s, 	Step: 51051, 	{'train/accuracy': 0.6678268909454346, 'train/loss': 1.75491201877594, 'train/bleu': 33.25293797178625, 'validation/accuracy': 0.6759990453720093, 'validation/loss': 1.6939091682434082, 'validation/bleu': 29.552817707166724, 'validation/num_examples': 3000, 'test/accuracy': 0.6865725517272949, 'test/loss': 1.620548129081726, 'test/bleu': 28.930613904916136, 'test/num_examples': 3003, 'score': 17667.6850566864, 'total_duration': 29472.580087661743, 'accumulated_submission_time': 17667.6850566864, 'accumulated_eval_time': 11802.670159101486, 'accumulated_logging_time': 0.5905261039733887}
I0207 22:19:03.483175 140050996819712 logging_writer.py:48] [51051] accumulated_eval_time=11802.670159, accumulated_logging_time=0.590526, accumulated_submission_time=17667.685057, global_step=51051, preemption_count=0, score=17667.685057, test/accuracy=0.686573, test/bleu=28.930614, test/loss=1.620548, test/num_examples=3003, total_duration=29472.580088, train/accuracy=0.667827, train/bleu=33.252938, train/loss=1.754912, validation/accuracy=0.675999, validation/bleu=29.552818, validation/loss=1.693909, validation/num_examples=3000
I0207 22:19:20.697491 140051005212416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23039355874061584, loss=3.9019083976745605
I0207 22:19:55.136375 140050996819712 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3187501132488251, loss=3.924741744995117
I0207 22:20:29.683360 140051005212416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.25153273344039917, loss=3.8875176906585693
I0207 22:21:04.245946 140050996819712 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.29653751850128174, loss=3.863389730453491
I0207 22:21:38.817513 140051005212416 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2642882168292999, loss=3.8529419898986816
I0207 22:22:13.381742 140050996819712 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3375265598297119, loss=3.876390218734741
I0207 22:22:47.960209 140051005212416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2689485251903534, loss=3.8506062030792236
I0207 22:23:22.521330 140050996819712 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2882454991340637, loss=3.9431257247924805
I0207 22:23:57.097785 140051005212416 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.22878921031951904, loss=3.825143575668335
I0207 22:24:31.672068 140050996819712 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2646961212158203, loss=3.809255599975586
I0207 22:25:06.234114 140051005212416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.27889278531074524, loss=3.8834142684936523
I0207 22:25:40.800130 140050996819712 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.25808173418045044, loss=3.876121997833252
I0207 22:26:15.364112 140051005212416 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.2946462631225586, loss=3.8447351455688477
I0207 22:26:49.924625 140050996819712 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.24412381649017334, loss=3.881608486175537
I0207 22:27:24.519770 140051005212416 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.24483919143676758, loss=3.8533456325531006
I0207 22:27:59.106887 140050996819712 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3389495611190796, loss=3.8215579986572266
I0207 22:28:33.697414 140051005212416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.25931793451309204, loss=3.8489749431610107
I0207 22:29:08.252240 140050996819712 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.257035493850708, loss=3.8975555896759033
I0207 22:29:42.802620 140051005212416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.26892125606536865, loss=3.9006378650665283
I0207 22:30:17.376550 140050996819712 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.25602012872695923, loss=3.8712010383605957
I0207 22:30:51.940890 140051005212416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.23304246366024017, loss=3.856241226196289
I0207 22:31:26.502562 140050996819712 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2268061339855194, loss=3.846177816390991
I0207 22:32:01.103559 140051005212416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2833682596683502, loss=3.8525829315185547
I0207 22:32:35.697155 140050996819712 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.24749819934368134, loss=3.848418951034546
I0207 22:33:03.756257 140225696298816 spec.py:321] Evaluating on the training split.
I0207 22:33:06.746317 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:36:35.512717 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 22:36:38.187442 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:39:17.772741 140225696298816 spec.py:349] Evaluating on the test split.
I0207 22:39:20.453092 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:41:54.040266 140225696298816 submission_runner.py:408] Time since start: 30843.16s, 	Step: 53483, 	{'train/accuracy': 0.6628263592720032, 'train/loss': 1.7955244779586792, 'train/bleu': 32.78826064342799, 'validation/accuracy': 0.6780325174331665, 'validation/loss': 1.6881910562515259, 'validation/bleu': 29.682209609916477, 'validation/num_examples': 3000, 'test/accuracy': 0.693265974521637, 'test/loss': 1.6090433597564697, 'test/bleu': 29.551221567046127, 'test/num_examples': 3003, 'score': 18507.86905694008, 'total_duration': 30843.158007144928, 'accumulated_submission_time': 18507.86905694008, 'accumulated_eval_time': 12332.954099178314, 'accumulated_logging_time': 0.6230416297912598}
I0207 22:41:54.062671 140051005212416 logging_writer.py:48] [53483] accumulated_eval_time=12332.954099, accumulated_logging_time=0.623042, accumulated_submission_time=18507.869057, global_step=53483, preemption_count=0, score=18507.869057, test/accuracy=0.693266, test/bleu=29.551222, test/loss=1.609043, test/num_examples=3003, total_duration=30843.158007, train/accuracy=0.662826, train/bleu=32.788261, train/loss=1.795524, validation/accuracy=0.678033, validation/bleu=29.682210, validation/loss=1.688191, validation/num_examples=3000
I0207 22:42:00.275000 140050996819712 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2557179927825928, loss=3.8873965740203857
I0207 22:42:34.717663 140051005212416 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.358858197927475, loss=3.90541410446167
I0207 22:43:09.281262 140050996819712 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3734321594238281, loss=3.9133691787719727
I0207 22:43:43.851407 140051005212416 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.26381176710128784, loss=3.861985921859741
I0207 22:44:18.423293 140050996819712 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.22903560101985931, loss=3.824986457824707
I0207 22:44:52.973488 140051005212416 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2693463861942291, loss=3.8576624393463135
I0207 22:45:27.530310 140050996819712 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2640257775783539, loss=3.806222438812256
I0207 22:46:02.098569 140051005212416 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2743483781814575, loss=3.852358341217041
I0207 22:46:36.669847 140050996819712 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2476479411125183, loss=3.8506460189819336
I0207 22:47:11.234785 140051005212416 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.29702457785606384, loss=3.838413953781128
I0207 22:47:45.838595 140050996819712 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.25271227955818176, loss=3.8240647315979004
I0207 22:48:20.432780 140051005212416 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2552281320095062, loss=3.877140522003174
I0207 22:48:55.044436 140050996819712 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2457047402858734, loss=3.855095386505127
I0207 22:49:29.630429 140051005212416 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2822751998901367, loss=3.8546299934387207
I0207 22:50:04.223954 140050996819712 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2729698717594147, loss=3.8964672088623047
I0207 22:50:38.793506 140051005212416 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.23260515928268433, loss=3.8328025341033936
I0207 22:51:13.364955 140050996819712 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2512141764163971, loss=3.9018964767456055
I0207 22:51:47.940493 140051005212416 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2615797221660614, loss=3.8639369010925293
I0207 22:52:22.501109 140050996819712 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.25545743107795715, loss=3.8528878688812256
I0207 22:52:57.087798 140051005212416 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3017694354057312, loss=3.834608793258667
I0207 22:53:31.646576 140050996819712 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.29449737071990967, loss=3.839144706726074
I0207 22:54:06.240852 140051005212416 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2833166718482971, loss=3.8625311851501465
I0207 22:54:40.806531 140050996819712 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.283915638923645, loss=3.838257312774658
I0207 22:55:15.386469 140051005212416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2652827799320221, loss=3.892587184906006
I0207 22:55:49.934103 140050996819712 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.25122299790382385, loss=3.8606865406036377
I0207 22:55:54.150990 140225696298816 spec.py:321] Evaluating on the training split.
I0207 22:55:57.123105 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 22:58:46.355070 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 22:58:49.036218 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 23:01:15.608164 140225696298816 spec.py:349] Evaluating on the test split.
I0207 23:01:18.298183 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 23:03:37.595674 140225696298816 submission_runner.py:408] Time since start: 32146.71s, 	Step: 55914, 	{'train/accuracy': 0.6604369282722473, 'train/loss': 1.8041754961013794, 'train/bleu': 33.13701132138776, 'validation/accuracy': 0.6777101159095764, 'validation/loss': 1.676916241645813, 'validation/bleu': 29.615411459159855, 'validation/num_examples': 3000, 'test/accuracy': 0.6930218935012817, 'test/loss': 1.5961912870407104, 'test/bleu': 29.350988973732594, 'test/num_examples': 3003, 'score': 19347.87039089203, 'total_duration': 32146.713432073593, 'accumulated_submission_time': 19347.87039089203, 'accumulated_eval_time': 12796.398730516434, 'accumulated_logging_time': 0.6548542976379395}
I0207 23:03:37.618775 140051005212416 logging_writer.py:48] [55914] accumulated_eval_time=12796.398731, accumulated_logging_time=0.654854, accumulated_submission_time=19347.870391, global_step=55914, preemption_count=0, score=19347.870391, test/accuracy=0.693022, test/bleu=29.350989, test/loss=1.596191, test/num_examples=3003, total_duration=32146.713432, train/accuracy=0.660437, train/bleu=33.137011, train/loss=1.804175, validation/accuracy=0.677710, validation/bleu=29.615411, validation/loss=1.676916, validation/num_examples=3000
I0207 23:04:07.593667 140050996819712 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2782432436943054, loss=3.8768904209136963
I0207 23:04:42.047096 140051005212416 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2412707507610321, loss=3.762803554534912
I0207 23:05:16.622064 140050996819712 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.24669066071510315, loss=3.826969623565674
I0207 23:05:51.188235 140051005212416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.26728081703186035, loss=3.8855581283569336
I0207 23:06:25.771241 140050996819712 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3404485583305359, loss=3.858881711959839
I0207 23:07:00.337643 140051005212416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.26974642276763916, loss=3.874890089035034
I0207 23:07:34.912531 140050996819712 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2385350465774536, loss=3.803246021270752
I0207 23:08:09.467522 140051005212416 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.23586997389793396, loss=3.8948724269866943
I0207 23:08:44.075255 140050996819712 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.25172725319862366, loss=3.8572299480438232
I0207 23:09:18.639405 140051005212416 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3076040744781494, loss=3.858938694000244
I0207 23:09:53.209486 140050996819712 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2373872697353363, loss=3.834374189376831
I0207 23:10:27.754770 140051005212416 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.0005040168762207, loss=6.006791591644287
I0207 23:11:02.258078 140050996819712 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.397331178188324, loss=5.490871429443359
I0207 23:11:36.760393 140051005212416 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.47814658284187317, loss=5.46678352355957
I0207 23:12:11.273035 140050996819712 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3166877329349518, loss=5.448461055755615
I0207 23:12:45.793632 140051005212416 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.34830284118652344, loss=5.4281768798828125
I0207 23:13:20.300053 140050996819712 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.542475163936615, loss=5.391366958618164
I0207 23:13:54.827512 140051005212416 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.4003169536590576, loss=5.391411781311035
I0207 23:14:29.281934 140050996819712 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.41851183772087097, loss=5.379457950592041
I0207 23:15:03.755077 140051005212416 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.1971882581710815, loss=5.345099449157715
I0207 23:15:38.282078 140050996819712 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1504794359207153, loss=5.372134685516357
I0207 23:16:12.792029 140051005212416 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7998668551445007, loss=5.332372188568115
I0207 23:16:47.281012 140050996819712 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.765768826007843, loss=5.30422830581665
I0207 23:17:21.796658 140051005212416 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.2589586973190308, loss=5.307278633117676
I0207 23:17:37.740323 140225696298816 spec.py:321] Evaluating on the training split.
I0207 23:17:40.707973 140225696298816 workload.py:181] Translating evaluation dataset.
W0207 23:22:15.039744 140225696298816 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0207 23:22:15.039972 140225696298816 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0207 23:22:15.040041 140225696298816 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0207 23:22:16.027850 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 23:22:18.698337 140225696298816 workload.py:181] Translating evaluation dataset.
W0207 23:26:42.493447 140225696298816 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0207 23:26:42.493684 140225696298816 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0207 23:26:42.493735 140225696298816 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0207 23:26:42.756503 140225696298816 spec.py:349] Evaluating on the test split.
I0207 23:26:45.425960 140225696298816 workload.py:181] Translating evaluation dataset.
W0207 23:31:11.308767 140225696298816 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0207 23:31:11.309032 140225696298816 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0207 23:31:11.309091 140225696298816 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0207 23:31:11.981468 140225696298816 submission_runner.py:408] Time since start: 33801.10s, 	Step: 58348, 	{'train/accuracy': 0.37330424785614014, 'train/loss': 3.6769845485687256, 'train/bleu': 1.7790266662847813, 'validation/accuracy': 0.3269147276878357, 'validation/loss': 4.14604377746582, 'validation/bleu': 0.3167417668719349, 'validation/num_examples': 3000, 'test/accuracy': 0.3172041177749634, 'test/loss': 4.3111090660095215, 'test/bleu': 0.2760901163922393, 'test/num_examples': 3003, 'score': 20187.9033575058, 'total_duration': 33801.09925675392, 'accumulated_submission_time': 20187.9033575058, 'accumulated_eval_time': 13610.639861106873, 'accumulated_logging_time': 0.6897470951080322}
I0207 23:31:12.003994 140050996819712 logging_writer.py:48] [58348] accumulated_eval_time=13610.639861, accumulated_logging_time=0.689747, accumulated_submission_time=20187.903358, global_step=58348, preemption_count=0, score=20187.903358, test/accuracy=0.317204, test/bleu=0.276090, test/loss=4.311109, test/num_examples=3003, total_duration=33801.099257, train/accuracy=0.373304, train/bleu=1.779027, train/loss=3.676985, validation/accuracy=0.326915, validation/bleu=0.316742, validation/loss=4.146044, validation/num_examples=3000
I0207 23:31:30.260672 140051005212416 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3828009366989136, loss=5.268263816833496
I0207 23:32:04.685984 140050996819712 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.398813247680664, loss=5.15802001953125
I0207 23:32:39.246067 140051005212416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2665734887123108, loss=3.945512533187866
I0207 23:33:13.815565 140050996819712 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2844873070716858, loss=3.9067811965942383
I0207 23:33:48.380311 140051005212416 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2709341049194336, loss=3.9058637619018555
I0207 23:34:22.935409 140050996819712 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.24271491169929504, loss=3.853599786758423
I0207 23:34:57.498706 140051005212416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.26187440752983093, loss=3.8010809421539307
I0207 23:35:32.074670 140050996819712 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2376195788383484, loss=3.870619773864746
I0207 23:36:06.631059 140051005212416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.263492614030838, loss=3.851738929748535
I0207 23:36:41.176107 140050996819712 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2900862991809845, loss=3.928718328475952
I0207 23:37:15.746909 140051005212416 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2527647316455841, loss=3.8404147624969482
I0207 23:37:50.368668 140050996819712 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2652356028556824, loss=3.8337485790252686
I0207 23:38:24.955347 140051005212416 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2533286213874817, loss=3.794769525527954
I0207 23:38:59.536032 140050996819712 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.25778135657310486, loss=3.8686115741729736
I0207 23:39:34.105079 140051005212416 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2449825555086136, loss=3.8295605182647705
I0207 23:40:08.670778 140050996819712 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2510177493095398, loss=3.810486078262329
I0207 23:40:43.246207 140051005212416 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.34755122661590576, loss=3.8387317657470703
I0207 23:41:17.822575 140050996819712 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2522396445274353, loss=3.8680036067962646
I0207 23:41:52.402753 140051005212416 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.25331664085388184, loss=3.835911750793457
I0207 23:42:26.992536 140050996819712 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.25713035464286804, loss=3.8714091777801514
I0207 23:43:01.574041 140051005212416 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.308293879032135, loss=3.881650447845459
I0207 23:43:36.143851 140050996819712 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.24636434018611908, loss=3.859909772872925
I0207 23:44:10.724811 140051005212416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.24792687594890594, loss=3.8451526165008545
I0207 23:44:45.292173 140050996819712 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3262316584587097, loss=3.7905337810516357
I0207 23:45:11.988107 140225696298816 spec.py:321] Evaluating on the training split.
I0207 23:45:14.960098 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 23:48:27.205650 140225696298816 spec.py:333] Evaluating on the validation split.
I0207 23:48:29.895447 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 23:51:11.315680 140225696298816 spec.py:349] Evaluating on the test split.
I0207 23:51:13.996319 140225696298816 workload.py:181] Translating evaluation dataset.
I0207 23:53:48.199598 140225696298816 submission_runner.py:408] Time since start: 35157.32s, 	Step: 60779, 	{'train/accuracy': 0.6671022176742554, 'train/loss': 1.770507574081421, 'train/bleu': 32.896878208625374, 'validation/accuracy': 0.6774373650550842, 'validation/loss': 1.6847100257873535, 'validation/bleu': 29.561452728455883, 'validation/num_examples': 3000, 'test/accuracy': 0.6925687193870544, 'test/loss': 1.6012877225875854, 'test/bleu': 29.86454461957114, 'test/num_examples': 3003, 'score': 21027.7988653183, 'total_duration': 35157.31735706329, 'accumulated_submission_time': 21027.7988653183, 'accumulated_eval_time': 14126.851303100586, 'accumulated_logging_time': 0.7242088317871094}
I0207 23:53:48.221754 140051005212416 logging_writer.py:48] [60779] accumulated_eval_time=14126.851303, accumulated_logging_time=0.724209, accumulated_submission_time=21027.798865, global_step=60779, preemption_count=0, score=21027.798865, test/accuracy=0.692569, test/bleu=29.864545, test/loss=1.601288, test/num_examples=3003, total_duration=35157.317357, train/accuracy=0.667102, train/bleu=32.896878, train/loss=1.770508, validation/accuracy=0.677437, validation/bleu=29.561453, validation/loss=1.684710, validation/num_examples=3000
I0207 23:53:55.792286 140050996819712 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2714906334877014, loss=3.913820505142212
I0207 23:54:30.234489 140051005212416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.24337273836135864, loss=3.8705451488494873
I0207 23:55:04.751305 140050996819712 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2735728621482849, loss=3.8522469997406006
I0207 23:55:39.295076 140051005212416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.27244460582733154, loss=3.833728790283203
I0207 23:56:13.848666 140050996819712 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.2716515362262726, loss=3.832090377807617
I0207 23:56:48.411492 140051005212416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.263903945684433, loss=3.8557443618774414
I0207 23:57:22.979477 140050996819712 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2656375467777252, loss=3.7992987632751465
I0207 23:57:57.544906 140051005212416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2528819441795349, loss=3.794316291809082
I0207 23:58:32.099221 140050996819712 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3071014881134033, loss=3.8374617099761963
I0207 23:59:06.673629 140051005212416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.2536109387874603, loss=3.8190855979919434
I0207 23:59:41.253559 140050996819712 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.29482439160346985, loss=3.863626480102539
I0208 00:00:15.817667 140051005212416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.2543027400970459, loss=3.8664119243621826
I0208 00:00:50.397246 140050996819712 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.32459861040115356, loss=3.866293430328369
I0208 00:01:24.949246 140051005212416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2535490393638611, loss=3.7997384071350098
I0208 00:01:59.514027 140050996819712 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.26557520031929016, loss=3.8631889820098877
I0208 00:02:34.086595 140051005212416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.270192414522171, loss=3.8518190383911133
I0208 00:03:08.668147 140050996819712 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.28028973937034607, loss=3.8066728115081787
I0208 00:03:43.255629 140051005212416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2727140188217163, loss=3.823957920074463
I0208 00:04:17.819314 140050996819712 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.261653870344162, loss=3.7942192554473877
I0208 00:04:52.376025 140051005212416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.2724251449108124, loss=3.8395564556121826
I0208 00:05:26.957126 140050996819712 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2854558527469635, loss=3.9033849239349365
I0208 00:06:01.513814 140051005212416 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.24836596846580505, loss=3.8578941822052
I0208 00:06:36.075390 140050996819712 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2625378668308258, loss=3.865088939666748
I0208 00:07:10.646046 140051005212416 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.26604270935058594, loss=3.817692518234253
I0208 00:07:45.208172 140050996819712 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2606356739997864, loss=3.7981362342834473
I0208 00:07:48.392533 140225696298816 spec.py:321] Evaluating on the training split.
I0208 00:07:51.370827 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:10:40.968531 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 00:10:43.657840 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:13:10.302829 140225696298816 spec.py:349] Evaluating on the test split.
I0208 00:13:12.991462 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:15:34.049946 140225696298816 submission_runner.py:408] Time since start: 36463.17s, 	Step: 63211, 	{'train/accuracy': 0.6855831146240234, 'train/loss': 1.6496340036392212, 'train/bleu': 34.1527935497728, 'validation/accuracy': 0.679582417011261, 'validation/loss': 1.6690008640289307, 'validation/bleu': 29.659998515776348, 'validation/num_examples': 3000, 'test/accuracy': 0.6939283013343811, 'test/loss': 1.5856915712356567, 'test/bleu': 29.44425505315059, 'test/num_examples': 3003, 'score': 21867.882091760635, 'total_duration': 36463.16767692566, 'accumulated_submission_time': 21867.882091760635, 'accumulated_eval_time': 14592.508635282516, 'accumulated_logging_time': 0.7560958862304688}
I0208 00:15:34.073156 140051005212416 logging_writer.py:48] [63211] accumulated_eval_time=14592.508635, accumulated_logging_time=0.756096, accumulated_submission_time=21867.882092, global_step=63211, preemption_count=0, score=21867.882092, test/accuracy=0.693928, test/bleu=29.444255, test/loss=1.585692, test/num_examples=3003, total_duration=36463.167677, train/accuracy=0.685583, train/bleu=34.152794, train/loss=1.649634, validation/accuracy=0.679582, validation/bleu=29.659999, validation/loss=1.669001, validation/num_examples=3000
I0208 00:16:05.083027 140050996819712 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.300694078207016, loss=3.8047051429748535
I0208 00:16:39.571129 140051005212416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.274112731218338, loss=3.890941858291626
I0208 00:17:14.149484 140050996819712 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.25234660506248474, loss=3.880359649658203
I0208 00:17:48.710224 140051005212416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.26744288206100464, loss=3.8758370876312256
I0208 00:18:23.269708 140050996819712 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.25095152854919434, loss=3.813692331314087
I0208 00:18:57.837850 140051005212416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2618899643421173, loss=3.8617663383483887
I0208 00:19:32.437045 140050996819712 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2702102065086365, loss=3.85361385345459
I0208 00:20:07.012586 140051005212416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.27079787850379944, loss=3.869053602218628
I0208 00:20:41.565154 140050996819712 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.28131574392318726, loss=3.843829393386841
I0208 00:21:16.123555 140051005212416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2746049761772156, loss=3.8743865489959717
I0208 00:21:50.689126 140050996819712 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.23813848197460175, loss=3.8674168586730957
I0208 00:22:25.270735 140051005212416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.26033249497413635, loss=3.884176254272461
I0208 00:22:59.863011 140050996819712 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2651752829551697, loss=3.892024278640747
I0208 00:23:34.448610 140051005212416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.24339184165000916, loss=3.863196849822998
I0208 00:24:09.018209 140050996819712 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.27064254879951477, loss=3.868314266204834
I0208 00:24:43.610958 140051005212416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2461249977350235, loss=3.839677095413208
I0208 00:25:18.185119 140050996819712 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2598545551300049, loss=3.8188161849975586
I0208 00:25:52.783190 140051005212416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.256561815738678, loss=3.8224480152130127
I0208 00:26:27.356236 140050996819712 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2581036388874054, loss=3.807894706726074
I0208 00:27:01.940087 140051005212416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.25636833906173706, loss=3.847747564315796
I0208 00:27:36.523663 140050996819712 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2538689374923706, loss=3.839674711227417
I0208 00:28:11.092497 140051005212416 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.27267560362815857, loss=3.865158796310425
I0208 00:28:45.676273 140050996819712 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.26022040843963623, loss=3.8428401947021484
I0208 00:29:20.255892 140051005212416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.25667038559913635, loss=3.841326951980591
I0208 00:29:34.141487 140225696298816 spec.py:321] Evaluating on the training split.
I0208 00:29:37.108866 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:32:18.628925 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 00:32:21.314162 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:34:48.522638 140225696298816 spec.py:349] Evaluating on the test split.
I0208 00:34:51.205368 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:37:13.232693 140225696298816 submission_runner.py:408] Time since start: 37762.35s, 	Step: 65642, 	{'train/accuracy': 0.6722832322120667, 'train/loss': 1.7180349826812744, 'train/bleu': 33.70603561239361, 'validation/accuracy': 0.681913435459137, 'validation/loss': 1.6529948711395264, 'validation/bleu': 30.087363587995426, 'validation/num_examples': 3000, 'test/accuracy': 0.6969264149665833, 'test/loss': 1.5698133707046509, 'test/bleu': 29.882720372420852, 'test/num_examples': 3003, 'score': 22707.863388299942, 'total_duration': 37762.350452661514, 'accumulated_submission_time': 22707.863388299942, 'accumulated_eval_time': 15051.599786758423, 'accumulated_logging_time': 0.7890956401824951}
I0208 00:37:13.255818 140050996819712 logging_writer.py:48] [65642] accumulated_eval_time=15051.599787, accumulated_logging_time=0.789096, accumulated_submission_time=22707.863388, global_step=65642, preemption_count=0, score=22707.863388, test/accuracy=0.696926, test/bleu=29.882720, test/loss=1.569813, test/num_examples=3003, total_duration=37762.350453, train/accuracy=0.672283, train/bleu=33.706036, train/loss=1.718035, validation/accuracy=0.681913, validation/bleu=30.087364, validation/loss=1.652995, validation/num_examples=3000
I0208 00:37:33.596921 140051005212416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.2422344833612442, loss=3.7911767959594727
I0208 00:38:08.064350 140050996819712 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.32175084948539734, loss=3.8611340522766113
I0208 00:38:42.605138 140051005212416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2508993148803711, loss=3.8199551105499268
I0208 00:39:17.163298 140050996819712 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3612169623374939, loss=3.8390464782714844
I0208 00:39:51.717878 140051005212416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2899060845375061, loss=3.7909111976623535
I0208 00:40:26.292255 140050996819712 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.252130925655365, loss=3.7936604022979736
I0208 00:41:00.879019 140051005212416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2625163793563843, loss=3.767585039138794
I0208 00:41:35.454238 140050996819712 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.25495460629463196, loss=3.7594857215881348
I0208 00:42:10.026615 140051005212416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2822674512863159, loss=3.8217358589172363
I0208 00:42:44.608452 140050996819712 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2694402039051056, loss=3.890561819076538
I0208 00:43:19.166702 140051005212416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2690873444080353, loss=3.792001724243164
I0208 00:43:53.758877 140050996819712 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2917812466621399, loss=3.8209640979766846
I0208 00:44:28.317141 140051005212416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2754669189453125, loss=3.789418935775757
I0208 00:45:02.900715 140050996819712 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2933413088321686, loss=3.8612749576568604
I0208 00:45:37.482820 140051005212416 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2636653780937195, loss=3.836181163787842
I0208 00:46:12.071723 140050996819712 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.25142455101013184, loss=3.7998099327087402
I0208 00:46:46.726101 140051005212416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.25735875964164734, loss=3.76261305809021
I0208 00:47:21.307327 140050996819712 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.2714356780052185, loss=3.8433587551116943
I0208 00:47:55.871984 140051005212416 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.2522423267364502, loss=3.7525224685668945
I0208 00:48:30.438308 140050996819712 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2699161767959595, loss=3.7268929481506348
I0208 00:49:05.026624 140051005212416 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.2588476836681366, loss=3.8132505416870117
I0208 00:49:39.609230 140050996819712 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.27615904808044434, loss=3.8233959674835205
I0208 00:50:14.206557 140051005212416 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.3195739686489105, loss=3.8445186614990234
I0208 00:50:48.782332 140050996819712 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2530386745929718, loss=3.76932954788208
I0208 00:51:13.407130 140225696298816 spec.py:321] Evaluating on the training split.
I0208 00:51:16.383373 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:53:51.776322 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 00:53:54.454804 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:56:22.883466 140225696298816 spec.py:349] Evaluating on the test split.
I0208 00:56:25.558793 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 00:58:44.644688 140225696298816 submission_runner.py:408] Time since start: 39053.76s, 	Step: 68073, 	{'train/accuracy': 0.6699897646903992, 'train/loss': 1.7524583339691162, 'train/bleu': 33.207613914396966, 'validation/accuracy': 0.6819630265235901, 'validation/loss': 1.6551176309585571, 'validation/bleu': 30.112537396175984, 'validation/num_examples': 3000, 'test/accuracy': 0.6972169280052185, 'test/loss': 1.5671659708023071, 'test/bleu': 29.769703618118545, 'test/num_examples': 3003, 'score': 23547.92662167549, 'total_duration': 39053.7624464035, 'accumulated_submission_time': 23547.92662167549, 'accumulated_eval_time': 15502.837298870087, 'accumulated_logging_time': 0.822166919708252}
I0208 00:58:44.670512 140051005212416 logging_writer.py:48] [68073] accumulated_eval_time=15502.837299, accumulated_logging_time=0.822167, accumulated_submission_time=23547.926622, global_step=68073, preemption_count=0, score=23547.926622, test/accuracy=0.697217, test/bleu=29.769704, test/loss=1.567166, test/num_examples=3003, total_duration=39053.762446, train/accuracy=0.669990, train/bleu=33.207614, train/loss=1.752458, validation/accuracy=0.681963, validation/bleu=30.112537, validation/loss=1.655118, validation/num_examples=3000
I0208 00:58:54.323698 140050996819712 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3529020845890045, loss=3.757683515548706
I0208 00:59:28.745191 140051005212416 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.30201292037963867, loss=3.860259771347046
I0208 01:00:03.274864 140050996819712 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.27506232261657715, loss=3.8519914150238037
I0208 01:00:37.840448 140051005212416 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.27119725942611694, loss=3.8357009887695312
I0208 01:01:12.418776 140050996819712 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2752152681350708, loss=3.8147103786468506
I0208 01:01:47.000512 140051005212416 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2955404818058014, loss=3.843435525894165
I0208 01:02:21.571446 140050996819712 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2791227698326111, loss=3.812194347381592
I0208 01:02:56.161432 140051005212416 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.30702632665634155, loss=3.8709468841552734
I0208 01:03:30.762440 140050996819712 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.24526984989643097, loss=3.8051133155822754
I0208 01:04:05.326383 140051005212416 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3044312596321106, loss=3.823650360107422
I0208 01:04:39.920011 140050996819712 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2776345908641815, loss=3.850435495376587
I0208 01:05:14.495381 140051005212416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2815868854522705, loss=3.8403067588806152
I0208 01:05:49.057770 140050996819712 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.25146210193634033, loss=3.7633957862854004
I0208 01:06:23.655087 140051005212416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2537950277328491, loss=3.782496929168701
I0208 01:06:58.257761 140050996819712 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.26683783531188965, loss=3.7780921459198
I0208 01:07:32.849603 140051005212416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2830752432346344, loss=3.8178365230560303
I0208 01:08:07.425046 140050996819712 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2574551999568939, loss=3.839623212814331
I0208 01:08:41.985651 140051005212416 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.2623016834259033, loss=3.759312152862549
I0208 01:09:16.545554 140050996819712 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.26326555013656616, loss=3.80429744720459
I0208 01:09:51.122905 140051005212416 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.26970627903938293, loss=3.777355432510376
I0208 01:10:25.679100 140050996819712 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2930193543434143, loss=3.8066320419311523
I0208 01:11:00.256770 140051005212416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2680193781852722, loss=3.7725768089294434
I0208 01:11:34.801575 140050996819712 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.27542930841445923, loss=3.7576892375946045
I0208 01:12:09.372070 140051005212416 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3068651854991913, loss=3.8197031021118164
I0208 01:12:43.963960 140050996819712 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2586732506752014, loss=3.8116118907928467
I0208 01:12:44.726460 140225696298816 spec.py:321] Evaluating on the training split.
I0208 01:12:47.709433 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:15:29.963022 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 01:15:32.646233 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:18:14.092876 140225696298816 spec.py:349] Evaluating on the test split.
I0208 01:18:16.776059 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:20:45.206337 140225696298816 submission_runner.py:408] Time since start: 40374.32s, 	Step: 70504, 	{'train/accuracy': 0.6783646941184998, 'train/loss': 1.6977791786193848, 'train/bleu': 34.29033621037187, 'validation/accuracy': 0.6820374131202698, 'validation/loss': 1.6467190980911255, 'validation/bleu': 30.249611344394367, 'validation/num_examples': 3000, 'test/accuracy': 0.6974377036094666, 'test/loss': 1.5637001991271973, 'test/bleu': 30.00090817367382, 'test/num_examples': 3003, 'score': 24387.89455485344, 'total_duration': 40374.32408332825, 'accumulated_submission_time': 24387.89455485344, 'accumulated_eval_time': 15983.3171210289, 'accumulated_logging_time': 0.857762336730957}
I0208 01:20:45.231899 140051005212416 logging_writer.py:48] [70504] accumulated_eval_time=15983.317121, accumulated_logging_time=0.857762, accumulated_submission_time=24387.894555, global_step=70504, preemption_count=0, score=24387.894555, test/accuracy=0.697438, test/bleu=30.000908, test/loss=1.563700, test/num_examples=3003, total_duration=40374.324083, train/accuracy=0.678365, train/bleu=34.290336, train/loss=1.697779, validation/accuracy=0.682037, validation/bleu=30.249611, validation/loss=1.646719, validation/num_examples=3000
I0208 01:21:18.636155 140050996819712 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.26016244292259216, loss=3.848670482635498
I0208 01:21:53.144692 140051005212416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.27728113532066345, loss=3.7738656997680664
I0208 01:22:27.722365 140050996819712 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.32757699489593506, loss=3.8621010780334473
I0208 01:23:02.291007 140051005212416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.26113420724868774, loss=3.8026905059814453
I0208 01:23:36.865869 140050996819712 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2621091902256012, loss=3.8722362518310547
I0208 01:24:11.432377 140051005212416 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.26396864652633667, loss=3.825652599334717
I0208 01:24:46.011364 140050996819712 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.25936076045036316, loss=3.805419683456421
I0208 01:25:20.593009 140051005212416 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.30463898181915283, loss=3.8298516273498535
I0208 01:25:55.176692 140050996819712 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.27202460169792175, loss=3.8236541748046875
I0208 01:26:29.761912 140051005212416 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3414310812950134, loss=3.8382833003997803
I0208 01:27:04.350758 140050996819712 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.2667616009712219, loss=3.785893678665161
I0208 01:27:38.935347 140051005212416 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.2743469774723053, loss=3.8285040855407715
I0208 01:28:13.533843 140050996819712 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.27693474292755127, loss=3.8232715129852295
I0208 01:28:48.098153 140051005212416 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.27937066555023193, loss=3.7802963256835938
I0208 01:29:22.682180 140050996819712 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2611674666404724, loss=3.8119466304779053
I0208 01:29:57.270890 140051005212416 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3037644624710083, loss=3.756375312805176
I0208 01:30:31.838998 140050996819712 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.28924310207366943, loss=3.832331895828247
I0208 01:31:06.412225 140051005212416 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.31454047560691833, loss=3.7783026695251465
I0208 01:31:40.995205 140050996819712 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3245512843132019, loss=3.7821640968322754
I0208 01:32:15.567399 140051005212416 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.26051998138427734, loss=3.7722840309143066
I0208 01:32:50.148344 140050996819712 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2681584358215332, loss=3.7562832832336426
I0208 01:33:24.763620 140051005212416 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2703252136707306, loss=3.720841646194458
I0208 01:33:59.334225 140050996819712 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.27606895565986633, loss=3.8058273792266846
I0208 01:34:33.947776 140051005212416 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2649347186088562, loss=3.7151317596435547
I0208 01:34:45.421396 140225696298816 spec.py:321] Evaluating on the training split.
I0208 01:34:48.395147 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:37:28.629831 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 01:37:31.321769 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:40:04.207787 140225696298816 spec.py:349] Evaluating on the test split.
I0208 01:40:06.885716 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:42:21.923895 140225696298816 submission_runner.py:408] Time since start: 41671.04s, 	Step: 72935, 	{'train/accuracy': 0.6749582886695862, 'train/loss': 1.7154207229614258, 'train/bleu': 33.6660576168447, 'validation/accuracy': 0.6837732791900635, 'validation/loss': 1.6429944038391113, 'validation/bleu': 29.911717014627982, 'validation/num_examples': 3000, 'test/accuracy': 0.6994480490684509, 'test/loss': 1.5574828386306763, 'test/bleu': 29.76888711725281, 'test/num_examples': 3003, 'score': 25227.994698286057, 'total_duration': 41671.04165291786, 'accumulated_submission_time': 25227.994698286057, 'accumulated_eval_time': 16439.81956934929, 'accumulated_logging_time': 0.8949284553527832}
I0208 01:42:21.951958 140050996819712 logging_writer.py:48] [72935] accumulated_eval_time=16439.819569, accumulated_logging_time=0.894928, accumulated_submission_time=25227.994698, global_step=72935, preemption_count=0, score=25227.994698, test/accuracy=0.699448, test/bleu=29.768887, test/loss=1.557483, test/num_examples=3003, total_duration=41671.041653, train/accuracy=0.674958, train/bleu=33.666058, train/loss=1.715421, validation/accuracy=0.683773, validation/bleu=29.911717, validation/loss=1.642994, validation/num_examples=3000
I0208 01:42:44.674306 140051005212416 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.27458858489990234, loss=3.8241019248962402
I0208 01:43:19.158775 140050996819712 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.34678855538368225, loss=3.745931625366211
I0208 01:43:53.728777 140051005212416 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.29720667004585266, loss=3.7751591205596924
I0208 01:44:28.315476 140050996819712 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.28941085934638977, loss=3.758068561553955
I0208 01:45:02.902334 140051005212416 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.28134673833847046, loss=3.7825584411621094
I0208 01:45:37.481037 140050996819712 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.31697842478752136, loss=3.7451937198638916
I0208 01:46:12.054909 140051005212416 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.29168397188186646, loss=3.7777435779571533
I0208 01:46:46.619064 140050996819712 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.28388893604278564, loss=3.7998900413513184
I0208 01:47:21.197044 140051005212416 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.2692694664001465, loss=3.7972943782806396
I0208 01:47:55.770792 140050996819712 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3017245829105377, loss=3.7557737827301025
I0208 01:48:30.339531 140051005212416 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.4251098036766052, loss=3.8072738647460938
I0208 01:49:04.952115 140050996819712 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2944132387638092, loss=3.7578952312469482
I0208 01:49:39.555313 140051005212416 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.2936718463897705, loss=3.81655216217041
I0208 01:50:14.129593 140050996819712 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.28058257699012756, loss=3.750844717025757
I0208 01:50:48.700683 140051005212416 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.28378400206565857, loss=3.790029525756836
I0208 01:51:23.273209 140050996819712 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.2888910472393036, loss=3.824929714202881
I0208 01:51:57.843919 140051005212416 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2808966040611267, loss=3.7610487937927246
I0208 01:52:32.434811 140050996819712 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.26340338587760925, loss=3.754653215408325
I0208 01:53:07.002110 140051005212416 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2683674097061157, loss=3.7603349685668945
I0208 01:53:41.553811 140050996819712 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.27249783277511597, loss=3.764876365661621
I0208 01:54:16.112616 140051005212416 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2822454869747162, loss=3.778108835220337
I0208 01:54:50.707226 140050996819712 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.28830909729003906, loss=3.785266637802124
I0208 01:55:25.308161 140051005212416 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.27446234226226807, loss=3.827636241912842
I0208 01:55:59.891562 140050996819712 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.27592092752456665, loss=3.7205965518951416
I0208 01:56:22.101353 140225696298816 spec.py:321] Evaluating on the training split.
I0208 01:56:25.084879 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 01:59:50.539844 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 01:59:53.239669 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:02:35.510098 140225696298816 spec.py:349] Evaluating on the test split.
I0208 02:02:38.201085 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:05:00.325780 140225696298816 submission_runner.py:408] Time since start: 43029.44s, 	Step: 75366, 	{'train/accuracy': 0.701172411441803, 'train/loss': 1.5690522193908691, 'train/bleu': 35.91140519112401, 'validation/accuracy': 0.6847156286239624, 'validation/loss': 1.6373581886291504, 'validation/bleu': 30.375385832128877, 'validation/num_examples': 3000, 'test/accuracy': 0.6980187296867371, 'test/loss': 1.5505318641662598, 'test/bleu': 29.991422765336544, 'test/num_examples': 3003, 'score': 26068.055867910385, 'total_duration': 43029.4435377121, 'accumulated_submission_time': 26068.055867910385, 'accumulated_eval_time': 16958.04394555092, 'accumulated_logging_time': 0.9335498809814453}
I0208 02:05:00.352723 140051005212416 logging_writer.py:48] [75366] accumulated_eval_time=16958.043946, accumulated_logging_time=0.933550, accumulated_submission_time=26068.055868, global_step=75366, preemption_count=0, score=26068.055868, test/accuracy=0.698019, test/bleu=29.991423, test/loss=1.550532, test/num_examples=3003, total_duration=43029.443538, train/accuracy=0.701172, train/bleu=35.911405, train/loss=1.569052, validation/accuracy=0.684716, validation/bleu=30.375386, validation/loss=1.637358, validation/num_examples=3000
I0208 02:05:12.423211 140050996819712 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.284466952085495, loss=3.741813898086548
I0208 02:05:46.871061 140051005212416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.26999592781066895, loss=3.730642080307007
I0208 02:06:21.403574 140050996819712 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.25520676374435425, loss=3.7584502696990967
I0208 02:06:55.960529 140051005212416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2955384850502014, loss=3.8027451038360596
I0208 02:07:30.535494 140050996819712 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.28755924105644226, loss=3.8425588607788086
I0208 02:08:05.107988 140051005212416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2810163199901581, loss=3.8300633430480957
I0208 02:08:39.700315 140050996819712 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2862061858177185, loss=3.808495283126831
I0208 02:09:14.246505 140051005212416 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.27442577481269836, loss=3.7871997356414795
I0208 02:09:48.832159 140050996819712 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.27887317538261414, loss=3.801739454269409
I0208 02:10:23.426021 140051005212416 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.29838693141937256, loss=3.7870113849639893
I0208 02:10:58.006266 140050996819712 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2604841887950897, loss=3.7910401821136475
I0208 02:11:32.576720 140051005212416 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.27421221137046814, loss=3.7948734760284424
I0208 02:12:07.161133 140050996819712 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.27975592017173767, loss=3.7166988849639893
I0208 02:12:41.713951 140051005212416 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3403001129627228, loss=3.8598523139953613
I0208 02:13:16.295668 140050996819712 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.285351037979126, loss=3.8657948970794678
I0208 02:13:50.869127 140051005212416 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.28973066806793213, loss=3.7435989379882812
I0208 02:14:25.453017 140050996819712 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2675546705722809, loss=3.7270429134368896
I0208 02:15:00.031112 140051005212416 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2910180985927582, loss=3.7995691299438477
I0208 02:15:34.637120 140050996819712 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2627807855606079, loss=3.763385772705078
I0208 02:16:09.194489 140051005212416 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2846718728542328, loss=3.813776731491089
I0208 02:16:43.784965 140050996819712 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.2706904113292694, loss=3.766339063644409
I0208 02:17:18.342414 140051005212416 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2895735800266266, loss=3.7589516639709473
I0208 02:17:52.931717 140050996819712 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.27824655175209045, loss=3.7725703716278076
I0208 02:18:27.501770 140051005212416 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.29954177141189575, loss=3.714277505874634
I0208 02:19:00.406625 140225696298816 spec.py:321] Evaluating on the training split.
I0208 02:19:03.390563 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:21:49.928238 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 02:21:52.614494 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:24:18.530635 140225696298816 spec.py:349] Evaluating on the test split.
I0208 02:24:21.207551 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:26:33.230794 140225696298816 submission_runner.py:408] Time since start: 44322.35s, 	Step: 77797, 	{'train/accuracy': 0.6788302659988403, 'train/loss': 1.6935021877288818, 'train/bleu': 34.17489567231058, 'validation/accuracy': 0.6846908330917358, 'validation/loss': 1.637537956237793, 'validation/bleu': 30.014238840241926, 'validation/num_examples': 3000, 'test/accuracy': 0.701714038848877, 'test/loss': 1.5495461225509644, 'test/bleu': 30.396515383250396, 'test/num_examples': 3003, 'score': 26908.02235507965, 'total_duration': 44322.348552942276, 'accumulated_submission_time': 26908.02235507965, 'accumulated_eval_time': 17410.868065595627, 'accumulated_logging_time': 0.9702820777893066}
I0208 02:26:33.257510 140050996819712 logging_writer.py:48] [77797] accumulated_eval_time=17410.868066, accumulated_logging_time=0.970282, accumulated_submission_time=26908.022355, global_step=77797, preemption_count=0, score=26908.022355, test/accuracy=0.701714, test/bleu=30.396515, test/loss=1.549546, test/num_examples=3003, total_duration=44322.348553, train/accuracy=0.678830, train/bleu=34.174896, train/loss=1.693502, validation/accuracy=0.684691, validation/bleu=30.014239, validation/loss=1.637538, validation/num_examples=3000
I0208 02:26:34.661732 140051005212416 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.26952099800109863, loss=3.7649528980255127
I0208 02:27:09.099071 140050996819712 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.28384119272232056, loss=3.7974319458007812
I0208 02:27:43.632072 140051005212416 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.29893893003463745, loss=3.8284528255462646
I0208 02:28:18.211279 140050996819712 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3023875057697296, loss=3.804614782333374
I0208 02:28:52.791800 140051005212416 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.30208247900009155, loss=3.8554294109344482
I0208 02:29:27.357770 140050996819712 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3187619745731354, loss=3.7581145763397217
I0208 02:30:01.931828 140051005212416 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.266538143157959, loss=3.6877918243408203
I0208 02:30:36.529447 140050996819712 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.28857797384262085, loss=3.7354719638824463
I0208 02:31:11.111776 140051005212416 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.29157137870788574, loss=3.7831175327301025
I0208 02:31:45.694764 140050996819712 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.290386825799942, loss=3.809386968612671
I0208 02:32:20.285191 140051005212416 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2766631245613098, loss=3.7561769485473633
I0208 02:32:54.867674 140050996819712 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.26434096693992615, loss=3.7384910583496094
I0208 02:33:29.447581 140051005212416 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.29636290669441223, loss=3.7601776123046875
I0208 02:34:04.047183 140050996819712 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.27760350704193115, loss=3.7950563430786133
I0208 02:34:38.667323 140051005212416 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2588043510913849, loss=3.7476465702056885
I0208 02:35:13.263306 140050996819712 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2671394646167755, loss=3.7623722553253174
I0208 02:35:47.859445 140051005212416 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.33375489711761475, loss=3.817796230316162
I0208 02:36:22.442534 140050996819712 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2832084000110626, loss=3.771048069000244
I0208 02:36:57.002413 140051005212416 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.29571929574012756, loss=3.7748663425445557
I0208 02:37:31.563639 140050996819712 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2926531136035919, loss=3.8073060512542725
I0208 02:38:06.145319 140051005212416 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3084356188774109, loss=3.7528865337371826
I0208 02:38:40.725709 140050996819712 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2921288311481476, loss=3.6932458877563477
I0208 02:39:15.292702 140051005212416 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2812894582748413, loss=3.7921149730682373
I0208 02:39:49.864950 140050996819712 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2832637131214142, loss=3.782946825027466
I0208 02:40:24.437701 140051005212416 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3139890134334564, loss=3.70953106880188
I0208 02:40:33.499127 140225696298816 spec.py:321] Evaluating on the training split.
I0208 02:40:36.475086 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:43:17.376380 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 02:43:20.052168 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:45:41.265784 140225696298816 spec.py:349] Evaluating on the test split.
I0208 02:45:43.943420 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 02:47:58.668364 140225696298816 submission_runner.py:408] Time since start: 45607.79s, 	Step: 80228, 	{'train/accuracy': 0.6816733479499817, 'train/loss': 1.680078387260437, 'train/bleu': 34.12771578283215, 'validation/accuracy': 0.6865506768226624, 'validation/loss': 1.6328672170639038, 'validation/bleu': 30.30910404470047, 'validation/num_examples': 3000, 'test/accuracy': 0.7003777027130127, 'test/loss': 1.5423470735549927, 'test/bleu': 30.21403086923635, 'test/num_examples': 3003, 'score': 27748.175061941147, 'total_duration': 45607.78610897064, 'accumulated_submission_time': 27748.175061941147, 'accumulated_eval_time': 17856.037237882614, 'accumulated_logging_time': 1.008216142654419}
I0208 02:47:58.694981 140050996819712 logging_writer.py:48] [80228] accumulated_eval_time=17856.037238, accumulated_logging_time=1.008216, accumulated_submission_time=27748.175062, global_step=80228, preemption_count=0, score=27748.175062, test/accuracy=0.700378, test/bleu=30.214031, test/loss=1.542347, test/num_examples=3003, total_duration=45607.786109, train/accuracy=0.681673, train/bleu=34.127716, train/loss=1.680078, validation/accuracy=0.686551, validation/bleu=30.309104, validation/loss=1.632867, validation/num_examples=3000
I0208 02:48:23.836461 140051005212416 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.28709033131599426, loss=3.785299777984619
I0208 02:48:58.306942 140050996819712 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3017351031303406, loss=3.7559585571289062
I0208 02:49:32.866914 140051005212416 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.257917195558548, loss=3.759697914123535
I0208 02:50:07.446965 140050996819712 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3474351167678833, loss=3.746445894241333
I0208 02:50:42.018910 140051005212416 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2916278541088104, loss=3.769669771194458
I0208 02:51:16.581053 140050996819712 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.26867982745170593, loss=3.8245222568511963
I0208 02:51:51.146598 140051005212416 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2661406993865967, loss=3.7579219341278076
I0208 02:52:25.718741 140050996819712 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.29690396785736084, loss=3.749891757965088
I0208 02:53:00.279303 140051005212416 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.327521413564682, loss=3.7927894592285156
I0208 02:53:34.836457 140050996819712 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.282196581363678, loss=3.7418441772460938
I0208 02:54:09.444451 140051005212416 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.2958691716194153, loss=3.7464101314544678
I0208 02:54:44.009082 140050996819712 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.27380508184432983, loss=3.779271364212036
I0208 02:55:18.587470 140051005212416 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2897854447364807, loss=3.7965221405029297
I0208 02:55:53.188423 140050996819712 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.28526943922042847, loss=3.8117780685424805
I0208 02:56:27.756143 140051005212416 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.29295358061790466, loss=3.7610597610473633
I0208 02:57:02.332897 140050996819712 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3138163983821869, loss=3.7755889892578125
I0208 02:57:36.898154 140051005212416 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.284404993057251, loss=3.788815498352051
I0208 02:58:11.483218 140050996819712 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.30162546038627625, loss=3.7977406978607178
I0208 02:58:46.040412 140051005212416 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.341292142868042, loss=3.7858951091766357
I0208 02:59:20.635435 140050996819712 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.311307430267334, loss=3.8423938751220703
I0208 02:59:55.213033 140051005212416 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.28193777799606323, loss=3.7995855808258057
I0208 03:00:29.786522 140050996819712 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2953798770904541, loss=3.777183771133423
I0208 03:01:04.342299 140051005212416 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.32463517785072327, loss=3.7401864528656006
I0208 03:01:38.908960 140050996819712 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.31462153792381287, loss=3.803654909133911
I0208 03:01:58.693368 140225696298816 spec.py:321] Evaluating on the training split.
I0208 03:02:01.678570 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:05:22.446639 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 03:05:25.124863 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:08:03.032981 140225696298816 spec.py:349] Evaluating on the test split.
I0208 03:08:05.712332 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:10:29.367814 140225696298816 submission_runner.py:408] Time since start: 46958.49s, 	Step: 82659, 	{'train/accuracy': 0.6934168934822083, 'train/loss': 1.601096272468567, 'train/bleu': 34.86383322748257, 'validation/accuracy': 0.6877533793449402, 'validation/loss': 1.6192811727523804, 'validation/bleu': 30.61256790425198, 'validation/num_examples': 3000, 'test/accuracy': 0.7024344801902771, 'test/loss': 1.531815767288208, 'test/bleu': 30.29900160524727, 'test/num_examples': 3003, 'score': 28588.085990428925, 'total_duration': 46958.485570430756, 'accumulated_submission_time': 28588.085990428925, 'accumulated_eval_time': 18366.711642980576, 'accumulated_logging_time': 1.0443508625030518}
I0208 03:10:29.395996 140051005212416 logging_writer.py:48] [82659] accumulated_eval_time=18366.711643, accumulated_logging_time=1.044351, accumulated_submission_time=28588.085990, global_step=82659, preemption_count=0, score=28588.085990, test/accuracy=0.702434, test/bleu=30.299002, test/loss=1.531816, test/num_examples=3003, total_duration=46958.485570, train/accuracy=0.693417, train/bleu=34.863833, train/loss=1.601096, validation/accuracy=0.687753, validation/bleu=30.612568, validation/loss=1.619281, validation/num_examples=3000
I0208 03:10:43.873446 140050996819712 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2993687391281128, loss=3.8071179389953613
I0208 03:11:18.313054 140051005212416 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.28553447127342224, loss=3.7901604175567627
I0208 03:11:52.860310 140050996819712 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.309653103351593, loss=3.7554855346679688
I0208 03:12:27.434440 140051005212416 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.2768722176551819, loss=3.7063207626342773
I0208 03:13:01.997024 140050996819712 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.2939378023147583, loss=3.8084278106689453
I0208 03:13:36.564934 140051005212416 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.2843548655509949, loss=3.8074398040771484
I0208 03:14:11.142162 140050996819712 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2912188470363617, loss=3.762860059738159
I0208 03:14:45.707467 140051005212416 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.337812215089798, loss=3.804715871810913
I0208 03:15:20.268898 140050996819712 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.2864055037498474, loss=3.790281295776367
I0208 03:15:54.856493 140051005212416 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2875954806804657, loss=3.740041971206665
I0208 03:16:29.434303 140050996819712 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2871396243572235, loss=3.7426066398620605
I0208 03:17:04.013499 140051005212416 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2813483774662018, loss=3.7118213176727295
I0208 03:17:38.584978 140050996819712 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2978646755218506, loss=3.7773730754852295
I0208 03:18:13.131381 140051005212416 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.3020012378692627, loss=3.7393600940704346
I0208 03:18:47.718255 140050996819712 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3037649989128113, loss=3.765228271484375
I0208 03:19:22.288866 140051005212416 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.27825260162353516, loss=3.7753825187683105
I0208 03:19:56.853995 140050996819712 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2810894250869751, loss=3.774381160736084
I0208 03:20:31.421892 140051005212416 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.31436988711357117, loss=3.8121626377105713
I0208 03:21:06.002905 140050996819712 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.30991214513778687, loss=3.7563319206237793
I0208 03:21:40.582520 140051005212416 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.28093352913856506, loss=3.7409913539886475
I0208 03:22:15.138821 140050996819712 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.2961865961551666, loss=3.7109858989715576
I0208 03:22:49.718313 140051005212416 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.30022308230400085, loss=3.7315900325775146
I0208 03:23:24.286950 140050996819712 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.32788965106010437, loss=3.7309181690216064
I0208 03:23:58.867475 140051005212416 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3031380772590637, loss=3.7458014488220215
I0208 03:24:29.687953 140225696298816 spec.py:321] Evaluating on the training split.
I0208 03:24:32.664782 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:27:44.365430 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 03:27:47.038736 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:30:12.518271 140225696298816 spec.py:349] Evaluating on the test split.
I0208 03:30:15.202494 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:32:31.573337 140225696298816 submission_runner.py:408] Time since start: 48280.69s, 	Step: 85091, 	{'train/accuracy': 0.6842333674430847, 'train/loss': 1.6667418479919434, 'train/bleu': 34.231418554266256, 'validation/accuracy': 0.6870714426040649, 'validation/loss': 1.6234965324401855, 'validation/bleu': 30.22605562863886, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.5303460359573364, 'test/bleu': 30.34697045957415, 'test/num_examples': 3003, 'score': 29428.2906563282, 'total_duration': 48280.69108605385, 'accumulated_submission_time': 29428.2906563282, 'accumulated_eval_time': 18848.596970558167, 'accumulated_logging_time': 1.0820410251617432}
I0208 03:32:31.601186 140050996819712 logging_writer.py:48] [85091] accumulated_eval_time=18848.596971, accumulated_logging_time=1.082041, accumulated_submission_time=29428.290656, global_step=85091, preemption_count=0, score=29428.290656, test/accuracy=0.703573, test/bleu=30.346970, test/loss=1.530346, test/num_examples=3003, total_duration=48280.691086, train/accuracy=0.684233, train/bleu=34.231419, train/loss=1.666742, validation/accuracy=0.687071, validation/bleu=30.226056, validation/loss=1.623497, validation/num_examples=3000
I0208 03:32:35.062077 140051005212416 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.29627490043640137, loss=3.8049399852752686
I0208 03:33:09.513067 140050996819712 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3036799728870392, loss=3.709171772003174
I0208 03:33:44.040203 140051005212416 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.32467740774154663, loss=3.754453182220459
I0208 03:34:18.631096 140050996819712 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.29384323954582214, loss=3.694161891937256
I0208 03:34:53.211738 140051005212416 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.29514747858047485, loss=3.738124132156372
I0208 03:35:27.780413 140050996819712 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2882521152496338, loss=3.7140097618103027
I0208 03:36:02.370489 140051005212416 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.29366186261177063, loss=3.773563861846924
I0208 03:36:36.947349 140050996819712 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3034932315349579, loss=3.8212008476257324
I0208 03:37:11.532994 140051005212416 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.29691511392593384, loss=3.7556800842285156
I0208 03:37:46.122754 140050996819712 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3000228703022003, loss=3.7572531700134277
I0208 03:38:20.685820 140051005212416 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.32606518268585205, loss=3.707761764526367
I0208 03:38:55.241240 140050996819712 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3026061952114105, loss=3.7664358615875244
I0208 03:39:29.794382 140051005212416 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.3010859489440918, loss=3.7441368103027344
I0208 03:40:04.348499 140050996819712 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.29951775074005127, loss=3.7782135009765625
I0208 03:40:38.931673 140051005212416 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3092721998691559, loss=3.7675490379333496
I0208 03:41:13.522433 140050996819712 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.31610196828842163, loss=3.7819154262542725
I0208 03:41:48.097949 140051005212416 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.31063809990882874, loss=3.7416722774505615
I0208 03:42:22.692618 140050996819712 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3037453293800354, loss=3.756124258041382
I0208 03:42:57.287244 140051005212416 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2883496880531311, loss=3.6807689666748047
I0208 03:43:31.856324 140050996819712 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.31328636407852173, loss=3.8022217750549316
I0208 03:44:06.426334 140051005212416 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.3945312201976776, loss=3.736757516860962
I0208 03:44:40.975550 140050996819712 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3131818175315857, loss=3.747180223464966
I0208 03:45:15.545148 140051005212416 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3065183162689209, loss=3.7447404861450195
I0208 03:45:50.106775 140050996819712 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3032072186470032, loss=3.710789442062378
I0208 03:46:24.691762 140051005212416 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.28966474533081055, loss=3.735966682434082
I0208 03:46:31.671858 140225696298816 spec.py:321] Evaluating on the training split.
I0208 03:46:34.653599 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:49:27.609195 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 03:49:30.300289 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:51:58.856791 140225696298816 spec.py:349] Evaluating on the test split.
I0208 03:52:01.547202 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 03:54:19.601171 140225696298816 submission_runner.py:408] Time since start: 49588.72s, 	Step: 87522, 	{'train/accuracy': 0.6859403252601624, 'train/loss': 1.6498194932937622, 'train/bleu': 34.449619726829866, 'validation/accuracy': 0.6880509853363037, 'validation/loss': 1.620361566543579, 'validation/bleu': 30.61483554539125, 'validation/num_examples': 3000, 'test/accuracy': 0.7043286561965942, 'test/loss': 1.5281420946121216, 'test/bleu': 30.191210875242326, 'test/num_examples': 3003, 'score': 30268.273729801178, 'total_duration': 49588.71892952919, 'accumulated_submission_time': 30268.273729801178, 'accumulated_eval_time': 19316.526229143143, 'accumulated_logging_time': 1.1193876266479492}
I0208 03:54:19.628660 140050996819712 logging_writer.py:48] [87522] accumulated_eval_time=19316.526229, accumulated_logging_time=1.119388, accumulated_submission_time=30268.273730, global_step=87522, preemption_count=0, score=30268.273730, test/accuracy=0.704329, test/bleu=30.191211, test/loss=1.528142, test/num_examples=3003, total_duration=49588.718930, train/accuracy=0.685940, train/bleu=34.449620, train/loss=1.649819, validation/accuracy=0.688051, validation/bleu=30.614836, validation/loss=1.620362, validation/num_examples=3000
I0208 03:54:46.802871 140051005212416 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2893427610397339, loss=3.7471463680267334
I0208 03:55:21.301112 140050996819712 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2983834743499756, loss=3.7227046489715576
I0208 03:55:55.871836 140051005212416 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3197895884513855, loss=3.723797082901001
I0208 03:56:30.442694 140050996819712 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3095729351043701, loss=3.711726665496826
I0208 03:57:04.994490 140051005212416 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3026738166809082, loss=3.7391510009765625
I0208 03:57:39.542762 140050996819712 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.30483755469322205, loss=3.7112185955047607
I0208 03:58:14.125155 140051005212416 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.31488582491874695, loss=3.777529239654541
I0208 03:58:48.750202 140050996819712 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.30427929759025574, loss=3.745807409286499
I0208 03:59:23.330312 140051005212416 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.30742812156677246, loss=3.753329038619995
I0208 03:59:57.920197 140050996819712 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.30556192994117737, loss=3.75319242477417
I0208 04:00:32.485936 140051005212416 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3099919557571411, loss=3.7346136569976807
I0208 04:01:07.085150 140050996819712 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.30001503229141235, loss=3.7460644245147705
I0208 04:01:41.662096 140051005212416 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3202013671398163, loss=3.8243353366851807
I0208 04:02:16.230726 140050996819712 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.30970853567123413, loss=3.7164785861968994
I0208 04:02:50.807790 140051005212416 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.30811190605163574, loss=3.7142093181610107
I0208 04:03:25.383975 140050996819712 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.337434321641922, loss=3.7790420055389404
I0208 04:03:59.977203 140051005212416 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.32632482051849365, loss=3.795111894607544
I0208 04:04:34.555401 140050996819712 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3139987289905548, loss=3.7763140201568604
I0208 04:05:09.119535 140051005212416 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.3002839684486389, loss=3.7148663997650146
I0208 04:05:43.695329 140050996819712 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.29602909088134766, loss=3.7062742710113525
I0208 04:06:18.282453 140051005212416 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.33165788650512695, loss=3.7068233489990234
I0208 04:06:52.862132 140050996819712 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3038606643676758, loss=3.718585729598999
I0208 04:07:27.465089 140051005212416 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.29665181040763855, loss=3.7338383197784424
I0208 04:08:02.035290 140050996819712 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.32444554567337036, loss=3.76173996925354
I0208 04:08:19.733191 140225696298816 spec.py:321] Evaluating on the training split.
I0208 04:08:22.705102 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:11:11.159229 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 04:11:13.839511 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:13:41.113137 140225696298816 spec.py:349] Evaluating on the test split.
I0208 04:13:43.822509 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:16:06.131230 140225696298816 submission_runner.py:408] Time since start: 50895.25s, 	Step: 89953, 	{'train/accuracy': 0.6895501613616943, 'train/loss': 1.6185948848724365, 'train/bleu': 35.17824123678239, 'validation/accuracy': 0.6874062418937683, 'validation/loss': 1.6165112257003784, 'validation/bleu': 30.320178766417413, 'validation/num_examples': 3000, 'test/accuracy': 0.7040846347808838, 'test/loss': 1.5222641229629517, 'test/bleu': 30.366536884259617, 'test/num_examples': 3003, 'score': 31108.289889335632, 'total_duration': 50895.24898195267, 'accumulated_submission_time': 31108.289889335632, 'accumulated_eval_time': 19782.92421078682, 'accumulated_logging_time': 1.1572742462158203}
I0208 04:16:06.160225 140051005212416 logging_writer.py:48] [89953] accumulated_eval_time=19782.924211, accumulated_logging_time=1.157274, accumulated_submission_time=31108.289889, global_step=89953, preemption_count=0, score=31108.289889, test/accuracy=0.704085, test/bleu=30.366537, test/loss=1.522264, test/num_examples=3003, total_duration=50895.248982, train/accuracy=0.689550, train/bleu=35.178241, train/loss=1.618595, validation/accuracy=0.687406, validation/bleu=30.320179, validation/loss=1.616511, validation/num_examples=3000
I0208 04:16:22.695559 140050996819712 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.31996166706085205, loss=3.7493486404418945
I0208 04:16:57.174198 140051005212416 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.311693400144577, loss=3.7415177822113037
I0208 04:17:31.729947 140050996819712 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.31232115626335144, loss=3.6591124534606934
I0208 04:18:06.306470 140051005212416 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.32058200240135193, loss=3.7529075145721436
I0208 04:18:40.892557 140050996819712 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.30390509963035583, loss=3.7275707721710205
I0208 04:19:15.484668 140051005212416 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2979578375816345, loss=3.6449170112609863
I0208 04:19:50.067820 140050996819712 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.3022356927394867, loss=3.7398126125335693
I0208 04:20:24.642513 140051005212416 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.3001529276371002, loss=3.6881587505340576
I0208 04:20:59.224510 140050996819712 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.3186641037464142, loss=3.7080135345458984
I0208 04:21:33.823139 140051005212416 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.31365588307380676, loss=3.7674367427825928
I0208 04:22:08.400508 140050996819712 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.30739498138427734, loss=3.727475643157959
I0208 04:22:42.980840 140051005212416 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.3455931842327118, loss=3.729614496231079
I0208 04:23:17.575985 140050996819712 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3074691891670227, loss=3.798644781112671
I0208 04:23:52.158366 140051005212416 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.30664652585983276, loss=3.6983132362365723
I0208 04:24:26.731544 140050996819712 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3097720444202423, loss=3.702570915222168
I0208 04:25:01.316238 140051005212416 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.31178516149520874, loss=3.767942428588867
I0208 04:25:35.888031 140050996819712 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.3029775023460388, loss=3.739278793334961
I0208 04:26:10.478490 140051005212416 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.3023539185523987, loss=3.7387871742248535
I0208 04:26:45.077174 140050996819712 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3127230107784271, loss=3.7980315685272217
I0208 04:27:19.654358 140051005212416 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2986437976360321, loss=3.696887254714966
I0208 04:27:54.227860 140050996819712 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3014034926891327, loss=3.7215383052825928
I0208 04:28:28.821553 140051005212416 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3136058449745178, loss=3.6719844341278076
I0208 04:29:03.381028 140050996819712 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.35653427243232727, loss=3.718834400177002
I0208 04:29:37.975052 140051005212416 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.33322080969810486, loss=3.7027034759521484
I0208 04:30:06.384778 140225696298816 spec.py:321] Evaluating on the training split.
I0208 04:30:09.362514 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:33:02.949379 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 04:33:05.627293 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:35:43.760480 140225696298816 spec.py:349] Evaluating on the test split.
I0208 04:35:46.436210 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:38:06.925236 140225696298816 submission_runner.py:408] Time since start: 52216.04s, 	Step: 92384, 	{'train/accuracy': 0.690657913684845, 'train/loss': 1.6202352046966553, 'train/bleu': 35.047649936271675, 'validation/accuracy': 0.6881749629974365, 'validation/loss': 1.61602783203125, 'validation/bleu': 30.26145651209316, 'validation/num_examples': 3000, 'test/accuracy': 0.7052234411239624, 'test/loss': 1.5193902254104614, 'test/bleu': 30.455149942129623, 'test/num_examples': 3003, 'score': 31948.42696595192, 'total_duration': 52216.04299545288, 'accumulated_submission_time': 31948.42696595192, 'accumulated_eval_time': 20263.464618206024, 'accumulated_logging_time': 1.1962149143218994}
I0208 04:38:06.953198 140050996819712 logging_writer.py:48] [92384] accumulated_eval_time=20263.464618, accumulated_logging_time=1.196215, accumulated_submission_time=31948.426966, global_step=92384, preemption_count=0, score=31948.426966, test/accuracy=0.705223, test/bleu=30.455150, test/loss=1.519390, test/num_examples=3003, total_duration=52216.042995, train/accuracy=0.690658, train/bleu=35.047650, train/loss=1.620235, validation/accuracy=0.688175, validation/bleu=30.261457, validation/loss=1.616028, validation/num_examples=3000
I0208 04:38:12.821359 140051005212416 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.31126758456230164, loss=3.7652580738067627
I0208 04:38:47.211970 140050996819712 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32282546162605286, loss=3.7489125728607178
I0208 04:39:21.739997 140051005212416 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.30945906043052673, loss=3.743797540664673
I0208 04:39:56.310425 140050996819712 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.30744466185569763, loss=3.725911855697632
I0208 04:40:30.882448 140051005212416 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.29758960008621216, loss=3.667961597442627
I0208 04:41:05.485850 140050996819712 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.30338945984840393, loss=3.683347225189209
I0208 04:41:40.052911 140051005212416 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.305209755897522, loss=3.717660903930664
I0208 04:42:14.613170 140050996819712 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.30647093057632446, loss=3.6872477531433105
I0208 04:42:49.198514 140051005212416 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.30768463015556335, loss=3.6587843894958496
I0208 04:43:23.795516 140050996819712 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.3010382056236267, loss=3.6820805072784424
I0208 04:43:58.369807 140051005212416 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.31437215209007263, loss=3.713308811187744
I0208 04:44:32.931597 140050996819712 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3476252555847168, loss=3.757939338684082
I0208 04:45:07.492486 140051005212416 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3233330547809601, loss=3.710134506225586
I0208 04:45:42.077341 140050996819712 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3332177400588989, loss=3.711604356765747
I0208 04:46:16.636462 140051005212416 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.31477001309394836, loss=3.7069523334503174
I0208 04:46:51.201051 140050996819712 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3206805884838104, loss=3.758453130722046
I0208 04:47:25.767392 140051005212416 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3112316429615021, loss=3.687775135040283
I0208 04:48:00.345711 140050996819712 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3046160936355591, loss=3.722114324569702
I0208 04:48:34.879297 140051005212416 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.31234538555145264, loss=3.6962053775787354
I0208 04:49:09.469138 140050996819712 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.31886085867881775, loss=3.746396064758301
I0208 04:49:44.058563 140051005212416 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.33659178018569946, loss=3.7136569023132324
I0208 04:50:18.643080 140050996819712 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.31001418828964233, loss=3.7448699474334717
I0208 04:50:53.238981 140051005212416 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3110791742801666, loss=3.7160699367523193
I0208 04:51:27.821950 140050996819712 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.31416288018226624, loss=3.7198126316070557
I0208 04:52:02.383640 140051005212416 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3273732364177704, loss=3.671128511428833
I0208 04:52:06.952700 140225696298816 spec.py:321] Evaluating on the training split.
I0208 04:52:09.927690 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:55:01.186473 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 04:55:03.872957 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 04:57:38.833111 140225696298816 spec.py:349] Evaluating on the test split.
I0208 04:57:41.509244 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:00:07.009262 140225696298816 submission_runner.py:408] Time since start: 53536.13s, 	Step: 94815, 	{'train/accuracy': 0.7034842371940613, 'train/loss': 1.5385183095932007, 'train/bleu': 35.83842989432134, 'validation/accuracy': 0.6895264983177185, 'validation/loss': 1.6064814329147339, 'validation/bleu': 30.30097926569033, 'validation/num_examples': 3000, 'test/accuracy': 0.7068154215812683, 'test/loss': 1.5066797733306885, 'test/bleu': 30.62494146013703, 'test/num_examples': 3003, 'score': 32788.33744096756, 'total_duration': 53536.12700033188, 'accumulated_submission_time': 32788.33744096756, 'accumulated_eval_time': 20743.521106243134, 'accumulated_logging_time': 1.2359349727630615}
I0208 05:00:07.037641 140050996819712 logging_writer.py:48] [94815] accumulated_eval_time=20743.521106, accumulated_logging_time=1.235935, accumulated_submission_time=32788.337441, global_step=94815, preemption_count=0, score=32788.337441, test/accuracy=0.706815, test/bleu=30.624941, test/loss=1.506680, test/num_examples=3003, total_duration=53536.127000, train/accuracy=0.703484, train/bleu=35.838430, train/loss=1.538518, validation/accuracy=0.689526, validation/bleu=30.300979, validation/loss=1.606481, validation/num_examples=3000
I0208 05:00:36.641822 140051005212416 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.30064377188682556, loss=3.6948814392089844
I0208 05:01:11.142227 140050996819712 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3155125379562378, loss=3.7188003063201904
I0208 05:01:45.705656 140051005212416 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.31076908111572266, loss=3.6976852416992188
I0208 05:02:20.279341 140050996819712 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3267455995082855, loss=3.7462151050567627
I0208 05:02:54.858532 140051005212416 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3115001618862152, loss=3.7382397651672363
I0208 05:03:29.439572 140050996819712 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3089497983455658, loss=3.6348137855529785
I0208 05:04:04.025439 140051005212416 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3197472393512726, loss=3.7561230659484863
I0208 05:04:38.615165 140050996819712 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.32315564155578613, loss=3.7163562774658203
I0208 05:05:13.192664 140051005212416 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3100830018520355, loss=3.68226957321167
I0208 05:05:47.770182 140050996819712 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.30565083026885986, loss=3.6724486351013184
I0208 05:06:22.344697 140051005212416 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3293212056159973, loss=3.6828553676605225
I0208 05:06:56.946335 140050996819712 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3233409523963928, loss=3.725687026977539
I0208 05:07:31.515572 140051005212416 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.31151145696640015, loss=3.7162892818450928
I0208 05:08:06.096037 140050996819712 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.31654050946235657, loss=3.689791440963745
I0208 05:08:40.647038 140051005212416 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3164980113506317, loss=3.7577919960021973
I0208 05:09:15.241050 140050996819712 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3515464961528778, loss=3.696913957595825
I0208 05:09:49.835788 140051005212416 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3243788182735443, loss=3.7281980514526367
I0208 05:10:24.443804 140050996819712 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.31328293681144714, loss=3.7343080043792725
I0208 05:10:58.997754 140051005212416 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3220341205596924, loss=3.670966625213623
I0208 05:11:33.580207 140050996819712 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3370576798915863, loss=3.754089832305908
I0208 05:12:08.162588 140051005212416 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.33433404564857483, loss=3.695944309234619
I0208 05:12:42.739783 140050996819712 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.33835330605506897, loss=3.7366998195648193
I0208 05:13:17.304085 140051005212416 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3449975848197937, loss=3.6491787433624268
I0208 05:13:51.862313 140050996819712 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3086759150028229, loss=3.7034924030303955
I0208 05:14:07.165476 140225696298816 spec.py:321] Evaluating on the training split.
I0208 05:14:10.146182 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:17:01.575679 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 05:17:04.263470 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:19:28.247894 140225696298816 spec.py:349] Evaluating on the test split.
I0208 05:19:30.927506 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:21:52.492546 140225696298816 submission_runner.py:408] Time since start: 54841.61s, 	Step: 97246, 	{'train/accuracy': 0.6954914927482605, 'train/loss': 1.5836246013641357, 'train/bleu': 35.719509026518935, 'validation/accuracy': 0.6894396543502808, 'validation/loss': 1.608036756515503, 'validation/bleu': 30.444119717517655, 'validation/num_examples': 3000, 'test/accuracy': 0.706466794013977, 'test/loss': 1.511273980140686, 'test/bleu': 30.46507838062227, 'test/num_examples': 3003, 'score': 33628.37856912613, 'total_duration': 54841.610292196274, 'accumulated_submission_time': 33628.37856912613, 'accumulated_eval_time': 21208.848113775253, 'accumulated_logging_time': 1.274146318435669}
I0208 05:21:52.520480 140051005212416 logging_writer.py:48] [97246] accumulated_eval_time=21208.848114, accumulated_logging_time=1.274146, accumulated_submission_time=33628.378569, global_step=97246, preemption_count=0, score=33628.378569, test/accuracy=0.706467, test/bleu=30.465078, test/loss=1.511274, test/num_examples=3003, total_duration=54841.610292, train/accuracy=0.695491, train/bleu=35.719509, train/loss=1.583625, validation/accuracy=0.689440, validation/bleu=30.444120, validation/loss=1.608037, validation/num_examples=3000
I0208 05:22:11.500568 140050996819712 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3241306245326996, loss=3.745993137359619
I0208 05:22:45.960461 140051005212416 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3224296569824219, loss=3.679755687713623
I0208 05:23:20.512236 140050996819712 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.35170361399650574, loss=3.7659876346588135
I0208 05:23:55.091045 140051005212416 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.323940634727478, loss=3.6929619312286377
I0208 05:24:29.661365 140050996819712 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.30677247047424316, loss=3.6940419673919678
I0208 05:25:04.297438 140051005212416 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3294261693954468, loss=3.6663012504577637
I0208 05:25:38.868619 140050996819712 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.32069942355155945, loss=3.724947690963745
I0208 05:26:13.456967 140051005212416 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.33600154519081116, loss=3.728710889816284
I0208 05:26:48.045760 140050996819712 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3284590542316437, loss=3.6756606101989746
I0208 05:27:22.628435 140051005212416 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.33359572291374207, loss=3.725160598754883
I0208 05:27:57.216404 140050996819712 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.33351635932922363, loss=3.7046732902526855
I0208 05:28:31.792368 140051005212416 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3209294080734253, loss=3.690525770187378
I0208 05:29:06.352571 140050996819712 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3174331784248352, loss=3.665118932723999
I0208 05:29:40.934079 140051005212416 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3394552767276764, loss=3.680546283721924
I0208 05:30:15.496545 140050996819712 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.322959840297699, loss=3.6413414478302
I0208 05:30:50.072534 140051005212416 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3734096586704254, loss=3.7509067058563232
I0208 05:31:24.676693 140050996819712 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.31863245368003845, loss=3.6705262660980225
I0208 05:31:59.245793 140051005212416 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3340895473957062, loss=3.6855993270874023
I0208 05:32:33.836327 140050996819712 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.32499614357948303, loss=3.7329342365264893
I0208 05:33:08.420237 140051005212416 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3197059631347656, loss=3.6520988941192627
I0208 05:33:43.017929 140050996819712 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.33021607995033264, loss=3.70155930519104
I0208 05:34:17.629614 140051005212416 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3290652632713318, loss=3.6812684535980225
I0208 05:34:52.214574 140050996819712 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3393097519874573, loss=3.6989500522613525
I0208 05:35:26.821711 140051005212416 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3422304093837738, loss=3.6571407318115234
I0208 05:35:52.820328 140225696298816 spec.py:321] Evaluating on the training split.
I0208 05:35:55.799422 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:38:45.448454 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 05:38:48.131087 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:41:17.316895 140225696298816 spec.py:349] Evaluating on the test split.
I0208 05:41:20.009724 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 05:43:39.601596 140225696298816 submission_runner.py:408] Time since start: 56148.72s, 	Step: 99677, 	{'train/accuracy': 0.6979652643203735, 'train/loss': 1.5806125402450562, 'train/bleu': 35.63073779555025, 'validation/accuracy': 0.6885221600532532, 'validation/loss': 1.6104274988174438, 'validation/bleu': 30.35029837043829, 'validation/num_examples': 3000, 'test/accuracy': 0.7069316506385803, 'test/loss': 1.5114892721176147, 'test/bleu': 30.37346591024028, 'test/num_examples': 3003, 'score': 34468.591000556946, 'total_duration': 56148.71933174133, 'accumulated_submission_time': 34468.591000556946, 'accumulated_eval_time': 21675.629316568375, 'accumulated_logging_time': 1.3121390342712402}
I0208 05:43:39.630786 140050996819712 logging_writer.py:48] [99677] accumulated_eval_time=21675.629317, accumulated_logging_time=1.312139, accumulated_submission_time=34468.591001, global_step=99677, preemption_count=0, score=34468.591001, test/accuracy=0.706932, test/bleu=30.373466, test/loss=1.511489, test/num_examples=3003, total_duration=56148.719332, train/accuracy=0.697965, train/bleu=35.630738, train/loss=1.580613, validation/accuracy=0.688522, validation/bleu=30.350298, validation/loss=1.610427, validation/num_examples=3000
I0208 05:43:47.908183 140051005212416 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.32447314262390137, loss=3.6883411407470703
I0208 05:44:22.346252 140050996819712 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.32518526911735535, loss=3.681837320327759
I0208 05:44:56.902232 140051005212416 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.345201313495636, loss=3.72255277633667
I0208 05:45:31.485220 140050996819712 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3512411415576935, loss=3.730611801147461
I0208 05:46:06.070730 140051005212416 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.34010010957717896, loss=3.686676025390625
I0208 05:46:40.646481 140050996819712 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3454563617706299, loss=3.6736536026000977
I0208 05:47:15.224776 140051005212416 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.31961384415626526, loss=3.685415744781494
I0208 05:47:49.791074 140050996819712 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3328508138656616, loss=3.6842448711395264
I0208 05:48:24.364993 140051005212416 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3365425765514374, loss=3.725153923034668
I0208 05:48:58.946693 140050996819712 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3346840739250183, loss=3.686678409576416
I0208 05:49:33.535035 140051005212416 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3533006012439728, loss=3.7106337547302246
I0208 05:50:08.127475 140050996819712 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.33194199204444885, loss=3.6467936038970947
I0208 05:50:42.710891 140051005212416 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.34027689695358276, loss=3.6712486743927
I0208 05:51:17.310855 140050996819712 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3418750464916229, loss=3.7022147178649902
I0208 05:51:51.880570 140051005212416 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3539147973060608, loss=3.6580138206481934
I0208 05:52:26.443943 140050996819712 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.33821019530296326, loss=3.712347984313965
I0208 05:53:01.042592 140051005212416 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.33021080493927, loss=3.753469467163086
I0208 05:53:35.621051 140050996819712 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.346623957157135, loss=3.688859462738037
I0208 05:54:10.203250 140051005212416 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3386659324169159, loss=3.724141836166382
I0208 05:54:44.776972 140050996819712 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3699384331703186, loss=3.764801263809204
I0208 05:55:19.368915 140051005212416 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.32453033328056335, loss=3.691141366958618
I0208 05:55:53.902596 140050996819712 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3620765507221222, loss=3.7046451568603516
I0208 05:56:28.456403 140051005212416 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3525241017341614, loss=3.7133235931396484
I0208 05:57:03.029919 140050996819712 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.30762529373168945, loss=3.670088529586792
I0208 05:57:37.623732 140051005212416 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.33125796914100647, loss=3.630584955215454
I0208 05:57:39.768149 140225696298816 spec.py:321] Evaluating on the training split.
I0208 05:57:42.745509 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:00:36.181515 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 06:00:38.861230 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:03:03.653962 140225696298816 spec.py:349] Evaluating on the test split.
I0208 06:03:06.342852 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:05:22.368722 140225696298816 submission_runner.py:408] Time since start: 57451.49s, 	Step: 102108, 	{'train/accuracy': 0.7036283016204834, 'train/loss': 1.5418643951416016, 'train/bleu': 36.0726587683943, 'validation/accuracy': 0.689315676689148, 'validation/loss': 1.6059999465942383, 'validation/bleu': 30.482721601595376, 'validation/num_examples': 3000, 'test/accuracy': 0.7062808871269226, 'test/loss': 1.5039721727371216, 'test/bleu': 30.5054449175256, 'test/num_examples': 3003, 'score': 35308.638811826706, 'total_duration': 57451.48646616936, 'accumulated_submission_time': 35308.638811826706, 'accumulated_eval_time': 22138.22982096672, 'accumulated_logging_time': 1.3539230823516846}
I0208 06:05:22.397192 140050996819712 logging_writer.py:48] [102108] accumulated_eval_time=22138.229821, accumulated_logging_time=1.353923, accumulated_submission_time=35308.638812, global_step=102108, preemption_count=0, score=35308.638812, test/accuracy=0.706281, test/bleu=30.505445, test/loss=1.503972, test/num_examples=3003, total_duration=57451.486466, train/accuracy=0.703628, train/bleu=36.072659, train/loss=1.541864, validation/accuracy=0.689316, validation/bleu=30.482722, validation/loss=1.606000, validation/num_examples=3000
I0208 06:05:54.456115 140051005212416 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3357039988040924, loss=3.6858530044555664
I0208 06:06:28.952838 140050996819712 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3417980968952179, loss=3.6770246028900146
I0208 06:07:03.506191 140051005212416 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3304625451564789, loss=3.6402108669281006
I0208 06:07:38.061978 140050996819712 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3328168988227844, loss=3.7147023677825928
I0208 06:08:12.643998 140051005212416 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3355603814125061, loss=3.678635358810425
I0208 06:08:47.226221 140050996819712 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3535897433757782, loss=3.7640366554260254
I0208 06:09:21.827769 140051005212416 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.33775171637535095, loss=3.719958543777466
I0208 06:09:56.413002 140050996819712 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3623599708080292, loss=3.6584396362304688
I0208 06:10:30.989264 140051005212416 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.34755632281303406, loss=3.710491895675659
I0208 06:11:05.556385 140050996819712 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.372329443693161, loss=3.695939540863037
I0208 06:11:40.131734 140051005212416 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3352864980697632, loss=3.6653177738189697
I0208 06:12:14.715349 140050996819712 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.35289594531059265, loss=3.7007620334625244
I0208 06:12:49.291563 140051005212416 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.33830347657203674, loss=3.67484188079834
I0208 06:13:23.847055 140050996819712 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3378956615924835, loss=3.6685025691986084
I0208 06:13:58.423434 140051005212416 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3649173974990845, loss=3.684368371963501
I0208 06:14:32.995223 140050996819712 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3333350419998169, loss=3.6820878982543945
I0208 06:15:07.563349 140051005212416 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3350820243358612, loss=3.6264595985412598
I0208 06:15:42.150599 140050996819712 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3532835841178894, loss=3.694281816482544
I0208 06:16:16.743988 140051005212416 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3457567095756531, loss=3.6591060161590576
I0208 06:16:51.302567 140050996819712 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.32744133472442627, loss=3.698669910430908
I0208 06:17:25.876147 140051005212416 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.33532676100730896, loss=3.6783032417297363
I0208 06:18:00.443474 140050996819712 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3592745065689087, loss=3.692997455596924
I0208 06:18:35.047348 140051005212416 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.33474400639533997, loss=3.68753981590271
I0208 06:19:09.622674 140050996819712 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3675105571746826, loss=3.733206272125244
I0208 06:19:22.472733 140225696298816 spec.py:321] Evaluating on the training split.
I0208 06:19:25.450287 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:22:21.636248 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 06:22:24.332091 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:24:46.996630 140225696298816 spec.py:349] Evaluating on the test split.
I0208 06:24:49.692789 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:27:18.445477 140225696298816 submission_runner.py:408] Time since start: 58767.56s, 	Step: 104539, 	{'train/accuracy': 0.6998471617698669, 'train/loss': 1.5669548511505127, 'train/bleu': 35.92477397313134, 'validation/accuracy': 0.6904687881469727, 'validation/loss': 1.6009769439697266, 'validation/bleu': 30.452983416343674, 'validation/num_examples': 3000, 'test/accuracy': 0.7068619132041931, 'test/loss': 1.5041028261184692, 'test/bleu': 30.359611628316312, 'test/num_examples': 3003, 'score': 36148.625277519226, 'total_duration': 58767.56322598457, 'accumulated_submission_time': 36148.625277519226, 'accumulated_eval_time': 22614.202502965927, 'accumulated_logging_time': 1.394146203994751}
I0208 06:27:18.475595 140051005212416 logging_writer.py:48] [104539] accumulated_eval_time=22614.202503, accumulated_logging_time=1.394146, accumulated_submission_time=36148.625278, global_step=104539, preemption_count=0, score=36148.625278, test/accuracy=0.706862, test/bleu=30.359612, test/loss=1.504103, test/num_examples=3003, total_duration=58767.563226, train/accuracy=0.699847, train/bleu=35.924774, train/loss=1.566955, validation/accuracy=0.690469, validation/bleu=30.452983, validation/loss=1.600977, validation/num_examples=3000
I0208 06:27:39.802146 140050996819712 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3323765993118286, loss=3.6459107398986816
I0208 06:28:14.286129 140051005212416 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.34199103713035583, loss=3.681501865386963
I0208 06:28:48.850856 140050996819712 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3592465817928314, loss=3.6756975650787354
I0208 06:29:23.383860 140051005212416 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.37465494871139526, loss=3.6703104972839355
I0208 06:29:57.950977 140050996819712 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.35783442854881287, loss=3.671372175216675
I0208 06:30:32.524585 140051005212416 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.34536057710647583, loss=3.642282485961914
I0208 06:31:07.078790 140050996819712 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.36418792605400085, loss=3.7092695236206055
I0208 06:31:41.652771 140051005212416 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.33085066080093384, loss=3.6828489303588867
I0208 06:32:16.238712 140050996819712 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3621118664741516, loss=3.647603750228882
I0208 06:32:50.813265 140051005212416 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.36421963572502136, loss=3.657090902328491
I0208 06:33:25.386532 140050996819712 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3556632399559021, loss=3.671539306640625
I0208 06:33:59.950376 140051005212416 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.33638688921928406, loss=3.659531831741333
I0208 06:34:34.512599 140050996819712 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.34951937198638916, loss=3.6715517044067383
I0208 06:35:09.054487 140051005212416 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.35401636362075806, loss=3.643563985824585
I0208 06:35:43.615473 140050996819712 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.3532736003398895, loss=3.6049885749816895
I0208 06:36:18.185120 140051005212416 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3548872470855713, loss=3.6336581707000732
I0208 06:36:52.774239 140050996819712 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3459586203098297, loss=3.694765090942383
I0208 06:37:27.341152 140051005212416 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.34793174266815186, loss=3.699650764465332
I0208 06:38:01.901776 140050996819712 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3501933813095093, loss=3.672224521636963
I0208 06:38:36.509175 140051005212416 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3478715121746063, loss=3.648322582244873
I0208 06:39:11.074856 140050996819712 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.35368427634239197, loss=3.6905126571655273
I0208 06:39:45.647372 140051005212416 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.37953078746795654, loss=3.697913885116577
I0208 06:40:20.216453 140050996819712 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.3640311062335968, loss=3.672186851501465
I0208 06:40:54.793073 140051005212416 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3467046320438385, loss=3.6390135288238525
I0208 06:41:18.740626 140225696298816 spec.py:321] Evaluating on the training split.
I0208 06:41:21.722796 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:44:24.177897 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 06:44:26.869231 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:47:16.514089 140225696298816 spec.py:349] Evaluating on the test split.
I0208 06:47:19.194789 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 06:49:47.087621 140225696298816 submission_runner.py:408] Time since start: 60116.21s, 	Step: 106971, 	{'train/accuracy': 0.7189033031463623, 'train/loss': 1.4635096788406372, 'train/bleu': 37.323135864973146, 'validation/accuracy': 0.6899852156639099, 'validation/loss': 1.603049635887146, 'validation/bleu': 30.498854117375075, 'validation/num_examples': 3000, 'test/accuracy': 0.7066411375999451, 'test/loss': 1.5038784742355347, 'test/bleu': 30.1928181417836, 'test/num_examples': 3003, 'score': 36988.803647994995, 'total_duration': 60116.20536804199, 'accumulated_submission_time': 36988.803647994995, 'accumulated_eval_time': 23122.549444437027, 'accumulated_logging_time': 1.4340860843658447}
I0208 06:49:47.117635 140050996819712 logging_writer.py:48] [106971] accumulated_eval_time=23122.549444, accumulated_logging_time=1.434086, accumulated_submission_time=36988.803648, global_step=106971, preemption_count=0, score=36988.803648, test/accuracy=0.706641, test/bleu=30.192818, test/loss=1.503878, test/num_examples=3003, total_duration=60116.205368, train/accuracy=0.718903, train/bleu=37.323136, train/loss=1.463510, validation/accuracy=0.689985, validation/bleu=30.498854, validation/loss=1.603050, validation/num_examples=3000
I0208 06:49:57.439943 140051005212416 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3645825684070587, loss=3.707841634750366
I0208 06:50:31.884383 140050996819712 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3670669198036194, loss=3.6807799339294434
I0208 06:51:06.443593 140051005212416 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.36098650097846985, loss=3.691932201385498
I0208 06:51:41.022942 140050996819712 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3521585762500763, loss=3.659714937210083
I0208 06:52:15.632075 140051005212416 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3586869239807129, loss=3.676729440689087
I0208 06:52:50.228263 140050996819712 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3490495979785919, loss=3.617480993270874
I0208 06:53:24.823246 140051005212416 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3423324227333069, loss=3.6346869468688965
I0208 06:53:59.404591 140050996819712 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3381250202655792, loss=3.632317066192627
I0208 06:54:34.000094 140051005212416 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.3487466275691986, loss=3.658670425415039
I0208 06:55:08.592648 140050996819712 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.48495176434516907, loss=3.6134486198425293
I0208 06:55:43.198851 140051005212416 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.36099401116371155, loss=3.6711082458496094
I0208 06:56:17.773398 140050996819712 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.35118329524993896, loss=3.6628875732421875
I0208 06:56:52.362757 140051005212416 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3583509624004364, loss=3.665783643722534
I0208 06:57:26.929218 140050996819712 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3925963342189789, loss=3.695709228515625
I0208 06:58:01.528934 140051005212416 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3558148145675659, loss=3.6425044536590576
I0208 06:58:36.116328 140050996819712 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.35481348633766174, loss=3.649235248565674
I0208 06:59:10.696782 140051005212416 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3641780912876129, loss=3.6844873428344727
I0208 06:59:45.264946 140050996819712 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3723524808883667, loss=3.6771862506866455
I0208 07:00:19.854556 140051005212416 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3449861705303192, loss=3.6352109909057617
I0208 07:00:54.444976 140050996819712 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.36776235699653625, loss=3.6221530437469482
I0208 07:01:29.032887 140051005212416 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3508074879646301, loss=3.6717004776000977
I0208 07:02:03.620383 140050996819712 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.35691022872924805, loss=3.6162595748901367
I0208 07:02:38.203822 140051005212416 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.36695000529289246, loss=3.6788833141326904
I0208 07:03:12.781869 140050996819712 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3719865679740906, loss=3.632429599761963
I0208 07:03:47.350777 140051005212416 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3503720760345459, loss=3.6356592178344727
I0208 07:03:47.357347 140225696298816 spec.py:321] Evaluating on the training split.
I0208 07:03:50.062533 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:06:32.498941 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 07:06:35.183321 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:08:58.675387 140225696298816 spec.py:349] Evaluating on the test split.
I0208 07:09:01.366648 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:11:14.738756 140225696298816 submission_runner.py:408] Time since start: 61403.86s, 	Step: 109401, 	{'train/accuracy': 0.7105783224105835, 'train/loss': 1.5062320232391357, 'train/bleu': 36.803288837604285, 'validation/accuracy': 0.6902704238891602, 'validation/loss': 1.5982686281204224, 'validation/bleu': 30.681621790036214, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511487960815, 'test/loss': 1.4960204362869263, 'test/bleu': 30.79524511078754, 'test/num_examples': 3003, 'score': 37828.95445275307, 'total_duration': 61403.85651016235, 'accumulated_submission_time': 37828.95445275307, 'accumulated_eval_time': 23569.930787324905, 'accumulated_logging_time': 1.4757776260375977}
I0208 07:11:14.770684 140050996819712 logging_writer.py:48] [109401] accumulated_eval_time=23569.930787, accumulated_logging_time=1.475778, accumulated_submission_time=37828.954453, global_step=109401, preemption_count=0, score=37828.954453, test/accuracy=0.709151, test/bleu=30.795245, test/loss=1.496020, test/num_examples=3003, total_duration=61403.856510, train/accuracy=0.710578, train/bleu=36.803289, train/loss=1.506232, validation/accuracy=0.690270, validation/bleu=30.681622, validation/loss=1.598269, validation/num_examples=3000
I0208 07:11:49.214358 140051005212416 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.38717007637023926, loss=3.701112985610962
I0208 07:12:23.734344 140050996819712 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.38192638754844666, loss=3.603649139404297
I0208 07:12:58.316413 140051005212416 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3488193452358246, loss=3.6689045429229736
I0208 07:13:32.872719 140050996819712 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.374180406332016, loss=3.596240282058716
I0208 07:14:07.444086 140051005212416 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.39045658707618713, loss=3.675530195236206
I0208 07:14:42.025109 140050996819712 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3504653871059418, loss=3.650076150894165
I0208 07:15:16.594999 140051005212416 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3772676885128021, loss=3.6674296855926514
I0208 07:15:51.180744 140050996819712 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3559093773365021, loss=3.6793415546417236
I0208 07:16:25.773173 140051005212416 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.35137903690338135, loss=3.6378300189971924
I0208 07:17:00.385482 140050996819712 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3709617257118225, loss=3.7209737300872803
I0208 07:17:34.969651 140051005212416 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.37573009729385376, loss=3.634641647338867
I0208 07:18:09.555357 140050996819712 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.37203994393348694, loss=3.603705644607544
I0208 07:18:44.128600 140051005212416 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.37709203362464905, loss=3.692577362060547
I0208 07:19:18.698526 140050996819712 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3669789433479309, loss=3.68215012550354
I0208 07:19:53.253562 140051005212416 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.34165477752685547, loss=3.7000248432159424
I0208 07:20:27.829090 140050996819712 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3645116686820984, loss=3.651606321334839
I0208 07:21:02.406803 140051005212416 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3597545027732849, loss=3.6662864685058594
I0208 07:21:37.001555 140050996819712 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3661915957927704, loss=3.6115939617156982
I0208 07:22:11.594534 140051005212416 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.374093621969223, loss=3.656867504119873
I0208 07:22:46.188486 140050996819712 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.36310726404190063, loss=3.6793699264526367
I0208 07:23:20.756930 140051005212416 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3676190972328186, loss=3.678403615951538
I0208 07:23:55.318788 140050996819712 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3786488473415375, loss=3.6522932052612305
I0208 07:24:29.919349 140051005212416 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3633691966533661, loss=3.6388871669769287
I0208 07:25:04.484999 140050996819712 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3613069951534271, loss=3.6265645027160645
I0208 07:25:14.939060 140225696298816 spec.py:321] Evaluating on the training split.
I0208 07:25:17.921057 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:27:58.245363 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 07:28:00.930997 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:30:32.324580 140225696298816 spec.py:349] Evaluating on the test split.
I0208 07:30:35.018658 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:32:57.832780 140225696298816 submission_runner.py:408] Time since start: 62706.95s, 	Step: 111832, 	{'train/accuracy': 0.7118147611618042, 'train/loss': 1.5033549070358276, 'train/bleu': 36.46637331018224, 'validation/accuracy': 0.6907911896705627, 'validation/loss': 1.6010395288467407, 'validation/bleu': 30.68753003150424, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.5006964206695557, 'test/bleu': 30.644621518964946, 'test/num_examples': 3003, 'score': 38669.03541469574, 'total_duration': 62706.950540065765, 'accumulated_submission_time': 38669.03541469574, 'accumulated_eval_time': 24032.82445716858, 'accumulated_logging_time': 1.5178844928741455}
I0208 07:32:57.864413 140051005212416 logging_writer.py:48] [111832] accumulated_eval_time=24032.824457, accumulated_logging_time=1.517884, accumulated_submission_time=38669.035415, global_step=111832, preemption_count=0, score=38669.035415, test/accuracy=0.709093, test/bleu=30.644622, test/loss=1.500696, test/num_examples=3003, total_duration=62706.950540, train/accuracy=0.711815, train/bleu=36.466373, train/loss=1.503355, validation/accuracy=0.690791, validation/bleu=30.687530, validation/loss=1.601040, validation/num_examples=3000
I0208 07:33:21.604460 140050996819712 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.3738739490509033, loss=3.687544345855713
I0208 07:33:56.104215 140051005212416 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3743809759616852, loss=3.676060199737549
I0208 07:34:30.666413 140050996819712 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.36254531145095825, loss=3.611300230026245
I0208 07:35:05.247587 140051005212416 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.40542346239089966, loss=3.611360788345337
I0208 07:35:39.833034 140050996819712 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.35471683740615845, loss=3.6221108436584473
I0208 07:36:14.400114 140051005212416 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3800559341907501, loss=3.6116690635681152
I0208 07:36:48.956472 140050996819712 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.38789239525794983, loss=3.6455283164978027
I0208 07:37:23.546448 140051005212416 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3872182369232178, loss=3.655102252960205
I0208 07:37:58.111396 140050996819712 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.37579479813575745, loss=3.5888376235961914
I0208 07:38:32.692400 140051005212416 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3660844564437866, loss=3.629973888397217
I0208 07:39:07.263410 140050996819712 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3579272925853729, loss=3.624951124191284
I0208 07:39:41.862194 140051005212416 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3451276421546936, loss=3.573209285736084
I0208 07:40:16.443493 140050996819712 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3731253147125244, loss=3.5833613872528076
I0208 07:40:51.022614 140051005212416 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.34623172879219055, loss=3.5811233520507812
I0208 07:41:25.614897 140050996819712 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.36788246035575867, loss=3.656787157058716
I0208 07:42:00.214036 140051005212416 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.37851154804229736, loss=3.6313040256500244
I0208 07:42:34.835048 140050996819712 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3688257336616516, loss=3.644876718521118
I0208 07:43:09.427243 140051005212416 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3843145966529846, loss=3.630481243133545
I0208 07:43:44.004086 140050996819712 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.380061537027359, loss=3.6443302631378174
I0208 07:44:18.597401 140051005212416 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3728726804256439, loss=3.6183390617370605
I0208 07:44:53.171684 140050996819712 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.3749644458293915, loss=3.6100668907165527
I0208 07:45:27.741226 140051005212416 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3678784966468811, loss=3.633967638015747
I0208 07:46:02.321913 140050996819712 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.35669127106666565, loss=3.630460739135742
I0208 07:46:36.871083 140051005212416 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.37912717461586, loss=3.6575191020965576
I0208 07:46:58.030964 140225696298816 spec.py:321] Evaluating on the training split.
I0208 07:47:01.032897 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:49:34.607424 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 07:49:37.295441 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:52:06.152517 140225696298816 spec.py:349] Evaluating on the test split.
I0208 07:52:08.835446 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 07:54:27.933128 140225696298816 submission_runner.py:408] Time since start: 63997.05s, 	Step: 114263, 	{'train/accuracy': 0.7177315354347229, 'train/loss': 1.4715121984481812, 'train/bleu': 37.124871771433476, 'validation/accuracy': 0.6904315948486328, 'validation/loss': 1.597692847251892, 'validation/bleu': 30.83881345974083, 'validation/num_examples': 3000, 'test/accuracy': 0.708512008190155, 'test/loss': 1.4953467845916748, 'test/bleu': 30.556692618619714, 'test/num_examples': 3003, 'score': 39509.11229848862, 'total_duration': 63997.05086636543, 'accumulated_submission_time': 39509.11229848862, 'accumulated_eval_time': 24482.726551771164, 'accumulated_logging_time': 1.5615882873535156}
I0208 07:54:27.963529 140050996819712 logging_writer.py:48] [114263] accumulated_eval_time=24482.726552, accumulated_logging_time=1.561588, accumulated_submission_time=39509.112298, global_step=114263, preemption_count=0, score=39509.112298, test/accuracy=0.708512, test/bleu=30.556693, test/loss=1.495347, test/num_examples=3003, total_duration=63997.050866, train/accuracy=0.717732, train/bleu=37.124872, train/loss=1.471512, validation/accuracy=0.690432, validation/bleu=30.838813, validation/loss=1.597693, validation/num_examples=3000
I0208 07:54:41.074769 140051005212416 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.38019558787345886, loss=3.649573564529419
I0208 07:55:15.562977 140050996819712 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.37288719415664673, loss=3.6316871643066406
I0208 07:55:50.130349 140051005212416 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3952372372150421, loss=3.6991050243377686
I0208 07:56:24.685031 140050996819712 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.39406466484069824, loss=3.6272263526916504
I0208 07:56:59.264945 140051005212416 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3851450979709625, loss=3.595547676086426
I0208 07:57:33.845037 140050996819712 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.36908724904060364, loss=3.6172423362731934
I0208 07:58:08.429180 140051005212416 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.3615425229072571, loss=3.628650665283203
I0208 07:58:42.985260 140050996819712 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.38400471210479736, loss=3.6073226928710938
I0208 07:59:17.549363 140051005212416 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.36410796642303467, loss=3.5963516235351562
I0208 07:59:52.109899 140050996819712 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3795538544654846, loss=3.647667407989502
I0208 08:00:26.673332 140051005212416 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.37455976009368896, loss=3.6485984325408936
I0208 08:01:01.229033 140050996819712 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.37588343024253845, loss=3.6528985500335693
I0208 08:01:35.794833 140051005212416 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.37976521253585815, loss=3.670413017272949
I0208 08:02:10.354060 140050996819712 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3711983561515808, loss=3.6480588912963867
I0208 08:02:44.946284 140051005212416 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.37921133637428284, loss=3.659961223602295
I0208 08:03:19.520502 140050996819712 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.371516615152359, loss=3.630880117416382
I0208 08:03:54.106514 140051005212416 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.38377460837364197, loss=3.637214183807373
I0208 08:04:28.678044 140050996819712 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3806760907173157, loss=3.6342031955718994
I0208 08:05:03.255187 140051005212416 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3744559586048126, loss=3.6061031818389893
I0208 08:05:37.845937 140050996819712 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.36936745047569275, loss=3.610546112060547
I0208 08:06:12.432347 140051005212416 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3801614046096802, loss=3.6261422634124756
I0208 08:06:47.022422 140050996819712 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3622830808162689, loss=3.569018602371216
I0208 08:07:21.625617 140051005212416 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.37995338439941406, loss=3.6279735565185547
I0208 08:07:56.208910 140050996819712 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.3781868517398834, loss=3.636410713195801
I0208 08:08:28.071502 140225696298816 spec.py:321] Evaluating on the training split.
I0208 08:08:31.055116 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:11:16.566057 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 08:11:19.249234 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:13:49.119003 140225696298816 spec.py:349] Evaluating on the test split.
I0208 08:13:51.810634 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:16:11.011991 140225696298816 submission_runner.py:408] Time since start: 65300.13s, 	Step: 116694, 	{'train/accuracy': 0.7157125473022461, 'train/loss': 1.4814000129699707, 'train/bleu': 36.77640951803491, 'validation/accuracy': 0.6911507248878479, 'validation/loss': 1.601112961769104, 'validation/bleu': 30.630743293458615, 'validation/num_examples': 3000, 'test/accuracy': 0.7089884281158447, 'test/loss': 1.5000479221343994, 'test/bleu': 30.72359865724317, 'test/num_examples': 3003, 'score': 40349.132508039474, 'total_duration': 65300.12974977493, 'accumulated_submission_time': 40349.132508039474, 'accumulated_eval_time': 24945.666995048523, 'accumulated_logging_time': 1.6028475761413574}
I0208 08:16:11.044908 140051005212416 logging_writer.py:48] [116694] accumulated_eval_time=24945.666995, accumulated_logging_time=1.602848, accumulated_submission_time=40349.132508, global_step=116694, preemption_count=0, score=40349.132508, test/accuracy=0.708988, test/bleu=30.723599, test/loss=1.500048, test/num_examples=3003, total_duration=65300.129750, train/accuracy=0.715713, train/bleu=36.776410, train/loss=1.481400, validation/accuracy=0.691151, validation/bleu=30.630743, validation/loss=1.601113, validation/num_examples=3000
I0208 08:16:13.474609 140050996819712 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.3603840470314026, loss=3.581273078918457
I0208 08:16:47.914490 140051005212416 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3698785901069641, loss=3.6188645362854004
I0208 08:17:22.445699 140050996819712 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3844022750854492, loss=3.6159348487854004
I0208 08:17:57.007326 140051005212416 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3922618627548218, loss=3.645515203475952
I0208 08:18:31.579999 140050996819712 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3765745759010315, loss=3.6334168910980225
I0208 08:19:06.151297 140051005212416 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.38751113414764404, loss=3.6189024448394775
I0208 08:19:40.729361 140050996819712 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.36973991990089417, loss=3.6476047039031982
I0208 08:20:15.309075 140051005212416 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.38457977771759033, loss=3.6516506671905518
I0208 08:20:49.890571 140050996819712 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.38256072998046875, loss=3.664499521255493
I0208 08:21:24.468825 140051005212416 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.37333863973617554, loss=3.5847747325897217
I0208 08:21:59.060035 140050996819712 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3749469518661499, loss=3.5920398235321045
I0208 08:22:33.665213 140051005212416 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.4029933214187622, loss=3.6122941970825195
I0208 08:23:08.250727 140050996819712 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.36363086104393005, loss=3.61611270904541
I0208 08:23:42.832388 140051005212416 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3757229447364807, loss=3.5594966411590576
I0208 08:24:17.425296 140050996819712 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3702727258205414, loss=3.6274335384368896
I0208 08:24:52.010740 140051005212416 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3726389408111572, loss=3.587310552597046
I0208 08:25:26.609838 140050996819712 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.37254783511161804, loss=3.6455495357513428
I0208 08:26:01.213188 140051005212416 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.37589937448501587, loss=3.619922399520874
I0208 08:26:35.782027 140050996819712 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3747836947441101, loss=3.6229445934295654
I0208 08:27:10.348388 140051005212416 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3730723261833191, loss=3.599001169204712
I0208 08:27:44.919202 140050996819712 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3736615777015686, loss=3.6671340465545654
I0208 08:28:19.510372 140051005212416 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.3829288184642792, loss=3.597196340560913
I0208 08:28:54.077685 140050996819712 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3811209797859192, loss=3.58666729927063
I0208 08:29:28.685739 140051005212416 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.41005754470825195, loss=3.6687192916870117
I0208 08:30:03.252341 140050996819712 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3707275688648224, loss=3.608891010284424
I0208 08:30:11.279133 140225696298816 spec.py:321] Evaluating on the training split.
I0208 08:30:14.258212 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:33:09.428453 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 08:33:12.110319 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:35:35.759388 140225696298816 spec.py:349] Evaluating on the test split.
I0208 08:35:38.449888 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:37:57.543148 140225696298816 submission_runner.py:408] Time since start: 66606.66s, 	Step: 119125, 	{'train/accuracy': 0.7265663146972656, 'train/loss': 1.425704002380371, 'train/bleu': 37.280058759108954, 'validation/accuracy': 0.6910267472267151, 'validation/loss': 1.5974948406219482, 'validation/bleu': 30.580480746826456, 'validation/num_examples': 3000, 'test/accuracy': 0.7088141441345215, 'test/loss': 1.4978607892990112, 'test/bleu': 30.61001220758412, 'test/num_examples': 3003, 'score': 41189.27968287468, 'total_duration': 66606.66089344025, 'accumulated_submission_time': 41189.27968287468, 'accumulated_eval_time': 25411.930948019028, 'accumulated_logging_time': 1.6457676887512207}
I0208 08:37:57.576140 140051005212416 logging_writer.py:48] [119125] accumulated_eval_time=25411.930948, accumulated_logging_time=1.645768, accumulated_submission_time=41189.279683, global_step=119125, preemption_count=0, score=41189.279683, test/accuracy=0.708814, test/bleu=30.610012, test/loss=1.497861, test/num_examples=3003, total_duration=66606.660893, train/accuracy=0.726566, train/bleu=37.280059, train/loss=1.425704, validation/accuracy=0.691027, validation/bleu=30.580481, validation/loss=1.597495, validation/num_examples=3000
I0208 08:38:23.787153 140050996819712 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3904009461402893, loss=3.6925652027130127
I0208 08:38:58.299729 140051005212416 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.38218697905540466, loss=3.607179880142212
I0208 08:39:32.878298 140050996819712 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.39214062690734863, loss=3.6330952644348145
I0208 08:40:07.464568 140051005212416 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.38956597447395325, loss=3.6337099075317383
I0208 08:40:42.030788 140050996819712 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3871859312057495, loss=3.6303257942199707
I0208 08:41:16.604609 140051005212416 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3891781270503998, loss=3.6476049423217773
I0208 08:41:51.175642 140050996819712 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.380417138338089, loss=3.5763726234436035
I0208 08:42:25.778106 140051005212416 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3715110421180725, loss=3.632105827331543
I0208 08:43:00.378816 140050996819712 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3783947825431824, loss=3.6156177520751953
I0208 08:43:34.983762 140051005212416 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.39038026332855225, loss=3.6397459506988525
I0208 08:44:09.565607 140050996819712 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.396958589553833, loss=3.604003667831421
I0208 08:44:44.144969 140051005212416 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3957469165325165, loss=3.6588971614837646
I0208 08:45:18.740915 140050996819712 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.39663970470428467, loss=3.6680448055267334
I0208 08:45:53.327259 140051005212416 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.37700459361076355, loss=3.634138584136963
I0208 08:46:27.922596 140050996819712 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.39158865809440613, loss=3.674030065536499
I0208 08:47:02.486147 140051005212416 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3851352334022522, loss=3.609410285949707
I0208 08:47:37.143426 140050996819712 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.39716407656669617, loss=3.608875036239624
I0208 08:48:11.745259 140051005212416 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3905812203884125, loss=3.63066029548645
I0208 08:48:46.319985 140050996819712 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.37797802686691284, loss=3.5823585987091064
I0208 08:49:20.896713 140051005212416 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3777678608894348, loss=3.594918966293335
I0208 08:49:55.487643 140050996819712 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.37424027919769287, loss=3.6276071071624756
I0208 08:50:30.078477 140051005212416 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.3785780668258667, loss=3.5580451488494873
I0208 08:51:04.670525 140050996819712 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.39586561918258667, loss=3.5981876850128174
I0208 08:51:39.281076 140051005212416 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3799434006214142, loss=3.6085023880004883
I0208 08:51:57.709478 140225696298816 spec.py:321] Evaluating on the training split.
I0208 08:52:00.677876 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:54:47.527488 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 08:54:50.217840 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:57:15.672273 140225696298816 spec.py:349] Evaluating on the test split.
I0208 08:57:18.366420 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 08:59:42.892432 140225696298816 submission_runner.py:408] Time since start: 67912.01s, 	Step: 121555, 	{'train/accuracy': 0.722062885761261, 'train/loss': 1.4466272592544556, 'train/bleu': 37.89198746749157, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.5995148420333862, 'validation/bleu': 30.64146443986613, 'validation/num_examples': 3000, 'test/accuracy': 0.7092208862304688, 'test/loss': 1.4971085786819458, 'test/bleu': 30.700750046279442, 'test/num_examples': 3003, 'score': 42029.3257484436, 'total_duration': 67912.01016974449, 'accumulated_submission_time': 42029.3257484436, 'accumulated_eval_time': 25877.113832235336, 'accumulated_logging_time': 1.688605785369873}
I0208 08:59:42.924891 140050996819712 logging_writer.py:48] [121555] accumulated_eval_time=25877.113832, accumulated_logging_time=1.688606, accumulated_submission_time=42029.325748, global_step=121555, preemption_count=0, score=42029.325748, test/accuracy=0.709221, test/bleu=30.700750, test/loss=1.497109, test/num_examples=3003, total_duration=67912.010170, train/accuracy=0.722063, train/bleu=37.891987, train/loss=1.446627, validation/accuracy=0.690717, validation/bleu=30.641464, validation/loss=1.599515, validation/num_examples=3000
I0208 08:59:58.781861 140051005212416 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.37169745564460754, loss=3.540627956390381
I0208 09:00:33.251102 140050996819712 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.39109811186790466, loss=3.6302053928375244
I0208 09:01:07.799092 140051005212416 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.39325547218322754, loss=3.6638405323028564
I0208 09:01:42.381487 140050996819712 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3817688524723053, loss=3.6360116004943848
I0208 09:02:16.950218 140051005212416 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3918503522872925, loss=3.588859796524048
I0208 09:02:51.555426 140050996819712 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.39035943150520325, loss=3.5559494495391846
I0208 09:03:26.126333 140051005212416 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.39716637134552, loss=3.636662483215332
I0208 09:04:00.686634 140050996819712 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3931601643562317, loss=3.5920913219451904
I0208 09:04:35.259691 140051005212416 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.3758542835712433, loss=3.578413248062134
I0208 09:05:09.849112 140050996819712 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3786151707172394, loss=3.5844204425811768
I0208 09:05:44.443105 140051005212416 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.392998605966568, loss=3.608351230621338
I0208 09:06:19.041019 140050996819712 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.4061760902404785, loss=3.634014368057251
I0208 09:06:53.622626 140051005212416 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3884832262992859, loss=3.641550064086914
I0208 09:07:28.201134 140050996819712 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.4012139141559601, loss=3.5994420051574707
I0208 09:08:02.776539 140051005212416 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3870214819908142, loss=3.6532530784606934
I0208 09:08:37.368175 140050996819712 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.39993953704833984, loss=3.6199381351470947
I0208 09:09:11.925388 140051005212416 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3697623312473297, loss=3.5663247108459473
I0208 09:09:46.496232 140050996819712 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.3733498752117157, loss=3.6173105239868164
I0208 09:10:21.061269 140051005212416 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3903118968009949, loss=3.6081225872039795
I0208 09:10:55.641028 140050996819712 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.4061553180217743, loss=3.5802690982818604
I0208 09:11:30.217344 140051005212416 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.39331477880477905, loss=3.646512746810913
I0208 09:12:04.766350 140050996819712 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.4121761918067932, loss=3.6036787033081055
I0208 09:12:39.341519 140051005212416 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3657378554344177, loss=3.5883593559265137
I0208 09:13:13.934860 140050996819712 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3917992115020752, loss=3.602748155593872
I0208 09:13:43.020631 140225696298816 spec.py:321] Evaluating on the training split.
I0208 09:13:46.006973 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:16:26.669732 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 09:16:29.354583 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:18:58.033580 140225696298816 spec.py:349] Evaluating on the test split.
I0208 09:19:00.723614 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:21:17.286706 140225696298816 submission_runner.py:408] Time since start: 69206.40s, 	Step: 123986, 	{'train/accuracy': 0.7214351892471313, 'train/loss': 1.4518346786499023, 'train/bleu': 37.47774055464697, 'validation/accuracy': 0.6916838884353638, 'validation/loss': 1.5992367267608643, 'validation/bleu': 30.64448680287795, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.4954748153686523, 'test/bleu': 30.69339181447463, 'test/num_examples': 3003, 'score': 42869.3330552578, 'total_duration': 69206.40446782112, 'accumulated_submission_time': 42869.3330552578, 'accumulated_eval_time': 26331.379861593246, 'accumulated_logging_time': 1.732133388519287}
I0208 09:21:17.319527 140051005212416 logging_writer.py:48] [123986] accumulated_eval_time=26331.379862, accumulated_logging_time=1.732133, accumulated_submission_time=42869.333055, global_step=123986, preemption_count=0, score=42869.333055, test/accuracy=0.709093, test/bleu=30.693392, test/loss=1.495475, test/num_examples=3003, total_duration=69206.404468, train/accuracy=0.721435, train/bleu=37.477741, train/loss=1.451835, validation/accuracy=0.691684, validation/bleu=30.644487, validation/loss=1.599237, validation/num_examples=3000
I0208 09:21:22.507795 140050996819712 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3770778775215149, loss=3.5607779026031494
I0208 09:21:56.963426 140051005212416 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3904128670692444, loss=3.581732988357544
I0208 09:22:31.495417 140050996819712 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.4037502110004425, loss=3.5976181030273438
I0208 09:23:06.062641 140051005212416 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.3963777720928192, loss=3.568157434463501
I0208 09:23:40.651526 140050996819712 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3951702117919922, loss=3.61598801612854
I0208 09:24:15.253326 140051005212416 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.38213905692100525, loss=3.6352813243865967
I0208 09:24:49.838855 140050996819712 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.40390369296073914, loss=3.6208953857421875
I0208 09:25:24.445326 140051005212416 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.39687931537628174, loss=3.602205991744995
I0208 09:25:59.027644 140050996819712 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.39613616466522217, loss=3.6320853233337402
I0208 09:26:33.598890 140051005212416 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.37731531262397766, loss=3.617992401123047
I0208 09:27:08.169025 140050996819712 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.38528355956077576, loss=3.6175317764282227
I0208 09:27:42.750682 140051005212416 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.4125092327594757, loss=3.600881814956665
I0208 09:28:17.326178 140050996819712 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.37173599004745483, loss=3.5722522735595703
I0208 09:28:51.913755 140051005212416 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3928881585597992, loss=3.5954747200012207
I0208 09:29:26.478529 140050996819712 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.3915199637413025, loss=3.663210153579712
I0208 09:30:01.072972 140051005212416 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.39940059185028076, loss=3.6162972450256348
I0208 09:30:35.659944 140050996819712 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3994452953338623, loss=3.6342391967773438
I0208 09:31:10.226036 140051005212416 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.39592647552490234, loss=3.5944199562072754
I0208 09:31:44.809724 140050996819712 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.370699942111969, loss=3.6225779056549072
I0208 09:32:19.390058 140051005212416 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.39758041501045227, loss=3.6152710914611816
I0208 09:32:53.993151 140050996819712 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.38721397519111633, loss=3.6134750843048096
I0208 09:33:28.572457 140051005212416 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.39442285895347595, loss=3.631687641143799
I0208 09:34:03.142997 140050996819712 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.3819190561771393, loss=3.587848424911499
I0208 09:34:37.756338 140051005212416 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3930038511753082, loss=3.6982617378234863
I0208 09:35:12.334682 140050996819712 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.39583081007003784, loss=3.6355018615722656
I0208 09:35:17.599978 140225696298816 spec.py:321] Evaluating on the training split.
I0208 09:35:20.581904 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:38:04.343836 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 09:38:07.039193 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:40:29.477467 140225696298816 spec.py:349] Evaluating on the test split.
I0208 09:40:32.188622 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:42:51.314867 140225696298816 submission_runner.py:408] Time since start: 70500.43s, 	Step: 126417, 	{'train/accuracy': 0.7267507910728455, 'train/loss': 1.4239526987075806, 'train/bleu': 38.03936471702318, 'validation/accuracy': 0.6922666430473328, 'validation/loss': 1.5971962213516235, 'validation/bleu': 30.60053445338147, 'validation/num_examples': 3000, 'test/accuracy': 0.7093719244003296, 'test/loss': 1.4938480854034424, 'test/bleu': 30.82672499037724, 'test/num_examples': 3003, 'score': 43709.52652788162, 'total_duration': 70500.4326248169, 'accumulated_submission_time': 43709.52652788162, 'accumulated_eval_time': 26785.094694375992, 'accumulated_logging_time': 1.7746250629425049}
I0208 09:42:51.347519 140051005212416 logging_writer.py:48] [126417] accumulated_eval_time=26785.094694, accumulated_logging_time=1.774625, accumulated_submission_time=43709.526528, global_step=126417, preemption_count=0, score=43709.526528, test/accuracy=0.709372, test/bleu=30.826725, test/loss=1.493848, test/num_examples=3003, total_duration=70500.432625, train/accuracy=0.726751, train/bleu=38.039365, train/loss=1.423953, validation/accuracy=0.692267, validation/bleu=30.600534, validation/loss=1.597196, validation/num_examples=3000
I0208 09:43:20.286905 140050996819712 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.37519896030426025, loss=3.6058034896850586
I0208 09:43:54.820286 140051005212416 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.38228169083595276, loss=3.560344696044922
I0208 09:44:29.372588 140050996819712 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.3749367296695709, loss=3.5533080101013184
I0208 09:45:03.943336 140051005212416 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.40782204270362854, loss=3.609088659286499
I0208 09:45:38.507558 140050996819712 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.3755929172039032, loss=3.5758771896362305
I0208 09:46:13.069493 140051005212416 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.3868078291416168, loss=3.5642216205596924
I0208 09:46:47.666679 140050996819712 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3867969810962677, loss=3.5896806716918945
I0208 09:47:22.243081 140051005212416 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.3847752511501312, loss=3.578641176223755
I0208 09:47:56.826475 140050996819712 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.3841104805469513, loss=3.624871015548706
I0208 09:48:31.421749 140051005212416 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.39021459221839905, loss=3.559175491333008
I0208 09:49:06.035015 140050996819712 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3801780343055725, loss=3.6455893516540527
I0208 09:49:40.614803 140051005212416 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.39040273427963257, loss=3.5956814289093018
I0208 09:50:15.231469 140050996819712 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.39343398809432983, loss=3.615142345428467
I0208 09:50:49.806907 140051005212416 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.3799566328525543, loss=3.5896120071411133
I0208 09:51:24.380776 140050996819712 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.38586124777793884, loss=3.585954189300537
I0208 09:51:58.949546 140051005212416 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.39744091033935547, loss=3.603264093399048
I0208 09:52:33.544399 140050996819712 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.39321190118789673, loss=3.6348342895507812
I0208 09:53:08.127467 140051005212416 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.37911343574523926, loss=3.6212782859802246
I0208 09:53:42.693553 140050996819712 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.4093158543109894, loss=3.6380372047424316
I0208 09:54:17.281212 140051005212416 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.3901883363723755, loss=3.5922482013702393
I0208 09:54:51.874261 140050996819712 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.39100995659828186, loss=3.604247808456421
I0208 09:55:26.464611 140051005212416 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.393314003944397, loss=3.6271450519561768
I0208 09:56:01.042011 140050996819712 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.40179428458213806, loss=3.6180975437164307
I0208 09:56:35.611440 140051005212416 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3868969678878784, loss=3.625729560852051
I0208 09:56:51.608573 140225696298816 spec.py:321] Evaluating on the training split.
I0208 09:56:54.588809 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 09:59:35.763372 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 09:59:38.439198 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:02:07.490686 140225696298816 spec.py:349] Evaluating on the test split.
I0208 10:02:10.181962 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:04:28.537455 140225696298816 submission_runner.py:408] Time since start: 71797.66s, 	Step: 128848, 	{'train/accuracy': 0.7229095101356506, 'train/loss': 1.4474419355392456, 'train/bleu': 37.66247762215224, 'validation/accuracy': 0.691745936870575, 'validation/loss': 1.5984151363372803, 'validation/bleu': 30.603091854338725, 'validation/num_examples': 3000, 'test/accuracy': 0.709511399269104, 'test/loss': 1.4947388172149658, 'test/bleu': 30.74373078206694, 'test/num_examples': 3003, 'score': 44549.701666116714, 'total_duration': 71797.655200243, 'accumulated_submission_time': 44549.701666116714, 'accumulated_eval_time': 27242.02351140976, 'accumulated_logging_time': 1.816523790359497}
I0208 10:04:28.569977 140050996819712 logging_writer.py:48] [128848] accumulated_eval_time=27242.023511, accumulated_logging_time=1.816524, accumulated_submission_time=44549.701666, global_step=128848, preemption_count=0, score=44549.701666, test/accuracy=0.709511, test/bleu=30.743731, test/loss=1.494739, test/num_examples=3003, total_duration=71797.655200, train/accuracy=0.722910, train/bleu=37.662478, train/loss=1.447442, validation/accuracy=0.691746, validation/bleu=30.603092, validation/loss=1.598415, validation/num_examples=3000
I0208 10:04:46.816090 140051005212416 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.3782459497451782, loss=3.609405040740967
I0208 10:05:21.313684 140050996819712 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.4113337993621826, loss=3.6131978034973145
I0208 10:05:55.902343 140051005212416 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.40279141068458557, loss=3.573183298110962
I0208 10:06:30.484160 140050996819712 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.3913297951221466, loss=3.6125454902648926
I0208 10:07:05.058187 140051005212416 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3788995146751404, loss=3.6041324138641357
I0208 10:07:39.619737 140050996819712 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.37337586283683777, loss=3.5822389125823975
I0208 10:08:14.195627 140051005212416 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.37952181696891785, loss=3.574683666229248
I0208 10:08:48.773726 140050996819712 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3712339997291565, loss=3.583101272583008
I0208 10:09:23.339279 140051005212416 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.3992891311645508, loss=3.61588454246521
I0208 10:09:57.936846 140050996819712 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.3831270635128021, loss=3.5749824047088623
I0208 10:10:32.537648 140051005212416 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.3748070299625397, loss=3.6390511989593506
I0208 10:11:07.143205 140050996819712 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.3771475851535797, loss=3.5609192848205566
I0208 10:11:41.731583 140051005212416 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.4040164053440094, loss=3.6123645305633545
I0208 10:12:16.308626 140050996819712 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.3987981975078583, loss=3.6253435611724854
I0208 10:12:50.927141 140051005212416 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.3988547921180725, loss=3.654958724975586
I0208 10:13:25.500084 140050996819712 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.38858261704444885, loss=3.6167447566986084
I0208 10:14:00.081182 140051005212416 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.3981712758541107, loss=3.6057112216949463
I0208 10:14:34.648150 140050996819712 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.37625616788864136, loss=3.6083967685699463
I0208 10:15:09.239847 140051005212416 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.38741785287857056, loss=3.591545343399048
I0208 10:15:43.807738 140050996819712 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.3890796899795532, loss=3.637080669403076
I0208 10:16:18.381052 140051005212416 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.37617647647857666, loss=3.579240083694458
I0208 10:16:52.964932 140050996819712 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.3788180947303772, loss=3.5728759765625
I0208 10:17:27.528308 140051005212416 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.3923586308956146, loss=3.632000684738159
I0208 10:18:02.110781 140050996819712 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.40319740772247314, loss=3.631072521209717
I0208 10:18:28.811475 140225696298816 spec.py:321] Evaluating on the training split.
I0208 10:18:31.789785 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:21:20.715061 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 10:21:23.403996 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:23:50.508965 140225696298816 spec.py:349] Evaluating on the test split.
I0208 10:23:53.196352 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:26:13.002353 140225696298816 submission_runner.py:408] Time since start: 73102.12s, 	Step: 131279, 	{'train/accuracy': 0.7239293456077576, 'train/loss': 1.4316215515136719, 'train/bleu': 37.897957791075164, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.5985901355743408, 'validation/bleu': 30.610609263383143, 'validation/num_examples': 3000, 'test/accuracy': 0.7093486785888672, 'test/loss': 1.4943861961364746, 'test/bleu': 30.861089142384305, 'test/num_examples': 3003, 'score': 45389.855187654495, 'total_duration': 73102.1201004982, 'accumulated_submission_time': 45389.855187654495, 'accumulated_eval_time': 27706.21432876587, 'accumulated_logging_time': 1.8598039150238037}
I0208 10:26:13.036681 140051005212416 logging_writer.py:48] [131279] accumulated_eval_time=27706.214329, accumulated_logging_time=1.859804, accumulated_submission_time=45389.855188, global_step=131279, preemption_count=0, score=45389.855188, test/accuracy=0.709349, test/bleu=30.861089, test/loss=1.494386, test/num_examples=3003, total_duration=73102.120100, train/accuracy=0.723929, train/bleu=37.897958, train/loss=1.431622, validation/accuracy=0.691845, validation/bleu=30.610609, validation/loss=1.598590, validation/num_examples=3000
I0208 10:26:20.625736 140050996819712 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.4004828929901123, loss=3.6583480834960938
I0208 10:26:55.092378 140051005212416 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.38356348872184753, loss=3.5902209281921387
I0208 10:27:29.646944 140050996819712 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.3897221088409424, loss=3.601553440093994
I0208 10:28:04.225486 140051005212416 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.3825020492076874, loss=3.6039438247680664
I0208 10:28:38.783851 140050996819712 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.3806172311306, loss=3.6237902641296387
I0208 10:29:13.364145 140051005212416 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.4032711684703827, loss=3.6183857917785645
I0208 10:29:47.902863 140050996819712 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.37584006786346436, loss=3.5552690029144287
I0208 10:30:22.465509 140051005212416 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.3960683047771454, loss=3.6313140392303467
I0208 10:30:57.037075 140050996819712 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.39835816621780396, loss=3.5538489818573
I0208 10:31:31.618977 140051005212416 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.4138956069946289, loss=3.6193857192993164
I0208 10:32:06.200534 140050996819712 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.3928323984146118, loss=3.6133978366851807
I0208 10:32:40.803832 140051005212416 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.3951180577278137, loss=3.6505095958709717
I0208 10:33:15.378960 140050996819712 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.391167551279068, loss=3.6309757232666016
I0208 10:33:49.958989 140051005212416 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.39242827892303467, loss=3.5960450172424316
I0208 10:34:24.534986 140050996819712 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3897925913333893, loss=3.6314449310302734
I0208 10:34:59.110666 140051005212416 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.38897839188575745, loss=3.6529929637908936
I0208 10:35:33.709024 140050996819712 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.40491169691085815, loss=3.6679751873016357
I0208 10:36:08.307500 140051005212416 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.38742709159851074, loss=3.640981674194336
I0208 10:36:42.916008 140050996819712 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.37114137411117554, loss=3.588393449783325
I0208 10:37:17.518636 140051005212416 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.40453261137008667, loss=3.60386061668396
I0208 10:37:52.095781 140050996819712 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.4077790677547455, loss=3.6618034839630127
I0208 10:38:02.885746 140225696298816 spec.py:321] Evaluating on the training split.
I0208 10:38:05.866682 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:40:40.700131 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 10:40:43.380377 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:43:08.207816 140225696298816 spec.py:349] Evaluating on the test split.
I0208 10:43:10.896574 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:45:29.657281 140225696298816 submission_runner.py:408] Time since start: 74258.78s, 	Step: 133333, 	{'train/accuracy': 0.7237716913223267, 'train/loss': 1.4409384727478027, 'train/bleu': 37.902471942390434, 'validation/accuracy': 0.691882312297821, 'validation/loss': 1.59796142578125, 'validation/bleu': 30.55257145746482, 'validation/num_examples': 3000, 'test/accuracy': 0.7095927000045776, 'test/loss': 1.4941442012786865, 'test/bleu': 30.79392196122609, 'test/num_examples': 3003, 'score': 46099.627485990524, 'total_duration': 74258.77504205704, 'accumulated_submission_time': 46099.627485990524, 'accumulated_eval_time': 28152.985813856125, 'accumulated_logging_time': 1.9054770469665527}
I0208 10:45:29.691167 140051005212416 logging_writer.py:48] [133333] accumulated_eval_time=28152.985814, accumulated_logging_time=1.905477, accumulated_submission_time=46099.627486, global_step=133333, preemption_count=0, score=46099.627486, test/accuracy=0.709593, test/bleu=30.793922, test/loss=1.494144, test/num_examples=3003, total_duration=74258.775042, train/accuracy=0.723772, train/bleu=37.902472, train/loss=1.440938, validation/accuracy=0.691882, validation/bleu=30.552571, validation/loss=1.597961, validation/num_examples=3000
I0208 10:45:29.724240 140050996819712 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46099.627486
I0208 10:45:30.939292 140225696298816 checkpoints.py:490] Saving checkpoint at step: 133333
I0208 10:45:35.014902 140225696298816 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2/checkpoint_133333
I0208 10:45:35.019830 140225696298816 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_2/checkpoint_133333.
I0208 10:45:35.070345 140225696298816 submission_runner.py:583] Tuning trial 2/5
I0208 10:45:35.070511 140225696298816 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0208 10:45:35.075915 140225696298816 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005942517309449613, 'train/loss': 11.024877548217773, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.1668484210968, 'total_duration': 882.6488099098206, 'accumulated_submission_time': 26.1668484210968, 'accumulated_eval_time': 856.4819233417511, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2430, {'train/accuracy': 0.4335348904132843, 'train/loss': 3.881221294403076, 'train/bleu': 15.486008673345255, 'validation/accuracy': 0.41708099842071533, 'validation/loss': 4.0259270668029785, 'validation/bleu': 10.90630801345595, 'validation/num_examples': 3000, 'test/accuracy': 0.4070304036140442, 'test/loss': 4.187845706939697, 'test/bleu': 9.262918884334642, 'test/num_examples': 3003, 'score': 866.3570308685303, 'total_duration': 2266.9725909233093, 'accumulated_submission_time': 866.3570308685303, 'accumulated_eval_time': 1400.5178980827332, 'accumulated_logging_time': 0.019860267639160156, 'global_step': 2430, 'preemption_count': 0}), (4858, {'train/accuracy': 0.5460978150367737, 'train/loss': 2.815052032470703, 'train/bleu': 23.83795839443816, 'validation/accuracy': 0.5495033860206604, 'validation/loss': 2.779167413711548, 'validation/bleu': 20.46226812080449, 'validation/num_examples': 3000, 'test/accuracy': 0.5491255521774292, 'test/loss': 2.8148353099823, 'test/bleu': 18.97626936557872, 'test/num_examples': 3003, 'score': 1706.3713157176971, 'total_duration': 3589.398926973343, 'accumulated_submission_time': 1706.3713157176971, 'accumulated_eval_time': 1882.8273282051086, 'accumulated_logging_time': 0.04439544677734375, 'global_step': 4858, 'preemption_count': 0}), (7289, {'train/accuracy': 0.583653450012207, 'train/loss': 2.4412691593170166, 'train/bleu': 27.028060189117536, 'validation/accuracy': 0.5920695066452026, 'validation/loss': 2.3615870475769043, 'validation/bleu': 23.554423470160216, 'validation/num_examples': 3000, 'test/accuracy': 0.5970367789268494, 'test/loss': 2.35249662399292, 'test/bleu': 22.367772014175003, 'test/num_examples': 3003, 'score': 2546.6103324890137, 'total_duration': 4886.784422636032, 'accumulated_submission_time': 2546.6103324890137, 'accumulated_eval_time': 2339.8702476024628, 'accumulated_logging_time': 0.07002735137939453, 'global_step': 7289, 'preemption_count': 0}), (9720, {'train/accuracy': 0.6012474894523621, 'train/loss': 2.2867743968963623, 'train/bleu': 29.023278515494034, 'validation/accuracy': 0.6191863417625427, 'validation/loss': 2.1540048122406006, 'validation/bleu': 25.267400374292624, 'validation/num_examples': 3000, 'test/accuracy': 0.6241241097450256, 'test/loss': 2.124663829803467, 'test/bleu': 24.386627330019042, 'test/num_examples': 3003, 'score': 3386.586151123047, 'total_duration': 6196.767947673798, 'accumulated_submission_time': 3386.586151123047, 'accumulated_eval_time': 2809.7746393680573, 'accumulated_logging_time': 0.09573030471801758, 'global_step': 9720, 'preemption_count': 0}), (12152, {'train/accuracy': 0.6117793917655945, 'train/loss': 2.2029600143432617, 'train/bleu': 29.49963286982012, 'validation/accuracy': 0.6326642036437988, 'validation/loss': 2.0467686653137207, 'validation/bleu': 26.453320841038845, 'validation/num_examples': 3000, 'test/accuracy': 0.6393585801124573, 'test/loss': 2.005385637283325, 'test/bleu': 25.511380252988697, 'test/num_examples': 3003, 'score': 4226.775480031967, 'total_duration': 7611.135657072067, 'accumulated_submission_time': 4226.775480031967, 'accumulated_eval_time': 3383.849452972412, 'accumulated_logging_time': 0.12136363983154297, 'global_step': 12152, 'preemption_count': 0}), (14583, {'train/accuracy': 0.6235041618347168, 'train/loss': 2.090024471282959, 'train/bleu': 30.151688486651715, 'validation/accuracy': 0.642856240272522, 'validation/loss': 1.947042465209961, 'validation/bleu': 26.987183390760876, 'validation/num_examples': 3000, 'test/accuracy': 0.6523270010948181, 'test/loss': 1.8937307596206665, 'test/bleu': 26.47813059844154, 'test/num_examples': 3003, 'score': 5066.86608338356, 'total_duration': 9038.87869143486, 'accumulated_submission_time': 5066.86608338356, 'accumulated_eval_time': 3971.3980989456177, 'accumulated_logging_time': 0.1472148895263672, 'global_step': 14583, 'preemption_count': 0}), (17014, {'train/accuracy': 0.6282114386558533, 'train/loss': 2.0542614459991455, 'train/bleu': 30.50936542867486, 'validation/accuracy': 0.6478034853935242, 'validation/loss': 1.9196758270263672, 'validation/bleu': 27.620231439534933, 'validation/num_examples': 3000, 'test/accuracy': 0.6583580374717712, 'test/loss': 1.854627013206482, 'test/bleu': 26.803849032849374, 'test/num_examples': 3003, 'score': 5906.852504491806, 'total_duration': 10337.573017835617, 'accumulated_submission_time': 5906.852504491806, 'accumulated_eval_time': 4430.000137805939, 'accumulated_logging_time': 0.17507600784301758, 'global_step': 17014, 'preemption_count': 0}), (19445, {'train/accuracy': 0.6460630893707275, 'train/loss': 1.9484455585479736, 'train/bleu': 31.32886972123233, 'validation/accuracy': 0.652415931224823, 'validation/loss': 1.8769932985305786, 'validation/bleu': 27.88024937862478, 'validation/num_examples': 3000, 'test/accuracy': 0.6621463298797607, 'test/loss': 1.8195322751998901, 'test/bleu': 27.046071671892715, 'test/num_examples': 3003, 'score': 6746.784756422043, 'total_duration': 11658.784047603607, 'accumulated_submission_time': 6746.784756422043, 'accumulated_eval_time': 4911.174456119537, 'accumulated_logging_time': 0.20150542259216309, 'global_step': 19445, 'preemption_count': 0}), (21876, {'train/accuracy': 0.6394797563552856, 'train/loss': 1.9793699979782104, 'train/bleu': 31.291298722145083, 'validation/accuracy': 0.6581319570541382, 'validation/loss': 1.8350783586502075, 'validation/bleu': 28.01187089919093, 'validation/num_examples': 3000, 'test/accuracy': 0.6679449081420898, 'test/loss': 1.7722675800323486, 'test/bleu': 27.51437928438289, 'test/num_examples': 3003, 'score': 7586.71210360527, 'total_duration': 12958.286858320236, 'accumulated_submission_time': 7586.71210360527, 'accumulated_eval_time': 5370.643972635269, 'accumulated_logging_time': 0.23011422157287598, 'global_step': 21876, 'preemption_count': 0}), (24307, {'train/accuracy': 0.6397490501403809, 'train/loss': 1.9547765254974365, 'train/bleu': 30.85258301009002, 'validation/accuracy': 0.6600165963172913, 'validation/loss': 1.805528998374939, 'validation/bleu': 28.20852868398465, 'validation/num_examples': 3000, 'test/accuracy': 0.672105073928833, 'test/loss': 1.7438435554504395, 'test/bleu': 27.47958904209218, 'test/num_examples': 3003, 'score': 8426.640635728836, 'total_duration': 14495.81839632988, 'accumulated_submission_time': 8426.640635728836, 'accumulated_eval_time': 6068.142309427261, 'accumulated_logging_time': 0.2572338581085205, 'global_step': 24307, 'preemption_count': 0}), (26738, {'train/accuracy': 0.6513376832008362, 'train/loss': 1.8955870866775513, 'train/bleu': 31.97804613904664, 'validation/accuracy': 0.6653606295585632, 'validation/loss': 1.7924036979675293, 'validation/bleu': 28.451939799484045, 'validation/num_examples': 3000, 'test/accuracy': 0.6745221018791199, 'test/loss': 1.7301757335662842, 'test/bleu': 28.200531420440008, 'test/num_examples': 3003, 'score': 9266.780019283295, 'total_duration': 16030.644038438797, 'accumulated_submission_time': 9266.780019283295, 'accumulated_eval_time': 6762.721871137619, 'accumulated_logging_time': 0.28625965118408203, 'global_step': 26738, 'preemption_count': 0}), (29170, {'train/accuracy': 0.6469125747680664, 'train/loss': 1.9222787618637085, 'train/bleu': 32.02554933303594, 'validation/accuracy': 0.6672824621200562, 'validation/loss': 1.7752902507781982, 'validation/bleu': 28.697036519419203, 'validation/num_examples': 3000, 'test/accuracy': 0.6792516708374023, 'test/loss': 1.7129313945770264, 'test/bleu': 28.453572109704602, 'test/num_examples': 3003, 'score': 10107.006126642227, 'total_duration': 17344.64367866516, 'accumulated_submission_time': 10107.006126642227, 'accumulated_eval_time': 7236.388736963272, 'accumulated_logging_time': 0.3159620761871338, 'global_step': 29170, 'preemption_count': 0}), (31601, {'train/accuracy': 0.6717249155044556, 'train/loss': 1.7598832845687866, 'train/bleu': 33.27960037029534, 'validation/accuracy': 0.6671584844589233, 'validation/loss': 1.7688673734664917, 'validation/bleu': 29.136006467228604, 'validation/num_examples': 3000, 'test/accuracy': 0.679449200630188, 'test/loss': 1.6971747875213623, 'test/bleu': 28.461407126008957, 'test/num_examples': 3003, 'score': 10947.211053609848, 'total_duration': 18654.48917913437, 'accumulated_submission_time': 10947.211053609848, 'accumulated_eval_time': 7705.9221296310425, 'accumulated_logging_time': 0.3449394702911377, 'global_step': 31601, 'preemption_count': 0}), (34032, {'train/accuracy': 0.6559919714927673, 'train/loss': 1.8558120727539062, 'train/bleu': 32.682312093461405, 'validation/accuracy': 0.6694275140762329, 'validation/loss': 1.746963381767273, 'validation/bleu': 29.291129779033984, 'validation/num_examples': 3000, 'test/accuracy': 0.6810644268989563, 'test/loss': 1.6823828220367432, 'test/bleu': 28.599915373846635, 'test/num_examples': 3003, 'score': 11787.294679403305, 'total_duration': 20008.522481679916, 'accumulated_submission_time': 11787.294679403305, 'accumulated_eval_time': 8219.765256166458, 'accumulated_logging_time': 0.3742711544036865, 'global_step': 34032, 'preemption_count': 0}), (36463, {'train/accuracy': 0.6533215045928955, 'train/loss': 1.8728480339050293, 'train/bleu': 32.04000187532934, 'validation/accuracy': 0.6712626218795776, 'validation/loss': 1.741132140159607, 'validation/bleu': 28.41640521392595, 'validation/num_examples': 3000, 'test/accuracy': 0.682819128036499, 'test/loss': 1.669180154800415, 'test/bleu': 29.095199098370255, 'test/num_examples': 3003, 'score': 12627.530505895615, 'total_duration': 21463.58862900734, 'accumulated_submission_time': 12627.530505895615, 'accumulated_eval_time': 8834.486620664597, 'accumulated_logging_time': 0.40384364128112793, 'global_step': 36463, 'preemption_count': 0}), (38894, {'train/accuracy': 0.6603764891624451, 'train/loss': 1.8058222532272339, 'train/bleu': 32.47630884958098, 'validation/accuracy': 0.6701342463493347, 'validation/loss': 1.730035662651062, 'validation/bleu': 29.079701147488368, 'validation/num_examples': 3000, 'test/accuracy': 0.6835280060768127, 'test/loss': 1.6568653583526611, 'test/bleu': 28.82156068715825, 'test/num_examples': 3003, 'score': 13467.513338565826, 'total_duration': 22775.415242671967, 'accumulated_submission_time': 13467.513338565826, 'accumulated_eval_time': 9306.22245168686, 'accumulated_logging_time': 0.4338409900665283, 'global_step': 38894, 'preemption_count': 0}), (41325, {'train/accuracy': 0.6599708795547485, 'train/loss': 1.8201743364334106, 'train/bleu': 32.381106417776536, 'validation/accuracy': 0.6728744506835938, 'validation/loss': 1.712181806564331, 'validation/bleu': 29.184639902057, 'validation/num_examples': 3000, 'test/accuracy': 0.6861310005187988, 'test/loss': 1.6359333992004395, 'test/bleu': 29.236897732583365, 'test/num_examples': 3003, 'score': 14307.620192050934, 'total_duration': 24121.71455836296, 'accumulated_submission_time': 14307.620192050934, 'accumulated_eval_time': 9812.306602954865, 'accumulated_logging_time': 0.464618444442749, 'global_step': 41325, 'preemption_count': 0}), (43756, {'train/accuracy': 0.6520457863807678, 'train/loss': 1.86770761013031, 'train/bleu': 32.20933811417118, 'validation/accuracy': 0.6751186847686768, 'validation/loss': 1.7147032022476196, 'validation/bleu': 29.250316554246652, 'validation/num_examples': 3000, 'test/accuracy': 0.6876997351646423, 'test/loss': 1.6349014043807983, 'test/bleu': 29.037590233653013, 'test/num_examples': 3003, 'score': 15147.648758888245, 'total_duration': 25533.225674152374, 'accumulated_submission_time': 15147.648758888245, 'accumulated_eval_time': 10383.679992437363, 'accumulated_logging_time': 0.4959902763366699, 'global_step': 43756, 'preemption_count': 0}), (46189, {'train/accuracy': 0.6639376878738403, 'train/loss': 1.7865409851074219, 'train/bleu': 32.48354802392069, 'validation/accuracy': 0.6739283800125122, 'validation/loss': 1.713200569152832, 'validation/bleu': 29.521032934089877, 'validation/num_examples': 3000, 'test/accuracy': 0.6856428980827332, 'test/loss': 1.6498336791992188, 'test/bleu': 29.08447388429547, 'test/num_examples': 3003, 'score': 15987.783904314041, 'total_duration': 26852.710567712784, 'accumulated_submission_time': 15987.783904314041, 'accumulated_eval_time': 10862.922046422958, 'accumulated_logging_time': 0.5261423587799072, 'global_step': 46189, 'preemption_count': 0}), (48620, {'train/accuracy': 0.6577057242393494, 'train/loss': 1.8336910009384155, 'train/bleu': 32.11022349497067, 'validation/accuracy': 0.6760362386703491, 'validation/loss': 1.7174301147460938, 'validation/bleu': 29.529682871449634, 'validation/num_examples': 3000, 'test/accuracy': 0.6883621215820312, 'test/loss': 1.642948031425476, 'test/bleu': 29.228574269408423, 'test/num_examples': 3003, 'score': 16827.680357694626, 'total_duration': 28166.43555521965, 'accumulated_submission_time': 16827.680357694626, 'accumulated_eval_time': 11336.640148878098, 'accumulated_logging_time': 0.5586118698120117, 'global_step': 48620, 'preemption_count': 0}), (51051, {'train/accuracy': 0.6678268909454346, 'train/loss': 1.75491201877594, 'train/bleu': 33.25293797178625, 'validation/accuracy': 0.6759990453720093, 'validation/loss': 1.6939091682434082, 'validation/bleu': 29.552817707166724, 'validation/num_examples': 3000, 'test/accuracy': 0.6865725517272949, 'test/loss': 1.620548129081726, 'test/bleu': 28.930613904916136, 'test/num_examples': 3003, 'score': 17667.6850566864, 'total_duration': 29472.580087661743, 'accumulated_submission_time': 17667.6850566864, 'accumulated_eval_time': 11802.670159101486, 'accumulated_logging_time': 0.5905261039733887, 'global_step': 51051, 'preemption_count': 0}), (53483, {'train/accuracy': 0.6628263592720032, 'train/loss': 1.7955244779586792, 'train/bleu': 32.78826064342799, 'validation/accuracy': 0.6780325174331665, 'validation/loss': 1.6881910562515259, 'validation/bleu': 29.682209609916477, 'validation/num_examples': 3000, 'test/accuracy': 0.693265974521637, 'test/loss': 1.6090433597564697, 'test/bleu': 29.551221567046127, 'test/num_examples': 3003, 'score': 18507.86905694008, 'total_duration': 30843.158007144928, 'accumulated_submission_time': 18507.86905694008, 'accumulated_eval_time': 12332.954099178314, 'accumulated_logging_time': 0.6230416297912598, 'global_step': 53483, 'preemption_count': 0}), (55914, {'train/accuracy': 0.6604369282722473, 'train/loss': 1.8041754961013794, 'train/bleu': 33.13701132138776, 'validation/accuracy': 0.6777101159095764, 'validation/loss': 1.676916241645813, 'validation/bleu': 29.615411459159855, 'validation/num_examples': 3000, 'test/accuracy': 0.6930218935012817, 'test/loss': 1.5961912870407104, 'test/bleu': 29.350988973732594, 'test/num_examples': 3003, 'score': 19347.87039089203, 'total_duration': 32146.713432073593, 'accumulated_submission_time': 19347.87039089203, 'accumulated_eval_time': 12796.398730516434, 'accumulated_logging_time': 0.6548542976379395, 'global_step': 55914, 'preemption_count': 0}), (58348, {'train/accuracy': 0.37330424785614014, 'train/loss': 3.6769845485687256, 'train/bleu': 1.7790266662847813, 'validation/accuracy': 0.3269147276878357, 'validation/loss': 4.14604377746582, 'validation/bleu': 0.3167417668719349, 'validation/num_examples': 3000, 'test/accuracy': 0.3172041177749634, 'test/loss': 4.3111090660095215, 'test/bleu': 0.2760901163922393, 'test/num_examples': 3003, 'score': 20187.9033575058, 'total_duration': 33801.09925675392, 'accumulated_submission_time': 20187.9033575058, 'accumulated_eval_time': 13610.639861106873, 'accumulated_logging_time': 0.6897470951080322, 'global_step': 58348, 'preemption_count': 0}), (60779, {'train/accuracy': 0.6671022176742554, 'train/loss': 1.770507574081421, 'train/bleu': 32.896878208625374, 'validation/accuracy': 0.6774373650550842, 'validation/loss': 1.6847100257873535, 'validation/bleu': 29.561452728455883, 'validation/num_examples': 3000, 'test/accuracy': 0.6925687193870544, 'test/loss': 1.6012877225875854, 'test/bleu': 29.86454461957114, 'test/num_examples': 3003, 'score': 21027.7988653183, 'total_duration': 35157.31735706329, 'accumulated_submission_time': 21027.7988653183, 'accumulated_eval_time': 14126.851303100586, 'accumulated_logging_time': 0.7242088317871094, 'global_step': 60779, 'preemption_count': 0}), (63211, {'train/accuracy': 0.6855831146240234, 'train/loss': 1.6496340036392212, 'train/bleu': 34.1527935497728, 'validation/accuracy': 0.679582417011261, 'validation/loss': 1.6690008640289307, 'validation/bleu': 29.659998515776348, 'validation/num_examples': 3000, 'test/accuracy': 0.6939283013343811, 'test/loss': 1.5856915712356567, 'test/bleu': 29.44425505315059, 'test/num_examples': 3003, 'score': 21867.882091760635, 'total_duration': 36463.16767692566, 'accumulated_submission_time': 21867.882091760635, 'accumulated_eval_time': 14592.508635282516, 'accumulated_logging_time': 0.7560958862304688, 'global_step': 63211, 'preemption_count': 0}), (65642, {'train/accuracy': 0.6722832322120667, 'train/loss': 1.7180349826812744, 'train/bleu': 33.70603561239361, 'validation/accuracy': 0.681913435459137, 'validation/loss': 1.6529948711395264, 'validation/bleu': 30.087363587995426, 'validation/num_examples': 3000, 'test/accuracy': 0.6969264149665833, 'test/loss': 1.5698133707046509, 'test/bleu': 29.882720372420852, 'test/num_examples': 3003, 'score': 22707.863388299942, 'total_duration': 37762.350452661514, 'accumulated_submission_time': 22707.863388299942, 'accumulated_eval_time': 15051.599786758423, 'accumulated_logging_time': 0.7890956401824951, 'global_step': 65642, 'preemption_count': 0}), (68073, {'train/accuracy': 0.6699897646903992, 'train/loss': 1.7524583339691162, 'train/bleu': 33.207613914396966, 'validation/accuracy': 0.6819630265235901, 'validation/loss': 1.6551176309585571, 'validation/bleu': 30.112537396175984, 'validation/num_examples': 3000, 'test/accuracy': 0.6972169280052185, 'test/loss': 1.5671659708023071, 'test/bleu': 29.769703618118545, 'test/num_examples': 3003, 'score': 23547.92662167549, 'total_duration': 39053.7624464035, 'accumulated_submission_time': 23547.92662167549, 'accumulated_eval_time': 15502.837298870087, 'accumulated_logging_time': 0.822166919708252, 'global_step': 68073, 'preemption_count': 0}), (70504, {'train/accuracy': 0.6783646941184998, 'train/loss': 1.6977791786193848, 'train/bleu': 34.29033621037187, 'validation/accuracy': 0.6820374131202698, 'validation/loss': 1.6467190980911255, 'validation/bleu': 30.249611344394367, 'validation/num_examples': 3000, 'test/accuracy': 0.6974377036094666, 'test/loss': 1.5637001991271973, 'test/bleu': 30.00090817367382, 'test/num_examples': 3003, 'score': 24387.89455485344, 'total_duration': 40374.32408332825, 'accumulated_submission_time': 24387.89455485344, 'accumulated_eval_time': 15983.3171210289, 'accumulated_logging_time': 0.857762336730957, 'global_step': 70504, 'preemption_count': 0}), (72935, {'train/accuracy': 0.6749582886695862, 'train/loss': 1.7154207229614258, 'train/bleu': 33.6660576168447, 'validation/accuracy': 0.6837732791900635, 'validation/loss': 1.6429944038391113, 'validation/bleu': 29.911717014627982, 'validation/num_examples': 3000, 'test/accuracy': 0.6994480490684509, 'test/loss': 1.5574828386306763, 'test/bleu': 29.76888711725281, 'test/num_examples': 3003, 'score': 25227.994698286057, 'total_duration': 41671.04165291786, 'accumulated_submission_time': 25227.994698286057, 'accumulated_eval_time': 16439.81956934929, 'accumulated_logging_time': 0.8949284553527832, 'global_step': 72935, 'preemption_count': 0}), (75366, {'train/accuracy': 0.701172411441803, 'train/loss': 1.5690522193908691, 'train/bleu': 35.91140519112401, 'validation/accuracy': 0.6847156286239624, 'validation/loss': 1.6373581886291504, 'validation/bleu': 30.375385832128877, 'validation/num_examples': 3000, 'test/accuracy': 0.6980187296867371, 'test/loss': 1.5505318641662598, 'test/bleu': 29.991422765336544, 'test/num_examples': 3003, 'score': 26068.055867910385, 'total_duration': 43029.4435377121, 'accumulated_submission_time': 26068.055867910385, 'accumulated_eval_time': 16958.04394555092, 'accumulated_logging_time': 0.9335498809814453, 'global_step': 75366, 'preemption_count': 0}), (77797, {'train/accuracy': 0.6788302659988403, 'train/loss': 1.6935021877288818, 'train/bleu': 34.17489567231058, 'validation/accuracy': 0.6846908330917358, 'validation/loss': 1.637537956237793, 'validation/bleu': 30.014238840241926, 'validation/num_examples': 3000, 'test/accuracy': 0.701714038848877, 'test/loss': 1.5495461225509644, 'test/bleu': 30.396515383250396, 'test/num_examples': 3003, 'score': 26908.02235507965, 'total_duration': 44322.348552942276, 'accumulated_submission_time': 26908.02235507965, 'accumulated_eval_time': 17410.868065595627, 'accumulated_logging_time': 0.9702820777893066, 'global_step': 77797, 'preemption_count': 0}), (80228, {'train/accuracy': 0.6816733479499817, 'train/loss': 1.680078387260437, 'train/bleu': 34.12771578283215, 'validation/accuracy': 0.6865506768226624, 'validation/loss': 1.6328672170639038, 'validation/bleu': 30.30910404470047, 'validation/num_examples': 3000, 'test/accuracy': 0.7003777027130127, 'test/loss': 1.5423470735549927, 'test/bleu': 30.21403086923635, 'test/num_examples': 3003, 'score': 27748.175061941147, 'total_duration': 45607.78610897064, 'accumulated_submission_time': 27748.175061941147, 'accumulated_eval_time': 17856.037237882614, 'accumulated_logging_time': 1.008216142654419, 'global_step': 80228, 'preemption_count': 0}), (82659, {'train/accuracy': 0.6934168934822083, 'train/loss': 1.601096272468567, 'train/bleu': 34.86383322748257, 'validation/accuracy': 0.6877533793449402, 'validation/loss': 1.6192811727523804, 'validation/bleu': 30.61256790425198, 'validation/num_examples': 3000, 'test/accuracy': 0.7024344801902771, 'test/loss': 1.531815767288208, 'test/bleu': 30.29900160524727, 'test/num_examples': 3003, 'score': 28588.085990428925, 'total_duration': 46958.485570430756, 'accumulated_submission_time': 28588.085990428925, 'accumulated_eval_time': 18366.711642980576, 'accumulated_logging_time': 1.0443508625030518, 'global_step': 82659, 'preemption_count': 0}), (85091, {'train/accuracy': 0.6842333674430847, 'train/loss': 1.6667418479919434, 'train/bleu': 34.231418554266256, 'validation/accuracy': 0.6870714426040649, 'validation/loss': 1.6234965324401855, 'validation/bleu': 30.22605562863886, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.5303460359573364, 'test/bleu': 30.34697045957415, 'test/num_examples': 3003, 'score': 29428.2906563282, 'total_duration': 48280.69108605385, 'accumulated_submission_time': 29428.2906563282, 'accumulated_eval_time': 18848.596970558167, 'accumulated_logging_time': 1.0820410251617432, 'global_step': 85091, 'preemption_count': 0}), (87522, {'train/accuracy': 0.6859403252601624, 'train/loss': 1.6498194932937622, 'train/bleu': 34.449619726829866, 'validation/accuracy': 0.6880509853363037, 'validation/loss': 1.620361566543579, 'validation/bleu': 30.61483554539125, 'validation/num_examples': 3000, 'test/accuracy': 0.7043286561965942, 'test/loss': 1.5281420946121216, 'test/bleu': 30.191210875242326, 'test/num_examples': 3003, 'score': 30268.273729801178, 'total_duration': 49588.71892952919, 'accumulated_submission_time': 30268.273729801178, 'accumulated_eval_time': 19316.526229143143, 'accumulated_logging_time': 1.1193876266479492, 'global_step': 87522, 'preemption_count': 0}), (89953, {'train/accuracy': 0.6895501613616943, 'train/loss': 1.6185948848724365, 'train/bleu': 35.17824123678239, 'validation/accuracy': 0.6874062418937683, 'validation/loss': 1.6165112257003784, 'validation/bleu': 30.320178766417413, 'validation/num_examples': 3000, 'test/accuracy': 0.7040846347808838, 'test/loss': 1.5222641229629517, 'test/bleu': 30.366536884259617, 'test/num_examples': 3003, 'score': 31108.289889335632, 'total_duration': 50895.24898195267, 'accumulated_submission_time': 31108.289889335632, 'accumulated_eval_time': 19782.92421078682, 'accumulated_logging_time': 1.1572742462158203, 'global_step': 89953, 'preemption_count': 0}), (92384, {'train/accuracy': 0.690657913684845, 'train/loss': 1.6202352046966553, 'train/bleu': 35.047649936271675, 'validation/accuracy': 0.6881749629974365, 'validation/loss': 1.61602783203125, 'validation/bleu': 30.26145651209316, 'validation/num_examples': 3000, 'test/accuracy': 0.7052234411239624, 'test/loss': 1.5193902254104614, 'test/bleu': 30.455149942129623, 'test/num_examples': 3003, 'score': 31948.42696595192, 'total_duration': 52216.04299545288, 'accumulated_submission_time': 31948.42696595192, 'accumulated_eval_time': 20263.464618206024, 'accumulated_logging_time': 1.1962149143218994, 'global_step': 92384, 'preemption_count': 0}), (94815, {'train/accuracy': 0.7034842371940613, 'train/loss': 1.5385183095932007, 'train/bleu': 35.83842989432134, 'validation/accuracy': 0.6895264983177185, 'validation/loss': 1.6064814329147339, 'validation/bleu': 30.30097926569033, 'validation/num_examples': 3000, 'test/accuracy': 0.7068154215812683, 'test/loss': 1.5066797733306885, 'test/bleu': 30.62494146013703, 'test/num_examples': 3003, 'score': 32788.33744096756, 'total_duration': 53536.12700033188, 'accumulated_submission_time': 32788.33744096756, 'accumulated_eval_time': 20743.521106243134, 'accumulated_logging_time': 1.2359349727630615, 'global_step': 94815, 'preemption_count': 0}), (97246, {'train/accuracy': 0.6954914927482605, 'train/loss': 1.5836246013641357, 'train/bleu': 35.719509026518935, 'validation/accuracy': 0.6894396543502808, 'validation/loss': 1.608036756515503, 'validation/bleu': 30.444119717517655, 'validation/num_examples': 3000, 'test/accuracy': 0.706466794013977, 'test/loss': 1.511273980140686, 'test/bleu': 30.46507838062227, 'test/num_examples': 3003, 'score': 33628.37856912613, 'total_duration': 54841.610292196274, 'accumulated_submission_time': 33628.37856912613, 'accumulated_eval_time': 21208.848113775253, 'accumulated_logging_time': 1.274146318435669, 'global_step': 97246, 'preemption_count': 0}), (99677, {'train/accuracy': 0.6979652643203735, 'train/loss': 1.5806125402450562, 'train/bleu': 35.63073779555025, 'validation/accuracy': 0.6885221600532532, 'validation/loss': 1.6104274988174438, 'validation/bleu': 30.35029837043829, 'validation/num_examples': 3000, 'test/accuracy': 0.7069316506385803, 'test/loss': 1.5114892721176147, 'test/bleu': 30.37346591024028, 'test/num_examples': 3003, 'score': 34468.591000556946, 'total_duration': 56148.71933174133, 'accumulated_submission_time': 34468.591000556946, 'accumulated_eval_time': 21675.629316568375, 'accumulated_logging_time': 1.3121390342712402, 'global_step': 99677, 'preemption_count': 0}), (102108, {'train/accuracy': 0.7036283016204834, 'train/loss': 1.5418643951416016, 'train/bleu': 36.0726587683943, 'validation/accuracy': 0.689315676689148, 'validation/loss': 1.6059999465942383, 'validation/bleu': 30.482721601595376, 'validation/num_examples': 3000, 'test/accuracy': 0.7062808871269226, 'test/loss': 1.5039721727371216, 'test/bleu': 30.5054449175256, 'test/num_examples': 3003, 'score': 35308.638811826706, 'total_duration': 57451.48646616936, 'accumulated_submission_time': 35308.638811826706, 'accumulated_eval_time': 22138.22982096672, 'accumulated_logging_time': 1.3539230823516846, 'global_step': 102108, 'preemption_count': 0}), (104539, {'train/accuracy': 0.6998471617698669, 'train/loss': 1.5669548511505127, 'train/bleu': 35.92477397313134, 'validation/accuracy': 0.6904687881469727, 'validation/loss': 1.6009769439697266, 'validation/bleu': 30.452983416343674, 'validation/num_examples': 3000, 'test/accuracy': 0.7068619132041931, 'test/loss': 1.5041028261184692, 'test/bleu': 30.359611628316312, 'test/num_examples': 3003, 'score': 36148.625277519226, 'total_duration': 58767.56322598457, 'accumulated_submission_time': 36148.625277519226, 'accumulated_eval_time': 22614.202502965927, 'accumulated_logging_time': 1.394146203994751, 'global_step': 104539, 'preemption_count': 0}), (106971, {'train/accuracy': 0.7189033031463623, 'train/loss': 1.4635096788406372, 'train/bleu': 37.323135864973146, 'validation/accuracy': 0.6899852156639099, 'validation/loss': 1.603049635887146, 'validation/bleu': 30.498854117375075, 'validation/num_examples': 3000, 'test/accuracy': 0.7066411375999451, 'test/loss': 1.5038784742355347, 'test/bleu': 30.1928181417836, 'test/num_examples': 3003, 'score': 36988.803647994995, 'total_duration': 60116.20536804199, 'accumulated_submission_time': 36988.803647994995, 'accumulated_eval_time': 23122.549444437027, 'accumulated_logging_time': 1.4340860843658447, 'global_step': 106971, 'preemption_count': 0}), (109401, {'train/accuracy': 0.7105783224105835, 'train/loss': 1.5062320232391357, 'train/bleu': 36.803288837604285, 'validation/accuracy': 0.6902704238891602, 'validation/loss': 1.5982686281204224, 'validation/bleu': 30.681621790036214, 'validation/num_examples': 3000, 'test/accuracy': 0.7091511487960815, 'test/loss': 1.4960204362869263, 'test/bleu': 30.79524511078754, 'test/num_examples': 3003, 'score': 37828.95445275307, 'total_duration': 61403.85651016235, 'accumulated_submission_time': 37828.95445275307, 'accumulated_eval_time': 23569.930787324905, 'accumulated_logging_time': 1.4757776260375977, 'global_step': 109401, 'preemption_count': 0}), (111832, {'train/accuracy': 0.7118147611618042, 'train/loss': 1.5033549070358276, 'train/bleu': 36.46637331018224, 'validation/accuracy': 0.6907911896705627, 'validation/loss': 1.6010395288467407, 'validation/bleu': 30.68753003150424, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.5006964206695557, 'test/bleu': 30.644621518964946, 'test/num_examples': 3003, 'score': 38669.03541469574, 'total_duration': 62706.950540065765, 'accumulated_submission_time': 38669.03541469574, 'accumulated_eval_time': 24032.82445716858, 'accumulated_logging_time': 1.5178844928741455, 'global_step': 111832, 'preemption_count': 0}), (114263, {'train/accuracy': 0.7177315354347229, 'train/loss': 1.4715121984481812, 'train/bleu': 37.124871771433476, 'validation/accuracy': 0.6904315948486328, 'validation/loss': 1.597692847251892, 'validation/bleu': 30.83881345974083, 'validation/num_examples': 3000, 'test/accuracy': 0.708512008190155, 'test/loss': 1.4953467845916748, 'test/bleu': 30.556692618619714, 'test/num_examples': 3003, 'score': 39509.11229848862, 'total_duration': 63997.05086636543, 'accumulated_submission_time': 39509.11229848862, 'accumulated_eval_time': 24482.726551771164, 'accumulated_logging_time': 1.5615882873535156, 'global_step': 114263, 'preemption_count': 0}), (116694, {'train/accuracy': 0.7157125473022461, 'train/loss': 1.4814000129699707, 'train/bleu': 36.77640951803491, 'validation/accuracy': 0.6911507248878479, 'validation/loss': 1.601112961769104, 'validation/bleu': 30.630743293458615, 'validation/num_examples': 3000, 'test/accuracy': 0.7089884281158447, 'test/loss': 1.5000479221343994, 'test/bleu': 30.72359865724317, 'test/num_examples': 3003, 'score': 40349.132508039474, 'total_duration': 65300.12974977493, 'accumulated_submission_time': 40349.132508039474, 'accumulated_eval_time': 24945.666995048523, 'accumulated_logging_time': 1.6028475761413574, 'global_step': 116694, 'preemption_count': 0}), (119125, {'train/accuracy': 0.7265663146972656, 'train/loss': 1.425704002380371, 'train/bleu': 37.280058759108954, 'validation/accuracy': 0.6910267472267151, 'validation/loss': 1.5974948406219482, 'validation/bleu': 30.580480746826456, 'validation/num_examples': 3000, 'test/accuracy': 0.7088141441345215, 'test/loss': 1.4978607892990112, 'test/bleu': 30.61001220758412, 'test/num_examples': 3003, 'score': 41189.27968287468, 'total_duration': 66606.66089344025, 'accumulated_submission_time': 41189.27968287468, 'accumulated_eval_time': 25411.930948019028, 'accumulated_logging_time': 1.6457676887512207, 'global_step': 119125, 'preemption_count': 0}), (121555, {'train/accuracy': 0.722062885761261, 'train/loss': 1.4466272592544556, 'train/bleu': 37.89198746749157, 'validation/accuracy': 0.6907168030738831, 'validation/loss': 1.5995148420333862, 'validation/bleu': 30.64146443986613, 'validation/num_examples': 3000, 'test/accuracy': 0.7092208862304688, 'test/loss': 1.4971085786819458, 'test/bleu': 30.700750046279442, 'test/num_examples': 3003, 'score': 42029.3257484436, 'total_duration': 67912.01016974449, 'accumulated_submission_time': 42029.3257484436, 'accumulated_eval_time': 25877.113832235336, 'accumulated_logging_time': 1.688605785369873, 'global_step': 121555, 'preemption_count': 0}), (123986, {'train/accuracy': 0.7214351892471313, 'train/loss': 1.4518346786499023, 'train/bleu': 37.47774055464697, 'validation/accuracy': 0.6916838884353638, 'validation/loss': 1.5992367267608643, 'validation/bleu': 30.64448680287795, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.4954748153686523, 'test/bleu': 30.69339181447463, 'test/num_examples': 3003, 'score': 42869.3330552578, 'total_duration': 69206.40446782112, 'accumulated_submission_time': 42869.3330552578, 'accumulated_eval_time': 26331.379861593246, 'accumulated_logging_time': 1.732133388519287, 'global_step': 123986, 'preemption_count': 0}), (126417, {'train/accuracy': 0.7267507910728455, 'train/loss': 1.4239526987075806, 'train/bleu': 38.03936471702318, 'validation/accuracy': 0.6922666430473328, 'validation/loss': 1.5971962213516235, 'validation/bleu': 30.60053445338147, 'validation/num_examples': 3000, 'test/accuracy': 0.7093719244003296, 'test/loss': 1.4938480854034424, 'test/bleu': 30.82672499037724, 'test/num_examples': 3003, 'score': 43709.52652788162, 'total_duration': 70500.4326248169, 'accumulated_submission_time': 43709.52652788162, 'accumulated_eval_time': 26785.094694375992, 'accumulated_logging_time': 1.7746250629425049, 'global_step': 126417, 'preemption_count': 0}), (128848, {'train/accuracy': 0.7229095101356506, 'train/loss': 1.4474419355392456, 'train/bleu': 37.66247762215224, 'validation/accuracy': 0.691745936870575, 'validation/loss': 1.5984151363372803, 'validation/bleu': 30.603091854338725, 'validation/num_examples': 3000, 'test/accuracy': 0.709511399269104, 'test/loss': 1.4947388172149658, 'test/bleu': 30.74373078206694, 'test/num_examples': 3003, 'score': 44549.701666116714, 'total_duration': 71797.655200243, 'accumulated_submission_time': 44549.701666116714, 'accumulated_eval_time': 27242.02351140976, 'accumulated_logging_time': 1.816523790359497, 'global_step': 128848, 'preemption_count': 0}), (131279, {'train/accuracy': 0.7239293456077576, 'train/loss': 1.4316215515136719, 'train/bleu': 37.897957791075164, 'validation/accuracy': 0.6918451189994812, 'validation/loss': 1.5985901355743408, 'validation/bleu': 30.610609263383143, 'validation/num_examples': 3000, 'test/accuracy': 0.7093486785888672, 'test/loss': 1.4943861961364746, 'test/bleu': 30.861089142384305, 'test/num_examples': 3003, 'score': 45389.855187654495, 'total_duration': 73102.1201004982, 'accumulated_submission_time': 45389.855187654495, 'accumulated_eval_time': 27706.21432876587, 'accumulated_logging_time': 1.8598039150238037, 'global_step': 131279, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7237716913223267, 'train/loss': 1.4409384727478027, 'train/bleu': 37.902471942390434, 'validation/accuracy': 0.691882312297821, 'validation/loss': 1.59796142578125, 'validation/bleu': 30.55257145746482, 'validation/num_examples': 3000, 'test/accuracy': 0.7095927000045776, 'test/loss': 1.4941442012786865, 'test/bleu': 30.79392196122609, 'test/num_examples': 3003, 'score': 46099.627485990524, 'total_duration': 74258.77504205704, 'accumulated_submission_time': 46099.627485990524, 'accumulated_eval_time': 28152.985813856125, 'accumulated_logging_time': 1.9054770469665527, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0208 10:45:35.076167 140225696298816 submission_runner.py:586] Timing: 46099.627485990524
I0208 10:45:35.076226 140225696298816 submission_runner.py:588] Total number of evals: 56
I0208 10:45:35.076265 140225696298816 submission_runner.py:589] ====================
I0208 10:45:35.076327 140225696298816 submission_runner.py:542] Using RNG seed 1037423020
I0208 10:45:35.078005 140225696298816 submission_runner.py:551] --- Tuning run 3/5 ---
I0208 10:45:35.078120 140225696298816 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3.
I0208 10:45:35.078349 140225696298816 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3/hparams.json.
I0208 10:45:35.079127 140225696298816 submission_runner.py:206] Initializing dataset.
I0208 10:45:35.081673 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 10:45:35.084894 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0208 10:45:35.121692 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 10:45:35.736850 140225696298816 submission_runner.py:213] Initializing model.
I0208 10:45:42.401076 140225696298816 submission_runner.py:255] Initializing optimizer.
I0208 10:45:43.171698 140225696298816 submission_runner.py:262] Initializing metrics bundle.
I0208 10:45:43.171870 140225696298816 submission_runner.py:280] Initializing checkpoint and logger.
I0208 10:45:43.172679 140225696298816 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3 with prefix checkpoint_
I0208 10:45:43.172800 140225696298816 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3/meta_data_0.json.
I0208 10:45:43.173020 140225696298816 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0208 10:45:43.173079 140225696298816 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0208 10:45:43.754880 140225696298816 logger_utils.py:220] Unable to record git information. Continuing without it.
I0208 10:45:44.321225 140225696298816 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3/flags_0.json.
I0208 10:45:44.324900 140225696298816 submission_runner.py:314] Starting training loop.
I0208 10:46:11.097668 140051005212416 logging_writer.py:48] [0] global_step=0, grad_norm=5.70151424407959, loss=11.02318286895752
I0208 10:46:11.109519 140225696298816 spec.py:321] Evaluating on the training split.
I0208 10:46:13.777395 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:50:56.712744 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 10:50:59.380825 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 10:55:41.993785 140225696298816 spec.py:349] Evaluating on the test split.
I0208 10:55:44.674183 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:00:27.389552 140225696298816 submission_runner.py:408] Time since start: 883.06s, 	Step: 1, 	{'train/accuracy': 0.0006154337315820158, 'train/loss': 11.025583267211914, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.784578561782837, 'total_duration': 883.0645732879639, 'accumulated_submission_time': 26.784578561782837, 'accumulated_eval_time': 856.2799527645111, 'accumulated_logging_time': 0}
I0208 11:00:27.399026 140051013605120 logging_writer.py:48] [1] accumulated_eval_time=856.279953, accumulated_logging_time=0, accumulated_submission_time=26.784579, global_step=1, preemption_count=0, score=26.784579, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.036274, test/num_examples=3003, total_duration=883.064573, train/accuracy=0.000615, train/bleu=0.000000, train/loss=11.025583, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.047277, validation/num_examples=3000
I0208 11:01:01.884315 140051005212416 logging_writer.py:48] [100] global_step=100, grad_norm=0.4269225299358368, loss=8.69248104095459
I0208 11:01:36.374666 140051013605120 logging_writer.py:48] [200] global_step=200, grad_norm=0.18607579171657562, loss=8.369735717773438
I0208 11:02:10.913185 140051005212416 logging_writer.py:48] [300] global_step=300, grad_norm=0.1943836361169815, loss=8.085269927978516
I0208 11:02:45.516204 140051013605120 logging_writer.py:48] [400] global_step=400, grad_norm=0.29111674427986145, loss=7.645389556884766
I0208 11:03:20.115174 140051005212416 logging_writer.py:48] [500] global_step=500, grad_norm=0.3085569739341736, loss=7.222339153289795
I0208 11:03:54.733501 140051013605120 logging_writer.py:48] [600] global_step=600, grad_norm=0.5080568790435791, loss=6.8993072509765625
I0208 11:04:29.354388 140051005212416 logging_writer.py:48] [700] global_step=700, grad_norm=0.6625404953956604, loss=6.738414287567139
I0208 11:05:03.974733 140051013605120 logging_writer.py:48] [800] global_step=800, grad_norm=0.8119605779647827, loss=6.433974266052246
I0208 11:05:38.589490 140051005212416 logging_writer.py:48] [900] global_step=900, grad_norm=0.6932745575904846, loss=6.205781936645508
I0208 11:06:13.168380 140051013605120 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5173838138580322, loss=6.000431060791016
I0208 11:06:47.784428 140051005212416 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6262744665145874, loss=5.841180801391602
I0208 11:07:22.412427 140051013605120 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6351032853126526, loss=5.529913425445557
I0208 11:07:57.043509 140051005212416 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.626311182975769, loss=5.4476318359375
I0208 11:08:31.663209 140051013605120 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.6886001825332642, loss=5.248095989227295
I0208 11:09:06.294665 140051005212416 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9422509074211121, loss=5.117588520050049
I0208 11:09:40.909518 140051013605120 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.3387504816055298, loss=4.987579345703125
I0208 11:10:15.541895 140051005212416 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9858227968215942, loss=4.910253524780273
I0208 11:10:50.181735 140051013605120 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.9472793340682983, loss=4.6384453773498535
I0208 11:11:24.813451 140051005212416 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8536354899406433, loss=4.603951930999756
I0208 11:11:59.481626 140051013605120 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.6323332786560059, loss=4.5192694664001465
I0208 11:12:34.141471 140051005212416 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.5345101356506348, loss=4.228578567504883
I0208 11:13:08.787216 140051013605120 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.973682165145874, loss=4.242386817932129
I0208 11:13:43.400612 140051005212416 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8115447759628296, loss=4.046808242797852
I0208 11:14:18.021846 140051013605120 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.4015605449676514, loss=3.937375545501709
I0208 11:14:27.439344 140225696298816 spec.py:321] Evaluating on the training split.
I0208 11:14:30.415048 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:17:32.302938 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 11:17:34.989676 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:20:38.612267 140225696298816 spec.py:349] Evaluating on the test split.
I0208 11:20:41.299336 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:23:25.067709 140225696298816 submission_runner.py:408] Time since start: 2260.74s, 	Step: 2429, 	{'train/accuracy': 0.4166429340839386, 'train/loss': 3.9009313583374023, 'train/bleu': 14.596829131524013, 'validation/accuracy': 0.4004042148590088, 'validation/loss': 4.036395072937012, 'validation/bleu': 9.782566022877925, 'validation/num_examples': 3000, 'test/accuracy': 0.38750800490379333, 'test/loss': 4.218213081359863, 'test/bleu': 8.298912293063825, 'test/num_examples': 3003, 'score': 866.7384278774261, 'total_duration': 2260.7427303791046, 'accumulated_submission_time': 866.7384278774261, 'accumulated_eval_time': 1393.9082560539246, 'accumulated_logging_time': 0.019158124923706055}
I0208 11:23:25.083322 140051005212416 logging_writer.py:48] [2429] accumulated_eval_time=1393.908256, accumulated_logging_time=0.019158, accumulated_submission_time=866.738428, global_step=2429, preemption_count=0, score=866.738428, test/accuracy=0.387508, test/bleu=8.298912, test/loss=4.218213, test/num_examples=3003, total_duration=2260.742730, train/accuracy=0.416643, train/bleu=14.596829, train/loss=3.900931, validation/accuracy=0.400404, validation/bleu=9.782566, validation/loss=4.036395, validation/num_examples=3000
I0208 11:23:49.911688 140051013605120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7784715890884399, loss=3.8770534992218018
I0208 11:24:24.459841 140051005212416 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8921748995780945, loss=3.704094886779785
I0208 11:24:59.075540 140051013605120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7892858982086182, loss=3.6354176998138428
I0208 11:25:33.700793 140051005212416 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8859619498252869, loss=3.61549711227417
I0208 11:26:08.298043 140051013605120 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0507729053497314, loss=3.432302713394165
I0208 11:26:42.909836 140051005212416 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.7520675659179688, loss=3.4537386894226074
I0208 11:27:17.528957 140051013605120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.7869630455970764, loss=3.361725091934204
I0208 11:27:52.156617 140051005212416 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9403066039085388, loss=3.294628858566284
I0208 11:28:26.792794 140051013605120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.865578293800354, loss=3.2149477005004883
I0208 11:29:01.430404 140051005212416 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.1603237390518188, loss=3.207836627960205
I0208 11:29:36.070465 140051013605120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.751439094543457, loss=3.033848762512207
I0208 11:30:10.725359 140051005212416 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8891195058822632, loss=3.1168432235717773
I0208 11:30:45.354255 140051013605120 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7452980279922485, loss=2.9967072010040283
I0208 11:31:19.953437 140051005212416 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8231009244918823, loss=2.9686877727508545
I0208 11:31:54.576382 140051013605120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8354814648628235, loss=2.906832695007324
I0208 11:32:29.231275 140051005212416 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6356099247932434, loss=2.9499692916870117
I0208 11:33:03.849841 140051013605120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6789188385009766, loss=2.8130619525909424
I0208 11:33:38.481786 140051005212416 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6693733334541321, loss=2.895031690597534
I0208 11:34:13.089327 140051013605120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9782236218452454, loss=2.787497043609619
I0208 11:34:47.712975 140051005212416 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7352078557014465, loss=2.714172601699829
I0208 11:35:22.327327 140051013605120 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6519702076911926, loss=2.652998208999634
I0208 11:35:56.945405 140051005212416 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6231776475906372, loss=2.704850673675537
I0208 11:36:31.595532 140051013605120 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6974138617515564, loss=2.635100841522217
I0208 11:37:06.223534 140051005212416 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6735407114028931, loss=2.7587804794311523
I0208 11:37:25.344532 140225696298816 spec.py:321] Evaluating on the training split.
I0208 11:37:28.323208 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:40:01.127159 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 11:40:03.812339 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:42:37.624852 140225696298816 spec.py:349] Evaluating on the test split.
I0208 11:42:40.304662 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 11:45:01.348550 140225696298816 submission_runner.py:408] Time since start: 3557.02s, 	Step: 4857, 	{'train/accuracy': 0.5418379306793213, 'train/loss': 2.6729602813720703, 'train/bleu': 24.88816508460163, 'validation/accuracy': 0.5444941520690918, 'validation/loss': 2.6168112754821777, 'validation/bleu': 20.575766431959572, 'validation/num_examples': 3000, 'test/accuracy': 0.5460693836212158, 'test/loss': 2.6440484523773193, 'test/bleu': 19.060328733347866, 'test/num_examples': 3003, 'score': 1706.9111173152924, 'total_duration': 3557.023582458496, 'accumulated_submission_time': 1706.9111173152924, 'accumulated_eval_time': 1849.9122297763824, 'accumulated_logging_time': 0.04645705223083496}
I0208 11:45:01.363373 140051013605120 logging_writer.py:48] [4857] accumulated_eval_time=1849.912230, accumulated_logging_time=0.046457, accumulated_submission_time=1706.911117, global_step=4857, preemption_count=0, score=1706.911117, test/accuracy=0.546069, test/bleu=19.060329, test/loss=2.644048, test/num_examples=3003, total_duration=3557.023582, train/accuracy=0.541838, train/bleu=24.888165, train/loss=2.672960, validation/accuracy=0.544494, validation/bleu=20.575766, validation/loss=2.616811, validation/num_examples=3000
I0208 11:45:16.566667 140051005212416 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6358479857444763, loss=2.6287379264831543
I0208 11:45:51.068826 140051013605120 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6407642364501953, loss=2.587430953979492
I0208 11:46:25.648106 140051005212416 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5680269002914429, loss=2.5791938304901123
I0208 11:47:00.245524 140051013605120 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6600022912025452, loss=2.638169527053833
I0208 11:47:34.845698 140051005212416 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8055273294448853, loss=2.5524420738220215
I0208 11:48:09.450237 140051013605120 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.7524958252906799, loss=2.533043146133423
I0208 11:48:44.073232 140051005212416 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6377277374267578, loss=2.5549745559692383
I0208 11:49:18.689642 140051013605120 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5606165528297424, loss=2.5016767978668213
I0208 11:49:53.298510 140051005212416 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.6062932014465332, loss=2.4861955642700195
I0208 11:50:27.903487 140051013605120 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5828378200531006, loss=2.455575466156006
I0208 11:51:02.512005 140051005212416 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.5617138743400574, loss=2.423846960067749
I0208 11:51:37.132995 140051013605120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5623844265937805, loss=2.4825949668884277
I0208 11:52:11.751634 140051005212416 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7698267102241516, loss=2.526961088180542
I0208 11:52:46.371972 140051013605120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6487867832183838, loss=2.508986473083496
I0208 11:53:20.978451 140051005212416 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5078302621841431, loss=2.481855869293213
I0208 11:53:55.603331 140051013605120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5836971402168274, loss=2.4089486598968506
I0208 11:54:30.200975 140051005212416 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5197665691375732, loss=2.374889612197876
I0208 11:55:04.801811 140051013605120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.48199009895324707, loss=2.3152518272399902
I0208 11:55:39.422109 140051005212416 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5018482804298401, loss=2.2752537727355957
I0208 11:56:14.006134 140051013605120 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4754899740219116, loss=2.3429300785064697
I0208 11:56:48.613034 140051005212416 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5828902125358582, loss=2.336198568344116
I0208 11:57:23.220608 140051013605120 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5977599620819092, loss=2.454686164855957
I0208 11:57:57.830538 140051005212416 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.48935991525650024, loss=2.2769999504089355
I0208 11:58:32.421161 140051013605120 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.45831942558288574, loss=2.244255542755127
I0208 11:59:01.540941 140225696298816 spec.py:321] Evaluating on the training split.
I0208 11:59:04.519462 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:01:35.123085 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 12:01:37.805156 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:04:10.748166 140225696298816 spec.py:349] Evaluating on the test split.
I0208 12:04:13.430609 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:06:35.335649 140225696298816 submission_runner.py:408] Time since start: 4851.01s, 	Step: 7286, 	{'train/accuracy': 0.5796564817428589, 'train/loss': 2.280468225479126, 'train/bleu': 27.28085169810225, 'validation/accuracy': 0.5861427783966064, 'validation/loss': 2.215071201324463, 'validation/bleu': 23.4061450554374, 'validation/num_examples': 3000, 'test/accuracy': 0.5889140963554382, 'test/loss': 2.2054107189178467, 'test/bleu': 21.840320736237747, 'test/num_examples': 3003, 'score': 2547.001212835312, 'total_duration': 4851.010681629181, 'accumulated_submission_time': 2547.001212835312, 'accumulated_eval_time': 2303.706892490387, 'accumulated_logging_time': 0.07251095771789551}
I0208 12:06:35.350935 140051005212416 logging_writer.py:48] [7286] accumulated_eval_time=2303.706892, accumulated_logging_time=0.072511, accumulated_submission_time=2547.001213, global_step=7286, preemption_count=0, score=2547.001213, test/accuracy=0.588914, test/bleu=21.840321, test/loss=2.205411, test/num_examples=3003, total_duration=4851.010682, train/accuracy=0.579656, train/bleu=27.280852, train/loss=2.280468, validation/accuracy=0.586143, validation/bleu=23.406145, validation/loss=2.215071, validation/num_examples=3000
I0208 12:06:40.552057 140051013605120 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.45073968172073364, loss=2.212172031402588
I0208 12:07:14.968126 140051005212416 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.5061417818069458, loss=2.319962978363037
I0208 12:07:49.515774 140051013605120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4740780293941498, loss=2.2633793354034424
I0208 12:08:24.114490 140051005212416 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.47782716155052185, loss=2.3045549392700195
I0208 12:08:58.722215 140051013605120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4338451623916626, loss=2.298372745513916
I0208 12:09:33.307935 140051005212416 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.42240557074546814, loss=2.1388511657714844
I0208 12:10:07.933377 140051013605120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4421249032020569, loss=2.2899134159088135
I0208 12:10:42.512881 140051005212416 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4300193786621094, loss=2.287013053894043
I0208 12:11:17.122467 140051013605120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.43938493728637695, loss=2.291821241378784
I0208 12:11:51.728541 140051005212416 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4336429238319397, loss=2.225463390350342
I0208 12:12:26.358864 140051013605120 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4377790689468384, loss=2.207873582839966
I0208 12:13:00.957365 140051005212416 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.43689438700675964, loss=2.3141956329345703
I0208 12:13:35.564726 140051013605120 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.41995128989219666, loss=2.25201678276062
I0208 12:14:10.148129 140051005212416 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.37783610820770264, loss=2.163036584854126
I0208 12:14:44.729641 140051013605120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.38343551754951477, loss=2.1339664459228516
I0208 12:15:19.310515 140051005212416 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.42463219165802, loss=2.244161367416382
I0208 12:15:53.911046 140051013605120 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3851945400238037, loss=2.172053337097168
I0208 12:16:28.503963 140051005212416 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3646402359008789, loss=2.2929790019989014
I0208 12:17:03.130271 140051013605120 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3522098958492279, loss=2.174452781677246
I0208 12:17:37.752323 140051005212416 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.4540086090564728, loss=2.300485849380493
I0208 12:18:12.358071 140051013605120 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3569691479206085, loss=2.1665780544281006
I0208 12:18:46.952751 140051005212416 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.415955513715744, loss=2.244905948638916
I0208 12:19:21.524587 140051013605120 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.364633172750473, loss=2.143867254257202
I0208 12:19:56.111426 140051005212416 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4135049283504486, loss=2.1686747074127197
I0208 12:20:30.721818 140051013605120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3310530185699463, loss=2.101421594619751
I0208 12:20:35.636222 140225696298816 spec.py:321] Evaluating on the training split.
I0208 12:20:38.608075 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:23:01.388632 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 12:23:04.087122 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:25:28.845816 140225696298816 spec.py:349] Evaluating on the test split.
I0208 12:25:31.534440 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:27:46.459429 140225696298816 submission_runner.py:408] Time since start: 6122.13s, 	Step: 9716, 	{'train/accuracy': 0.5948212146759033, 'train/loss': 2.13262677192688, 'train/bleu': 28.156699199500373, 'validation/accuracy': 0.606229305267334, 'validation/loss': 2.028346538543701, 'validation/bleu': 24.546534607890653, 'validation/num_examples': 3000, 'test/accuracy': 0.6107489466667175, 'test/loss': 2.0003881454467773, 'test/bleu': 23.531982121236606, 'test/num_examples': 3003, 'score': 3387.1982963085175, 'total_duration': 6122.1344566345215, 'accumulated_submission_time': 3387.1982963085175, 'accumulated_eval_time': 2734.530050754547, 'accumulated_logging_time': 0.0988461971282959}
I0208 12:27:46.476034 140051005212416 logging_writer.py:48] [9716] accumulated_eval_time=2734.530051, accumulated_logging_time=0.098846, accumulated_submission_time=3387.198296, global_step=9716, preemption_count=0, score=3387.198296, test/accuracy=0.610749, test/bleu=23.531982, test/loss=2.000388, test/num_examples=3003, total_duration=6122.134457, train/accuracy=0.594821, train/bleu=28.156699, train/loss=2.132627, validation/accuracy=0.606229, validation/bleu=24.546535, validation/loss=2.028347, validation/num_examples=3000
I0208 12:28:15.753082 140051013605120 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.36159542202949524, loss=2.145289659500122
I0208 12:28:50.237260 140051005212416 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.4200032651424408, loss=2.0560407638549805
I0208 12:29:24.784903 140051013605120 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3506826162338257, loss=2.15809965133667
I0208 12:29:59.364380 140051005212416 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3459555208683014, loss=2.081925392150879
I0208 12:30:33.966017 140051013605120 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.4133337438106537, loss=2.1592390537261963
I0208 12:31:08.535661 140051005212416 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.3146849274635315, loss=2.065669536590576
I0208 12:31:43.107167 140051013605120 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.32049018144607544, loss=2.068235158920288
I0208 12:32:17.658137 140051005212416 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.30347922444343567, loss=2.153125762939453
I0208 12:32:52.251474 140051013605120 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.37868356704711914, loss=2.0456037521362305
I0208 12:33:26.834326 140051005212416 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3724246621131897, loss=2.1001393795013428
I0208 12:34:01.408773 140051013605120 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.3155023753643036, loss=2.13879656791687
I0208 12:34:35.952780 140051005212416 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.335038423538208, loss=2.1342480182647705
I0208 12:35:10.538551 140051013605120 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.2979404628276825, loss=2.186509132385254
I0208 12:35:45.121587 140051005212416 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3260897994041443, loss=2.0323567390441895
I0208 12:36:19.704403 140051013605120 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.31229740381240845, loss=2.1034040451049805
I0208 12:36:54.292310 140051005212416 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3238860070705414, loss=2.0620157718658447
I0208 12:37:28.851821 140051013605120 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.28325966000556946, loss=2.0220038890838623
I0208 12:38:03.416562 140051005212416 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2773461937904358, loss=2.149123430252075
I0208 12:38:37.995241 140051013605120 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.33591076731681824, loss=2.1487882137298584
I0208 12:39:12.556627 140051005212416 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2891932725906372, loss=2.0075478553771973
I0208 12:39:47.109519 140051013605120 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.2761387825012207, loss=2.118734121322632
I0208 12:40:21.685082 140051005212416 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.27543431520462036, loss=2.0480804443359375
I0208 12:40:56.288660 140051013605120 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.3000446856021881, loss=2.128396987915039
I0208 12:41:30.859795 140051005212416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.30036312341690063, loss=2.0158755779266357
I0208 12:41:46.499378 140225696298816 spec.py:321] Evaluating on the training split.
I0208 12:41:49.476142 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:45:16.351896 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 12:45:19.038841 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:49:28.740668 140225696298816 spec.py:349] Evaluating on the test split.
I0208 12:49:31.424062 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 12:52:50.525433 140225696298816 submission_runner.py:408] Time since start: 7626.20s, 	Step: 12147, 	{'train/accuracy': 0.6030737161636353, 'train/loss': 2.049139976501465, 'train/bleu': 29.213199924869418, 'validation/accuracy': 0.6187896132469177, 'validation/loss': 1.9111295938491821, 'validation/bleu': 24.313988813799572, 'validation/num_examples': 3000, 'test/accuracy': 0.6287490725517273, 'test/loss': 1.8641327619552612, 'test/bleu': 24.73216670379908, 'test/num_examples': 3003, 'score': 4227.132491111755, 'total_duration': 7626.200456619263, 'accumulated_submission_time': 4227.132491111755, 'accumulated_eval_time': 3398.5560586452484, 'accumulated_logging_time': 0.12773990631103516}
I0208 12:52:50.542700 140051013605120 logging_writer.py:48] [12147] accumulated_eval_time=3398.556059, accumulated_logging_time=0.127740, accumulated_submission_time=4227.132491, global_step=12147, preemption_count=0, score=4227.132491, test/accuracy=0.628749, test/bleu=24.732167, test/loss=1.864133, test/num_examples=3003, total_duration=7626.200457, train/accuracy=0.603074, train/bleu=29.213200, train/loss=2.049140, validation/accuracy=0.618790, validation/bleu=24.313989, validation/loss=1.911130, validation/num_examples=3000
I0208 12:53:09.152416 140051005212416 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.27839377522468567, loss=2.063567638397217
I0208 12:53:43.622956 140051013605120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.28893715143203735, loss=2.109436273574829
I0208 12:54:18.189809 140051005212416 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2743864357471466, loss=2.028693199157715
I0208 12:54:52.928596 140051013605120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3234524726867676, loss=2.0404932498931885
I0208 12:55:27.534322 140051005212416 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2734936475753784, loss=1.9590448141098022
I0208 12:56:02.126786 140051013605120 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.27600744366645813, loss=1.9464551210403442
I0208 12:56:36.698276 140051005212416 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.27136239409446716, loss=2.0559909343719482
I0208 12:57:11.260851 140051013605120 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.262060284614563, loss=2.031195878982544
I0208 12:57:45.876408 140051005212416 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.28733372688293457, loss=1.9944144487380981
I0208 12:58:20.446430 140051013605120 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.30127856135368347, loss=2.0371639728546143
I0208 12:58:55.019378 140051005212416 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.271014004945755, loss=2.0320687294006348
I0208 12:59:29.581140 140051013605120 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.297101229429245, loss=2.035290241241455
I0208 13:00:04.188991 140051005212416 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.264028400182724, loss=2.063223361968994
I0208 13:00:38.787931 140051013605120 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2614896297454834, loss=1.9709587097167969
I0208 13:01:13.378121 140051005212416 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.35972273349761963, loss=2.004725933074951
I0208 13:01:47.972064 140051013605120 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2569713294506073, loss=1.9826914072036743
I0208 13:02:22.541038 140051005212416 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2614128589630127, loss=2.0145251750946045
I0208 13:02:57.152381 140051013605120 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3220113515853882, loss=2.088632822036743
I0208 13:03:31.730696 140051005212416 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2813567519187927, loss=1.9032299518585205
I0208 13:04:06.341571 140051013605120 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2652372121810913, loss=1.9775012731552124
I0208 13:04:40.920820 140051005212416 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.27757298946380615, loss=1.9395493268966675
I0208 13:05:15.490724 140051013605120 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.27475041151046753, loss=1.96271550655365
I0208 13:05:50.056452 140051005212416 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.30034297704696655, loss=2.010576009750366
I0208 13:06:24.630010 140051013605120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.36536815762519836, loss=1.9860399961471558
I0208 13:06:50.622508 140225696298816 spec.py:321] Evaluating on the training split.
I0208 13:06:53.594884 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:09:47.621082 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 13:09:50.290208 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:12:26.612495 140225696298816 spec.py:349] Evaluating on the test split.
I0208 13:12:29.306812 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:14:51.961738 140225696298816 submission_runner.py:408] Time since start: 8947.64s, 	Step: 14577, 	{'train/accuracy': 0.6107993721961975, 'train/loss': 1.964206337928772, 'train/bleu': 29.97132197779252, 'validation/accuracy': 0.6309283375740051, 'validation/loss': 1.819167971611023, 'validation/bleu': 26.438481201404674, 'validation/num_examples': 3000, 'test/accuracy': 0.6393702030181885, 'test/loss': 1.7721221446990967, 'test/bleu': 25.362275219367604, 'test/num_examples': 3003, 'score': 5067.12614607811, 'total_duration': 8947.63675236702, 'accumulated_submission_time': 5067.12614607811, 'accumulated_eval_time': 3879.895225048065, 'accumulated_logging_time': 0.15470433235168457}
I0208 13:14:51.978810 140051005212416 logging_writer.py:48] [14577] accumulated_eval_time=3879.895225, accumulated_logging_time=0.154704, accumulated_submission_time=5067.126146, global_step=14577, preemption_count=0, score=5067.126146, test/accuracy=0.639370, test/bleu=25.362275, test/loss=1.772122, test/num_examples=3003, total_duration=8947.636752, train/accuracy=0.610799, train/bleu=29.971322, train/loss=1.964206, validation/accuracy=0.630928, validation/bleu=26.438481, validation/loss=1.819168, validation/num_examples=3000
I0208 13:15:00.246439 140051013605120 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.31817662715911865, loss=2.074709177017212
I0208 13:15:34.685875 140051005212416 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2607848346233368, loss=1.9468560218811035
I0208 13:16:09.199306 140051013605120 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.2828421890735626, loss=1.9845266342163086
I0208 13:16:43.761929 140051005212416 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.30040547251701355, loss=1.9225361347198486
I0208 13:17:18.314451 140051013605120 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3453957438468933, loss=1.8687011003494263
I0208 13:17:52.880188 140051005212416 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.37440192699432373, loss=2.0154805183410645
I0208 13:18:27.442010 140051013605120 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2816062271595001, loss=1.9879326820373535
I0208 13:19:02.003166 140051005212416 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.3003116548061371, loss=1.9517035484313965
I0208 13:19:36.559982 140051013605120 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2651582956314087, loss=1.8651973009109497
I0208 13:20:11.136890 140051005212416 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.28859373927116394, loss=1.9867578744888306
I0208 13:20:45.704235 140051013605120 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2816355228424072, loss=1.9547491073608398
I0208 13:21:20.243270 140051005212416 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2952840030193329, loss=1.9218661785125732
I0208 13:21:54.808186 140051013605120 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2748088240623474, loss=1.8617295026779175
I0208 13:22:29.400322 140051005212416 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.3191862404346466, loss=1.970914602279663
I0208 13:23:03.984304 140051013605120 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.324458509683609, loss=1.9317752122879028
I0208 13:23:38.556558 140051005212416 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.32192566990852356, loss=2.0174858570098877
I0208 13:24:13.119819 140051013605120 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3274605870246887, loss=1.8963122367858887
I0208 13:24:47.657023 140051005212416 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.31215935945510864, loss=1.8881511688232422
I0208 13:25:22.227525 140051013605120 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.3345162272453308, loss=1.834375262260437
I0208 13:25:56.805153 140051005212416 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.33887842297554016, loss=1.9036691188812256
I0208 13:26:31.382978 140051013605120 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.39059484004974365, loss=1.9100250005722046
I0208 13:27:05.953684 140051005212416 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2993115186691284, loss=1.8931541442871094
I0208 13:27:40.547304 140051013605120 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3086096942424774, loss=1.9341518878936768
I0208 13:28:15.126024 140051005212416 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3960719704627991, loss=1.9149657487869263
I0208 13:28:49.699549 140051013605120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.28622379899024963, loss=1.9152880907058716
I0208 13:28:52.192696 140225696298816 spec.py:321] Evaluating on the training split.
I0208 13:28:55.168326 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:31:24.103187 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 13:31:26.779932 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:33:54.945418 140225696298816 spec.py:349] Evaluating on the test split.
I0208 13:33:57.620988 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:36:13.585797 140225696298816 submission_runner.py:408] Time since start: 10229.26s, 	Step: 17009, 	{'train/accuracy': 0.6224886775016785, 'train/loss': 1.8965948820114136, 'train/bleu': 30.31643486549434, 'validation/accuracy': 0.6383429765701294, 'validation/loss': 1.756270408630371, 'validation/bleu': 26.823648244559074, 'validation/num_examples': 3000, 'test/accuracy': 0.6478647589683533, 'test/loss': 1.6986522674560547, 'test/bleu': 26.263295660080857, 'test/num_examples': 3003, 'score': 5907.2546446323395, 'total_duration': 10229.260805130005, 'accumulated_submission_time': 5907.2546446323395, 'accumulated_eval_time': 4321.288250684738, 'accumulated_logging_time': 0.1813514232635498}
I0208 13:36:13.603068 140051005212416 logging_writer.py:48] [17009] accumulated_eval_time=4321.288251, accumulated_logging_time=0.181351, accumulated_submission_time=5907.254645, global_step=17009, preemption_count=0, score=5907.254645, test/accuracy=0.647865, test/bleu=26.263296, test/loss=1.698652, test/num_examples=3003, total_duration=10229.260805, train/accuracy=0.622489, train/bleu=30.316435, train/loss=1.896595, validation/accuracy=0.638343, validation/bleu=26.823648, validation/loss=1.756270, validation/num_examples=3000
I0208 13:36:45.260756 140051013605120 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3263044059276581, loss=1.939113974571228
I0208 13:37:19.769509 140051005212416 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.28770962357521057, loss=1.8734915256500244
I0208 13:37:54.300877 140051013605120 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.30855792760849, loss=1.9405348300933838
I0208 13:38:28.864716 140051005212416 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.33564502000808716, loss=1.878602147102356
I0208 13:39:03.437837 140051013605120 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3277793824672699, loss=1.910437822341919
I0208 13:39:37.983314 140051005212416 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33643391728401184, loss=1.9573613405227661
I0208 13:40:12.570371 140051013605120 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3584803640842438, loss=1.8889025449752808
I0208 13:40:47.125036 140051005212416 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.30393096804618835, loss=1.8752977848052979
I0208 13:41:21.687735 140051013605120 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3001355528831482, loss=1.920799970626831
I0208 13:41:56.259329 140051005212416 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.35289910435676575, loss=1.8954771757125854
I0208 13:42:30.839060 140051013605120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.6221987009048462, loss=1.9872416257858276
I0208 13:43:05.391030 140051005212416 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.4605521559715271, loss=1.9901092052459717
I0208 13:43:39.950903 140051013605120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3514188230037689, loss=1.9753727912902832
I0208 13:44:14.510999 140051005212416 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3385922610759735, loss=1.9351409673690796
I0208 13:44:49.074699 140051013605120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3189733922481537, loss=1.9274581670761108
I0208 13:45:23.648064 140051005212416 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.31860485672950745, loss=1.8545583486557007
I0208 13:45:58.226115 140051013605120 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.33246076107025146, loss=1.8528558015823364
I0208 13:46:32.791742 140051005212416 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3217019736766815, loss=1.953248381614685
I0208 13:47:07.349901 140051013605120 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.30321168899536133, loss=1.7739955186843872
I0208 13:47:41.889433 140051005212416 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.32762622833251953, loss=1.84464693069458
I0208 13:48:16.481727 140051013605120 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.33503735065460205, loss=1.8888397216796875
I0208 13:48:51.073439 140051005212416 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.33744844794273376, loss=1.8695297241210938
I0208 13:49:25.652329 140051013605120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3579151928424835, loss=1.8523552417755127
I0208 13:50:00.228817 140051005212416 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.5067622065544128, loss=1.9050699472427368
I0208 13:50:13.775313 140225696298816 spec.py:321] Evaluating on the training split.
I0208 13:50:16.748082 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:53:07.809220 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 13:53:10.488193 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:55:38.900117 140225696298816 spec.py:349] Evaluating on the test split.
I0208 13:55:41.586141 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 13:58:00.231429 140225696298816 submission_runner.py:408] Time since start: 11535.91s, 	Step: 19441, 	{'train/accuracy': 0.6325414180755615, 'train/loss': 1.800859808921814, 'train/bleu': 30.4674639846037, 'validation/accuracy': 0.6434885859489441, 'validation/loss': 1.7217135429382324, 'validation/bleu': 27.554292710135996, 'validation/num_examples': 3000, 'test/accuracy': 0.6533263921737671, 'test/loss': 1.6645915508270264, 'test/bleu': 26.58671307572002, 'test/num_examples': 3003, 'score': 6747.339457988739, 'total_duration': 11535.906452178955, 'accumulated_submission_time': 6747.339457988739, 'accumulated_eval_time': 4787.744311332703, 'accumulated_logging_time': 0.20973849296569824}
I0208 13:58:00.251092 140051013605120 logging_writer.py:48] [19441] accumulated_eval_time=4787.744311, accumulated_logging_time=0.209738, accumulated_submission_time=6747.339458, global_step=19441, preemption_count=0, score=6747.339458, test/accuracy=0.653326, test/bleu=26.586713, test/loss=1.664592, test/num_examples=3003, total_duration=11535.906452, train/accuracy=0.632541, train/bleu=30.467464, train/loss=1.800860, validation/accuracy=0.643489, validation/bleu=27.554293, validation/loss=1.721714, validation/num_examples=3000
I0208 13:58:20.893745 140051005212416 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3477730453014374, loss=1.873581051826477
I0208 13:58:55.367604 140051013605120 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3713350296020508, loss=1.8453810214996338
I0208 13:59:29.910105 140051005212416 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3472611606121063, loss=1.8983967304229736
I0208 14:00:04.498426 140051013605120 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3371131420135498, loss=1.9716662168502808
I0208 14:00:39.069650 140051005212416 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3607492446899414, loss=1.8850617408752441
I0208 14:01:13.625234 140051013605120 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3583095073699951, loss=1.8883914947509766
I0208 14:01:48.202317 140051005212416 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.369615375995636, loss=1.8265403509140015
I0208 14:02:22.772516 140051013605120 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3372085988521576, loss=1.8382266759872437
I0208 14:02:57.334417 140051005212416 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3541790544986725, loss=1.8748183250427246
I0208 14:03:31.907680 140051013605120 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.33154764771461487, loss=1.9260119199752808
I0208 14:04:06.480190 140051005212416 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.36361613869667053, loss=1.884704351425171
I0208 14:04:41.038075 140051013605120 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3443618714809418, loss=1.914484977722168
I0208 14:05:15.591200 140051005212416 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.3726543188095093, loss=1.8629951477050781
I0208 14:05:50.167012 140051013605120 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4461906850337982, loss=1.837357521057129
I0208 14:06:24.743520 140051005212416 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3694036304950714, loss=1.789417028427124
I0208 14:06:59.305973 140051013605120 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4387066960334778, loss=1.8206336498260498
I0208 14:07:33.851952 140051005212416 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.4168815314769745, loss=1.8109045028686523
I0208 14:08:08.412317 140051013605120 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4242643117904663, loss=1.850706934928894
I0208 14:08:42.976403 140051005212416 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.5350892543792725, loss=1.8563817739486694
I0208 14:09:17.564153 140051013605120 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3580702245235443, loss=1.8624495267868042
I0208 14:09:52.120458 140051005212416 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.35041543841362, loss=1.878031849861145
I0208 14:10:26.668653 140051013605120 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3870026767253876, loss=1.8385798931121826
I0208 14:11:01.193335 140051005212416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.42074844241142273, loss=1.808489441871643
I0208 14:11:35.759243 140051013605120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.35404083132743835, loss=1.8437409400939941
I0208 14:12:00.351990 140225696298816 spec.py:321] Evaluating on the training split.
I0208 14:12:03.338732 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:15:09.198659 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 14:15:11.866683 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:17:40.494601 140225696298816 spec.py:349] Evaluating on the test split.
I0208 14:17:43.177727 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:20:05.648544 140225696298816 submission_runner.py:408] Time since start: 12861.32s, 	Step: 21873, 	{'train/accuracy': 0.6281017065048218, 'train/loss': 1.841070532798767, 'train/bleu': 30.706210179092377, 'validation/accuracy': 0.6485598087310791, 'validation/loss': 1.6945736408233643, 'validation/bleu': 27.498524011031034, 'validation/num_examples': 3000, 'test/accuracy': 0.6594852209091187, 'test/loss': 1.6314548254013062, 'test/bleu': 27.107368915175847, 'test/num_examples': 3003, 'score': 7587.3537764549255, 'total_duration': 12861.323557853699, 'accumulated_submission_time': 7587.3537764549255, 'accumulated_eval_time': 5273.040806770325, 'accumulated_logging_time': 0.2392292022705078}
I0208 14:20:05.665848 140051005212416 logging_writer.py:48] [21873] accumulated_eval_time=5273.040807, accumulated_logging_time=0.239229, accumulated_submission_time=7587.353776, global_step=21873, preemption_count=0, score=7587.353776, test/accuracy=0.659485, test/bleu=27.107369, test/loss=1.631455, test/num_examples=3003, total_duration=12861.323558, train/accuracy=0.628102, train/bleu=30.706210, train/loss=1.841071, validation/accuracy=0.648560, validation/bleu=27.498524, validation/loss=1.694574, validation/num_examples=3000
I0208 14:20:15.318949 140051013605120 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.39068639278411865, loss=1.9310810565948486
I0208 14:20:49.748462 140051005212416 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.34784841537475586, loss=1.9019666910171509
I0208 14:21:24.281715 140051013605120 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.5152899026870728, loss=1.8952776193618774
I0208 14:21:58.813657 140051005212416 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.42324915528297424, loss=1.9424173831939697
I0208 14:22:33.380119 140051013605120 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.40053290128707886, loss=1.754122018814087
I0208 14:23:07.970966 140051005212416 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.4070526957511902, loss=1.9192074537277222
I0208 14:23:42.516457 140051013605120 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3705023229122162, loss=1.8896105289459229
I0208 14:24:17.074823 140051005212416 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3435756266117096, loss=1.8101081848144531
I0208 14:24:51.636286 140051013605120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.37664994597435, loss=1.8795340061187744
I0208 14:25:26.172498 140051005212416 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3381422758102417, loss=1.8304959535598755
I0208 14:26:00.699630 140051013605120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.37825995683670044, loss=1.848167896270752
I0208 14:26:35.238524 140051005212416 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4367404580116272, loss=1.9388569593429565
I0208 14:27:09.779097 140051013605120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3590109050273895, loss=1.8746668100357056
I0208 14:27:44.318728 140051005212416 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.4044104814529419, loss=1.8177567720413208
I0208 14:28:18.882573 140051013605120 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3647514879703522, loss=1.8424036502838135
I0208 14:28:53.414711 140051005212416 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.34213492274284363, loss=1.8397263288497925
I0208 14:29:27.960538 140051013605120 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.511789858341217, loss=1.8173940181732178
I0208 14:30:02.522494 140051005212416 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4071914255619049, loss=1.8269076347351074
I0208 14:30:37.075913 140051013605120 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3814801871776581, loss=1.7866325378417969
I0208 14:31:11.642858 140051005212416 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.530291736125946, loss=1.8606687784194946
I0208 14:31:46.198701 140051013605120 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3882448971271515, loss=1.9409316778182983
I0208 14:32:20.759209 140051005212416 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.4064711928367615, loss=1.7984389066696167
I0208 14:32:55.341158 140051013605120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.3931426405906677, loss=1.879085659980774
I0208 14:33:29.904610 140051005212416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.37047499418258667, loss=1.883836030960083
I0208 14:34:04.476526 140051013605120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.38070157170295715, loss=1.8652771711349487
I0208 14:34:05.936825 140225696298816 spec.py:321] Evaluating on the training split.
I0208 14:34:08.900707 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:37:10.769103 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 14:37:13.445746 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:39:41.285946 140225696298816 spec.py:349] Evaluating on the test split.
I0208 14:39:43.967248 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:42:07.503881 140225696298816 submission_runner.py:408] Time since start: 14183.18s, 	Step: 24306, 	{'train/accuracy': 0.6283778548240662, 'train/loss': 1.8341097831726074, 'train/bleu': 30.231787337608484, 'validation/accuracy': 0.6494525671005249, 'validation/loss': 1.675093650817871, 'validation/bleu': 27.867396431428066, 'validation/num_examples': 3000, 'test/accuracy': 0.6609842777252197, 'test/loss': 1.61290442943573, 'test/bleu': 27.086848862449706, 'test/num_examples': 3003, 'score': 8427.538682699203, 'total_duration': 14183.1789124012, 'accumulated_submission_time': 8427.538682699203, 'accumulated_eval_time': 5754.607835054398, 'accumulated_logging_time': 0.26638150215148926}
I0208 14:42:07.522159 140051005212416 logging_writer.py:48] [24306] accumulated_eval_time=5754.607835, accumulated_logging_time=0.266382, accumulated_submission_time=8427.538683, global_step=24306, preemption_count=0, score=8427.538683, test/accuracy=0.660984, test/bleu=27.086849, test/loss=1.612904, test/num_examples=3003, total_duration=14183.178912, train/accuracy=0.628378, train/bleu=30.231787, train/loss=1.834110, validation/accuracy=0.649453, validation/bleu=27.867396, validation/loss=1.675094, validation/num_examples=3000
I0208 14:42:40.204128 140051013605120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4003884196281433, loss=1.8573360443115234
I0208 14:43:14.675098 140051005212416 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3880087733268738, loss=1.8501917123794556
I0208 14:43:49.220913 140051013605120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.41761764883995056, loss=1.8660621643066406
I0208 14:44:23.762000 140051005212416 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.37129542231559753, loss=1.8420522212982178
I0208 14:44:58.308472 140051013605120 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.4660167396068573, loss=1.857370376586914
I0208 14:45:32.878706 140051005212416 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.48516643047332764, loss=1.8005709648132324
I0208 14:46:07.425182 140051013605120 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4618956744670868, loss=1.7970830202102661
I0208 14:46:41.978240 140051005212416 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3415752351284027, loss=1.816062569618225
I0208 14:47:16.528764 140051013605120 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.4358115792274475, loss=1.8442007303237915
I0208 14:47:51.069655 140051005212416 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.44751840829849243, loss=1.9397982358932495
I0208 14:48:25.620884 140051013605120 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3967556655406952, loss=1.7999974489212036
I0208 14:49:00.150059 140051005212416 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4083300530910492, loss=1.7297152280807495
I0208 14:49:34.711402 140051013605120 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.38121479749679565, loss=1.7588796615600586
I0208 14:50:09.256052 140051005212416 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.4483100473880768, loss=1.8642421960830688
I0208 14:50:43.817292 140051013605120 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3805086314678192, loss=1.9731006622314453
I0208 14:51:18.395949 140051005212416 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3611111342906952, loss=1.7536958456039429
I0208 14:51:52.952543 140051013605120 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3671484589576721, loss=1.7458817958831787
I0208 14:52:27.521641 140051005212416 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.34650564193725586, loss=1.7797726392745972
I0208 14:53:02.085802 140051013605120 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.3925298750400543, loss=1.8697344064712524
I0208 14:53:36.649756 140051005212416 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.37860530614852905, loss=1.8061944246292114
I0208 14:54:11.208800 140051013605120 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.36492887139320374, loss=1.8614223003387451
I0208 14:54:45.746329 140051005212416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3807884156703949, loss=1.8183573484420776
I0208 14:55:20.321913 140051013605120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.48163002729415894, loss=1.8878450393676758
I0208 14:55:54.875355 140051005212416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.40678635239601135, loss=1.82377028465271
I0208 14:56:07.719941 140225696298816 spec.py:321] Evaluating on the training split.
I0208 14:56:10.685593 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 14:58:48.945545 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 14:58:51.633066 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:01:20.427418 140225696298816 spec.py:349] Evaluating on the test split.
I0208 15:01:23.112343 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:03:42.945969 140225696298816 submission_runner.py:408] Time since start: 15478.62s, 	Step: 26739, 	{'train/accuracy': 0.6350250840187073, 'train/loss': 1.7813962697982788, 'train/bleu': 31.10056987152565, 'validation/accuracy': 0.6521431803703308, 'validation/loss': 1.6590827703475952, 'validation/bleu': 27.86187178986665, 'validation/num_examples': 3000, 'test/accuracy': 0.661495566368103, 'test/loss': 1.5966767072677612, 'test/bleu': 27.351616233989287, 'test/num_examples': 3003, 'score': 9267.650310277939, 'total_duration': 15478.620971679688, 'accumulated_submission_time': 9267.650310277939, 'accumulated_eval_time': 6209.833786487579, 'accumulated_logging_time': 0.29460883140563965}
I0208 15:03:42.964162 140051013605120 logging_writer.py:48] [26739] accumulated_eval_time=6209.833786, accumulated_logging_time=0.294609, accumulated_submission_time=9267.650310, global_step=26739, preemption_count=0, score=9267.650310, test/accuracy=0.661496, test/bleu=27.351616, test/loss=1.596677, test/num_examples=3003, total_duration=15478.620972, train/accuracy=0.635025, train/bleu=31.100570, train/loss=1.781396, validation/accuracy=0.652143, validation/bleu=27.861872, validation/loss=1.659083, validation/num_examples=3000
I0208 15:04:04.302843 140051005212416 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.4488295614719391, loss=1.8319151401519775
I0208 15:04:38.726511 140051013605120 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4195805788040161, loss=1.8860464096069336
I0208 15:05:13.245078 140051005212416 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4034062623977661, loss=1.7604271173477173
I0208 15:05:47.787531 140051013605120 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.3753480315208435, loss=1.7754876613616943
I0208 15:06:22.336170 140051005212416 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3934246897697449, loss=1.7745040655136108
I0208 15:06:56.888499 140051013605120 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.46494606137275696, loss=1.847504734992981
I0208 15:07:31.429577 140051005212416 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.44870078563690186, loss=1.7301921844482422
I0208 15:08:05.983495 140051013605120 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5572955012321472, loss=1.8536336421966553
I0208 15:08:40.528062 140051005212416 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.39666926860809326, loss=1.824733018875122
I0208 15:09:15.082132 140051013605120 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.39685168862342834, loss=1.8090485334396362
I0208 15:09:49.648423 140051005212416 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4450118839740753, loss=1.8427656888961792
I0208 15:10:24.202338 140051013605120 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.38882651925086975, loss=1.806740403175354
I0208 15:10:58.730195 140051005212416 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4169614315032959, loss=1.8282082080841064
I0208 15:11:33.286162 140051013605120 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.39046692848205566, loss=1.808292031288147
I0208 15:12:07.815066 140051005212416 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.4711931645870209, loss=1.8017027378082275
I0208 15:12:42.366536 140051013605120 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4018736183643341, loss=1.7807084321975708
I0208 15:13:16.884176 140051005212416 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.44259339570999146, loss=1.861907720565796
I0208 15:13:51.430453 140051013605120 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.4192938804626465, loss=1.856111764907837
I0208 15:14:25.963370 140051005212416 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.38011857867240906, loss=1.841705322265625
I0208 15:15:00.509881 140051013605120 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.42578306794166565, loss=1.8487359285354614
I0208 15:15:35.054843 140051005212416 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.42319023609161377, loss=1.838214635848999
I0208 15:16:09.616028 140051013605120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3689388334751129, loss=1.825346827507019
I0208 15:16:44.140021 140051005212416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.41875094175338745, loss=1.956536889076233
I0208 15:17:18.676683 140051013605120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.40392616391181946, loss=1.7920641899108887
I0208 15:17:43.239491 140225696298816 spec.py:321] Evaluating on the training split.
I0208 15:17:46.208107 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:20:26.728018 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 15:20:29.417914 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:23:07.371353 140225696298816 spec.py:349] Evaluating on the test split.
I0208 15:23:10.060355 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:25:43.198326 140225696298816 submission_runner.py:408] Time since start: 16798.87s, 	Step: 29173, 	{'train/accuracy': 0.6313155889511108, 'train/loss': 1.8113305568695068, 'train/bleu': 31.167266657108822, 'validation/accuracy': 0.6551065444946289, 'validation/loss': 1.6476982831954956, 'validation/bleu': 28.01659926041504, 'validation/num_examples': 3000, 'test/accuracy': 0.6651443839073181, 'test/loss': 1.5824037790298462, 'test/bleu': 27.611150169070616, 'test/num_examples': 3003, 'score': 10107.838258981705, 'total_duration': 16798.87334752083, 'accumulated_submission_time': 10107.838258981705, 'accumulated_eval_time': 6689.792566776276, 'accumulated_logging_time': 0.323559045791626}
I0208 15:25:43.216656 140051005212416 logging_writer.py:48] [29173] accumulated_eval_time=6689.792567, accumulated_logging_time=0.323559, accumulated_submission_time=10107.838259, global_step=29173, preemption_count=0, score=10107.838259, test/accuracy=0.665144, test/bleu=27.611150, test/loss=1.582404, test/num_examples=3003, total_duration=16798.873348, train/accuracy=0.631316, train/bleu=31.167267, train/loss=1.811331, validation/accuracy=0.655107, validation/bleu=28.016599, validation/loss=1.647698, validation/num_examples=3000
I0208 15:25:52.853376 140051013605120 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.41089096665382385, loss=1.7845433950424194
I0208 15:26:27.263448 140051005212416 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3624802231788635, loss=1.8183740377426147
I0208 15:27:01.749408 140051013605120 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.397697389125824, loss=1.8109630346298218
I0208 15:27:36.261096 140051005212416 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.37953516840934753, loss=1.7689063549041748
I0208 15:28:10.784966 140051013605120 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4635413885116577, loss=1.7768948078155518
I0208 15:28:45.322248 140051005212416 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.42295852303504944, loss=1.8538686037063599
I0208 15:29:19.861422 140051013605120 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.40426889061927795, loss=1.8097798824310303
I0208 15:29:54.411132 140051005212416 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3877140283584595, loss=1.7630945444107056
I0208 15:30:28.945005 140051013605120 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.4049301743507385, loss=1.7780038118362427
I0208 15:31:03.479817 140051005212416 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.41748809814453125, loss=1.7404793500900269
I0208 15:31:38.031000 140051013605120 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.4278951585292816, loss=1.866665244102478
I0208 15:32:12.599789 140051005212416 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3468729853630066, loss=1.775631070137024
I0208 15:32:47.161158 140051013605120 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.41642817854881287, loss=1.769998550415039
I0208 15:33:21.688035 140051005212416 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.41845598816871643, loss=1.8355222940444946
I0208 15:33:56.221366 140051013605120 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.3977062702178955, loss=1.8030431270599365
I0208 15:34:30.747013 140051005212416 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.4115278720855713, loss=1.772966742515564
I0208 15:35:05.266297 140051013605120 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.40467169880867004, loss=1.8098046779632568
I0208 15:35:39.806666 140051005212416 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.38138145208358765, loss=1.6763865947723389
I0208 15:36:14.363273 140051013605120 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3961048126220703, loss=1.7797738313674927
I0208 15:36:48.895519 140051005212416 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3644782304763794, loss=1.7530369758605957
I0208 15:37:23.445079 140051013605120 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.42071568965911865, loss=1.7518720626831055
I0208 15:37:58.000894 140051005212416 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.4537857472896576, loss=1.7948336601257324
I0208 15:38:32.570225 140051013605120 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.43744319677352905, loss=1.7091821432113647
I0208 15:39:07.133519 140051005212416 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4771786630153656, loss=1.7784857749938965
I0208 15:39:41.692862 140051013605120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.41605043411254883, loss=1.832717776298523
I0208 15:39:43.499918 140225696298816 spec.py:321] Evaluating on the training split.
I0208 15:39:46.476591 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:42:48.455289 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 15:42:51.125107 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:45:45.245114 140225696298816 spec.py:349] Evaluating on the test split.
I0208 15:45:47.928489 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 15:48:22.564749 140225696298816 submission_runner.py:408] Time since start: 18158.24s, 	Step: 31607, 	{'train/accuracy': 0.6526026129722595, 'train/loss': 1.652001142501831, 'train/bleu': 32.13943278256692, 'validation/accuracy': 0.657090425491333, 'validation/loss': 1.6380786895751953, 'validation/bleu': 28.228548539202578, 'validation/num_examples': 3000, 'test/accuracy': 0.6655162572860718, 'test/loss': 1.5741702318191528, 'test/bleu': 27.550129631044033, 'test/num_examples': 3003, 'score': 10948.034049272537, 'total_duration': 18158.23977446556, 'accumulated_submission_time': 10948.034049272537, 'accumulated_eval_time': 7208.857345581055, 'accumulated_logging_time': 0.35329389572143555}
I0208 15:48:22.584778 140051005212416 logging_writer.py:48] [31607] accumulated_eval_time=7208.857346, accumulated_logging_time=0.353294, accumulated_submission_time=10948.034049, global_step=31607, preemption_count=0, score=10948.034049, test/accuracy=0.665516, test/bleu=27.550130, test/loss=1.574170, test/num_examples=3003, total_duration=18158.239774, train/accuracy=0.652603, train/bleu=32.139433, train/loss=1.652001, validation/accuracy=0.657090, validation/bleu=28.228549, validation/loss=1.638079, validation/num_examples=3000
I0208 15:48:54.905164 140051013605120 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.46429339051246643, loss=1.8057576417922974
I0208 15:49:29.375637 140051005212416 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.4151316285133362, loss=1.8312619924545288
I0208 15:50:03.913022 140051013605120 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3920515477657318, loss=1.825325846672058
I0208 15:50:38.451090 140051005212416 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.372567743062973, loss=1.7919044494628906
I0208 15:51:12.961605 140051013605120 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.4087563157081604, loss=1.85660982131958
I0208 15:51:47.518782 140051005212416 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.3797019124031067, loss=1.769057273864746
I0208 15:52:22.058479 140051013605120 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3762417137622833, loss=1.7282428741455078
I0208 15:52:56.617880 140051005212416 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3963826894760132, loss=1.7740501165390015
I0208 15:53:31.165523 140051013605120 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3838355839252472, loss=1.7513679265975952
I0208 15:54:05.712040 140051005212416 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.4608321189880371, loss=1.7123483419418335
I0208 15:54:40.238950 140051013605120 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3630146086215973, loss=1.8332051038742065
I0208 15:55:14.788287 140051005212416 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.4144630432128906, loss=1.8377379179000854
I0208 15:55:49.315768 140051013605120 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.38308510184288025, loss=1.8324332237243652
I0208 15:56:23.846419 140051005212416 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.39916056394577026, loss=1.795531988143921
I0208 15:56:58.416682 140051013605120 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4273803234100342, loss=1.8104058504104614
I0208 15:57:33.005534 140051005212416 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3585788309574127, loss=1.814239740371704
I0208 15:58:07.626615 140051013605120 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3779755234718323, loss=1.7933751344680786
I0208 15:58:42.174164 140051005212416 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.3548474907875061, loss=1.8655403852462769
I0208 15:59:16.718673 140051013605120 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4264381229877472, loss=1.8163392543792725
I0208 15:59:51.268591 140051005212416 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.45389336347579956, loss=1.8017919063568115
I0208 16:00:25.794372 140051013605120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.4125092327594757, loss=1.7926026582717896
I0208 16:01:00.341994 140051005212416 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.39709946513175964, loss=1.8086122274398804
I0208 16:01:34.887652 140051013605120 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.3440956771373749, loss=1.712256669998169
I0208 16:02:09.439719 140051005212416 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3676878809928894, loss=1.8097440004348755
I0208 16:02:22.628773 140225696298816 spec.py:321] Evaluating on the training split.
I0208 16:02:25.628343 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:05:53.560202 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 16:05:56.225620 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:08:48.877679 140225696298816 spec.py:349] Evaluating on the test split.
I0208 16:08:51.543353 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:11:04.239566 140225696298816 submission_runner.py:408] Time since start: 19519.91s, 	Step: 34040, 	{'train/accuracy': 0.6393951773643494, 'train/loss': 1.768338680267334, 'train/bleu': 31.008950206319334, 'validation/accuracy': 0.6554041504859924, 'validation/loss': 1.6242969036102295, 'validation/bleu': 28.08600000210978, 'validation/num_examples': 3000, 'test/accuracy': 0.6660507917404175, 'test/loss': 1.565197229385376, 'test/bleu': 27.395916308501732, 'test/num_examples': 3003, 'score': 11787.991804122925, 'total_duration': 19519.91458582878, 'accumulated_submission_time': 11787.991804122925, 'accumulated_eval_time': 7730.468078613281, 'accumulated_logging_time': 0.3834218978881836}
I0208 16:11:04.260068 140051013605120 logging_writer.py:48] [34040] accumulated_eval_time=7730.468079, accumulated_logging_time=0.383422, accumulated_submission_time=11787.991804, global_step=34040, preemption_count=0, score=11787.991804, test/accuracy=0.666051, test/bleu=27.395916, test/loss=1.565197, test/num_examples=3003, total_duration=19519.914586, train/accuracy=0.639395, train/bleu=31.008950, train/loss=1.768339, validation/accuracy=0.655404, validation/bleu=28.086000, validation/loss=1.624297, validation/num_examples=3000
I0208 16:11:25.242207 140051005212416 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.38896283507347107, loss=1.8166110515594482
I0208 16:11:59.668649 140051013605120 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.40552130341529846, loss=1.8288559913635254
I0208 16:12:34.221483 140051005212416 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.41088056564331055, loss=1.7374773025512695
I0208 16:13:08.761307 140051013605120 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.37242645025253296, loss=1.7162469625473022
I0208 16:13:43.288991 140051005212416 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.36854663491249084, loss=1.782055139541626
I0208 16:14:17.807511 140051013605120 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.4114731252193451, loss=1.7569444179534912
I0208 16:14:52.307665 140051005212416 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.38627365231513977, loss=1.796460509300232
I0208 16:15:26.852473 140051013605120 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4442996382713318, loss=1.7574081420898438
I0208 16:16:01.398210 140051005212416 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3898715674877167, loss=1.8148486614227295
I0208 16:16:35.941805 140051013605120 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.36590680480003357, loss=1.7305878400802612
I0208 16:17:10.471174 140051005212416 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.409432053565979, loss=1.8924682140350342
I0208 16:17:44.983600 140051013605120 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3694172501564026, loss=1.7577486038208008
I0208 16:18:19.552629 140051005212416 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.4445624351501465, loss=1.8671385049819946
I0208 16:18:54.079692 140051013605120 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4297054409980774, loss=1.7987419366836548
I0208 16:19:28.625418 140051005212416 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.4038792550563812, loss=1.7905752658843994
I0208 16:20:03.141588 140051013605120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.41868820786476135, loss=1.7288570404052734
I0208 16:20:37.694913 140051005212416 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.6583237051963806, loss=1.7941019535064697
I0208 16:21:12.221280 140051013605120 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3836567997932434, loss=1.8104113340377808
I0208 16:21:46.799208 140051005212416 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.41246306896209717, loss=1.7608321905136108
I0208 16:22:21.325275 140051013605120 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.4007892906665802, loss=1.8309727907180786
I0208 16:22:55.884157 140051005212416 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.41320863366127014, loss=1.8428930044174194
I0208 16:23:30.429845 140051013605120 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.47031059861183167, loss=1.775730013847351
I0208 16:24:04.987153 140051005212416 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.4028361737728119, loss=1.7670665979385376
I0208 16:24:39.521057 140051013605120 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3976140320301056, loss=1.85615873336792
I0208 16:25:04.442208 140225696298816 spec.py:321] Evaluating on the training split.
I0208 16:25:07.406857 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:28:43.939414 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 16:28:46.613718 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:32:33.690454 140225696298816 spec.py:349] Evaluating on the test split.
I0208 16:32:36.368978 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:35:41.358667 140225696298816 submission_runner.py:408] Time since start: 20997.03s, 	Step: 36474, 	{'train/accuracy': 0.6336399912834167, 'train/loss': 1.7970607280731201, 'train/bleu': 31.23075734922129, 'validation/accuracy': 0.6571400165557861, 'validation/loss': 1.6196331977844238, 'validation/bleu': 28.072348613153586, 'validation/num_examples': 3000, 'test/accuracy': 0.667793869972229, 'test/loss': 1.5579041242599487, 'test/bleu': 27.544555632291114, 'test/num_examples': 3003, 'score': 12628.087430715561, 'total_duration': 20997.033695459366, 'accumulated_submission_time': 12628.087430715561, 'accumulated_eval_time': 8367.384491205215, 'accumulated_logging_time': 0.41417574882507324}
I0208 16:35:41.378342 140051005212416 logging_writer.py:48] [36474] accumulated_eval_time=8367.384491, accumulated_logging_time=0.414176, accumulated_submission_time=12628.087431, global_step=36474, preemption_count=0, score=12628.087431, test/accuracy=0.667794, test/bleu=27.544556, test/loss=1.557904, test/num_examples=3003, total_duration=20997.033695, train/accuracy=0.633640, train/bleu=31.230757, train/loss=1.797061, validation/accuracy=0.657140, validation/bleu=28.072349, validation/loss=1.619633, validation/num_examples=3000
I0208 16:35:50.653269 140051013605120 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.42337122559547424, loss=1.720162034034729
I0208 16:36:25.018200 140051005212416 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.43391498923301697, loss=1.8300107717514038
I0208 16:36:59.471544 140051013605120 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.39197447896003723, loss=1.738625407218933
I0208 16:37:34.035400 140051005212416 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.39707839488983154, loss=1.8186992406845093
I0208 16:38:08.572876 140051013605120 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.39377185702323914, loss=1.8166545629501343
I0208 16:38:43.071910 140051005212416 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.4191838204860687, loss=1.7517049312591553
I0208 16:39:17.614532 140051013605120 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.4129308760166168, loss=1.7465794086456299
I0208 16:39:52.145938 140051005212416 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4157353937625885, loss=1.8180017471313477
I0208 16:40:26.700379 140051013605120 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.46863096952438354, loss=1.7579116821289062
I0208 16:41:01.236882 140051005212416 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.44017094373703003, loss=1.7850720882415771
I0208 16:41:35.794080 140051013605120 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.4229695200920105, loss=1.7992398738861084
I0208 16:42:10.293332 140051005212416 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.45229455828666687, loss=1.7708752155303955
I0208 16:42:44.842742 140051013605120 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.5036884546279907, loss=1.8274697065353394
I0208 16:43:19.353966 140051005212416 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3646572530269623, loss=1.7235965728759766
I0208 16:43:53.890290 140051013605120 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.39552047848701477, loss=1.8367022275924683
I0208 16:44:28.440391 140051005212416 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.433563232421875, loss=1.756497859954834
I0208 16:45:02.972401 140051013605120 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.43533647060394287, loss=1.7359143495559692
I0208 16:45:37.534982 140051005212416 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.36274996399879456, loss=1.7789279222488403
I0208 16:46:12.071049 140051013605120 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.4053727388381958, loss=1.7577763795852661
I0208 16:46:46.608797 140051005212416 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.426150918006897, loss=1.724307656288147
I0208 16:47:21.143529 140051013605120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3895997107028961, loss=1.7653934955596924
I0208 16:47:55.690695 140051005212416 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.4404664933681488, loss=1.8202482461929321
I0208 16:48:30.229632 140051013605120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.35450223088264465, loss=1.7205414772033691
I0208 16:49:04.738710 140051005212416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.4541498124599457, loss=1.747983694076538
I0208 16:49:39.290911 140051013605120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.4025854766368866, loss=1.827606439590454
I0208 16:49:41.440931 140225696298816 spec.py:321] Evaluating on the training split.
I0208 16:49:44.418593 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:52:58.512334 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 16:53:01.192152 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:55:40.950337 140225696298816 spec.py:349] Evaluating on the test split.
I0208 16:55:43.616027 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 16:58:23.189362 140225696298816 submission_runner.py:408] Time since start: 22358.86s, 	Step: 38908, 	{'train/accuracy': 0.6437124013900757, 'train/loss': 1.718425989151001, 'train/bleu': 31.870111946059616, 'validation/accuracy': 0.6600537896156311, 'validation/loss': 1.6126872301101685, 'validation/bleu': 28.302752187830926, 'validation/num_examples': 3000, 'test/accuracy': 0.6705943942070007, 'test/loss': 1.5488643646240234, 'test/bleu': 27.784236811734953, 'test/num_examples': 3003, 'score': 13468.063895463943, 'total_duration': 22358.86438035965, 'accumulated_submission_time': 13468.063895463943, 'accumulated_eval_time': 8889.13286614418, 'accumulated_logging_time': 0.4446568489074707}
I0208 16:58:23.209850 140051005212416 logging_writer.py:48] [38908] accumulated_eval_time=8889.132866, accumulated_logging_time=0.444657, accumulated_submission_time=13468.063895, global_step=38908, preemption_count=0, score=13468.063895, test/accuracy=0.670594, test/bleu=27.784237, test/loss=1.548864, test/num_examples=3003, total_duration=22358.864380, train/accuracy=0.643712, train/bleu=31.870112, train/loss=1.718426, validation/accuracy=0.660054, validation/bleu=28.302752, validation/loss=1.612687, validation/num_examples=3000
I0208 16:58:55.187469 140051013605120 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3885044753551483, loss=1.698648452758789
I0208 16:59:29.653922 140051005212416 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.40893658995628357, loss=1.7679777145385742
I0208 17:00:04.175147 140051013605120 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.37422147393226624, loss=1.804807424545288
I0208 17:00:38.708265 140051005212416 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.36214038729667664, loss=1.8866442441940308
I0208 17:01:13.249088 140051013605120 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4131833612918854, loss=1.7618900537490845
I0208 17:01:47.791503 140051005212416 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3639371991157532, loss=1.7339454889297485
I0208 17:02:22.341848 140051013605120 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.41225141286849976, loss=1.753328800201416
I0208 17:02:56.909971 140051005212416 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.40309229493141174, loss=1.76027250289917
I0208 17:03:31.441485 140051013605120 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4802395701408386, loss=1.7432959079742432
I0208 17:04:05.993172 140051005212416 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.41291356086730957, loss=1.699995994567871
I0208 17:04:40.534811 140051013605120 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4272259771823883, loss=1.7343199253082275
I0208 17:05:15.089675 140051005212416 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3770238161087036, loss=1.7081003189086914
I0208 17:05:49.634231 140051013605120 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3860063850879669, loss=1.7429730892181396
I0208 17:06:24.145149 140051005212416 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.46428003907203674, loss=1.8206621408462524
I0208 17:06:58.679959 140051013605120 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.49618035554885864, loss=1.8347078561782837
I0208 17:07:33.231666 140051005212416 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4417639672756195, loss=1.763363242149353
I0208 17:08:07.777870 140051013605120 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3536580801010132, loss=1.7018463611602783
I0208 17:08:42.335998 140051005212416 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3952781558036804, loss=1.7745670080184937
I0208 17:09:16.881999 140051013605120 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.40927305817604065, loss=1.800371527671814
I0208 17:09:51.420210 140051005212416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.4135377109050751, loss=1.841164469718933
I0208 17:10:25.966763 140051013605120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.3826114535331726, loss=1.7041112184524536
I0208 17:11:00.522084 140051005212416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.3884049654006958, loss=1.8132190704345703
I0208 17:11:35.080690 140051013605120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3748406171798706, loss=1.688527226448059
I0208 17:12:09.647679 140051005212416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.4193694293498993, loss=1.7742446660995483
I0208 17:12:23.526952 140225696298816 spec.py:321] Evaluating on the training split.
I0208 17:12:26.522883 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:15:44.007414 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 17:15:46.678638 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:19:19.154293 140225696298816 spec.py:349] Evaluating on the test split.
I0208 17:19:21.827408 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:22:23.741802 140225696298816 submission_runner.py:408] Time since start: 23799.42s, 	Step: 41342, 	{'train/accuracy': 0.6403557658195496, 'train/loss': 1.7567963600158691, 'train/bleu': 31.643399674411956, 'validation/accuracy': 0.659694254398346, 'validation/loss': 1.6073123216629028, 'validation/bleu': 27.765434272920903, 'validation/num_examples': 3000, 'test/accuracy': 0.6712451577186584, 'test/loss': 1.5380420684814453, 'test/bleu': 27.958816788714334, 'test/num_examples': 3003, 'score': 14308.293565273285, 'total_duration': 23799.416815519333, 'accumulated_submission_time': 14308.293565273285, 'accumulated_eval_time': 9489.347652196884, 'accumulated_logging_time': 0.47722625732421875}
I0208 17:22:23.766516 140051013605120 logging_writer.py:48] [41342] accumulated_eval_time=9489.347652, accumulated_logging_time=0.477226, accumulated_submission_time=14308.293565, global_step=41342, preemption_count=0, score=14308.293565, test/accuracy=0.671245, test/bleu=27.958817, test/loss=1.538042, test/num_examples=3003, total_duration=23799.416816, train/accuracy=0.640356, train/bleu=31.643400, train/loss=1.756796, validation/accuracy=0.659694, validation/bleu=27.765434, validation/loss=1.607312, validation/num_examples=3000
I0208 17:22:44.074023 140051005212416 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.41498586535453796, loss=1.7718425989151
I0208 17:23:18.525007 140051013605120 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3777797222137451, loss=1.636080026626587
I0208 17:23:53.056558 140051005212416 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.4031074643135071, loss=1.817430853843689
I0208 17:24:27.594450 140051013605120 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3806946277618408, loss=1.6970908641815186
I0208 17:25:02.119017 140051005212416 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.3938572108745575, loss=1.8220051527023315
I0208 17:25:36.652589 140051013605120 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.4129257798194885, loss=1.8552132844924927
I0208 17:26:11.173382 140051005212416 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.38883116841316223, loss=1.744983196258545
I0208 17:26:45.689137 140051013605120 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.4384133517742157, loss=1.8720306158065796
I0208 17:27:20.215349 140051005212416 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4400484263896942, loss=1.785477876663208
I0208 17:27:54.770546 140051013605120 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.4045192301273346, loss=1.8364406824111938
I0208 17:28:29.313850 140051005212416 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.39811524748802185, loss=1.7223491668701172
I0208 17:29:03.839727 140051013605120 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.43193188309669495, loss=1.755967378616333
I0208 17:29:38.386959 140051005212416 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3598311245441437, loss=1.778350591659546
I0208 17:30:12.907679 140051013605120 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3636091947555542, loss=1.7205169200897217
I0208 17:30:47.430701 140051005212416 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.5019048452377319, loss=1.782282829284668
I0208 17:31:21.970737 140051013605120 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.40436628460884094, loss=1.7572556734085083
I0208 17:31:56.514078 140051005212416 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.38755035400390625, loss=1.7957565784454346
I0208 17:32:31.032015 140051013605120 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.3613061308860779, loss=1.7631633281707764
I0208 17:33:05.593659 140051005212416 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.5151976943016052, loss=1.7025715112686157
I0208 17:33:40.134794 140051013605120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3771320879459381, loss=1.7536723613739014
I0208 17:34:14.670831 140051005212416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3867217004299164, loss=1.733020544052124
I0208 17:34:49.214383 140051013605120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3917153775691986, loss=1.7110905647277832
I0208 17:35:23.764313 140051005212416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.4890810251235962, loss=1.7431634664535522
I0208 17:35:58.320407 140051013605120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.385194331407547, loss=1.739066481590271
I0208 17:36:23.954570 140225696298816 spec.py:321] Evaluating on the training split.
I0208 17:36:26.930302 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:39:31.160827 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 17:39:33.847413 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:42:21.768326 140225696298816 spec.py:349] Evaluating on the test split.
I0208 17:42:24.471817 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 17:44:57.198344 140225696298816 submission_runner.py:408] Time since start: 25152.87s, 	Step: 43776, 	{'train/accuracy': 0.6402798295021057, 'train/loss': 1.7504782676696777, 'train/bleu': 31.42158195384738, 'validation/accuracy': 0.6611325144767761, 'validation/loss': 1.5975406169891357, 'validation/bleu': 28.680036466352934, 'validation/num_examples': 3000, 'test/accuracy': 0.6745337247848511, 'test/loss': 1.532659888267517, 'test/bleu': 27.9198714817124, 'test/num_examples': 3003, 'score': 15148.395472049713, 'total_duration': 25152.873359441757, 'accumulated_submission_time': 15148.395472049713, 'accumulated_eval_time': 10002.591368198395, 'accumulated_logging_time': 0.5125997066497803}
I0208 17:44:57.219252 140051005212416 logging_writer.py:48] [43776] accumulated_eval_time=10002.591368, accumulated_logging_time=0.512600, accumulated_submission_time=15148.395472, global_step=43776, preemption_count=0, score=15148.395472, test/accuracy=0.674534, test/bleu=27.919871, test/loss=1.532660, test/num_examples=3003, total_duration=25152.873359, train/accuracy=0.640280, train/bleu=31.421582, train/loss=1.750478, validation/accuracy=0.661133, validation/bleu=28.680036, validation/loss=1.597541, validation/num_examples=3000
I0208 17:45:05.823526 140051013605120 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4188458025455475, loss=1.80980384349823
I0208 17:45:40.208830 140051005212416 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.41279149055480957, loss=1.7040925025939941
I0208 17:46:14.715082 140051013605120 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3946229815483093, loss=1.7410534620285034
I0208 17:46:49.274377 140051005212416 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.4606785774230957, loss=1.8089749813079834
I0208 17:47:23.780191 140051013605120 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.37905874848365784, loss=1.662800908088684
I0208 17:47:58.298726 140051005212416 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.4265575110912323, loss=1.7996948957443237
I0208 17:48:32.832566 140051013605120 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.40785351395606995, loss=1.7448774576187134
I0208 17:49:07.372887 140051005212416 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.4009338319301605, loss=1.7724275588989258
I0208 17:49:41.911869 140051013605120 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.39309412240982056, loss=1.7472492456436157
I0208 17:50:16.465298 140051005212416 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.36168837547302246, loss=1.7912200689315796
I0208 17:50:51.028327 140051013605120 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.38152679800987244, loss=1.734804391860962
I0208 17:51:25.583903 140051005212416 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.3747970759868622, loss=1.693289875984192
I0208 17:52:00.135090 140051013605120 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.39229312539100647, loss=1.7165958881378174
I0208 17:52:34.671816 140051005212416 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3873496651649475, loss=1.7779088020324707
I0208 17:53:09.203996 140051013605120 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.44746848940849304, loss=1.7530158758163452
I0208 17:53:43.737798 140051005212416 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.4241863787174225, loss=1.7277613878250122
I0208 17:54:18.277417 140051013605120 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3872680366039276, loss=1.7835724353790283
I0208 17:54:52.827927 140051005212416 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.4682546854019165, loss=1.750664234161377
I0208 17:55:27.389866 140051013605120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.43353894352912903, loss=1.740027666091919
I0208 17:56:01.939209 140051005212416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.38910454511642456, loss=1.7991300821304321
I0208 17:56:36.482062 140051013605120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3997534513473511, loss=1.764911413192749
I0208 17:57:11.019923 140051005212416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.4007207453250885, loss=1.7870932817459106
I0208 17:57:45.550194 140051013605120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.41568002104759216, loss=1.7449970245361328
I0208 17:58:20.081644 140051005212416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3807944357395172, loss=1.8028250932693481
I0208 17:58:54.632955 140051013605120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.4472697079181671, loss=1.7629005908966064
I0208 17:58:57.469616 140225696298816 spec.py:321] Evaluating on the training split.
I0208 17:59:00.444276 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:02:12.541098 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 18:02:15.215033 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:05:03.359711 140225696298816 spec.py:349] Evaluating on the test split.
I0208 18:05:06.042023 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:07:39.147815 140225696298816 submission_runner.py:408] Time since start: 26514.82s, 	Step: 46210, 	{'train/accuracy': 0.6428989768028259, 'train/loss': 1.7245334386825562, 'train/bleu': 31.507355253150934, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.5930095911026, 'validation/bleu': 28.495653401907823, 'validation/num_examples': 3000, 'test/accuracy': 0.6745337247848511, 'test/loss': 1.5249536037445068, 'test/bleu': 28.18982572482699, 'test/num_examples': 3003, 'score': 15988.560994148254, 'total_duration': 26514.822845697403, 'accumulated_submission_time': 15988.560994148254, 'accumulated_eval_time': 10524.269515752792, 'accumulated_logging_time': 0.5430536270141602}
I0208 18:07:39.168367 140051005212416 logging_writer.py:48] [46210] accumulated_eval_time=10524.269516, accumulated_logging_time=0.543054, accumulated_submission_time=15988.560994, global_step=46210, preemption_count=0, score=15988.560994, test/accuracy=0.674534, test/bleu=28.189826, test/loss=1.524954, test/num_examples=3003, total_duration=26514.822846, train/accuracy=0.642899, train/bleu=31.507355, train/loss=1.724533, validation/accuracy=0.662062, validation/bleu=28.495653, validation/loss=1.593010, validation/num_examples=3000
I0208 18:08:10.455037 140051013605120 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.43778789043426514, loss=1.686469554901123
I0208 18:08:44.904083 140051005212416 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.4005531668663025, loss=1.6692456007003784
I0208 18:09:19.431859 140051013605120 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4194149672985077, loss=1.8861807584762573
I0208 18:09:53.940553 140051005212416 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3820568323135376, loss=1.7674695253372192
I0208 18:10:28.473120 140051013605120 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.5543040037155151, loss=1.7682348489761353
I0208 18:11:02.998549 140051005212416 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.4006395936012268, loss=1.739690899848938
I0208 18:11:37.542966 140051013605120 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.36998099088668823, loss=1.7027181386947632
I0208 18:12:12.117829 140051005212416 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.36640000343322754, loss=1.6675797700881958
I0208 18:12:46.666924 140051013605120 logging_writer.py:48] [47100] global_step=47100, grad_norm=11.590187072753906, loss=1.7628803253173828
I0208 18:13:21.202281 140051005212416 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.4382236897945404, loss=1.8014419078826904
I0208 18:13:55.737174 140051013605120 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.411565899848938, loss=1.6908020973205566
I0208 18:14:30.249439 140051005212416 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.4236982464790344, loss=1.8042103052139282
I0208 18:15:04.790257 140051013605120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3753323554992676, loss=1.733840823173523
I0208 18:15:39.327119 140051005212416 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.38022011518478394, loss=1.671317219734192
I0208 18:16:13.849224 140051013605120 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.42292577028274536, loss=1.7210825681686401
I0208 18:16:48.366135 140051005212416 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4009495675563812, loss=1.758054256439209
I0208 18:17:22.882683 140051013605120 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3950769901275635, loss=1.7543737888336182
I0208 18:17:57.425918 140051005212416 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.4036824107170105, loss=1.7264641523361206
I0208 18:18:31.966412 140051013605120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3930022418498993, loss=1.7632852792739868
I0208 18:19:06.503592 140051005212416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3707301914691925, loss=1.8046015501022339
I0208 18:19:41.061052 140051013605120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.39671245217323303, loss=1.6819617748260498
I0208 18:20:15.639211 140051005212416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.36615896224975586, loss=1.6948814392089844
I0208 18:20:50.190137 140051013605120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3815198242664337, loss=1.751044750213623
I0208 18:21:24.735918 140051005212416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3973723351955414, loss=1.7888859510421753
I0208 18:21:39.306493 140225696298816 spec.py:321] Evaluating on the training split.
I0208 18:21:42.268710 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:25:07.100135 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 18:25:09.776720 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:27:46.880869 140225696298816 spec.py:349] Evaluating on the test split.
I0208 18:27:49.561615 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:30:13.196550 140225696298816 submission_runner.py:408] Time since start: 27868.87s, 	Step: 48644, 	{'train/accuracy': 0.6434541940689087, 'train/loss': 1.7303590774536133, 'train/bleu': 31.336669119899753, 'validation/accuracy': 0.6629676222801208, 'validation/loss': 1.5866459608078003, 'validation/bleu': 28.315392529444374, 'validation/num_examples': 3000, 'test/accuracy': 0.6765324473381042, 'test/loss': 1.513628602027893, 'test/bleu': 28.04292709290652, 'test/num_examples': 3003, 'score': 16828.613729715347, 'total_duration': 27868.871577739716, 'accumulated_submission_time': 16828.613729715347, 'accumulated_eval_time': 11038.159530639648, 'accumulated_logging_time': 0.5735483169555664}
I0208 18:30:13.217911 140051013605120 logging_writer.py:48] [48644] accumulated_eval_time=11038.159531, accumulated_logging_time=0.573548, accumulated_submission_time=16828.613730, global_step=48644, preemption_count=0, score=16828.613730, test/accuracy=0.676532, test/bleu=28.042927, test/loss=1.513629, test/num_examples=3003, total_duration=27868.871578, train/accuracy=0.643454, train/bleu=31.336669, train/loss=1.730359, validation/accuracy=0.662968, validation/bleu=28.315393, validation/loss=1.586646, validation/num_examples=3000
I0208 18:30:32.850018 140051005212416 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.4074402451515198, loss=1.7476565837860107
I0208 18:31:07.277654 140051013605120 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3603228032588959, loss=1.6976019144058228
I0208 18:31:41.792174 140051005212416 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3803499937057495, loss=1.7334437370300293
I0208 18:32:16.330729 140051013605120 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.40014219284057617, loss=1.6716662645339966
I0208 18:32:50.901743 140051005212416 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3606378436088562, loss=1.7821398973464966
I0208 18:33:25.430944 140051013605120 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3515836000442505, loss=1.6344945430755615
I0208 18:33:59.976210 140051005212416 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.37939998507499695, loss=1.7595361471176147
I0208 18:34:34.505793 140051013605120 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.380058228969574, loss=1.7193810939788818
I0208 18:35:09.059049 140051005212416 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.37985286116600037, loss=1.7637860774993896
I0208 18:35:43.582414 140051013605120 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.41904664039611816, loss=1.7259482145309448
I0208 18:36:18.107622 140051005212416 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.43848079442977905, loss=1.7505041360855103
I0208 18:36:52.663704 140051013605120 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.602137565612793, loss=1.7592988014221191
I0208 18:37:27.188207 140051005212416 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.44201087951660156, loss=1.810247778892517
I0208 18:38:01.742512 140051013605120 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.37555205821990967, loss=1.7213832139968872
I0208 18:38:36.296649 140051005212416 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.4495794177055359, loss=1.756582260131836
I0208 18:39:10.846927 140051013605120 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.4049525856971741, loss=1.679464340209961
I0208 18:39:45.402158 140051005212416 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.42883050441741943, loss=1.7211449146270752
I0208 18:40:19.939076 140051013605120 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.435055673122406, loss=1.771559715270996
I0208 18:40:54.472609 140051005212416 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.4054785668849945, loss=1.711814045906067
I0208 18:41:29.017636 140051013605120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3672960102558136, loss=1.6547000408172607
I0208 18:42:03.558903 140051005212416 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.40036776661872864, loss=1.7872601747512817
I0208 18:42:38.115409 140051013605120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.46971407532691956, loss=1.706120491027832
I0208 18:43:12.679924 140051005212416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.38008666038513184, loss=1.8031524419784546
I0208 18:43:47.224828 140051013605120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.401946097612381, loss=1.707956314086914
I0208 18:44:13.201480 140225696298816 spec.py:321] Evaluating on the training split.
I0208 18:44:16.169948 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:47:03.142766 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 18:47:05.819273 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:49:42.950488 140225696298816 spec.py:349] Evaluating on the test split.
I0208 18:49:45.619598 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 18:52:21.567917 140225696298816 submission_runner.py:408] Time since start: 29197.24s, 	Step: 51077, 	{'train/accuracy': 0.6509331464767456, 'train/loss': 1.667337417602539, 'train/bleu': 32.02152703215946, 'validation/accuracy': 0.6633147597312927, 'validation/loss': 1.5808254480361938, 'validation/bleu': 28.656745396147187, 'validation/num_examples': 3000, 'test/accuracy': 0.6761838793754578, 'test/loss': 1.5135340690612793, 'test/bleu': 28.28753882239032, 'test/num_examples': 3003, 'score': 17668.511252641678, 'total_duration': 29197.242948055267, 'accumulated_submission_time': 17668.511252641678, 'accumulated_eval_time': 11526.525929927826, 'accumulated_logging_time': 0.6058268547058105}
I0208 18:52:21.589461 140051005212416 logging_writer.py:48] [51077] accumulated_eval_time=11526.525930, accumulated_logging_time=0.605827, accumulated_submission_time=17668.511253, global_step=51077, preemption_count=0, score=17668.511253, test/accuracy=0.676184, test/bleu=28.287539, test/loss=1.513534, test/num_examples=3003, total_duration=29197.242948, train/accuracy=0.650933, train/bleu=32.021527, train/loss=1.667337, validation/accuracy=0.663315, validation/bleu=28.656745, validation/loss=1.580825, validation/num_examples=3000
I0208 18:52:29.875093 140051013605120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3706373870372772, loss=1.7451814413070679
I0208 18:53:04.247367 140051005212416 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.44600334763526917, loss=1.7969366312026978
I0208 18:53:38.729532 140051013605120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.40095534920692444, loss=1.7437822818756104
I0208 18:54:13.237523 140051005212416 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.39143019914627075, loss=1.7099627256393433
I0208 18:54:47.756824 140051013605120 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.428751140832901, loss=1.700186848640442
I0208 18:55:22.278636 140051005212416 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.35987040400505066, loss=1.7370684146881104
I0208 18:55:56.840353 140051013605120 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.40847277641296387, loss=1.7003955841064453
I0208 18:56:31.367956 140051005212416 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.39045068621635437, loss=1.8171114921569824
I0208 18:57:05.901144 140051013605120 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.39864200353622437, loss=1.671197533607483
I0208 18:57:40.415991 140051005212416 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.35666072368621826, loss=1.6408005952835083
I0208 18:58:14.944630 140051013605120 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.35280662775039673, loss=1.743177056312561
I0208 18:58:49.484678 140051005212416 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.36842358112335205, loss=1.7388105392456055
I0208 18:59:24.031997 140051013605120 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.37272217869758606, loss=1.7010722160339355
I0208 18:59:58.551607 140051005212416 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.3905912935733795, loss=1.7524515390396118
I0208 19:00:33.105969 140051013605120 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.36854931712150574, loss=1.7069737911224365
I0208 19:01:07.650493 140051005212416 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.3949255645275116, loss=1.6834603548049927
I0208 19:01:42.182806 140051013605120 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3970654308795929, loss=1.71486496925354
I0208 19:02:16.744978 140051005212416 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.390607625246048, loss=1.7768787145614624
I0208 19:02:51.304324 140051013605120 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.40285634994506836, loss=1.776582956314087
I0208 19:03:25.834523 140051005212416 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3887926936149597, loss=1.7439738512039185
I0208 19:04:00.356603 140051013605120 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.35172832012176514, loss=1.7212435007095337
I0208 19:04:34.894690 140051005212416 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.3708694875240326, loss=1.7006117105484009
I0208 19:05:09.434782 140051013605120 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3943859338760376, loss=1.7128206491470337
I0208 19:05:43.981455 140051005212416 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.40632179379463196, loss=1.7034056186676025
I0208 19:06:18.508548 140051013605120 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3907177746295929, loss=1.7554150819778442
I0208 19:06:21.691501 140225696298816 spec.py:321] Evaluating on the training split.
I0208 19:06:24.667043 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:09:59.811328 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 19:10:02.485264 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:12:52.507686 140225696298816 spec.py:349] Evaluating on the test split.
I0208 19:12:55.190175 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:15:46.785693 140225696298816 submission_runner.py:408] Time since start: 30602.46s, 	Step: 53511, 	{'train/accuracy': 0.6447697877883911, 'train/loss': 1.7171337604522705, 'train/bleu': 31.75867017336076, 'validation/accuracy': 0.6656085848808289, 'validation/loss': 1.5713614225387573, 'validation/bleu': 28.81990449034723, 'validation/num_examples': 3000, 'test/accuracy': 0.6800999641418457, 'test/loss': 1.4995527267456055, 'test/bleu': 28.593444733008027, 'test/num_examples': 3003, 'score': 18508.527975320816, 'total_duration': 30602.46071600914, 'accumulated_submission_time': 18508.527975320816, 'accumulated_eval_time': 12091.620062828064, 'accumulated_logging_time': 0.637232780456543}
I0208 19:15:46.808929 140051005212416 logging_writer.py:48] [53511] accumulated_eval_time=12091.620063, accumulated_logging_time=0.637233, accumulated_submission_time=18508.527975, global_step=53511, preemption_count=0, score=18508.527975, test/accuracy=0.680100, test/bleu=28.593445, test/loss=1.499553, test/num_examples=3003, total_duration=30602.460716, train/accuracy=0.644770, train/bleu=31.758670, train/loss=1.717134, validation/accuracy=0.665609, validation/bleu=28.819904, validation/loss=1.571361, validation/num_examples=3000
I0208 19:16:17.737310 140051013605120 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.37818410992622375, loss=1.7821049690246582
I0208 19:16:52.211998 140051005212416 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.49916160106658936, loss=1.8011218309402466
I0208 19:17:26.774317 140051013605120 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.49327078461647034, loss=1.7308897972106934
I0208 19:18:01.294632 140051005212416 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.4102552533149719, loss=1.675624132156372
I0208 19:18:35.835698 140051013605120 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.4214799106121063, loss=1.7202740907669067
I0208 19:19:10.383776 140051005212416 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.39784175157546997, loss=1.6460554599761963
I0208 19:19:44.910005 140051013605120 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.3940252959728241, loss=1.7182551622390747
I0208 19:20:19.457944 140051005212416 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.47036123275756836, loss=1.7075161933898926
I0208 19:20:54.008990 140051013605120 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3912680149078369, loss=1.6841466426849365
I0208 19:21:28.559452 140051005212416 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.38430577516555786, loss=1.6724238395690918
I0208 19:22:03.110399 140051013605120 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.4014732837677002, loss=1.7577284574508667
I0208 19:22:37.669786 140051005212416 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.36701324582099915, loss=1.7185455560684204
I0208 19:23:12.211336 140051013605120 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.39878910779953003, loss=1.7155345678329468
I0208 19:23:46.759423 140051005212416 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.42755746841430664, loss=1.7711478471755981
I0208 19:24:21.299766 140051013605120 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.39016029238700867, loss=1.6927388906478882
I0208 19:24:55.848363 140051005212416 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.39997074007987976, loss=1.7898212671279907
I0208 19:25:30.375580 140051013605120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.41080591082572937, loss=1.733154058456421
I0208 19:26:04.901243 140051005212416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.37610501050949097, loss=1.7204747200012207
I0208 19:26:39.442245 140051013605120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.37909048795700073, loss=1.6835147142410278
I0208 19:27:13.995443 140051005212416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.39918139576911926, loss=1.6956087350845337
I0208 19:27:48.530032 140051013605120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.4106324017047882, loss=1.7377715110778809
I0208 19:28:23.076504 140051005212416 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.39290985465049744, loss=1.6982684135437012
I0208 19:28:57.632658 140051013605120 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.39121732115745544, loss=1.7694505453109741
I0208 19:29:32.150499 140051005212416 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.4072457551956177, loss=1.7190340757369995
I0208 19:29:47.086530 140225696298816 spec.py:321] Evaluating on the training split.
I0208 19:29:50.057157 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:33:34.780057 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 19:33:37.456107 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:37:46.086734 140225696298816 spec.py:349] Evaluating on the test split.
I0208 19:37:48.768761 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:40:56.088261 140225696298816 submission_runner.py:408] Time since start: 32111.76s, 	Step: 55945, 	{'train/accuracy': 0.6402867436408997, 'train/loss': 1.7420943975448608, 'train/bleu': 31.603690698245167, 'validation/accuracy': 0.6657822132110596, 'validation/loss': 1.5687559843063354, 'validation/bleu': 28.31823448140098, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.496812343597412, 'test/bleu': 28.487429038274655, 'test/num_examples': 3003, 'score': 19348.719173908234, 'total_duration': 32111.7632894516, 'accumulated_submission_time': 19348.719173908234, 'accumulated_eval_time': 12760.62174320221, 'accumulated_logging_time': 0.6705992221832275}
I0208 19:40:56.111146 140051013605120 logging_writer.py:48] [55945] accumulated_eval_time=12760.621743, accumulated_logging_time=0.670599, accumulated_submission_time=19348.719174, global_step=55945, preemption_count=0, score=19348.719174, test/accuracy=0.678345, test/bleu=28.487429, test/loss=1.496812, test/num_examples=3003, total_duration=32111.763289, train/accuracy=0.640287, train/bleu=31.603691, train/loss=1.742094, validation/accuracy=0.665782, validation/bleu=28.318234, validation/loss=1.568756, validation/num_examples=3000
I0208 19:41:15.357586 140051005212416 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.4123341143131256, loss=1.7509623765945435
I0208 19:41:49.777175 140051013605120 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3810272216796875, loss=1.59818696975708
I0208 19:42:24.310383 140051005212416 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.42390382289886475, loss=1.6891868114471436
I0208 19:42:58.825212 140051013605120 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.36714115738868713, loss=1.7665925025939941
I0208 19:43:33.361713 140051005212416 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.39505481719970703, loss=1.7029486894607544
I0208 19:44:07.908604 140051013605120 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3677891492843628, loss=1.72769033908844
I0208 19:44:42.444402 140051005212416 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3585263788700104, loss=1.6395679712295532
I0208 19:45:16.960608 140051013605120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.4049921929836273, loss=1.754460334777832
I0208 19:45:51.515366 140051005212416 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.39890381693840027, loss=1.7169744968414307
I0208 19:46:26.043489 140051013605120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.4015001058578491, loss=1.7171788215637207
I0208 19:47:00.589375 140051005212416 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.36894601583480835, loss=1.6829631328582764
I0208 19:47:35.137023 140051013605120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.39391183853149414, loss=1.803442120552063
I0208 19:48:09.687605 140051005212416 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.40780067443847656, loss=1.6439030170440674
I0208 19:48:44.227083 140051013605120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3937338590621948, loss=1.7274665832519531
I0208 19:49:18.793541 140051005212416 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3728857636451721, loss=1.7039257287979126
I0208 19:49:53.298658 140051013605120 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3859069347381592, loss=1.752413034439087
I0208 19:50:27.874666 140051005212416 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.41752558946609497, loss=1.7209317684173584
I0208 19:51:02.405379 140051013605120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.36162111163139343, loss=1.6619361639022827
I0208 19:51:36.959335 140051005212416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.4056761562824249, loss=1.6641985177993774
I0208 19:52:11.496387 140051013605120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.38250112533569336, loss=1.6900712251663208
I0208 19:52:46.050041 140051005212416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4254287779331207, loss=1.6469993591308594
I0208 19:53:20.579814 140051013605120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.37166786193847656, loss=1.7232969999313354
I0208 19:53:55.144523 140051005212416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3883284628391266, loss=1.6730973720550537
I0208 19:54:29.664080 140051013605120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.37860405445098877, loss=1.6820013523101807
I0208 19:54:56.314598 140225696298816 spec.py:321] Evaluating on the training split.
I0208 19:54:59.282253 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 19:58:06.465382 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 19:58:09.143013 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:00:43.183538 140225696298816 spec.py:349] Evaluating on the test split.
I0208 20:00:45.869522 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:03:16.574528 140225696298816 submission_runner.py:408] Time since start: 33452.25s, 	Step: 58379, 	{'train/accuracy': 0.651101291179657, 'train/loss': 1.6768395900726318, 'train/bleu': 31.862773994581925, 'validation/accuracy': 0.6660177707672119, 'validation/loss': 1.565895915031433, 'validation/bleu': 28.684740364153395, 'validation/num_examples': 3000, 'test/accuracy': 0.6788333058357239, 'test/loss': 1.4934437274932861, 'test/bleu': 28.461223409180462, 'test/num_examples': 3003, 'score': 20188.837403059006, 'total_duration': 33452.249553442, 'accumulated_submission_time': 20188.837403059006, 'accumulated_eval_time': 13260.881618976593, 'accumulated_logging_time': 0.7030997276306152}
I0208 20:03:16.597176 140051005212416 logging_writer.py:48] [58379] accumulated_eval_time=13260.881619, accumulated_logging_time=0.703100, accumulated_submission_time=20188.837403, global_step=58379, preemption_count=0, score=20188.837403, test/accuracy=0.678833, test/bleu=28.461223, test/loss=1.493444, test/num_examples=3003, total_duration=33452.249553, train/accuracy=0.651101, train/bleu=31.862774, train/loss=1.676840, validation/accuracy=0.666018, validation/bleu=28.684740, validation/loss=1.565896, validation/num_examples=3000
I0208 20:03:24.178778 140051013605120 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3845807611942291, loss=1.6902724504470825
I0208 20:03:58.557242 140051005212416 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.39078986644744873, loss=1.7459495067596436
I0208 20:04:33.032563 140051013605120 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3852882981300354, loss=1.719284176826477
I0208 20:05:07.527877 140051005212416 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3910537660121918, loss=1.7330893278121948
I0208 20:05:42.057316 140051013605120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3973429501056671, loss=1.745240330696106
I0208 20:06:16.607026 140051005212416 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3936004340648651, loss=1.6949989795684814
I0208 20:06:51.141870 140051013605120 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.398613303899765, loss=1.631365418434143
I0208 20:07:25.685757 140051005212416 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.3752092719078064, loss=1.73362398147583
I0208 20:08:00.227353 140051013605120 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.39916321635246277, loss=1.7168338298797607
I0208 20:08:34.769469 140051005212416 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.37416765093803406, loss=1.8186018466949463
I0208 20:09:09.325709 140051013605120 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.37864258885383606, loss=1.7033885717391968
I0208 20:09:43.871045 140051005212416 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3846648335456848, loss=1.698067545890808
I0208 20:10:18.425953 140051013605120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.35916629433631897, loss=1.6389905214309692
I0208 20:10:52.994458 140051005212416 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.4365476667881012, loss=1.7284486293792725
I0208 20:11:27.515826 140051013605120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.39585012197494507, loss=1.6756417751312256
I0208 20:12:02.060954 140051005212416 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.4857758581638336, loss=1.6500216722488403
I0208 20:12:36.611089 140051013605120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.42025795578956604, loss=1.687849521636963
I0208 20:13:11.131156 140051005212416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3980327546596527, loss=1.728919267654419
I0208 20:13:45.674918 140051013605120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.4009626805782318, loss=1.6848918199539185
I0208 20:14:20.214674 140051005212416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.37865886092185974, loss=1.7413389682769775
I0208 20:14:54.776238 140051013605120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.378337562084198, loss=1.7467674016952515
I0208 20:15:29.324028 140051005212416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.39918458461761475, loss=1.721506953239441
I0208 20:16:03.881660 140051013605120 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.3748057782649994, loss=1.7063016891479492
I0208 20:16:38.415038 140051005212416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.40317586064338684, loss=1.6484321355819702
I0208 20:17:12.938144 140051013605120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.40683743357658386, loss=1.7975448369979858
I0208 20:17:16.804565 140225696298816 spec.py:321] Evaluating on the training split.
I0208 20:17:19.776670 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:20:25.212605 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 20:20:27.893914 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:22:57.893318 140225696298816 spec.py:349] Evaluating on the test split.
I0208 20:23:00.577447 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:25:25.869255 140225696298816 submission_runner.py:408] Time since start: 34781.54s, 	Step: 60813, 	{'train/accuracy': 0.6452680826187134, 'train/loss': 1.7102110385894775, 'train/bleu': 31.57398436310127, 'validation/accuracy': 0.6664021611213684, 'validation/loss': 1.5667551755905151, 'validation/bleu': 28.685950258944292, 'validation/num_examples': 3000, 'test/accuracy': 0.6794724464416504, 'test/loss': 1.485634207725525, 'test/bleu': 28.543103089635263, 'test/num_examples': 3003, 'score': 21028.95820069313, 'total_duration': 34781.54425239563, 'accumulated_submission_time': 21028.95820069313, 'accumulated_eval_time': 13749.946232795715, 'accumulated_logging_time': 0.7364275455474854}
I0208 20:25:25.892274 140051005212416 logging_writer.py:48] [60813] accumulated_eval_time=13749.946233, accumulated_logging_time=0.736428, accumulated_submission_time=21028.958201, global_step=60813, preemption_count=0, score=21028.958201, test/accuracy=0.679472, test/bleu=28.543103, test/loss=1.485634, test/num_examples=3003, total_duration=34781.544252, train/accuracy=0.645268, train/bleu=31.573984, train/loss=1.710211, validation/accuracy=0.666402, validation/bleu=28.685950, validation/loss=1.566755, validation/num_examples=3000
I0208 20:25:56.178070 140051013605120 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.375257283449173, loss=1.727989673614502
I0208 20:26:30.656155 140051005212416 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.37842419743537903, loss=1.7207164764404297
I0208 20:27:05.162451 140051013605120 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.41125020384788513, loss=1.683742880821228
I0208 20:27:39.698876 140051005212416 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.38444414734840393, loss=1.6943904161453247
I0208 20:28:14.243776 140051013605120 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.38159945607185364, loss=1.7229615449905396
I0208 20:28:48.769763 140051005212416 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.3943408727645874, loss=1.6359939575195312
I0208 20:29:23.343380 140051013605120 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.44386741518974304, loss=1.6408107280731201
I0208 20:29:57.905153 140051005212416 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.3859785497188568, loss=1.6988111734390259
I0208 20:30:32.447637 140051013605120 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3993547856807709, loss=1.6688381433486938
I0208 20:31:06.980285 140051005212416 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.3522983193397522, loss=1.7303208112716675
I0208 20:31:41.519435 140051013605120 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.4132421016693115, loss=1.73564875125885
I0208 20:32:16.087749 140051005212416 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.4608328342437744, loss=1.7458769083023071
I0208 20:32:50.652353 140051013605120 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0952647924423218, loss=1.6468197107315063
I0208 20:33:25.187134 140051005212416 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.41600680351257324, loss=1.730098009109497
I0208 20:33:59.709151 140051013605120 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.37497222423553467, loss=1.7143139839172363
I0208 20:34:34.247334 140051005212416 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3754635751247406, loss=1.6638065576553345
I0208 20:35:08.808065 140051013605120 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3759033679962158, loss=1.6779320240020752
I0208 20:35:43.348542 140051005212416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.4159347712993622, loss=1.6440236568450928
I0208 20:36:17.896451 140051013605120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3750804364681244, loss=1.6887404918670654
I0208 20:36:52.442759 140051005212416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3942687213420868, loss=1.7755805253982544
I0208 20:37:26.958368 140051013605120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.39406532049179077, loss=1.7226483821868896
I0208 20:38:01.477817 140051005212416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.39396435022354126, loss=1.728822112083435
I0208 20:38:36.017701 140051013605120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3692065179347992, loss=1.6763477325439453
I0208 20:39:10.536050 140051005212416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.4031480848789215, loss=1.6377589702606201
I0208 20:39:26.154113 140225696298816 spec.py:321] Evaluating on the training split.
I0208 20:39:29.127616 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:42:27.281579 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 20:42:29.972587 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:45:05.145092 140225696298816 spec.py:349] Evaluating on the test split.
I0208 20:45:07.825831 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 20:47:35.333891 140225696298816 submission_runner.py:408] Time since start: 36111.01s, 	Step: 63247, 	{'train/accuracy': 0.6609110236167908, 'train/loss': 1.613291621208191, 'train/bleu': 32.737365321346005, 'validation/accuracy': 0.6695019006729126, 'validation/loss': 1.5515433549880981, 'validation/bleu': 29.214455853774165, 'validation/num_examples': 3000, 'test/accuracy': 0.6820870637893677, 'test/loss': 1.4776993989944458, 'test/bleu': 28.981448713141738, 'test/num_examples': 3003, 'score': 21869.133969783783, 'total_duration': 36111.008915662766, 'accumulated_submission_time': 21869.133969783783, 'accumulated_eval_time': 14239.12595629692, 'accumulated_logging_time': 0.7697396278381348}
I0208 20:47:35.357456 140051013605120 logging_writer.py:48] [63247] accumulated_eval_time=14239.125956, accumulated_logging_time=0.769740, accumulated_submission_time=21869.133970, global_step=63247, preemption_count=0, score=21869.133970, test/accuracy=0.682087, test/bleu=28.981449, test/loss=1.477699, test/num_examples=3003, total_duration=36111.008916, train/accuracy=0.660911, train/bleu=32.737365, train/loss=1.613292, validation/accuracy=0.669502, validation/bleu=29.214456, validation/loss=1.551543, validation/num_examples=3000
I0208 20:47:53.954726 140051005212416 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.4158843457698822, loss=1.6514825820922852
I0208 20:48:28.387716 140051013605120 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.38491109013557434, loss=1.7296596765518188
I0208 20:49:02.899469 140051005212416 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.37488532066345215, loss=1.7261370420455933
I0208 20:49:37.432223 140051013605120 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.4071093499660492, loss=1.7116830348968506
I0208 20:50:11.961460 140051005212416 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.37280628085136414, loss=1.6447075605392456
I0208 20:50:46.479044 140051013605120 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.38431334495544434, loss=1.6946866512298584
I0208 20:51:21.017469 140051005212416 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.36625853180885315, loss=1.6940639019012451
I0208 20:51:55.536288 140051013605120 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.37099793553352356, loss=1.7166842222213745
I0208 20:52:30.096300 140051005212416 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.4222659170627594, loss=1.6856731176376343
I0208 20:53:04.644798 140051013605120 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3865101933479309, loss=1.7265690565109253
I0208 20:53:39.163936 140051005212416 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3799259066581726, loss=1.7195698022842407
I0208 20:54:13.676213 140051013605120 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.39979028701782227, loss=1.7289990186691284
I0208 20:54:48.222646 140051005212416 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.38985323905944824, loss=1.7474029064178467
I0208 20:55:22.764503 140051013605120 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.4058743417263031, loss=1.7101762294769287
I0208 20:55:57.307052 140051005212416 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.46212196350097656, loss=1.7267073392868042
I0208 20:56:31.840212 140051013605120 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3819572627544403, loss=1.6890218257904053
I0208 20:57:06.374863 140051005212416 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.40765804052352905, loss=1.669433355331421
I0208 20:57:40.903472 140051013605120 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3777741491794586, loss=1.6784520149230957
I0208 20:58:15.420869 140051005212416 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3685281574726105, loss=1.65670907497406
I0208 20:58:49.974647 140051013605120 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3604109287261963, loss=1.7080541849136353
I0208 20:59:24.516715 140051005212416 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.3867852985858917, loss=1.7019239664077759
I0208 20:59:59.065734 140051013605120 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.38936886191368103, loss=1.7356537580490112
I0208 21:00:33.603109 140051005212416 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.410893052816391, loss=1.7067041397094727
I0208 21:01:08.143590 140051013605120 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3773869574069977, loss=1.7038904428482056
I0208 21:01:35.481485 140225696298816 spec.py:321] Evaluating on the training split.
I0208 21:01:38.448197 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:05:40.936056 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 21:05:43.614788 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:08:51.315391 140225696298816 spec.py:349] Evaluating on the test split.
I0208 21:08:53.988699 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:11:45.933315 140225696298816 submission_runner.py:408] Time since start: 37561.61s, 	Step: 65681, 	{'train/accuracy': 0.6530311703681946, 'train/loss': 1.671697735786438, 'train/bleu': 32.311872612330355, 'validation/accuracy': 0.6706550121307373, 'validation/loss': 1.538474202156067, 'validation/bleu': 28.821102985125254, 'validation/num_examples': 3000, 'test/accuracy': 0.6832723617553711, 'test/loss': 1.465322732925415, 'test/bleu': 28.777813891257313, 'test/num_examples': 3003, 'score': 22709.169924020767, 'total_duration': 37561.60833859444, 'accumulated_submission_time': 22709.169924020767, 'accumulated_eval_time': 14849.57773900032, 'accumulated_logging_time': 0.8056454658508301}
I0208 21:11:45.957859 140051005212416 logging_writer.py:48] [65681] accumulated_eval_time=14849.577739, accumulated_logging_time=0.805645, accumulated_submission_time=22709.169924, global_step=65681, preemption_count=0, score=22709.169924, test/accuracy=0.683272, test/bleu=28.777814, test/loss=1.465323, test/num_examples=3003, total_duration=37561.608339, train/accuracy=0.653031, train/bleu=32.311873, train/loss=1.671698, validation/accuracy=0.670655, validation/bleu=28.821103, validation/loss=1.538474, validation/num_examples=3000
I0208 21:11:52.856883 140051013605120 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.3852907121181488, loss=1.6331675052642822
I0208 21:12:27.230229 140051005212416 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.37475597858428955, loss=1.7226958274841309
I0208 21:13:01.822255 140051013605120 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.38201841711997986, loss=1.6819595098495483
I0208 21:13:36.345123 140051005212416 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.40139472484588623, loss=1.6944857835769653
I0208 21:14:10.854029 140051013605120 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3691486120223999, loss=1.6385838985443115
I0208 21:14:45.385681 140051005212416 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.4061927795410156, loss=1.64054274559021
I0208 21:15:19.935616 140051013605120 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3873859941959381, loss=1.6137555837631226
I0208 21:15:54.468989 140051005212416 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3549425005912781, loss=1.5946440696716309
I0208 21:16:28.990430 140051013605120 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.4730314314365387, loss=1.687872052192688
I0208 21:17:03.524043 140051005212416 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.4151129424571991, loss=1.7619303464889526
I0208 21:17:38.048435 140051013605120 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.42976638674736023, loss=1.6532351970672607
I0208 21:18:12.598951 140051005212416 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3998037874698639, loss=1.68575119972229
I0208 21:18:47.134078 140051013605120 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3786466717720032, loss=1.6344999074935913
I0208 21:19:21.659157 140051005212416 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.4162524938583374, loss=1.741899013519287
I0208 21:19:56.197739 140051013605120 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.36261963844299316, loss=1.7085967063903809
I0208 21:20:30.744528 140051005212416 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.3986362814903259, loss=1.6477395296096802
I0208 21:21:05.296835 140051013605120 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3862908184528351, loss=1.5942212343215942
I0208 21:21:39.838585 140051005212416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3946360647678375, loss=1.714786171913147
I0208 21:22:14.382413 140051013605120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.4056662917137146, loss=1.5919381380081177
I0208 21:22:48.938253 140051005212416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.382720947265625, loss=1.5594544410705566
I0208 21:23:23.464134 140051013605120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.4022256135940552, loss=1.6645194292068481
I0208 21:23:58.009034 140051005212416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3910812735557556, loss=1.6894972324371338
I0208 21:24:32.564213 140051013605120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.4610256850719452, loss=1.7308927774429321
I0208 21:25:07.135480 140051005212416 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3926336467266083, loss=1.6235253810882568
I0208 21:25:41.663999 140051013605120 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.4245803952217102, loss=1.5936484336853027
I0208 21:25:46.213136 140225696298816 spec.py:321] Evaluating on the training split.
I0208 21:25:49.182460 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:29:44.618768 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 21:29:47.299139 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:32:51.757375 140225696298816 spec.py:349] Evaluating on the test split.
I0208 21:32:54.441090 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:35:15.711270 140225696298816 submission_runner.py:408] Time since start: 38971.39s, 	Step: 68115, 	{'train/accuracy': 0.6517167091369629, 'train/loss': 1.6799516677856445, 'train/bleu': 32.40043379300699, 'validation/accuracy': 0.670059859752655, 'validation/loss': 1.5349482297897339, 'validation/bleu': 28.841342669109036, 'validation/num_examples': 3000, 'test/accuracy': 0.6844460368156433, 'test/loss': 1.4604904651641846, 'test/bleu': 28.78482850239207, 'test/num_examples': 3003, 'score': 23549.336899280548, 'total_duration': 38971.38629126549, 'accumulated_submission_time': 23549.336899280548, 'accumulated_eval_time': 15419.075818538666, 'accumulated_logging_time': 0.8422815799713135}
I0208 21:35:15.736794 140051005212416 logging_writer.py:48] [68115] accumulated_eval_time=15419.075819, accumulated_logging_time=0.842282, accumulated_submission_time=23549.336899, global_step=68115, preemption_count=0, score=23549.336899, test/accuracy=0.684446, test/bleu=28.784829, test/loss=1.460490, test/num_examples=3003, total_duration=38971.386291, train/accuracy=0.651717, train/bleu=32.400434, train/loss=1.679952, validation/accuracy=0.670060, validation/bleu=28.841343, validation/loss=1.534948, validation/num_examples=3000
I0208 21:35:45.318603 140051013605120 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.40661001205444336, loss=1.7417997121810913
I0208 21:36:19.805686 140051005212416 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3772147297859192, loss=1.72958505153656
I0208 21:36:54.336213 140051013605120 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.36152055859565735, loss=1.69955313205719
I0208 21:37:28.880450 140051005212416 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.41150298714637756, loss=1.6837167739868164
I0208 21:38:03.426824 140051013605120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.4230608642101288, loss=1.7239609956741333
I0208 21:38:37.956723 140051005212416 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.4052584767341614, loss=1.6755836009979248
I0208 21:39:12.500273 140051013605120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.38237902522087097, loss=1.7424284219741821
I0208 21:39:47.033983 140051005212416 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.370116263628006, loss=1.6563142538070679
I0208 21:40:21.592683 140051013605120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4106407165527344, loss=1.6769169569015503
I0208 21:40:56.133260 140051005212416 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.38664907217025757, loss=1.7101444005966187
I0208 21:41:30.693419 140051013605120 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.4004266858100891, loss=1.6989716291427612
I0208 21:42:05.225930 140051005212416 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.37928372621536255, loss=1.6032991409301758
I0208 21:42:39.786449 140051013605120 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.36140528321266174, loss=1.6251338720321655
I0208 21:43:14.305553 140051005212416 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.4517100751399994, loss=1.6260501146316528
I0208 21:43:48.847954 140051013605120 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.37939006090164185, loss=1.6623624563217163
I0208 21:44:23.395331 140051005212416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3878869414329529, loss=1.6851327419281006
I0208 21:44:57.955338 140051013605120 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.4126162827014923, loss=1.5761277675628662
I0208 21:45:32.499009 140051005212416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3753400444984436, loss=1.6380695104599
I0208 21:46:07.055914 140051013605120 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.39567694067955017, loss=1.6062370538711548
I0208 21:46:41.605438 140051005212416 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.41800814867019653, loss=1.642362356185913
I0208 21:47:16.127951 140051013605120 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4081207513809204, loss=1.6069015264511108
I0208 21:47:50.652091 140051005212416 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.40103164315223694, loss=1.5834258794784546
I0208 21:48:25.203655 140051013605120 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.426311194896698, loss=1.6676051616668701
I0208 21:48:59.753646 140051005212416 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.404201865196228, loss=1.6537981033325195
I0208 21:49:16.051225 140225696298816 spec.py:321] Evaluating on the training split.
I0208 21:49:19.030948 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:51:58.953566 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 21:52:01.648452 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:54:30.434182 140225696298816 spec.py:349] Evaluating on the test split.
I0208 21:54:33.118009 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 21:56:52.878879 140225696298816 submission_runner.py:408] Time since start: 40268.55s, 	Step: 70549, 	{'train/accuracy': 0.6591607332229614, 'train/loss': 1.6216074228286743, 'train/bleu': 32.18171612106351, 'validation/accuracy': 0.6709278225898743, 'validation/loss': 1.5275872945785522, 'validation/bleu': 28.927073459030193, 'validation/num_examples': 3000, 'test/accuracy': 0.6852594614028931, 'test/loss': 1.4523561000823975, 'test/bleu': 28.99412723224184, 'test/num_examples': 3003, 'score': 24389.565573453903, 'total_duration': 40268.55391001701, 'accumulated_submission_time': 24389.565573453903, 'accumulated_eval_time': 15875.903427124023, 'accumulated_logging_time': 0.8780360221862793}
I0208 21:56:52.903820 140051013605120 logging_writer.py:48] [70549] accumulated_eval_time=15875.903427, accumulated_logging_time=0.878036, accumulated_submission_time=24389.565573, global_step=70549, preemption_count=0, score=24389.565573, test/accuracy=0.685259, test/bleu=28.994127, test/loss=1.452356, test/num_examples=3003, total_duration=40268.553910, train/accuracy=0.659161, train/bleu=32.181716, train/loss=1.621607, validation/accuracy=0.670928, validation/bleu=28.927073, validation/loss=1.527587, validation/num_examples=3000
I0208 21:57:10.805310 140051005212416 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.40976089239120483, loss=1.7080720663070679
I0208 21:57:45.244476 140051013605120 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.38260114192962646, loss=1.6066445112228394
I0208 21:58:19.723924 140051005212416 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.42454931139945984, loss=1.7259635925292969
I0208 21:58:54.260079 140051013605120 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.38047489523887634, loss=1.6424098014831543
I0208 21:59:28.776708 140051005212416 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.4194333851337433, loss=1.737801194190979
I0208 22:00:03.296903 140051013605120 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.39324718713760376, loss=1.6810071468353271
I0208 22:00:37.823372 140051005212416 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.38736701011657715, loss=1.6601094007492065
I0208 22:01:12.364544 140051013605120 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.39416760206222534, loss=1.700423240661621
I0208 22:01:46.911011 140051005212416 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.41237911581993103, loss=1.6897785663604736
I0208 22:02:21.429889 140051013605120 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.41574254631996155, loss=1.714137315750122
I0208 22:02:56.003819 140051005212416 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.4207751750946045, loss=1.6314609050750732
I0208 22:03:30.529429 140051013605120 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.4708055257797241, loss=1.702300786972046
I0208 22:04:05.083943 140051005212416 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.40852466225624084, loss=1.6879578828811646
I0208 22:04:39.641240 140051013605120 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3703480362892151, loss=1.6289894580841064
I0208 22:05:14.201289 140051005212416 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3860861361026764, loss=1.675918698310852
I0208 22:05:48.722557 140051013605120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.428152859210968, loss=1.6041982173919678
I0208 22:06:23.220829 140051005212416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3871780037879944, loss=1.6971379518508911
I0208 22:06:57.728728 140051013605120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3725471496582031, loss=1.6299936771392822
I0208 22:07:32.279540 140051005212416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.4062349498271942, loss=1.6286407709121704
I0208 22:08:06.806757 140051013605120 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.38317397236824036, loss=1.6161894798278809
I0208 22:08:41.338515 140051005212416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3978881239891052, loss=1.6022083759307861
I0208 22:09:15.887745 140051013605120 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3742055296897888, loss=1.5544830560684204
I0208 22:09:50.433429 140051005212416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.4019670784473419, loss=1.6663910150527954
I0208 22:10:24.965895 140051013605120 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.37048250436782837, loss=1.5494334697723389
I0208 22:10:53.006268 140225696298816 spec.py:321] Evaluating on the training split.
I0208 22:10:55.984426 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:13:33.894210 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 22:13:36.575623 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:16:09.570481 140225696298816 spec.py:349] Evaluating on the test split.
I0208 22:16:12.253687 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:18:37.865325 140225696298816 submission_runner.py:408] Time since start: 41573.54s, 	Step: 72983, 	{'train/accuracy': 0.6545958518981934, 'train/loss': 1.6556434631347656, 'train/bleu': 32.03680376481512, 'validation/accuracy': 0.6736928224563599, 'validation/loss': 1.5203146934509277, 'validation/bleu': 29.4972412252912, 'validation/num_examples': 3000, 'test/accuracy': 0.6866422891616821, 'test/loss': 1.4460179805755615, 'test/bleu': 28.961118007307395, 'test/num_examples': 3003, 'score': 25229.58171772957, 'total_duration': 41573.54033780098, 'accumulated_submission_time': 25229.58171772957, 'accumulated_eval_time': 16340.762417078018, 'accumulated_logging_time': 0.9139454364776611}
I0208 22:18:37.891091 140051005212416 logging_writer.py:48] [72983] accumulated_eval_time=16340.762417, accumulated_logging_time=0.913945, accumulated_submission_time=25229.581718, global_step=72983, preemption_count=0, score=25229.581718, test/accuracy=0.686642, test/bleu=28.961118, test/loss=1.446018, test/num_examples=3003, total_duration=41573.540338, train/accuracy=0.654596, train/bleu=32.036804, train/loss=1.655643, validation/accuracy=0.673693, validation/bleu=29.497241, validation/loss=1.520315, validation/num_examples=3000
I0208 22:18:44.081756 140051013605120 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.36748284101486206, loss=1.6907958984375
I0208 22:19:18.451137 140051005212416 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3917876183986664, loss=1.5854568481445312
I0208 22:19:52.958398 140051013605120 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.4200732409954071, loss=1.624582290649414
I0208 22:20:27.489523 140051005212416 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.45327264070510864, loss=1.607481598854065
I0208 22:21:02.017091 140051013605120 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.39908644556999207, loss=1.6501177549362183
I0208 22:21:36.567048 140051005212416 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.40045827627182007, loss=1.589774489402771
I0208 22:22:11.125031 140051013605120 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.41249287128448486, loss=1.631836175918579
I0208 22:22:45.667102 140051005212416 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3547731935977936, loss=1.6598384380340576
I0208 22:23:20.204765 140051013605120 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.39775142073631287, loss=1.661361575126648
I0208 22:23:54.751025 140051005212416 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.4340372383594513, loss=1.606279969215393
I0208 22:24:29.308186 140051013605120 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.4073539972305298, loss=1.6421544551849365
I0208 22:25:03.872080 140051005212416 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.4565836191177368, loss=1.6064882278442383
I0208 22:25:38.437831 140051013605120 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.41995954513549805, loss=1.684935450553894
I0208 22:26:12.972546 140051005212416 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.38883405923843384, loss=1.5969221591949463
I0208 22:26:47.528922 140051013605120 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.42108285427093506, loss=1.6591551303863525
I0208 22:27:22.078515 140051005212416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.39290693402290344, loss=1.6955301761627197
I0208 22:27:56.613571 140051013605120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.38944244384765625, loss=1.6113817691802979
I0208 22:28:31.146129 140051005212416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.4064302146434784, loss=1.6072754859924316
I0208 22:29:05.668203 140051013605120 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.3929290175437927, loss=1.6053928136825562
I0208 22:29:40.200565 140051005212416 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.38096147775650024, loss=1.6172341108322144
I0208 22:30:14.755093 140051013605120 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.40993624925613403, loss=1.6426516771316528
I0208 22:30:49.337115 140051005212416 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3799501359462738, loss=1.6455190181732178
I0208 22:31:23.866796 140051013605120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.40055835247039795, loss=1.6897159814834595
I0208 22:31:58.390343 140051005212416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.41909846663475037, loss=1.5497478246688843
I0208 22:32:32.952328 140051013605120 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3802310526371002, loss=1.5845221281051636
I0208 22:32:38.208582 140225696298816 spec.py:321] Evaluating on the training split.
I0208 22:32:41.179096 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:35:38.207013 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 22:35:40.889768 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:38:25.114450 140225696298816 spec.py:349] Evaluating on the test split.
I0208 22:38:27.788570 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:41:10.085643 140225696298816 submission_runner.py:408] Time since start: 42925.76s, 	Step: 75417, 	{'train/accuracy': 0.6752561330795288, 'train/loss': 1.5263938903808594, 'train/bleu': 33.863299321673196, 'validation/accuracy': 0.6756518483161926, 'validation/loss': 1.5147626399993896, 'validation/bleu': 29.306960178991066, 'validation/num_examples': 3000, 'test/accuracy': 0.6866190433502197, 'test/loss': 1.439462661743164, 'test/bleu': 29.221898187490247, 'test/num_examples': 3003, 'score': 26069.81137084961, 'total_duration': 42925.760655641556, 'accumulated_submission_time': 26069.81137084961, 'accumulated_eval_time': 16852.63940834999, 'accumulated_logging_time': 0.9512898921966553}
I0208 22:41:10.113403 140051005212416 logging_writer.py:48] [75417] accumulated_eval_time=16852.639408, accumulated_logging_time=0.951290, accumulated_submission_time=26069.811371, global_step=75417, preemption_count=0, score=26069.811371, test/accuracy=0.686619, test/bleu=29.221898, test/loss=1.439463, test/num_examples=3003, total_duration=42925.760656, train/accuracy=0.675256, train/bleu=33.863299, train/loss=1.526394, validation/accuracy=0.675652, validation/bleu=29.306960, validation/loss=1.514763, validation/num_examples=3000
I0208 22:41:38.991325 140051013605120 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.396067351102829, loss=1.559965968132019
I0208 22:42:13.453573 140051005212416 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.393333375453949, loss=1.6021968126296997
I0208 22:42:47.986205 140051013605120 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.40306803584098816, loss=1.658503532409668
I0208 22:43:22.522531 140051005212416 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.4384269416332245, loss=1.7202181816101074
I0208 22:43:57.046186 140051013605120 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.37117999792099, loss=1.6837462186813354
I0208 22:44:31.568511 140051005212416 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.36089465022087097, loss=1.6599518060684204
I0208 22:45:06.098973 140051013605120 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3908660113811493, loss=1.6314966678619385
I0208 22:45:40.647997 140051005212416 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.41867226362228394, loss=1.647718071937561
I0208 22:46:15.198938 140051013605120 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.39830610156059265, loss=1.6321637630462646
I0208 22:46:49.744007 140051005212416 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3897410035133362, loss=1.6496870517730713
I0208 22:47:24.273189 140051013605120 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.384231299161911, loss=1.6490895748138428
I0208 22:47:58.820029 140051005212416 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3844141364097595, loss=1.5436997413635254
I0208 22:48:33.345565 140051013605120 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.5114338397979736, loss=1.7308650016784668
I0208 22:49:07.883240 140051005212416 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3915238082408905, loss=1.741613745689392
I0208 22:49:42.419403 140051013605120 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.390586793422699, loss=1.584830403327942
I0208 22:50:16.962523 140051005212416 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4420522153377533, loss=1.5561769008636475
I0208 22:50:51.499249 140051013605120 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.4310806393623352, loss=1.6549028158187866
I0208 22:51:26.023209 140051005212416 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.3792842626571655, loss=1.6072349548339844
I0208 22:52:00.517565 140051013605120 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.437546968460083, loss=1.6741141080856323
I0208 22:52:35.040647 140051005212416 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.43796858191490173, loss=1.6153655052185059
I0208 22:53:09.613068 140051013605120 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.4123169481754303, loss=1.6100066900253296
I0208 22:53:44.152384 140051005212416 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.405798077583313, loss=1.6306779384613037
I0208 22:54:18.686177 140051013605120 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3781796991825104, loss=1.545300006866455
I0208 22:54:53.240006 140051005212416 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3981987535953522, loss=1.6181663274765015
I0208 22:55:10.235008 140225696298816 spec.py:321] Evaluating on the training split.
I0208 22:55:13.217327 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 22:58:55.929985 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 22:58:58.607545 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:01:24.970406 140225696298816 spec.py:349] Evaluating on the test split.
I0208 23:01:27.654429 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:03:55.826432 140225696298816 submission_runner.py:408] Time since start: 44291.50s, 	Step: 77851, 	{'train/accuracy': 0.6602402329444885, 'train/loss': 1.6101831197738647, 'train/bleu': 32.94359332980648, 'validation/accuracy': 0.6758378744125366, 'validation/loss': 1.5022680759429932, 'validation/bleu': 29.88321465163858, 'validation/num_examples': 3000, 'test/accuracy': 0.6893149614334106, 'test/loss': 1.4282652139663696, 'test/bleu': 29.199608789390265, 'test/num_examples': 3003, 'score': 26909.846825361252, 'total_duration': 44291.501447439194, 'accumulated_submission_time': 26909.846825361252, 'accumulated_eval_time': 17378.230769634247, 'accumulated_logging_time': 0.9887261390686035}
I0208 23:03:55.853129 140051013605120 logging_writer.py:48] [77851] accumulated_eval_time=17378.230770, accumulated_logging_time=0.988726, accumulated_submission_time=26909.846825, global_step=77851, preemption_count=0, score=26909.846825, test/accuracy=0.689315, test/bleu=29.199609, test/loss=1.428265, test/num_examples=3003, total_duration=44291.501447, train/accuracy=0.660240, train/bleu=32.943593, train/loss=1.610183, validation/accuracy=0.675838, validation/bleu=29.883215, validation/loss=1.502268, validation/num_examples=3000
I0208 23:04:13.048247 140051005212416 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.42667633295059204, loss=1.647712230682373
I0208 23:04:47.478752 140051013605120 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.437368780374527, loss=1.7076772451400757
I0208 23:05:21.991529 140051005212416 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.391724169254303, loss=1.6736621856689453
I0208 23:05:56.522835 140051013605120 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.4687790870666504, loss=1.7424150705337524
I0208 23:06:31.072006 140051005212416 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3932383358478546, loss=1.6076799631118774
I0208 23:07:05.638270 140051013605120 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.40747395157814026, loss=1.5116602182388306
I0208 23:07:40.183271 140051005212416 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.46618008613586426, loss=1.575661301612854
I0208 23:08:14.703363 140051013605120 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.395927757024765, loss=1.6393346786499023
I0208 23:08:49.240093 140051005212416 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4249945282936096, loss=1.6780189275741577
I0208 23:09:23.767218 140051013605120 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3854008615016937, loss=1.60176420211792
I0208 23:09:58.345021 140051005212416 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3957849144935608, loss=1.5720946788787842
I0208 23:10:32.887637 140051013605120 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.419557124376297, loss=1.6115305423736572
I0208 23:11:07.429401 140051005212416 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.39933300018310547, loss=1.6467716693878174
I0208 23:11:41.976098 140051013605120 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.4003256559371948, loss=1.592650294303894
I0208 23:12:16.515237 140051005212416 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.4351593554019928, loss=1.6135658025741577
I0208 23:12:51.053724 140051013605120 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4139934182167053, loss=1.690841555595398
I0208 23:13:25.600316 140051005212416 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.42317453026771545, loss=1.6179254055023193
I0208 23:14:00.138232 140051013605120 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3985230326652527, loss=1.6283148527145386
I0208 23:14:34.684063 140051005212416 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.44453731179237366, loss=1.673918604850769
I0208 23:15:09.230604 140051013605120 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.41410163044929504, loss=1.6052918434143066
I0208 23:15:43.787246 140051005212416 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.39804401993751526, loss=1.521773338317871
I0208 23:16:18.339618 140051013605120 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3976568877696991, loss=1.6602028608322144
I0208 23:16:52.886982 140051005212416 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.41026443243026733, loss=1.640886664390564
I0208 23:17:27.424029 140051013605120 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.39040568470954895, loss=1.5451651811599731
I0208 23:17:56.168225 140225696298816 spec.py:321] Evaluating on the training split.
I0208 23:17:59.133029 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:21:28.925916 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 23:21:31.608744 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:23:55.826916 140225696298816 spec.py:349] Evaluating on the test split.
I0208 23:23:58.511815 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:26:15.337190 140225696298816 submission_runner.py:408] Time since start: 45631.01s, 	Step: 80285, 	{'train/accuracy': 0.6605494022369385, 'train/loss': 1.6196547746658325, 'train/bleu': 32.74820463146153, 'validation/accuracy': 0.6773629784584045, 'validation/loss': 1.497270107269287, 'validation/bleu': 29.766035010112983, 'validation/num_examples': 3000, 'test/accuracy': 0.6895241737365723, 'test/loss': 1.4248106479644775, 'test/bleu': 29.169380704529548, 'test/num_examples': 3003, 'score': 27750.07419347763, 'total_duration': 45631.012216091156, 'accumulated_submission_time': 27750.07419347763, 'accumulated_eval_time': 17877.399688482285, 'accumulated_logging_time': 1.0267176628112793}
I0208 23:26:15.363685 140051005212416 logging_writer.py:48] [80285] accumulated_eval_time=17877.399688, accumulated_logging_time=1.026718, accumulated_submission_time=27750.074193, global_step=80285, preemption_count=0, score=27750.074193, test/accuracy=0.689524, test/bleu=29.169381, test/loss=1.424811, test/num_examples=3003, total_duration=45631.012216, train/accuracy=0.660549, train/bleu=32.748205, train/loss=1.619655, validation/accuracy=0.677363, validation/bleu=29.766035, validation/loss=1.497270, validation/num_examples=3000
I0208 23:26:20.889919 140051013605120 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4520251154899597, loss=1.6450799703598022
I0208 23:26:55.286184 140051005212416 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3842470049858093, loss=1.5961886644363403
I0208 23:27:29.795819 140051013605120 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.38790786266326904, loss=1.6162782907485962
I0208 23:28:04.343248 140051005212416 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4109301269054413, loss=1.5935382843017578
I0208 23:28:38.881389 140051013605120 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.3976588845252991, loss=1.629603385925293
I0208 23:29:13.425611 140051005212416 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.40799960494041443, loss=1.7068499326705933
I0208 23:29:47.964002 140051013605120 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.42321154475212097, loss=1.610790729522705
I0208 23:30:22.470438 140051005212416 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3805197477340698, loss=1.598130226135254
I0208 23:30:56.997582 140051013605120 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.4111938178539276, loss=1.6640923023223877
I0208 23:31:31.519570 140051005212416 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4055722653865814, loss=1.5907344818115234
I0208 23:32:06.068548 140051013605120 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.40798938274383545, loss=1.5949379205703735
I0208 23:32:40.630489 140051005212416 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.4204537570476532, loss=1.6330571174621582
I0208 23:33:15.143719 140051013605120 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3795729875564575, loss=1.6529327630996704
I0208 23:33:49.697555 140051005212416 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.408637672662735, loss=1.6738075017929077
I0208 23:34:24.241537 140051013605120 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.41288357973098755, loss=1.610632300376892
I0208 23:34:58.779754 140051005212416 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.42767858505249023, loss=1.6323721408843994
I0208 23:35:33.311371 140051013605120 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.4546678066253662, loss=1.655470848083496
I0208 23:36:07.864201 140051005212416 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4591277837753296, loss=1.6693470478057861
I0208 23:36:42.420651 140051013605120 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.43321481347084045, loss=1.6364301443099976
I0208 23:37:16.967802 140051005212416 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.46347981691360474, loss=1.715399146080017
I0208 23:37:51.498343 140051013605120 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.4433993995189667, loss=1.6553361415863037
I0208 23:38:26.040265 140051005212416 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4339981973171234, loss=1.6210922002792358
I0208 23:39:00.535798 140051013605120 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3936202824115753, loss=1.570417046546936
I0208 23:39:35.045253 140051005212416 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4258682429790497, loss=1.659880518913269
I0208 23:40:09.605133 140051013605120 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.4075619578361511, loss=1.6630992889404297
I0208 23:40:15.547730 140225696298816 spec.py:321] Evaluating on the training split.
I0208 23:40:18.510361 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:44:07.267008 140225696298816 spec.py:333] Evaluating on the validation split.
I0208 23:44:09.946175 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:47:00.156435 140225696298816 spec.py:349] Evaluating on the test split.
I0208 23:47:02.847238 140225696298816 workload.py:181] Translating evaluation dataset.
I0208 23:49:36.800753 140225696298816 submission_runner.py:408] Time since start: 47032.48s, 	Step: 82719, 	{'train/accuracy': 0.6689773201942444, 'train/loss': 1.5642541646957397, 'train/bleu': 33.41044851396017, 'validation/accuracy': 0.6781564950942993, 'validation/loss': 1.491786003112793, 'validation/bleu': 29.560270364388217, 'validation/num_examples': 3000, 'test/accuracy': 0.6920806765556335, 'test/loss': 1.4164384603500366, 'test/bleu': 29.278406234177744, 'test/num_examples': 3003, 'score': 28590.17192029953, 'total_duration': 47032.475782871246, 'accumulated_submission_time': 28590.17192029953, 'accumulated_eval_time': 18438.652660131454, 'accumulated_logging_time': 1.062680959701538}
I0208 23:49:36.827312 140051005212416 logging_writer.py:48] [82719] accumulated_eval_time=18438.652660, accumulated_logging_time=1.062681, accumulated_submission_time=28590.171920, global_step=82719, preemption_count=0, score=28590.171920, test/accuracy=0.692081, test/bleu=29.278406, test/loss=1.416438, test/num_examples=3003, total_duration=47032.475783, train/accuracy=0.668977, train/bleu=33.410449, train/loss=1.564254, validation/accuracy=0.678156, validation/bleu=29.560270, validation/loss=1.491786, validation/num_examples=3000
I0208 23:50:05.057021 140051013605120 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4298159182071686, loss=1.6458725929260254
I0208 23:50:39.505345 140051005212416 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.4254406690597534, loss=1.5983062982559204
I0208 23:51:14.006516 140051013605120 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.40087035298347473, loss=1.5345966815948486
I0208 23:51:48.521350 140051005212416 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.40962281823158264, loss=1.6728886365890503
I0208 23:52:23.034606 140051013605120 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4054267108440399, loss=1.6612333059310913
I0208 23:52:57.547353 140051005212416 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.41364771127700806, loss=1.6000237464904785
I0208 23:53:32.058176 140051013605120 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.4416666626930237, loss=1.6669505834579468
I0208 23:54:06.582376 140051005212416 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.44472041726112366, loss=1.6502301692962646
I0208 23:54:41.120490 140051013605120 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.4203406572341919, loss=1.5901265144348145
I0208 23:55:15.660798 140051005212416 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3939030170440674, loss=1.5892753601074219
I0208 23:55:50.224702 140051013605120 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.41808629035949707, loss=1.549201250076294
I0208 23:56:24.765626 140051005212416 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.42282170057296753, loss=1.640000581741333
I0208 23:56:59.293436 140051013605120 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.43127644062042236, loss=1.585472583770752
I0208 23:57:33.838630 140051005212416 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.42000314593315125, loss=1.6207219362258911
I0208 23:58:08.398968 140051013605120 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.41894564032554626, loss=1.6472792625427246
I0208 23:58:42.959297 140051005212416 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4284921884536743, loss=1.6261835098266602
I0208 23:59:17.519381 140051013605120 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.4330190122127533, loss=1.6875962018966675
I0208 23:59:52.065292 140051005212416 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.4652397930622101, loss=1.6136047840118408
I0209 00:00:26.602510 140051013605120 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.4369543790817261, loss=1.5895111560821533
I0209 00:01:01.136069 140051005212416 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.41934874653816223, loss=1.543923258781433
I0209 00:01:35.673587 140051013605120 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.4157576560974121, loss=1.5776557922363281
I0209 00:02:10.197435 140051005212416 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4289562404155731, loss=1.5780580043792725
I0209 00:02:44.744551 140051013605120 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.46287867426872253, loss=1.5962446928024292
I0209 00:03:19.288082 140051005212416 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4137725234031677, loss=1.6721107959747314
I0209 00:03:36.972127 140225696298816 spec.py:321] Evaluating on the training split.
I0209 00:03:39.947299 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:07:25.568587 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 00:07:28.244172 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:10:39.504131 140225696298816 spec.py:349] Evaluating on the test split.
I0209 00:10:42.177350 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:13:55.454810 140225696298816 submission_runner.py:408] Time since start: 48491.13s, 	Step: 85153, 	{'train/accuracy': 0.6641597151756287, 'train/loss': 1.5821653604507446, 'train/bleu': 32.891950379823754, 'validation/accuracy': 0.679582417011261, 'validation/loss': 1.4825611114501953, 'validation/bleu': 29.699337732733976, 'validation/num_examples': 3000, 'test/accuracy': 0.6939166784286499, 'test/loss': 1.4057952165603638, 'test/bleu': 29.56086936865955, 'test/num_examples': 3003, 'score': 29430.228848457336, 'total_duration': 48491.12983894348, 'accumulated_submission_time': 29430.228848457336, 'accumulated_eval_time': 19057.13529086113, 'accumulated_logging_time': 1.1003947257995605}
I0209 00:13:55.481943 140051013605120 logging_writer.py:48] [85153] accumulated_eval_time=19057.135291, accumulated_logging_time=1.100395, accumulated_submission_time=29430.228848, global_step=85153, preemption_count=0, score=29430.228848, test/accuracy=0.693917, test/bleu=29.560869, test/loss=1.405795, test/num_examples=3003, total_duration=48491.129839, train/accuracy=0.664160, train/bleu=32.891950, train/loss=1.582165, validation/accuracy=0.679582, validation/bleu=29.699338, validation/loss=1.482561, validation/num_examples=3000
I0209 00:14:12.009727 140051005212416 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4309810400009155, loss=1.5464873313903809
I0209 00:14:46.414869 140051013605120 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.42219269275665283, loss=1.6047205924987793
I0209 00:15:20.906332 140051005212416 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.433414101600647, loss=1.533732533454895
I0209 00:15:55.452405 140051013605120 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.44222378730773926, loss=1.5863733291625977
I0209 00:16:29.977080 140051005212416 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.4305814504623413, loss=1.552984595298767
I0209 00:17:04.490286 140051013605120 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.4195891320705414, loss=1.6310914754867554
I0209 00:17:39.027038 140051005212416 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.4304676055908203, loss=1.6928471326828003
I0209 00:18:13.571455 140051013605120 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4478663504123688, loss=1.6081608533859253
I0209 00:18:48.091804 140051005212416 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.4492945373058319, loss=1.614797830581665
I0209 00:19:22.651426 140051013605120 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4680495262145996, loss=1.5427170991897583
I0209 00:19:57.195137 140051005212416 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.42143750190734863, loss=1.6230733394622803
I0209 00:20:31.745727 140051013605120 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.4465774893760681, loss=1.5925629138946533
I0209 00:21:06.326840 140051005212416 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4324159622192383, loss=1.6538233757019043
I0209 00:21:40.842484 140051013605120 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.4570041596889496, loss=1.6244465112686157
I0209 00:22:15.390846 140051005212416 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.44334283471107483, loss=1.656525731086731
I0209 00:22:49.937209 140051013605120 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.4384025037288666, loss=1.5964951515197754
I0209 00:23:24.468357 140051005212416 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4298143684864044, loss=1.6190277338027954
I0209 00:23:59.006437 140051013605120 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4143671691417694, loss=1.5072933435440063
I0209 00:24:33.517936 140051005212416 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.44151920080184937, loss=1.678929328918457
I0209 00:25:08.072697 140051013605120 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4546239674091339, loss=1.5962458848953247
I0209 00:25:42.604154 140051005212416 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.43300938606262207, loss=1.6097686290740967
I0209 00:26:17.149062 140051013605120 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4357776939868927, loss=1.5997658967971802
I0209 00:26:51.707304 140051005212416 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.4233306348323822, loss=1.5489333868026733
I0209 00:27:26.239687 140051013605120 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4402988851070404, loss=1.5949562788009644
I0209 00:27:55.662493 140225696298816 spec.py:321] Evaluating on the training split.
I0209 00:27:58.629895 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:31:43.754339 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 00:31:46.431729 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:34:41.216288 140225696298816 spec.py:349] Evaluating on the test split.
I0209 00:34:43.896093 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:37:19.546798 140225696298816 submission_runner.py:408] Time since start: 49895.22s, 	Step: 87587, 	{'train/accuracy': 0.6998754143714905, 'train/loss': 1.4042586088180542, 'train/bleu': 36.14299179613099, 'validation/accuracy': 0.6811694502830505, 'validation/loss': 1.4776188135147095, 'validation/bleu': 30.155293354746227, 'validation/num_examples': 3000, 'test/accuracy': 0.6963105201721191, 'test/loss': 1.3956819772720337, 'test/bleu': 29.990684621811305, 'test/num_examples': 3003, 'score': 30270.323910713196, 'total_duration': 49895.22182369232, 'accumulated_submission_time': 30270.323910713196, 'accumulated_eval_time': 19621.019548416138, 'accumulated_logging_time': 1.1373250484466553}
I0209 00:37:19.573838 140051005212416 logging_writer.py:48] [87587] accumulated_eval_time=19621.019548, accumulated_logging_time=1.137325, accumulated_submission_time=30270.323911, global_step=87587, preemption_count=0, score=30270.323911, test/accuracy=0.696311, test/bleu=29.990685, test/loss=1.395682, test/num_examples=3003, total_duration=49895.221824, train/accuracy=0.699875, train/bleu=36.142992, train/loss=1.404259, validation/accuracy=0.681169, validation/bleu=30.155293, validation/loss=1.477619, validation/num_examples=3000
I0209 00:37:24.407629 140051013605120 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.42319202423095703, loss=1.5935858488082886
I0209 00:37:58.796537 140051005212416 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.44307082891464233, loss=1.5650027990341187
I0209 00:38:33.255485 140051013605120 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.42490389943122864, loss=1.5585007667541504
I0209 00:39:07.775045 140051005212416 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.4185972809791565, loss=1.5497372150421143
I0209 00:39:42.299720 140051013605120 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.45288532972335815, loss=1.5835868120193481
I0209 00:40:16.828678 140051005212416 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4362684488296509, loss=1.5487420558929443
I0209 00:40:51.349711 140051013605120 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4347667694091797, loss=1.638469934463501
I0209 00:41:25.883676 140051005212416 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.4382505416870117, loss=1.5957748889923096
I0209 00:42:00.398964 140051013605120 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4120190739631653, loss=1.6017799377441406
I0209 00:42:34.938021 140051005212416 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.45417749881744385, loss=1.5911821126937866
I0209 00:43:09.470262 140051013605120 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.40867289900779724, loss=1.5645713806152344
I0209 00:43:43.984169 140051005212416 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.44093459844589233, loss=1.589663028717041
I0209 00:44:18.523179 140051013605120 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.43740132451057434, loss=1.7022079229354858
I0209 00:44:53.046068 140051005212416 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.447260320186615, loss=1.5498045682907104
I0209 00:45:27.566673 140051013605120 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.44556891918182373, loss=1.5447341203689575
I0209 00:46:02.117594 140051005212416 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4693376123905182, loss=1.6332283020019531
I0209 00:46:36.695796 140051013605120 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4592010974884033, loss=1.6563458442687988
I0209 00:47:11.224541 140051005212416 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.45067304372787476, loss=1.6225776672363281
I0209 00:47:45.780650 140051013605120 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.45315980911254883, loss=1.552330732345581
I0209 00:48:20.342752 140051005212416 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4283366799354553, loss=1.5419946908950806
I0209 00:48:54.863804 140051013605120 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.48207491636276245, loss=1.538211464881897
I0209 00:49:29.406574 140051005212416 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4583640694618225, loss=1.551399827003479
I0209 00:50:03.964422 140051013605120 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4551309049129486, loss=1.5661873817443848
I0209 00:50:38.519290 140051005212416 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.43879199028015137, loss=1.6126770973205566
I0209 00:51:13.080572 140051013605120 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4405858516693115, loss=1.5995877981185913
I0209 00:51:19.705411 140225696298816 spec.py:321] Evaluating on the training split.
I0209 00:51:22.676361 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:54:33.228401 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 00:54:35.918684 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:57:11.745056 140225696298816 spec.py:349] Evaluating on the test split.
I0209 00:57:14.420791 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 00:59:36.494484 140225696298816 submission_runner.py:408] Time since start: 51232.17s, 	Step: 90021, 	{'train/accuracy': 0.6689482927322388, 'train/loss': 1.5577067136764526, 'train/bleu': 33.551267722025884, 'validation/accuracy': 0.6799047589302063, 'validation/loss': 1.4750844240188599, 'validation/bleu': 29.56897782735325, 'validation/num_examples': 3000, 'test/accuracy': 0.6959386467933655, 'test/loss': 1.3930952548980713, 'test/bleu': 29.751664538682242, 'test/num_examples': 3003, 'score': 31110.369894504547, 'total_duration': 51232.16949701309, 'accumulated_submission_time': 31110.369894504547, 'accumulated_eval_time': 20117.80855345726, 'accumulated_logging_time': 1.1739370822906494}
I0209 00:59:36.522907 140051005212416 logging_writer.py:48] [90021] accumulated_eval_time=20117.808553, accumulated_logging_time=1.173937, accumulated_submission_time=31110.369895, global_step=90021, preemption_count=0, score=31110.369895, test/accuracy=0.695939, test/bleu=29.751665, test/loss=1.393095, test/num_examples=3003, total_duration=51232.169497, train/accuracy=0.668948, train/bleu=33.551268, train/loss=1.557707, validation/accuracy=0.679905, validation/bleu=29.568978, validation/loss=1.475084, validation/num_examples=3000
I0209 01:00:04.005432 140051013605120 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.44199854135513306, loss=1.5880976915359497
I0209 01:00:38.458658 140051005212416 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.4501320421695709, loss=1.4786381721496582
I0209 01:01:12.991866 140051013605120 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.47884437441825867, loss=1.6105154752731323
I0209 01:01:47.526795 140051005212416 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4414108097553253, loss=1.5703266859054565
I0209 01:02:22.071112 140051013605120 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.439574271440506, loss=1.4538416862487793
I0209 01:02:56.600109 140051005212416 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4338257908821106, loss=1.5893616676330566
I0209 01:03:31.146527 140051013605120 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4500049948692322, loss=1.5242197513580322
I0209 01:04:05.658358 140051005212416 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4465036690235138, loss=1.534119725227356
I0209 01:04:40.213278 140051013605120 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4502238929271698, loss=1.6230710744857788
I0209 01:05:14.748590 140051005212416 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.44148364663124084, loss=1.5668911933898926
I0209 01:05:49.288981 140051013605120 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.643353283405304, loss=1.5671422481536865
I0209 01:06:23.842412 140051005212416 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.46478745341300964, loss=1.6687397956848145
I0209 01:06:58.377130 140051013605120 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.46295055747032166, loss=1.5327017307281494
I0209 01:07:32.915001 140051005212416 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.45455026626586914, loss=1.5400923490524292
I0209 01:08:07.457052 140051013605120 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4512467086315155, loss=1.6267683506011963
I0209 01:08:42.019619 140051005212416 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4441099166870117, loss=1.5941295623779297
I0209 01:09:16.555045 140051013605120 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.46746590733528137, loss=1.5879799127578735
I0209 01:09:51.109056 140051005212416 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.48432597517967224, loss=1.6770942211151123
I0209 01:10:25.650283 140051013605120 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4414747953414917, loss=1.5300794839859009
I0209 01:11:00.210466 140051005212416 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.44886431097984314, loss=1.57257080078125
I0209 01:11:34.751919 140051013605120 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4712296426296234, loss=1.4999475479125977
I0209 01:12:09.310742 140051005212416 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.4562571346759796, loss=1.569540023803711
I0209 01:12:43.849275 140051013605120 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.5005178451538086, loss=1.5443897247314453
I0209 01:13:18.396007 140051005212416 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4756777882575989, loss=1.6292762756347656
I0209 01:13:36.753315 140225696298816 spec.py:321] Evaluating on the training split.
I0209 01:13:39.723417 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:17:05.981429 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 01:17:08.669183 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:19:38.834650 140225696298816 spec.py:349] Evaluating on the test split.
I0209 01:19:41.516713 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:22:08.725780 140225696298816 submission_runner.py:408] Time since start: 52584.40s, 	Step: 92455, 	{'train/accuracy': 0.6670649647712708, 'train/loss': 1.5666025876998901, 'train/bleu': 33.667583219883554, 'validation/accuracy': 0.6828681230545044, 'validation/loss': 1.465275764465332, 'validation/bleu': 30.192484170285006, 'validation/num_examples': 3000, 'test/accuracy': 0.6989483833312988, 'test/loss': 1.3823401927947998, 'test/bleu': 29.880037934469392, 'test/num_examples': 3003, 'score': 31950.513469696045, 'total_duration': 52584.40080785751, 'accumulated_submission_time': 31950.513469696045, 'accumulated_eval_time': 20629.78096461296, 'accumulated_logging_time': 1.2131965160369873}
I0209 01:22:08.753309 140051013605120 logging_writer.py:48] [92455] accumulated_eval_time=20629.780965, accumulated_logging_time=1.213197, accumulated_submission_time=31950.513470, global_step=92455, preemption_count=0, score=31950.513470, test/accuracy=0.698948, test/bleu=29.880038, test/loss=1.382340, test/num_examples=3003, total_duration=52584.400808, train/accuracy=0.667065, train/bleu=33.667583, train/loss=1.566603, validation/accuracy=0.682868, validation/bleu=30.192484, validation/loss=1.465276, validation/num_examples=3000
I0209 01:22:24.582689 140051005212416 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4749802052974701, loss=1.606515884399414
I0209 01:22:59.005089 140051013605120 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.45909935235977173, loss=1.595102071762085
I0209 01:23:33.488194 140051005212416 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4777335822582245, loss=1.56471586227417
I0209 01:24:08.021713 140051013605120 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.44440051913261414, loss=1.4970871210098267
I0209 01:24:42.554406 140051005212416 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4583125412464142, loss=1.5138912200927734
I0209 01:25:17.067459 140051013605120 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4739254415035248, loss=1.5627537965774536
I0209 01:25:51.626192 140051005212416 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4244430363178253, loss=1.5267645120620728
I0209 01:26:26.137531 140051013605120 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4408137798309326, loss=1.484375238418579
I0209 01:27:00.678142 140051005212416 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.42832043766975403, loss=1.496742606163025
I0209 01:27:35.240070 140051013605120 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4467751979827881, loss=1.5564370155334473
I0209 01:28:09.792334 140051005212416 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.49139612913131714, loss=1.6150587797164917
I0209 01:28:44.324498 140051013605120 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.46015459299087524, loss=1.5550787448883057
I0209 01:29:18.870565 140051005212416 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.47591060400009155, loss=1.5516185760498047
I0209 01:29:53.416595 140051013605120 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4697834253311157, loss=1.5452903509140015
I0209 01:30:27.957288 140051005212416 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.46948450803756714, loss=1.6118718385696411
I0209 01:31:02.527258 140051013605120 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4793868064880371, loss=1.5184297561645508
I0209 01:31:37.057396 140051005212416 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.45911407470703125, loss=1.55722177028656
I0209 01:32:11.626471 140051013605120 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4804784059524536, loss=1.5339113473892212
I0209 01:32:46.208674 140051005212416 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.465753972530365, loss=1.6002016067504883
I0209 01:33:20.756622 140051013605120 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.48414233326911926, loss=1.5507431030273438
I0209 01:33:55.290772 140051005212416 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4788270592689514, loss=1.5877481698989868
I0209 01:34:29.827027 140051013605120 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.4640837013721466, loss=1.547088861465454
I0209 01:35:04.386906 140051005212416 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.48592039942741394, loss=1.5408943891525269
I0209 01:35:38.917504 140051013605120 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4982670545578003, loss=1.4814118146896362
I0209 01:36:09.012983 140225696298816 spec.py:321] Evaluating on the training split.
I0209 01:36:11.983237 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:39:02.797583 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 01:39:05.474693 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:42:02.596983 140225696298816 spec.py:349] Evaluating on the test split.
I0209 01:42:05.280076 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 01:44:35.355317 140225696298816 submission_runner.py:408] Time since start: 53931.03s, 	Step: 94889, 	{'train/accuracy': 0.6854655146598816, 'train/loss': 1.4662498235702515, 'train/bleu': 34.65779037824217, 'validation/accuracy': 0.6841452717781067, 'validation/loss': 1.458268165588379, 'validation/bleu': 30.1208920740266, 'validation/num_examples': 3000, 'test/accuracy': 0.6998547315597534, 'test/loss': 1.3732168674468994, 'test/bleu': 30.19016397136998, 'test/num_examples': 3003, 'score': 32790.68526005745, 'total_duration': 53931.030331134796, 'accumulated_submission_time': 32790.68526005745, 'accumulated_eval_time': 21136.123238801956, 'accumulated_logging_time': 1.2523298263549805}
I0209 01:44:35.383783 140051005212416 logging_writer.py:48] [94889] accumulated_eval_time=21136.123239, accumulated_logging_time=1.252330, accumulated_submission_time=32790.685260, global_step=94889, preemption_count=0, score=32790.685260, test/accuracy=0.699855, test/bleu=30.190164, test/loss=1.373217, test/num_examples=3003, total_duration=53931.030331, train/accuracy=0.685466, train/bleu=34.657790, train/loss=1.466250, validation/accuracy=0.684145, validation/bleu=30.120892, validation/loss=1.458268, validation/num_examples=3000
I0209 01:44:39.527437 140051013605120 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.4740561544895172, loss=1.5159313678741455
I0209 01:45:13.932828 140051005212416 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5371096730232239, loss=1.5498446226119995
I0209 01:45:48.419994 140051013605120 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.5033734440803528, loss=1.5280143022537231
I0209 01:46:22.967037 140051005212416 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.46603333950042725, loss=1.5862866640090942
I0209 01:46:57.484607 140051013605120 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.47073572874069214, loss=1.5721973180770874
I0209 01:47:32.039365 140051005212416 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.4840907156467438, loss=1.443554401397705
I0209 01:48:06.575281 140051013605120 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.49771735072135925, loss=1.6040385961532593
I0209 01:48:41.117659 140051005212416 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4895133972167969, loss=1.545262336730957
I0209 01:49:15.652066 140051013605120 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4700527489185333, loss=1.4948338270187378
I0209 01:49:50.202438 140051005212416 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4724411964416504, loss=1.498915433883667
I0209 01:50:24.709467 140051013605120 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.48495855927467346, loss=1.506531000137329
I0209 01:50:59.246607 140051005212416 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5170389413833618, loss=1.5585051774978638
I0209 01:51:33.777622 140051013605120 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.47005778551101685, loss=1.551152229309082
I0209 01:52:08.331405 140051005212416 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.4712420105934143, loss=1.511934757232666
I0209 01:52:42.890421 140051013605120 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4847472012042999, loss=1.609562635421753
I0209 01:53:17.426113 140051005212416 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5084158778190613, loss=1.5324358940124512
I0209 01:53:51.965281 140051013605120 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.487658828496933, loss=1.5708447694778442
I0209 01:54:26.514829 140051005212416 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.9354192018508911, loss=1.5974678993225098
I0209 01:55:01.044172 140051013605120 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.4819554090499878, loss=1.48594331741333
I0209 01:55:35.574215 140051005212416 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5060641169548035, loss=1.6082789897918701
I0209 01:56:10.114485 140051013605120 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.47330543398857117, loss=1.5314944982528687
I0209 01:56:44.644059 140051005212416 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4779936373233795, loss=1.5830384492874146
I0209 01:57:19.183198 140051013605120 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.49008944630622864, loss=1.4614108800888062
I0209 01:57:53.746177 140051005212416 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.4818046987056732, loss=1.546188473701477
I0209 01:58:28.302213 140051013605120 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5051358938217163, loss=1.5978713035583496
I0209 01:58:35.617434 140225696298816 spec.py:321] Evaluating on the training split.
I0209 01:58:38.601686 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:02:19.247404 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 02:02:21.929150 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:05:08.786634 140225696298816 spec.py:349] Evaluating on the test split.
I0209 02:05:11.465204 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:08:03.628712 140225696298816 submission_runner.py:408] Time since start: 55339.30s, 	Step: 97323, 	{'train/accuracy': 0.6764490008354187, 'train/loss': 1.5147420167922974, 'train/bleu': 34.23678263585544, 'validation/accuracy': 0.686042308807373, 'validation/loss': 1.4486397504806519, 'validation/bleu': 30.389317993276933, 'validation/num_examples': 3000, 'test/accuracy': 0.7023648023605347, 'test/loss': 1.3640131950378418, 'test/bleu': 30.336006678823065, 'test/num_examples': 3003, 'score': 33630.83154082298, 'total_duration': 55339.30372548103, 'accumulated_submission_time': 33630.83154082298, 'accumulated_eval_time': 21704.13444662094, 'accumulated_logging_time': 1.2920780181884766}
I0209 02:08:03.657382 140051005212416 logging_writer.py:48] [97323] accumulated_eval_time=21704.134447, accumulated_logging_time=1.292078, accumulated_submission_time=33630.831541, global_step=97323, preemption_count=0, score=33630.831541, test/accuracy=0.702365, test/bleu=30.336007, test/loss=1.364013, test/num_examples=3003, total_duration=55339.303725, train/accuracy=0.676449, train/bleu=34.236783, train/loss=1.514742, validation/accuracy=0.686042, validation/bleu=30.389318, validation/loss=1.448640, validation/num_examples=3000
I0209 02:08:30.474106 140051013605120 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.4923844635486603, loss=1.5071756839752197
I0209 02:09:04.945544 140051005212416 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5106025338172913, loss=1.611191749572754
I0209 02:09:39.470082 140051013605120 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4745153784751892, loss=1.5184249877929688
I0209 02:10:13.994849 140051005212416 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.4989858865737915, loss=1.5193220376968384
I0209 02:10:48.510166 140051013605120 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.4949120581150055, loss=1.4918569326400757
I0209 02:11:23.039681 140051005212416 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.47107964754104614, loss=1.5657685995101929
I0209 02:11:57.597637 140051013605120 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5014083981513977, loss=1.5799654722213745
I0209 02:12:32.167594 140051005212416 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.49718862771987915, loss=1.5015431642532349
I0209 02:13:06.712284 140051013605120 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.502196729183197, loss=1.570622444152832
I0209 02:13:41.253414 140051005212416 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.49574190378189087, loss=1.546751618385315
I0209 02:14:15.820221 140051013605120 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5130212306976318, loss=1.5223398208618164
I0209 02:14:50.366219 140051005212416 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5008835792541504, loss=1.4960575103759766
I0209 02:15:24.929964 140051013605120 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5089519023895264, loss=1.5152192115783691
I0209 02:15:59.494733 140051005212416 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5366641879081726, loss=1.4588994979858398
I0209 02:16:34.026042 140051013605120 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5273013114929199, loss=1.6113603115081787
I0209 02:17:08.574612 140051005212416 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5405222773551941, loss=1.49423348903656
I0209 02:17:43.118551 140051013605120 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5005887150764465, loss=1.5092347860336304
I0209 02:18:17.660364 140051005212416 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5157687664031982, loss=1.5815331935882568
I0209 02:18:52.238605 140051013605120 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.4798554480075836, loss=1.4659476280212402
I0209 02:19:26.811825 140051005212416 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5334954261779785, loss=1.5399516820907593
I0209 02:20:01.372114 140051013605120 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5144965052604675, loss=1.5106453895568848
I0209 02:20:35.930382 140051005212416 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5431793332099915, loss=1.5395764112472534
I0209 02:21:10.468069 140051013605120 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5225798487663269, loss=1.4763476848602295
I0209 02:21:44.990439 140051005212416 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5265461206436157, loss=1.521582007408142
I0209 02:22:03.702497 140225696298816 spec.py:321] Evaluating on the training split.
I0209 02:22:06.676603 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:26:36.915731 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 02:26:39.601791 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:29:31.988060 140225696298816 spec.py:349] Evaluating on the test split.
I0209 02:29:34.664375 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:32:18.404642 140225696298816 submission_runner.py:408] Time since start: 56794.08s, 	Step: 99756, 	{'train/accuracy': 0.6764749884605408, 'train/loss': 1.5274385213851929, 'train/bleu': 34.04751952929381, 'validation/accuracy': 0.6861662864685059, 'validation/loss': 1.4491263628005981, 'validation/bleu': 30.30977062684815, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.360594391822815, 'test/bleu': 30.23364767228356, 'test/num_examples': 3003, 'score': 34470.79104375839, 'total_duration': 56794.07966709137, 'accumulated_submission_time': 34470.79104375839, 'accumulated_eval_time': 22318.836542367935, 'accumulated_logging_time': 1.3307271003723145}
I0209 02:32:18.434378 140051013605120 logging_writer.py:48] [99756] accumulated_eval_time=22318.836542, accumulated_logging_time=1.330727, accumulated_submission_time=34470.791044, global_step=99756, preemption_count=0, score=34470.791044, test/accuracy=0.700959, test/bleu=30.233648, test/loss=1.360594, test/num_examples=3003, total_duration=56794.079667, train/accuracy=0.676475, train/bleu=34.047520, train/loss=1.527439, validation/accuracy=0.686166, validation/bleu=30.309771, validation/loss=1.449126, validation/num_examples=3000
I0209 02:32:33.912078 140051005212416 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.48616355657577515, loss=1.5107117891311646
I0209 02:33:08.350116 140051013605120 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5456316471099854, loss=1.5688437223434448
I0209 02:33:42.893330 140051005212416 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5210905075073242, loss=1.569771647453308
I0209 02:34:17.423506 140051013605120 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5242904424667358, loss=1.5133811235427856
I0209 02:34:51.947125 140051005212416 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5493466258049011, loss=1.5024226903915405
I0209 02:35:26.487466 140051013605120 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5037006139755249, loss=1.505994439125061
I0209 02:36:01.021826 140051005212416 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5176078081130981, loss=1.5277949571609497
I0209 02:36:35.554000 140051013605120 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5467876195907593, loss=1.5682742595672607
I0209 02:37:10.125731 140051005212416 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5372450351715088, loss=1.505397081375122
I0209 02:37:44.675390 140051013605120 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.55036860704422, loss=1.561523199081421
I0209 02:38:19.214760 140051005212416 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5393224358558655, loss=1.4687620401382446
I0209 02:38:53.791178 140051013605120 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5294210314750671, loss=1.4865623712539673
I0209 02:39:28.306157 140051005212416 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.536015510559082, loss=1.5218955278396606
I0209 02:40:02.853354 140051013605120 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5427352786064148, loss=1.467315435409546
I0209 02:40:37.414651 140051005212416 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5326893329620361, loss=1.536616325378418
I0209 02:41:11.947991 140051013605120 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5389102101325989, loss=1.5878021717071533
I0209 02:41:46.483422 140051005212416 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5182127952575684, loss=1.5121445655822754
I0209 02:42:21.030439 140051013605120 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5519738793373108, loss=1.5593112707138062
I0209 02:42:55.588064 140051005212416 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5566368699073792, loss=1.6111140251159668
I0209 02:43:30.135878 140051013605120 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5256526470184326, loss=1.5148887634277344
I0209 02:44:04.709615 140051005212416 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5203407406806946, loss=1.5321714878082275
I0209 02:44:39.247015 140051013605120 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.5641927719116211, loss=1.5520308017730713
I0209 02:45:13.805176 140051005212416 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.4934767782688141, loss=1.4903260469436646
I0209 02:45:48.360865 140051013605120 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5025256872177124, loss=1.425123691558838
I0209 02:46:18.483723 140225696298816 spec.py:321] Evaluating on the training split.
I0209 02:46:21.453981 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:50:12.130810 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 02:50:14.807319 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:52:45.077210 140225696298816 spec.py:349] Evaluating on the test split.
I0209 02:52:47.767969 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 02:55:07.841179 140225696298816 submission_runner.py:408] Time since start: 58163.52s, 	Step: 102189, 	{'train/accuracy': 0.6875606179237366, 'train/loss': 1.4551113843917847, 'train/bleu': 34.74172992301118, 'validation/accuracy': 0.6875798106193542, 'validation/loss': 1.437204360961914, 'validation/bleu': 30.38678618421871, 'validation/num_examples': 3000, 'test/accuracy': 0.7030736207962036, 'test/loss': 1.3559722900390625, 'test/bleu': 30.379172198859756, 'test/num_examples': 3003, 'score': 35310.75461125374, 'total_duration': 58163.51621007919, 'accumulated_submission_time': 35310.75461125374, 'accumulated_eval_time': 22848.193954706192, 'accumulated_logging_time': 1.3705039024353027}
I0209 02:55:07.870355 140051005212416 logging_writer.py:48] [102189] accumulated_eval_time=22848.193955, accumulated_logging_time=1.370504, accumulated_submission_time=35310.754611, global_step=102189, preemption_count=0, score=35310.754611, test/accuracy=0.703074, test/bleu=30.379172, test/loss=1.355972, test/num_examples=3003, total_duration=58163.516210, train/accuracy=0.687561, train/bleu=34.741730, train/loss=1.455111, validation/accuracy=0.687580, validation/bleu=30.386786, validation/loss=1.437204, validation/num_examples=3000
I0209 02:55:12.021009 140051013605120 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5537997484207153, loss=1.5145410299301147
I0209 02:55:46.405301 140051005212416 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5208848714828491, loss=1.4934184551239014
I0209 02:56:20.892280 140051013605120 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5118110179901123, loss=1.440255045890808
I0209 02:56:55.435807 140051005212416 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5444983243942261, loss=1.5436415672302246
I0209 02:57:29.964151 140051013605120 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5348396897315979, loss=1.5109432935714722
I0209 02:58:04.468066 140051005212416 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5669800639152527, loss=1.61744225025177
I0209 02:58:39.011790 140051013605120 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5290247201919556, loss=1.5626076459884644
I0209 02:59:13.531862 140051005212416 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5563187003135681, loss=1.4751659631729126
I0209 02:59:48.072971 140051013605120 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5286242961883545, loss=1.5534698963165283
I0209 03:00:22.629215 140051005212416 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5666722655296326, loss=1.5371335744857788
I0209 03:00:57.168483 140051013605120 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5437464714050293, loss=1.4954339265823364
I0209 03:01:31.717114 140051005212416 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5387628078460693, loss=1.5416395664215088
I0209 03:02:06.239602 140051013605120 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5334846377372742, loss=1.4988445043563843
I0209 03:02:40.787369 140051005212416 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5539379119873047, loss=1.488808035850525
I0209 03:03:15.314480 140051013605120 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5612204670906067, loss=1.5177265405654907
I0209 03:03:49.864749 140051005212416 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.5329111218452454, loss=1.5065786838531494
I0209 03:04:24.407326 140051013605120 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.548821747303009, loss=1.4337502717971802
I0209 03:04:58.948161 140051005212416 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.5668801665306091, loss=1.5280448198318481
I0209 03:05:33.508181 140051013605120 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5614287853240967, loss=1.472476840019226
I0209 03:06:08.064738 140051005212416 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5645851492881775, loss=1.5274919271469116
I0209 03:06:42.600092 140051013605120 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.5816935300827026, loss=1.499298095703125
I0209 03:07:17.148228 140051005212416 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.552236795425415, loss=1.5233769416809082
I0209 03:07:51.689842 140051013605120 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.56541907787323, loss=1.5165659189224243
I0209 03:08:26.233541 140051005212416 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5666579604148865, loss=1.5641943216323853
I0209 03:09:00.792907 140051013605120 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5467290878295898, loss=1.460898995399475
I0209 03:09:08.106666 140225696298816 spec.py:321] Evaluating on the training split.
I0209 03:09:11.083841 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:13:28.335817 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 03:13:31.028165 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:16:11.254576 140225696298816 spec.py:349] Evaluating on the test split.
I0209 03:16:13.944949 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:18:56.187656 140225696298816 submission_runner.py:408] Time since start: 59591.86s, 	Step: 104623, 	{'train/accuracy': 0.6816208958625793, 'train/loss': 1.488276720046997, 'train/bleu': 34.384006160637064, 'validation/accuracy': 0.6881005764007568, 'validation/loss': 1.4366995096206665, 'validation/bleu': 30.368678371068093, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.3490632772445679, 'test/bleu': 30.614908680396404, 'test/num_examples': 3003, 'score': 36150.90387272835, 'total_duration': 59591.86266922951, 'accumulated_submission_time': 36150.90387272835, 'accumulated_eval_time': 23436.27487707138, 'accumulated_logging_time': 1.4109792709350586}
I0209 03:18:56.217481 140051005212416 logging_writer.py:48] [104623] accumulated_eval_time=23436.274877, accumulated_logging_time=1.410979, accumulated_submission_time=36150.903873, global_step=104623, preemption_count=0, score=36150.903873, test/accuracy=0.704317, test/bleu=30.614909, test/loss=1.349063, test/num_examples=3003, total_duration=59591.862669, train/accuracy=0.681621, train/bleu=34.384006, train/loss=1.488277, validation/accuracy=0.688101, validation/bleu=30.368678, validation/loss=1.436700, validation/num_examples=3000
I0209 03:19:23.035622 140051013605120 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.5454650521278381, loss=1.5148992538452148
I0209 03:19:57.530250 140051005212416 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.5759701132774353, loss=1.498988151550293
I0209 03:20:32.082401 140051013605120 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.595140278339386, loss=1.487210750579834
I0209 03:21:06.617462 140051005212416 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5759742259979248, loss=1.4932825565338135
I0209 03:21:41.172876 140051013605120 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5760110020637512, loss=1.4603608846664429
I0209 03:22:15.723749 140051005212416 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.590168833732605, loss=1.5363458395004272
I0209 03:22:50.256647 140051013605120 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5728856921195984, loss=1.5052927732467651
I0209 03:23:24.805043 140051005212416 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5645955801010132, loss=1.4581427574157715
I0209 03:23:59.350983 140051013605120 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.5514652132987976, loss=1.4694637060165405
I0209 03:24:33.901096 140051005212416 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5506874918937683, loss=1.4841878414154053
I0209 03:25:08.475134 140051013605120 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5758134126663208, loss=1.4720031023025513
I0209 03:25:43.041586 140051005212416 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5763623118400574, loss=1.4991430044174194
I0209 03:26:17.596737 140051013605120 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.6037746667861938, loss=1.4521455764770508
I0209 03:26:52.137815 140051005212416 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5621493458747864, loss=1.4105823040008545
I0209 03:27:26.684129 140051013605120 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5979469418525696, loss=1.4422203302383423
I0209 03:28:01.217395 140051005212416 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.5578539371490479, loss=1.5289980173110962
I0209 03:28:35.767039 140051013605120 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.6004473567008972, loss=1.5315122604370117
I0209 03:29:10.330654 140051005212416 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5692342519760132, loss=1.4902387857437134
I0209 03:29:44.918752 140051013605120 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6146497130393982, loss=1.4676531553268433
I0209 03:30:19.470711 140051005212416 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.5951117873191833, loss=1.5110666751861572
I0209 03:30:54.007264 140051013605120 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5835151076316833, loss=1.5175169706344604
I0209 03:31:28.572746 140051005212416 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5779364705085754, loss=1.4947843551635742
I0209 03:32:03.098539 140051013605120 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5761258006095886, loss=1.4388647079467773
I0209 03:32:37.646018 140051005212416 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5923492908477783, loss=1.5383124351501465
I0209 03:32:56.375634 140225696298816 spec.py:321] Evaluating on the training split.
I0209 03:32:59.345130 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:36:32.713108 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 03:36:35.383769 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:39:23.318424 140225696298816 spec.py:349] Evaluating on the test split.
I0209 03:39:25.994589 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:42:06.461265 140225696298816 submission_runner.py:408] Time since start: 60982.14s, 	Step: 107056, 	{'train/accuracy': 0.6984134316444397, 'train/loss': 1.395855188369751, 'train/bleu': 35.94551384243376, 'validation/accuracy': 0.6892660856246948, 'validation/loss': 1.4338706731796265, 'validation/bleu': 30.78100347405398, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.348414421081543, 'test/bleu': 30.546431014686842, 'test/num_examples': 3003, 'score': 36990.97637438774, 'total_duration': 60982.136293411255, 'accumulated_submission_time': 36990.97637438774, 'accumulated_eval_time': 23986.36045742035, 'accumulated_logging_time': 1.4505560398101807}
I0209 03:42:06.496381 140051013605120 logging_writer.py:48] [107056] accumulated_eval_time=23986.360457, accumulated_logging_time=1.450556, accumulated_submission_time=36990.976374, global_step=107056, preemption_count=0, score=36990.976374, test/accuracy=0.704817, test/bleu=30.546431, test/loss=1.348414, test/num_examples=3003, total_duration=60982.136293, train/accuracy=0.698413, train/bleu=35.945514, train/loss=1.395855, validation/accuracy=0.689266, validation/bleu=30.781003, validation/loss=1.433871, validation/num_examples=3000
I0209 03:42:21.964828 140051005212416 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.601555585861206, loss=1.501454472541809
I0209 03:42:56.388910 140051013605120 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.6214591264724731, loss=1.5010440349578857
I0209 03:43:30.903459 140051005212416 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.5943484902381897, loss=1.4694769382476807
I0209 03:44:05.424600 140051013605120 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.5773980021476746, loss=1.4826486110687256
I0209 03:44:39.922745 140051005212416 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6078438758850098, loss=1.4064031839370728
I0209 03:45:14.462701 140051013605120 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.6027041673660278, loss=1.4408992528915405
I0209 03:45:49.013733 140051005212416 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.5725831985473633, loss=1.4233441352844238
I0209 03:46:23.570491 140051013605120 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.5907042026519775, loss=1.4650824069976807
I0209 03:46:58.123347 140051005212416 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.5938824415206909, loss=1.4072176218032837
I0209 03:47:32.674353 140051013605120 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6016448140144348, loss=1.474924087524414
I0209 03:48:07.209038 140051005212416 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5944082140922546, loss=1.4740300178527832
I0209 03:48:41.761632 140051013605120 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6579959988594055, loss=1.473974347114563
I0209 03:49:16.319920 140051005212416 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6145591735839844, loss=1.517032504081726
I0209 03:49:50.869941 140051013605120 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.596930205821991, loss=1.4446148872375488
I0209 03:50:25.429534 140051005212416 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6198080778121948, loss=1.457665205001831
I0209 03:50:59.959302 140051013605120 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.6005949974060059, loss=1.4952188730239868
I0209 03:51:34.496029 140051005212416 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6299073696136475, loss=1.505273461341858
I0209 03:52:09.055374 140051013605120 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.601078987121582, loss=1.4378302097320557
I0209 03:52:43.607178 140051005212416 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6282938122749329, loss=1.4249134063720703
I0209 03:53:18.150856 140051013605120 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6418342590332031, loss=1.4897764921188354
I0209 03:53:52.684066 140051005212416 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.5840228199958801, loss=1.4012172222137451
I0209 03:54:27.241142 140051013605120 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6312620639801025, loss=1.4993442296981812
I0209 03:55:01.786050 140051005212416 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6382842063903809, loss=1.431405782699585
I0209 03:55:36.335809 140051013605120 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6346558332443237, loss=1.432400107383728
I0209 03:56:06.786168 140225696298816 spec.py:321] Evaluating on the training split.
I0209 03:56:09.756021 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 03:59:46.929505 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 03:59:49.596646 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:02:15.945337 140225696298816 spec.py:349] Evaluating on the test split.
I0209 04:02:18.633074 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:04:58.804271 140225696298816 submission_runner.py:408] Time since start: 62354.48s, 	Step: 109490, 	{'train/accuracy': 0.6916165947914124, 'train/loss': 1.4298664331436157, 'train/bleu': 35.22134886001856, 'validation/accuracy': 0.6891669034957886, 'validation/loss': 1.4241664409637451, 'validation/bleu': 30.790310256380376, 'validation/num_examples': 3000, 'test/accuracy': 0.7041543126106262, 'test/loss': 1.3388774394989014, 'test/bleu': 30.61245571439726, 'test/num_examples': 3003, 'score': 37831.17963075638, 'total_duration': 62354.47929620743, 'accumulated_submission_time': 37831.17963075638, 'accumulated_eval_time': 24518.378514528275, 'accumulated_logging_time': 1.4953322410583496}
I0209 04:04:58.834481 140051005212416 logging_writer.py:48] [109490] accumulated_eval_time=24518.378515, accumulated_logging_time=1.495332, accumulated_submission_time=37831.179631, global_step=109490, preemption_count=0, score=37831.179631, test/accuracy=0.704154, test/bleu=30.612456, test/loss=1.338877, test/num_examples=3003, total_duration=62354.479296, train/accuracy=0.691617, train/bleu=35.221349, train/loss=1.429866, validation/accuracy=0.689167, validation/bleu=30.790310, validation/loss=1.424166, validation/num_examples=3000
I0209 04:05:02.646203 140051013605120 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6126713156700134, loss=1.5191385746002197
I0209 04:05:37.032031 140051005212416 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6193486452102661, loss=1.3982248306274414
I0209 04:06:11.507173 140051013605120 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.6192761659622192, loss=1.4860254526138306
I0209 04:06:46.049051 140051005212416 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.6225014925003052, loss=1.3862898349761963
I0209 04:07:20.586168 140051013605120 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6355398893356323, loss=1.4895130395889282
I0209 04:07:55.127984 140051005212416 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6142024397850037, loss=1.4567899703979492
I0209 04:08:29.686519 140051013605120 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6336134076118469, loss=1.4785581827163696
I0209 04:09:04.227388 140051005212416 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6421359181404114, loss=1.495205044746399
I0209 04:09:38.771253 140051013605120 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.6520906686782837, loss=1.4404503107070923
I0209 04:10:13.317243 140051005212416 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6350358724594116, loss=1.5454422235488892
I0209 04:10:47.870968 140051013605120 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6551801562309265, loss=1.4364416599273682
I0209 04:11:22.430488 140051005212416 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6353962421417236, loss=1.3932291269302368
I0209 04:11:56.963521 140051013605120 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6413487792015076, loss=1.5145448446273804
I0209 04:12:31.527066 140051005212416 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.6265955567359924, loss=1.494789958000183
I0209 04:13:06.063680 140051013605120 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6367631554603577, loss=1.5294547080993652
I0209 04:13:40.599670 140051005212416 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6322571635246277, loss=1.458976149559021
I0209 04:14:15.139019 140051013605120 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6354646682739258, loss=1.4742788076400757
I0209 04:14:49.675297 140051005212416 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6226345300674438, loss=1.4022554159164429
I0209 04:15:24.217174 140051013605120 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.6418641209602356, loss=1.46927809715271
I0209 04:15:58.776168 140051005212416 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.6477681398391724, loss=1.4931461811065674
I0209 04:16:33.317358 140051013605120 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.647054135799408, loss=1.4959604740142822
I0209 04:17:07.890328 140051005212416 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6643891334533691, loss=1.4600868225097656
I0209 04:17:42.446230 140051013605120 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6539847254753113, loss=1.4393818378448486
I0209 04:18:17.026978 140051005212416 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.6589660048484802, loss=1.430594563484192
I0209 04:18:51.557655 140051013605120 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6505792737007141, loss=1.5058177709579468
I0209 04:18:58.887547 140225696298816 spec.py:321] Evaluating on the training split.
I0209 04:19:01.869254 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:23:05.138078 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 04:23:07.821623 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:25:35.372186 140225696298816 spec.py:349] Evaluating on the test split.
I0209 04:25:38.055901 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:27:59.050146 140225696298816 submission_runner.py:408] Time since start: 63734.73s, 	Step: 111923, 	{'train/accuracy': 0.6884708404541016, 'train/loss': 1.4604493379592896, 'train/bleu': 35.23403588662617, 'validation/accuracy': 0.6906672120094299, 'validation/loss': 1.4245483875274658, 'validation/bleu': 30.639060315221954, 'validation/num_examples': 3000, 'test/accuracy': 0.7065016627311707, 'test/loss': 1.336126446723938, 'test/bleu': 30.754648086173646, 'test/num_examples': 3003, 'score': 38671.1463496685, 'total_duration': 63734.725130319595, 'accumulated_submission_time': 38671.1463496685, 'accumulated_eval_time': 25058.541039943695, 'accumulated_logging_time': 1.5352284908294678}
I0209 04:27:59.080869 140051005212416 logging_writer.py:48] [111923] accumulated_eval_time=25058.541040, accumulated_logging_time=1.535228, accumulated_submission_time=38671.146350, global_step=111923, preemption_count=0, score=38671.146350, test/accuracy=0.706502, test/bleu=30.754648, test/loss=1.336126, test/num_examples=3003, total_duration=63734.725130, train/accuracy=0.688471, train/bleu=35.234036, train/loss=1.460449, validation/accuracy=0.690667, validation/bleu=30.639060, validation/loss=1.424548, validation/num_examples=3000
I0209 04:28:25.902779 140051013605120 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6996191143989563, loss=1.489043951034546
I0209 04:29:00.361934 140051005212416 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6471850275993347, loss=1.4054441452026367
I0209 04:29:34.908424 140051013605120 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.6581509709358215, loss=1.407846212387085
I0209 04:30:09.446970 140051005212416 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6583360433578491, loss=1.4175758361816406
I0209 04:30:43.987570 140051013605120 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6549597382545471, loss=1.403456687927246
I0209 04:31:18.542301 140051005212416 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.691605269908905, loss=1.4443954229354858
I0209 04:31:53.069303 140051013605120 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.6722154021263123, loss=1.4531084299087524
I0209 04:32:27.639325 140051005212416 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.6725367307662964, loss=1.3780755996704102
I0209 04:33:02.211106 140051013605120 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6545162200927734, loss=1.4244943857192993
I0209 04:33:36.758543 140051005212416 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6622443199157715, loss=1.4171903133392334
I0209 04:34:11.305204 140051013605120 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6523812413215637, loss=1.3551422357559204
I0209 04:34:45.837896 140051005212416 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6571885347366333, loss=1.3584235906600952
I0209 04:35:20.389261 140051013605120 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.6467583179473877, loss=1.364006519317627
I0209 04:35:54.949476 140051005212416 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.6632454991340637, loss=1.4548460245132446
I0209 04:36:29.492375 140051013605120 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.6852037310600281, loss=1.423579454421997
I0209 04:37:04.052264 140051005212416 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6608299612998962, loss=1.4382209777832031
I0209 04:37:38.580539 140051013605120 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6705800890922546, loss=1.4202303886413574
I0209 04:38:13.130203 140051005212416 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7038044333457947, loss=1.431288719177246
I0209 04:38:47.700280 140051013605120 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.697492241859436, loss=1.4061568975448608
I0209 04:39:22.243335 140051005212416 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6614940166473389, loss=1.3861552476882935
I0209 04:39:56.794359 140051013605120 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6567831635475159, loss=1.4147567749023438
I0209 04:40:31.335127 140051005212416 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6637521982192993, loss=1.4175019264221191
I0209 04:41:05.870181 140051013605120 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6695309281349182, loss=1.4519785642623901
I0209 04:41:40.409383 140051005212416 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.6726222634315491, loss=1.4518266916275024
I0209 04:41:59.126975 140225696298816 spec.py:321] Evaluating on the training split.
I0209 04:42:02.112800 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:45:38.256618 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 04:45:40.935837 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:48:37.091722 140225696298816 spec.py:349] Evaluating on the test split.
I0209 04:48:39.777785 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 04:51:13.979466 140225696298816 submission_runner.py:408] Time since start: 65129.65s, 	Step: 114356, 	{'train/accuracy': 0.7008587121963501, 'train/loss': 1.3816499710083008, 'train/bleu': 36.201180058925885, 'validation/accuracy': 0.6914979219436646, 'validation/loss': 1.4168438911437988, 'validation/bleu': 30.521495408385693, 'validation/num_examples': 3000, 'test/accuracy': 0.7080007195472717, 'test/loss': 1.3302934169769287, 'test/bleu': 30.880743489149467, 'test/num_examples': 3003, 'score': 39511.106392621994, 'total_duration': 65129.65449762344, 'accumulated_submission_time': 39511.106392621994, 'accumulated_eval_time': 25613.393503665924, 'accumulated_logging_time': 1.575636625289917}
I0209 04:51:14.011224 140051013605120 logging_writer.py:48] [114356] accumulated_eval_time=25613.393504, accumulated_logging_time=1.575637, accumulated_submission_time=39511.106393, global_step=114356, preemption_count=0, score=39511.106393, test/accuracy=0.708001, test/bleu=30.880743, test/loss=1.330293, test/num_examples=3003, total_duration=65129.654498, train/accuracy=0.700859, train/bleu=36.201180, train/loss=1.381650, validation/accuracy=0.691498, validation/bleu=30.521495, validation/loss=1.416844, validation/num_examples=3000
I0209 04:51:29.502340 140051005212416 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.7039113640785217, loss=1.4245953559875488
I0209 04:52:03.907295 140051013605120 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.7057203650474548, loss=1.5051919221878052
I0209 04:52:38.419890 140051005212416 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.7123668789863586, loss=1.4109121561050415
I0209 04:53:12.966905 140051013605120 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.6979916095733643, loss=1.3753947019577026
I0209 04:53:47.505548 140051005212416 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6852039694786072, loss=1.4004900455474854
I0209 04:54:22.032038 140051013605120 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.69357830286026, loss=1.4215657711029053
I0209 04:54:56.607579 140051005212416 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.6657635569572449, loss=1.3840593099594116
I0209 04:55:31.161848 140051013605120 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6767029762268066, loss=1.3748961687088013
I0209 04:56:05.704807 140051005212416 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.7467888593673706, loss=1.4389969110488892
I0209 04:56:40.254230 140051013605120 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.7289900779724121, loss=1.4497994184494019
I0209 04:57:14.796197 140051005212416 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.718016505241394, loss=1.4453173875808716
I0209 04:57:49.346826 140051013605120 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.7174760103225708, loss=1.4724844694137573
I0209 04:58:23.921892 140051005212416 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.6795515418052673, loss=1.4353203773498535
I0209 04:58:58.430640 140051013605120 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.7079203128814697, loss=1.4551708698272705
I0209 04:59:32.980132 140051005212416 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.7260270714759827, loss=1.4140493869781494
I0209 05:00:07.530695 140051013605120 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.6485448479652405, loss=1.4220434427261353
I0209 05:00:42.055759 140051005212416 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.7245370745658875, loss=1.435673475265503
I0209 05:01:16.601054 140051013605120 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.7036901116371155, loss=1.3877967596054077
I0209 05:01:51.161098 140051005212416 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.7272317409515381, loss=1.3966779708862305
I0209 05:02:25.714723 140051013605120 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.6773259043693542, loss=1.4114007949829102
I0209 05:03:00.264630 140051005212416 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.714596688747406, loss=1.3293921947479248
I0209 05:03:34.807434 140051013605120 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6824530363082886, loss=1.4097274541854858
I0209 05:04:09.358979 140051005212416 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.6906182765960693, loss=1.4275054931640625
I0209 05:04:43.871399 140051013605120 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7141227126121521, loss=1.352128267288208
I0209 05:05:14.311243 140225696298816 spec.py:321] Evaluating on the training split.
I0209 05:05:17.295319 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:08:57.776998 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 05:09:00.450032 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:11:38.745065 140225696298816 spec.py:349] Evaluating on the test split.
I0209 05:11:41.431554 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:14:01.582031 140225696298816 submission_runner.py:408] Time since start: 66497.26s, 	Step: 116790, 	{'train/accuracy': 0.7000380158424377, 'train/loss': 1.39041268825531, 'train/bleu': 35.78679107524839, 'validation/accuracy': 0.6912747621536255, 'validation/loss': 1.4197064638137817, 'validation/bleu': 30.574397190601577, 'validation/num_examples': 3000, 'test/accuracy': 0.7080704569816589, 'test/loss': 1.329336166381836, 'test/bleu': 30.986274615235534, 'test/num_examples': 3003, 'score': 40351.31870722771, 'total_duration': 66497.25703215599, 'accumulated_submission_time': 40351.31870722771, 'accumulated_eval_time': 26140.66422200203, 'accumulated_logging_time': 1.618021011352539}
I0209 05:14:01.613366 140051005212416 logging_writer.py:48] [116790] accumulated_eval_time=26140.664222, accumulated_logging_time=1.618021, accumulated_submission_time=40351.318707, global_step=116790, preemption_count=0, score=40351.318707, test/accuracy=0.708070, test/bleu=30.986275, test/loss=1.329336, test/num_examples=3003, total_duration=66497.257032, train/accuracy=0.700038, train/bleu=35.786791, train/loss=1.390413, validation/accuracy=0.691275, validation/bleu=30.574397, validation/loss=1.419706, validation/num_examples=3000
I0209 05:14:05.422236 140051013605120 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.727083146572113, loss=1.4017977714538574
I0209 05:14:39.839712 140051005212416 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.7349250912666321, loss=1.3989330530166626
I0209 05:15:14.365766 140051013605120 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.7268908023834229, loss=1.4445717334747314
I0209 05:15:48.896963 140051005212416 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.735945999622345, loss=1.4210436344146729
I0209 05:16:23.448484 140051013605120 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7549468874931335, loss=1.406481146812439
I0209 05:16:57.999166 140051005212416 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.7171639800071716, loss=1.4399724006652832
I0209 05:17:32.534242 140051013605120 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.7109751105308533, loss=1.4490156173706055
I0209 05:18:07.042717 140051005212416 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.7405000925064087, loss=1.4717687368392944
I0209 05:18:41.568040 140051013605120 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.7126083374023438, loss=1.3524147272109985
I0209 05:19:16.124246 140051005212416 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.6984708309173584, loss=1.3634114265441895
I0209 05:19:50.675147 140051013605120 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7065412402153015, loss=1.3906522989273071
I0209 05:20:25.220843 140051005212416 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7628297209739685, loss=1.3938473463058472
I0209 05:20:59.779478 140051013605120 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.7010628581047058, loss=1.3166395425796509
I0209 05:21:34.345830 140051005212416 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7181934118270874, loss=1.4122169017791748
I0209 05:22:08.899283 140051013605120 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.738121747970581, loss=1.3608986139297485
I0209 05:22:43.444905 140051005212416 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7260071635246277, loss=1.4328360557556152
I0209 05:23:17.967298 140051013605120 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.7105854153633118, loss=1.4035425186157227
I0209 05:23:52.494095 140051005212416 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.7367743253707886, loss=1.4076628684997559
I0209 05:24:27.070489 140051013605120 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.7199185490608215, loss=1.37680184841156
I0209 05:25:01.599797 140051005212416 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.7291475534439087, loss=1.4635982513427734
I0209 05:25:36.141948 140051013605120 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7201324701309204, loss=1.379219889640808
I0209 05:26:10.708700 140051005212416 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7407970428466797, loss=1.3505345582962036
I0209 05:26:45.271581 140051013605120 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7523111701011658, loss=1.4608709812164307
I0209 05:27:19.796415 140051005212416 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.7348428964614868, loss=1.3801171779632568
I0209 05:27:54.345813 140051013605120 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7500892281532288, loss=1.4928010702133179
I0209 05:28:01.681057 140225696298816 spec.py:321] Evaluating on the training split.
I0209 05:28:04.660753 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:31:14.006654 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 05:31:16.674885 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:33:56.919841 140225696298816 spec.py:349] Evaluating on the test split.
I0209 05:33:59.609052 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:36:31.743018 140225696298816 submission_runner.py:408] Time since start: 67847.42s, 	Step: 119223, 	{'train/accuracy': 0.7113429307937622, 'train/loss': 1.3300620317459106, 'train/bleu': 36.80936519873941, 'validation/accuracy': 0.6935933828353882, 'validation/loss': 1.412529468536377, 'validation/bleu': 30.943243782887972, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.3264704942703247, 'test/bleu': 31.189875956554665, 'test/num_examples': 3003, 'score': 41191.29754805565, 'total_duration': 67847.41802787781, 'accumulated_submission_time': 41191.29754805565, 'accumulated_eval_time': 26650.726126909256, 'accumulated_logging_time': 1.6613097190856934}
I0209 05:36:31.775103 140051005212416 logging_writer.py:48] [119223] accumulated_eval_time=26650.726127, accumulated_logging_time=1.661310, accumulated_submission_time=41191.297548, global_step=119223, preemption_count=0, score=41191.297548, test/accuracy=0.709093, test/bleu=31.189876, test/loss=1.326470, test/num_examples=3003, total_duration=67847.418028, train/accuracy=0.711343, train/bleu=36.809365, train/loss=1.330062, validation/accuracy=0.693593, validation/bleu=30.943244, validation/loss=1.412529, validation/num_examples=3000
I0209 05:36:58.637599 140051013605120 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.7222025394439697, loss=1.3766801357269287
I0209 05:37:33.168443 140051005212416 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7650801539421082, loss=1.4236009120941162
I0209 05:38:07.701604 140051013605120 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7468233704566956, loss=1.4089703559875488
I0209 05:38:42.220812 140051005212416 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7272624969482422, loss=1.406252145767212
I0209 05:39:16.731209 140051013605120 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7400392889976501, loss=1.4233745336532593
I0209 05:39:51.241563 140051005212416 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.723623514175415, loss=1.3342541456222534
I0209 05:40:25.781364 140051013605120 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.7303558588027954, loss=1.4048924446105957
I0209 05:41:00.304964 140051005212416 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7420565485954285, loss=1.3802918195724487
I0209 05:41:34.858782 140051013605120 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.7550500631332397, loss=1.4113880395889282
I0209 05:42:09.413568 140051005212416 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.7516644597053528, loss=1.3682750463485718
I0209 05:42:43.982081 140051013605120 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.7743736505508423, loss=1.4442908763885498
I0209 05:43:18.542504 140051005212416 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.7550731897354126, loss=1.4468375444412231
I0209 05:43:53.113410 140051013605120 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7327438592910767, loss=1.4062939882278442
I0209 05:44:27.668836 140051005212416 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.7438021302223206, loss=1.4663140773773193
I0209 05:45:02.211313 140051013605120 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.7580218315124512, loss=1.3819983005523682
I0209 05:45:36.759178 140051005212416 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7575240731239319, loss=1.37949538230896
I0209 05:46:11.308053 140051013605120 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.796836793422699, loss=1.4025592803955078
I0209 05:46:45.882607 140051005212416 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7656062841415405, loss=1.3346965312957764
I0209 05:47:20.452568 140051013605120 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7526893615722656, loss=1.3601053953170776
I0209 05:47:54.995511 140051005212416 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.7707183361053467, loss=1.4049664735794067
I0209 05:48:29.533847 140051013605120 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7467808723449707, loss=1.3090004920959473
I0209 05:49:04.071427 140051005212416 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.7958571314811707, loss=1.3694255352020264
I0209 05:49:38.620526 140051013605120 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7374908328056335, loss=1.3860605955123901
I0209 05:50:13.145312 140051005212416 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7140186429023743, loss=1.2896416187286377
I0209 05:50:31.879433 140225696298816 spec.py:321] Evaluating on the training split.
I0209 05:50:34.858487 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:54:34.635131 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 05:54:37.315909 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 05:57:38.902932 140225696298816 spec.py:349] Evaluating on the test split.
I0209 05:57:41.596412 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:00:12.295312 140225696298816 submission_runner.py:408] Time since start: 69267.97s, 	Step: 121656, 	{'train/accuracy': 0.707312285900116, 'train/loss': 1.3521544933319092, 'train/bleu': 36.62262186936308, 'validation/accuracy': 0.6932833790779114, 'validation/loss': 1.41163969039917, 'validation/bleu': 30.834937831447107, 'validation/num_examples': 3000, 'test/accuracy': 0.7095810770988464, 'test/loss': 1.3253158330917358, 'test/bleu': 30.906804638756277, 'test/num_examples': 3003, 'score': 42031.314935684204, 'total_duration': 69267.97033762932, 'accumulated_submission_time': 42031.314935684204, 'accumulated_eval_time': 27231.141949653625, 'accumulated_logging_time': 1.7042453289031982}
I0209 06:00:12.327830 140051013605120 logging_writer.py:48] [121656] accumulated_eval_time=27231.141950, accumulated_logging_time=1.704245, accumulated_submission_time=42031.314936, global_step=121656, preemption_count=0, score=42031.314936, test/accuracy=0.709581, test/bleu=30.906805, test/loss=1.325316, test/num_examples=3003, total_duration=69267.970338, train/accuracy=0.707312, train/bleu=36.622622, train/loss=1.352154, validation/accuracy=0.693283, validation/bleu=30.834938, validation/loss=1.411640, validation/num_examples=3000
I0209 06:00:27.794231 140051005212416 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.7430466413497925, loss=1.4053356647491455
I0209 06:01:02.219419 140051013605120 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7820233702659607, loss=1.4545884132385254
I0209 06:01:36.746195 140051005212416 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7495625615119934, loss=1.407217264175415
I0209 06:02:11.332919 140051013605120 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7782375812530518, loss=1.35132896900177
I0209 06:02:45.888523 140051005212416 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7713305950164795, loss=1.3018951416015625
I0209 06:03:20.439446 140051013605120 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.760391354560852, loss=1.4112731218338013
I0209 06:03:54.971251 140051005212416 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7610765695571899, loss=1.3550657033920288
I0209 06:04:29.526164 140051013605120 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.7898891568183899, loss=1.327903389930725
I0209 06:05:04.069512 140051005212416 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7365022301673889, loss=1.3381896018981934
I0209 06:05:38.594352 140051013605120 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.7447689175605774, loss=1.3741562366485596
I0209 06:06:13.157459 140051005212416 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7716305255889893, loss=1.3921104669570923
I0209 06:06:47.705176 140051013605120 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.754132866859436, loss=1.4195590019226074
I0209 06:07:22.254607 140051005212416 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7695579528808594, loss=1.3517134189605713
I0209 06:07:56.815958 140051013605120 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7737115621566772, loss=1.4377535581588745
I0209 06:08:31.341760 140051005212416 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.7814465761184692, loss=1.3896535634994507
I0209 06:09:05.897283 140051013605120 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7726154327392578, loss=1.3187799453735352
I0209 06:09:40.429485 140051005212416 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.7829884886741638, loss=1.3862833976745605
I0209 06:10:14.989526 140051013605120 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.775598406791687, loss=1.3817442655563354
I0209 06:10:49.543727 140051005212416 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.8241738677024841, loss=1.3421045541763306
I0209 06:11:24.101672 140051013605120 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.7907172441482544, loss=1.4274390935897827
I0209 06:11:58.635087 140051005212416 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.7639550566673279, loss=1.369402289390564
I0209 06:12:33.201611 140051013605120 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.7665458917617798, loss=1.346060872077942
I0209 06:13:07.747071 140051005212416 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7819687724113464, loss=1.366881251335144
I0209 06:13:42.314547 140051013605120 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7618885040283203, loss=1.3061773777008057
I0209 06:14:12.458459 140225696298816 spec.py:321] Evaluating on the training split.
I0209 06:14:15.430011 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:17:46.635178 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 06:17:49.319429 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:20:31.380846 140225696298816 spec.py:349] Evaluating on the test split.
I0209 06:20:34.075075 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:23:32.272677 140225696298816 submission_runner.py:408] Time since start: 70667.95s, 	Step: 124089, 	{'train/accuracy': 0.707473874092102, 'train/loss': 1.3492178916931152, 'train/bleu': 36.849319559218614, 'validation/accuracy': 0.6941513419151306, 'validation/loss': 1.4105970859527588, 'validation/bleu': 30.965560621091676, 'validation/num_examples': 3000, 'test/accuracy': 0.7108128666877747, 'test/loss': 1.3213181495666504, 'test/bleu': 31.108249855042047, 'test/num_examples': 3003, 'score': 42871.35903739929, 'total_duration': 70667.94770431519, 'accumulated_submission_time': 42871.35903739929, 'accumulated_eval_time': 27790.95612001419, 'accumulated_logging_time': 1.74674654006958}
I0209 06:23:32.305405 140051005212416 logging_writer.py:48] [124089] accumulated_eval_time=27790.956120, accumulated_logging_time=1.746747, accumulated_submission_time=42871.359037, global_step=124089, preemption_count=0, score=42871.359037, test/accuracy=0.710813, test/bleu=31.108250, test/loss=1.321318, test/num_examples=3003, total_duration=70667.947704, train/accuracy=0.707474, train/bleu=36.849320, train/loss=1.349218, validation/accuracy=0.694151, validation/bleu=30.965561, validation/loss=1.410597, validation/num_examples=3000
I0209 06:23:36.444264 140051013605120 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.7786094546318054, loss=1.3290573358535767
I0209 06:24:10.855255 140051005212416 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.7528982162475586, loss=1.3597314357757568
I0209 06:24:45.341033 140051013605120 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.7867733836174011, loss=1.322937250137329
I0209 06:25:19.892283 140051005212416 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.7826375365257263, loss=1.3960044384002686
I0209 06:25:54.441065 140051013605120 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7660093307495117, loss=1.4022228717803955
I0209 06:26:28.977042 140051005212416 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.7738966941833496, loss=1.3895654678344727
I0209 06:27:03.520107 140051013605120 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.8089472651481628, loss=1.3697689771652222
I0209 06:27:38.079405 140051005212416 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.8065854907035828, loss=1.40008544921875
I0209 06:28:12.623625 140051013605120 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.7575017213821411, loss=1.38669753074646
I0209 06:28:47.187699 140051005212416 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.7747520208358765, loss=1.3790082931518555
I0209 06:29:21.741312 140051013605120 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.7949168682098389, loss=1.3572285175323486
I0209 06:29:56.303141 140051005212416 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.770329475402832, loss=1.3110731840133667
I0209 06:30:30.848402 140051013605120 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.7588973641395569, loss=1.357330083847046
I0209 06:31:05.407100 140051005212416 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.7752876281738281, loss=1.4433813095092773
I0209 06:31:39.945189 140051013605120 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7993140816688538, loss=1.3861037492752075
I0209 06:32:14.492348 140051005212416 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.7792760729789734, loss=1.4076571464538574
I0209 06:32:49.073607 140051013605120 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.7687321901321411, loss=1.3497806787490845
I0209 06:33:23.629125 140051005212416 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7778302431106567, loss=1.3903398513793945
I0209 06:33:58.171363 140051013605120 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7796265482902527, loss=1.3716108798980713
I0209 06:34:32.711184 140051005212416 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7946975231170654, loss=1.3687952756881714
I0209 06:35:07.296175 140051013605120 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.7613393664360046, loss=1.388010859489441
I0209 06:35:41.836818 140051005212416 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.7768337726593018, loss=1.3306821584701538
I0209 06:36:16.380083 140051013605120 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.7869354486465454, loss=1.4796946048736572
I0209 06:36:50.933334 140051005212416 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.7801779508590698, loss=1.381536841392517
I0209 06:37:25.492052 140051013605120 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.749374508857727, loss=1.3594286441802979
I0209 06:37:32.471980 140225696298816 spec.py:321] Evaluating on the training split.
I0209 06:37:35.445357 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:41:03.296610 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 06:41:05.991055 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:43:59.026291 140225696298816 spec.py:349] Evaluating on the test split.
I0209 06:44:01.715394 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 06:46:49.768554 140225696298816 submission_runner.py:408] Time since start: 72065.44s, 	Step: 126522, 	{'train/accuracy': 0.7100922465324402, 'train/loss': 1.3384053707122803, 'train/bleu': 37.03802375742551, 'validation/accuracy': 0.6936057806015015, 'validation/loss': 1.4109114408493042, 'validation/bleu': 30.960405634997983, 'validation/num_examples': 3000, 'test/accuracy': 0.7102434635162354, 'test/loss': 1.3205872774124146, 'test/bleu': 31.24458192290439, 'test/num_examples': 3003, 'score': 43711.439470529556, 'total_duration': 72065.4435763359, 'accumulated_submission_time': 43711.439470529556, 'accumulated_eval_time': 28348.25263595581, 'accumulated_logging_time': 1.7892396450042725}
I0209 06:46:49.802618 140051005212416 logging_writer.py:48] [126522] accumulated_eval_time=28348.252636, accumulated_logging_time=1.789240, accumulated_submission_time=43711.439471, global_step=126522, preemption_count=0, score=43711.439471, test/accuracy=0.710243, test/bleu=31.244582, test/loss=1.320587, test/num_examples=3003, total_duration=72065.443576, train/accuracy=0.710092, train/bleu=37.038024, train/loss=1.338405, validation/accuracy=0.693606, validation/bleu=30.960406, validation/loss=1.410911, validation/num_examples=3000
I0209 06:47:16.966705 140051013605120 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.7610141634941101, loss=1.2907358407974243
I0209 06:47:51.416870 140051005212416 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.7653846144676208, loss=1.2851567268371582
I0209 06:48:25.931600 140051013605120 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.8270453810691833, loss=1.3555359840393066
I0209 06:49:00.454170 140051005212416 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7645715475082397, loss=1.3068537712097168
I0209 06:49:34.995724 140051013605120 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7763684988021851, loss=1.305314540863037
I0209 06:50:09.552898 140051005212416 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.7709277272224426, loss=1.3288942575454712
I0209 06:50:44.087601 140051013605120 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.7482085227966309, loss=1.3213635683059692
I0209 06:51:18.667381 140051005212416 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7637960910797119, loss=1.380508303642273
I0209 06:51:53.194585 140051013605120 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.7724189758300781, loss=1.3034099340438843
I0209 06:52:27.759484 140051005212416 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.7918981909751892, loss=1.4089962244033813
I0209 06:53:02.295700 140051013605120 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7701340913772583, loss=1.3568367958068848
I0209 06:53:36.834162 140051005212416 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.7811570167541504, loss=1.3695095777511597
I0209 06:54:11.348184 140051013605120 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.7944654822349548, loss=1.3373678922653198
I0209 06:54:45.896748 140051005212416 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.7748861312866211, loss=1.3257158994674683
I0209 06:55:20.469810 140051013605120 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.7852339148521423, loss=1.3676894903182983
I0209 06:55:55.010927 140051005212416 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.7748600244522095, loss=1.4017544984817505
I0209 06:56:29.557336 140051013605120 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.7640586495399475, loss=1.3875635862350464
I0209 06:57:04.123614 140051005212416 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.790566623210907, loss=1.4025707244873047
I0209 06:57:38.681506 140051013605120 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.7947928309440613, loss=1.3382285833358765
I0209 06:58:13.245962 140051005212416 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.7663744688034058, loss=1.3583016395568848
I0209 06:58:47.808944 140051013605120 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.7759485840797424, loss=1.3943861722946167
I0209 06:59:22.358057 140051005212416 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.8102848529815674, loss=1.3722844123840332
I0209 06:59:56.901072 140051013605120 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.7775143384933472, loss=1.387243628501892
I0209 07:00:31.462981 140051005212416 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.768631637096405, loss=1.364630103111267
I0209 07:00:49.844154 140225696298816 spec.py:321] Evaluating on the training split.
I0209 07:00:52.820710 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:04:47.917165 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 07:04:50.590542 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:07:34.474820 140225696298816 spec.py:349] Evaluating on the test split.
I0209 07:07:37.159474 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:10:19.611157 140225696298816 submission_runner.py:408] Time since start: 73475.29s, 	Step: 128955, 	{'train/accuracy': 0.7126824855804443, 'train/loss': 1.3262940645217896, 'train/bleu': 36.65160501353665, 'validation/accuracy': 0.6942629218101501, 'validation/loss': 1.4080395698547363, 'validation/bleu': 31.022167451861694, 'validation/num_examples': 3000, 'test/accuracy': 0.7098715901374817, 'test/loss': 1.3178081512451172, 'test/bleu': 31.098821084681767, 'test/num_examples': 3003, 'score': 44551.3946352005, 'total_duration': 73475.28617930412, 'accumulated_submission_time': 44551.3946352005, 'accumulated_eval_time': 28918.01958155632, 'accumulated_logging_time': 1.8332068920135498}
I0209 07:10:19.644356 140051013605120 logging_writer.py:48] [128955] accumulated_eval_time=28918.019582, accumulated_logging_time=1.833207, accumulated_submission_time=44551.394635, global_step=128955, preemption_count=0, score=44551.394635, test/accuracy=0.709872, test/bleu=31.098821, test/loss=1.317808, test/num_examples=3003, total_duration=73475.286179, train/accuracy=0.712682, train/bleu=36.651605, train/loss=1.326294, validation/accuracy=0.694263, validation/bleu=31.022167, validation/loss=1.408040, validation/num_examples=3000
I0209 07:10:35.474503 140051005212416 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.7978132367134094, loss=1.3763785362243652
I0209 07:11:09.855278 140051013605120 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.8364593982696533, loss=1.3128243684768677
I0209 07:11:44.358553 140051005212416 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.8077424168586731, loss=1.3683292865753174
I0209 07:12:18.880306 140051013605120 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.7869020104408264, loss=1.3555656671524048
I0209 07:12:53.423955 140051005212416 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.7600180506706238, loss=1.3199703693389893
I0209 07:13:27.965423 140051013605120 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7702386975288391, loss=1.32053804397583
I0209 07:14:02.505474 140051005212416 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.7543604969978333, loss=1.3244924545288086
I0209 07:14:37.057435 140051013605120 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.7961598634719849, loss=1.371973991394043
I0209 07:15:11.597354 140051005212416 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.7795013189315796, loss=1.3136736154556274
I0209 07:15:46.137287 140051013605120 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7717003226280212, loss=1.4056038856506348
I0209 07:16:20.664221 140051005212416 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.8025444149971008, loss=1.2983720302581787
I0209 07:16:55.222752 140051013605120 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7766337394714355, loss=1.3687011003494263
I0209 07:17:29.764591 140051005212416 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.777391254901886, loss=1.3926335573196411
I0209 07:18:04.295973 140051013605120 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.778835117816925, loss=1.4244359731674194
I0209 07:18:38.851053 140051005212416 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7894734144210815, loss=1.3631905317306519
I0209 07:19:13.398553 140051013605120 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.7989532947540283, loss=1.3720979690551758
I0209 07:19:47.930578 140051005212416 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7674481868743896, loss=1.3583900928497314
I0209 07:20:22.444537 140051013605120 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7904552221298218, loss=1.3483679294586182
I0209 07:20:57.000055 140051005212416 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.7904304265975952, loss=1.408744215965271
I0209 07:21:31.563392 140051013605120 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.7701541185379028, loss=1.327600359916687
I0209 07:22:06.121604 140051005212416 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7593056559562683, loss=1.3054951429367065
I0209 07:22:40.668775 140051013605120 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.7689815163612366, loss=1.3860377073287964
I0209 07:23:15.223452 140051005212416 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.7827505469322205, loss=1.394533395767212
I0209 07:23:49.762827 140051013605120 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.7980574369430542, loss=1.4282840490341187
I0209 07:24:19.904996 140225696298816 spec.py:321] Evaluating on the training split.
I0209 07:24:22.883158 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:28:08.192127 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 07:28:10.866736 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:31:04.721008 140225696298816 spec.py:349] Evaluating on the test split.
I0209 07:31:07.401027 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:33:49.302573 140225696298816 submission_runner.py:408] Time since start: 74884.98s, 	Step: 131389, 	{'train/accuracy': 0.712949275970459, 'train/loss': 1.319365382194519, 'train/bleu': 36.297809192303134, 'validation/accuracy': 0.6945109367370605, 'validation/loss': 1.4088495969772339, 'validation/bleu': 30.947350711661613, 'validation/num_examples': 3000, 'test/accuracy': 0.7102085947990417, 'test/loss': 1.3181660175323486, 'test/bleu': 31.13512948906848, 'test/num_examples': 3003, 'score': 45391.56788253784, 'total_duration': 74884.97759580612, 'accumulated_submission_time': 45391.56788253784, 'accumulated_eval_time': 29487.417108535767, 'accumulated_logging_time': 1.8770146369934082}
I0209 07:33:49.336481 140051005212416 logging_writer.py:48] [131389] accumulated_eval_time=29487.417109, accumulated_logging_time=1.877015, accumulated_submission_time=45391.567883, global_step=131389, preemption_count=0, score=45391.567883, test/accuracy=0.710209, test/bleu=31.135129, test/loss=1.318166, test/num_examples=3003, total_duration=74884.977596, train/accuracy=0.712949, train/bleu=36.297809, train/loss=1.319365, validation/accuracy=0.694511, validation/bleu=30.947351, validation/loss=1.408850, validation/num_examples=3000
I0209 07:33:53.478461 140051013605120 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.784812867641449, loss=1.3441764116287231
I0209 07:34:27.848583 140051005212416 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7993210554122925, loss=1.3539851903915405
I0209 07:35:02.320138 140051013605120 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.7975886464118958, loss=1.3606265783309937
I0209 07:35:36.848825 140051005212416 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.8077972531318665, loss=1.379081130027771
I0209 07:36:11.361294 140051013605120 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7888184189796448, loss=1.379475712776184
I0209 07:36:45.907515 140051005212416 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.773621141910553, loss=1.296918272972107
I0209 07:37:20.473092 140051013605120 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7952101230621338, loss=1.3892676830291748
I0209 07:37:55.018983 140051005212416 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.7848621606826782, loss=1.287182092666626
I0209 07:38:29.541113 140051013605120 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.7681729793548584, loss=1.363020658493042
I0209 07:39:04.089184 140051005212416 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.7800352573394775, loss=1.357866644859314
I0209 07:39:38.652256 140051013605120 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.8061739802360535, loss=1.402773380279541
I0209 07:40:13.189384 140051005212416 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.771565318107605, loss=1.3752555847167969
I0209 07:40:47.739673 140051013605120 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.7765017151832581, loss=1.3301100730895996
I0209 07:41:22.285353 140051005212416 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7835244536399841, loss=1.381482481956482
I0209 07:41:56.824174 140051013605120 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.7816184163093567, loss=1.4152424335479736
I0209 07:42:31.398079 140051005212416 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.7617484927177429, loss=1.4289442300796509
I0209 07:43:05.941715 140051013605120 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.7714738845825195, loss=1.3942657709121704
I0209 07:43:40.461467 140051005212416 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.7772716283798218, loss=1.3298505544662476
I0209 07:44:15.017626 140051013605120 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.7775488495826721, loss=1.344481348991394
I0209 07:44:49.567686 140051005212416 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7642447352409363, loss=1.4284051656723022
I0209 07:45:00.373943 140225696298816 spec.py:321] Evaluating on the training split.
I0209 07:45:03.354945 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:48:52.979084 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 07:48:55.665499 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:51:50.619019 140225696298816 spec.py:349] Evaluating on the test split.
I0209 07:51:53.300558 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 07:54:38.248533 140225696298816 submission_runner.py:408] Time since start: 76133.92s, 	Step: 133333, 	{'train/accuracy': 0.7099116444587708, 'train/loss': 1.3356733322143555, 'train/bleu': 37.02394184889104, 'validation/accuracy': 0.6946597099304199, 'validation/loss': 1.4091264009475708, 'validation/bleu': 31.011342101754487, 'validation/num_examples': 3000, 'test/accuracy': 0.7103132009506226, 'test/loss': 1.3183356523513794, 'test/bleu': 31.18299976012298, 'test/num_examples': 3003, 'score': 46062.535388469696, 'total_duration': 76133.92356038094, 'accumulated_submission_time': 46062.535388469696, 'accumulated_eval_time': 30065.29164481163, 'accumulated_logging_time': 1.9203734397888184}
I0209 07:54:38.282700 140051013605120 logging_writer.py:48] [133333] accumulated_eval_time=30065.291645, accumulated_logging_time=1.920373, accumulated_submission_time=46062.535388, global_step=133333, preemption_count=0, score=46062.535388, test/accuracy=0.710313, test/bleu=31.183000, test/loss=1.318336, test/num_examples=3003, total_duration=76133.923560, train/accuracy=0.709912, train/bleu=37.023942, train/loss=1.335673, validation/accuracy=0.694660, validation/bleu=31.011342, validation/loss=1.409126, validation/num_examples=3000
I0209 07:54:38.317644 140051005212416 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46062.535388
I0209 07:54:39.553481 140225696298816 checkpoints.py:490] Saving checkpoint at step: 133333
I0209 07:54:43.623304 140225696298816 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3/checkpoint_133333
I0209 07:54:43.628319 140225696298816 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_3/checkpoint_133333.
I0209 07:54:43.680344 140225696298816 submission_runner.py:583] Tuning trial 3/5
I0209 07:54:43.680517 140225696298816 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0209 07:54:43.690749 140225696298816 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006154337315820158, 'train/loss': 11.025583267211914, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.784578561782837, 'total_duration': 883.0645732879639, 'accumulated_submission_time': 26.784578561782837, 'accumulated_eval_time': 856.2799527645111, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2429, {'train/accuracy': 0.4166429340839386, 'train/loss': 3.9009313583374023, 'train/bleu': 14.596829131524013, 'validation/accuracy': 0.4004042148590088, 'validation/loss': 4.036395072937012, 'validation/bleu': 9.782566022877925, 'validation/num_examples': 3000, 'test/accuracy': 0.38750800490379333, 'test/loss': 4.218213081359863, 'test/bleu': 8.298912293063825, 'test/num_examples': 3003, 'score': 866.7384278774261, 'total_duration': 2260.7427303791046, 'accumulated_submission_time': 866.7384278774261, 'accumulated_eval_time': 1393.9082560539246, 'accumulated_logging_time': 0.019158124923706055, 'global_step': 2429, 'preemption_count': 0}), (4857, {'train/accuracy': 0.5418379306793213, 'train/loss': 2.6729602813720703, 'train/bleu': 24.88816508460163, 'validation/accuracy': 0.5444941520690918, 'validation/loss': 2.6168112754821777, 'validation/bleu': 20.575766431959572, 'validation/num_examples': 3000, 'test/accuracy': 0.5460693836212158, 'test/loss': 2.6440484523773193, 'test/bleu': 19.060328733347866, 'test/num_examples': 3003, 'score': 1706.9111173152924, 'total_duration': 3557.023582458496, 'accumulated_submission_time': 1706.9111173152924, 'accumulated_eval_time': 1849.9122297763824, 'accumulated_logging_time': 0.04645705223083496, 'global_step': 4857, 'preemption_count': 0}), (7286, {'train/accuracy': 0.5796564817428589, 'train/loss': 2.280468225479126, 'train/bleu': 27.28085169810225, 'validation/accuracy': 0.5861427783966064, 'validation/loss': 2.215071201324463, 'validation/bleu': 23.4061450554374, 'validation/num_examples': 3000, 'test/accuracy': 0.5889140963554382, 'test/loss': 2.2054107189178467, 'test/bleu': 21.840320736237747, 'test/num_examples': 3003, 'score': 2547.001212835312, 'total_duration': 4851.010681629181, 'accumulated_submission_time': 2547.001212835312, 'accumulated_eval_time': 2303.706892490387, 'accumulated_logging_time': 0.07251095771789551, 'global_step': 7286, 'preemption_count': 0}), (9716, {'train/accuracy': 0.5948212146759033, 'train/loss': 2.13262677192688, 'train/bleu': 28.156699199500373, 'validation/accuracy': 0.606229305267334, 'validation/loss': 2.028346538543701, 'validation/bleu': 24.546534607890653, 'validation/num_examples': 3000, 'test/accuracy': 0.6107489466667175, 'test/loss': 2.0003881454467773, 'test/bleu': 23.531982121236606, 'test/num_examples': 3003, 'score': 3387.1982963085175, 'total_duration': 6122.1344566345215, 'accumulated_submission_time': 3387.1982963085175, 'accumulated_eval_time': 2734.530050754547, 'accumulated_logging_time': 0.0988461971282959, 'global_step': 9716, 'preemption_count': 0}), (12147, {'train/accuracy': 0.6030737161636353, 'train/loss': 2.049139976501465, 'train/bleu': 29.213199924869418, 'validation/accuracy': 0.6187896132469177, 'validation/loss': 1.9111295938491821, 'validation/bleu': 24.313988813799572, 'validation/num_examples': 3000, 'test/accuracy': 0.6287490725517273, 'test/loss': 1.8641327619552612, 'test/bleu': 24.73216670379908, 'test/num_examples': 3003, 'score': 4227.132491111755, 'total_duration': 7626.200456619263, 'accumulated_submission_time': 4227.132491111755, 'accumulated_eval_time': 3398.5560586452484, 'accumulated_logging_time': 0.12773990631103516, 'global_step': 12147, 'preemption_count': 0}), (14577, {'train/accuracy': 0.6107993721961975, 'train/loss': 1.964206337928772, 'train/bleu': 29.97132197779252, 'validation/accuracy': 0.6309283375740051, 'validation/loss': 1.819167971611023, 'validation/bleu': 26.438481201404674, 'validation/num_examples': 3000, 'test/accuracy': 0.6393702030181885, 'test/loss': 1.7721221446990967, 'test/bleu': 25.362275219367604, 'test/num_examples': 3003, 'score': 5067.12614607811, 'total_duration': 8947.63675236702, 'accumulated_submission_time': 5067.12614607811, 'accumulated_eval_time': 3879.895225048065, 'accumulated_logging_time': 0.15470433235168457, 'global_step': 14577, 'preemption_count': 0}), (17009, {'train/accuracy': 0.6224886775016785, 'train/loss': 1.8965948820114136, 'train/bleu': 30.31643486549434, 'validation/accuracy': 0.6383429765701294, 'validation/loss': 1.756270408630371, 'validation/bleu': 26.823648244559074, 'validation/num_examples': 3000, 'test/accuracy': 0.6478647589683533, 'test/loss': 1.6986522674560547, 'test/bleu': 26.263295660080857, 'test/num_examples': 3003, 'score': 5907.2546446323395, 'total_duration': 10229.260805130005, 'accumulated_submission_time': 5907.2546446323395, 'accumulated_eval_time': 4321.288250684738, 'accumulated_logging_time': 0.1813514232635498, 'global_step': 17009, 'preemption_count': 0}), (19441, {'train/accuracy': 0.6325414180755615, 'train/loss': 1.800859808921814, 'train/bleu': 30.4674639846037, 'validation/accuracy': 0.6434885859489441, 'validation/loss': 1.7217135429382324, 'validation/bleu': 27.554292710135996, 'validation/num_examples': 3000, 'test/accuracy': 0.6533263921737671, 'test/loss': 1.6645915508270264, 'test/bleu': 26.58671307572002, 'test/num_examples': 3003, 'score': 6747.339457988739, 'total_duration': 11535.906452178955, 'accumulated_submission_time': 6747.339457988739, 'accumulated_eval_time': 4787.744311332703, 'accumulated_logging_time': 0.20973849296569824, 'global_step': 19441, 'preemption_count': 0}), (21873, {'train/accuracy': 0.6281017065048218, 'train/loss': 1.841070532798767, 'train/bleu': 30.706210179092377, 'validation/accuracy': 0.6485598087310791, 'validation/loss': 1.6945736408233643, 'validation/bleu': 27.498524011031034, 'validation/num_examples': 3000, 'test/accuracy': 0.6594852209091187, 'test/loss': 1.6314548254013062, 'test/bleu': 27.107368915175847, 'test/num_examples': 3003, 'score': 7587.3537764549255, 'total_duration': 12861.323557853699, 'accumulated_submission_time': 7587.3537764549255, 'accumulated_eval_time': 5273.040806770325, 'accumulated_logging_time': 0.2392292022705078, 'global_step': 21873, 'preemption_count': 0}), (24306, {'train/accuracy': 0.6283778548240662, 'train/loss': 1.8341097831726074, 'train/bleu': 30.231787337608484, 'validation/accuracy': 0.6494525671005249, 'validation/loss': 1.675093650817871, 'validation/bleu': 27.867396431428066, 'validation/num_examples': 3000, 'test/accuracy': 0.6609842777252197, 'test/loss': 1.61290442943573, 'test/bleu': 27.086848862449706, 'test/num_examples': 3003, 'score': 8427.538682699203, 'total_duration': 14183.1789124012, 'accumulated_submission_time': 8427.538682699203, 'accumulated_eval_time': 5754.607835054398, 'accumulated_logging_time': 0.26638150215148926, 'global_step': 24306, 'preemption_count': 0}), (26739, {'train/accuracy': 0.6350250840187073, 'train/loss': 1.7813962697982788, 'train/bleu': 31.10056987152565, 'validation/accuracy': 0.6521431803703308, 'validation/loss': 1.6590827703475952, 'validation/bleu': 27.86187178986665, 'validation/num_examples': 3000, 'test/accuracy': 0.661495566368103, 'test/loss': 1.5966767072677612, 'test/bleu': 27.351616233989287, 'test/num_examples': 3003, 'score': 9267.650310277939, 'total_duration': 15478.620971679688, 'accumulated_submission_time': 9267.650310277939, 'accumulated_eval_time': 6209.833786487579, 'accumulated_logging_time': 0.29460883140563965, 'global_step': 26739, 'preemption_count': 0}), (29173, {'train/accuracy': 0.6313155889511108, 'train/loss': 1.8113305568695068, 'train/bleu': 31.167266657108822, 'validation/accuracy': 0.6551065444946289, 'validation/loss': 1.6476982831954956, 'validation/bleu': 28.01659926041504, 'validation/num_examples': 3000, 'test/accuracy': 0.6651443839073181, 'test/loss': 1.5824037790298462, 'test/bleu': 27.611150169070616, 'test/num_examples': 3003, 'score': 10107.838258981705, 'total_duration': 16798.87334752083, 'accumulated_submission_time': 10107.838258981705, 'accumulated_eval_time': 6689.792566776276, 'accumulated_logging_time': 0.323559045791626, 'global_step': 29173, 'preemption_count': 0}), (31607, {'train/accuracy': 0.6526026129722595, 'train/loss': 1.652001142501831, 'train/bleu': 32.13943278256692, 'validation/accuracy': 0.657090425491333, 'validation/loss': 1.6380786895751953, 'validation/bleu': 28.228548539202578, 'validation/num_examples': 3000, 'test/accuracy': 0.6655162572860718, 'test/loss': 1.5741702318191528, 'test/bleu': 27.550129631044033, 'test/num_examples': 3003, 'score': 10948.034049272537, 'total_duration': 18158.23977446556, 'accumulated_submission_time': 10948.034049272537, 'accumulated_eval_time': 7208.857345581055, 'accumulated_logging_time': 0.35329389572143555, 'global_step': 31607, 'preemption_count': 0}), (34040, {'train/accuracy': 0.6393951773643494, 'train/loss': 1.768338680267334, 'train/bleu': 31.008950206319334, 'validation/accuracy': 0.6554041504859924, 'validation/loss': 1.6242969036102295, 'validation/bleu': 28.08600000210978, 'validation/num_examples': 3000, 'test/accuracy': 0.6660507917404175, 'test/loss': 1.565197229385376, 'test/bleu': 27.395916308501732, 'test/num_examples': 3003, 'score': 11787.991804122925, 'total_duration': 19519.91458582878, 'accumulated_submission_time': 11787.991804122925, 'accumulated_eval_time': 7730.468078613281, 'accumulated_logging_time': 0.3834218978881836, 'global_step': 34040, 'preemption_count': 0}), (36474, {'train/accuracy': 0.6336399912834167, 'train/loss': 1.7970607280731201, 'train/bleu': 31.23075734922129, 'validation/accuracy': 0.6571400165557861, 'validation/loss': 1.6196331977844238, 'validation/bleu': 28.072348613153586, 'validation/num_examples': 3000, 'test/accuracy': 0.667793869972229, 'test/loss': 1.5579041242599487, 'test/bleu': 27.544555632291114, 'test/num_examples': 3003, 'score': 12628.087430715561, 'total_duration': 20997.033695459366, 'accumulated_submission_time': 12628.087430715561, 'accumulated_eval_time': 8367.384491205215, 'accumulated_logging_time': 0.41417574882507324, 'global_step': 36474, 'preemption_count': 0}), (38908, {'train/accuracy': 0.6437124013900757, 'train/loss': 1.718425989151001, 'train/bleu': 31.870111946059616, 'validation/accuracy': 0.6600537896156311, 'validation/loss': 1.6126872301101685, 'validation/bleu': 28.302752187830926, 'validation/num_examples': 3000, 'test/accuracy': 0.6705943942070007, 'test/loss': 1.5488643646240234, 'test/bleu': 27.784236811734953, 'test/num_examples': 3003, 'score': 13468.063895463943, 'total_duration': 22358.86438035965, 'accumulated_submission_time': 13468.063895463943, 'accumulated_eval_time': 8889.13286614418, 'accumulated_logging_time': 0.4446568489074707, 'global_step': 38908, 'preemption_count': 0}), (41342, {'train/accuracy': 0.6403557658195496, 'train/loss': 1.7567963600158691, 'train/bleu': 31.643399674411956, 'validation/accuracy': 0.659694254398346, 'validation/loss': 1.6073123216629028, 'validation/bleu': 27.765434272920903, 'validation/num_examples': 3000, 'test/accuracy': 0.6712451577186584, 'test/loss': 1.5380420684814453, 'test/bleu': 27.958816788714334, 'test/num_examples': 3003, 'score': 14308.293565273285, 'total_duration': 23799.416815519333, 'accumulated_submission_time': 14308.293565273285, 'accumulated_eval_time': 9489.347652196884, 'accumulated_logging_time': 0.47722625732421875, 'global_step': 41342, 'preemption_count': 0}), (43776, {'train/accuracy': 0.6402798295021057, 'train/loss': 1.7504782676696777, 'train/bleu': 31.42158195384738, 'validation/accuracy': 0.6611325144767761, 'validation/loss': 1.5975406169891357, 'validation/bleu': 28.680036466352934, 'validation/num_examples': 3000, 'test/accuracy': 0.6745337247848511, 'test/loss': 1.532659888267517, 'test/bleu': 27.9198714817124, 'test/num_examples': 3003, 'score': 15148.395472049713, 'total_duration': 25152.873359441757, 'accumulated_submission_time': 15148.395472049713, 'accumulated_eval_time': 10002.591368198395, 'accumulated_logging_time': 0.5125997066497803, 'global_step': 43776, 'preemption_count': 0}), (46210, {'train/accuracy': 0.6428989768028259, 'train/loss': 1.7245334386825562, 'train/bleu': 31.507355253150934, 'validation/accuracy': 0.6620624661445618, 'validation/loss': 1.5930095911026, 'validation/bleu': 28.495653401907823, 'validation/num_examples': 3000, 'test/accuracy': 0.6745337247848511, 'test/loss': 1.5249536037445068, 'test/bleu': 28.18982572482699, 'test/num_examples': 3003, 'score': 15988.560994148254, 'total_duration': 26514.822845697403, 'accumulated_submission_time': 15988.560994148254, 'accumulated_eval_time': 10524.269515752792, 'accumulated_logging_time': 0.5430536270141602, 'global_step': 46210, 'preemption_count': 0}), (48644, {'train/accuracy': 0.6434541940689087, 'train/loss': 1.7303590774536133, 'train/bleu': 31.336669119899753, 'validation/accuracy': 0.6629676222801208, 'validation/loss': 1.5866459608078003, 'validation/bleu': 28.315392529444374, 'validation/num_examples': 3000, 'test/accuracy': 0.6765324473381042, 'test/loss': 1.513628602027893, 'test/bleu': 28.04292709290652, 'test/num_examples': 3003, 'score': 16828.613729715347, 'total_duration': 27868.871577739716, 'accumulated_submission_time': 16828.613729715347, 'accumulated_eval_time': 11038.159530639648, 'accumulated_logging_time': 0.5735483169555664, 'global_step': 48644, 'preemption_count': 0}), (51077, {'train/accuracy': 0.6509331464767456, 'train/loss': 1.667337417602539, 'train/bleu': 32.02152703215946, 'validation/accuracy': 0.6633147597312927, 'validation/loss': 1.5808254480361938, 'validation/bleu': 28.656745396147187, 'validation/num_examples': 3000, 'test/accuracy': 0.6761838793754578, 'test/loss': 1.5135340690612793, 'test/bleu': 28.28753882239032, 'test/num_examples': 3003, 'score': 17668.511252641678, 'total_duration': 29197.242948055267, 'accumulated_submission_time': 17668.511252641678, 'accumulated_eval_time': 11526.525929927826, 'accumulated_logging_time': 0.6058268547058105, 'global_step': 51077, 'preemption_count': 0}), (53511, {'train/accuracy': 0.6447697877883911, 'train/loss': 1.7171337604522705, 'train/bleu': 31.75867017336076, 'validation/accuracy': 0.6656085848808289, 'validation/loss': 1.5713614225387573, 'validation/bleu': 28.81990449034723, 'validation/num_examples': 3000, 'test/accuracy': 0.6800999641418457, 'test/loss': 1.4995527267456055, 'test/bleu': 28.593444733008027, 'test/num_examples': 3003, 'score': 18508.527975320816, 'total_duration': 30602.46071600914, 'accumulated_submission_time': 18508.527975320816, 'accumulated_eval_time': 12091.620062828064, 'accumulated_logging_time': 0.637232780456543, 'global_step': 53511, 'preemption_count': 0}), (55945, {'train/accuracy': 0.6402867436408997, 'train/loss': 1.7420943975448608, 'train/bleu': 31.603690698245167, 'validation/accuracy': 0.6657822132110596, 'validation/loss': 1.5687559843063354, 'validation/bleu': 28.31823448140098, 'validation/num_examples': 3000, 'test/accuracy': 0.678345263004303, 'test/loss': 1.496812343597412, 'test/bleu': 28.487429038274655, 'test/num_examples': 3003, 'score': 19348.719173908234, 'total_duration': 32111.7632894516, 'accumulated_submission_time': 19348.719173908234, 'accumulated_eval_time': 12760.62174320221, 'accumulated_logging_time': 0.6705992221832275, 'global_step': 55945, 'preemption_count': 0}), (58379, {'train/accuracy': 0.651101291179657, 'train/loss': 1.6768395900726318, 'train/bleu': 31.862773994581925, 'validation/accuracy': 0.6660177707672119, 'validation/loss': 1.565895915031433, 'validation/bleu': 28.684740364153395, 'validation/num_examples': 3000, 'test/accuracy': 0.6788333058357239, 'test/loss': 1.4934437274932861, 'test/bleu': 28.461223409180462, 'test/num_examples': 3003, 'score': 20188.837403059006, 'total_duration': 33452.249553442, 'accumulated_submission_time': 20188.837403059006, 'accumulated_eval_time': 13260.881618976593, 'accumulated_logging_time': 0.7030997276306152, 'global_step': 58379, 'preemption_count': 0}), (60813, {'train/accuracy': 0.6452680826187134, 'train/loss': 1.7102110385894775, 'train/bleu': 31.57398436310127, 'validation/accuracy': 0.6664021611213684, 'validation/loss': 1.5667551755905151, 'validation/bleu': 28.685950258944292, 'validation/num_examples': 3000, 'test/accuracy': 0.6794724464416504, 'test/loss': 1.485634207725525, 'test/bleu': 28.543103089635263, 'test/num_examples': 3003, 'score': 21028.95820069313, 'total_duration': 34781.54425239563, 'accumulated_submission_time': 21028.95820069313, 'accumulated_eval_time': 13749.946232795715, 'accumulated_logging_time': 0.7364275455474854, 'global_step': 60813, 'preemption_count': 0}), (63247, {'train/accuracy': 0.6609110236167908, 'train/loss': 1.613291621208191, 'train/bleu': 32.737365321346005, 'validation/accuracy': 0.6695019006729126, 'validation/loss': 1.5515433549880981, 'validation/bleu': 29.214455853774165, 'validation/num_examples': 3000, 'test/accuracy': 0.6820870637893677, 'test/loss': 1.4776993989944458, 'test/bleu': 28.981448713141738, 'test/num_examples': 3003, 'score': 21869.133969783783, 'total_duration': 36111.008915662766, 'accumulated_submission_time': 21869.133969783783, 'accumulated_eval_time': 14239.12595629692, 'accumulated_logging_time': 0.7697396278381348, 'global_step': 63247, 'preemption_count': 0}), (65681, {'train/accuracy': 0.6530311703681946, 'train/loss': 1.671697735786438, 'train/bleu': 32.311872612330355, 'validation/accuracy': 0.6706550121307373, 'validation/loss': 1.538474202156067, 'validation/bleu': 28.821102985125254, 'validation/num_examples': 3000, 'test/accuracy': 0.6832723617553711, 'test/loss': 1.465322732925415, 'test/bleu': 28.777813891257313, 'test/num_examples': 3003, 'score': 22709.169924020767, 'total_duration': 37561.60833859444, 'accumulated_submission_time': 22709.169924020767, 'accumulated_eval_time': 14849.57773900032, 'accumulated_logging_time': 0.8056454658508301, 'global_step': 65681, 'preemption_count': 0}), (68115, {'train/accuracy': 0.6517167091369629, 'train/loss': 1.6799516677856445, 'train/bleu': 32.40043379300699, 'validation/accuracy': 0.670059859752655, 'validation/loss': 1.5349482297897339, 'validation/bleu': 28.841342669109036, 'validation/num_examples': 3000, 'test/accuracy': 0.6844460368156433, 'test/loss': 1.4604904651641846, 'test/bleu': 28.78482850239207, 'test/num_examples': 3003, 'score': 23549.336899280548, 'total_duration': 38971.38629126549, 'accumulated_submission_time': 23549.336899280548, 'accumulated_eval_time': 15419.075818538666, 'accumulated_logging_time': 0.8422815799713135, 'global_step': 68115, 'preemption_count': 0}), (70549, {'train/accuracy': 0.6591607332229614, 'train/loss': 1.6216074228286743, 'train/bleu': 32.18171612106351, 'validation/accuracy': 0.6709278225898743, 'validation/loss': 1.5275872945785522, 'validation/bleu': 28.927073459030193, 'validation/num_examples': 3000, 'test/accuracy': 0.6852594614028931, 'test/loss': 1.4523561000823975, 'test/bleu': 28.99412723224184, 'test/num_examples': 3003, 'score': 24389.565573453903, 'total_duration': 40268.55391001701, 'accumulated_submission_time': 24389.565573453903, 'accumulated_eval_time': 15875.903427124023, 'accumulated_logging_time': 0.8780360221862793, 'global_step': 70549, 'preemption_count': 0}), (72983, {'train/accuracy': 0.6545958518981934, 'train/loss': 1.6556434631347656, 'train/bleu': 32.03680376481512, 'validation/accuracy': 0.6736928224563599, 'validation/loss': 1.5203146934509277, 'validation/bleu': 29.4972412252912, 'validation/num_examples': 3000, 'test/accuracy': 0.6866422891616821, 'test/loss': 1.4460179805755615, 'test/bleu': 28.961118007307395, 'test/num_examples': 3003, 'score': 25229.58171772957, 'total_duration': 41573.54033780098, 'accumulated_submission_time': 25229.58171772957, 'accumulated_eval_time': 16340.762417078018, 'accumulated_logging_time': 0.9139454364776611, 'global_step': 72983, 'preemption_count': 0}), (75417, {'train/accuracy': 0.6752561330795288, 'train/loss': 1.5263938903808594, 'train/bleu': 33.863299321673196, 'validation/accuracy': 0.6756518483161926, 'validation/loss': 1.5147626399993896, 'validation/bleu': 29.306960178991066, 'validation/num_examples': 3000, 'test/accuracy': 0.6866190433502197, 'test/loss': 1.439462661743164, 'test/bleu': 29.221898187490247, 'test/num_examples': 3003, 'score': 26069.81137084961, 'total_duration': 42925.760655641556, 'accumulated_submission_time': 26069.81137084961, 'accumulated_eval_time': 16852.63940834999, 'accumulated_logging_time': 0.9512898921966553, 'global_step': 75417, 'preemption_count': 0}), (77851, {'train/accuracy': 0.6602402329444885, 'train/loss': 1.6101831197738647, 'train/bleu': 32.94359332980648, 'validation/accuracy': 0.6758378744125366, 'validation/loss': 1.5022680759429932, 'validation/bleu': 29.88321465163858, 'validation/num_examples': 3000, 'test/accuracy': 0.6893149614334106, 'test/loss': 1.4282652139663696, 'test/bleu': 29.199608789390265, 'test/num_examples': 3003, 'score': 26909.846825361252, 'total_duration': 44291.501447439194, 'accumulated_submission_time': 26909.846825361252, 'accumulated_eval_time': 17378.230769634247, 'accumulated_logging_time': 0.9887261390686035, 'global_step': 77851, 'preemption_count': 0}), (80285, {'train/accuracy': 0.6605494022369385, 'train/loss': 1.6196547746658325, 'train/bleu': 32.74820463146153, 'validation/accuracy': 0.6773629784584045, 'validation/loss': 1.497270107269287, 'validation/bleu': 29.766035010112983, 'validation/num_examples': 3000, 'test/accuracy': 0.6895241737365723, 'test/loss': 1.4248106479644775, 'test/bleu': 29.169380704529548, 'test/num_examples': 3003, 'score': 27750.07419347763, 'total_duration': 45631.012216091156, 'accumulated_submission_time': 27750.07419347763, 'accumulated_eval_time': 17877.399688482285, 'accumulated_logging_time': 1.0267176628112793, 'global_step': 80285, 'preemption_count': 0}), (82719, {'train/accuracy': 0.6689773201942444, 'train/loss': 1.5642541646957397, 'train/bleu': 33.41044851396017, 'validation/accuracy': 0.6781564950942993, 'validation/loss': 1.491786003112793, 'validation/bleu': 29.560270364388217, 'validation/num_examples': 3000, 'test/accuracy': 0.6920806765556335, 'test/loss': 1.4164384603500366, 'test/bleu': 29.278406234177744, 'test/num_examples': 3003, 'score': 28590.17192029953, 'total_duration': 47032.475782871246, 'accumulated_submission_time': 28590.17192029953, 'accumulated_eval_time': 18438.652660131454, 'accumulated_logging_time': 1.062680959701538, 'global_step': 82719, 'preemption_count': 0}), (85153, {'train/accuracy': 0.6641597151756287, 'train/loss': 1.5821653604507446, 'train/bleu': 32.891950379823754, 'validation/accuracy': 0.679582417011261, 'validation/loss': 1.4825611114501953, 'validation/bleu': 29.699337732733976, 'validation/num_examples': 3000, 'test/accuracy': 0.6939166784286499, 'test/loss': 1.4057952165603638, 'test/bleu': 29.56086936865955, 'test/num_examples': 3003, 'score': 29430.228848457336, 'total_duration': 48491.12983894348, 'accumulated_submission_time': 29430.228848457336, 'accumulated_eval_time': 19057.13529086113, 'accumulated_logging_time': 1.1003947257995605, 'global_step': 85153, 'preemption_count': 0}), (87587, {'train/accuracy': 0.6998754143714905, 'train/loss': 1.4042586088180542, 'train/bleu': 36.14299179613099, 'validation/accuracy': 0.6811694502830505, 'validation/loss': 1.4776188135147095, 'validation/bleu': 30.155293354746227, 'validation/num_examples': 3000, 'test/accuracy': 0.6963105201721191, 'test/loss': 1.3956819772720337, 'test/bleu': 29.990684621811305, 'test/num_examples': 3003, 'score': 30270.323910713196, 'total_duration': 49895.22182369232, 'accumulated_submission_time': 30270.323910713196, 'accumulated_eval_time': 19621.019548416138, 'accumulated_logging_time': 1.1373250484466553, 'global_step': 87587, 'preemption_count': 0}), (90021, {'train/accuracy': 0.6689482927322388, 'train/loss': 1.5577067136764526, 'train/bleu': 33.551267722025884, 'validation/accuracy': 0.6799047589302063, 'validation/loss': 1.4750844240188599, 'validation/bleu': 29.56897782735325, 'validation/num_examples': 3000, 'test/accuracy': 0.6959386467933655, 'test/loss': 1.3930952548980713, 'test/bleu': 29.751664538682242, 'test/num_examples': 3003, 'score': 31110.369894504547, 'total_duration': 51232.16949701309, 'accumulated_submission_time': 31110.369894504547, 'accumulated_eval_time': 20117.80855345726, 'accumulated_logging_time': 1.1739370822906494, 'global_step': 90021, 'preemption_count': 0}), (92455, {'train/accuracy': 0.6670649647712708, 'train/loss': 1.5666025876998901, 'train/bleu': 33.667583219883554, 'validation/accuracy': 0.6828681230545044, 'validation/loss': 1.465275764465332, 'validation/bleu': 30.192484170285006, 'validation/num_examples': 3000, 'test/accuracy': 0.6989483833312988, 'test/loss': 1.3823401927947998, 'test/bleu': 29.880037934469392, 'test/num_examples': 3003, 'score': 31950.513469696045, 'total_duration': 52584.40080785751, 'accumulated_submission_time': 31950.513469696045, 'accumulated_eval_time': 20629.78096461296, 'accumulated_logging_time': 1.2131965160369873, 'global_step': 92455, 'preemption_count': 0}), (94889, {'train/accuracy': 0.6854655146598816, 'train/loss': 1.4662498235702515, 'train/bleu': 34.65779037824217, 'validation/accuracy': 0.6841452717781067, 'validation/loss': 1.458268165588379, 'validation/bleu': 30.1208920740266, 'validation/num_examples': 3000, 'test/accuracy': 0.6998547315597534, 'test/loss': 1.3732168674468994, 'test/bleu': 30.19016397136998, 'test/num_examples': 3003, 'score': 32790.68526005745, 'total_duration': 53931.030331134796, 'accumulated_submission_time': 32790.68526005745, 'accumulated_eval_time': 21136.123238801956, 'accumulated_logging_time': 1.2523298263549805, 'global_step': 94889, 'preemption_count': 0}), (97323, {'train/accuracy': 0.6764490008354187, 'train/loss': 1.5147420167922974, 'train/bleu': 34.23678263585544, 'validation/accuracy': 0.686042308807373, 'validation/loss': 1.4486397504806519, 'validation/bleu': 30.389317993276933, 'validation/num_examples': 3000, 'test/accuracy': 0.7023648023605347, 'test/loss': 1.3640131950378418, 'test/bleu': 30.336006678823065, 'test/num_examples': 3003, 'score': 33630.83154082298, 'total_duration': 55339.30372548103, 'accumulated_submission_time': 33630.83154082298, 'accumulated_eval_time': 21704.13444662094, 'accumulated_logging_time': 1.2920780181884766, 'global_step': 97323, 'preemption_count': 0}), (99756, {'train/accuracy': 0.6764749884605408, 'train/loss': 1.5274385213851929, 'train/bleu': 34.04751952929381, 'validation/accuracy': 0.6861662864685059, 'validation/loss': 1.4491263628005981, 'validation/bleu': 30.30977062684815, 'validation/num_examples': 3000, 'test/accuracy': 0.7009587287902832, 'test/loss': 1.360594391822815, 'test/bleu': 30.23364767228356, 'test/num_examples': 3003, 'score': 34470.79104375839, 'total_duration': 56794.07966709137, 'accumulated_submission_time': 34470.79104375839, 'accumulated_eval_time': 22318.836542367935, 'accumulated_logging_time': 1.3307271003723145, 'global_step': 99756, 'preemption_count': 0}), (102189, {'train/accuracy': 0.6875606179237366, 'train/loss': 1.4551113843917847, 'train/bleu': 34.74172992301118, 'validation/accuracy': 0.6875798106193542, 'validation/loss': 1.437204360961914, 'validation/bleu': 30.38678618421871, 'validation/num_examples': 3000, 'test/accuracy': 0.7030736207962036, 'test/loss': 1.3559722900390625, 'test/bleu': 30.379172198859756, 'test/num_examples': 3003, 'score': 35310.75461125374, 'total_duration': 58163.51621007919, 'accumulated_submission_time': 35310.75461125374, 'accumulated_eval_time': 22848.193954706192, 'accumulated_logging_time': 1.3705039024353027, 'global_step': 102189, 'preemption_count': 0}), (104623, {'train/accuracy': 0.6816208958625793, 'train/loss': 1.488276720046997, 'train/bleu': 34.384006160637064, 'validation/accuracy': 0.6881005764007568, 'validation/loss': 1.4366995096206665, 'validation/bleu': 30.368678371068093, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.3490632772445679, 'test/bleu': 30.614908680396404, 'test/num_examples': 3003, 'score': 36150.90387272835, 'total_duration': 59591.86266922951, 'accumulated_submission_time': 36150.90387272835, 'accumulated_eval_time': 23436.27487707138, 'accumulated_logging_time': 1.4109792709350586, 'global_step': 104623, 'preemption_count': 0}), (107056, {'train/accuracy': 0.6984134316444397, 'train/loss': 1.395855188369751, 'train/bleu': 35.94551384243376, 'validation/accuracy': 0.6892660856246948, 'validation/loss': 1.4338706731796265, 'validation/bleu': 30.78100347405398, 'validation/num_examples': 3000, 'test/accuracy': 0.7048166990280151, 'test/loss': 1.348414421081543, 'test/bleu': 30.546431014686842, 'test/num_examples': 3003, 'score': 36990.97637438774, 'total_duration': 60982.136293411255, 'accumulated_submission_time': 36990.97637438774, 'accumulated_eval_time': 23986.36045742035, 'accumulated_logging_time': 1.4505560398101807, 'global_step': 107056, 'preemption_count': 0}), (109490, {'train/accuracy': 0.6916165947914124, 'train/loss': 1.4298664331436157, 'train/bleu': 35.22134886001856, 'validation/accuracy': 0.6891669034957886, 'validation/loss': 1.4241664409637451, 'validation/bleu': 30.790310256380376, 'validation/num_examples': 3000, 'test/accuracy': 0.7041543126106262, 'test/loss': 1.3388774394989014, 'test/bleu': 30.61245571439726, 'test/num_examples': 3003, 'score': 37831.17963075638, 'total_duration': 62354.47929620743, 'accumulated_submission_time': 37831.17963075638, 'accumulated_eval_time': 24518.378514528275, 'accumulated_logging_time': 1.4953322410583496, 'global_step': 109490, 'preemption_count': 0}), (111923, {'train/accuracy': 0.6884708404541016, 'train/loss': 1.4604493379592896, 'train/bleu': 35.23403588662617, 'validation/accuracy': 0.6906672120094299, 'validation/loss': 1.4245483875274658, 'validation/bleu': 30.639060315221954, 'validation/num_examples': 3000, 'test/accuracy': 0.7065016627311707, 'test/loss': 1.336126446723938, 'test/bleu': 30.754648086173646, 'test/num_examples': 3003, 'score': 38671.1463496685, 'total_duration': 63734.725130319595, 'accumulated_submission_time': 38671.1463496685, 'accumulated_eval_time': 25058.541039943695, 'accumulated_logging_time': 1.5352284908294678, 'global_step': 111923, 'preemption_count': 0}), (114356, {'train/accuracy': 0.7008587121963501, 'train/loss': 1.3816499710083008, 'train/bleu': 36.201180058925885, 'validation/accuracy': 0.6914979219436646, 'validation/loss': 1.4168438911437988, 'validation/bleu': 30.521495408385693, 'validation/num_examples': 3000, 'test/accuracy': 0.7080007195472717, 'test/loss': 1.3302934169769287, 'test/bleu': 30.880743489149467, 'test/num_examples': 3003, 'score': 39511.106392621994, 'total_duration': 65129.65449762344, 'accumulated_submission_time': 39511.106392621994, 'accumulated_eval_time': 25613.393503665924, 'accumulated_logging_time': 1.575636625289917, 'global_step': 114356, 'preemption_count': 0}), (116790, {'train/accuracy': 0.7000380158424377, 'train/loss': 1.39041268825531, 'train/bleu': 35.78679107524839, 'validation/accuracy': 0.6912747621536255, 'validation/loss': 1.4197064638137817, 'validation/bleu': 30.574397190601577, 'validation/num_examples': 3000, 'test/accuracy': 0.7080704569816589, 'test/loss': 1.329336166381836, 'test/bleu': 30.986274615235534, 'test/num_examples': 3003, 'score': 40351.31870722771, 'total_duration': 66497.25703215599, 'accumulated_submission_time': 40351.31870722771, 'accumulated_eval_time': 26140.66422200203, 'accumulated_logging_time': 1.618021011352539, 'global_step': 116790, 'preemption_count': 0}), (119223, {'train/accuracy': 0.7113429307937622, 'train/loss': 1.3300620317459106, 'train/bleu': 36.80936519873941, 'validation/accuracy': 0.6935933828353882, 'validation/loss': 1.412529468536377, 'validation/bleu': 30.943243782887972, 'validation/num_examples': 3000, 'test/accuracy': 0.7090930342674255, 'test/loss': 1.3264704942703247, 'test/bleu': 31.189875956554665, 'test/num_examples': 3003, 'score': 41191.29754805565, 'total_duration': 67847.41802787781, 'accumulated_submission_time': 41191.29754805565, 'accumulated_eval_time': 26650.726126909256, 'accumulated_logging_time': 1.6613097190856934, 'global_step': 119223, 'preemption_count': 0}), (121656, {'train/accuracy': 0.707312285900116, 'train/loss': 1.3521544933319092, 'train/bleu': 36.62262186936308, 'validation/accuracy': 0.6932833790779114, 'validation/loss': 1.41163969039917, 'validation/bleu': 30.834937831447107, 'validation/num_examples': 3000, 'test/accuracy': 0.7095810770988464, 'test/loss': 1.3253158330917358, 'test/bleu': 30.906804638756277, 'test/num_examples': 3003, 'score': 42031.314935684204, 'total_duration': 69267.97033762932, 'accumulated_submission_time': 42031.314935684204, 'accumulated_eval_time': 27231.141949653625, 'accumulated_logging_time': 1.7042453289031982, 'global_step': 121656, 'preemption_count': 0}), (124089, {'train/accuracy': 0.707473874092102, 'train/loss': 1.3492178916931152, 'train/bleu': 36.849319559218614, 'validation/accuracy': 0.6941513419151306, 'validation/loss': 1.4105970859527588, 'validation/bleu': 30.965560621091676, 'validation/num_examples': 3000, 'test/accuracy': 0.7108128666877747, 'test/loss': 1.3213181495666504, 'test/bleu': 31.108249855042047, 'test/num_examples': 3003, 'score': 42871.35903739929, 'total_duration': 70667.94770431519, 'accumulated_submission_time': 42871.35903739929, 'accumulated_eval_time': 27790.95612001419, 'accumulated_logging_time': 1.74674654006958, 'global_step': 124089, 'preemption_count': 0}), (126522, {'train/accuracy': 0.7100922465324402, 'train/loss': 1.3384053707122803, 'train/bleu': 37.03802375742551, 'validation/accuracy': 0.6936057806015015, 'validation/loss': 1.4109114408493042, 'validation/bleu': 30.960405634997983, 'validation/num_examples': 3000, 'test/accuracy': 0.7102434635162354, 'test/loss': 1.3205872774124146, 'test/bleu': 31.24458192290439, 'test/num_examples': 3003, 'score': 43711.439470529556, 'total_duration': 72065.4435763359, 'accumulated_submission_time': 43711.439470529556, 'accumulated_eval_time': 28348.25263595581, 'accumulated_logging_time': 1.7892396450042725, 'global_step': 126522, 'preemption_count': 0}), (128955, {'train/accuracy': 0.7126824855804443, 'train/loss': 1.3262940645217896, 'train/bleu': 36.65160501353665, 'validation/accuracy': 0.6942629218101501, 'validation/loss': 1.4080395698547363, 'validation/bleu': 31.022167451861694, 'validation/num_examples': 3000, 'test/accuracy': 0.7098715901374817, 'test/loss': 1.3178081512451172, 'test/bleu': 31.098821084681767, 'test/num_examples': 3003, 'score': 44551.3946352005, 'total_duration': 73475.28617930412, 'accumulated_submission_time': 44551.3946352005, 'accumulated_eval_time': 28918.01958155632, 'accumulated_logging_time': 1.8332068920135498, 'global_step': 128955, 'preemption_count': 0}), (131389, {'train/accuracy': 0.712949275970459, 'train/loss': 1.319365382194519, 'train/bleu': 36.297809192303134, 'validation/accuracy': 0.6945109367370605, 'validation/loss': 1.4088495969772339, 'validation/bleu': 30.947350711661613, 'validation/num_examples': 3000, 'test/accuracy': 0.7102085947990417, 'test/loss': 1.3181660175323486, 'test/bleu': 31.13512948906848, 'test/num_examples': 3003, 'score': 45391.56788253784, 'total_duration': 74884.97759580612, 'accumulated_submission_time': 45391.56788253784, 'accumulated_eval_time': 29487.417108535767, 'accumulated_logging_time': 1.8770146369934082, 'global_step': 131389, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7099116444587708, 'train/loss': 1.3356733322143555, 'train/bleu': 37.02394184889104, 'validation/accuracy': 0.6946597099304199, 'validation/loss': 1.4091264009475708, 'validation/bleu': 31.011342101754487, 'validation/num_examples': 3000, 'test/accuracy': 0.7103132009506226, 'test/loss': 1.3183356523513794, 'test/bleu': 31.18299976012298, 'test/num_examples': 3003, 'score': 46062.535388469696, 'total_duration': 76133.92356038094, 'accumulated_submission_time': 46062.535388469696, 'accumulated_eval_time': 30065.29164481163, 'accumulated_logging_time': 1.9203734397888184, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0209 07:54:43.690991 140225696298816 submission_runner.py:586] Timing: 46062.535388469696
I0209 07:54:43.691054 140225696298816 submission_runner.py:588] Total number of evals: 56
I0209 07:54:43.691129 140225696298816 submission_runner.py:589] ====================
I0209 07:54:43.691198 140225696298816 submission_runner.py:542] Using RNG seed 1037423020
I0209 07:54:43.692970 140225696298816 submission_runner.py:551] --- Tuning run 4/5 ---
I0209 07:54:43.693073 140225696298816 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4.
I0209 07:54:43.693300 140225696298816 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4/hparams.json.
I0209 07:54:43.694110 140225696298816 submission_runner.py:206] Initializing dataset.
I0209 07:54:43.696575 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 07:54:43.700170 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0209 07:54:43.737817 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 07:54:44.438144 140225696298816 submission_runner.py:213] Initializing model.
I0209 07:54:50.920014 140225696298816 submission_runner.py:255] Initializing optimizer.
I0209 07:54:51.679144 140225696298816 submission_runner.py:262] Initializing metrics bundle.
I0209 07:54:51.679307 140225696298816 submission_runner.py:280] Initializing checkpoint and logger.
I0209 07:54:51.680110 140225696298816 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4 with prefix checkpoint_
I0209 07:54:51.680219 140225696298816 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4/meta_data_0.json.
I0209 07:54:51.680419 140225696298816 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 07:54:51.680483 140225696298816 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 07:54:52.287027 140225696298816 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 07:54:52.914996 140225696298816 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4/flags_0.json.
I0209 07:54:52.918344 140225696298816 submission_runner.py:314] Starting training loop.
I0209 07:55:20.682535 140050996819712 logging_writer.py:48] [0] global_step=0, grad_norm=5.70151424407959, loss=11.02318286895752
I0209 07:55:20.695705 140225696298816 spec.py:321] Evaluating on the training split.
I0209 07:55:23.374164 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:00:07.268850 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 08:00:09.944036 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:04:52.252164 140225696298816 spec.py:349] Evaluating on the test split.
I0209 08:04:54.927096 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:09:38.088370 140225696298816 submission_runner.py:408] Time since start: 885.17s, 	Step: 1, 	{'train/accuracy': 0.0006736774812452495, 'train/loss': 11.026397705078125, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.777300596237183, 'total_duration': 885.1699132919312, 'accumulated_submission_time': 27.777300596237183, 'accumulated_eval_time': 857.3925604820251, 'accumulated_logging_time': 0}
I0209 08:09:38.098036 140051005212416 logging_writer.py:48] [1] accumulated_eval_time=857.392560, accumulated_logging_time=0, accumulated_submission_time=27.777301, global_step=1, preemption_count=0, score=27.777301, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.036274, test/num_examples=3003, total_duration=885.169913, train/accuracy=0.000674, train/bleu=0.000000, train/loss=11.026398, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.047277, validation/num_examples=3000
I0209 08:10:12.541827 140050996819712 logging_writer.py:48] [100] global_step=100, grad_norm=0.723619818687439, loss=7.5908522605896
I0209 08:10:47.051902 140051005212416 logging_writer.py:48] [200] global_step=200, grad_norm=0.49577656388282776, loss=6.655087471008301
I0209 08:11:21.624605 140050996819712 logging_writer.py:48] [300] global_step=300, grad_norm=0.469377726316452, loss=5.877396106719971
I0209 08:11:56.199300 140051005212416 logging_writer.py:48] [400] global_step=400, grad_norm=0.6034638285636902, loss=5.4385247230529785
I0209 08:12:30.800893 140050996819712 logging_writer.py:48] [500] global_step=500, grad_norm=0.5935078263282776, loss=5.072423934936523
I0209 08:13:05.377269 140051005212416 logging_writer.py:48] [600] global_step=600, grad_norm=0.5109654068946838, loss=4.710037708282471
I0209 08:13:39.957469 140050996819712 logging_writer.py:48] [700] global_step=700, grad_norm=0.5978085398674011, loss=4.519240856170654
I0209 08:14:14.512281 140051005212416 logging_writer.py:48] [800] global_step=800, grad_norm=0.5068848729133606, loss=4.207457065582275
I0209 08:14:49.066842 140050996819712 logging_writer.py:48] [900] global_step=900, grad_norm=0.48865145444869995, loss=4.0253753662109375
I0209 08:15:23.656237 140051005212416 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4926040768623352, loss=3.8581695556640625
I0209 08:15:58.233563 140050996819712 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.47481679916381836, loss=3.732477903366089
I0209 08:16:32.813631 140051005212416 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.41239020228385925, loss=3.5146641731262207
I0209 08:17:07.398200 140050996819712 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.39196470379829407, loss=3.5366458892822266
I0209 08:17:41.979704 140051005212416 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3191770017147064, loss=3.356609582901001
I0209 08:18:16.571969 140050996819712 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.28331583738327026, loss=3.1859962940216064
I0209 08:18:51.163857 140051005212416 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.28794386982917786, loss=3.1678242683410645
I0209 08:19:25.737886 140050996819712 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.27733173966407776, loss=3.107713222503662
I0209 08:20:00.307151 140051005212416 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.341645747423172, loss=2.898041248321533
I0209 08:20:34.891238 140050996819712 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.22663193941116333, loss=2.882619619369507
I0209 08:21:09.492384 140051005212416 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.2877855598926544, loss=2.8468093872070312
I0209 08:21:44.059112 140050996819712 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.19944193959236145, loss=2.6527624130249023
I0209 08:22:18.616065 140051005212416 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.23008999228477478, loss=2.7333266735076904
I0209 08:22:53.166683 140050996819712 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.18532095849514008, loss=2.622300863265991
I0209 08:23:27.742573 140051005212416 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.1704207956790924, loss=2.591369390487671
I0209 08:23:38.178783 140225696298816 spec.py:321] Evaluating on the training split.
I0209 08:23:41.138405 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:26:24.754310 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 08:26:27.423177 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:29:02.393911 140225696298816 spec.py:349] Evaluating on the test split.
I0209 08:29:05.063286 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:31:25.135530 140225696298816 submission_runner.py:408] Time since start: 2192.22s, 	Step: 2432, 	{'train/accuracy': 0.5334533452987671, 'train/loss': 2.6014294624328613, 'train/bleu': 24.246267876536, 'validation/accuracy': 0.5396957397460938, 'validation/loss': 2.53312087059021, 'validation/bleu': 20.273622232300472, 'validation/num_examples': 3000, 'test/accuracy': 0.5400848388671875, 'test/loss': 2.5434446334838867, 'test/bleu': 18.86934210352786, 'test/num_examples': 3003, 'score': 867.7713329792023, 'total_duration': 2192.2171177864075, 'accumulated_submission_time': 867.7713329792023, 'accumulated_eval_time': 1324.3492550849915, 'accumulated_logging_time': 0.019840002059936523}
I0209 08:31:25.150521 140050996819712 logging_writer.py:48] [2432] accumulated_eval_time=1324.349255, accumulated_logging_time=0.019840, accumulated_submission_time=867.771333, global_step=2432, preemption_count=0, score=867.771333, test/accuracy=0.540085, test/bleu=18.869342, test/loss=2.543445, test/num_examples=3003, total_duration=2192.217118, train/accuracy=0.533453, train/bleu=24.246268, train/loss=2.601429, validation/accuracy=0.539696, validation/bleu=20.273622, validation/loss=2.533121, validation/num_examples=3000
I0209 08:31:48.884245 140051005212416 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.18613705039024353, loss=2.6048495769500732
I0209 08:32:23.347246 140050996819712 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2694944739341736, loss=2.4804389476776123
I0209 08:32:57.912582 140051005212416 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.17555119097232819, loss=2.450631856918335
I0209 08:33:32.473897 140050996819712 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.23116296529769897, loss=2.4826698303222656
I0209 08:34:07.007365 140051005212416 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.18724872171878815, loss=2.3711886405944824
I0209 08:34:41.598878 140050996819712 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.19500623643398285, loss=2.440603733062744
I0209 08:35:16.142541 140051005212416 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.3952599763870239, loss=2.432884693145752
I0209 08:35:50.700496 140050996819712 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.17980144917964935, loss=2.3488643169403076
I0209 08:36:25.222860 140051005212416 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.2942303419113159, loss=2.3208937644958496
I0209 08:36:59.771988 140050996819712 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.45653390884399414, loss=2.37426495552063
I0209 08:37:34.333645 140051005212416 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.26408377289772034, loss=2.2654128074645996
I0209 08:38:08.897271 140050996819712 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3900689482688904, loss=2.353743314743042
I0209 08:38:43.414526 140051005212416 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4506978690624237, loss=2.2726011276245117
I0209 08:39:17.981845 140050996819712 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.27867627143859863, loss=2.275843858718872
I0209 08:39:52.536985 140051005212416 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.40691208839416504, loss=2.253877639770508
I0209 08:40:27.074934 140050996819712 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6701393723487854, loss=2.304651975631714
I0209 08:41:01.629046 140051005212416 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.24824458360671997, loss=2.1842057704925537
I0209 08:41:36.184773 140050996819712 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.2453306019306183, loss=2.2791495323181152
I0209 08:42:10.758563 140051005212416 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.30309346318244934, loss=2.202039957046509
I0209 08:42:45.320269 140050996819712 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.28992214798927307, loss=2.1736814975738525
I0209 08:43:19.864777 140051005212416 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3740862309932709, loss=2.1380550861358643
I0209 08:43:54.394386 140050996819712 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.3305206298828125, loss=2.1987483501434326
I0209 08:44:28.973147 140051005212416 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.32222938537597656, loss=2.1569814682006836
I0209 08:45:03.545913 140050996819712 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5465013980865479, loss=2.305927038192749
I0209 08:45:25.379713 140225696298816 spec.py:321] Evaluating on the training split.
I0209 08:45:28.345500 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:48:44.052909 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 08:48:46.705905 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:51:34.137920 140225696298816 spec.py:349] Evaluating on the test split.
I0209 08:51:36.794430 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 08:53:49.405932 140225696298816 submission_runner.py:408] Time since start: 3536.49s, 	Step: 4865, 	{'train/accuracy': 0.5815564393997192, 'train/loss': 2.2129969596862793, 'train/bleu': 27.397051490301113, 'validation/accuracy': 0.5966076254844666, 'validation/loss': 2.080467700958252, 'validation/bleu': 23.915241366267704, 'validation/num_examples': 3000, 'test/accuracy': 0.6017082333564758, 'test/loss': 2.0296666622161865, 'test/bleu': 22.691728876503866, 'test/num_examples': 3003, 'score': 1707.9139828681946, 'total_duration': 3536.4875156879425, 'accumulated_submission_time': 1707.9139828681946, 'accumulated_eval_time': 1828.3754241466522, 'accumulated_logging_time': 0.046137332916259766}
I0209 08:53:49.420959 140051005212416 logging_writer.py:48] [4865] accumulated_eval_time=1828.375424, accumulated_logging_time=0.046137, accumulated_submission_time=1707.913983, global_step=4865, preemption_count=0, score=1707.913983, test/accuracy=0.601708, test/bleu=22.691729, test/loss=2.029667, test/num_examples=3003, total_duration=3536.487516, train/accuracy=0.581556, train/bleu=27.397051, train/loss=2.212997, validation/accuracy=0.596608, validation/bleu=23.915241, validation/loss=2.080468, validation/num_examples=3000
I0209 08:54:01.815206 140050996819712 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3116284906864166, loss=2.174236297607422
I0209 08:54:36.224203 140051005212416 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.47189292311668396, loss=2.182339668273926
I0209 08:55:10.758737 140050996819712 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.2857372462749481, loss=2.167806386947632
I0209 08:55:45.289447 140051005212416 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.40765514969825745, loss=2.224423408508301
I0209 08:56:19.842732 140050996819712 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.2791321575641632, loss=2.158095359802246
I0209 08:56:54.418237 140051005212416 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.274833083152771, loss=2.1668224334716797
I0209 08:57:28.988974 140050996819712 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.42815452814102173, loss=2.2107019424438477
I0209 08:58:03.556068 140051005212416 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.31906524300575256, loss=2.1443734169006348
I0209 08:58:38.109857 140050996819712 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.3568781614303589, loss=2.1543900966644287
I0209 08:59:12.686113 140051005212416 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.40050289034843445, loss=2.1387109756469727
I0209 08:59:47.247235 140050996819712 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6075432300567627, loss=2.134605646133423
I0209 09:00:21.802357 140051005212416 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.2707611620426178, loss=2.185399293899536
I0209 09:00:56.354189 140050996819712 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5276182293891907, loss=2.237435817718506
I0209 09:01:30.924410 140051005212416 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6666398048400879, loss=2.254815101623535
I0209 09:02:05.485903 140050996819712 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5987547039985657, loss=2.230651617050171
I0209 09:02:40.054817 140051005212416 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.32119059562683105, loss=2.1490509510040283
I0209 09:03:14.623132 140050996819712 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.44822630286216736, loss=2.147684335708618
I0209 09:03:49.204225 140051005212416 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.495974063873291, loss=2.0925207138061523
I0209 09:04:23.766762 140050996819712 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5954237580299377, loss=2.072597026824951
I0209 09:04:58.322040 140051005212416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.44412389397621155, loss=2.1379635334014893
I0209 09:05:32.867492 140050996819712 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.3324113190174103, loss=2.1444268226623535
I0209 09:06:07.434804 140051005212416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3086817264556885, loss=2.2461113929748535
I0209 09:06:42.004273 140050996819712 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.41372931003570557, loss=2.1136889457702637
I0209 09:07:16.578485 140051005212416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3391607105731964, loss=2.0802416801452637
I0209 09:07:49.475508 140225696298816 spec.py:321] Evaluating on the training split.
I0209 09:07:52.437785 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:11:10.954640 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 09:11:13.623019 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:13:42.717036 140225696298816 spec.py:349] Evaluating on the test split.
I0209 09:13:45.388324 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:16:03.238047 140225696298816 submission_runner.py:408] Time since start: 4870.32s, 	Step: 7297, 	{'train/accuracy': 0.5896901488304138, 'train/loss': 2.140878677368164, 'train/bleu': 27.75413258659114, 'validation/accuracy': 0.6057829260826111, 'validation/loss': 2.008117437362671, 'validation/bleu': 24.422510760879085, 'validation/num_examples': 3000, 'test/accuracy': 0.6117367148399353, 'test/loss': 1.962164282798767, 'test/bleu': 23.379135257735474, 'test/num_examples': 3003, 'score': 2547.882658958435, 'total_duration': 4870.319628000259, 'accumulated_submission_time': 2547.882658958435, 'accumulated_eval_time': 2322.1379055976868, 'accumulated_logging_time': 0.0710291862487793}
I0209 09:16:03.253791 140050996819712 logging_writer.py:48] [7297] accumulated_eval_time=2322.137906, accumulated_logging_time=0.071029, accumulated_submission_time=2547.882659, global_step=7297, preemption_count=0, score=2547.882659, test/accuracy=0.611737, test/bleu=23.379135, test/loss=1.962164, test/num_examples=3003, total_duration=4870.319628, train/accuracy=0.589690, train/bleu=27.754133, train/loss=2.140879, validation/accuracy=0.605783, validation/bleu=24.422511, validation/loss=2.008117, validation/num_examples=3000
I0209 09:16:04.665908 140051005212416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.45769667625427246, loss=2.060554265975952
I0209 09:16:39.080488 140050996819712 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.42904767394065857, loss=2.1472108364105225
I0209 09:17:13.571975 140051005212416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5055073499679565, loss=2.105151891708374
I0209 09:17:48.139296 140050996819712 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.357909619808197, loss=2.1530864238739014
I0209 09:18:22.700462 140051005212416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6033315658569336, loss=2.161010265350342
I0209 09:18:57.227429 140050996819712 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.31766802072525024, loss=2.023416757583618
I0209 09:19:31.781296 140051005212416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3203239142894745, loss=2.1719226837158203
I0209 09:20:06.328969 140050996819712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.4762781858444214, loss=2.1590332984924316
I0209 09:20:40.870884 140051005212416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.28108739852905273, loss=2.1598892211914062
I0209 09:21:15.421896 140050996819712 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.47374168038368225, loss=2.1250429153442383
I0209 09:21:50.000370 140051005212416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.6396384239196777, loss=2.1142609119415283
I0209 09:22:24.517480 140050996819712 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.34894683957099915, loss=2.211904525756836
I0209 09:22:59.105588 140051005212416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.28953132033348083, loss=2.140104293823242
I0209 09:23:33.669253 140050996819712 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5196545720100403, loss=2.0753612518310547
I0209 09:24:08.215543 140051005212416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.2817792296409607, loss=2.0559732913970947
I0209 09:24:42.767310 140050996819712 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.27762287855148315, loss=2.176736354827881
I0209 09:25:17.324744 140051005212416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.33217018842697144, loss=2.096893787384033
I0209 09:25:51.868645 140050996819712 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3069154620170593, loss=2.2143869400024414
I0209 09:26:26.417387 140051005212416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.3320128321647644, loss=2.1224255561828613
I0209 09:27:00.971437 140050996819712 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.514150083065033, loss=2.2470531463623047
I0209 09:27:35.528585 140051005212416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.4081849753856659, loss=2.1181883811950684
I0209 09:28:10.073995 140050996819712 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3887074887752533, loss=2.188211441040039
I0209 09:28:44.621190 140051005212416 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.293029248714447, loss=2.0929574966430664
I0209 09:29:19.182634 140050996819712 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.37605881690979004, loss=2.1278085708618164
I0209 09:29:53.742064 140051005212416 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.31887343525886536, loss=2.0699689388275146
I0209 09:30:03.498778 140225696298816 spec.py:321] Evaluating on the training split.
I0209 09:30:06.457640 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:32:58.884303 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 09:33:01.544185 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:35:42.893928 140225696298816 spec.py:349] Evaluating on the test split.
I0209 09:35:45.561528 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:38:22.530747 140225696298816 submission_runner.py:408] Time since start: 6209.61s, 	Step: 9730, 	{'train/accuracy': 0.5907862186431885, 'train/loss': 2.14593243598938, 'train/bleu': 27.758313068089066, 'validation/accuracy': 0.6103581786155701, 'validation/loss': 1.9752626419067383, 'validation/bleu': 24.93721700930189, 'validation/num_examples': 3000, 'test/accuracy': 0.6183835864067078, 'test/loss': 1.9371329545974731, 'test/bleu': 23.38670207942713, 'test/num_examples': 3003, 'score': 3388.040452480316, 'total_duration': 6209.612332820892, 'accumulated_submission_time': 3388.040452480316, 'accumulated_eval_time': 2821.1698200702667, 'accumulated_logging_time': 0.09875035285949707}
I0209 09:38:22.547008 140050996819712 logging_writer.py:48] [9730] accumulated_eval_time=2821.169820, accumulated_logging_time=0.098750, accumulated_submission_time=3388.040452, global_step=9730, preemption_count=0, score=3388.040452, test/accuracy=0.618384, test/bleu=23.386702, test/loss=1.937133, test/num_examples=3003, total_duration=6209.612333, train/accuracy=0.590786, train/bleu=27.758313, train/loss=2.145932, validation/accuracy=0.610358, validation/bleu=24.937217, validation/loss=1.975263, validation/num_examples=3000
I0209 09:38:46.964659 140051005212416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.29032275080680847, loss=2.1165616512298584
I0209 09:39:21.420048 140050996819712 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.292185515165329, loss=2.032440185546875
I0209 09:39:55.939974 140051005212416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3413577079772949, loss=2.136418342590332
I0209 09:40:30.492461 140050996819712 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2589515149593353, loss=2.0691280364990234
I0209 09:41:05.035531 140051005212416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.35825520753860474, loss=2.139415740966797
I0209 09:41:39.572013 140050996819712 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4776647686958313, loss=2.0502891540527344
I0209 09:42:14.117013 140051005212416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.31512391567230225, loss=2.0661935806274414
I0209 09:42:48.660398 140050996819712 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2609587609767914, loss=2.1502792835235596
I0209 09:43:23.205763 140051005212416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3195559084415436, loss=2.0327868461608887
I0209 09:43:57.735720 140050996819712 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3230648338794708, loss=2.106159210205078
I0209 09:44:32.301347 140051005212416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.26811665296554565, loss=2.1277849674224854
I0209 09:45:06.834192 140050996819712 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.43871796131134033, loss=2.138700246810913
I0209 09:45:41.387312 140051005212416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5082281231880188, loss=2.1888153553009033
I0209 09:46:15.934446 140050996819712 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.28969821333885193, loss=2.051377534866333
I0209 09:46:50.494889 140051005212416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.39481663703918457, loss=2.1184494495391846
I0209 09:47:25.059507 140050996819712 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.652160108089447, loss=2.104844093322754
I0209 09:47:59.595527 140051005212416 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.7419835925102234, loss=2.0352377891540527
I0209 09:48:34.145628 140050996819712 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.38486844301223755, loss=2.170659065246582
I0209 09:49:08.694995 140051005212416 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.41702425479888916, loss=2.198446750640869
I0209 09:49:43.422633 140050996819712 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.435648113489151, loss=2.0567708015441895
I0209 09:50:17.966452 140051005212416 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.29802635312080383, loss=2.1439332962036133
I0209 09:50:52.504245 140050996819712 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.292712926864624, loss=2.0871853828430176
I0209 09:51:27.061817 140051005212416 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2977515161037445, loss=2.163318157196045
I0209 09:52:01.609675 140050996819712 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.3843236565589905, loss=2.052954912185669
I0209 09:52:22.757432 140225696298816 spec.py:321] Evaluating on the training split.
I0209 09:52:25.744750 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:55:32.165735 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 09:55:34.836721 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 09:58:25.337103 140225696298816 spec.py:349] Evaluating on the test split.
I0209 09:58:28.006670 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:01:18.534126 140225696298816 submission_runner.py:408] Time since start: 7585.62s, 	Step: 12163, 	{'train/accuracy': 0.5988882184028625, 'train/loss': 2.0783145427703857, 'train/bleu': 27.654304678780385, 'validation/accuracy': 0.6146730780601501, 'validation/loss': 1.9561139345169067, 'validation/bleu': 24.84778671859338, 'validation/num_examples': 3000, 'test/accuracy': 0.6207425594329834, 'test/loss': 1.8987210988998413, 'test/bleu': 23.72557107950406, 'test/num_examples': 3003, 'score': 4228.165338039398, 'total_duration': 7585.615716218948, 'accumulated_submission_time': 4228.165338039398, 'accumulated_eval_time': 3356.9464728832245, 'accumulated_logging_time': 0.1251380443572998}
I0209 10:01:18.551147 140051005212416 logging_writer.py:48] [12163] accumulated_eval_time=3356.946473, accumulated_logging_time=0.125138, accumulated_submission_time=4228.165338, global_step=12163, preemption_count=0, score=4228.165338, test/accuracy=0.620743, test/bleu=23.725571, test/loss=1.898721, test/num_examples=3003, total_duration=7585.615716, train/accuracy=0.598888, train/bleu=27.654305, train/loss=2.078315, validation/accuracy=0.614673, validation/bleu=24.847787, validation/loss=1.956114, validation/num_examples=3000
I0209 10:01:31.616683 140050996819712 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2921084761619568, loss=2.120070457458496
I0209 10:02:06.017867 140051005212416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.4158158004283905, loss=2.1552717685699463
I0209 10:02:40.545747 140050996819712 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.322297602891922, loss=2.080794334411621
I0209 10:03:15.087917 140051005212416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.36297786235809326, loss=2.106863498687744
I0209 10:03:49.651272 140050996819712 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.4419994652271271, loss=2.0229344367980957
I0209 10:04:24.208540 140051005212416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.3077908158302307, loss=2.0049314498901367
I0209 10:04:58.778940 140050996819712 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.6364445686340332, loss=2.107205629348755
I0209 10:05:33.327064 140051005212416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.592481791973114, loss=2.104543924331665
I0209 10:06:07.881263 140050996819712 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5654438734054565, loss=2.0667643547058105
I0209 10:06:42.435864 140051005212416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.32983726263046265, loss=2.0964810848236084
I0209 10:07:16.967444 140050996819712 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.26529785990715027, loss=2.1111416816711426
I0209 10:07:51.557548 140051005212416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3650585114955902, loss=2.1145219802856445
I0209 10:08:26.101331 140050996819712 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.620029091835022, loss=2.140625
I0209 10:09:00.650508 140051005212416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7569207549095154, loss=2.0531041622161865
I0209 10:09:35.173509 140050996819712 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5045956969261169, loss=2.0891263484954834
I0209 10:10:09.731071 140051005212416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2776356637477875, loss=2.0757040977478027
I0209 10:10:44.239906 140050996819712 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5721344947814941, loss=2.1076571941375732
I0209 10:11:18.808293 140051005212416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.5803384184837341, loss=2.174250841140747
I0209 10:11:53.329946 140050996819712 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.49511444568634033, loss=1.9971681833267212
I0209 10:12:27.910464 140051005212416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.45776113867759705, loss=2.073751211166382
I0209 10:13:02.487630 140050996819712 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.42284417152404785, loss=2.0425453186035156
I0209 10:13:37.036941 140051005212416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2888646721839905, loss=2.0525856018066406
I0209 10:14:11.614446 140050996819712 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.28533604741096497, loss=2.108048915863037
I0209 10:14:46.170336 140051005212416 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.27918288111686707, loss=2.0932857990264893
I0209 10:15:18.708710 140225696298816 spec.py:321] Evaluating on the training split.
I0209 10:15:21.688994 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:19:04.820997 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 10:19:07.503612 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:22:09.001777 140225696298816 spec.py:349] Evaluating on the test split.
I0209 10:22:11.670130 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:24:42.773771 140225696298816 submission_runner.py:408] Time since start: 8989.86s, 	Step: 14596, 	{'train/accuracy': 0.5957963466644287, 'train/loss': 2.0913431644439697, 'train/bleu': 27.566099296535413, 'validation/accuracy': 0.6168429255485535, 'validation/loss': 1.918594479560852, 'validation/bleu': 24.633238759292187, 'validation/num_examples': 3000, 'test/accuracy': 0.6213933229446411, 'test/loss': 1.8927571773529053, 'test/bleu': 23.30884255123911, 'test/num_examples': 3003, 'score': 5068.237158060074, 'total_duration': 8989.855357408524, 'accumulated_submission_time': 5068.237158060074, 'accumulated_eval_time': 3921.0114846229553, 'accumulated_logging_time': 0.15222644805908203}
I0209 10:24:42.790408 140050996819712 logging_writer.py:48] [14596] accumulated_eval_time=3921.011485, accumulated_logging_time=0.152226, accumulated_submission_time=5068.237158, global_step=14596, preemption_count=0, score=5068.237158, test/accuracy=0.621393, test/bleu=23.308843, test/loss=1.892757, test/num_examples=3003, total_duration=8989.855357, train/accuracy=0.595796, train/bleu=27.566099, train/loss=2.091343, validation/accuracy=0.616843, validation/bleu=24.633239, validation/loss=1.918594, validation/num_examples=3000
I0209 10:24:44.532968 140051005212416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2836029529571533, loss=2.187443971633911
I0209 10:25:18.964502 140050996819712 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.4180868864059448, loss=2.0624966621398926
I0209 10:25:53.455007 140051005212416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.4823373258113861, loss=2.1123106479644775
I0209 10:26:27.970289 140050996819712 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.394683837890625, loss=2.0343198776245117
I0209 10:27:02.525096 140051005212416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7252098917961121, loss=1.9957473278045654
I0209 10:27:37.068760 140050996819712 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2709404528141022, loss=2.1294872760772705
I0209 10:28:11.604413 140051005212416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.3728971481323242, loss=2.12422251701355
I0209 10:28:46.143182 140050996819712 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.5925225019454956, loss=2.073974847793579
I0209 10:29:20.684191 140051005212416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.28906023502349854, loss=1.9887198209762573
I0209 10:29:55.212993 140050996819712 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6240956783294678, loss=2.124891757965088
I0209 10:30:29.760579 140051005212416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.4620896875858307, loss=2.0822718143463135
I0209 10:31:04.317880 140050996819712 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3876546621322632, loss=2.0563485622406006
I0209 10:31:38.857566 140051005212416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.25567764043807983, loss=1.9914045333862305
I0209 10:32:13.415936 140050996819712 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.29505252838134766, loss=2.104095935821533
I0209 10:32:47.975800 140051005212416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.35007405281066895, loss=2.0646374225616455
I0209 10:33:22.525193 140050996819712 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.3569362163543701, loss=2.168398141860962
I0209 10:33:57.066679 140051005212416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.6732438206672668, loss=2.050273895263672
I0209 10:34:31.631006 140050996819712 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3309306800365448, loss=2.033677101135254
I0209 10:35:06.171993 140051005212416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.47185030579566956, loss=1.981276035308838
I0209 10:35:40.733950 140050996819712 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.30010125041007996, loss=2.047335386276245
I0209 10:36:15.270763 140051005212416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.305723637342453, loss=2.0589215755462646
I0209 10:36:49.836109 140050996819712 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.5428664684295654, loss=2.0490987300872803
I0209 10:37:24.395989 140051005212416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.3595803380012512, loss=2.0746915340423584
I0209 10:37:58.931416 140050996819712 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3688903748989105, loss=2.0614607334136963
I0209 10:38:33.489789 140051005212416 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2368021309375763, loss=2.0631093978881836
I0209 10:38:42.893245 140225696298816 spec.py:321] Evaluating on the training split.
I0209 10:38:45.861485 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:41:36.024287 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 10:41:38.695542 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:44:17.823111 140225696298816 spec.py:349] Evaluating on the test split.
I0209 10:44:20.489274 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 10:46:50.958674 140225696298816 submission_runner.py:408] Time since start: 10318.04s, 	Step: 17029, 	{'train/accuracy': 0.5992476940155029, 'train/loss': 2.066373348236084, 'train/bleu': 27.728670866665368, 'validation/accuracy': 0.6194343566894531, 'validation/loss': 1.9151461124420166, 'validation/bleu': 25.00878222369035, 'validation/num_examples': 3000, 'test/accuracy': 0.628063440322876, 'test/loss': 1.8578368425369263, 'test/bleu': 24.449780344884086, 'test/num_examples': 3003, 'score': 5908.252819776535, 'total_duration': 10318.040254831314, 'accumulated_submission_time': 5908.252819776535, 'accumulated_eval_time': 4409.076854467392, 'accumulated_logging_time': 0.1800706386566162}
I0209 10:46:50.976505 140050996819712 logging_writer.py:48] [17029] accumulated_eval_time=4409.076854, accumulated_logging_time=0.180071, accumulated_submission_time=5908.252820, global_step=17029, preemption_count=0, score=5908.252820, test/accuracy=0.628063, test/bleu=24.449780, test/loss=1.857837, test/num_examples=3003, total_duration=10318.040255, train/accuracy=0.599248, train/bleu=27.728671, train/loss=2.066373, validation/accuracy=0.619434, validation/bleu=25.008782, validation/loss=1.915146, validation/num_examples=3000
I0209 10:47:15.752915 140051005212416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.4216575026512146, loss=2.0987634658813477
I0209 10:47:50.230247 140050996819712 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.23709741234779358, loss=2.0224387645721436
I0209 10:48:24.745517 140051005212416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.397267609834671, loss=2.112126350402832
I0209 10:48:59.309304 140050996819712 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.26594802737236023, loss=2.0325989723205566
I0209 10:49:33.865678 140051005212416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5067014694213867, loss=2.069441556930542
I0209 10:50:08.426402 140050996819712 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.5753195285797119, loss=2.1311511993408203
I0209 10:50:42.969363 140051005212416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2469191551208496, loss=2.046578884124756
I0209 10:51:17.528242 140050996819712 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.462268590927124, loss=2.0466256141662598
I0209 10:51:52.103751 140051005212416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.4765324294567108, loss=2.0807433128356934
I0209 10:52:26.690762 140050996819712 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.31834864616394043, loss=2.066473960876465
I0209 10:53:01.272294 140051005212416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.32406753301620483, loss=2.113330125808716
I0209 10:53:35.810609 140050996819712 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3416833281517029, loss=2.069809675216675
I0209 10:54:10.386789 140051005212416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6773526072502136, loss=2.123772621154785
I0209 10:54:44.940584 140050996819712 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.4274044632911682, loss=2.0932230949401855
I0209 10:55:19.522393 140051005212416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.47971558570861816, loss=2.103886842727661
I0209 10:55:54.103702 140050996819712 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.507145345211029, loss=2.043916940689087
I0209 10:56:28.679307 140051005212416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.2825428247451782, loss=2.014256000518799
I0209 10:57:03.228188 140050996819712 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.2661753296852112, loss=2.13950514793396
I0209 10:57:37.814499 140051005212416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.5106801390647888, loss=1.9557560682296753
I0209 10:58:12.364413 140050996819712 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2874952256679535, loss=2.023874044418335
I0209 10:58:46.917915 140051005212416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.3469994068145752, loss=2.0747838020324707
I0209 10:59:21.469571 140050996819712 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2702830135822296, loss=2.0417239665985107
I0209 10:59:56.032579 140051005212416 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.35112619400024414, loss=2.035796642303467
I0209 11:00:30.612737 140050996819712 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.26045840978622437, loss=2.0765180587768555
I0209 11:00:51.055409 140225696298816 spec.py:321] Evaluating on the training split.
I0209 11:00:54.019072 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:03:34.601055 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 11:03:37.274100 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:06:40.358124 140225696298816 spec.py:349] Evaluating on the test split.
I0209 11:06:43.038501 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:09:29.955727 140225696298816 submission_runner.py:408] Time since start: 11677.04s, 	Step: 19461, 	{'train/accuracy': 0.6093730926513672, 'train/loss': 1.9898121356964111, 'train/bleu': 28.350201916222264, 'validation/accuracy': 0.6198559403419495, 'validation/loss': 1.9141809940338135, 'validation/bleu': 25.218665959343333, 'validation/num_examples': 3000, 'test/accuracy': 0.6275056600570679, 'test/loss': 1.8592458963394165, 'test/bleu': 24.24569041878532, 'test/num_examples': 3003, 'score': 6748.244628667831, 'total_duration': 11677.037281274796, 'accumulated_submission_time': 6748.244628667831, 'accumulated_eval_time': 4927.977090597153, 'accumulated_logging_time': 0.20806145668029785}
I0209 11:09:29.973652 140051005212416 logging_writer.py:48] [19461] accumulated_eval_time=4927.977091, accumulated_logging_time=0.208061, accumulated_submission_time=6748.244629, global_step=19461, preemption_count=0, score=6748.244629, test/accuracy=0.627506, test/bleu=24.245690, test/loss=1.859246, test/num_examples=3003, total_duration=11677.037281, train/accuracy=0.609373, train/bleu=28.350202, train/loss=1.989812, validation/accuracy=0.619856, validation/bleu=25.218666, validation/loss=1.914181, validation/num_examples=3000
I0209 11:09:43.758692 140050996819712 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.3257307708263397, loss=2.05461049079895
I0209 11:10:18.160132 140051005212416 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.4683557450771332, loss=2.020195484161377
I0209 11:10:52.691266 140050996819712 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.8628877401351929, loss=2.0956146717071533
I0209 11:11:27.232802 140051005212416 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.2833862900733948, loss=2.161874532699585
I0209 11:12:01.775176 140050996819712 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5532210469245911, loss=2.06252121925354
I0209 11:12:36.336353 140051005212416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3649213910102844, loss=2.0717623233795166
I0209 11:13:10.894567 140050996819712 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.5463957786560059, loss=2.0160133838653564
I0209 11:13:45.468455 140051005212416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3224261999130249, loss=2.0329012870788574
I0209 11:14:20.016671 140050996819712 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.5936963558197021, loss=2.0630435943603516
I0209 11:14:54.552313 140051005212416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.2905409336090088, loss=2.10465407371521
I0209 11:15:29.114089 140050996819712 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6892502307891846, loss=2.0692338943481445
I0209 11:16:03.700626 140051005212416 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3375604450702667, loss=2.090479850769043
I0209 11:16:38.275771 140050996819712 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.5359761118888855, loss=2.0500762462615967
I0209 11:17:12.944959 140051005212416 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.2696409225463867, loss=2.0213539600372314
I0209 11:17:47.496865 140050996819712 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.32622089982032776, loss=1.9837795495986938
I0209 11:18:22.057315 140051005212416 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.27019259333610535, loss=2.007581949234009
I0209 11:18:56.591251 140050996819712 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.26065993309020996, loss=2.001669406890869
I0209 11:19:31.157321 140051005212416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.592331051826477, loss=2.0579774379730225
I0209 11:20:05.722734 140050996819712 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.4851612448692322, loss=2.055093288421631
I0209 11:20:40.263492 140051005212416 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3048054277896881, loss=2.0489513874053955
I0209 11:21:14.835158 140050996819712 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.35240304470062256, loss=2.067349433898926
I0209 11:21:49.405566 140051005212416 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.5887174010276794, loss=2.0406620502471924
I0209 11:22:23.944868 140050996819712 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5180318355560303, loss=2.015266180038452
I0209 11:22:58.503608 140051005212416 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5455784797668457, loss=2.051898717880249
I0209 11:23:30.004849 140225696298816 spec.py:321] Evaluating on the training split.
I0209 11:23:32.972752 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:27:17.344959 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 11:27:20.014760 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:31:07.545525 140225696298816 spec.py:349] Evaluating on the test split.
I0209 11:31:10.218607 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:34:37.321106 140225696298816 submission_runner.py:408] Time since start: 13184.40s, 	Step: 21893, 	{'train/accuracy': 0.6032451391220093, 'train/loss': 2.035078763961792, 'train/bleu': 27.34081747482811, 'validation/accuracy': 0.6205874681472778, 'validation/loss': 1.8935205936431885, 'validation/bleu': 25.087785595498843, 'validation/num_examples': 3000, 'test/accuracy': 0.6247748732566833, 'test/loss': 1.851194977760315, 'test/bleu': 23.557981255248695, 'test/num_examples': 3003, 'score': 7588.188913345337, 'total_duration': 13184.40267777443, 'accumulated_submission_time': 7588.188913345337, 'accumulated_eval_time': 5595.293299913406, 'accumulated_logging_time': 0.2365128993988037}
I0209 11:34:37.338988 140050996819712 logging_writer.py:48] [21893] accumulated_eval_time=5595.293300, accumulated_logging_time=0.236513, accumulated_submission_time=7588.188913, global_step=21893, preemption_count=0, score=7588.188913, test/accuracy=0.624775, test/bleu=23.557981, test/loss=1.851195, test/num_examples=3003, total_duration=13184.402678, train/accuracy=0.603245, train/bleu=27.340817, train/loss=2.035079, validation/accuracy=0.620587, validation/bleu=25.087786, validation/loss=1.893521, validation/num_examples=3000
I0209 11:34:40.099630 140051005212416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.5864450931549072, loss=2.1319899559020996
I0209 11:35:14.469911 140050996819712 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.27537044882774353, loss=2.095562696456909
I0209 11:35:48.964356 140051005212416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2649148106575012, loss=2.090125322341919
I0209 11:36:23.491607 140050996819712 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.3518906831741333, loss=2.1407008171081543
I0209 11:36:58.047612 140051005212416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.40448877215385437, loss=1.955122947692871
I0209 11:37:32.590460 140050996819712 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.2840251624584198, loss=2.1204288005828857
I0209 11:38:07.125185 140051005212416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5501675009727478, loss=2.0941715240478516
I0209 11:38:41.666429 140050996819712 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.5656886696815491, loss=2.037135601043701
I0209 11:39:16.232643 140051005212416 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4780905842781067, loss=2.0900444984436035
I0209 11:39:50.818889 140050996819712 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.5971707105636597, loss=2.038203239440918
I0209 11:40:25.390200 140051005212416 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3621598482131958, loss=2.056791305541992
I0209 11:40:59.952893 140050996819712 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2769090235233307, loss=2.1340208053588867
I0209 11:41:34.503249 140051005212416 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4007035791873932, loss=2.062307119369507
I0209 11:42:09.040297 140050996819712 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3610641658306122, loss=2.011326789855957
I0209 11:42:43.609041 140051005212416 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.36740949749946594, loss=2.0522589683532715
I0209 11:43:18.146129 140050996819712 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.40174347162246704, loss=2.046147584915161
I0209 11:43:52.703139 140051005212416 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.33783093094825745, loss=2.0081088542938232
I0209 11:44:27.261680 140050996819712 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2998887300491333, loss=2.0301618576049805
I0209 11:45:01.780337 140051005212416 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.25516432523727417, loss=1.969560980796814
I0209 11:45:36.311810 140050996819712 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.5720809102058411, loss=2.0904178619384766
I0209 11:46:10.849256 140051005212416 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.5748318433761597, loss=2.156872034072876
I0209 11:46:45.394694 140050996819712 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5896413326263428, loss=2.012586832046509
I0209 11:47:19.937067 140051005212416 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.41981491446495056, loss=2.077185869216919
I0209 11:47:54.490673 140050996819712 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.5128368139266968, loss=2.094630718231201
I0209 11:48:29.005028 140051005212416 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.32316461205482483, loss=2.067613124847412
I0209 11:48:37.370325 140225696298816 spec.py:321] Evaluating on the training split.
I0209 11:48:40.340474 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:51:41.860796 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 11:51:44.528630 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:55:01.972872 140225696298816 spec.py:349] Evaluating on the test split.
I0209 11:55:04.649551 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 11:57:39.890097 140225696298816 submission_runner.py:408] Time since start: 14566.97s, 	Step: 24326, 	{'train/accuracy': 0.6028209328651428, 'train/loss': 2.0420238971710205, 'train/bleu': 28.100459720929965, 'validation/accuracy': 0.6226333379745483, 'validation/loss': 1.8778119087219238, 'validation/bleu': 24.9253727665481, 'validation/num_examples': 3000, 'test/accuracy': 0.6287490725517273, 'test/loss': 1.8355058431625366, 'test/bleu': 24.262781323959942, 'test/num_examples': 3003, 'score': 8428.134506702423, 'total_duration': 14566.971681118011, 'accumulated_submission_time': 8428.134506702423, 'accumulated_eval_time': 6137.813019990921, 'accumulated_logging_time': 0.26431941986083984}
I0209 11:57:39.908235 140050996819712 logging_writer.py:48] [24326] accumulated_eval_time=6137.813020, accumulated_logging_time=0.264319, accumulated_submission_time=8428.134507, global_step=24326, preemption_count=0, score=8428.134507, test/accuracy=0.628749, test/bleu=24.262781, test/loss=1.835506, test/num_examples=3003, total_duration=14566.971681, train/accuracy=0.602821, train/bleu=28.100460, train/loss=2.042024, validation/accuracy=0.622633, validation/bleu=24.925373, validation/loss=1.877812, validation/num_examples=3000
I0209 11:58:05.763275 140051005212416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2784055769443512, loss=2.0588786602020264
I0209 11:58:40.205693 140050996819712 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2999097406864166, loss=2.0433051586151123
I0209 11:59:14.740386 140051005212416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3308278024196625, loss=2.0635416507720947
I0209 11:59:49.290302 140050996819712 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.3621462881565094, loss=2.0472652912139893
I0209 12:00:23.845281 140051005212416 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3123711049556732, loss=2.071342945098877
I0209 12:00:58.410907 140050996819712 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.4118078649044037, loss=2.019308567047119
I0209 12:01:32.949208 140051005212416 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3132230341434479, loss=1.9980388879776
I0209 12:02:07.513859 140050996819712 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2780289947986603, loss=2.026557683944702
I0209 12:02:42.102076 140051005212416 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3487670123577118, loss=2.046410083770752
I0209 12:03:16.635998 140050996819712 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4492294490337372, loss=2.1518924236297607
I0209 12:03:51.170998 140051005212416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.252266526222229, loss=2.026548385620117
I0209 12:04:25.729074 140050996819712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2666078805923462, loss=1.9293737411499023
I0209 12:05:00.289203 140051005212416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.5538670420646667, loss=1.9654215574264526
I0209 12:05:34.844545 140050996819712 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.34681588411331177, loss=2.073960542678833
I0209 12:06:09.405846 140051005212416 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2854771614074707, loss=2.185338258743286
I0209 12:06:43.987418 140050996819712 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.3404597342014313, loss=1.947934627532959
I0209 12:07:18.553979 140051005212416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5629242658615112, loss=1.9687961339950562
I0209 12:07:53.109099 140050996819712 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.4946150779724121, loss=1.9938898086547852
I0209 12:08:27.683472 140051005212416 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.39069733023643494, loss=2.08194899559021
I0209 12:09:02.228984 140050996819712 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4418538808822632, loss=2.0073909759521484
I0209 12:09:36.773151 140051005212416 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.30939358472824097, loss=2.079219102859497
I0209 12:10:11.338598 140050996819712 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.34621918201446533, loss=2.0373947620391846
I0209 12:10:45.901165 140051005212416 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.4392241835594177, loss=2.092665910720825
I0209 12:11:20.473500 140050996819712 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3851900100708008, loss=2.037184000015259
I0209 12:11:40.224694 140225696298816 spec.py:321] Evaluating on the training split.
I0209 12:11:43.189875 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:15:15.971363 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 12:15:18.645421 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:18:15.948419 140225696298816 spec.py:349] Evaluating on the test split.
I0209 12:18:18.614600 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:20:58.102261 140225696298816 submission_runner.py:408] Time since start: 15965.18s, 	Step: 26759, 	{'train/accuracy': 0.6044977307319641, 'train/loss': 2.021979808807373, 'train/bleu': 28.284201263564533, 'validation/accuracy': 0.624183177947998, 'validation/loss': 1.8740408420562744, 'validation/bleu': 25.36061576960488, 'validation/num_examples': 3000, 'test/accuracy': 0.6294578909873962, 'test/loss': 1.8357408046722412, 'test/bleu': 24.53579871136742, 'test/num_examples': 3003, 'score': 9268.36467552185, 'total_duration': 15965.183842658997, 'accumulated_submission_time': 9268.36467552185, 'accumulated_eval_time': 6695.690530538559, 'accumulated_logging_time': 0.29262280464172363}
I0209 12:20:58.120948 140051005212416 logging_writer.py:48] [26759] accumulated_eval_time=6695.690531, accumulated_logging_time=0.292623, accumulated_submission_time=9268.364676, global_step=26759, preemption_count=0, score=9268.364676, test/accuracy=0.629458, test/bleu=24.535799, test/loss=1.835741, test/num_examples=3003, total_duration=15965.183843, train/accuracy=0.604498, train/bleu=28.284201, train/loss=2.021980, validation/accuracy=0.624183, validation/bleu=25.360616, validation/loss=1.874041, validation/num_examples=3000
I0209 12:21:12.565871 140050996819712 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.36845386028289795, loss=2.044854164123535
I0209 12:21:46.998085 140051005212416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.426323801279068, loss=2.1061463356018066
I0209 12:22:21.533543 140050996819712 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5100690126419067, loss=1.9709776639938354
I0209 12:22:56.089242 140051005212416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.44918569922447205, loss=2.00809907913208
I0209 12:23:30.637391 140050996819712 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3084762990474701, loss=1.9899762868881226
I0209 12:24:05.179551 140051005212416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3692527413368225, loss=2.0610835552215576
I0209 12:24:39.711503 140050996819712 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.2671453058719635, loss=1.9304088354110718
I0209 12:25:14.250702 140051005212416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4660875201225281, loss=2.076171875
I0209 12:25:48.789505 140050996819712 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4126138985157013, loss=2.030956268310547
I0209 12:26:23.346435 140051005212416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.42072147130966187, loss=2.0262508392333984
I0209 12:26:57.892840 140050996819712 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.45209234952926636, loss=2.0725600719451904
I0209 12:27:32.438236 140051005212416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.2857038378715515, loss=2.0129058361053467
I0209 12:28:06.979475 140050996819712 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.42979589104652405, loss=2.0806989669799805
I0209 12:28:41.515870 140051005212416 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.27280402183532715, loss=2.018683671951294
I0209 12:29:16.037286 140050996819712 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.32275477051734924, loss=2.0254406929016113
I0209 12:29:50.584948 140051005212416 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.6777692437171936, loss=2.0086116790771484
I0209 12:30:25.112572 140050996819712 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3158666491508484, loss=2.0717859268188477
I0209 12:30:59.638504 140051005212416 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3380373418331146, loss=2.083627462387085
I0209 12:31:34.180716 140050996819712 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.28189483284950256, loss=2.0674824714660645
I0209 12:32:08.740586 140051005212416 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.25719842314720154, loss=2.065511465072632
I0209 12:32:43.303246 140050996819712 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3403599262237549, loss=2.064307451248169
I0209 12:33:17.853267 140051005212416 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.5749557018280029, loss=2.0495426654815674
I0209 12:33:52.404134 140050996819712 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.40419313311576843, loss=2.184831142425537
I0209 12:34:26.963214 140051005212416 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.4237205982208252, loss=2.0119125843048096
I0209 12:34:58.121536 140225696298816 spec.py:321] Evaluating on the training split.
I0209 12:35:01.095156 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:37:57.498732 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 12:38:00.171854 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:40:39.796737 140225696298816 spec.py:349] Evaluating on the test split.
I0209 12:40:42.473742 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 12:43:10.887899 140225696298816 submission_runner.py:408] Time since start: 17297.97s, 	Step: 29192, 	{'train/accuracy': 0.6068159937858582, 'train/loss': 2.0194811820983887, 'train/bleu': 28.95645104575732, 'validation/accuracy': 0.623960018157959, 'validation/loss': 1.8713575601577759, 'validation/bleu': 25.256518914297175, 'validation/num_examples': 3000, 'test/accuracy': 0.6307710409164429, 'test/loss': 1.8219873905181885, 'test/bleu': 24.321099015768755, 'test/num_examples': 3003, 'score': 10108.279440879822, 'total_duration': 17297.96948671341, 'accumulated_submission_time': 10108.279440879822, 'accumulated_eval_time': 7188.456845998764, 'accumulated_logging_time': 0.3211045265197754}
I0209 12:43:10.906659 140050996819712 logging_writer.py:48] [29192] accumulated_eval_time=7188.456846, accumulated_logging_time=0.321105, accumulated_submission_time=10108.279441, global_step=29192, preemption_count=0, score=10108.279441, test/accuracy=0.630771, test/bleu=24.321099, test/loss=1.821987, test/num_examples=3003, total_duration=17297.969487, train/accuracy=0.606816, train/bleu=28.956451, train/loss=2.019481, validation/accuracy=0.623960, validation/bleu=25.256519, validation/loss=1.871358, validation/num_examples=3000
I0209 12:43:14.012705 140051005212416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5422027111053467, loss=2.0008738040924072
I0209 12:43:48.392033 140050996819712 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.28768622875213623, loss=2.0430257320404053
I0209 12:44:22.861747 140051005212416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.2600243091583252, loss=2.0263521671295166
I0209 12:44:57.373695 140050996819712 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.4211442172527313, loss=1.9883090257644653
I0209 12:45:31.938695 140051005212416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3604462146759033, loss=1.9889286756515503
I0209 12:46:06.483895 140050996819712 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.38232526183128357, loss=2.083420753479004
I0209 12:46:41.048156 140051005212416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5503671765327454, loss=2.0338222980499268
I0209 12:47:15.591182 140050996819712 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.32156363129615784, loss=1.9704197645187378
I0209 12:47:50.141649 140051005212416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.30834051966667175, loss=1.9960542917251587
I0209 12:48:24.672908 140050996819712 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.4469611644744873, loss=1.9612674713134766
I0209 12:48:59.192387 140051005212416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.34322962164878845, loss=2.098142385482788
I0209 12:49:33.719170 140050996819712 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3539329767227173, loss=1.9879260063171387
I0209 12:50:08.273551 140051005212416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.3991604447364807, loss=1.97665274143219
I0209 12:50:42.812707 140050996819712 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.40791749954223633, loss=2.072737455368042
I0209 12:51:17.370618 140051005212416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.23671196401119232, loss=2.0081558227539062
I0209 12:51:51.929451 140050996819712 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.3571739196777344, loss=1.9992114305496216
I0209 12:52:26.502007 140051005212416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.29175037145614624, loss=2.0362319946289062
I0209 12:53:01.070100 140050996819712 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.26826849579811096, loss=1.8919682502746582
I0209 12:53:35.628530 140051005212416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.26516735553741455, loss=1.9835801124572754
I0209 12:54:10.196192 140050996819712 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.24393104016780853, loss=1.961661458015442
I0209 12:54:44.755631 140051005212416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2767386734485626, loss=1.9658198356628418
I0209 12:55:19.314706 140050996819712 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.4696606695652008, loss=2.02671217918396
I0209 12:55:53.888686 140051005212416 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2874816358089447, loss=1.9265787601470947
I0209 12:56:28.428117 140050996819712 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.37690678238868713, loss=2.0025265216827393
I0209 12:57:02.990017 140051005212416 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.3074515759944916, loss=2.058640480041504
I0209 12:57:11.008747 140225696298816 spec.py:321] Evaluating on the training split.
I0209 12:57:13.970487 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:00:09.409096 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 13:00:12.105110 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:03:03.355954 140225696298816 spec.py:349] Evaluating on the test split.
I0209 13:03:06.044251 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:05:47.941836 140225696298816 submission_runner.py:408] Time since start: 18655.02s, 	Step: 31625, 	{'train/accuracy': 0.6236925721168518, 'train/loss': 1.8840527534484863, 'train/bleu': 30.04438775326001, 'validation/accuracy': 0.6268489956855774, 'validation/loss': 1.861114740371704, 'validation/bleu': 25.799430214237688, 'validation/num_examples': 3000, 'test/accuracy': 0.6341409683227539, 'test/loss': 1.8083019256591797, 'test/bleu': 25.021743492191217, 'test/num_examples': 3003, 'score': 10948.29575920105, 'total_duration': 18655.023404359818, 'accumulated_submission_time': 10948.29575920105, 'accumulated_eval_time': 7705.389865159988, 'accumulated_logging_time': 0.3495962619781494}
I0209 13:05:47.962472 140050996819712 logging_writer.py:48] [31625] accumulated_eval_time=7705.389865, accumulated_logging_time=0.349596, accumulated_submission_time=10948.295759, global_step=31625, preemption_count=0, score=10948.295759, test/accuracy=0.634141, test/bleu=25.021743, test/loss=1.808302, test/num_examples=3003, total_duration=18655.023404, train/accuracy=0.623693, train/bleu=30.044388, train/loss=1.884053, validation/accuracy=0.626849, validation/bleu=25.799430, validation/loss=1.861115, validation/num_examples=3000
I0209 13:06:14.136634 140051005212416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4074423313140869, loss=2.00675630569458
I0209 13:06:48.580688 140050996819712 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.21735496819019318, loss=2.0427608489990234
I0209 13:07:23.117674 140051005212416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.35405752062797546, loss=2.0592010021209717
I0209 13:07:57.657348 140050996819712 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3831097483634949, loss=2.0202322006225586
I0209 13:08:32.234534 140051005212416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3752583861351013, loss=2.0858066082000732
I0209 13:09:06.810542 140050996819712 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.3278617858886719, loss=1.9854272603988647
I0209 13:09:41.368083 140051005212416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.27700570225715637, loss=1.945290207862854
I0209 13:10:15.924107 140050996819712 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.24288800358772278, loss=1.9922562837600708
I0209 13:10:50.481197 140051005212416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3037455379962921, loss=1.966856837272644
I0209 13:11:25.037007 140050996819712 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2806350886821747, loss=1.9246128797531128
I0209 13:11:59.573776 140051005212416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.43272918462753296, loss=2.072186231613159
I0209 13:12:34.137410 140050996819712 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.41753217577934265, loss=2.074716091156006
I0209 13:13:08.687965 140051005212416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.33112919330596924, loss=2.0518271923065186
I0209 13:13:43.248043 140050996819712 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.30171576142311096, loss=2.0130653381347656
I0209 13:14:17.755220 140051005212416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.2953643202781677, loss=2.020561695098877
I0209 13:14:52.290698 140050996819712 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.25978460907936096, loss=2.0413320064544678
I0209 13:15:26.859578 140051005212416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.26788222789764404, loss=2.0224075317382812
I0209 13:16:01.422717 140050996819712 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.29508501291275024, loss=2.091740608215332
I0209 13:16:35.973168 140051005212416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3009411096572876, loss=2.0447874069213867
I0209 13:17:10.528878 140050996819712 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2821042239665985, loss=2.0076587200164795
I0209 13:17:45.092324 140051005212416 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.3123885989189148, loss=2.0098888874053955
I0209 13:18:19.683743 140050996819712 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3105621933937073, loss=2.044095516204834
I0209 13:18:54.219995 140051005212416 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.2536754012107849, loss=1.9169247150421143
I0209 13:19:28.742789 140050996819712 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.27466246485710144, loss=2.0279176235198975
I0209 13:19:48.181619 140225696298816 spec.py:321] Evaluating on the training split.
I0209 13:19:51.149426 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:23:59.235604 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 13:24:01.903954 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:26:48.276350 140225696298816 spec.py:349] Evaluating on the test split.
I0209 13:26:50.955411 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:29:45.931464 140225696298816 submission_runner.py:408] Time since start: 20093.01s, 	Step: 34058, 	{'train/accuracy': 0.6058012843132019, 'train/loss': 2.0277810096740723, 'train/bleu': 28.870511638615522, 'validation/accuracy': 0.6283245086669922, 'validation/loss': 1.8390883207321167, 'validation/bleu': 25.78896790070437, 'validation/num_examples': 3000, 'test/accuracy': 0.6344314813613892, 'test/loss': 1.7929351329803467, 'test/bleu': 24.712103237909357, 'test/num_examples': 3003, 'score': 11788.428247213364, 'total_duration': 20093.013048648834, 'accumulated_submission_time': 11788.428247213364, 'accumulated_eval_time': 8303.139657497406, 'accumulated_logging_time': 0.38037562370300293}
I0209 13:29:45.950836 140051005212416 logging_writer.py:48] [34058] accumulated_eval_time=8303.139657, accumulated_logging_time=0.380376, accumulated_submission_time=11788.428247, global_step=34058, preemption_count=0, score=11788.428247, test/accuracy=0.634431, test/bleu=24.712103, test/loss=1.792935, test/num_examples=3003, total_duration=20093.013049, train/accuracy=0.605801, train/bleu=28.870512, train/loss=2.027781, validation/accuracy=0.628325, validation/bleu=25.788968, validation/loss=1.839088, validation/num_examples=3000
I0209 13:30:00.743714 140050996819712 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.30763307213783264, loss=2.043081760406494
I0209 13:30:35.176612 140051005212416 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.26967114210128784, loss=2.056440830230713
I0209 13:31:09.672426 140050996819712 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.4409824311733246, loss=1.9569441080093384
I0209 13:31:44.221072 140051005212416 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.28921347856521606, loss=1.9451770782470703
I0209 13:32:18.778614 140050996819712 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2684697210788727, loss=2.0004048347473145
I0209 13:32:53.345733 140051005212416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.29646486043930054, loss=1.9847168922424316
I0209 13:33:27.898248 140050996819712 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.43251922726631165, loss=2.023160696029663
I0209 13:34:02.446572 140051005212416 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.3274332880973816, loss=1.9609310626983643
I0209 13:34:37.010532 140050996819712 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.373504102230072, loss=2.0438764095306396
I0209 13:35:11.573839 140051005212416 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.45133325457572937, loss=1.9598431587219238
I0209 13:35:46.131700 140050996819712 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.38164880871772766, loss=2.110872983932495
I0209 13:36:20.680486 140051005212416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3214205801486969, loss=1.9790061712265015
I0209 13:36:55.231642 140050996819712 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2320118397474289, loss=2.0794577598571777
I0209 13:37:29.757932 140051005212416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.36528831720352173, loss=2.014531373977661
I0209 13:38:04.282175 140050996819712 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.38837793469429016, loss=2.021935224533081
I0209 13:38:38.849611 140051005212416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3945339620113373, loss=1.9681766033172607
I0209 13:39:13.404975 140050996819712 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.34117263555526733, loss=2.0229363441467285
I0209 13:39:47.959973 140051005212416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.32261109352111816, loss=2.0341742038726807
I0209 13:40:22.512550 140050996819712 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.3766743242740631, loss=1.9681109189987183
I0209 13:40:57.069017 140051005212416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3535159230232239, loss=2.049975872039795
I0209 13:41:31.610981 140050996819712 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2501763105392456, loss=2.0775866508483887
I0209 13:42:06.161002 140051005212416 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.33653149008750916, loss=1.9996211528778076
I0209 13:42:40.741684 140050996819712 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2667543590068817, loss=1.9856204986572266
I0209 13:43:15.289694 140051005212416 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.38762569427490234, loss=2.0808041095733643
I0209 13:43:46.120087 140225696298816 spec.py:321] Evaluating on the training split.
I0209 13:43:49.082915 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:47:02.206559 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 13:47:04.883557 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:49:57.683153 140225696298816 spec.py:349] Evaluating on the test split.
I0209 13:50:00.357677 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 13:52:37.560970 140225696298816 submission_runner.py:408] Time since start: 21464.64s, 	Step: 36491, 	{'train/accuracy': 0.608106255531311, 'train/loss': 2.017210006713867, 'train/bleu': 28.904658555369576, 'validation/accuracy': 0.6289692521095276, 'validation/loss': 1.8452839851379395, 'validation/bleu': 26.038305212224035, 'validation/num_examples': 3000, 'test/accuracy': 0.6373133659362793, 'test/loss': 1.7856651544570923, 'test/bleu': 25.120160265789195, 'test/num_examples': 3003, 'score': 12628.50937962532, 'total_duration': 21464.642553329468, 'accumulated_submission_time': 12628.50937962532, 'accumulated_eval_time': 8834.580487966537, 'accumulated_logging_time': 0.41150975227355957}
I0209 13:52:37.581056 140050996819712 logging_writer.py:48] [36491] accumulated_eval_time=8834.580488, accumulated_logging_time=0.411510, accumulated_submission_time=12628.509380, global_step=36491, preemption_count=0, score=12628.509380, test/accuracy=0.637313, test/bleu=25.120160, test/loss=1.785665, test/num_examples=3003, total_duration=21464.642553, train/accuracy=0.608106, train/bleu=28.904659, train/loss=2.017210, validation/accuracy=0.628969, validation/bleu=26.038305, validation/loss=1.845284, validation/num_examples=3000
I0209 13:52:41.045352 140051005212416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.49584898352622986, loss=1.9451748132705688
I0209 13:53:15.460712 140050996819712 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.31396031379699707, loss=2.0446083545684814
I0209 13:53:49.943482 140051005212416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.25113609433174133, loss=1.95671808719635
I0209 13:54:24.507100 140050996819712 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.29693058133125305, loss=2.041224241256714
I0209 13:54:59.036169 140051005212416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.2634219527244568, loss=2.040262460708618
I0209 13:55:33.607767 140050996819712 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3499566316604614, loss=1.9778578281402588
I0209 13:56:08.146153 140051005212416 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.4276624321937561, loss=1.9843873977661133
I0209 13:56:42.712166 140050996819712 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.48586133122444153, loss=2.059269666671753
I0209 13:57:17.293524 140051005212416 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4326293170452118, loss=1.992140293121338
I0209 13:57:51.850480 140050996819712 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.31560003757476807, loss=2.0078110694885254
I0209 13:58:26.403410 140051005212416 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.41460666060447693, loss=2.028998613357544
I0209 13:59:00.969727 140050996819712 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.674567461013794, loss=2.01001238822937
I0209 13:59:35.533985 140051005212416 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.4306011497974396, loss=2.0276384353637695
I0209 14:00:10.092502 140050996819712 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2630106806755066, loss=1.9406780004501343
I0209 14:00:44.661208 140051005212416 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.3443464934825897, loss=2.0777010917663574
I0209 14:01:19.221260 140050996819712 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3211632966995239, loss=1.9772599935531616
I0209 14:01:53.761155 140051005212416 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.29097288846969604, loss=1.9413832426071167
I0209 14:02:28.352749 140050996819712 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.32183361053466797, loss=2.007081985473633
I0209 14:03:02.921584 140051005212416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.29031458497047424, loss=1.9908487796783447
I0209 14:03:37.505790 140050996819712 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.41436779499053955, loss=1.9564541578292847
I0209 14:04:12.052961 140051005212416 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3945947587490082, loss=1.9923979043960571
I0209 14:04:46.644645 140050996819712 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.4209066927433014, loss=2.0471975803375244
I0209 14:05:21.190495 140051005212416 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.46470704674720764, loss=1.957963228225708
I0209 14:05:55.764855 140050996819712 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.35030752420425415, loss=1.9741098880767822
I0209 14:06:30.312925 140051005212416 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.36157166957855225, loss=2.0539848804473877
I0209 14:06:37.643595 140225696298816 spec.py:321] Evaluating on the training split.
I0209 14:06:40.621026 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:09:46.893476 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 14:09:49.559506 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:12:27.687854 140225696298816 spec.py:349] Evaluating on the test split.
I0209 14:12:30.365971 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:14:52.756196 140225696298816 submission_runner.py:408] Time since start: 22799.84s, 	Step: 38923, 	{'train/accuracy': 0.6152269244194031, 'train/loss': 1.9483067989349365, 'train/bleu': 28.735497170260977, 'validation/accuracy': 0.6286717057228088, 'validation/loss': 1.8417868614196777, 'validation/bleu': 25.679533221535827, 'validation/num_examples': 3000, 'test/accuracy': 0.6345128417015076, 'test/loss': 1.7959977388381958, 'test/bleu': 24.616820990422042, 'test/num_examples': 3003, 'score': 13468.485274791718, 'total_duration': 22799.837777614594, 'accumulated_submission_time': 13468.485274791718, 'accumulated_eval_time': 9329.693039894104, 'accumulated_logging_time': 0.4427659511566162}
I0209 14:14:52.776779 140050996819712 logging_writer.py:48] [38923] accumulated_eval_time=9329.693040, accumulated_logging_time=0.442766, accumulated_submission_time=13468.485275, global_step=38923, preemption_count=0, score=13468.485275, test/accuracy=0.634513, test/bleu=24.616821, test/loss=1.795998, test/num_examples=3003, total_duration=22799.837778, train/accuracy=0.615227, train/bleu=28.735497, train/loss=1.948307, validation/accuracy=0.628672, validation/bleu=25.679533, validation/loss=1.841787, validation/num_examples=3000
I0209 14:15:19.604909 140051005212416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3060113787651062, loss=1.9049592018127441
I0209 14:15:54.057570 140050996819712 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.2778420150279999, loss=1.9981236457824707
I0209 14:16:28.607876 140051005212416 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.34218868613243103, loss=2.0494542121887207
I0209 14:17:03.143124 140050996819712 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.39543813467025757, loss=2.12542462348938
I0209 14:17:37.690930 140051005212416 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.4069973826408386, loss=1.9893453121185303
I0209 14:18:12.222586 140050996819712 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.27969878911972046, loss=1.951271653175354
I0209 14:18:46.771922 140051005212416 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.39058586955070496, loss=1.9746636152267456
I0209 14:19:21.344193 140050996819712 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3258879780769348, loss=1.9950839281082153
I0209 14:19:55.912687 140051005212416 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4081023931503296, loss=1.9944740533828735
I0209 14:20:30.438710 140050996819712 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3210115432739258, loss=1.9198065996170044
I0209 14:21:04.990084 140051005212416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.34728434681892395, loss=1.95522940158844
I0209 14:21:39.534715 140050996819712 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2848559021949768, loss=1.9223597049713135
I0209 14:22:14.069863 140051005212416 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3233477473258972, loss=1.9668134450912476
I0209 14:22:48.626368 140050996819712 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.3140525221824646, loss=2.043677568435669
I0209 14:23:23.187198 140051005212416 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.32132282853126526, loss=2.056513547897339
I0209 14:23:57.737243 140050996819712 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4440934360027313, loss=1.981905221939087
I0209 14:24:32.270661 140051005212416 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.36886724829673767, loss=1.9268403053283691
I0209 14:25:06.807249 140050996819712 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3394624590873718, loss=1.9867366552352905
I0209 14:25:41.369945 140051005212416 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3256116211414337, loss=2.0275211334228516
I0209 14:26:15.941047 140050996819712 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.2683170735836029, loss=2.058202028274536
I0209 14:26:50.471140 140051005212416 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.38578125834465027, loss=1.9173376560211182
I0209 14:27:25.038438 140050996819712 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.5456178188323975, loss=2.0534355640411377
I0209 14:27:59.613030 140051005212416 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.26164814829826355, loss=1.8958356380462646
I0209 14:28:34.166438 140050996819712 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2870583236217499, loss=1.9862996339797974
I0209 14:28:52.895523 140225696298816 spec.py:321] Evaluating on the training split.
I0209 14:28:55.857916 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:31:28.422842 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 14:31:31.098076 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:33:56.800288 140225696298816 spec.py:349] Evaluating on the test split.
I0209 14:33:59.466901 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:36:19.011208 140225696298816 submission_runner.py:408] Time since start: 24086.09s, 	Step: 41356, 	{'train/accuracy': 0.6089126467704773, 'train/loss': 1.994400978088379, 'train/bleu': 28.389003946961708, 'validation/accuracy': 0.6299487948417664, 'validation/loss': 1.834946632385254, 'validation/bleu': 25.0824188802018, 'validation/num_examples': 3000, 'test/accuracy': 0.6367555856704712, 'test/loss': 1.777856707572937, 'test/bleu': 24.162457512803574, 'test/num_examples': 3003, 'score': 14308.5158598423, 'total_duration': 24086.092776298523, 'accumulated_submission_time': 14308.5158598423, 'accumulated_eval_time': 9775.80866074562, 'accumulated_logging_time': 0.4757249355316162}
I0209 14:36:19.032163 140051005212416 logging_writer.py:48] [41356] accumulated_eval_time=9775.808661, accumulated_logging_time=0.475725, accumulated_submission_time=14308.515860, global_step=41356, preemption_count=0, score=14308.515860, test/accuracy=0.636756, test/bleu=24.162458, test/loss=1.777857, test/num_examples=3003, total_duration=24086.092776, train/accuracy=0.608913, train/bleu=28.389004, train/loss=1.994401, validation/accuracy=0.629949, validation/bleu=25.082419, validation/loss=1.834947, validation/num_examples=3000
I0209 14:36:34.511052 140050996819712 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.29480716586112976, loss=2.0028345584869385
I0209 14:37:08.941772 140051005212416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.30301791429519653, loss=1.8349772691726685
I0209 14:37:43.412027 140050996819712 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.36209744215011597, loss=2.037245512008667
I0209 14:38:17.986260 140051005212416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.30413907766342163, loss=1.9259402751922607
I0209 14:38:52.504096 140050996819712 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.44570761919021606, loss=2.059887409210205
I0209 14:39:27.061491 140051005212416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3095338046550751, loss=2.0849149227142334
I0209 14:40:01.620665 140050996819712 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.24042505025863647, loss=1.9614934921264648
I0209 14:40:36.171942 140051005212416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.40152719616889954, loss=2.108321189880371
I0209 14:41:10.726219 140050996819712 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4218917191028595, loss=2.003586769104004
I0209 14:41:45.307778 140051005212416 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.33425283432006836, loss=2.066542625427246
I0209 14:42:19.843962 140050996819712 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2771998941898346, loss=1.945900321006775
I0209 14:42:54.392382 140051005212416 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.43582990765571594, loss=1.9694163799285889
I0209 14:43:28.946715 140050996819712 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.316056489944458, loss=2.006662607192993
I0209 14:44:03.490687 140051005212416 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3110730051994324, loss=1.9298151731491089
I0209 14:44:38.048043 140050996819712 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3559737801551819, loss=2.0188891887664795
I0209 14:45:12.616377 140051005212416 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.39537084102630615, loss=1.9962564706802368
I0209 14:45:47.129486 140050996819712 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.2917998731136322, loss=2.0161986351013184
I0209 14:46:21.672986 140051005212416 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.33870643377304077, loss=1.9839266538619995
I0209 14:46:56.228691 140050996819712 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.2969510555267334, loss=1.9531986713409424
I0209 14:47:30.795477 140051005212416 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3174023926258087, loss=1.9598612785339355
I0209 14:48:05.355115 140050996819712 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.26443496346473694, loss=1.9456158876419067
I0209 14:48:39.913312 140051005212416 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3511945903301239, loss=1.9407527446746826
I0209 14:49:14.479506 140050996819712 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.29627546668052673, loss=1.9754490852355957
I0209 14:49:49.012728 140051005212416 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.39535415172576904, loss=1.9547631740570068
I0209 14:50:19.131327 140225696298816 spec.py:321] Evaluating on the training split.
I0209 14:50:22.096766 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:54:32.472908 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 14:54:35.140136 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 14:59:00.657970 140225696298816 spec.py:349] Evaluating on the test split.
I0209 14:59:03.334565 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:02:54.506158 140225696298816 submission_runner.py:408] Time since start: 25681.59s, 	Step: 43789, 	{'train/accuracy': 0.6921001672744751, 'train/loss': 1.4866576194763184, 'train/bleu': 35.19845928097932, 'validation/accuracy': 0.6321682333946228, 'validation/loss': 1.8150848150253296, 'validation/bleu': 25.890601540900896, 'validation/num_examples': 3000, 'test/accuracy': 0.6381616592407227, 'test/loss': 1.7659757137298584, 'test/bleu': 25.0090002959881, 'test/num_examples': 3003, 'score': 15148.529332399368, 'total_duration': 25681.5877430439, 'accumulated_submission_time': 15148.529332399368, 'accumulated_eval_time': 10531.183442354202, 'accumulated_logging_time': 0.5067262649536133}
I0209 15:02:54.526582 140050996819712 logging_writer.py:48] [43789] accumulated_eval_time=10531.183442, accumulated_logging_time=0.506726, accumulated_submission_time=15148.529332, global_step=43789, preemption_count=0, score=15148.529332, test/accuracy=0.638162, test/bleu=25.009000, test/loss=1.765976, test/num_examples=3003, total_duration=25681.587743, train/accuracy=0.692100, train/bleu=35.198459, train/loss=1.486658, validation/accuracy=0.632168, validation/bleu=25.890602, validation/loss=1.815085, validation/num_examples=3000
I0209 15:02:58.677618 140051005212416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.29593998193740845, loss=2.04793643951416
I0209 15:03:33.058712 140050996819712 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.4192264974117279, loss=1.9286999702453613
I0209 15:04:07.575690 140051005212416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2873364984989166, loss=1.9591139554977417
I0209 15:04:42.127114 140050996819712 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3226774036884308, loss=2.040354013442993
I0209 15:05:16.656025 140051005212416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.261107474565506, loss=1.8704721927642822
I0209 15:05:51.233186 140050996819712 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2569833993911743, loss=2.030266523361206
I0209 15:06:25.773249 140051005212416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.28854644298553467, loss=1.9442310333251953
I0209 15:07:00.300014 140050996819712 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.37239357829093933, loss=2.001495122909546
I0209 15:07:34.822657 140051005212416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.29741448163986206, loss=1.9776650667190552
I0209 15:08:09.393293 140050996819712 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2768165171146393, loss=2.0196304321289062
I0209 15:08:43.952066 140051005212416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.3462927043437958, loss=1.9425556659698486
I0209 15:09:18.484212 140050996819712 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.31668421626091003, loss=1.9159958362579346
I0209 15:09:53.042301 140051005212416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.28998905420303345, loss=1.9435398578643799
I0209 15:10:27.601280 140050996819712 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.26558348536491394, loss=1.9929381608963013
I0209 15:11:02.133571 140051005212416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.22835691273212433, loss=1.9670319557189941
I0209 15:11:36.648763 140050996819712 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3724001348018646, loss=1.945016622543335
I0209 15:12:11.194245 140051005212416 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3295689821243286, loss=2.00142502784729
I0209 15:12:45.746692 140050996819712 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2620173990726471, loss=1.9631351232528687
I0209 15:13:20.303330 140051005212416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3727465271949768, loss=1.94935142993927
I0209 15:13:54.830726 140050996819712 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.26167887449264526, loss=2.0165793895721436
I0209 15:14:29.402209 140051005212416 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2652392089366913, loss=1.9700840711593628
I0209 15:15:03.974089 140050996819712 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.5634061694145203, loss=2.012643575668335
I0209 15:15:38.537492 140051005212416 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3495801091194153, loss=1.9668254852294922
I0209 15:16:13.096497 140050996819712 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.4569340944290161, loss=2.0298454761505127
I0209 15:16:47.676538 140051005212416 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.297123521566391, loss=1.9840009212493896
I0209 15:16:54.653406 140225696298816 spec.py:321] Evaluating on the training split.
I0209 15:16:57.622984 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:20:23.820788 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 15:20:26.491026 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:23:06.449933 140225696298816 spec.py:349] Evaluating on the test split.
I0209 15:23:09.127754 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:25:35.630859 140225696298816 submission_runner.py:408] Time since start: 27042.71s, 	Step: 46222, 	{'train/accuracy': 0.613900899887085, 'train/loss': 1.9534741640090942, 'train/bleu': 28.758410706935436, 'validation/accuracy': 0.6313250660896301, 'validation/loss': 1.8070567846298218, 'validation/bleu': 25.706606452819305, 'validation/num_examples': 3000, 'test/accuracy': 0.6401255130767822, 'test/loss': 1.7569904327392578, 'test/bleu': 24.833041601835642, 'test/num_examples': 3003, 'score': 15988.571270942688, 'total_duration': 27042.712433576584, 'accumulated_submission_time': 15988.571270942688, 'accumulated_eval_time': 11052.160829782486, 'accumulated_logging_time': 0.5364077091217041}
I0209 15:25:35.651966 140050996819712 logging_writer.py:48] [46222] accumulated_eval_time=11052.160830, accumulated_logging_time=0.536408, accumulated_submission_time=15988.571271, global_step=46222, preemption_count=0, score=15988.571271, test/accuracy=0.640126, test/bleu=24.833042, test/loss=1.756990, test/num_examples=3003, total_duration=27042.712434, train/accuracy=0.613901, train/bleu=28.758411, train/loss=1.953474, validation/accuracy=0.631325, validation/bleu=25.706606, validation/loss=1.807057, validation/num_examples=3000
I0209 15:26:02.831367 140051005212416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.22606819868087769, loss=1.9006104469299316
I0209 15:26:37.280693 140050996819712 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.32396164536476135, loss=1.8898861408233643
I0209 15:27:11.791723 140051005212416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4966898560523987, loss=2.110686779022217
I0209 15:27:46.350834 140050996819712 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.28067100048065186, loss=1.98671293258667
I0209 15:28:20.866672 140051005212416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3303246796131134, loss=1.9989733695983887
I0209 15:28:55.422302 140050996819712 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2807997763156891, loss=1.9693567752838135
I0209 15:29:29.966017 140051005212416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.3083059787750244, loss=1.9255704879760742
I0209 15:30:04.544395 140050996819712 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.35835838317871094, loss=1.8908727169036865
I0209 15:30:39.128378 140051005212416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2938586175441742, loss=1.9768896102905273
I0209 15:31:13.695430 140050996819712 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.34504130482673645, loss=2.0086493492126465
I0209 15:31:48.245894 140051005212416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.29617059230804443, loss=1.8970234394073486
I0209 15:32:22.800071 140050996819712 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.28040701150894165, loss=2.0230441093444824
I0209 15:32:57.371158 140051005212416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.25940486788749695, loss=1.9625380039215088
I0209 15:33:31.938910 140050996819712 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2993465065956116, loss=1.8932092189788818
I0209 15:34:06.500362 140051005212416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.3835591673851013, loss=1.9334306716918945
I0209 15:34:41.058056 140050996819712 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.29377058148384094, loss=1.975908637046814
I0209 15:35:15.635323 140051005212416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3843551576137543, loss=1.9758864641189575
I0209 15:35:50.178015 140050996819712 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.28464412689208984, loss=1.95792555809021
I0209 15:36:24.732091 140051005212416 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.3555562496185303, loss=1.9831193685531616
I0209 15:36:59.250373 140050996819712 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.263760507106781, loss=2.0316858291625977
I0209 15:37:33.802851 140051005212416 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2681615352630615, loss=1.892938256263733
I0209 15:38:08.336885 140050996819712 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.43533042073249817, loss=1.9273382425308228
I0209 15:38:42.915638 140051005212416 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.32032138109207153, loss=1.9809004068374634
I0209 15:39:17.505023 140050996819712 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2929743528366089, loss=2.009753942489624
I0209 15:39:35.886018 140225696298816 spec.py:321] Evaluating on the training split.
I0209 15:39:38.859321 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:42:35.255313 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 15:42:37.922340 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:45:21.132615 140225696298816 spec.py:349] Evaluating on the test split.
I0209 15:45:23.793781 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 15:48:02.733041 140225696298816 submission_runner.py:408] Time since start: 28389.81s, 	Step: 48655, 	{'train/accuracy': 0.6083666682243347, 'train/loss': 1.9990887641906738, 'train/bleu': 29.043505470052917, 'validation/accuracy': 0.6298372149467468, 'validation/loss': 1.8104116916656494, 'validation/bleu': 25.873173393998723, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7637287378311157, 'test/bleu': 24.83728357032393, 'test/num_examples': 3003, 'score': 16828.72014260292, 'total_duration': 28389.814615249634, 'accumulated_submission_time': 16828.72014260292, 'accumulated_eval_time': 11559.00778746605, 'accumulated_logging_time': 0.5670428276062012}
I0209 15:48:02.755214 140051005212416 logging_writer.py:48] [48655] accumulated_eval_time=11559.007787, accumulated_logging_time=0.567043, accumulated_submission_time=16828.720143, global_step=48655, preemption_count=0, score=16828.720143, test/accuracy=0.638045, test/bleu=24.837284, test/loss=1.763729, test/num_examples=3003, total_duration=28389.814615, train/accuracy=0.608367, train/bleu=29.043505, train/loss=1.999089, validation/accuracy=0.629837, validation/bleu=25.873173, validation/loss=1.810412, validation/num_examples=3000
I0209 15:48:18.594251 140050996819712 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2798394560813904, loss=1.9655722379684448
I0209 15:48:52.987537 140051005212416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3827667236328125, loss=1.9167799949645996
I0209 15:49:27.540395 140050996819712 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.26762065291404724, loss=1.9363410472869873
I0209 15:50:02.102953 140051005212416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.33095505833625793, loss=1.8913791179656982
I0209 15:50:36.635607 140050996819712 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.289493203163147, loss=1.9998481273651123
I0209 15:51:11.225358 140051005212416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.26358798146247864, loss=1.8486664295196533
I0209 15:51:45.780419 140050996819712 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3142397403717041, loss=1.96902596950531
I0209 15:52:20.332219 140051005212416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3172931969165802, loss=1.9415804147720337
I0209 15:52:54.906593 140050996819712 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3328148126602173, loss=1.9933568239212036
I0209 15:53:29.440690 140051005212416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.35294073820114136, loss=1.9474056959152222
I0209 15:54:04.002078 140050996819712 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.36849790811538696, loss=1.983397364616394
I0209 15:54:38.587937 140051005212416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.3559148907661438, loss=1.9691166877746582
I0209 15:55:13.166448 140050996819712 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3044418394565582, loss=2.035574436187744
I0209 15:55:47.746324 140051005212416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2906542122364044, loss=1.9311237335205078
I0209 15:56:22.318866 140050996819712 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.26629185676574707, loss=1.9661155939102173
I0209 15:56:56.893085 140051005212416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3251892924308777, loss=1.8983949422836304
I0209 15:57:31.439621 140050996819712 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2977786958217621, loss=1.9028576612472534
I0209 15:58:05.991058 140051005212416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.28326624631881714, loss=1.9818729162216187
I0209 15:58:40.536443 140050996819712 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3027915954589844, loss=1.915331244468689
I0209 15:59:15.093085 140051005212416 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.2801622152328491, loss=1.8614914417266846
I0209 15:59:49.651443 140050996819712 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3790399134159088, loss=1.9984986782073975
I0209 16:00:24.202085 140051005212416 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3467779755592346, loss=1.9266148805618286
I0209 16:00:58.727366 140050996819712 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.3290485143661499, loss=2.0250706672668457
I0209 16:01:33.291636 140051005212416 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.30044248700141907, loss=1.9109246730804443
I0209 16:02:03.067012 140225696298816 spec.py:321] Evaluating on the training split.
I0209 16:02:06.036892 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:05:08.972240 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 16:05:11.650173 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:08:03.302901 140225696298816 spec.py:349] Evaluating on the test split.
I0209 16:08:05.977893 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:10:32.739365 140225696298816 submission_runner.py:408] Time since start: 29739.82s, 	Step: 51088, 	{'train/accuracy': 0.618570864200592, 'train/loss': 1.924091100692749, 'train/bleu': 29.38637507894043, 'validation/accuracy': 0.6350200176239014, 'validation/loss': 1.7849268913269043, 'validation/bleu': 26.589383299087874, 'validation/num_examples': 3000, 'test/accuracy': 0.6439370512962341, 'test/loss': 1.7313753366470337, 'test/bleu': 25.48859377960028, 'test/num_examples': 3003, 'score': 17668.943604707718, 'total_duration': 29739.820941209793, 'accumulated_submission_time': 17668.943604707718, 'accumulated_eval_time': 12068.680082798004, 'accumulated_logging_time': 0.6013104915618896}
I0209 16:10:32.761615 140050996819712 logging_writer.py:48] [51088] accumulated_eval_time=12068.680083, accumulated_logging_time=0.601310, accumulated_submission_time=17668.943605, global_step=51088, preemption_count=0, score=17668.943605, test/accuracy=0.643937, test/bleu=25.488594, test/loss=1.731375, test/num_examples=3003, total_duration=29739.820941, train/accuracy=0.618571, train/bleu=29.386375, train/loss=1.924091, validation/accuracy=0.635020, validation/bleu=26.589383, validation/loss=1.784927, validation/num_examples=3000
I0209 16:10:37.259524 140051005212416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3245446979999542, loss=1.970334529876709
I0209 16:11:11.643609 140050996819712 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3254905045032501, loss=2.0295748710632324
I0209 16:11:46.166543 140051005212416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2623135447502136, loss=1.9447399377822876
I0209 16:12:20.706325 140050996819712 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.25994619727134705, loss=1.9278013706207275
I0209 16:12:55.227203 140051005212416 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3604818284511566, loss=1.9171634912490845
I0209 16:13:29.750135 140050996819712 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.30542710423469543, loss=1.9420685768127441
I0209 16:14:04.313168 140051005212416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3326272666454315, loss=1.9213192462921143
I0209 16:14:38.845024 140050996819712 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3484353721141815, loss=2.048452377319336
I0209 16:15:13.424594 140051005212416 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3439938426017761, loss=1.8901702165603638
I0209 16:15:47.984905 140050996819712 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3730680048465729, loss=1.85042405128479
I0209 16:16:22.536448 140051005212416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3447340428829193, loss=1.9662234783172607
I0209 16:16:57.082244 140050996819712 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3069241940975189, loss=1.971487283706665
I0209 16:17:31.598378 140051005212416 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.34946408867836, loss=1.9126131534576416
I0209 16:18:06.138611 140050996819712 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.39992663264274597, loss=1.9626048803329468
I0209 16:18:40.698238 140051005212416 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.26728323101997375, loss=1.9238996505737305
I0209 16:19:15.227495 140050996819712 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.44498857855796814, loss=1.9030568599700928
I0209 16:19:49.795505 140051005212416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.44057708978652954, loss=1.9392900466918945
I0209 16:20:24.376322 140050996819712 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.2687194049358368, loss=1.9926351308822632
I0209 16:20:58.949341 140051005212416 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.4424271583557129, loss=2.0007781982421875
I0209 16:21:33.500193 140050996819712 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.38547661900520325, loss=1.9604160785675049
I0209 16:22:08.061324 140051005212416 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2928667664527893, loss=1.9381799697875977
I0209 16:22:42.629221 140050996819712 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2525700330734253, loss=1.911670207977295
I0209 16:23:17.166171 140051005212416 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.34778010845184326, loss=1.9189577102661133
I0209 16:23:51.717462 140050996819712 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.33518803119659424, loss=1.9184989929199219
I0209 16:24:26.283615 140051005212416 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.4521445035934448, loss=1.9666860103607178
I0209 16:24:32.908981 140225696298816 spec.py:321] Evaluating on the training split.
I0209 16:24:35.875835 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:27:44.067908 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 16:27:46.742335 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:30:31.540081 140225696298816 spec.py:349] Evaluating on the test split.
I0209 16:30:34.232547 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:33:19.312301 140225696298816 submission_runner.py:408] Time since start: 31106.39s, 	Step: 53521, 	{'train/accuracy': 0.6205177903175354, 'train/loss': 1.9186264276504517, 'train/bleu': 29.460057986444184, 'validation/accuracy': 0.6394093036651611, 'validation/loss': 1.774772047996521, 'validation/bleu': 26.36329864950495, 'validation/num_examples': 3000, 'test/accuracy': 0.6456336379051208, 'test/loss': 1.7239888906478882, 'test/bleu': 25.94893984098375, 'test/num_examples': 3003, 'score': 18509.00431752205, 'total_duration': 31106.393884658813, 'accumulated_submission_time': 18509.00431752205, 'accumulated_eval_time': 12595.083347082138, 'accumulated_logging_time': 0.6347942352294922}
I0209 16:33:19.334925 140050996819712 logging_writer.py:48] [53521] accumulated_eval_time=12595.083347, accumulated_logging_time=0.634794, accumulated_submission_time=18509.004318, global_step=53521, preemption_count=0, score=18509.004318, test/accuracy=0.645634, test/bleu=25.948940, test/loss=1.723989, test/num_examples=3003, total_duration=31106.393885, train/accuracy=0.620518, train/bleu=29.460058, train/loss=1.918626, validation/accuracy=0.639409, validation/bleu=26.363299, validation/loss=1.774772, validation/num_examples=3000
I0209 16:33:46.860308 140051005212416 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.31081733107566833, loss=1.9860520362854004
I0209 16:34:21.305305 140050996819712 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3620010316371918, loss=2.0268630981445312
I0209 16:34:55.829399 140051005212416 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9659013748168945, loss=1.9314939975738525
I0209 16:35:30.355850 140050996819712 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.29341283440589905, loss=1.8839396238327026
I0209 16:36:04.869507 140051005212416 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2630348801612854, loss=1.9388151168823242
I0209 16:36:39.422796 140050996819712 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2777782082557678, loss=1.8645758628845215
I0209 16:37:13.993903 140051005212416 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.339799702167511, loss=1.936269760131836
I0209 16:37:48.545998 140050996819712 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.28920745849609375, loss=1.904013752937317
I0209 16:38:23.090163 140051005212416 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.2821066379547119, loss=1.9021753072738647
I0209 16:38:57.649198 140050996819712 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.39757758378982544, loss=1.8960926532745361
I0209 16:39:32.194513 140051005212416 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.29681092500686646, loss=1.9777289628982544
I0209 16:40:06.750379 140050996819712 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.27183109521865845, loss=1.9231613874435425
I0209 16:40:41.328020 140051005212416 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.4600408971309662, loss=1.9411665201187134
I0209 16:41:15.874412 140050996819712 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.3542345464229584, loss=1.985661268234253
I0209 16:41:50.420883 140051005212416 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3200748860836029, loss=1.9185540676116943
I0209 16:42:24.983170 140050996819712 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.34088245034217834, loss=2.0190038681030273
I0209 16:42:59.506432 140051005212416 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2862528860569, loss=1.940726637840271
I0209 16:43:34.048792 140050996819712 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3917929530143738, loss=1.938036322593689
I0209 16:44:08.556146 140051005212416 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.3569198548793793, loss=1.9071063995361328
I0209 16:44:43.075969 140050996819712 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2867681682109833, loss=1.905437707901001
I0209 16:45:17.643726 140051005212416 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.32279154658317566, loss=1.9374645948410034
I0209 16:45:52.193289 140050996819712 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3915518820285797, loss=1.9108421802520752
I0209 16:46:26.758061 140051005212416 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2687746286392212, loss=1.9864122867584229
I0209 16:47:01.310228 140050996819712 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.3261832296848297, loss=1.933083415031433
I0209 16:47:19.360336 140225696298816 spec.py:321] Evaluating on the training split.
I0209 16:47:22.336141 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:50:03.583819 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 16:50:06.263311 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:53:01.473309 140225696298816 spec.py:349] Evaluating on the test split.
I0209 16:53:04.154693 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 16:55:51.563342 140225696298816 submission_runner.py:408] Time since start: 32458.64s, 	Step: 55954, 	{'train/accuracy': 0.616606593132019, 'train/loss': 1.9429373741149902, 'train/bleu': 29.716919854833712, 'validation/accuracy': 0.6380329728126526, 'validation/loss': 1.7670636177062988, 'validation/bleu': 26.879836011913316, 'validation/num_examples': 3000, 'test/accuracy': 0.650665283203125, 'test/loss': 1.6983579397201538, 'test/bleu': 26.0390079997861, 'test/num_examples': 3003, 'score': 19348.941828727722, 'total_duration': 32458.6449136734, 'accumulated_submission_time': 19348.941828727722, 'accumulated_eval_time': 13107.286288261414, 'accumulated_logging_time': 0.6692543029785156}
I0209 16:55:51.585625 140051005212416 logging_writer.py:48] [55954] accumulated_eval_time=13107.286288, accumulated_logging_time=0.669254, accumulated_submission_time=19348.941829, global_step=55954, preemption_count=0, score=19348.941829, test/accuracy=0.650665, test/bleu=26.039008, test/loss=1.698358, test/num_examples=3003, total_duration=32458.644914, train/accuracy=0.616607, train/bleu=29.716920, train/loss=1.942937, validation/accuracy=0.638033, validation/bleu=26.879836, validation/loss=1.767064, validation/num_examples=3000
I0209 16:56:07.758095 140050996819712 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.30507931113243103, loss=1.9610093832015991
I0209 16:56:42.152838 140051005212416 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3458819091320038, loss=1.801209568977356
I0209 16:57:16.678998 140050996819712 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.4961378872394562, loss=1.9115073680877686
I0209 16:57:51.238937 140051005212416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.3454315662384033, loss=1.9976972341537476
I0209 16:58:25.791125 140050996819712 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3407565951347351, loss=1.9135247468948364
I0209 16:59:00.345271 140051005212416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3463718593120575, loss=1.9418816566467285
I0209 16:59:34.915071 140050996819712 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2675726115703583, loss=1.8385381698608398
I0209 17:00:09.475956 140051005212416 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.41812893748283386, loss=1.9783718585968018
I0209 17:00:44.008636 140050996819712 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.28918641805648804, loss=1.9152828454971313
I0209 17:01:18.546428 140051005212416 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.29262080788612366, loss=1.9266175031661987
I0209 17:01:53.096137 140050996819712 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.30007943511009216, loss=1.8957431316375732
I0209 17:02:27.653014 140051005212416 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.32061460614204407, loss=2.014104127883911
I0209 17:03:02.240375 140050996819712 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.27589356899261475, loss=1.8516592979431152
I0209 17:03:36.811879 140051005212416 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3134060204029083, loss=1.9369906187057495
I0209 17:04:11.358515 140050996819712 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.36677083373069763, loss=1.9166048765182495
I0209 17:04:45.930914 140051005212416 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.27486592531204224, loss=1.9725154638290405
I0209 17:05:20.464607 140050996819712 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2949572205543518, loss=1.9196958541870117
I0209 17:05:55.026949 140051005212416 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.27831584215164185, loss=1.8762695789337158
I0209 17:06:29.597740 140050996819712 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2944401800632477, loss=1.8709135055541992
I0209 17:07:04.146947 140051005212416 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2966557443141937, loss=1.910619854927063
I0209 17:07:38.687733 140050996819712 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.34495478868484497, loss=1.8596267700195312
I0209 17:08:13.254976 140051005212416 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2840646207332611, loss=1.9251229763031006
I0209 17:08:47.789796 140050996819712 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3258720338344574, loss=1.878224492073059
I0209 17:09:22.335574 140051005212416 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3891398012638092, loss=1.893808126449585
I0209 17:09:51.772272 140225696298816 spec.py:321] Evaluating on the training split.
I0209 17:09:54.738090 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:13:52.836342 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 17:13:55.516905 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:17:00.135942 140225696298816 spec.py:349] Evaluating on the test split.
I0209 17:17:02.818079 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:20:18.601185 140225696298816 submission_runner.py:408] Time since start: 33925.68s, 	Step: 58387, 	{'train/accuracy': 0.6255083680152893, 'train/loss': 1.8838893175125122, 'train/bleu': 29.25000924980348, 'validation/accuracy': 0.63966965675354, 'validation/loss': 1.7545181512832642, 'validation/bleu': 26.65371403234823, 'validation/num_examples': 3000, 'test/accuracy': 0.6511068940162659, 'test/loss': 1.6911041736602783, 'test/bleu': 25.839829990207257, 'test/num_examples': 3003, 'score': 20189.042788743973, 'total_duration': 33925.682745695114, 'accumulated_submission_time': 20189.042788743973, 'accumulated_eval_time': 13734.115124940872, 'accumulated_logging_time': 0.7012717723846436}
I0209 17:20:18.624267 140050996819712 logging_writer.py:48] [58387] accumulated_eval_time=13734.115125, accumulated_logging_time=0.701272, accumulated_submission_time=20189.042789, global_step=58387, preemption_count=0, score=20189.042789, test/accuracy=0.651107, test/bleu=25.839830, test/loss=1.691104, test/num_examples=3003, total_duration=33925.682746, train/accuracy=0.625508, train/bleu=29.250009, train/loss=1.883889, validation/accuracy=0.639670, validation/bleu=26.653714, validation/loss=1.754518, validation/num_examples=3000
I0209 17:20:23.461953 140051005212416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.4703333079814911, loss=1.897101879119873
I0209 17:20:57.875016 140050996819712 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.35144519805908203, loss=1.9558846950531006
I0209 17:21:32.370426 140051005212416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.3363972306251526, loss=1.9284178018569946
I0209 17:22:06.909931 140050996819712 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3618023693561554, loss=1.9576356410980225
I0209 17:22:41.452154 140051005212416 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.35636231303215027, loss=1.9576178789138794
I0209 17:23:16.018157 140050996819712 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.4354206323623657, loss=1.8942890167236328
I0209 17:23:50.564120 140051005212416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3351691663265228, loss=1.849608063697815
I0209 17:24:25.118727 140050996819712 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.3308775722980499, loss=1.946624994277954
I0209 17:24:59.689316 140051005212416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3185409605503082, loss=1.917832374572754
I0209 17:25:34.233232 140050996819712 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3376559615135193, loss=2.0301191806793213
I0209 17:26:08.785658 140051005212416 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2596139907836914, loss=1.91048002243042
I0209 17:26:43.325272 140050996819712 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3358360826969147, loss=1.9135494232177734
I0209 17:27:17.877908 140051005212416 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.27450811862945557, loss=1.8360265493392944
I0209 17:27:52.436040 140050996819712 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.32959306240081787, loss=1.9421534538269043
I0209 17:28:26.993520 140051005212416 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.34994831681251526, loss=1.8794081211090088
I0209 17:29:01.545833 140050996819712 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3247838020324707, loss=1.8548160791397095
I0209 17:29:36.123312 140051005212416 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.31900015473365784, loss=1.8878931999206543
I0209 17:30:10.662329 140050996819712 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3075072467327118, loss=1.931444525718689
I0209 17:30:45.214312 140051005212416 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.26099497079849243, loss=1.8856209516525269
I0209 17:31:19.773916 140050996819712 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.245818629860878, loss=1.9436852931976318
I0209 17:31:54.316969 140051005212416 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.3396710455417633, loss=1.9602468013763428
I0209 17:32:28.888771 140050996819712 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2969689667224884, loss=1.917580008506775
I0209 17:33:03.472660 140051005212416 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.26825404167175293, loss=1.9218395948410034
I0209 17:33:38.024319 140050996819712 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.37042829394340515, loss=1.8513213396072388
I0209 17:34:12.570765 140051005212416 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2969297468662262, loss=1.992217779159546
I0209 17:34:18.858360 140225696298816 spec.py:321] Evaluating on the training split.
I0209 17:34:21.830918 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:37:07.367362 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 17:37:10.040444 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:39:42.099125 140225696298816 spec.py:349] Evaluating on the test split.
I0209 17:39:44.772225 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:42:01.727354 140225696298816 submission_runner.py:408] Time since start: 35228.81s, 	Step: 60820, 	{'train/accuracy': 0.6229991912841797, 'train/loss': 1.9016964435577393, 'train/bleu': 29.731746344592317, 'validation/accuracy': 0.6420503258705139, 'validation/loss': 1.7479721307754517, 'validation/bleu': 26.703019452981483, 'validation/num_examples': 3000, 'test/accuracy': 0.6530939936637878, 'test/loss': 1.672269582748413, 'test/bleu': 25.896172575343115, 'test/num_examples': 3003, 'score': 21029.19118499756, 'total_duration': 35228.80893397331, 'accumulated_submission_time': 21029.19118499756, 'accumulated_eval_time': 14196.984060525894, 'accumulated_logging_time': 0.7340254783630371}
I0209 17:42:01.750762 140050996819712 logging_writer.py:48] [60820] accumulated_eval_time=14196.984061, accumulated_logging_time=0.734025, accumulated_submission_time=21029.191185, global_step=60820, preemption_count=0, score=21029.191185, test/accuracy=0.653094, test/bleu=25.896173, test/loss=1.672270, test/num_examples=3003, total_duration=35228.808934, train/accuracy=0.622999, train/bleu=29.731746, train/loss=1.901696, validation/accuracy=0.642050, validation/bleu=26.703019, validation/loss=1.747972, validation/num_examples=3000
I0209 17:42:29.633647 140051005212416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2665346562862396, loss=1.937666654586792
I0209 17:43:04.142164 140050996819712 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.42447319626808167, loss=1.9324731826782227
I0209 17:43:38.678041 140051005212416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.27918341755867004, loss=1.901706337928772
I0209 17:44:13.215972 140050996819712 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.27888479828834534, loss=1.8926856517791748
I0209 17:44:47.746303 140051005212416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.257925808429718, loss=1.9275838136672974
I0209 17:45:22.308552 140050996819712 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.26734283566474915, loss=1.836883544921875
I0209 17:45:56.860597 140051005212416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.37292274832725525, loss=1.8421508073806763
I0209 17:46:31.391479 140050996819712 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.34489479660987854, loss=1.9101771116256714
I0209 17:47:05.956710 140051005212416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.37970593571662903, loss=1.8774875402450562
I0209 17:47:40.523479 140050996819712 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.40070974826812744, loss=1.9509316682815552
I0209 17:48:15.065097 140051005212416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.26751449704170227, loss=1.9481348991394043
I0209 17:48:49.614736 140050996819712 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.2750507891178131, loss=1.9376636743545532
I0209 17:49:24.171625 140051005212416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.32696545124053955, loss=1.8533778190612793
I0209 17:49:58.729351 140050996819712 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.34022024273872375, loss=1.9437741041183472
I0209 17:50:33.283200 140051005212416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.2862432897090912, loss=1.9290392398834229
I0209 17:51:07.863861 140050996819712 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.3415287435054779, loss=1.8740127086639404
I0209 17:51:42.417650 140051005212416 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2465561032295227, loss=1.8698291778564453
I0209 17:52:16.979937 140050996819712 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2978365123271942, loss=1.847573161125183
I0209 17:52:51.539932 140051005212416 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3827471137046814, loss=1.9059104919433594
I0209 17:53:26.078162 140050996819712 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.28011178970336914, loss=1.98325777053833
I0209 17:54:00.625336 140051005212416 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.3240077495574951, loss=1.9294941425323486
I0209 17:54:35.169587 140050996819712 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.33629539608955383, loss=1.9331923723220825
I0209 17:55:09.711947 140051005212416 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3279857933521271, loss=1.8781880140304565
I0209 17:55:44.266710 140050996819712 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.3306921124458313, loss=1.8477935791015625
I0209 17:56:01.970686 140225696298816 spec.py:321] Evaluating on the training split.
I0209 17:56:04.934650 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 17:58:49.233273 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 17:58:51.899194 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:01:33.583228 140225696298816 spec.py:349] Evaluating on the test split.
I0209 18:01:36.261368 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:04:08.397357 140225696298816 submission_runner.py:408] Time since start: 36555.48s, 	Step: 63253, 	{'train/accuracy': 0.6290479302406311, 'train/loss': 1.827910304069519, 'train/bleu': 30.297907849320147, 'validation/accuracy': 0.6419262886047363, 'validation/loss': 1.7374597787857056, 'validation/bleu': 26.966711675602447, 'validation/num_examples': 3000, 'test/accuracy': 0.6543838381767273, 'test/loss': 1.6732841730117798, 'test/bleu': 26.206675737115404, 'test/num_examples': 3003, 'score': 21869.32551908493, 'total_duration': 36555.478934049606, 'accumulated_submission_time': 21869.32551908493, 'accumulated_eval_time': 14683.410673379898, 'accumulated_logging_time': 0.7670059204101562}
I0209 18:04:08.420843 140051005212416 logging_writer.py:48] [63253] accumulated_eval_time=14683.410673, accumulated_logging_time=0.767006, accumulated_submission_time=21869.325519, global_step=63253, preemption_count=0, score=21869.325519, test/accuracy=0.654384, test/bleu=26.206676, test/loss=1.673284, test/num_examples=3003, total_duration=36555.478934, train/accuracy=0.629048, train/bleu=30.297908, train/loss=1.827910, validation/accuracy=0.641926, validation/bleu=26.966712, validation/loss=1.737460, validation/num_examples=3000
I0209 18:04:24.940948 140050996819712 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.27623072266578674, loss=1.8432947397232056
I0209 18:04:59.431173 140051005212416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.35089045763015747, loss=1.9358175992965698
I0209 18:05:33.982276 140050996819712 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.30913129448890686, loss=1.935341477394104
I0209 18:06:08.530506 140051005212416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2913045883178711, loss=1.9291672706604004
I0209 18:06:43.085510 140050996819712 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2974741458892822, loss=1.8410149812698364
I0209 18:07:17.624582 140051005212416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3691798448562622, loss=1.8977900743484497
I0209 18:07:52.170717 140050996819712 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.30753815174102783, loss=1.889870524406433
I0209 18:08:26.716512 140051005212416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2637537717819214, loss=1.9295870065689087
I0209 18:09:01.276404 140050996819712 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2931802272796631, loss=1.8768839836120605
I0209 18:09:35.835256 140051005212416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2901504635810852, loss=1.9327846765518188
I0209 18:10:10.412604 140050996819712 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.32602593302726746, loss=1.9182692766189575
I0209 18:10:44.975015 140051005212416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.31010279059410095, loss=1.9330135583877563
I0209 18:11:19.532634 140050996819712 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.26498889923095703, loss=1.944779634475708
I0209 18:11:54.085505 140051005212416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2688985764980316, loss=1.9217052459716797
I0209 18:12:28.624189 140050996819712 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3664194345474243, loss=1.9467462301254272
I0209 18:13:03.177270 140051005212416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.31802913546562195, loss=1.8908579349517822
I0209 18:13:37.732068 140050996819712 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3118700087070465, loss=1.8676538467407227
I0209 18:14:12.278105 140051005212416 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3090415298938751, loss=1.8769707679748535
I0209 18:14:46.937608 140050996819712 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.26497185230255127, loss=1.8572055101394653
I0209 18:15:21.482049 140051005212416 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.30070430040359497, loss=1.9076489210128784
I0209 18:15:56.036639 140050996819712 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.3033769726753235, loss=1.90626060962677
I0209 18:16:30.576545 140051005212416 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.343580961227417, loss=1.9349788427352905
I0209 18:17:05.134886 140050996819712 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.29139411449432373, loss=1.9057941436767578
I0209 18:17:39.738401 140051005212416 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.29441872239112854, loss=1.9052907228469849
I0209 18:18:08.492039 140225696298816 spec.py:321] Evaluating on the training split.
I0209 18:18:11.467638 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:21:57.818166 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 18:22:00.492474 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:24:58.902367 140225696298816 spec.py:349] Evaluating on the test split.
I0209 18:25:01.579276 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:27:41.575040 140225696298816 submission_runner.py:408] Time since start: 37968.66s, 	Step: 65685, 	{'train/accuracy': 0.6229196786880493, 'train/loss': 1.90165114402771, 'train/bleu': 29.822310457333295, 'validation/accuracy': 0.6441333293914795, 'validation/loss': 1.7247092723846436, 'validation/bleu': 27.3569961752832, 'validation/num_examples': 3000, 'test/accuracy': 0.6523967385292053, 'test/loss': 1.6633965969085693, 'test/bleu': 26.01282070100662, 'test/num_examples': 3003, 'score': 22709.308198928833, 'total_duration': 37968.656623363495, 'accumulated_submission_time': 22709.308198928833, 'accumulated_eval_time': 15256.493627786636, 'accumulated_logging_time': 0.8021972179412842}
I0209 18:27:41.602897 140050996819712 logging_writer.py:48] [65685] accumulated_eval_time=15256.493628, accumulated_logging_time=0.802197, accumulated_submission_time=22709.308199, global_step=65685, preemption_count=0, score=22709.308199, test/accuracy=0.652397, test/bleu=26.012821, test/loss=1.663397, test/num_examples=3003, total_duration=37968.656623, train/accuracy=0.622920, train/bleu=29.822310, train/loss=1.901651, validation/accuracy=0.644133, validation/bleu=27.356996, validation/loss=1.724709, validation/num_examples=3000
I0209 18:27:47.144751 140051005212416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.26408594846725464, loss=1.834057092666626
I0209 18:28:21.578498 140050996819712 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.32037296891212463, loss=1.9263066053390503
I0209 18:28:56.109745 140051005212416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2756361961364746, loss=1.877386450767517
I0209 18:29:30.674433 140050996819712 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3592289090156555, loss=1.902544379234314
I0209 18:30:05.243471 140051005212416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3298039734363556, loss=1.8322268724441528
I0209 18:30:39.792437 140050996819712 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3211257755756378, loss=1.8429102897644043
I0209 18:31:14.333637 140051005212416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.30182337760925293, loss=1.8057198524475098
I0209 18:31:48.881586 140050996819712 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2982005178928375, loss=1.7817927598953247
I0209 18:32:23.445667 140051005212416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.32295888662338257, loss=1.8683953285217285
I0209 18:32:57.988156 140050996819712 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.37006935477256775, loss=1.9779963493347168
I0209 18:33:32.546507 140051005212416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.31246083974838257, loss=1.8536443710327148
I0209 18:34:07.091926 140050996819712 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.291450172662735, loss=1.8774745464324951
I0209 18:34:41.653523 140051005212416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.26971888542175293, loss=1.8332486152648926
I0209 18:35:16.208284 140050996819712 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3106914460659027, loss=1.9327572584152222
I0209 18:35:50.759445 140051005212416 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3116038739681244, loss=1.9061284065246582
I0209 18:36:25.298829 140050996819712 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2894916236400604, loss=1.8506231307983398
I0209 18:36:59.821536 140051005212416 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.285624623298645, loss=1.795954942703247
I0209 18:37:34.381838 140050996819712 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.2937317192554474, loss=1.9225034713745117
I0209 18:38:08.920435 140051005212416 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.35431861877441406, loss=1.7912988662719727
I0209 18:38:43.468375 140050996819712 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2790830731391907, loss=1.7534313201904297
I0209 18:39:18.043324 140051005212416 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.2590545415878296, loss=1.8664132356643677
I0209 18:39:52.596742 140050996819712 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2501392960548401, loss=1.8781843185424805
I0209 18:40:27.124904 140051005212416 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2884249985218048, loss=1.9321025609970093
I0209 18:41:01.677741 140050996819712 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.306772917509079, loss=1.824207067489624
I0209 18:41:36.210349 140051005212416 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3389413356781006, loss=1.7935243844985962
I0209 18:41:41.800262 140225696298816 spec.py:321] Evaluating on the training split.
I0209 18:41:44.775388 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:44:58.913769 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 18:45:01.594890 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:47:45.947711 140225696298816 spec.py:349] Evaluating on the test split.
I0209 18:47:48.631129 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 18:50:20.425488 140225696298816 submission_runner.py:408] Time since start: 39327.51s, 	Step: 68118, 	{'train/accuracy': 0.6257489323616028, 'train/loss': 1.8845466375350952, 'train/bleu': 29.903225654359392, 'validation/accuracy': 0.6469975709915161, 'validation/loss': 1.7029153108596802, 'validation/bleu': 27.369239895807823, 'validation/num_examples': 3000, 'test/accuracy': 0.6561850309371948, 'test/loss': 1.6488022804260254, 'test/bleu': 26.68360440541797, 'test/num_examples': 3003, 'score': 23549.419113636017, 'total_duration': 39327.507074832916, 'accumulated_submission_time': 23549.419113636017, 'accumulated_eval_time': 15775.118801116943, 'accumulated_logging_time': 0.8397126197814941}
I0209 18:50:20.451514 140050996819712 logging_writer.py:48] [68118] accumulated_eval_time=15775.118801, accumulated_logging_time=0.839713, accumulated_submission_time=23549.419114, global_step=68118, preemption_count=0, score=23549.419114, test/accuracy=0.656185, test/bleu=26.683604, test/loss=1.648802, test/num_examples=3003, total_duration=39327.507075, train/accuracy=0.625749, train/bleu=29.903226, train/loss=1.884547, validation/accuracy=0.646998, validation/bleu=27.369240, validation/loss=1.702915, validation/num_examples=3000
I0209 18:50:48.994707 140051005212416 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.273343026638031, loss=1.9423896074295044
I0209 18:51:23.468831 140050996819712 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.32241493463516235, loss=1.9417763948440552
I0209 18:51:57.977758 140051005212416 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.28975942730903625, loss=1.9032455682754517
I0209 18:52:32.516224 140050996819712 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.279800683259964, loss=1.873869776725769
I0209 18:53:07.028627 140051005212416 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.2562514841556549, loss=1.9063611030578613
I0209 18:53:41.550474 140050996819712 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.29177236557006836, loss=1.8608180284500122
I0209 18:54:16.078267 140051005212416 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.33799734711647034, loss=1.9554543495178223
I0209 18:54:50.627961 140050996819712 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.49597612023353577, loss=1.8514189720153809
I0209 18:55:25.156624 140051005212416 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2805359363555908, loss=1.8716988563537598
I0209 18:55:59.700442 140050996819712 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3296509087085724, loss=1.9123497009277344
I0209 18:56:34.241504 140051005212416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.28430697321891785, loss=1.891068458557129
I0209 18:57:08.784429 140050996819712 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.27105388045310974, loss=1.7876867055892944
I0209 18:57:43.316032 140051005212416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2594221234321594, loss=1.8106346130371094
I0209 18:58:17.861943 140050996819712 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.28732383251190186, loss=1.8139936923980713
I0209 18:58:52.363700 140051005212416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.26436787843704224, loss=1.8604339361190796
I0209 18:59:26.886987 140050996819712 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3022626042366028, loss=1.8907947540283203
I0209 19:00:01.420572 140051005212416 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3478822112083435, loss=1.7754783630371094
I0209 19:00:35.954571 140050996819712 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3405458927154541, loss=1.8384435176849365
I0209 19:01:10.489690 140051005212416 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.27122896909713745, loss=1.7952216863632202
I0209 19:01:45.042249 140050996819712 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2951493263244629, loss=1.8495827913284302
I0209 19:02:19.591145 140051005212416 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.31158241629600525, loss=1.7874385118484497
I0209 19:02:54.127346 140050996819712 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.27193042635917664, loss=1.7845253944396973
I0209 19:03:28.699104 140051005212416 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.287503719329834, loss=1.8519856929779053
I0209 19:04:03.270952 140050996819712 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2941130995750427, loss=1.8637895584106445
I0209 19:04:20.611249 140225696298816 spec.py:321] Evaluating on the training split.
I0209 19:04:23.592926 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:07:23.382477 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 19:07:26.061411 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:09:53.470170 140225696298816 spec.py:349] Evaluating on the test split.
I0209 19:09:56.156835 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:12:21.636249 140225696298816 submission_runner.py:408] Time since start: 40648.72s, 	Step: 70552, 	{'train/accuracy': 0.6302703022956848, 'train/loss': 1.8330473899841309, 'train/bleu': 30.171382342838893, 'validation/accuracy': 0.649502158164978, 'validation/loss': 1.6973282098770142, 'validation/bleu': 27.077008915014122, 'validation/num_examples': 3000, 'test/accuracy': 0.6587531566619873, 'test/loss': 1.639639139175415, 'test/bleu': 26.84871294586024, 'test/num_examples': 3003, 'score': 24389.49199271202, 'total_duration': 40648.717832803726, 'accumulated_submission_time': 24389.49199271202, 'accumulated_eval_time': 16256.143748998642, 'accumulated_logging_time': 0.8760302066802979}
I0209 19:12:21.662131 140051005212416 logging_writer.py:48] [70552] accumulated_eval_time=16256.143749, accumulated_logging_time=0.876030, accumulated_submission_time=24389.491993, global_step=70552, preemption_count=0, score=24389.491993, test/accuracy=0.658753, test/bleu=26.848713, test/loss=1.639639, test/num_examples=3003, total_duration=40648.717833, train/accuracy=0.630270, train/bleu=30.171382, train/loss=1.833047, validation/accuracy=0.649502, validation/bleu=27.077009, validation/loss=1.697328, validation/num_examples=3000
I0209 19:12:38.528913 140050996819712 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.27314430475234985, loss=1.9076772928237915
I0209 19:13:12.928918 140051005212416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3224524259567261, loss=1.7924922704696655
I0209 19:13:47.457340 140050996819712 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3171348571777344, loss=1.9158189296722412
I0209 19:14:21.983005 140051005212416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.30756956338882446, loss=1.8479862213134766
I0209 19:14:56.530774 140050996819712 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.32297074794769287, loss=1.9317762851715088
I0209 19:15:31.110677 140051005212416 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2788839042186737, loss=1.8772082328796387
I0209 19:16:05.663052 140050996819712 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.2680649161338806, loss=1.8496819734573364
I0209 19:16:40.210996 140051005212416 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2953624427318573, loss=1.888868808746338
I0209 19:17:14.797332 140050996819712 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2876621186733246, loss=1.8630011081695557
I0209 19:17:49.366490 140051005212416 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2624921202659607, loss=1.9033902883529663
I0209 19:18:23.915834 140050996819712 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.29908400774002075, loss=1.8196278810501099
I0209 19:18:58.478390 140051005212416 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.27243533730506897, loss=1.8910408020019531
I0209 19:19:33.059603 140050996819712 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.271058589220047, loss=1.8815903663635254
I0209 19:20:07.608441 140051005212416 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.29398113489151, loss=1.818031668663025
I0209 19:20:42.166120 140050996819712 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.27193307876586914, loss=1.8555186986923218
I0209 19:21:16.703826 140051005212416 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.29779988527297974, loss=1.7972474098205566
I0209 19:21:51.234994 140050996819712 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3054279088973999, loss=1.8900834321975708
I0209 19:22:25.795021 140051005212416 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3033961057662964, loss=1.8280285596847534
I0209 19:23:00.362129 140050996819712 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3003758490085602, loss=1.8111371994018555
I0209 19:23:34.928245 140051005212416 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.26834550499916077, loss=1.7977262735366821
I0209 19:24:09.481621 140050996819712 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3028009831905365, loss=1.7902016639709473
I0209 19:24:44.041313 140051005212416 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2982516288757324, loss=1.7400015592575073
I0209 19:25:18.557072 140050996819712 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2865537106990814, loss=1.855281949043274
I0209 19:25:53.109163 140051005212416 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.37021350860595703, loss=1.7451493740081787
I0209 19:26:21.842853 140225696298816 spec.py:321] Evaluating on the training split.
I0209 19:26:24.812046 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:29:28.358385 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 19:29:31.036288 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:32:18.885179 140225696298816 spec.py:349] Evaluating on the test split.
I0209 19:32:21.581174 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:34:50.425298 140225696298816 submission_runner.py:408] Time since start: 41997.51s, 	Step: 72985, 	{'train/accuracy': 0.6285916566848755, 'train/loss': 1.8592113256454468, 'train/bleu': 30.52954279286608, 'validation/accuracy': 0.6510024666786194, 'validation/loss': 1.6859381198883057, 'validation/bleu': 27.65884298530821, 'validation/num_examples': 3000, 'test/accuracy': 0.6605426669120789, 'test/loss': 1.6174166202545166, 'test/bleu': 26.853673620428214, 'test/num_examples': 3003, 'score': 25229.58637213707, 'total_duration': 41997.50688147545, 'accumulated_submission_time': 25229.58637213707, 'accumulated_eval_time': 16764.72614622116, 'accumulated_logging_time': 0.911757230758667}
I0209 19:34:50.450046 140050996819712 logging_writer.py:48] [72985] accumulated_eval_time=16764.726146, accumulated_logging_time=0.911757, accumulated_submission_time=25229.586372, global_step=72985, preemption_count=0, score=25229.586372, test/accuracy=0.660543, test/bleu=26.853674, test/loss=1.617417, test/num_examples=3003, total_duration=41997.506881, train/accuracy=0.628592, train/bleu=30.529543, train/loss=1.859211, validation/accuracy=0.651002, validation/bleu=27.658843, validation/loss=1.685938, validation/num_examples=3000
I0209 19:34:55.965991 140051005212416 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3016960620880127, loss=1.8795816898345947
I0209 19:35:30.376577 140050996819712 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2877083718776703, loss=1.7656469345092773
I0209 19:36:04.880467 140051005212416 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3160625398159027, loss=1.816437840461731
I0209 19:36:39.444751 140050996819712 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2998914420604706, loss=1.8086402416229248
I0209 19:37:13.996331 140051005212416 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3054148554801941, loss=1.8450154066085815
I0209 19:37:48.538557 140050996819712 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2758781909942627, loss=1.779331088066101
I0209 19:38:23.081022 140051005212416 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.38382023572921753, loss=1.82392156124115
I0209 19:38:57.637054 140050996819712 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.28633278608322144, loss=1.8439630270004272
I0209 19:39:32.204776 140051005212416 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.28522989153862, loss=1.8503763675689697
I0209 19:40:06.759050 140050996819712 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.2927260100841522, loss=1.7918131351470947
I0209 19:40:41.300400 140051005212416 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3103959262371063, loss=1.8348088264465332
I0209 19:41:15.845336 140050996819712 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3043777048587799, loss=1.7923146486282349
I0209 19:41:50.413216 140051005212416 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3525503873825073, loss=1.8776735067367554
I0209 19:42:24.975146 140050996819712 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.28009167313575745, loss=1.7864960432052612
I0209 19:42:59.546502 140051005212416 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.27169618010520935, loss=1.8358138799667358
I0209 19:43:34.094778 140050996819712 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.31976985931396484, loss=1.8901219367980957
I0209 19:44:08.666799 140051005212416 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.3010038733482361, loss=1.806965708732605
I0209 19:44:43.232986 140050996819712 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2765997052192688, loss=1.7960879802703857
I0209 19:45:17.789461 140051005212416 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.24967296421527863, loss=1.7947782278060913
I0209 19:45:52.319969 140050996819712 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2803483009338379, loss=1.8133772611618042
I0209 19:46:26.870682 140051005212416 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3076251149177551, loss=1.8252837657928467
I0209 19:47:01.424833 140050996819712 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.28677693009376526, loss=1.8306238651275635
I0209 19:47:35.989015 140051005212416 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3286757171154022, loss=1.8777984380722046
I0209 19:48:10.524672 140050996819712 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.2516074776649475, loss=1.7149646282196045
I0209 19:48:45.049343 140051005212416 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.26806846261024475, loss=1.7663739919662476
I0209 19:48:50.656634 140225696298816 spec.py:321] Evaluating on the training split.
I0209 19:48:53.629234 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:52:42.814890 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 19:52:45.484672 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:56:00.834240 140225696298816 spec.py:349] Evaluating on the test split.
I0209 19:56:03.523648 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 19:58:42.940613 140225696298816 submission_runner.py:408] Time since start: 43430.02s, 	Step: 75418, 	{'train/accuracy': 0.6453965306282043, 'train/loss': 1.716774582862854, 'train/bleu': 31.558854951636462, 'validation/accuracy': 0.6527011394500732, 'validation/loss': 1.681450605392456, 'validation/bleu': 27.563748681111136, 'validation/num_examples': 3000, 'test/accuracy': 0.6633432507514954, 'test/loss': 1.6029932498931885, 'test/bleu': 27.128032352674563, 'test/num_examples': 3003, 'score': 26069.70525288582, 'total_duration': 43430.02218937874, 'accumulated_submission_time': 26069.70525288582, 'accumulated_eval_time': 17357.010063409805, 'accumulated_logging_time': 0.9460341930389404}
I0209 19:58:42.966852 140050996819712 logging_writer.py:48] [75418] accumulated_eval_time=17357.010063, accumulated_logging_time=0.946034, accumulated_submission_time=26069.705253, global_step=75418, preemption_count=0, score=26069.705253, test/accuracy=0.663343, test/bleu=27.128032, test/loss=1.602993, test/num_examples=3003, total_duration=43430.022189, train/accuracy=0.645397, train/bleu=31.558855, train/loss=1.716775, validation/accuracy=0.652701, validation/bleu=27.563749, validation/loss=1.681451, validation/num_examples=3000
I0209 19:59:11.539561 140051005212416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.2603982388973236, loss=1.73844575881958
I0209 19:59:45.974365 140050996819712 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.29114967584609985, loss=1.795141577720642
I0209 20:00:20.468405 140051005212416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.31906768679618835, loss=1.8439325094223022
I0209 20:00:54.994149 140050996819712 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2842405140399933, loss=1.901600956916809
I0209 20:01:29.527163 140051005212416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.32201817631721497, loss=1.8820561170578003
I0209 20:02:04.031636 140050996819712 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2560519278049469, loss=1.8365682363510132
I0209 20:02:38.579645 140051005212416 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2987416982650757, loss=1.8090872764587402
I0209 20:03:13.104946 140050996819712 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2994847297668457, loss=1.8223826885223389
I0209 20:03:47.639374 140051005212416 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2891412079334259, loss=1.8168094158172607
I0209 20:04:22.152951 140050996819712 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2875032424926758, loss=1.8365886211395264
I0209 20:04:56.701541 140051005212416 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.29565364122390747, loss=1.829466462135315
I0209 20:05:31.231986 140050996819712 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3174874186515808, loss=1.7285157442092896
I0209 20:06:05.775917 140051005212416 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.3230477273464203, loss=1.9176095724105835
I0209 20:06:40.308718 140050996819712 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.28466328978538513, loss=1.9367798566818237
I0209 20:07:14.824358 140051005212416 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.31128332018852234, loss=1.7494285106658936
I0209 20:07:49.375281 140050996819712 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.26207610964775085, loss=1.7345495223999023
I0209 20:08:23.916416 140051005212416 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.32889339327812195, loss=1.8406885862350464
I0209 20:08:58.467548 140050996819712 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.27281054854393005, loss=1.7824472188949585
I0209 20:09:32.994838 140051005212416 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.6021895408630371, loss=1.8929951190948486
I0209 20:10:07.518300 140050996819712 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.40631866455078125, loss=1.798537254333496
I0209 20:10:42.054636 140051005212416 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2862512767314911, loss=1.803664207458496
I0209 20:11:16.611525 140050996819712 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.28659698367118835, loss=1.8062459230422974
I0209 20:11:51.161138 140051005212416 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.28111520409584045, loss=1.7254632711410522
I0209 20:12:25.720096 140050996819712 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.33507800102233887, loss=1.8039242029190063
I0209 20:12:43.069370 140225696298816 spec.py:321] Evaluating on the training split.
I0209 20:12:46.037597 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:16:44.359144 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 20:16:47.032174 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:19:13.201423 140225696298816 spec.py:349] Evaluating on the test split.
I0209 20:19:15.872759 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:21:38.827077 140225696298816 submission_runner.py:408] Time since start: 44805.91s, 	Step: 77852, 	{'train/accuracy': 0.6361210942268372, 'train/loss': 1.8001832962036133, 'train/bleu': 30.33095928486013, 'validation/accuracy': 0.6541270613670349, 'validation/loss': 1.6627322435379028, 'validation/bleu': 27.47509347000726, 'validation/num_examples': 3000, 'test/accuracy': 0.667956531047821, 'test/loss': 1.5868098735809326, 'test/bleu': 27.65037334871951, 'test/num_examples': 3003, 'score': 26909.717646598816, 'total_duration': 44805.90866136551, 'accumulated_submission_time': 26909.717646598816, 'accumulated_eval_time': 17892.767714738846, 'accumulated_logging_time': 0.9847004413604736}
I0209 20:21:38.853116 140051005212416 logging_writer.py:48] [77852] accumulated_eval_time=17892.767715, accumulated_logging_time=0.984700, accumulated_submission_time=26909.717647, global_step=77852, preemption_count=0, score=26909.717647, test/accuracy=0.667957, test/bleu=27.650373, test/loss=1.586810, test/num_examples=3003, total_duration=44805.908661, train/accuracy=0.636121, train/bleu=30.330959, train/loss=1.800183, validation/accuracy=0.654127, validation/bleu=27.475093, validation/loss=1.662732, validation/num_examples=3000
I0209 20:21:55.725459 140050996819712 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.3109322190284729, loss=1.8408149480819702
I0209 20:22:30.193874 140051005212416 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.31829598546028137, loss=1.8944281339645386
I0209 20:23:04.719540 140050996819712 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.30546504259109497, loss=1.8619389533996582
I0209 20:23:39.243231 140051005212416 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.31609514355659485, loss=1.9177031517028809
I0209 20:24:13.782047 140050996819712 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3474552631378174, loss=1.7947450876235962
I0209 20:24:48.334203 140051005212416 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3297048509120941, loss=1.6847878694534302
I0209 20:25:22.914653 140050996819712 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2988298535346985, loss=1.7626454830169678
I0209 20:25:57.469178 140051005212416 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2876502573490143, loss=1.8249636888504028
I0209 20:26:31.991153 140050996819712 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.29338428378105164, loss=1.869195818901062
I0209 20:27:06.546297 140051005212416 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3028203845024109, loss=1.7940751314163208
I0209 20:27:41.091599 140050996819712 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.30175575613975525, loss=1.7503681182861328
I0209 20:28:15.634413 140051005212416 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2937071919441223, loss=1.7822327613830566
I0209 20:28:50.196722 140050996819712 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.29706647992134094, loss=1.8342814445495605
I0209 20:29:24.745523 140051005212416 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.301887184381485, loss=1.7652696371078491
I0209 20:29:59.279527 140050996819712 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.3432157039642334, loss=1.7910469770431519
I0209 20:30:33.829514 140051005212416 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3030191957950592, loss=1.8788176774978638
I0209 20:31:08.362431 140050996819712 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.31144338846206665, loss=1.798606514930725
I0209 20:31:42.906838 140051005212416 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3168480396270752, loss=1.8017600774765015
I0209 20:32:17.486034 140050996819712 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.27488940954208374, loss=1.8518080711364746
I0209 20:32:52.057508 140051005212416 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3230421543121338, loss=1.7858015298843384
I0209 20:33:26.590236 140050996819712 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.3255844712257385, loss=1.6949723958969116
I0209 20:34:01.133992 140051005212416 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3699377775192261, loss=1.8402265310287476
I0209 20:34:35.663330 140050996819712 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.4013599753379822, loss=1.8409292697906494
I0209 20:35:10.221298 140051005212416 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3216465711593628, loss=1.7088243961334229
I0209 20:35:38.964946 140225696298816 spec.py:321] Evaluating on the training split.
I0209 20:35:41.928754 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:38:32.586112 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 20:38:35.254678 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:41:13.746411 140225696298816 spec.py:349] Evaluating on the test split.
I0209 20:41:16.434603 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 20:44:02.395714 140225696298816 submission_runner.py:408] Time since start: 46149.48s, 	Step: 80285, 	{'train/accuracy': 0.6344415545463562, 'train/loss': 1.814192771911621, 'train/bleu': 30.737479092771107, 'validation/accuracy': 0.6556149125099182, 'validation/loss': 1.6477035284042358, 'validation/bleu': 27.724983358387746, 'validation/num_examples': 3000, 'test/accuracy': 0.6648771166801453, 'test/loss': 1.5862025022506714, 'test/bleu': 27.20680830071649, 'test/num_examples': 3003, 'score': 27749.741545915604, 'total_duration': 46149.47729349136, 'accumulated_submission_time': 27749.741545915604, 'accumulated_eval_time': 18396.198426246643, 'accumulated_logging_time': 1.0218725204467773}
I0209 20:44:02.423667 140050996819712 logging_writer.py:48] [80285] accumulated_eval_time=18396.198426, accumulated_logging_time=1.021873, accumulated_submission_time=27749.741546, global_step=80285, preemption_count=0, score=27749.741546, test/accuracy=0.664877, test/bleu=27.206808, test/loss=1.586203, test/num_examples=3003, total_duration=46149.477293, train/accuracy=0.634442, train/bleu=30.737479, train/loss=1.814193, validation/accuracy=0.655615, validation/bleu=27.724983, validation/loss=1.647704, validation/num_examples=3000
I0209 20:44:07.939282 140051005212416 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.28575265407562256, loss=1.8231160640716553
I0209 20:44:42.324918 140050996819712 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.32285162806510925, loss=1.7775442600250244
I0209 20:45:16.824800 140051005212416 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.26266005635261536, loss=1.7882378101348877
I0209 20:45:51.338858 140050996819712 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.28520694375038147, loss=1.7557505369186401
I0209 20:46:25.885772 140051005212416 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2777542769908905, loss=1.8084746599197388
I0209 20:47:00.419983 140050996819712 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.30746030807495117, loss=1.8835082054138184
I0209 20:47:34.952706 140051005212416 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.2641395926475525, loss=1.791243553161621
I0209 20:48:09.501749 140050996819712 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.30255424976348877, loss=1.7810287475585938
I0209 20:48:44.055384 140051005212416 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3346059322357178, loss=1.84589421749115
I0209 20:49:18.590611 140050996819712 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.28381213545799255, loss=1.7545483112335205
I0209 20:49:53.136554 140051005212416 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.27787432074546814, loss=1.7768138647079468
I0209 20:50:27.670131 140050996819712 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.31198635697364807, loss=1.8053479194641113
I0209 20:51:02.215701 140051005212416 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3396008014678955, loss=1.8320393562316895
I0209 20:51:36.762128 140050996819712 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3194180130958557, loss=1.8554635047912598
I0209 20:52:11.316933 140051005212416 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.30710017681121826, loss=1.7799019813537598
I0209 20:52:45.877898 140050996819712 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.35830503702163696, loss=1.8176891803741455
I0209 20:53:20.440962 140051005212416 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.31810757517814636, loss=1.837001085281372
I0209 20:53:54.946892 140050996819712 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.32238394021987915, loss=1.843843698501587
I0209 20:54:29.497964 140051005212416 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3239355683326721, loss=1.8182878494262695
I0209 20:55:04.043563 140050996819712 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3147451877593994, loss=1.8983029127120972
I0209 20:55:38.576397 140051005212416 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3504768908023834, loss=1.8353779315948486
I0209 20:56:13.117743 140050996819712 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2932974696159363, loss=1.7916786670684814
I0209 20:56:47.662541 140051005212416 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.32322588562965393, loss=1.7414653301239014
I0209 20:57:22.201739 140050996819712 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.31746023893356323, loss=1.8441860675811768
I0209 20:57:56.750143 140051005212416 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2777310609817505, loss=1.8299927711486816
I0209 20:58:02.695517 140225696298816 spec.py:321] Evaluating on the training split.
I0209 20:58:05.667101 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:02:03.118310 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 21:02:05.787682 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:04:55.584084 140225696298816 spec.py:349] Evaluating on the test split.
I0209 21:04:58.246651 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:07:31.376087 140225696298816 submission_runner.py:408] Time since start: 47558.46s, 	Step: 82719, 	{'train/accuracy': 0.6453226804733276, 'train/loss': 1.7232457399368286, 'train/bleu': 31.61777031006529, 'validation/accuracy': 0.6575492024421692, 'validation/loss': 1.6341495513916016, 'validation/bleu': 28.112447135184922, 'validation/num_examples': 3000, 'test/accuracy': 0.6703736186027527, 'test/loss': 1.5644491910934448, 'test/bleu': 27.516618205330463, 'test/num_examples': 3003, 'score': 28589.926423549652, 'total_duration': 47558.457666397095, 'accumulated_submission_time': 28589.926423549652, 'accumulated_eval_time': 18964.878935098648, 'accumulated_logging_time': 1.0599958896636963}
I0209 21:07:31.402793 140050996819712 logging_writer.py:48] [82719] accumulated_eval_time=18964.878935, accumulated_logging_time=1.059996, accumulated_submission_time=28589.926424, global_step=82719, preemption_count=0, score=28589.926424, test/accuracy=0.670374, test/bleu=27.516618, test/loss=1.564449, test/num_examples=3003, total_duration=47558.457666, train/accuracy=0.645323, train/bleu=31.617770, train/loss=1.723246, validation/accuracy=0.657549, validation/bleu=28.112447, validation/loss=1.634150, validation/num_examples=3000
I0209 21:07:59.612420 140051005212416 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.31048285961151123, loss=1.8318071365356445
I0209 21:08:34.066561 140050996819712 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2873818576335907, loss=1.7695763111114502
I0209 21:09:08.600964 140051005212416 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3167191445827484, loss=1.7138763666152954
I0209 21:09:43.123247 140050996819712 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.32007622718811035, loss=1.844369649887085
I0209 21:10:17.649296 140051005212416 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.30129435658454895, loss=1.831827998161316
I0209 21:10:52.166098 140050996819712 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.27076372504234314, loss=1.7780845165252686
I0209 21:11:26.706100 140051005212416 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3076920807361603, loss=1.8374741077423096
I0209 21:12:01.233653 140050996819712 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.28650468587875366, loss=1.816812515258789
I0209 21:12:35.782633 140051005212416 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.280442476272583, loss=1.7636981010437012
I0209 21:13:10.333388 140050996819712 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.27405858039855957, loss=1.7510349750518799
I0209 21:13:44.888656 140051005212416 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.29314374923706055, loss=1.7200548648834229
I0209 21:14:19.430855 140050996819712 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3474693298339844, loss=1.8137749433517456
I0209 21:14:53.976156 140051005212416 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.33159762620925903, loss=1.7658225297927856
I0209 21:15:28.517350 140050996819712 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.26072216033935547, loss=1.7912713289260864
I0209 21:16:03.033808 140051005212416 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3010181188583374, loss=1.8176437616348267
I0209 21:16:37.570907 140050996819712 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3263484537601471, loss=1.8060964345932007
I0209 21:17:12.094770 140051005212416 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3154391050338745, loss=1.8557509183883667
I0209 21:17:46.673423 140050996819712 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.2959388494491577, loss=1.7886899709701538
I0209 21:18:21.231063 140051005212416 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.30305176973342896, loss=1.7707642316818237
I0209 21:18:55.792853 140050996819712 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3074174225330353, loss=1.7182238101959229
I0209 21:19:30.338965 140051005212416 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2796456813812256, loss=1.7508466243743896
I0209 21:20:04.892062 140050996819712 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2779397666454315, loss=1.731490135192871
I0209 21:20:39.439369 140051005212416 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.28619953989982605, loss=1.766789436340332
I0209 21:21:13.994838 140050996819712 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.28527364134788513, loss=1.8527660369873047
I0209 21:21:31.692601 140225696298816 spec.py:321] Evaluating on the training split.
I0209 21:21:34.669248 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:25:01.232420 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 21:25:03.922368 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:27:49.263135 140225696298816 spec.py:349] Evaluating on the test split.
I0209 21:27:51.948862 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:30:16.752539 140225696298816 submission_runner.py:408] Time since start: 48923.83s, 	Step: 85153, 	{'train/accuracy': 0.6420574188232422, 'train/loss': 1.7634731531143188, 'train/bleu': 31.85978635812725, 'validation/accuracy': 0.6593594551086426, 'validation/loss': 1.616284966468811, 'validation/bleu': 28.13691576680429, 'validation/num_examples': 3000, 'test/accuracy': 0.672267735004425, 'test/loss': 1.5401736497879028, 'test/bleu': 27.951467340137658, 'test/num_examples': 3003, 'score': 29430.129603147507, 'total_duration': 48923.83412575722, 'accumulated_submission_time': 29430.129603147507, 'accumulated_eval_time': 19489.938822746277, 'accumulated_logging_time': 1.096651315689087}
I0209 21:30:16.780798 140051005212416 logging_writer.py:48] [85153] accumulated_eval_time=19489.938823, accumulated_logging_time=1.096651, accumulated_submission_time=29430.129603, global_step=85153, preemption_count=0, score=29430.129603, test/accuracy=0.672268, test/bleu=27.951467, test/loss=1.540174, test/num_examples=3003, total_duration=48923.834126, train/accuracy=0.642057, train/bleu=31.859786, train/loss=1.763473, validation/accuracy=0.659359, validation/bleu=28.136916, validation/loss=1.616285, validation/num_examples=3000
I0209 21:30:33.317142 140050996819712 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2903377115726471, loss=1.7043426036834717
I0209 21:31:07.743805 140051005212416 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.3256050944328308, loss=1.7686829566955566
I0209 21:31:42.286102 140050996819712 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.36271005868911743, loss=1.7097800970077515
I0209 21:32:16.828327 140051005212416 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.2907339036464691, loss=1.745016098022461
I0209 21:32:51.411315 140050996819712 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.301248699426651, loss=1.719481110572815
I0209 21:33:25.955575 140051005212416 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.26977258920669556, loss=1.7981685400009155
I0209 21:34:00.522617 140050996819712 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3604622781276703, loss=1.871161937713623
I0209 21:34:35.098823 140051005212416 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2956959903240204, loss=1.7694553136825562
I0209 21:35:09.669899 140050996819712 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3336174488067627, loss=1.7893420457839966
I0209 21:35:44.222803 140051005212416 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.328041672706604, loss=1.709567666053772
I0209 21:36:18.747971 140050996819712 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.28638002276420593, loss=1.7857059240341187
I0209 21:36:53.283207 140051005212416 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.3019564747810364, loss=1.7644354104995728
I0209 21:37:27.841464 140050996819712 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.29961898922920227, loss=1.8137778043746948
I0209 21:38:02.410135 140051005212416 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3133852183818817, loss=1.7921921014785767
I0209 21:38:36.944593 140050996819712 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.35521256923675537, loss=1.8305710554122925
I0209 21:39:11.485615 140051005212416 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3097648620605469, loss=1.7689253091812134
I0209 21:39:45.997668 140050996819712 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.30437740683555603, loss=1.7873413562774658
I0209 21:40:20.547764 140051005212416 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.27716735005378723, loss=1.6771020889282227
I0209 21:40:55.064085 140050996819712 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.37433499097824097, loss=1.854936122894287
I0209 21:41:29.616623 140051005212416 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.30924057960510254, loss=1.753316879272461
I0209 21:42:04.166163 140050996819712 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.28732872009277344, loss=1.7803820371627808
I0209 21:42:38.721363 140051005212416 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.28071364760398865, loss=1.7697936296463013
I0209 21:43:13.318623 140050996819712 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.2848314642906189, loss=1.7121301889419556
I0209 21:43:47.877168 140051005212416 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3142530620098114, loss=1.7619915008544922
I0209 21:44:16.960696 140225696298816 spec.py:321] Evaluating on the training split.
I0209 21:44:19.931993 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:47:42.640788 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 21:47:45.328222 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:50:36.812562 140225696298816 spec.py:349] Evaluating on the test split.
I0209 21:50:39.503669 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 21:53:28.736949 140225696298816 submission_runner.py:408] Time since start: 50315.82s, 	Step: 87586, 	{'train/accuracy': 0.706721842288971, 'train/loss': 1.3691809177398682, 'train/bleu': 36.764196846397844, 'validation/accuracy': 0.664182722568512, 'validation/loss': 1.5927585363388062, 'validation/bleu': 28.740379778097818, 'validation/num_examples': 3000, 'test/accuracy': 0.6754401326179504, 'test/loss': 1.5277425050735474, 'test/bleu': 27.999811967158358, 'test/num_examples': 3003, 'score': 30270.221952676773, 'total_duration': 50315.81853723526, 'accumulated_submission_time': 30270.221952676773, 'accumulated_eval_time': 20041.715032100677, 'accumulated_logging_time': 1.1359143257141113}
I0209 21:53:28.764107 140050996819712 logging_writer.py:48] [87586] accumulated_eval_time=20041.715032, accumulated_logging_time=1.135914, accumulated_submission_time=30270.221953, global_step=87586, preemption_count=0, score=30270.221953, test/accuracy=0.675440, test/bleu=27.999812, test/loss=1.527743, test/num_examples=3003, total_duration=50315.818537, train/accuracy=0.706722, train/bleu=36.764197, train/loss=1.369181, validation/accuracy=0.664183, validation/bleu=28.740380, validation/loss=1.592759, validation/num_examples=3000
I0209 21:53:33.940511 140051005212416 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.30362677574157715, loss=1.7665008306503296
I0209 21:54:08.343539 140050996819712 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.293561190366745, loss=1.7213624715805054
I0209 21:54:42.808589 140051005212416 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3007100820541382, loss=1.7197538614273071
I0209 21:55:17.330942 140050996819712 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.29117071628570557, loss=1.7118154764175415
I0209 21:55:51.894731 140051005212416 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3252234160900116, loss=1.7546824216842651
I0209 21:56:26.419649 140050996819712 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3043011426925659, loss=1.717071294784546
I0209 21:57:00.990524 140051005212416 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.30615416169166565, loss=1.8040064573287964
I0209 21:57:35.516347 140050996819712 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2847023606300354, loss=1.7561914920806885
I0209 21:58:10.086594 140051005212416 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3119857907295227, loss=1.777108907699585
I0209 21:58:44.631726 140050996819712 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3073652386665344, loss=1.7516494989395142
I0209 21:59:19.168646 140051005212416 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.2697674036026001, loss=1.7134267091751099
I0209 21:59:53.700530 140050996819712 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.32542645931243896, loss=1.750333547592163
I0209 22:00:28.254443 140051005212416 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3223668038845062, loss=1.8726378679275513
I0209 22:01:02.790889 140050996819712 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.32051101326942444, loss=1.709148645401001
I0209 22:01:37.319639 140051005212416 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.30910831689834595, loss=1.7033964395523071
I0209 22:02:11.890231 140050996819712 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.35617414116859436, loss=1.7979429960250854
I0209 22:02:46.453907 140051005212416 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.35098665952682495, loss=1.8228514194488525
I0209 22:03:20.999282 140050996819712 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3110426664352417, loss=1.7825887203216553
I0209 22:03:55.581480 140051005212416 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.3163547217845917, loss=1.7080906629562378
I0209 22:04:30.118824 140050996819712 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2790960967540741, loss=1.6964969635009766
I0209 22:05:04.667138 140051005212416 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.2895171344280243, loss=1.6939436197280884
I0209 22:05:39.211376 140050996819712 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.29158303141593933, loss=1.721803903579712
I0209 22:06:13.764177 140051005212416 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.29765692353248596, loss=1.7359806299209595
I0209 22:06:48.341205 140050996819712 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.30048808455467224, loss=1.7759824991226196
I0209 22:07:22.876355 140051005212416 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.31158560514450073, loss=1.7672762870788574
I0209 22:07:28.816044 140225696298816 spec.py:321] Evaluating on the training split.
I0209 22:07:31.790065 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:11:23.806278 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 22:11:26.492221 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:14:11.974436 140225696298816 spec.py:349] Evaluating on the test split.
I0209 22:14:14.654108 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:16:58.050452 140225696298816 submission_runner.py:408] Time since start: 51725.13s, 	Step: 90019, 	{'train/accuracy': 0.6472726464271545, 'train/loss': 1.7228424549102783, 'train/bleu': 31.716836297151044, 'validation/accuracy': 0.6628807783126831, 'validation/loss': 1.5885562896728516, 'validation/bleu': 28.023800742966134, 'validation/num_examples': 3000, 'test/accuracy': 0.6758701205253601, 'test/loss': 1.5183173418045044, 'test/bleu': 27.95895404149604, 'test/num_examples': 3003, 'score': 31110.187237501144, 'total_duration': 51725.13202857971, 'accumulated_submission_time': 31110.187237501144, 'accumulated_eval_time': 20610.949380874634, 'accumulated_logging_time': 1.1731517314910889}
I0209 22:16:58.078138 140050996819712 logging_writer.py:48] [90019] accumulated_eval_time=20610.949381, accumulated_logging_time=1.173152, accumulated_submission_time=31110.187238, global_step=90019, preemption_count=0, score=31110.187238, test/accuracy=0.675870, test/bleu=27.958954, test/loss=1.518317, test/num_examples=3003, total_duration=51725.132029, train/accuracy=0.647273, train/bleu=31.716836, train/loss=1.722842, validation/accuracy=0.662881, validation/bleu=28.023801, validation/loss=1.588556, validation/num_examples=3000
I0209 22:17:26.308792 140051005212416 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2941451966762543, loss=1.750280737876892
I0209 22:18:00.770149 140050996819712 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2852945923805237, loss=1.63619863986969
I0209 22:18:35.320025 140051005212416 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.31443989276885986, loss=1.7674343585968018
I0209 22:19:09.873132 140050996819712 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3004726767539978, loss=1.7336496114730835
I0209 22:19:44.426394 140051005212416 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2759913206100464, loss=1.6050845384597778
I0209 22:20:18.961764 140050996819712 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.28549402952194214, loss=1.7593492269515991
I0209 22:20:53.496206 140051005212416 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.29984918236732483, loss=1.6851279735565186
I0209 22:21:28.024091 140050996819712 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.2641487419605255, loss=1.6965090036392212
I0209 22:22:02.559817 140051005212416 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.3322742283344269, loss=1.7878583669662476
I0209 22:22:37.154682 140050996819712 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3022019863128662, loss=1.7363111972808838
I0209 22:23:11.704097 140051005212416 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.2916397452354431, loss=1.7196468114852905
I0209 22:23:46.254328 140050996819712 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.33968889713287354, loss=1.8337526321411133
I0209 22:24:20.783356 140051005212416 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.28640204668045044, loss=1.6843596696853638
I0209 22:24:55.327469 140050996819712 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.31940776109695435, loss=1.7027502059936523
I0209 22:25:29.878492 140051005212416 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.31634843349456787, loss=1.7942731380462646
I0209 22:26:04.437404 140050996819712 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.2784648537635803, loss=1.74588143825531
I0209 22:26:38.960372 140051005212416 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.2945103049278259, loss=1.7493075132369995
I0209 22:27:13.494527 140050996819712 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3077813386917114, loss=1.8371456861495972
I0209 22:27:48.011403 140051005212416 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.2956770062446594, loss=1.6873610019683838
I0209 22:28:22.562453 140050996819712 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2735162377357483, loss=1.726865530014038
I0209 22:28:57.083363 140051005212416 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3065555691719055, loss=1.6528693437576294
I0209 22:29:31.642424 140050996819712 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.2919865548610687, loss=1.7284398078918457
I0209 22:30:06.199917 140051005212416 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3370348811149597, loss=1.6988608837127686
I0209 22:30:40.742104 140050996819712 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.30694320797920227, loss=1.7920559644699097
I0209 22:30:58.076957 140225696298816 spec.py:321] Evaluating on the training split.
I0209 22:31:01.040089 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:35:05.089087 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 22:35:07.753364 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:38:13.851648 140225696298816 spec.py:349] Evaluating on the test split.
I0209 22:38:16.520639 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:41:02.469619 140225696298816 submission_runner.py:408] Time since start: 53169.55s, 	Step: 92452, 	{'train/accuracy': 0.6476435661315918, 'train/loss': 1.7178657054901123, 'train/bleu': 31.692039256295246, 'validation/accuracy': 0.6662781834602356, 'validation/loss': 1.5724620819091797, 'validation/bleu': 28.568028748018282, 'validation/num_examples': 3000, 'test/accuracy': 0.6786009073257446, 'test/loss': 1.4941082000732422, 'test/bleu': 28.248444795106696, 'test/num_examples': 3003, 'score': 31950.097545862198, 'total_duration': 53169.55118060112, 'accumulated_submission_time': 31950.097545862198, 'accumulated_eval_time': 21215.34196662903, 'accumulated_logging_time': 1.2124512195587158}
I0209 22:41:02.497431 140051005212416 logging_writer.py:48] [92452] accumulated_eval_time=21215.341967, accumulated_logging_time=1.212451, accumulated_submission_time=31950.097546, global_step=92452, preemption_count=0, score=31950.097546, test/accuracy=0.678601, test/bleu=28.248445, test/loss=1.494108, test/num_examples=3003, total_duration=53169.551181, train/accuracy=0.647644, train/bleu=31.692039, train/loss=1.717866, validation/accuracy=0.666278, validation/bleu=28.568029, validation/loss=1.572462, validation/num_examples=3000
I0209 22:41:19.355670 140050996819712 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.29930636286735535, loss=1.7598214149475098
I0209 22:41:53.783333 140051005212416 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.30298489332199097, loss=1.7572264671325684
I0209 22:42:28.301821 140050996819712 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.31928348541259766, loss=1.7262170314788818
I0209 22:43:02.842546 140051005212416 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.2679010033607483, loss=1.6487207412719727
I0209 22:43:37.362283 140050996819712 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3131308853626251, loss=1.6793667078018188
I0209 22:44:11.906609 140051005212416 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3042682409286499, loss=1.7186952829360962
I0209 22:44:46.462967 140050996819712 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3047531545162201, loss=1.6810483932495117
I0209 22:45:20.993806 140051005212416 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3229861557483673, loss=1.6363856792449951
I0209 22:45:55.548382 140050996819712 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.27200767397880554, loss=1.6498677730560303
I0209 22:46:30.081483 140051005212416 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3005281686782837, loss=1.7006258964538574
I0209 22:47:04.625948 140050996819712 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.36327311396598816, loss=1.7726391553878784
I0209 22:47:39.162149 140051005212416 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.28942644596099854, loss=1.7068116664886475
I0209 22:48:13.690119 140050996819712 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.328337162733078, loss=1.714807152748108
I0209 22:48:48.216413 140051005212416 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2896101176738739, loss=1.702597975730896
I0209 22:49:22.746603 140050996819712 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.2873888611793518, loss=1.7668273448944092
I0209 22:49:57.300712 140051005212416 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.27834552526474, loss=1.6633007526397705
I0209 22:50:31.835600 140050996819712 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.30026111006736755, loss=1.7240583896636963
I0209 22:51:06.384267 140051005212416 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2868787944316864, loss=1.6911792755126953
I0209 22:51:40.927284 140050996819712 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.28570303320884705, loss=1.7607420682907104
I0209 22:52:15.446016 140051005212416 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.43832454085350037, loss=1.7142088413238525
I0209 22:52:49.999958 140050996819712 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.31445780396461487, loss=1.7467443943023682
I0209 22:53:24.564693 140051005212416 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.28129857778549194, loss=1.7074767351150513
I0209 22:53:59.115328 140050996819712 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.30157989263534546, loss=1.6937916278839111
I0209 22:54:33.663079 140051005212416 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.31266793608665466, loss=1.6436271667480469
I0209 22:55:02.753473 140225696298816 spec.py:321] Evaluating on the training split.
I0209 22:55:05.726909 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 22:57:57.187217 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 22:57:59.864981 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:00:35.587929 140225696298816 spec.py:349] Evaluating on the test split.
I0209 23:00:38.260017 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:03:00.046316 140225696298816 submission_runner.py:408] Time since start: 54487.13s, 	Step: 94886, 	{'train/accuracy': 0.6604101657867432, 'train/loss': 1.6313503980636597, 'train/bleu': 32.403113391945894, 'validation/accuracy': 0.6679768562316895, 'validation/loss': 1.559630036354065, 'validation/bleu': 28.760747178515107, 'validation/num_examples': 3000, 'test/accuracy': 0.6810412406921387, 'test/loss': 1.4820163249969482, 'test/bleu': 28.72655015719533, 'test/num_examples': 3003, 'score': 32790.26389718056, 'total_duration': 54487.12789297104, 'accumulated_submission_time': 32790.26389718056, 'accumulated_eval_time': 21692.634751558304, 'accumulated_logging_time': 1.252263069152832}
I0209 23:03:00.076124 140050996819712 logging_writer.py:48] [94886] accumulated_eval_time=21692.634752, accumulated_logging_time=1.252263, accumulated_submission_time=32790.263897, global_step=94886, preemption_count=0, score=32790.263897, test/accuracy=0.681041, test/bleu=28.726550, test/loss=1.482016, test/num_examples=3003, total_duration=54487.127893, train/accuracy=0.660410, train/bleu=32.403113, train/loss=1.631350, validation/accuracy=0.667977, validation/bleu=28.760747, validation/loss=1.559630, validation/num_examples=3000
I0209 23:03:05.266421 140051005212416 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3256654739379883, loss=1.6682263612747192
I0209 23:03:39.694599 140050996819712 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.33007144927978516, loss=1.70795476436615
I0209 23:04:14.200068 140051005212416 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2800789475440979, loss=1.6636422872543335
I0209 23:04:48.735213 140050996819712 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3366287052631378, loss=1.7476221323013306
I0209 23:05:23.271916 140051005212416 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3132953643798828, loss=1.714698314666748
I0209 23:05:57.800863 140050996819712 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.30935510993003845, loss=1.5949074029922485
I0209 23:06:32.350029 140051005212416 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.31067904829978943, loss=1.7615947723388672
I0209 23:07:06.882794 140050996819712 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.30222904682159424, loss=1.707074522972107
I0209 23:07:41.426790 140051005212416 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3129607141017914, loss=1.6525191068649292
I0209 23:08:15.960165 140050996819712 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3037901818752289, loss=1.6495003700256348
I0209 23:08:50.491513 140051005212416 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.2872599959373474, loss=1.6565308570861816
I0209 23:09:25.034015 140050996819712 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.30526337027549744, loss=1.7161412239074707
I0209 23:09:59.594847 140051005212416 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.35695838928222656, loss=1.7054121494293213
I0209 23:10:34.116928 140050996819712 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.2904295027256012, loss=1.6560646295547485
I0209 23:11:08.662961 140051005212416 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.30295461416244507, loss=1.765401005744934
I0209 23:11:43.204717 140050996819712 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3111637532711029, loss=1.6830261945724487
I0209 23:12:17.743930 140051005212416 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.32699260115623474, loss=1.725696325302124
I0209 23:12:52.306589 140050996819712 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3570481240749359, loss=1.7463111877441406
I0209 23:13:26.860930 140051005212416 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3121893107891083, loss=1.6425350904464722
I0209 23:14:01.418977 140050996819712 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.3282061815261841, loss=1.7744208574295044
I0209 23:14:35.957128 140051005212416 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.311262309551239, loss=1.683210015296936
I0209 23:15:10.495684 140050996819712 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3338181972503662, loss=1.7415488958358765
I0209 23:15:45.030447 140051005212416 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3234405815601349, loss=1.6122238636016846
I0209 23:16:19.599270 140050996819712 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.286913126707077, loss=1.6943799257278442
I0209 23:16:54.127428 140051005212416 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3165929913520813, loss=1.7530958652496338
I0209 23:17:00.073501 140225696298816 spec.py:321] Evaluating on the training split.
I0209 23:17:03.054864 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:20:06.539481 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 23:20:09.216476 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:22:41.523502 140225696298816 spec.py:349] Evaluating on the test split.
I0209 23:22:44.205571 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:25:03.501860 140225696298816 submission_runner.py:408] Time since start: 55810.58s, 	Step: 97319, 	{'train/accuracy': 0.6549776792526245, 'train/loss': 1.674398422241211, 'train/bleu': 32.00376825833275, 'validation/accuracy': 0.6725149154663086, 'validation/loss': 1.5401153564453125, 'validation/bleu': 28.802378796047854, 'validation/num_examples': 3000, 'test/accuracy': 0.6848992109298706, 'test/loss': 1.4625848531723022, 'test/bleu': 28.58278102882752, 'test/num_examples': 3003, 'score': 33630.172709703445, 'total_duration': 55810.58344745636, 'accumulated_submission_time': 33630.172709703445, 'accumulated_eval_time': 22176.063058376312, 'accumulated_logging_time': 1.2936749458312988}
I0209 23:25:03.530245 140050996819712 logging_writer.py:48] [97319] accumulated_eval_time=22176.063058, accumulated_logging_time=1.293675, accumulated_submission_time=33630.172710, global_step=97319, preemption_count=0, score=33630.172710, test/accuracy=0.684899, test/bleu=28.582781, test/loss=1.462585, test/num_examples=3003, total_duration=55810.583447, train/accuracy=0.654978, train/bleu=32.003768, train/loss=1.674398, validation/accuracy=0.672515, validation/bleu=28.802379, validation/loss=1.540115, validation/num_examples=3000
I0209 23:25:31.745579 140051005212416 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3188888430595398, loss=1.6556118726730347
I0209 23:26:06.219937 140050996819712 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3187374174594879, loss=1.7685657739639282
I0209 23:26:40.743983 140051005212416 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.2945650815963745, loss=1.6640937328338623
I0209 23:27:15.253863 140050996819712 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.30415913462638855, loss=1.6721915006637573
I0209 23:27:49.798318 140051005212416 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.30305594205856323, loss=1.6320339441299438
I0209 23:28:24.326912 140050996819712 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3066454529762268, loss=1.7078033685684204
I0209 23:28:58.917502 140051005212416 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.29293957352638245, loss=1.7274492979049683
I0209 23:29:33.436743 140050996819712 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.31133466958999634, loss=1.6461361646652222
I0209 23:30:07.981479 140051005212416 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.31020915508270264, loss=1.7244218587875366
I0209 23:30:42.500423 140050996819712 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.30403319001197815, loss=1.6850310564041138
I0209 23:31:17.058806 140051005212416 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3121545612812042, loss=1.6709578037261963
I0209 23:31:51.597813 140050996819712 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.30950531363487244, loss=1.6447432041168213
I0209 23:32:26.143478 140051005212416 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3242049217224121, loss=1.655516266822815
I0209 23:33:00.722227 140050996819712 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.2942620813846588, loss=1.5880500078201294
I0209 23:33:35.274410 140051005212416 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3159576952457428, loss=1.764459490776062
I0209 23:34:09.811659 140050996819712 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.29085466265678406, loss=1.636918544769287
I0209 23:34:44.346217 140051005212416 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3080058991909027, loss=1.6567108631134033
I0209 23:35:18.888124 140050996819712 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.30335310101509094, loss=1.723062515258789
I0209 23:35:53.413850 140051005212416 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3231671452522278, loss=1.606687307357788
I0209 23:36:27.965179 140050996819712 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3048395812511444, loss=1.6799241304397583
I0209 23:37:02.500614 140051005212416 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.2786266803741455, loss=1.6526529788970947
I0209 23:37:37.058495 140050996819712 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33028531074523926, loss=1.6818360090255737
I0209 23:38:11.620836 140051005212416 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2880679965019226, loss=1.622643232345581
I0209 23:38:46.154707 140050996819712 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.32839521765708923, loss=1.6562778949737549
I0209 23:39:03.502954 140225696298816 spec.py:321] Evaluating on the training split.
I0209 23:39:06.480300 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:42:09.073471 140225696298816 spec.py:333] Evaluating on the validation split.
I0209 23:42:11.745968 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:45:01.633449 140225696298816 spec.py:349] Evaluating on the test split.
I0209 23:45:04.320446 140225696298816 workload.py:181] Translating evaluation dataset.
I0209 23:47:43.747097 140225696298816 submission_runner.py:408] Time since start: 57170.83s, 	Step: 99752, 	{'train/accuracy': 0.6542562246322632, 'train/loss': 1.6796025037765503, 'train/bleu': 32.45218523737319, 'validation/accuracy': 0.6749699115753174, 'validation/loss': 1.5269558429718018, 'validation/bleu': 29.279505165766782, 'validation/num_examples': 3000, 'test/accuracy': 0.6886526346206665, 'test/loss': 1.4499832391738892, 'test/bleu': 28.744171233574672, 'test/num_examples': 3003, 'score': 34470.056736946106, 'total_duration': 57170.82868671417, 'accumulated_submission_time': 34470.056736946106, 'accumulated_eval_time': 22696.30715584755, 'accumulated_logging_time': 1.3333721160888672}
I0209 23:47:43.775703 140051005212416 logging_writer.py:48] [99752] accumulated_eval_time=22696.307156, accumulated_logging_time=1.333372, accumulated_submission_time=34470.056737, global_step=99752, preemption_count=0, score=34470.056737, test/accuracy=0.688653, test/bleu=28.744171, test/loss=1.449983, test/num_examples=3003, total_duration=57170.828687, train/accuracy=0.654256, train/bleu=32.452185, train/loss=1.679603, validation/accuracy=0.674970, validation/bleu=29.279505, validation/loss=1.526956, validation/num_examples=3000
I0209 23:48:00.654756 140050996819712 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.306645929813385, loss=1.6506030559539795
I0209 23:48:35.091784 140051005212416 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.30339759588241577, loss=1.7198940515518188
I0209 23:49:09.637861 140050996819712 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3262457847595215, loss=1.725623607635498
I0209 23:49:44.166323 140051005212416 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2970086634159088, loss=1.6602451801300049
I0209 23:50:18.718554 140050996819712 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.30396515130996704, loss=1.6525719165802002
I0209 23:50:53.253827 140051005212416 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.31123894453048706, loss=1.6483123302459717
I0209 23:51:27.771222 140050996819712 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.2862229645252228, loss=1.6610450744628906
I0209 23:52:02.308296 140051005212416 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.31240227818489075, loss=1.7120506763458252
I0209 23:52:36.882106 140050996819712 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.2999182939529419, loss=1.651676058769226
I0209 23:53:11.431277 140051005212416 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3045634329319, loss=1.706273078918457
I0209 23:53:45.984018 140050996819712 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.30795228481292725, loss=1.6008543968200684
I0209 23:54:20.539929 140051005212416 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3396196663379669, loss=1.628988265991211
I0209 23:54:55.095098 140050996819712 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3209734559059143, loss=1.669738531112671
I0209 23:55:29.643509 140051005212416 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.31406787037849426, loss=1.6039044857025146
I0209 23:56:04.176543 140050996819712 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.31342262029647827, loss=1.6956015825271606
I0209 23:56:38.733402 140051005212416 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.29459646344184875, loss=1.7376084327697754
I0209 23:57:13.253834 140050996819712 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.32766973972320557, loss=1.6474181413650513
I0209 23:57:47.807640 140051005212416 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3285678029060364, loss=1.7040503025054932
I0209 23:58:22.393201 140050996819712 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.33727791905403137, loss=1.7591313123703003
I0209 23:58:56.935731 140051005212416 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2970527112483978, loss=1.658520221710205
I0209 23:59:31.456668 140050996819712 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.30131977796554565, loss=1.6745177507400513
I0210 00:00:06.014637 140051005212416 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.2983734607696533, loss=1.681241512298584
I0210 00:00:40.537835 140050996819712 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.30456146597862244, loss=1.624760389328003
I0210 00:01:15.036409 140051005212416 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.2993164360523224, loss=1.569097876548767
I0210 00:01:44.089434 140225696298816 spec.py:321] Evaluating on the training split.
I0210 00:01:47.060914 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:06:11.558945 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 00:06:14.239933 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:09:29.579112 140225696298816 spec.py:349] Evaluating on the test split.
I0210 00:09:32.249405 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:12:08.026051 140225696298816 submission_runner.py:408] Time since start: 58635.11s, 	Step: 102186, 	{'train/accuracy': 0.6620293259620667, 'train/loss': 1.618472933769226, 'train/bleu': 32.64789488614793, 'validation/accuracy': 0.6765817999839783, 'validation/loss': 1.5140327215194702, 'validation/bleu': 29.198657105886326, 'validation/num_examples': 3000, 'test/accuracy': 0.6881877779960632, 'test/loss': 1.4374244213104248, 'test/bleu': 28.851760085084145, 'test/num_examples': 3003, 'score': 35310.28398871422, 'total_duration': 58635.107639312744, 'accumulated_submission_time': 35310.28398871422, 'accumulated_eval_time': 23320.24372291565, 'accumulated_logging_time': 1.3716270923614502}
I0210 00:12:08.055689 140050996819712 logging_writer.py:48] [102186] accumulated_eval_time=23320.243723, accumulated_logging_time=1.371627, accumulated_submission_time=35310.283989, global_step=102186, preemption_count=0, score=35310.283989, test/accuracy=0.688188, test/bleu=28.851760, test/loss=1.437424, test/num_examples=3003, total_duration=58635.107639, train/accuracy=0.662029, train/bleu=32.647895, train/loss=1.618473, validation/accuracy=0.676582, validation/bleu=29.198657, validation/loss=1.514033, validation/num_examples=3000
I0210 00:12:13.208459 140051005212416 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3195337951183319, loss=1.6424134969711304
I0210 00:12:47.565112 140050996819712 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.29823511838912964, loss=1.634634256362915
I0210 00:13:22.045836 140051005212416 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.28320929408073425, loss=1.5750198364257812
I0210 00:13:56.537651 140050996819712 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3106243312358856, loss=1.6897796392440796
I0210 00:14:31.033766 140051005212416 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.33594468235969543, loss=1.644600749015808
I0210 00:15:05.570274 140050996819712 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3283129930496216, loss=1.7547335624694824
I0210 00:15:40.077430 140051005212416 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3158104717731476, loss=1.7086882591247559
I0210 00:16:14.569641 140050996819712 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3144811689853668, loss=1.6159733533859253
I0210 00:16:49.087312 140051005212416 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2877770960330963, loss=1.676908254623413
I0210 00:17:23.620193 140050996819712 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.33665502071380615, loss=1.683864712715149
I0210 00:17:58.131049 140051005212416 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3111228346824646, loss=1.623488426208496
I0210 00:18:32.678834 140050996819712 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.33106672763824463, loss=1.681838035583496
I0210 00:19:07.239580 140051005212416 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.31377682089805603, loss=1.6345536708831787
I0210 00:19:41.765843 140050996819712 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3133068382740021, loss=1.6249390840530396
I0210 00:20:16.313403 140051005212416 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3182014524936676, loss=1.648065209388733
I0210 00:20:50.823110 140050996819712 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.36780261993408203, loss=1.6518523693084717
I0210 00:21:25.373049 140051005212416 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.33652594685554504, loss=1.5641920566558838
I0210 00:21:59.920002 140050996819712 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.34855836629867554, loss=1.6620583534240723
I0210 00:22:34.479394 140051005212416 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.30190548300743103, loss=1.6191011667251587
I0210 00:23:09.001524 140050996819712 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2863766849040985, loss=1.6641061305999756
I0210 00:23:43.542393 140051005212416 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3245835304260254, loss=1.6370137929916382
I0210 00:24:18.078387 140050996819712 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3018379807472229, loss=1.6540064811706543
I0210 00:24:52.623012 140051005212416 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.31111472845077515, loss=1.6492177248001099
I0210 00:25:27.165184 140050996819712 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3108139634132385, loss=1.703310251235962
I0210 00:26:01.675963 140051005212416 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3127974569797516, loss=1.5868358612060547
I0210 00:26:08.309336 140225696298816 spec.py:321] Evaluating on the training split.
I0210 00:26:11.274539 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:30:08.054625 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 00:30:10.718295 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:32:54.401826 140225696298816 spec.py:349] Evaluating on the test split.
I0210 00:32:57.082013 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:35:23.394750 140225696298816 submission_runner.py:408] Time since start: 60030.48s, 	Step: 104621, 	{'train/accuracy': 0.6610704660415649, 'train/loss': 1.626672625541687, 'train/bleu': 32.60958248876339, 'validation/accuracy': 0.6787516474723816, 'validation/loss': 1.5002140998840332, 'validation/bleu': 29.66641032835177, 'validation/num_examples': 3000, 'test/accuracy': 0.693591296672821, 'test/loss': 1.4133522510528564, 'test/bleu': 29.587910915299563, 'test/num_examples': 3003, 'score': 36150.45073056221, 'total_duration': 60030.47633481026, 'accumulated_submission_time': 36150.45073056221, 'accumulated_eval_time': 23875.329081773758, 'accumulated_logging_time': 1.4112050533294678}
I0210 00:35:23.424577 140050996819712 logging_writer.py:48] [104621] accumulated_eval_time=23875.329082, accumulated_logging_time=1.411205, accumulated_submission_time=36150.450731, global_step=104621, preemption_count=0, score=36150.450731, test/accuracy=0.693591, test/bleu=29.587911, test/loss=1.413352, test/num_examples=3003, total_duration=60030.476335, train/accuracy=0.661070, train/bleu=32.609582, train/loss=1.626673, validation/accuracy=0.678752, validation/bleu=29.666410, validation/loss=1.500214, validation/num_examples=3000
I0210 00:35:50.965813 140051005212416 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.2973712980747223, loss=1.637353539466858
I0210 00:36:25.416086 140050996819712 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.31760889291763306, loss=1.628808617591858
I0210 00:36:59.911911 140051005212416 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3187572956085205, loss=1.6288211345672607
I0210 00:37:34.415190 140050996819712 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3073894679546356, loss=1.618959665298462
I0210 00:38:08.923694 140051005212416 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3212520182132721, loss=1.5892421007156372
I0210 00:38:43.445678 140050996819712 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3136619031429291, loss=1.6706808805465698
I0210 00:39:17.978435 140051005212416 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3093520998954773, loss=1.63600492477417
I0210 00:39:52.535402 140050996819712 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3258858919143677, loss=1.5805059671401978
I0210 00:40:27.121492 140051005212416 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.32468342781066895, loss=1.6018524169921875
I0210 00:41:01.663534 140050996819712 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3220997154712677, loss=1.619691252708435
I0210 00:41:36.211109 140051005212416 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.32177409529685974, loss=1.5963103771209717
I0210 00:42:10.760888 140050996819712 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.31356433033943176, loss=1.619668960571289
I0210 00:42:45.333846 140051005212416 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.33516234159469604, loss=1.587356686592102
I0210 00:43:19.881460 140050996819712 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.298172265291214, loss=1.5360392332077026
I0210 00:43:54.422468 140051005212416 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.30840611457824707, loss=1.5685925483703613
I0210 00:44:28.986785 140050996819712 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.32451528310775757, loss=1.6594529151916504
I0210 00:45:03.513586 140051005212416 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.3267301917076111, loss=1.6689517498016357
I0210 00:45:38.039497 140050996819712 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.33387547731399536, loss=1.6266453266143799
I0210 00:46:12.571532 140051005212416 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.31749364733695984, loss=1.592381238937378
I0210 00:46:47.142012 140050996819712 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3386211693286896, loss=1.6446424722671509
I0210 00:47:21.673645 140051005212416 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.31713634729385376, loss=1.6501408815383911
I0210 00:47:56.216211 140050996819712 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.31431546807289124, loss=1.6191169023513794
I0210 00:48:30.789840 140051005212416 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.33286118507385254, loss=1.563895344734192
I0210 00:49:05.347390 140050996819712 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.31272438168525696, loss=1.6657441854476929
I0210 00:49:23.734842 140225696298816 spec.py:321] Evaluating on the training split.
I0210 00:49:26.709259 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:53:19.620915 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 00:53:22.311811 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 00:57:31.677962 140225696298816 spec.py:349] Evaluating on the test split.
I0210 00:57:34.358423 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:01:25.948077 140225696298816 submission_runner.py:408] Time since start: 61593.03s, 	Step: 107055, 	{'train/accuracy': 0.6778357028961182, 'train/loss': 1.5199074745178223, 'train/bleu': 33.98085596494906, 'validation/accuracy': 0.6803759336471558, 'validation/loss': 1.4900200366973877, 'validation/bleu': 29.69537641347098, 'validation/num_examples': 3000, 'test/accuracy': 0.6934751272201538, 'test/loss': 1.4053597450256348, 'test/bleu': 29.51269855331246, 'test/num_examples': 3003, 'score': 36990.67479014397, 'total_duration': 61593.029661655426, 'accumulated_submission_time': 36990.67479014397, 'accumulated_eval_time': 24597.542265176773, 'accumulated_logging_time': 1.4509341716766357}
I0210 01:01:25.979642 140051005212416 logging_writer.py:48] [107055] accumulated_eval_time=24597.542265, accumulated_logging_time=1.450934, accumulated_submission_time=36990.674790, global_step=107055, preemption_count=0, score=36990.674790, test/accuracy=0.693475, test/bleu=29.512699, test/loss=1.405360, test/num_examples=3003, total_duration=61593.029662, train/accuracy=0.677836, train/bleu=33.980856, train/loss=1.519907, validation/accuracy=0.680376, validation/bleu=29.695376, validation/loss=1.490020, validation/num_examples=3000
I0210 01:01:41.819601 140050996819712 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.31546714901924133, loss=1.6294951438903809
I0210 01:02:16.241606 140051005212416 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3542338013648987, loss=1.6269996166229248
I0210 01:02:50.755087 140050996819712 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.33746036887168884, loss=1.5965569019317627
I0210 01:03:25.290790 140051005212416 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3276105225086212, loss=1.6153298616409302
I0210 01:03:59.796182 140050996819712 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3180180490016937, loss=1.5244354009628296
I0210 01:04:34.329792 140051005212416 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3073110580444336, loss=1.5536606311798096
I0210 01:05:08.840092 140050996819712 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3280976414680481, loss=1.5557087659835815
I0210 01:05:43.372519 140051005212416 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.31357407569885254, loss=1.5841312408447266
I0210 01:06:17.901414 140050996819712 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.31986647844314575, loss=1.5138537883758545
I0210 01:06:52.418629 140051005212416 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3000274896621704, loss=1.595585823059082
I0210 01:07:26.965029 140050996819712 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3423983156681061, loss=1.6012775897979736
I0210 01:08:01.502573 140051005212416 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3330439031124115, loss=1.6027566194534302
I0210 01:08:36.049992 140050996819712 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.32758089900016785, loss=1.6487852334976196
I0210 01:09:10.570935 140051005212416 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3383572995662689, loss=1.574041485786438
I0210 01:09:45.122766 140050996819712 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.30735552310943604, loss=1.5856565237045288
I0210 01:10:19.674691 140051005212416 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.33331772685050964, loss=1.6130144596099854
I0210 01:10:54.211525 140050996819712 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3444267213344574, loss=1.6263322830200195
I0210 01:11:28.753723 140051005212416 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3126975893974304, loss=1.5576990842819214
I0210 01:12:03.299102 140050996819712 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.32171112298965454, loss=1.5378056764602661
I0210 01:12:37.828541 140051005212416 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3257884979248047, loss=1.6103605031967163
I0210 01:13:12.359284 140050996819712 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3137921690940857, loss=1.534369707107544
I0210 01:13:46.895732 140051005212416 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3139917850494385, loss=1.6204819679260254
I0210 01:14:21.436455 140050996819712 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3126707673072815, loss=1.549200177192688
I0210 01:14:55.969906 140051005212416 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.32896625995635986, loss=1.5575288534164429
I0210 01:15:26.080237 140225696298816 spec.py:321] Evaluating on the training split.
I0210 01:15:29.065358 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:19:28.697114 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 01:19:31.386278 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:22:46.113741 140225696298816 spec.py:349] Evaluating on the test split.
I0210 01:22:48.788232 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:26:18.946399 140225696298816 submission_runner.py:408] Time since start: 63086.03s, 	Step: 109489, 	{'train/accuracy': 0.6716055870056152, 'train/loss': 1.5597528219223022, 'train/bleu': 33.4808430765383, 'validation/accuracy': 0.6831905245780945, 'validation/loss': 1.4681103229522705, 'validation/bleu': 30.04091275834891, 'validation/num_examples': 3000, 'test/accuracy': 0.6989367604255676, 'test/loss': 1.3856124877929688, 'test/bleu': 29.94549352250294, 'test/num_examples': 3003, 'score': 37830.68860411644, 'total_duration': 63086.02798628807, 'accumulated_submission_time': 37830.68860411644, 'accumulated_eval_time': 25250.408385038376, 'accumulated_logging_time': 1.4924664497375488}
I0210 01:26:18.977005 140050996819712 logging_writer.py:48] [109489] accumulated_eval_time=25250.408385, accumulated_logging_time=1.492466, accumulated_submission_time=37830.688604, global_step=109489, preemption_count=0, score=37830.688604, test/accuracy=0.698937, test/bleu=29.945494, test/loss=1.385612, test/num_examples=3003, total_duration=63086.027986, train/accuracy=0.671606, train/bleu=33.480843, train/loss=1.559753, validation/accuracy=0.683191, validation/bleu=30.040913, validation/loss=1.468110, validation/num_examples=3000
I0210 01:26:23.120134 140051005212416 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.31905195116996765, loss=1.6408542394638062
I0210 01:26:57.521272 140050996819712 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.33841365575790405, loss=1.5147066116333008
I0210 01:27:32.038638 140051005212416 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3421036899089813, loss=1.6069632768630981
I0210 01:28:06.586203 140050996819712 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.33312755823135376, loss=1.4957547187805176
I0210 01:28:41.105859 140051005212416 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.354965478181839, loss=1.6172919273376465
I0210 01:29:15.667707 140050996819712 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.32760995626449585, loss=1.5765092372894287
I0210 01:29:50.213672 140051005212416 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.3307986557483673, loss=1.5887223482131958
I0210 01:30:24.747140 140050996819712 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.32550904154777527, loss=1.6154937744140625
I0210 01:30:59.282071 140051005212416 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.32745280861854553, loss=1.567366123199463
I0210 01:31:33.855774 140050996819712 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3363499045372009, loss=1.664771318435669
I0210 01:32:08.414338 140051005212416 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3385770618915558, loss=1.5662012100219727
I0210 01:32:42.978038 140050996819712 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3110872209072113, loss=1.514212727546692
I0210 01:33:17.512038 140051005212416 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.3312820494174957, loss=1.6327546834945679
I0210 01:33:52.025689 140050996819712 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3314352333545685, loss=1.6240438222885132
I0210 01:34:26.574710 140051005212416 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3195292353630066, loss=1.6429778337478638
I0210 01:35:01.136156 140050996819712 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3176504373550415, loss=1.5758329629898071
I0210 01:35:35.693471 140051005212416 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3511445224285126, loss=1.5991567373275757
I0210 01:36:10.209063 140050996819712 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3492720425128937, loss=1.511595368385315
I0210 01:36:44.704212 140051005212416 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.34237706661224365, loss=1.5817030668258667
I0210 01:37:19.263893 140050996819712 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.34092995524406433, loss=1.6116999387741089
I0210 01:37:53.791829 140051005212416 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3350231647491455, loss=1.611230731010437
I0210 01:38:28.350747 140050996819712 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.34757283329963684, loss=1.5856095552444458
I0210 01:39:02.901794 140051005212416 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.32461920380592346, loss=1.5533283948898315
I0210 01:39:37.444736 140050996819712 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.33485373854637146, loss=1.5450595617294312
I0210 01:40:11.969318 140051005212416 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.33763256669044495, loss=1.6238367557525635
I0210 01:40:19.288245 140225696298816 spec.py:321] Evaluating on the training split.
I0210 01:40:22.264595 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:44:03.567968 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 01:44:06.251399 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:46:42.360489 140225696298816 spec.py:349] Evaluating on the test split.
I0210 01:46:45.043619 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 01:49:21.318391 140225696298816 submission_runner.py:408] Time since start: 64468.40s, 	Step: 111923, 	{'train/accuracy': 0.6700042486190796, 'train/loss': 1.5730342864990234, 'train/bleu': 33.42104699643389, 'validation/accuracy': 0.6848644018173218, 'validation/loss': 1.4581775665283203, 'validation/bleu': 30.067353708829728, 'validation/num_examples': 3000, 'test/accuracy': 0.6991691589355469, 'test/loss': 1.375052571296692, 'test/bleu': 29.854163621406556, 'test/num_examples': 3003, 'score': 38670.9139418602, 'total_duration': 64468.39996099472, 'accumulated_submission_time': 38670.9139418602, 'accumulated_eval_time': 25792.438461780548, 'accumulated_logging_time': 1.5327885150909424}
I0210 01:49:21.353748 140050996819712 logging_writer.py:48] [111923] accumulated_eval_time=25792.438462, accumulated_logging_time=1.532789, accumulated_submission_time=38670.913942, global_step=111923, preemption_count=0, score=38670.913942, test/accuracy=0.699169, test/bleu=29.854164, test/loss=1.375053, test/num_examples=3003, total_duration=64468.399961, train/accuracy=0.670004, train/bleu=33.421047, train/loss=1.573034, validation/accuracy=0.684864, validation/bleu=30.067354, validation/loss=1.458178, validation/num_examples=3000
I0210 01:49:48.166462 140051005212416 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3438540995121002, loss=1.603858232498169
I0210 01:50:22.584372 140050996819712 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.339180588722229, loss=1.5063284635543823
I0210 01:50:57.113355 140051005212416 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.4484611451625824, loss=1.5185797214508057
I0210 01:51:31.646969 140050996819712 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.32463160157203674, loss=1.5379564762115479
I0210 01:52:06.174891 140051005212416 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3295249938964844, loss=1.5207602977752686
I0210 01:52:40.721605 140050996819712 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3356025815010071, loss=1.5623362064361572
I0210 01:53:15.273662 140051005212416 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3327791690826416, loss=1.5801142454147339
I0210 01:53:49.811914 140050996819712 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.322547048330307, loss=1.4909385442733765
I0210 01:54:24.344895 140051005212416 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.32924214005470276, loss=1.533729076385498
I0210 01:54:58.864179 140050996819712 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3279290795326233, loss=1.533860683441162
I0210 01:55:33.410139 140051005212416 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3220863938331604, loss=1.4524320363998413
I0210 01:56:07.957988 140050996819712 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3411438465118408, loss=1.4668962955474854
I0210 01:56:42.504205 140051005212416 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.32348117232322693, loss=1.4617396593093872
I0210 01:57:17.039389 140050996819712 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.338172048330307, loss=1.5725895166397095
I0210 01:57:51.574440 140051005212416 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.34815457463264465, loss=1.5306804180145264
I0210 01:58:26.112996 140050996819712 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.33632174134254456, loss=1.5395543575286865
I0210 01:59:00.620024 140051005212416 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.33828768134117126, loss=1.527063012123108
I0210 01:59:35.140618 140050996819712 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.3429982364177704, loss=1.543572187423706
I0210 02:00:09.709928 140051005212416 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3315252363681793, loss=1.5096718072891235
I0210 02:00:44.243150 140050996819712 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.356026828289032, loss=1.4953104257583618
I0210 02:01:18.768744 140051005212416 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3320216238498688, loss=1.518651008605957
I0210 02:01:53.276235 140050996819712 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3429633677005768, loss=1.5205696821212769
I0210 02:02:27.804878 140051005212416 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3219454288482666, loss=1.5516042709350586
I0210 02:03:02.302604 140050996819712 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3508610427379608, loss=1.5516337156295776
I0210 02:03:21.380457 140225696298816 spec.py:321] Evaluating on the training split.
I0210 02:03:24.351626 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:07:11.997553 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 02:07:14.681416 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:10:00.964966 140225696298816 spec.py:349] Evaluating on the test split.
I0210 02:10:03.636352 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:12:48.884960 140225696298816 submission_runner.py:408] Time since start: 65875.97s, 	Step: 114357, 	{'train/accuracy': 0.6836004257202148, 'train/loss': 1.4913737773895264, 'train/bleu': 34.42451429782527, 'validation/accuracy': 0.6876541972160339, 'validation/loss': 1.4455455541610718, 'validation/bleu': 30.17884093970701, 'validation/num_examples': 3000, 'test/accuracy': 0.702213704586029, 'test/loss': 1.3610974550247192, 'test/bleu': 30.163905195478442, 'test/num_examples': 3003, 'score': 39510.85387992859, 'total_duration': 65875.96654629707, 'accumulated_submission_time': 39510.85387992859, 'accumulated_eval_time': 26359.942928552628, 'accumulated_logging_time': 1.5786361694335938}
I0210 02:12:48.915335 140051005212416 logging_writer.py:48] [114357] accumulated_eval_time=26359.942929, accumulated_logging_time=1.578636, accumulated_submission_time=39510.853880, global_step=114357, preemption_count=0, score=39510.853880, test/accuracy=0.702214, test/bleu=30.163905, test/loss=1.361097, test/num_examples=3003, total_duration=65875.966546, train/accuracy=0.683600, train/bleu=34.424514, train/loss=1.491374, validation/accuracy=0.687654, validation/bleu=30.178841, validation/loss=1.445546, validation/num_examples=3000
I0210 02:13:04.057745 140050996819712 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3506588637828827, loss=1.5363636016845703
I0210 02:13:38.480487 140051005212416 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.34265661239624023, loss=1.6197224855422974
I0210 02:14:12.980529 140050996819712 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3548896312713623, loss=1.5122742652893066
I0210 02:14:47.499425 140051005212416 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3392234146595001, loss=1.478304386138916
I0210 02:15:22.014806 140050996819712 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3403047025203705, loss=1.5090562105178833
I0210 02:15:56.529181 140051005212416 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.3393270969390869, loss=1.5285861492156982
I0210 02:16:31.044004 140050996819712 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.324113130569458, loss=1.4832854270935059
I0210 02:17:05.567599 140051005212416 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3280617892742157, loss=1.4826421737670898
I0210 02:17:40.065625 140050996819712 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.36999717354774475, loss=1.5477560758590698
I0210 02:18:14.602580 140051005212416 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.35676491260528564, loss=1.5512115955352783
I0210 02:18:49.140383 140050996819712 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.33000993728637695, loss=1.5549900531768799
I0210 02:19:23.659004 140051005212416 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.34967392683029175, loss=1.580243706703186
I0210 02:19:58.180265 140050996819712 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.36058172583580017, loss=1.551952838897705
I0210 02:20:32.705008 140051005212416 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3351541757583618, loss=1.5639697313308716
I0210 02:21:07.224899 140050996819712 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3540360629558563, loss=1.520203709602356
I0210 02:21:41.774370 140051005212416 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.35560905933380127, loss=1.531213641166687
I0210 02:22:16.312975 140050996819712 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3589034080505371, loss=1.5362696647644043
I0210 02:22:50.848727 140051005212416 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.33479630947113037, loss=1.4876413345336914
I0210 02:23:25.380313 140050996819712 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3454185128211975, loss=1.5053412914276123
I0210 02:23:59.917171 140051005212416 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3592408001422882, loss=1.5124986171722412
I0210 02:24:34.446208 140050996819712 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.33533093333244324, loss=1.4243794679641724
I0210 02:25:09.002568 140051005212416 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3674585521221161, loss=1.5118306875228882
I0210 02:25:43.540453 140050996819712 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.35569727420806885, loss=1.5297422409057617
I0210 02:26:18.088540 140051005212416 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.34454992413520813, loss=1.4560914039611816
I0210 02:26:48.891129 140225696298816 spec.py:321] Evaluating on the training split.
I0210 02:26:51.867589 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:30:25.528246 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 02:30:28.207190 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:33:17.224069 140225696298816 spec.py:349] Evaluating on the test split.
I0210 02:33:19.888444 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:36:22.783778 140225696298816 submission_runner.py:408] Time since start: 67289.87s, 	Step: 116791, 	{'train/accuracy': 0.6783912181854248, 'train/loss': 1.516280174255371, 'train/bleu': 34.31626903846658, 'validation/accuracy': 0.6888321042060852, 'validation/loss': 1.4344900846481323, 'validation/bleu': 30.485961117752794, 'validation/num_examples': 3000, 'test/accuracy': 0.7040497660636902, 'test/loss': 1.344609022140503, 'test/bleu': 30.197523386417203, 'test/num_examples': 3003, 'score': 40350.74137663841, 'total_duration': 67289.86536455154, 'accumulated_submission_time': 40350.74137663841, 'accumulated_eval_time': 26933.835524082184, 'accumulated_logging_time': 1.6199851036071777}
I0210 02:36:22.815438 140050996819712 logging_writer.py:48] [116791] accumulated_eval_time=26933.835524, accumulated_logging_time=1.619985, accumulated_submission_time=40350.741377, global_step=116791, preemption_count=0, score=40350.741377, test/accuracy=0.704050, test/bleu=30.197523, test/loss=1.344609, test/num_examples=3003, total_duration=67289.865365, train/accuracy=0.678391, train/bleu=34.316269, train/loss=1.516280, validation/accuracy=0.688832, validation/bleu=30.485961, validation/loss=1.434490, validation/num_examples=3000
I0210 02:36:26.273898 140051005212416 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3285333812236786, loss=1.5009711980819702
I0210 02:37:00.680495 140050996819712 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.3633860647678375, loss=1.503595232963562
I0210 02:37:35.163139 140051005212416 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.35262855887413025, loss=1.5385572910308838
I0210 02:38:09.678693 140050996819712 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.38499757647514343, loss=1.525827169418335
I0210 02:38:44.217372 140051005212416 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3566642701625824, loss=1.513706088066101
I0210 02:39:18.758225 140050996819712 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3443182706832886, loss=1.5390545129776
I0210 02:39:53.305583 140051005212416 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.33934497833251953, loss=1.557847499847412
I0210 02:40:27.844121 140050996819712 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.36189916729927063, loss=1.5659558773040771
I0210 02:41:02.376687 140051005212416 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.33493876457214355, loss=1.4470263719558716
I0210 02:41:36.942838 140050996819712 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3474056124687195, loss=1.4556965827941895
I0210 02:42:11.505590 140051005212416 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.34261903166770935, loss=1.4880775213241577
I0210 02:42:46.027558 140050996819712 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3542555868625641, loss=1.4923211336135864
I0210 02:43:20.587237 140051005212416 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.332911878824234, loss=1.4096083641052246
I0210 02:43:55.129523 140050996819712 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.36417585611343384, loss=1.5141043663024902
I0210 02:44:29.685597 140051005212416 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.36051613092422485, loss=1.4617972373962402
I0210 02:45:04.234064 140050996819712 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.35026901960372925, loss=1.5273712873458862
I0210 02:45:38.761789 140051005212416 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3601692020893097, loss=1.4953688383102417
I0210 02:46:13.278336 140050996819712 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3521782159805298, loss=1.5036920309066772
I0210 02:46:47.891160 140051005212416 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3617209196090698, loss=1.4613659381866455
I0210 02:47:22.400916 140050996819712 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3507624566555023, loss=1.5610069036483765
I0210 02:47:56.914820 140051005212416 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.37157952785491943, loss=1.4668773412704468
I0210 02:48:31.473995 140050996819712 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.36583155393600464, loss=1.4428879022598267
I0210 02:49:06.011656 140051005212416 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3651396930217743, loss=1.5607918500900269
I0210 02:49:40.546591 140050996819712 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3579033315181732, loss=1.4673439264297485
I0210 02:50:15.092286 140051005212416 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.35779762268066406, loss=1.5874959230422974
I0210 02:50:23.104748 140225696298816 spec.py:321] Evaluating on the training split.
I0210 02:50:26.077270 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:54:43.321779 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 02:54:46.017527 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 02:58:15.096571 140225696298816 spec.py:349] Evaluating on the test split.
I0210 02:58:17.771379 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:01:56.643263 140225696298816 submission_runner.py:408] Time since start: 68823.72s, 	Step: 119225, 	{'train/accuracy': 0.6942856907844543, 'train/loss': 1.4276137351989746, 'train/bleu': 35.552780823300324, 'validation/accuracy': 0.6898612380027771, 'validation/loss': 1.4264612197875977, 'validation/bleu': 30.59610711383455, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.340147614479065, 'test/bleu': 30.534950471351095, 'test/num_examples': 3003, 'score': 41190.94413161278, 'total_duration': 68823.72482323647, 'accumulated_submission_time': 41190.94413161278, 'accumulated_eval_time': 27627.37395954132, 'accumulated_logging_time': 1.6613929271697998}
I0210 03:01:56.675749 140050996819712 logging_writer.py:48] [119225] accumulated_eval_time=27627.373960, accumulated_logging_time=1.661393, accumulated_submission_time=41190.944132, global_step=119225, preemption_count=0, score=41190.944132, test/accuracy=0.704317, test/bleu=30.534950, test/loss=1.340148, test/num_examples=3003, total_duration=68823.724823, train/accuracy=0.694286, train/bleu=35.552781, train/loss=1.427614, validation/accuracy=0.689861, validation/bleu=30.596107, validation/loss=1.426461, validation/num_examples=3000
I0210 03:02:22.800685 140051005212416 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3561638593673706, loss=1.4687532186508179
I0210 03:02:57.279147 140050996819712 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3602170944213867, loss=1.5197668075561523
I0210 03:03:31.794672 140051005212416 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.3544359505176544, loss=1.499874234199524
I0210 03:04:06.303339 140050996819712 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.345163494348526, loss=1.496849536895752
I0210 03:04:40.791335 140051005212416 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.34500786662101746, loss=1.509637713432312
I0210 03:05:15.336207 140050996819712 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.3456207513809204, loss=1.4256056547164917
I0210 03:05:49.865298 140051005212416 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.3579248785972595, loss=1.4966777563095093
I0210 03:06:24.387729 140050996819712 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.37918582558631897, loss=1.4680540561676025
I0210 03:06:58.926018 140051005212416 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3486279547214508, loss=1.5112007856369019
I0210 03:07:33.420026 140050996819712 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3474193811416626, loss=1.4496175050735474
I0210 03:08:07.958667 140051005212416 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.34450557827949524, loss=1.5324429273605347
I0210 03:08:42.507034 140050996819712 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.35981485247612, loss=1.5410007238388062
I0210 03:09:17.047233 140051005212416 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3632756769657135, loss=1.4997010231018066
I0210 03:09:51.577965 140050996819712 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.367797315120697, loss=1.5505454540252686
I0210 03:10:26.120434 140051005212416 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.3550516366958618, loss=1.4510831832885742
I0210 03:11:00.637657 140050996819712 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3698772192001343, loss=1.460453748703003
I0210 03:11:35.167883 140051005212416 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3804630637168884, loss=1.4949945211410522
I0210 03:12:09.714665 140050996819712 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.36035287380218506, loss=1.4120973348617554
I0210 03:12:44.269817 140051005212416 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.36170196533203125, loss=1.4467225074768066
I0210 03:13:18.804342 140050996819712 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3582443296909332, loss=1.4894949197769165
I0210 03:13:53.348047 140051005212416 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.35182616114616394, loss=1.3938875198364258
I0210 03:14:27.884462 140050996819712 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.36311355233192444, loss=1.4500688314437866
I0210 03:15:02.440511 140051005212416 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.35110464692115784, loss=1.459647297859192
I0210 03:15:36.984636 140050996819712 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.35457807779312134, loss=1.370660662651062
I0210 03:15:56.741355 140225696298816 spec.py:321] Evaluating on the training split.
I0210 03:15:59.713066 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:19:40.592738 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 03:19:43.270595 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:22:41.587906 140225696298816 spec.py:349] Evaluating on the test split.
I0210 03:22:44.279349 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:25:42.081480 140225696298816 submission_runner.py:408] Time since start: 70249.16s, 	Step: 121659, 	{'train/accuracy': 0.6907327771186829, 'train/loss': 1.4434564113616943, 'train/bleu': 34.881001369806654, 'validation/accuracy': 0.6906051635742188, 'validation/loss': 1.420662522315979, 'validation/bleu': 30.869333107168814, 'validation/num_examples': 3000, 'test/accuracy': 0.708349347114563, 'test/loss': 1.3267370462417603, 'test/bleu': 30.807794306392818, 'test/num_examples': 3003, 'score': 42030.92356848717, 'total_duration': 70249.16306734085, 'accumulated_submission_time': 42030.92356848717, 'accumulated_eval_time': 28212.71403479576, 'accumulated_logging_time': 1.7038319110870361}
I0210 03:25:42.115169 140051005212416 logging_writer.py:48] [121659] accumulated_eval_time=28212.714035, accumulated_logging_time=1.703832, accumulated_submission_time=42030.923568, global_step=121659, preemption_count=0, score=42030.923568, test/accuracy=0.708349, test/bleu=30.807794, test/loss=1.326737, test/num_examples=3003, total_duration=70249.163067, train/accuracy=0.690733, train/bleu=34.881001, train/loss=1.443456, validation/accuracy=0.690605, validation/bleu=30.869333, validation/loss=1.420663, validation/num_examples=3000
I0210 03:25:56.575570 140050996819712 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3752483129501343, loss=1.4928386211395264
I0210 03:26:31.013050 140051005212416 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.3684762418270111, loss=1.5430408716201782
I0210 03:27:05.525843 140050996819712 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3726237118244171, loss=1.5030179023742676
I0210 03:27:40.049248 140051005212416 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.35824769735336304, loss=1.4308809041976929
I0210 03:28:14.593559 140050996819712 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.3611283302307129, loss=1.3901475667953491
I0210 03:28:49.126697 140051005212416 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.38141947984695435, loss=1.4973900318145752
I0210 03:29:23.640344 140050996819712 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.35292723774909973, loss=1.436414361000061
I0210 03:29:58.173863 140051005212416 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.37424537539482117, loss=1.4146898984909058
I0210 03:30:32.739943 140050996819712 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3570444881916046, loss=1.4168368577957153
I0210 03:31:07.264165 140051005212416 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3837724030017853, loss=1.4578492641448975
I0210 03:31:41.811434 140050996819712 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.37179630994796753, loss=1.4853317737579346
I0210 03:32:16.363702 140051005212416 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.37369707226753235, loss=1.5035330057144165
I0210 03:32:50.896688 140050996819712 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3885495960712433, loss=1.4365079402923584
I0210 03:33:25.447640 140051005212416 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3703243136405945, loss=1.5221173763275146
I0210 03:33:59.982667 140050996819712 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3658125698566437, loss=1.471400499343872
I0210 03:34:34.521567 140051005212416 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3736080825328827, loss=1.3998830318450928
I0210 03:35:09.064158 140050996819712 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.38216814398765564, loss=1.467651605606079
I0210 03:35:43.582753 140051005212416 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.3689776062965393, loss=1.4633084535598755
I0210 03:36:18.134573 140050996819712 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.3808302879333496, loss=1.4160442352294922
I0210 03:36:52.629850 140051005212416 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.393656849861145, loss=1.5089622735977173
I0210 03:37:27.169308 140050996819712 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3706037998199463, loss=1.452582597732544
I0210 03:38:01.732682 140051005212416 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3619439899921417, loss=1.4238481521606445
I0210 03:38:36.260886 140050996819712 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.39387670159339905, loss=1.4448010921478271
I0210 03:39:10.786750 140051005212416 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3683028519153595, loss=1.3855689764022827
I0210 03:39:42.295519 140225696298816 spec.py:321] Evaluating on the training split.
I0210 03:39:45.266636 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:43:49.383177 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 03:43:52.061983 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:47:04.949345 140225696298816 spec.py:349] Evaluating on the test split.
I0210 03:47:07.632946 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 03:49:59.269626 140225696298816 submission_runner.py:408] Time since start: 71706.35s, 	Step: 124093, 	{'train/accuracy': 0.6904439330101013, 'train/loss': 1.4435969591140747, 'train/bleu': 35.23399677830485, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.4169179201126099, 'validation/bleu': 30.74350009523277, 'validation/num_examples': 3000, 'test/accuracy': 0.708453893661499, 'test/loss': 1.3230172395706177, 'test/bleu': 30.912976176032, 'test/num_examples': 3003, 'score': 42871.01779818535, 'total_duration': 71706.35120844841, 'accumulated_submission_time': 42871.01779818535, 'accumulated_eval_time': 28829.68808722496, 'accumulated_logging_time': 1.747204303741455}
I0210 03:49:59.303068 140050996819712 logging_writer.py:48] [124093] accumulated_eval_time=28829.688087, accumulated_logging_time=1.747204, accumulated_submission_time=42871.017798, global_step=124093, preemption_count=0, score=42871.017798, test/accuracy=0.708454, test/bleu=30.912976, test/loss=1.323017, test/num_examples=3003, total_duration=71706.351208, train/accuracy=0.690444, train/bleu=35.233997, train/loss=1.443597, validation/accuracy=0.691622, validation/bleu=30.743500, validation/loss=1.416918, validation/num_examples=3000
I0210 03:50:02.080309 140051005212416 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.35925137996673584, loss=1.404481291770935
I0210 03:50:36.416835 140050996819712 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.366071879863739, loss=1.4344910383224487
I0210 03:51:10.885585 140051005212416 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39395758509635925, loss=1.3947809934616089
I0210 03:51:45.384521 140050996819712 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3829757273197174, loss=1.4711706638336182
I0210 03:52:19.917243 140051005212416 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.38556304574012756, loss=1.4710696935653687
I0210 03:52:54.469638 140050996819712 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3719210624694824, loss=1.464343786239624
I0210 03:53:29.005212 140051005212416 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.38489586114883423, loss=1.4503099918365479
I0210 03:54:03.524905 140050996819712 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.37834039330482483, loss=1.4784715175628662
I0210 03:54:38.093352 140051005212416 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.38003024458885193, loss=1.4628713130950928
I0210 03:55:12.657056 140050996819712 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3634505271911621, loss=1.4569486379623413
I0210 03:55:47.217762 140051005212416 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.39147648215293884, loss=1.4320241212844849
I0210 03:56:21.779354 140050996819712 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.36709320545196533, loss=1.3911515474319458
I0210 03:56:56.321382 140051005212416 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3785967230796814, loss=1.4333771467208862
I0210 03:57:30.863224 140050996819712 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.3882932960987091, loss=1.5227428674697876
I0210 03:58:05.405292 140051005212416 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3792566657066345, loss=1.4612160921096802
I0210 03:58:39.944103 140050996819712 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.37504303455352783, loss=1.4863616228103638
I0210 03:59:14.511845 140051005212416 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.36040645837783813, loss=1.417446255683899
I0210 03:59:49.100852 140050996819712 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.3723183274269104, loss=1.4668422937393188
I0210 04:00:23.622756 140051005212416 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3637712895870209, loss=1.4425444602966309
I0210 04:00:58.166350 140050996819712 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3957303464412689, loss=1.4510983228683472
I0210 04:01:32.724721 140051005212416 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.37481167912483215, loss=1.4636991024017334
I0210 04:02:07.262326 140050996819712 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.36099156737327576, loss=1.4025893211364746
I0210 04:02:41.809976 140051005212416 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.38260418176651, loss=1.5431584119796753
I0210 04:03:16.340934 140050996819712 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.36104094982147217, loss=1.461046576499939
I0210 04:03:50.873373 140051005212416 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3679317533969879, loss=1.4325439929962158
I0210 04:03:59.597167 140225696298816 spec.py:321] Evaluating on the training split.
I0210 04:04:02.575798 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:07:58.776681 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 04:08:01.450007 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:11:07.511930 140225696298816 spec.py:349] Evaluating on the test split.
I0210 04:11:10.181860 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:13:58.354192 140225696298816 submission_runner.py:408] Time since start: 73145.44s, 	Step: 126527, 	{'train/accuracy': 0.6974536180496216, 'train/loss': 1.413323998451233, 'train/bleu': 35.86837829412484, 'validation/accuracy': 0.693134605884552, 'validation/loss': 1.4092453718185425, 'validation/bleu': 30.930916769026155, 'validation/num_examples': 3000, 'test/accuracy': 0.7094765305519104, 'test/loss': 1.3161181211471558, 'test/bleu': 30.907800321136087, 'test/num_examples': 3003, 'score': 43711.224172115326, 'total_duration': 73145.43577122688, 'accumulated_submission_time': 43711.224172115326, 'accumulated_eval_time': 29428.44505548477, 'accumulated_logging_time': 1.790480613708496}
I0210 04:13:58.387274 140050996819712 logging_writer.py:48] [126527] accumulated_eval_time=29428.445055, accumulated_logging_time=1.790481, accumulated_submission_time=43711.224172, global_step=126527, preemption_count=0, score=43711.224172, test/accuracy=0.709477, test/bleu=30.907800, test/loss=1.316118, test/num_examples=3003, total_duration=73145.435771, train/accuracy=0.697454, train/bleu=35.868378, train/loss=1.413324, validation/accuracy=0.693135, validation/bleu=30.930917, validation/loss=1.409245, validation/num_examples=3000
I0210 04:14:23.866626 140051005212416 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.3642857074737549, loss=1.3617063760757446
I0210 04:14:58.296466 140050996819712 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.37624508142471313, loss=1.357007384300232
I0210 04:15:32.826183 140051005212416 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.37053897976875305, loss=1.429404377937317
I0210 04:16:07.375412 140050996819712 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.3657819330692291, loss=1.381368637084961
I0210 04:16:41.879580 140051005212416 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.3596108555793762, loss=1.3717255592346191
I0210 04:17:16.413880 140050996819712 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3729337751865387, loss=1.3947148323059082
I0210 04:17:50.927790 140051005212416 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.38158756494522095, loss=1.3911828994750977
I0210 04:18:25.440982 140050996819712 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.36739397048950195, loss=1.4527404308319092
I0210 04:18:59.976234 140051005212416 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.39074549078941345, loss=1.3686108589172363
I0210 04:19:34.487272 140050996819712 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.37025004625320435, loss=1.4869334697723389
I0210 04:20:09.016599 140051005212416 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.37729567289352417, loss=1.4347026348114014
I0210 04:20:43.574841 140050996819712 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.3849678039550781, loss=1.4448750019073486
I0210 04:21:18.106895 140051005212416 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.3646298944950104, loss=1.4071518182754517
I0210 04:21:52.655743 140050996819712 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.3662247359752655, loss=1.3968524932861328
I0210 04:22:27.205705 140051005212416 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.3752205967903137, loss=1.4424083232879639
I0210 04:23:01.761797 140050996819712 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.38288915157318115, loss=1.4751107692718506
I0210 04:23:36.299715 140051005212416 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.38147950172424316, loss=1.4500168561935425
I0210 04:24:10.867755 140050996819712 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.39891675114631653, loss=1.4756883382797241
I0210 04:24:45.383326 140051005212416 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.36927878856658936, loss=1.4067652225494385
I0210 04:25:19.909557 140050996819712 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3770500123500824, loss=1.4273492097854614
I0210 04:25:54.454266 140051005212416 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.3985782265663147, loss=1.4603040218353271
I0210 04:26:28.995617 140050996819712 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3778693377971649, loss=1.4454867839813232
I0210 04:27:03.522387 140051005212416 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3840511739253998, loss=1.4544557332992554
I0210 04:27:38.057403 140050996819712 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.3690636456012726, loss=1.4285318851470947
I0210 04:27:58.489777 140225696298816 spec.py:321] Evaluating on the training split.
I0210 04:28:01.467064 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:31:50.539257 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 04:31:53.215326 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:35:05.512080 140225696298816 spec.py:349] Evaluating on the test split.
I0210 04:35:08.187296 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:38:22.610876 140225696298816 submission_runner.py:408] Time since start: 74609.69s, 	Step: 128961, 	{'train/accuracy': 0.6983780860900879, 'train/loss': 1.4064826965332031, 'train/bleu': 35.716242509446154, 'validation/accuracy': 0.692638635635376, 'validation/loss': 1.4071071147918701, 'validation/bleu': 31.06077647495805, 'validation/num_examples': 3000, 'test/accuracy': 0.7107547521591187, 'test/loss': 1.3119847774505615, 'test/bleu': 31.213135886632305, 'test/num_examples': 3003, 'score': 44551.23852276802, 'total_duration': 74609.69246411324, 'accumulated_submission_time': 44551.23852276802, 'accumulated_eval_time': 30052.566106319427, 'accumulated_logging_time': 1.8353376388549805}
I0210 04:38:22.644610 140051005212416 logging_writer.py:48] [128961] accumulated_eval_time=30052.566106, accumulated_logging_time=1.835338, accumulated_submission_time=44551.238523, global_step=128961, preemption_count=0, score=44551.238523, test/accuracy=0.710755, test/bleu=31.213136, test/loss=1.311985, test/num_examples=3003, total_duration=74609.692464, train/accuracy=0.698378, train/bleu=35.716243, train/loss=1.406483, validation/accuracy=0.692639, validation/bleu=31.060776, validation/loss=1.407107, validation/num_examples=3000
I0210 04:38:36.405389 140050996819712 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.38338786363601685, loss=1.438746690750122
I0210 04:39:10.813087 140051005212416 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.3746604025363922, loss=1.3786426782608032
I0210 04:39:45.342947 140050996819712 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.3787960708141327, loss=1.4300990104675293
I0210 04:40:19.868007 140051005212416 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.37759339809417725, loss=1.423946499824524
I0210 04:40:54.391234 140050996819712 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3707711100578308, loss=1.394554853439331
I0210 04:41:28.931482 140051005212416 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.3805828392505646, loss=1.3811531066894531
I0210 04:42:03.448512 140050996819712 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3602741062641144, loss=1.3917438983917236
I0210 04:42:37.958841 140051005212416 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.3840537369251251, loss=1.442328691482544
I0210 04:43:12.495794 140050996819712 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.3794148862361908, loss=1.3801275491714478
I0210 04:43:47.020385 140051005212416 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.371548593044281, loss=1.4766511917114258
I0210 04:44:21.561049 140050996819712 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.37252336740493774, loss=1.360694408416748
I0210 04:44:56.099272 140051005212416 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.38206562399864197, loss=1.4421720504760742
I0210 04:45:30.644665 140050996819712 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.37486234307289124, loss=1.458216905593872
I0210 04:46:05.200996 140051005212416 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.3933776319026947, loss=1.4876861572265625
I0210 04:46:39.741887 140050996819712 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.3840250074863434, loss=1.434005856513977
I0210 04:47:14.258851 140051005212416 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.39831799268722534, loss=1.4297473430633545
I0210 04:47:48.815331 140050996819712 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3756471872329712, loss=1.4170682430267334
I0210 04:48:23.358881 140051005212416 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.37130728363990784, loss=1.4145578145980835
I0210 04:48:57.918281 140050996819712 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.37219512462615967, loss=1.474224328994751
I0210 04:49:32.472705 140051005212416 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.36068370938301086, loss=1.3885606527328491
I0210 04:50:06.994180 140050996819712 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.36652302742004395, loss=1.378167748451233
I0210 04:50:41.502159 140051005212416 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.36836037039756775, loss=1.4620498418807983
I0210 04:51:16.016551 140050996819712 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3820306956768036, loss=1.4686522483825684
I0210 04:51:50.565786 140051005212416 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.39276108145713806, loss=1.4918242692947388
I0210 04:52:22.757847 140225696298816 spec.py:321] Evaluating on the training split.
I0210 04:52:25.754593 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:56:16.624348 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 04:56:19.306437 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 04:59:10.182871 140225696298816 spec.py:349] Evaluating on the test split.
I0210 04:59:12.853482 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:02:05.523843 140225696298816 submission_runner.py:408] Time since start: 76032.61s, 	Step: 131395, 	{'train/accuracy': 0.6976325511932373, 'train/loss': 1.4082444906234741, 'train/bleu': 35.93356190724933, 'validation/accuracy': 0.6933081746101379, 'validation/loss': 1.4070932865142822, 'validation/bleu': 30.89430551114946, 'validation/num_examples': 3000, 'test/accuracy': 0.711498498916626, 'test/loss': 1.3109595775604248, 'test/bleu': 31.106689500794634, 'test/num_examples': 3003, 'score': 45391.26353955269, 'total_duration': 76032.60543179512, 'accumulated_submission_time': 45391.26353955269, 'accumulated_eval_time': 30635.33205962181, 'accumulated_logging_time': 1.8811628818511963}
I0210 05:02:05.556917 140050996819712 logging_writer.py:48] [131395] accumulated_eval_time=30635.332060, accumulated_logging_time=1.881163, accumulated_submission_time=45391.263540, global_step=131395, preemption_count=0, score=45391.263540, test/accuracy=0.711498, test/bleu=31.106690, test/loss=1.310960, test/num_examples=3003, total_duration=76032.605432, train/accuracy=0.697633, train/bleu=35.933562, train/loss=1.408244, validation/accuracy=0.693308, validation/bleu=30.894306, validation/loss=1.407093, validation/num_examples=3000
I0210 05:02:07.646671 140051005212416 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.36366015672683716, loss=1.4092313051223755
I0210 05:02:42.034290 140050996819712 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.37619805335998535, loss=1.4069451093673706
I0210 05:03:16.513557 140051005212416 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.38178420066833496, loss=1.4262796640396118
I0210 05:03:51.055987 140050996819712 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.3758608102798462, loss=1.4409667253494263
I0210 05:04:25.604691 140051005212416 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.3810522258281708, loss=1.4447213411331177
I0210 05:05:00.114162 140050996819712 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3674524128437042, loss=1.363460898399353
I0210 05:05:34.649665 140051005212416 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.38423746824264526, loss=1.4542495012283325
I0210 05:06:09.180319 140050996819712 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.37834110856056213, loss=1.3526626825332642
I0210 05:06:43.712446 140051005212416 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.36160653829574585, loss=1.4286212921142578
I0210 05:07:18.250369 140050996819712 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.3809632956981659, loss=1.4284132719039917
I0210 05:07:52.789610 140051005212416 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.38758188486099243, loss=1.4693725109100342
I0210 05:08:27.330639 140050996819712 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.3873940706253052, loss=1.4397211074829102
I0210 05:09:01.866701 140051005212416 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.37146785855293274, loss=1.3926191329956055
I0210 05:09:36.394557 140050996819712 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3818008005619049, loss=1.4447565078735352
I0210 05:10:10.924690 140051005212416 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.3804525136947632, loss=1.4804387092590332
I0210 05:10:45.483063 140050996819712 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.40897276997566223, loss=1.5043660402297974
I0210 05:11:20.013275 140051005212416 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.36801955103874207, loss=1.4524455070495605
I0210 05:11:54.565223 140050996819712 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.372762531042099, loss=1.3911067247390747
I0210 05:12:29.145767 140051005212416 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.36985132098197937, loss=1.408935308456421
I0210 05:13:03.689149 140050996819712 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.367389976978302, loss=1.4891884326934814
I0210 05:13:14.456642 140225696298816 spec.py:321] Evaluating on the training split.
I0210 05:13:17.428174 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:17:12.826336 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 05:17:15.502921 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:20:04.199269 140225696298816 spec.py:349] Evaluating on the test split.
I0210 05:20:06.880696 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:23:09.209051 140225696298816 submission_runner.py:408] Time since start: 77296.29s, 	Step: 133333, 	{'train/accuracy': 0.6962321400642395, 'train/loss': 1.4215978384017944, 'train/bleu': 35.18021952446278, 'validation/accuracy': 0.6931717991828918, 'validation/loss': 1.4065862894058228, 'validation/bleu': 30.9194758232504, 'validation/num_examples': 3000, 'test/accuracy': 0.7112892866134644, 'test/loss': 1.3105939626693726, 'test/bleu': 31.039245815704213, 'test/num_examples': 3003, 'score': 46060.09185528755, 'total_duration': 77296.29063916206, 'accumulated_submission_time': 46060.09185528755, 'accumulated_eval_time': 31230.08441901207, 'accumulated_logging_time': 1.925365686416626}
I0210 05:23:09.242170 140051005212416 logging_writer.py:48] [133333] accumulated_eval_time=31230.084419, accumulated_logging_time=1.925366, accumulated_submission_time=46060.091855, global_step=133333, preemption_count=0, score=46060.091855, test/accuracy=0.711289, test/bleu=31.039246, test/loss=1.310594, test/num_examples=3003, total_duration=77296.290639, train/accuracy=0.696232, train/bleu=35.180220, train/loss=1.421598, validation/accuracy=0.693172, validation/bleu=30.919476, validation/loss=1.406586, validation/num_examples=3000
I0210 05:23:09.275500 140050996819712 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46060.091855
I0210 05:23:10.725506 140225696298816 checkpoints.py:490] Saving checkpoint at step: 133333
I0210 05:23:14.820621 140225696298816 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4/checkpoint_133333
I0210 05:23:14.825854 140225696298816 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_4/checkpoint_133333.
I0210 05:23:14.904053 140225696298816 submission_runner.py:583] Tuning trial 4/5
I0210 05:23:14.904338 140225696298816 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0210 05:23:14.915366 140225696298816 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006736774812452495, 'train/loss': 11.026397705078125, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 27.777300596237183, 'total_duration': 885.1699132919312, 'accumulated_submission_time': 27.777300596237183, 'accumulated_eval_time': 857.3925604820251, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2432, {'train/accuracy': 0.5334533452987671, 'train/loss': 2.6014294624328613, 'train/bleu': 24.246267876536, 'validation/accuracy': 0.5396957397460938, 'validation/loss': 2.53312087059021, 'validation/bleu': 20.273622232300472, 'validation/num_examples': 3000, 'test/accuracy': 0.5400848388671875, 'test/loss': 2.5434446334838867, 'test/bleu': 18.86934210352786, 'test/num_examples': 3003, 'score': 867.7713329792023, 'total_duration': 2192.2171177864075, 'accumulated_submission_time': 867.7713329792023, 'accumulated_eval_time': 1324.3492550849915, 'accumulated_logging_time': 0.019840002059936523, 'global_step': 2432, 'preemption_count': 0}), (4865, {'train/accuracy': 0.5815564393997192, 'train/loss': 2.2129969596862793, 'train/bleu': 27.397051490301113, 'validation/accuracy': 0.5966076254844666, 'validation/loss': 2.080467700958252, 'validation/bleu': 23.915241366267704, 'validation/num_examples': 3000, 'test/accuracy': 0.6017082333564758, 'test/loss': 2.0296666622161865, 'test/bleu': 22.691728876503866, 'test/num_examples': 3003, 'score': 1707.9139828681946, 'total_duration': 3536.4875156879425, 'accumulated_submission_time': 1707.9139828681946, 'accumulated_eval_time': 1828.3754241466522, 'accumulated_logging_time': 0.046137332916259766, 'global_step': 4865, 'preemption_count': 0}), (7297, {'train/accuracy': 0.5896901488304138, 'train/loss': 2.140878677368164, 'train/bleu': 27.75413258659114, 'validation/accuracy': 0.6057829260826111, 'validation/loss': 2.008117437362671, 'validation/bleu': 24.422510760879085, 'validation/num_examples': 3000, 'test/accuracy': 0.6117367148399353, 'test/loss': 1.962164282798767, 'test/bleu': 23.379135257735474, 'test/num_examples': 3003, 'score': 2547.882658958435, 'total_duration': 4870.319628000259, 'accumulated_submission_time': 2547.882658958435, 'accumulated_eval_time': 2322.1379055976868, 'accumulated_logging_time': 0.0710291862487793, 'global_step': 7297, 'preemption_count': 0}), (9730, {'train/accuracy': 0.5907862186431885, 'train/loss': 2.14593243598938, 'train/bleu': 27.758313068089066, 'validation/accuracy': 0.6103581786155701, 'validation/loss': 1.9752626419067383, 'validation/bleu': 24.93721700930189, 'validation/num_examples': 3000, 'test/accuracy': 0.6183835864067078, 'test/loss': 1.9371329545974731, 'test/bleu': 23.38670207942713, 'test/num_examples': 3003, 'score': 3388.040452480316, 'total_duration': 6209.612332820892, 'accumulated_submission_time': 3388.040452480316, 'accumulated_eval_time': 2821.1698200702667, 'accumulated_logging_time': 0.09875035285949707, 'global_step': 9730, 'preemption_count': 0}), (12163, {'train/accuracy': 0.5988882184028625, 'train/loss': 2.0783145427703857, 'train/bleu': 27.654304678780385, 'validation/accuracy': 0.6146730780601501, 'validation/loss': 1.9561139345169067, 'validation/bleu': 24.84778671859338, 'validation/num_examples': 3000, 'test/accuracy': 0.6207425594329834, 'test/loss': 1.8987210988998413, 'test/bleu': 23.72557107950406, 'test/num_examples': 3003, 'score': 4228.165338039398, 'total_duration': 7585.615716218948, 'accumulated_submission_time': 4228.165338039398, 'accumulated_eval_time': 3356.9464728832245, 'accumulated_logging_time': 0.1251380443572998, 'global_step': 12163, 'preemption_count': 0}), (14596, {'train/accuracy': 0.5957963466644287, 'train/loss': 2.0913431644439697, 'train/bleu': 27.566099296535413, 'validation/accuracy': 0.6168429255485535, 'validation/loss': 1.918594479560852, 'validation/bleu': 24.633238759292187, 'validation/num_examples': 3000, 'test/accuracy': 0.6213933229446411, 'test/loss': 1.8927571773529053, 'test/bleu': 23.30884255123911, 'test/num_examples': 3003, 'score': 5068.237158060074, 'total_duration': 8989.855357408524, 'accumulated_submission_time': 5068.237158060074, 'accumulated_eval_time': 3921.0114846229553, 'accumulated_logging_time': 0.15222644805908203, 'global_step': 14596, 'preemption_count': 0}), (17029, {'train/accuracy': 0.5992476940155029, 'train/loss': 2.066373348236084, 'train/bleu': 27.728670866665368, 'validation/accuracy': 0.6194343566894531, 'validation/loss': 1.9151461124420166, 'validation/bleu': 25.00878222369035, 'validation/num_examples': 3000, 'test/accuracy': 0.628063440322876, 'test/loss': 1.8578368425369263, 'test/bleu': 24.449780344884086, 'test/num_examples': 3003, 'score': 5908.252819776535, 'total_duration': 10318.040254831314, 'accumulated_submission_time': 5908.252819776535, 'accumulated_eval_time': 4409.076854467392, 'accumulated_logging_time': 0.1800706386566162, 'global_step': 17029, 'preemption_count': 0}), (19461, {'train/accuracy': 0.6093730926513672, 'train/loss': 1.9898121356964111, 'train/bleu': 28.350201916222264, 'validation/accuracy': 0.6198559403419495, 'validation/loss': 1.9141809940338135, 'validation/bleu': 25.218665959343333, 'validation/num_examples': 3000, 'test/accuracy': 0.6275056600570679, 'test/loss': 1.8592458963394165, 'test/bleu': 24.24569041878532, 'test/num_examples': 3003, 'score': 6748.244628667831, 'total_duration': 11677.037281274796, 'accumulated_submission_time': 6748.244628667831, 'accumulated_eval_time': 4927.977090597153, 'accumulated_logging_time': 0.20806145668029785, 'global_step': 19461, 'preemption_count': 0}), (21893, {'train/accuracy': 0.6032451391220093, 'train/loss': 2.035078763961792, 'train/bleu': 27.34081747482811, 'validation/accuracy': 0.6205874681472778, 'validation/loss': 1.8935205936431885, 'validation/bleu': 25.087785595498843, 'validation/num_examples': 3000, 'test/accuracy': 0.6247748732566833, 'test/loss': 1.851194977760315, 'test/bleu': 23.557981255248695, 'test/num_examples': 3003, 'score': 7588.188913345337, 'total_duration': 13184.40267777443, 'accumulated_submission_time': 7588.188913345337, 'accumulated_eval_time': 5595.293299913406, 'accumulated_logging_time': 0.2365128993988037, 'global_step': 21893, 'preemption_count': 0}), (24326, {'train/accuracy': 0.6028209328651428, 'train/loss': 2.0420238971710205, 'train/bleu': 28.100459720929965, 'validation/accuracy': 0.6226333379745483, 'validation/loss': 1.8778119087219238, 'validation/bleu': 24.9253727665481, 'validation/num_examples': 3000, 'test/accuracy': 0.6287490725517273, 'test/loss': 1.8355058431625366, 'test/bleu': 24.262781323959942, 'test/num_examples': 3003, 'score': 8428.134506702423, 'total_duration': 14566.971681118011, 'accumulated_submission_time': 8428.134506702423, 'accumulated_eval_time': 6137.813019990921, 'accumulated_logging_time': 0.26431941986083984, 'global_step': 24326, 'preemption_count': 0}), (26759, {'train/accuracy': 0.6044977307319641, 'train/loss': 2.021979808807373, 'train/bleu': 28.284201263564533, 'validation/accuracy': 0.624183177947998, 'validation/loss': 1.8740408420562744, 'validation/bleu': 25.36061576960488, 'validation/num_examples': 3000, 'test/accuracy': 0.6294578909873962, 'test/loss': 1.8357408046722412, 'test/bleu': 24.53579871136742, 'test/num_examples': 3003, 'score': 9268.36467552185, 'total_duration': 15965.183842658997, 'accumulated_submission_time': 9268.36467552185, 'accumulated_eval_time': 6695.690530538559, 'accumulated_logging_time': 0.29262280464172363, 'global_step': 26759, 'preemption_count': 0}), (29192, {'train/accuracy': 0.6068159937858582, 'train/loss': 2.0194811820983887, 'train/bleu': 28.95645104575732, 'validation/accuracy': 0.623960018157959, 'validation/loss': 1.8713575601577759, 'validation/bleu': 25.256518914297175, 'validation/num_examples': 3000, 'test/accuracy': 0.6307710409164429, 'test/loss': 1.8219873905181885, 'test/bleu': 24.321099015768755, 'test/num_examples': 3003, 'score': 10108.279440879822, 'total_duration': 17297.96948671341, 'accumulated_submission_time': 10108.279440879822, 'accumulated_eval_time': 7188.456845998764, 'accumulated_logging_time': 0.3211045265197754, 'global_step': 29192, 'preemption_count': 0}), (31625, {'train/accuracy': 0.6236925721168518, 'train/loss': 1.8840527534484863, 'train/bleu': 30.04438775326001, 'validation/accuracy': 0.6268489956855774, 'validation/loss': 1.861114740371704, 'validation/bleu': 25.799430214237688, 'validation/num_examples': 3000, 'test/accuracy': 0.6341409683227539, 'test/loss': 1.8083019256591797, 'test/bleu': 25.021743492191217, 'test/num_examples': 3003, 'score': 10948.29575920105, 'total_duration': 18655.023404359818, 'accumulated_submission_time': 10948.29575920105, 'accumulated_eval_time': 7705.389865159988, 'accumulated_logging_time': 0.3495962619781494, 'global_step': 31625, 'preemption_count': 0}), (34058, {'train/accuracy': 0.6058012843132019, 'train/loss': 2.0277810096740723, 'train/bleu': 28.870511638615522, 'validation/accuracy': 0.6283245086669922, 'validation/loss': 1.8390883207321167, 'validation/bleu': 25.78896790070437, 'validation/num_examples': 3000, 'test/accuracy': 0.6344314813613892, 'test/loss': 1.7929351329803467, 'test/bleu': 24.712103237909357, 'test/num_examples': 3003, 'score': 11788.428247213364, 'total_duration': 20093.013048648834, 'accumulated_submission_time': 11788.428247213364, 'accumulated_eval_time': 8303.139657497406, 'accumulated_logging_time': 0.38037562370300293, 'global_step': 34058, 'preemption_count': 0}), (36491, {'train/accuracy': 0.608106255531311, 'train/loss': 2.017210006713867, 'train/bleu': 28.904658555369576, 'validation/accuracy': 0.6289692521095276, 'validation/loss': 1.8452839851379395, 'validation/bleu': 26.038305212224035, 'validation/num_examples': 3000, 'test/accuracy': 0.6373133659362793, 'test/loss': 1.7856651544570923, 'test/bleu': 25.120160265789195, 'test/num_examples': 3003, 'score': 12628.50937962532, 'total_duration': 21464.642553329468, 'accumulated_submission_time': 12628.50937962532, 'accumulated_eval_time': 8834.580487966537, 'accumulated_logging_time': 0.41150975227355957, 'global_step': 36491, 'preemption_count': 0}), (38923, {'train/accuracy': 0.6152269244194031, 'train/loss': 1.9483067989349365, 'train/bleu': 28.735497170260977, 'validation/accuracy': 0.6286717057228088, 'validation/loss': 1.8417868614196777, 'validation/bleu': 25.679533221535827, 'validation/num_examples': 3000, 'test/accuracy': 0.6345128417015076, 'test/loss': 1.7959977388381958, 'test/bleu': 24.616820990422042, 'test/num_examples': 3003, 'score': 13468.485274791718, 'total_duration': 22799.837777614594, 'accumulated_submission_time': 13468.485274791718, 'accumulated_eval_time': 9329.693039894104, 'accumulated_logging_time': 0.4427659511566162, 'global_step': 38923, 'preemption_count': 0}), (41356, {'train/accuracy': 0.6089126467704773, 'train/loss': 1.994400978088379, 'train/bleu': 28.389003946961708, 'validation/accuracy': 0.6299487948417664, 'validation/loss': 1.834946632385254, 'validation/bleu': 25.0824188802018, 'validation/num_examples': 3000, 'test/accuracy': 0.6367555856704712, 'test/loss': 1.777856707572937, 'test/bleu': 24.162457512803574, 'test/num_examples': 3003, 'score': 14308.5158598423, 'total_duration': 24086.092776298523, 'accumulated_submission_time': 14308.5158598423, 'accumulated_eval_time': 9775.80866074562, 'accumulated_logging_time': 0.4757249355316162, 'global_step': 41356, 'preemption_count': 0}), (43789, {'train/accuracy': 0.6921001672744751, 'train/loss': 1.4866576194763184, 'train/bleu': 35.19845928097932, 'validation/accuracy': 0.6321682333946228, 'validation/loss': 1.8150848150253296, 'validation/bleu': 25.890601540900896, 'validation/num_examples': 3000, 'test/accuracy': 0.6381616592407227, 'test/loss': 1.7659757137298584, 'test/bleu': 25.0090002959881, 'test/num_examples': 3003, 'score': 15148.529332399368, 'total_duration': 25681.5877430439, 'accumulated_submission_time': 15148.529332399368, 'accumulated_eval_time': 10531.183442354202, 'accumulated_logging_time': 0.5067262649536133, 'global_step': 43789, 'preemption_count': 0}), (46222, {'train/accuracy': 0.613900899887085, 'train/loss': 1.9534741640090942, 'train/bleu': 28.758410706935436, 'validation/accuracy': 0.6313250660896301, 'validation/loss': 1.8070567846298218, 'validation/bleu': 25.706606452819305, 'validation/num_examples': 3000, 'test/accuracy': 0.6401255130767822, 'test/loss': 1.7569904327392578, 'test/bleu': 24.833041601835642, 'test/num_examples': 3003, 'score': 15988.571270942688, 'total_duration': 27042.712433576584, 'accumulated_submission_time': 15988.571270942688, 'accumulated_eval_time': 11052.160829782486, 'accumulated_logging_time': 0.5364077091217041, 'global_step': 46222, 'preemption_count': 0}), (48655, {'train/accuracy': 0.6083666682243347, 'train/loss': 1.9990887641906738, 'train/bleu': 29.043505470052917, 'validation/accuracy': 0.6298372149467468, 'validation/loss': 1.8104116916656494, 'validation/bleu': 25.873173393998723, 'validation/num_examples': 3000, 'test/accuracy': 0.6380454301834106, 'test/loss': 1.7637287378311157, 'test/bleu': 24.83728357032393, 'test/num_examples': 3003, 'score': 16828.72014260292, 'total_duration': 28389.814615249634, 'accumulated_submission_time': 16828.72014260292, 'accumulated_eval_time': 11559.00778746605, 'accumulated_logging_time': 0.5670428276062012, 'global_step': 48655, 'preemption_count': 0}), (51088, {'train/accuracy': 0.618570864200592, 'train/loss': 1.924091100692749, 'train/bleu': 29.38637507894043, 'validation/accuracy': 0.6350200176239014, 'validation/loss': 1.7849268913269043, 'validation/bleu': 26.589383299087874, 'validation/num_examples': 3000, 'test/accuracy': 0.6439370512962341, 'test/loss': 1.7313753366470337, 'test/bleu': 25.48859377960028, 'test/num_examples': 3003, 'score': 17668.943604707718, 'total_duration': 29739.820941209793, 'accumulated_submission_time': 17668.943604707718, 'accumulated_eval_time': 12068.680082798004, 'accumulated_logging_time': 0.6013104915618896, 'global_step': 51088, 'preemption_count': 0}), (53521, {'train/accuracy': 0.6205177903175354, 'train/loss': 1.9186264276504517, 'train/bleu': 29.460057986444184, 'validation/accuracy': 0.6394093036651611, 'validation/loss': 1.774772047996521, 'validation/bleu': 26.36329864950495, 'validation/num_examples': 3000, 'test/accuracy': 0.6456336379051208, 'test/loss': 1.7239888906478882, 'test/bleu': 25.94893984098375, 'test/num_examples': 3003, 'score': 18509.00431752205, 'total_duration': 31106.393884658813, 'accumulated_submission_time': 18509.00431752205, 'accumulated_eval_time': 12595.083347082138, 'accumulated_logging_time': 0.6347942352294922, 'global_step': 53521, 'preemption_count': 0}), (55954, {'train/accuracy': 0.616606593132019, 'train/loss': 1.9429373741149902, 'train/bleu': 29.716919854833712, 'validation/accuracy': 0.6380329728126526, 'validation/loss': 1.7670636177062988, 'validation/bleu': 26.879836011913316, 'validation/num_examples': 3000, 'test/accuracy': 0.650665283203125, 'test/loss': 1.6983579397201538, 'test/bleu': 26.0390079997861, 'test/num_examples': 3003, 'score': 19348.941828727722, 'total_duration': 32458.6449136734, 'accumulated_submission_time': 19348.941828727722, 'accumulated_eval_time': 13107.286288261414, 'accumulated_logging_time': 0.6692543029785156, 'global_step': 55954, 'preemption_count': 0}), (58387, {'train/accuracy': 0.6255083680152893, 'train/loss': 1.8838893175125122, 'train/bleu': 29.25000924980348, 'validation/accuracy': 0.63966965675354, 'validation/loss': 1.7545181512832642, 'validation/bleu': 26.65371403234823, 'validation/num_examples': 3000, 'test/accuracy': 0.6511068940162659, 'test/loss': 1.6911041736602783, 'test/bleu': 25.839829990207257, 'test/num_examples': 3003, 'score': 20189.042788743973, 'total_duration': 33925.682745695114, 'accumulated_submission_time': 20189.042788743973, 'accumulated_eval_time': 13734.115124940872, 'accumulated_logging_time': 0.7012717723846436, 'global_step': 58387, 'preemption_count': 0}), (60820, {'train/accuracy': 0.6229991912841797, 'train/loss': 1.9016964435577393, 'train/bleu': 29.731746344592317, 'validation/accuracy': 0.6420503258705139, 'validation/loss': 1.7479721307754517, 'validation/bleu': 26.703019452981483, 'validation/num_examples': 3000, 'test/accuracy': 0.6530939936637878, 'test/loss': 1.672269582748413, 'test/bleu': 25.896172575343115, 'test/num_examples': 3003, 'score': 21029.19118499756, 'total_duration': 35228.80893397331, 'accumulated_submission_time': 21029.19118499756, 'accumulated_eval_time': 14196.984060525894, 'accumulated_logging_time': 0.7340254783630371, 'global_step': 60820, 'preemption_count': 0}), (63253, {'train/accuracy': 0.6290479302406311, 'train/loss': 1.827910304069519, 'train/bleu': 30.297907849320147, 'validation/accuracy': 0.6419262886047363, 'validation/loss': 1.7374597787857056, 'validation/bleu': 26.966711675602447, 'validation/num_examples': 3000, 'test/accuracy': 0.6543838381767273, 'test/loss': 1.6732841730117798, 'test/bleu': 26.206675737115404, 'test/num_examples': 3003, 'score': 21869.32551908493, 'total_duration': 36555.478934049606, 'accumulated_submission_time': 21869.32551908493, 'accumulated_eval_time': 14683.410673379898, 'accumulated_logging_time': 0.7670059204101562, 'global_step': 63253, 'preemption_count': 0}), (65685, {'train/accuracy': 0.6229196786880493, 'train/loss': 1.90165114402771, 'train/bleu': 29.822310457333295, 'validation/accuracy': 0.6441333293914795, 'validation/loss': 1.7247092723846436, 'validation/bleu': 27.3569961752832, 'validation/num_examples': 3000, 'test/accuracy': 0.6523967385292053, 'test/loss': 1.6633965969085693, 'test/bleu': 26.01282070100662, 'test/num_examples': 3003, 'score': 22709.308198928833, 'total_duration': 37968.656623363495, 'accumulated_submission_time': 22709.308198928833, 'accumulated_eval_time': 15256.493627786636, 'accumulated_logging_time': 0.8021972179412842, 'global_step': 65685, 'preemption_count': 0}), (68118, {'train/accuracy': 0.6257489323616028, 'train/loss': 1.8845466375350952, 'train/bleu': 29.903225654359392, 'validation/accuracy': 0.6469975709915161, 'validation/loss': 1.7029153108596802, 'validation/bleu': 27.369239895807823, 'validation/num_examples': 3000, 'test/accuracy': 0.6561850309371948, 'test/loss': 1.6488022804260254, 'test/bleu': 26.68360440541797, 'test/num_examples': 3003, 'score': 23549.419113636017, 'total_duration': 39327.507074832916, 'accumulated_submission_time': 23549.419113636017, 'accumulated_eval_time': 15775.118801116943, 'accumulated_logging_time': 0.8397126197814941, 'global_step': 68118, 'preemption_count': 0}), (70552, {'train/accuracy': 0.6302703022956848, 'train/loss': 1.8330473899841309, 'train/bleu': 30.171382342838893, 'validation/accuracy': 0.649502158164978, 'validation/loss': 1.6973282098770142, 'validation/bleu': 27.077008915014122, 'validation/num_examples': 3000, 'test/accuracy': 0.6587531566619873, 'test/loss': 1.639639139175415, 'test/bleu': 26.84871294586024, 'test/num_examples': 3003, 'score': 24389.49199271202, 'total_duration': 40648.717832803726, 'accumulated_submission_time': 24389.49199271202, 'accumulated_eval_time': 16256.143748998642, 'accumulated_logging_time': 0.8760302066802979, 'global_step': 70552, 'preemption_count': 0}), (72985, {'train/accuracy': 0.6285916566848755, 'train/loss': 1.8592113256454468, 'train/bleu': 30.52954279286608, 'validation/accuracy': 0.6510024666786194, 'validation/loss': 1.6859381198883057, 'validation/bleu': 27.65884298530821, 'validation/num_examples': 3000, 'test/accuracy': 0.6605426669120789, 'test/loss': 1.6174166202545166, 'test/bleu': 26.853673620428214, 'test/num_examples': 3003, 'score': 25229.58637213707, 'total_duration': 41997.50688147545, 'accumulated_submission_time': 25229.58637213707, 'accumulated_eval_time': 16764.72614622116, 'accumulated_logging_time': 0.911757230758667, 'global_step': 72985, 'preemption_count': 0}), (75418, {'train/accuracy': 0.6453965306282043, 'train/loss': 1.716774582862854, 'train/bleu': 31.558854951636462, 'validation/accuracy': 0.6527011394500732, 'validation/loss': 1.681450605392456, 'validation/bleu': 27.563748681111136, 'validation/num_examples': 3000, 'test/accuracy': 0.6633432507514954, 'test/loss': 1.6029932498931885, 'test/bleu': 27.128032352674563, 'test/num_examples': 3003, 'score': 26069.70525288582, 'total_duration': 43430.02218937874, 'accumulated_submission_time': 26069.70525288582, 'accumulated_eval_time': 17357.010063409805, 'accumulated_logging_time': 0.9460341930389404, 'global_step': 75418, 'preemption_count': 0}), (77852, {'train/accuracy': 0.6361210942268372, 'train/loss': 1.8001832962036133, 'train/bleu': 30.33095928486013, 'validation/accuracy': 0.6541270613670349, 'validation/loss': 1.6627322435379028, 'validation/bleu': 27.47509347000726, 'validation/num_examples': 3000, 'test/accuracy': 0.667956531047821, 'test/loss': 1.5868098735809326, 'test/bleu': 27.65037334871951, 'test/num_examples': 3003, 'score': 26909.717646598816, 'total_duration': 44805.90866136551, 'accumulated_submission_time': 26909.717646598816, 'accumulated_eval_time': 17892.767714738846, 'accumulated_logging_time': 0.9847004413604736, 'global_step': 77852, 'preemption_count': 0}), (80285, {'train/accuracy': 0.6344415545463562, 'train/loss': 1.814192771911621, 'train/bleu': 30.737479092771107, 'validation/accuracy': 0.6556149125099182, 'validation/loss': 1.6477035284042358, 'validation/bleu': 27.724983358387746, 'validation/num_examples': 3000, 'test/accuracy': 0.6648771166801453, 'test/loss': 1.5862025022506714, 'test/bleu': 27.20680830071649, 'test/num_examples': 3003, 'score': 27749.741545915604, 'total_duration': 46149.47729349136, 'accumulated_submission_time': 27749.741545915604, 'accumulated_eval_time': 18396.198426246643, 'accumulated_logging_time': 1.0218725204467773, 'global_step': 80285, 'preemption_count': 0}), (82719, {'train/accuracy': 0.6453226804733276, 'train/loss': 1.7232457399368286, 'train/bleu': 31.61777031006529, 'validation/accuracy': 0.6575492024421692, 'validation/loss': 1.6341495513916016, 'validation/bleu': 28.112447135184922, 'validation/num_examples': 3000, 'test/accuracy': 0.6703736186027527, 'test/loss': 1.5644491910934448, 'test/bleu': 27.516618205330463, 'test/num_examples': 3003, 'score': 28589.926423549652, 'total_duration': 47558.457666397095, 'accumulated_submission_time': 28589.926423549652, 'accumulated_eval_time': 18964.878935098648, 'accumulated_logging_time': 1.0599958896636963, 'global_step': 82719, 'preemption_count': 0}), (85153, {'train/accuracy': 0.6420574188232422, 'train/loss': 1.7634731531143188, 'train/bleu': 31.85978635812725, 'validation/accuracy': 0.6593594551086426, 'validation/loss': 1.616284966468811, 'validation/bleu': 28.13691576680429, 'validation/num_examples': 3000, 'test/accuracy': 0.672267735004425, 'test/loss': 1.5401736497879028, 'test/bleu': 27.951467340137658, 'test/num_examples': 3003, 'score': 29430.129603147507, 'total_duration': 48923.83412575722, 'accumulated_submission_time': 29430.129603147507, 'accumulated_eval_time': 19489.938822746277, 'accumulated_logging_time': 1.096651315689087, 'global_step': 85153, 'preemption_count': 0}), (87586, {'train/accuracy': 0.706721842288971, 'train/loss': 1.3691809177398682, 'train/bleu': 36.764196846397844, 'validation/accuracy': 0.664182722568512, 'validation/loss': 1.5927585363388062, 'validation/bleu': 28.740379778097818, 'validation/num_examples': 3000, 'test/accuracy': 0.6754401326179504, 'test/loss': 1.5277425050735474, 'test/bleu': 27.999811967158358, 'test/num_examples': 3003, 'score': 30270.221952676773, 'total_duration': 50315.81853723526, 'accumulated_submission_time': 30270.221952676773, 'accumulated_eval_time': 20041.715032100677, 'accumulated_logging_time': 1.1359143257141113, 'global_step': 87586, 'preemption_count': 0}), (90019, {'train/accuracy': 0.6472726464271545, 'train/loss': 1.7228424549102783, 'train/bleu': 31.716836297151044, 'validation/accuracy': 0.6628807783126831, 'validation/loss': 1.5885562896728516, 'validation/bleu': 28.023800742966134, 'validation/num_examples': 3000, 'test/accuracy': 0.6758701205253601, 'test/loss': 1.5183173418045044, 'test/bleu': 27.95895404149604, 'test/num_examples': 3003, 'score': 31110.187237501144, 'total_duration': 51725.13202857971, 'accumulated_submission_time': 31110.187237501144, 'accumulated_eval_time': 20610.949380874634, 'accumulated_logging_time': 1.1731517314910889, 'global_step': 90019, 'preemption_count': 0}), (92452, {'train/accuracy': 0.6476435661315918, 'train/loss': 1.7178657054901123, 'train/bleu': 31.692039256295246, 'validation/accuracy': 0.6662781834602356, 'validation/loss': 1.5724620819091797, 'validation/bleu': 28.568028748018282, 'validation/num_examples': 3000, 'test/accuracy': 0.6786009073257446, 'test/loss': 1.4941082000732422, 'test/bleu': 28.248444795106696, 'test/num_examples': 3003, 'score': 31950.097545862198, 'total_duration': 53169.55118060112, 'accumulated_submission_time': 31950.097545862198, 'accumulated_eval_time': 21215.34196662903, 'accumulated_logging_time': 1.2124512195587158, 'global_step': 92452, 'preemption_count': 0}), (94886, {'train/accuracy': 0.6604101657867432, 'train/loss': 1.6313503980636597, 'train/bleu': 32.403113391945894, 'validation/accuracy': 0.6679768562316895, 'validation/loss': 1.559630036354065, 'validation/bleu': 28.760747178515107, 'validation/num_examples': 3000, 'test/accuracy': 0.6810412406921387, 'test/loss': 1.4820163249969482, 'test/bleu': 28.72655015719533, 'test/num_examples': 3003, 'score': 32790.26389718056, 'total_duration': 54487.12789297104, 'accumulated_submission_time': 32790.26389718056, 'accumulated_eval_time': 21692.634751558304, 'accumulated_logging_time': 1.252263069152832, 'global_step': 94886, 'preemption_count': 0}), (97319, {'train/accuracy': 0.6549776792526245, 'train/loss': 1.674398422241211, 'train/bleu': 32.00376825833275, 'validation/accuracy': 0.6725149154663086, 'validation/loss': 1.5401153564453125, 'validation/bleu': 28.802378796047854, 'validation/num_examples': 3000, 'test/accuracy': 0.6848992109298706, 'test/loss': 1.4625848531723022, 'test/bleu': 28.58278102882752, 'test/num_examples': 3003, 'score': 33630.172709703445, 'total_duration': 55810.58344745636, 'accumulated_submission_time': 33630.172709703445, 'accumulated_eval_time': 22176.063058376312, 'accumulated_logging_time': 1.2936749458312988, 'global_step': 97319, 'preemption_count': 0}), (99752, {'train/accuracy': 0.6542562246322632, 'train/loss': 1.6796025037765503, 'train/bleu': 32.45218523737319, 'validation/accuracy': 0.6749699115753174, 'validation/loss': 1.5269558429718018, 'validation/bleu': 29.279505165766782, 'validation/num_examples': 3000, 'test/accuracy': 0.6886526346206665, 'test/loss': 1.4499832391738892, 'test/bleu': 28.744171233574672, 'test/num_examples': 3003, 'score': 34470.056736946106, 'total_duration': 57170.82868671417, 'accumulated_submission_time': 34470.056736946106, 'accumulated_eval_time': 22696.30715584755, 'accumulated_logging_time': 1.3333721160888672, 'global_step': 99752, 'preemption_count': 0}), (102186, {'train/accuracy': 0.6620293259620667, 'train/loss': 1.618472933769226, 'train/bleu': 32.64789488614793, 'validation/accuracy': 0.6765817999839783, 'validation/loss': 1.5140327215194702, 'validation/bleu': 29.198657105886326, 'validation/num_examples': 3000, 'test/accuracy': 0.6881877779960632, 'test/loss': 1.4374244213104248, 'test/bleu': 28.851760085084145, 'test/num_examples': 3003, 'score': 35310.28398871422, 'total_duration': 58635.107639312744, 'accumulated_submission_time': 35310.28398871422, 'accumulated_eval_time': 23320.24372291565, 'accumulated_logging_time': 1.3716270923614502, 'global_step': 102186, 'preemption_count': 0}), (104621, {'train/accuracy': 0.6610704660415649, 'train/loss': 1.626672625541687, 'train/bleu': 32.60958248876339, 'validation/accuracy': 0.6787516474723816, 'validation/loss': 1.5002140998840332, 'validation/bleu': 29.66641032835177, 'validation/num_examples': 3000, 'test/accuracy': 0.693591296672821, 'test/loss': 1.4133522510528564, 'test/bleu': 29.587910915299563, 'test/num_examples': 3003, 'score': 36150.45073056221, 'total_duration': 60030.47633481026, 'accumulated_submission_time': 36150.45073056221, 'accumulated_eval_time': 23875.329081773758, 'accumulated_logging_time': 1.4112050533294678, 'global_step': 104621, 'preemption_count': 0}), (107055, {'train/accuracy': 0.6778357028961182, 'train/loss': 1.5199074745178223, 'train/bleu': 33.98085596494906, 'validation/accuracy': 0.6803759336471558, 'validation/loss': 1.4900200366973877, 'validation/bleu': 29.69537641347098, 'validation/num_examples': 3000, 'test/accuracy': 0.6934751272201538, 'test/loss': 1.4053597450256348, 'test/bleu': 29.51269855331246, 'test/num_examples': 3003, 'score': 36990.67479014397, 'total_duration': 61593.029661655426, 'accumulated_submission_time': 36990.67479014397, 'accumulated_eval_time': 24597.542265176773, 'accumulated_logging_time': 1.4509341716766357, 'global_step': 107055, 'preemption_count': 0}), (109489, {'train/accuracy': 0.6716055870056152, 'train/loss': 1.5597528219223022, 'train/bleu': 33.4808430765383, 'validation/accuracy': 0.6831905245780945, 'validation/loss': 1.4681103229522705, 'validation/bleu': 30.04091275834891, 'validation/num_examples': 3000, 'test/accuracy': 0.6989367604255676, 'test/loss': 1.3856124877929688, 'test/bleu': 29.94549352250294, 'test/num_examples': 3003, 'score': 37830.68860411644, 'total_duration': 63086.02798628807, 'accumulated_submission_time': 37830.68860411644, 'accumulated_eval_time': 25250.408385038376, 'accumulated_logging_time': 1.4924664497375488, 'global_step': 109489, 'preemption_count': 0}), (111923, {'train/accuracy': 0.6700042486190796, 'train/loss': 1.5730342864990234, 'train/bleu': 33.42104699643389, 'validation/accuracy': 0.6848644018173218, 'validation/loss': 1.4581775665283203, 'validation/bleu': 30.067353708829728, 'validation/num_examples': 3000, 'test/accuracy': 0.6991691589355469, 'test/loss': 1.375052571296692, 'test/bleu': 29.854163621406556, 'test/num_examples': 3003, 'score': 38670.9139418602, 'total_duration': 64468.39996099472, 'accumulated_submission_time': 38670.9139418602, 'accumulated_eval_time': 25792.438461780548, 'accumulated_logging_time': 1.5327885150909424, 'global_step': 111923, 'preemption_count': 0}), (114357, {'train/accuracy': 0.6836004257202148, 'train/loss': 1.4913737773895264, 'train/bleu': 34.42451429782527, 'validation/accuracy': 0.6876541972160339, 'validation/loss': 1.4455455541610718, 'validation/bleu': 30.17884093970701, 'validation/num_examples': 3000, 'test/accuracy': 0.702213704586029, 'test/loss': 1.3610974550247192, 'test/bleu': 30.163905195478442, 'test/num_examples': 3003, 'score': 39510.85387992859, 'total_duration': 65875.96654629707, 'accumulated_submission_time': 39510.85387992859, 'accumulated_eval_time': 26359.942928552628, 'accumulated_logging_time': 1.5786361694335938, 'global_step': 114357, 'preemption_count': 0}), (116791, {'train/accuracy': 0.6783912181854248, 'train/loss': 1.516280174255371, 'train/bleu': 34.31626903846658, 'validation/accuracy': 0.6888321042060852, 'validation/loss': 1.4344900846481323, 'validation/bleu': 30.485961117752794, 'validation/num_examples': 3000, 'test/accuracy': 0.7040497660636902, 'test/loss': 1.344609022140503, 'test/bleu': 30.197523386417203, 'test/num_examples': 3003, 'score': 40350.74137663841, 'total_duration': 67289.86536455154, 'accumulated_submission_time': 40350.74137663841, 'accumulated_eval_time': 26933.835524082184, 'accumulated_logging_time': 1.6199851036071777, 'global_step': 116791, 'preemption_count': 0}), (119225, {'train/accuracy': 0.6942856907844543, 'train/loss': 1.4276137351989746, 'train/bleu': 35.552780823300324, 'validation/accuracy': 0.6898612380027771, 'validation/loss': 1.4264612197875977, 'validation/bleu': 30.59610711383455, 'validation/num_examples': 3000, 'test/accuracy': 0.704317033290863, 'test/loss': 1.340147614479065, 'test/bleu': 30.534950471351095, 'test/num_examples': 3003, 'score': 41190.94413161278, 'total_duration': 68823.72482323647, 'accumulated_submission_time': 41190.94413161278, 'accumulated_eval_time': 27627.37395954132, 'accumulated_logging_time': 1.6613929271697998, 'global_step': 119225, 'preemption_count': 0}), (121659, {'train/accuracy': 0.6907327771186829, 'train/loss': 1.4434564113616943, 'train/bleu': 34.881001369806654, 'validation/accuracy': 0.6906051635742188, 'validation/loss': 1.420662522315979, 'validation/bleu': 30.869333107168814, 'validation/num_examples': 3000, 'test/accuracy': 0.708349347114563, 'test/loss': 1.3267370462417603, 'test/bleu': 30.807794306392818, 'test/num_examples': 3003, 'score': 42030.92356848717, 'total_duration': 70249.16306734085, 'accumulated_submission_time': 42030.92356848717, 'accumulated_eval_time': 28212.71403479576, 'accumulated_logging_time': 1.7038319110870361, 'global_step': 121659, 'preemption_count': 0}), (124093, {'train/accuracy': 0.6904439330101013, 'train/loss': 1.4435969591140747, 'train/bleu': 35.23399677830485, 'validation/accuracy': 0.6916218996047974, 'validation/loss': 1.4169179201126099, 'validation/bleu': 30.74350009523277, 'validation/num_examples': 3000, 'test/accuracy': 0.708453893661499, 'test/loss': 1.3230172395706177, 'test/bleu': 30.912976176032, 'test/num_examples': 3003, 'score': 42871.01779818535, 'total_duration': 71706.35120844841, 'accumulated_submission_time': 42871.01779818535, 'accumulated_eval_time': 28829.68808722496, 'accumulated_logging_time': 1.747204303741455, 'global_step': 124093, 'preemption_count': 0}), (126527, {'train/accuracy': 0.6974536180496216, 'train/loss': 1.413323998451233, 'train/bleu': 35.86837829412484, 'validation/accuracy': 0.693134605884552, 'validation/loss': 1.4092453718185425, 'validation/bleu': 30.930916769026155, 'validation/num_examples': 3000, 'test/accuracy': 0.7094765305519104, 'test/loss': 1.3161181211471558, 'test/bleu': 30.907800321136087, 'test/num_examples': 3003, 'score': 43711.224172115326, 'total_duration': 73145.43577122688, 'accumulated_submission_time': 43711.224172115326, 'accumulated_eval_time': 29428.44505548477, 'accumulated_logging_time': 1.790480613708496, 'global_step': 126527, 'preemption_count': 0}), (128961, {'train/accuracy': 0.6983780860900879, 'train/loss': 1.4064826965332031, 'train/bleu': 35.716242509446154, 'validation/accuracy': 0.692638635635376, 'validation/loss': 1.4071071147918701, 'validation/bleu': 31.06077647495805, 'validation/num_examples': 3000, 'test/accuracy': 0.7107547521591187, 'test/loss': 1.3119847774505615, 'test/bleu': 31.213135886632305, 'test/num_examples': 3003, 'score': 44551.23852276802, 'total_duration': 74609.69246411324, 'accumulated_submission_time': 44551.23852276802, 'accumulated_eval_time': 30052.566106319427, 'accumulated_logging_time': 1.8353376388549805, 'global_step': 128961, 'preemption_count': 0}), (131395, {'train/accuracy': 0.6976325511932373, 'train/loss': 1.4082444906234741, 'train/bleu': 35.93356190724933, 'validation/accuracy': 0.6933081746101379, 'validation/loss': 1.4070932865142822, 'validation/bleu': 30.89430551114946, 'validation/num_examples': 3000, 'test/accuracy': 0.711498498916626, 'test/loss': 1.3109595775604248, 'test/bleu': 31.106689500794634, 'test/num_examples': 3003, 'score': 45391.26353955269, 'total_duration': 76032.60543179512, 'accumulated_submission_time': 45391.26353955269, 'accumulated_eval_time': 30635.33205962181, 'accumulated_logging_time': 1.8811628818511963, 'global_step': 131395, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6962321400642395, 'train/loss': 1.4215978384017944, 'train/bleu': 35.18021952446278, 'validation/accuracy': 0.6931717991828918, 'validation/loss': 1.4065862894058228, 'validation/bleu': 30.9194758232504, 'validation/num_examples': 3000, 'test/accuracy': 0.7112892866134644, 'test/loss': 1.3105939626693726, 'test/bleu': 31.039245815704213, 'test/num_examples': 3003, 'score': 46060.09185528755, 'total_duration': 77296.29063916206, 'accumulated_submission_time': 46060.09185528755, 'accumulated_eval_time': 31230.08441901207, 'accumulated_logging_time': 1.925365686416626, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0210 05:23:14.915596 140225696298816 submission_runner.py:586] Timing: 46060.09185528755
I0210 05:23:14.915697 140225696298816 submission_runner.py:588] Total number of evals: 56
I0210 05:23:14.915772 140225696298816 submission_runner.py:589] ====================
I0210 05:23:14.916032 140225696298816 submission_runner.py:542] Using RNG seed 1037423020
I0210 05:23:14.917974 140225696298816 submission_runner.py:551] --- Tuning run 5/5 ---
I0210 05:23:14.918105 140225696298816 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5.
I0210 05:23:14.918435 140225696298816 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5/hparams.json.
I0210 05:23:14.919309 140225696298816 submission_runner.py:206] Initializing dataset.
I0210 05:23:14.922224 140225696298816 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 05:23:14.925758 140225696298816 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 05:23:14.969929 140225696298816 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 05:23:15.731136 140225696298816 submission_runner.py:213] Initializing model.
I0210 05:23:22.242860 140225696298816 submission_runner.py:255] Initializing optimizer.
I0210 05:23:23.011957 140225696298816 submission_runner.py:262] Initializing metrics bundle.
I0210 05:23:23.012147 140225696298816 submission_runner.py:280] Initializing checkpoint and logger.
I0210 05:23:23.013181 140225696298816 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5 with prefix checkpoint_
I0210 05:23:23.013304 140225696298816 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5/meta_data_0.json.
I0210 05:23:23.013518 140225696298816 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 05:23:23.013581 140225696298816 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 05:23:23.615433 140225696298816 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 05:23:24.192420 140225696298816 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5/flags_0.json.
I0210 05:23:24.195839 140225696298816 submission_runner.py:314] Starting training loop.
I0210 05:23:55.093236 140050897565440 logging_writer.py:48] [0] global_step=0, grad_norm=4.789247989654541, loss=11.021476745605469
I0210 05:23:55.106260 140225696298816 spec.py:321] Evaluating on the training split.
I0210 05:23:57.776726 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:28:40.195788 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 05:28:42.869435 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:33:25.867594 140225696298816 spec.py:349] Evaluating on the test split.
I0210 05:33:28.545673 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:38:11.334405 140225696298816 submission_runner.py:408] Time since start: 887.14s, 	Step: 1, 	{'train/accuracy': 0.0005945574957877398, 'train/loss': 11.024234771728516, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.910383462905884, 'total_duration': 887.1384572982788, 'accumulated_submission_time': 30.910383462905884, 'accumulated_eval_time': 856.2280375957489, 'accumulated_logging_time': 0}
I0210 05:38:11.343537 140050979010304 logging_writer.py:48] [1] accumulated_eval_time=856.228038, accumulated_logging_time=0, accumulated_submission_time=30.910383, global_step=1, preemption_count=0, score=30.910383, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.036274, test/num_examples=3003, total_duration=887.138457, train/accuracy=0.000595, train/bleu=0.000000, train/loss=11.024235, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.047277, validation/num_examples=3000
I0210 05:38:46.662326 140050897565440 logging_writer.py:48] [100] global_step=100, grad_norm=0.1496126502752304, loss=8.248980522155762
I0210 05:39:22.052953 140050979010304 logging_writer.py:48] [200] global_step=200, grad_norm=0.49750253558158875, loss=7.485761642456055
I0210 05:39:57.467157 140050897565440 logging_writer.py:48] [300] global_step=300, grad_norm=0.370887815952301, loss=6.868781566619873
I0210 05:40:32.904250 140050979010304 logging_writer.py:48] [400] global_step=400, grad_norm=0.5543487071990967, loss=6.3305583000183105
I0210 05:41:08.325487 140050897565440 logging_writer.py:48] [500] global_step=500, grad_norm=0.4456428289413452, loss=5.862081527709961
I0210 05:41:43.774004 140050979010304 logging_writer.py:48] [600] global_step=600, grad_norm=0.4598492681980133, loss=5.494670391082764
I0210 05:42:19.217144 140050897565440 logging_writer.py:48] [700] global_step=700, grad_norm=0.41917625069618225, loss=5.290529251098633
I0210 05:42:54.693130 140050979010304 logging_writer.py:48] [800] global_step=800, grad_norm=0.3763640224933624, loss=5.005205154418945
I0210 05:43:30.143231 140050897565440 logging_writer.py:48] [900] global_step=900, grad_norm=0.4479650557041168, loss=4.816192150115967
I0210 05:44:05.571933 140050979010304 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5083536505699158, loss=4.5925822257995605
I0210 05:44:41.020637 140050897565440 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5439562201499939, loss=4.348013877868652
I0210 05:45:16.453145 140050979010304 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4926135540008545, loss=4.020853519439697
I0210 05:45:51.885565 140050897565440 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.4810989797115326, loss=4.002508163452148
I0210 05:46:27.314098 140050979010304 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5133811831474304, loss=3.771695613861084
I0210 05:47:02.738193 140050897565440 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7034692168235779, loss=3.6612281799316406
I0210 05:47:38.156028 140050979010304 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.44685640931129456, loss=3.5535013675689697
I0210 05:48:13.605767 140050897565440 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.4392106533050537, loss=3.5051796436309814
I0210 05:48:49.048635 140050979010304 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.4126760959625244, loss=3.2649645805358887
I0210 05:49:24.499472 140050897565440 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6610723733901978, loss=3.300431489944458
I0210 05:49:59.949624 140050979010304 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4442600607872009, loss=3.2287046909332275
I0210 05:50:35.390995 140050897565440 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.37322998046875, loss=3.0162947177886963
I0210 05:51:10.837501 140050979010304 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3687697649002075, loss=3.0847933292388916
I0210 05:51:46.261860 140050897565440 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4183123707771301, loss=2.966547966003418
I0210 05:52:11.509957 140225696298816 spec.py:321] Evaluating on the training split.
I0210 05:52:14.476241 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:55:00.204826 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 05:55:02.890691 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 05:57:55.165305 140225696298816 spec.py:349] Evaluating on the test split.
I0210 05:57:57.845831 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:00:32.579673 140225696298816 submission_runner.py:408] Time since start: 2228.38s, 	Step: 2373, 	{'train/accuracy': 0.5159180164337158, 'train/loss': 2.8420560359954834, 'train/bleu': 23.051630204159792, 'validation/accuracy': 0.5170301795005798, 'validation/loss': 2.82480525970459, 'validation/bleu': 18.71438425767787, 'validation/num_examples': 3000, 'test/accuracy': 0.5161815285682678, 'test/loss': 2.86415958404541, 'test/bleu': 17.2187217021128, 'test/num_examples': 3003, 'score': 870.9926633834839, 'total_duration': 2228.3837530612946, 'accumulated_submission_time': 870.9926633834839, 'accumulated_eval_time': 1357.2976994514465, 'accumulated_logging_time': 0.018973827362060547}
I0210 06:00:32.595120 140050979010304 logging_writer.py:48] [2373] accumulated_eval_time=1357.297699, accumulated_logging_time=0.018974, accumulated_submission_time=870.992663, global_step=2373, preemption_count=0, score=870.992663, test/accuracy=0.516182, test/bleu=17.218722, test/loss=2.864160, test/num_examples=3003, total_duration=2228.383753, train/accuracy=0.515918, train/bleu=23.051630, train/loss=2.842056, validation/accuracy=0.517030, validation/bleu=18.714384, validation/loss=2.824805, validation/num_examples=3000
I0210 06:00:42.475578 140050897565440 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3147754371166229, loss=2.917475461959839
I0210 06:01:17.733206 140050979010304 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3308829367160797, loss=2.9229209423065186
I0210 06:01:53.103486 140050897565440 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.3628811240196228, loss=2.785848617553711
I0210 06:02:28.532574 140050979010304 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.29407089948654175, loss=2.773353338241577
I0210 06:03:03.927165 140050897565440 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.3403575122356415, loss=2.7842764854431152
I0210 06:03:39.325418 140050979010304 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2991385757923126, loss=2.651689052581787
I0210 06:04:14.750253 140050897565440 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.28800785541534424, loss=2.7051360607147217
I0210 06:04:50.148576 140050979010304 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.4304908514022827, loss=2.6926662921905518
I0210 06:05:25.560197 140050897565440 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.23656705021858215, loss=2.6127796173095703
I0210 06:06:00.957202 140050979010304 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.22881928086280823, loss=2.549565553665161
I0210 06:06:36.361414 140050897565440 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.25294002890586853, loss=2.5916333198547363
I0210 06:07:11.760004 140050979010304 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2623698115348816, loss=2.471292734146118
I0210 06:07:47.137810 140050897565440 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.22242647409439087, loss=2.5604162216186523
I0210 06:08:22.554580 140050979010304 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.1950279027223587, loss=2.4590539932250977
I0210 06:08:57.947941 140050897565440 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.227520152926445, loss=2.4616568088531494
I0210 06:09:33.373589 140050979010304 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.21709628403186798, loss=2.4427525997161865
I0210 06:10:08.797881 140050897565440 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.19905081391334534, loss=2.484442710876465
I0210 06:10:44.200564 140050979010304 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.18598544597625732, loss=2.3602378368377686
I0210 06:11:19.599685 140050897565440 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.21281182765960693, loss=2.452755928039551
I0210 06:11:54.992864 140050979010304 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.2138512134552002, loss=2.356196641921997
I0210 06:12:30.411763 140050897565440 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.17305836081504822, loss=2.3046374320983887
I0210 06:13:05.823002 140050979010304 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.18857358396053314, loss=2.271334171295166
I0210 06:13:41.235907 140050897565440 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.21143731474876404, loss=2.326192617416382
I0210 06:14:16.638448 140050979010304 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15674394369125366, loss=2.267055034637451
I0210 06:14:32.647877 140225696298816 spec.py:321] Evaluating on the training split.
I0210 06:14:35.623205 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:18:55.901692 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 06:18:58.572633 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:22:00.777124 140225696298816 spec.py:349] Evaluating on the test split.
I0210 06:22:03.462249 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:24:35.607333 140225696298816 submission_runner.py:408] Time since start: 3671.41s, 	Step: 4747, 	{'train/accuracy': 0.5766600966453552, 'train/loss': 2.2446744441986084, 'train/bleu': 26.829642974541517, 'validation/accuracy': 0.5912140011787415, 'validation/loss': 2.1382405757904053, 'validation/bleu': 23.472348577549567, 'validation/num_examples': 3000, 'test/accuracy': 0.5934344530105591, 'test/loss': 2.123737335205078, 'test/bleu': 22.196164092048317, 'test/num_examples': 3003, 'score': 1710.9601809978485, 'total_duration': 3671.4114067554474, 'accumulated_submission_time': 1710.9601809978485, 'accumulated_eval_time': 1960.2570941448212, 'accumulated_logging_time': 0.04469037055969238}
I0210 06:24:35.622046 140050897565440 logging_writer.py:48] [4747] accumulated_eval_time=1960.257094, accumulated_logging_time=0.044690, accumulated_submission_time=1710.960181, global_step=4747, preemption_count=0, score=1710.960181, test/accuracy=0.593434, test/bleu=22.196164, test/loss=2.123737, test/num_examples=3003, total_duration=3671.411407, train/accuracy=0.576660, train/bleu=26.829643, train/loss=2.244674, validation/accuracy=0.591214, validation/bleu=23.472349, validation/loss=2.138241, validation/num_examples=3000
I0210 06:24:54.653053 140050979010304 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.18502555787563324, loss=2.409349203109741
I0210 06:25:29.932477 140050897565440 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.18408970534801483, loss=2.283862590789795
I0210 06:26:05.331587 140050979010304 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.16584378480911255, loss=2.2596631050109863
I0210 06:26:40.713821 140050897565440 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.16547490656375885, loss=2.2485949993133545
I0210 06:27:16.123764 140050979010304 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.15216046571731567, loss=2.298581600189209
I0210 06:27:51.526226 140050897565440 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.1790660172700882, loss=2.240504026412964
I0210 06:28:26.941043 140050979010304 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1842203289270401, loss=2.2472074031829834
I0210 06:29:02.344931 140050897565440 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.16699787974357605, loss=2.2718417644500732
I0210 06:29:37.745572 140050979010304 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.17937298119068146, loss=2.217531681060791
I0210 06:30:13.154344 140050897565440 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.15680915117263794, loss=2.2091453075408936
I0210 06:30:48.544018 140050979010304 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.17354390025138855, loss=2.1903886795043945
I0210 06:31:23.945566 140050897565440 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.17857089638710022, loss=2.1723737716674805
I0210 06:31:59.325691 140050979010304 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.17097437381744385, loss=2.226034164428711
I0210 06:32:34.737889 140050897565440 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.17922443151474, loss=2.268446683883667
I0210 06:33:10.150364 140050979010304 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.16670538485050201, loss=2.2620368003845215
I0210 06:33:45.551313 140050897565440 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.16550041735172272, loss=2.250492811203003
I0210 06:34:20.924325 140050979010304 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.16303636133670807, loss=2.1735241413116455
I0210 06:34:56.317561 140050897565440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1590777039527893, loss=2.1539952754974365
I0210 06:35:31.702292 140050979010304 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.14954276382923126, loss=2.109644889831543
I0210 06:36:07.098113 140050897565440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.28742653131484985, loss=2.0747625827789307
I0210 06:36:42.495600 140050979010304 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.19393384456634521, loss=2.140183210372925
I0210 06:37:17.894244 140050897565440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.16542349755764008, loss=2.124624490737915
I0210 06:37:53.297474 140050979010304 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.18360789120197296, loss=2.2380733489990234
I0210 06:38:28.700212 140050897565440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1645137071609497, loss=2.0854482650756836
I0210 06:38:35.859778 140225696298816 spec.py:321] Evaluating on the training split.
I0210 06:38:38.825278 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:40:59.006724 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 06:41:01.690704 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:43:26.454306 140225696298816 spec.py:349] Evaluating on the test split.
I0210 06:43:29.139285 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 06:45:36.070135 140225696298816 submission_runner.py:408] Time since start: 4931.87s, 	Step: 7122, 	{'train/accuracy': 0.604808509349823, 'train/loss': 2.0004196166992188, 'train/bleu': 29.179246592431355, 'validation/accuracy': 0.6162725687026978, 'validation/loss': 1.923214077949524, 'validation/bleu': 25.082469012091778, 'validation/num_examples': 3000, 'test/accuracy': 0.6211028099060059, 'test/loss': 1.8899255990982056, 'test/bleu': 24.172361442868134, 'test/num_examples': 3003, 'score': 2551.1123156547546, 'total_duration': 4931.874216079712, 'accumulated_submission_time': 2551.1123156547546, 'accumulated_eval_time': 2380.467398405075, 'accumulated_logging_time': 0.07047128677368164}
I0210 06:45:36.085869 140050979010304 logging_writer.py:48] [7122] accumulated_eval_time=2380.467398, accumulated_logging_time=0.070471, accumulated_submission_time=2551.112316, global_step=7122, preemption_count=0, score=2551.112316, test/accuracy=0.621103, test/bleu=24.172361, test/loss=1.889926, test/num_examples=3003, total_duration=4931.874216, train/accuracy=0.604809, train/bleu=29.179247, train/loss=2.000420, validation/accuracy=0.616273, validation/bleu=25.082469, validation/loss=1.923214, validation/num_examples=3000
I0210 06:46:03.922792 140050897565440 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.16514836251735687, loss=2.0674431324005127
I0210 06:46:39.232881 140050979010304 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1579982191324234, loss=2.0267937183380127
I0210 06:47:14.619332 140050897565440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.15117619931697845, loss=2.124234199523926
I0210 06:47:50.013428 140050979010304 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.1511920839548111, loss=2.081022262573242
I0210 06:48:25.409986 140050897565440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.1621403843164444, loss=2.117785930633545
I0210 06:49:00.807353 140050979010304 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.18701399862766266, loss=2.1322710514068604
I0210 06:49:36.227610 140050897565440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1576986461877823, loss=1.9718186855316162
I0210 06:50:11.649905 140050979010304 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.2055814564228058, loss=2.129979133605957
I0210 06:50:47.062421 140050897565440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.2091657668352127, loss=2.1165823936462402
I0210 06:51:22.455220 140050979010304 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.16368024051189423, loss=2.116422653198242
I0210 06:51:57.853266 140050897565440 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1667974591255188, loss=2.0603699684143066
I0210 06:52:33.271115 140050979010304 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.2007211297750473, loss=2.050910234451294
I0210 06:53:08.653076 140050897565440 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.17205479741096497, loss=2.140533924102783
I0210 06:53:44.037602 140050979010304 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20518511533737183, loss=2.0969860553741455
I0210 06:54:19.438431 140050897565440 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.17382963001728058, loss=2.0092225074768066
I0210 06:54:54.846177 140050979010304 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.16309694945812225, loss=1.9876567125320435
I0210 06:55:30.256571 140050897565440 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.26148560643196106, loss=2.0875630378723145
I0210 06:56:05.672972 140050979010304 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.224883571267128, loss=2.0279593467712402
I0210 06:56:41.069218 140050897565440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15836121141910553, loss=2.1428163051605225
I0210 06:57:16.472957 140050979010304 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.1745864748954773, loss=2.036649703979492
I0210 06:57:51.873250 140050897565440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.18500655889511108, loss=2.1646814346313477
I0210 06:58:27.263207 140050979010304 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.18617789447307587, loss=2.0380403995513916
I0210 06:59:02.656627 140050897565440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.24311810731887817, loss=2.1116015911102295
I0210 06:59:36.336571 140225696298816 spec.py:321] Evaluating on the training split.
I0210 06:59:39.311545 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:02:42.245511 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 07:02:44.929969 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:05:12.770833 140225696298816 spec.py:349] Evaluating on the test split.
I0210 07:05:15.447859 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:07:28.157551 140225696298816 submission_runner.py:408] Time since start: 6243.96s, 	Step: 9497, 	{'train/accuracy': 0.6149968504905701, 'train/loss': 1.941268801689148, 'train/bleu': 29.185273045229128, 'validation/accuracy': 0.6318706274032593, 'validation/loss': 1.8118021488189697, 'validation/bleu': 26.343849434747742, 'validation/num_examples': 3000, 'test/accuracy': 0.6391029357910156, 'test/loss': 1.7555793523788452, 'test/bleu': 25.708341384430515, 'test/num_examples': 3003, 'score': 3391.2784502506256, 'total_duration': 6243.961631298065, 'accumulated_submission_time': 3391.2784502506256, 'accumulated_eval_time': 2852.2883291244507, 'accumulated_logging_time': 0.09632468223571777}
I0210 07:07:28.174766 140050979010304 logging_writer.py:48] [9497] accumulated_eval_time=2852.288329, accumulated_logging_time=0.096325, accumulated_submission_time=3391.278450, global_step=9497, preemption_count=0, score=3391.278450, test/accuracy=0.639103, test/bleu=25.708341, test/loss=1.755579, test/num_examples=3003, total_duration=6243.961631, train/accuracy=0.614997, train/bleu=29.185273, train/loss=1.941269, validation/accuracy=0.631871, validation/bleu=26.343849, validation/loss=1.811802, validation/num_examples=3000
I0210 07:07:29.610810 140050897565440 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17756617069244385, loss=2.0143024921417236
I0210 07:08:04.828027 140050979010304 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.19009509682655334, loss=2.027651309967041
I0210 07:08:40.124105 140050897565440 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.18527866899967194, loss=1.9689160585403442
I0210 07:09:15.500284 140050979010304 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.2058575302362442, loss=2.003040075302124
I0210 07:09:50.884744 140050897565440 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.22139659523963928, loss=1.9420900344848633
I0210 07:10:26.253838 140050979010304 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1930212378501892, loss=2.038456678390503
I0210 07:11:01.653871 140050897565440 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.18376760184764862, loss=1.966421365737915
I0210 07:11:37.063703 140050979010304 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.24505116045475006, loss=2.0380594730377197
I0210 07:12:12.638219 140050897565440 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.1596156358718872, loss=1.942507266998291
I0210 07:12:48.039356 140050979010304 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17708902060985565, loss=1.9657126665115356
I0210 07:13:23.424339 140050897565440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.17204642295837402, loss=2.0408506393432617
I0210 07:13:58.822518 140050979010304 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.168779656291008, loss=1.924220323562622
I0210 07:14:34.238306 140050897565440 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.19986897706985474, loss=1.9877654314041138
I0210 07:15:09.671946 140050979010304 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.19205841422080994, loss=2.028568744659424
I0210 07:15:45.063575 140050897565440 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2011720836162567, loss=2.0305466651916504
I0210 07:16:20.454999 140050979010304 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17895518243312836, loss=2.0736336708068848
I0210 07:16:55.842546 140050897565440 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.18811576068401337, loss=1.9286696910858154
I0210 07:17:31.245411 140050979010304 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2031327784061432, loss=2.0029489994049072
I0210 07:18:06.633299 140050897565440 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.19593414664268494, loss=1.9704174995422363
I0210 07:18:42.032727 140050979010304 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.17860355973243713, loss=1.9232596158981323
I0210 07:19:17.437380 140050897565440 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.18646281957626343, loss=2.044754981994629
I0210 07:19:52.853773 140050979010304 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2008591890335083, loss=2.0520193576812744
I0210 07:20:28.238714 140050897565440 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2933122217655182, loss=1.913032054901123
I0210 07:21:03.629847 140050979010304 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.21708469092845917, loss=2.0282235145568848
I0210 07:21:28.496175 140225696298816 spec.py:321] Evaluating on the training split.
I0210 07:21:31.473110 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:24:18.345444 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 07:24:21.037527 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:26:43.122417 140225696298816 spec.py:349] Evaluating on the test split.
I0210 07:26:45.804257 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:28:48.912503 140225696298816 submission_runner.py:408] Time since start: 7524.72s, 	Step: 11872, 	{'train/accuracy': 0.6193379759788513, 'train/loss': 1.8914982080459595, 'train/bleu': 29.845714220736607, 'validation/accuracy': 0.6386777758598328, 'validation/loss': 1.7426763772964478, 'validation/bleu': 26.896069609013942, 'validation/num_examples': 3000, 'test/accuracy': 0.6488640904426575, 'test/loss': 1.6925368309020996, 'test/bleu': 25.705053088847816, 'test/num_examples': 3003, 'score': 4231.514441490173, 'total_duration': 7524.716588258743, 'accumulated_submission_time': 4231.514441490173, 'accumulated_eval_time': 3292.704610824585, 'accumulated_logging_time': 0.12389349937438965}
I0210 07:28:48.929749 140050897565440 logging_writer.py:48] [11872] accumulated_eval_time=3292.704611, accumulated_logging_time=0.123893, accumulated_submission_time=4231.514441, global_step=11872, preemption_count=0, score=4231.514441, test/accuracy=0.648864, test/bleu=25.705053, test/loss=1.692537, test/num_examples=3003, total_duration=7524.716588, train/accuracy=0.619338, train/bleu=29.845714, train/loss=1.891498, validation/accuracy=0.638678, validation/bleu=26.896070, validation/loss=1.742676, validation/num_examples=3000
I0210 07:28:59.147494 140050979010304 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2391180396080017, loss=1.9495941400527954
I0210 07:29:34.405868 140050897565440 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.19490639865398407, loss=2.0380773544311523
I0210 07:30:09.775842 140050979010304 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.27447575330734253, loss=1.9193214178085327
I0210 07:30:45.163836 140050897565440 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18060258030891418, loss=1.9819467067718506
I0210 07:31:20.581505 140050979010304 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2584120035171509, loss=2.023637294769287
I0210 07:31:55.974750 140050897565440 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.18338648974895477, loss=1.9436365365982056
I0210 07:32:31.367189 140050979010304 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2516965866088867, loss=1.963064193725586
I0210 07:33:06.758802 140050897565440 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.19437691569328308, loss=1.8925228118896484
I0210 07:33:42.150698 140050979010304 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1970885545015335, loss=1.8760099411010742
I0210 07:34:17.533692 140050897565440 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.20079949498176575, loss=1.9733856916427612
I0210 07:34:52.905054 140050979010304 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.18276293575763702, loss=1.9399231672286987
I0210 07:35:28.336849 140050897565440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.23044349253177643, loss=1.9152753353118896
I0210 07:36:03.759023 140050979010304 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.1786026805639267, loss=1.959142804145813
I0210 07:36:39.188376 140050897565440 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.25555288791656494, loss=1.9587976932525635
I0210 07:37:14.618138 140050979010304 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.20897650718688965, loss=1.9604841470718384
I0210 07:37:50.042677 140050897565440 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.17950817942619324, loss=1.9923800230026245
I0210 07:38:25.437536 140050979010304 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2578282058238983, loss=1.904906153678894
I0210 07:39:00.836864 140050897565440 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.8883460760116577, loss=2.1780619621276855
I0210 07:39:36.239922 140050979010304 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.20074905455112457, loss=1.9301403760910034
I0210 07:40:11.654045 140050897565440 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2025933861732483, loss=1.9582465887069702
I0210 07:40:47.035912 140050979010304 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2497575730085373, loss=2.0239925384521484
I0210 07:41:22.435137 140050897565440 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.22745470702648163, loss=1.8591265678405762
I0210 07:41:57.824969 140050979010304 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.17408043146133423, loss=1.9222183227539062
I0210 07:42:33.240740 140050897565440 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.20513886213302612, loss=1.887556791305542
I0210 07:42:49.246479 140225696298816 spec.py:321] Evaluating on the training split.
I0210 07:42:52.213677 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:45:58.964156 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 07:46:01.662216 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:48:34.799930 140225696298816 spec.py:349] Evaluating on the test split.
I0210 07:48:37.461730 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 07:51:01.521220 140225696298816 submission_runner.py:408] Time since start: 8857.33s, 	Step: 14247, 	{'train/accuracy': 0.6294394135475159, 'train/loss': 1.81944739818573, 'train/bleu': 30.450921466562807, 'validation/accuracy': 0.6437985897064209, 'validation/loss': 1.7072787284851074, 'validation/bleu': 27.1843550621261, 'validation/num_examples': 3000, 'test/accuracy': 0.6537911891937256, 'test/loss': 1.640441656112671, 'test/bleu': 26.303994280825968, 'test/num_examples': 3003, 'score': 5071.745926856995, 'total_duration': 8857.325303792953, 'accumulated_submission_time': 5071.745926856995, 'accumulated_eval_time': 3784.9792997837067, 'accumulated_logging_time': 0.1510317325592041}
I0210 07:51:01.537671 140050979010304 logging_writer.py:48] [14247] accumulated_eval_time=3784.979300, accumulated_logging_time=0.151032, accumulated_submission_time=5071.745927, global_step=14247, preemption_count=0, score=5071.745927, test/accuracy=0.653791, test/bleu=26.303994, test/loss=1.640442, test/num_examples=3003, total_duration=8857.325304, train/accuracy=0.629439, train/bleu=30.450921, train/loss=1.819447, validation/accuracy=0.643799, validation/bleu=27.184355, validation/loss=1.707279, validation/num_examples=3000
I0210 07:51:20.582026 140050897565440 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.16481468081474304, loss=1.9086802005767822
I0210 07:51:55.849565 140050979010304 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17857587337493896, loss=1.9553166627883911
I0210 07:52:31.233825 140050897565440 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.21263568103313446, loss=1.9320363998413086
I0210 07:53:06.630558 140050979010304 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.23951365053653717, loss=2.0143086910247803
I0210 07:53:42.043831 140050897565440 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.19371497631072998, loss=1.9014686346054077
I0210 07:54:17.447978 140050979010304 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.20846286416053772, loss=1.937280535697937
I0210 07:54:52.876043 140050897565440 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.2741662263870239, loss=1.8823726177215576
I0210 07:55:28.288593 140050979010304 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.3267882764339447, loss=1.821149230003357
I0210 07:56:03.694468 140050897565440 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.2082158625125885, loss=1.9749562740325928
I0210 07:56:39.091870 140050979010304 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.18522897362709045, loss=1.9382139444351196
I0210 07:57:14.507523 140050897565440 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.19015567004680634, loss=1.9248098134994507
I0210 07:57:49.901022 140050979010304 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.1835931539535522, loss=1.8450586795806885
I0210 07:58:25.295225 140050897565440 logging_writer.py:48] [15500] global_step=15500, grad_norm=5.871780872344971, loss=1.9443236589431763
I0210 07:59:00.690601 140050979010304 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.21096347272396088, loss=1.919853925704956
I0210 07:59:36.095484 140050897565440 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.20247095823287964, loss=1.8939878940582275
I0210 08:00:11.497539 140050979010304 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.1816161572933197, loss=1.8257328271865845
I0210 08:00:46.898628 140050897565440 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.2027614712715149, loss=1.9310662746429443
I0210 08:01:22.327535 140050979010304 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.35384833812713623, loss=1.8993626832962036
I0210 08:01:57.718724 140050897565440 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2201761156320572, loss=1.9908852577209473
I0210 08:02:33.113765 140050979010304 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3126063942909241, loss=1.8781425952911377
I0210 08:03:08.512094 140050897565440 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.19114083051681519, loss=1.864155650138855
I0210 08:03:43.941475 140050979010304 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2624327838420868, loss=1.8126519918441772
I0210 08:04:19.352561 140050897565440 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.20759552717208862, loss=1.876197099685669
I0210 08:04:54.757079 140050979010304 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.22499848902225494, loss=1.8828580379486084
I0210 08:05:01.567215 140225696298816 spec.py:321] Evaluating on the training split.
I0210 08:05:04.534781 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:09:19.403108 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 08:09:22.079467 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:13:58.461859 140225696298816 spec.py:349] Evaluating on the test split.
I0210 08:14:01.136638 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:18:27.209497 140225696298816 submission_runner.py:408] Time since start: 10503.01s, 	Step: 16621, 	{'train/accuracy': 0.6266187429428101, 'train/loss': 1.8285259008407593, 'train/bleu': 30.356420384125588, 'validation/accuracy': 0.6463403701782227, 'validation/loss': 1.6791563034057617, 'validation/bleu': 27.10367908047491, 'validation/num_examples': 3000, 'test/accuracy': 0.6579513549804688, 'test/loss': 1.6053942441940308, 'test/bleu': 26.439084023361506, 'test/num_examples': 3003, 'score': 5911.688598394394, 'total_duration': 10503.013573646545, 'accumulated_submission_time': 5911.688598394394, 'accumulated_eval_time': 4590.6215217113495, 'accumulated_logging_time': 0.17937827110290527}
I0210 08:18:27.228130 140050897565440 logging_writer.py:48] [16621] accumulated_eval_time=4590.621522, accumulated_logging_time=0.179378, accumulated_submission_time=5911.688598, global_step=16621, preemption_count=0, score=5911.688598, test/accuracy=0.657951, test/bleu=26.439084, test/loss=1.605394, test/num_examples=3003, total_duration=10503.013574, train/accuracy=0.626619, train/bleu=30.356420, train/loss=1.828526, validation/accuracy=0.646340, validation/bleu=27.103679, validation/loss=1.679156, validation/num_examples=3000
I0210 08:18:55.393831 140050979010304 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.214565709233284, loss=1.8730487823486328
I0210 08:19:30.712095 140050897565440 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.25496891140937805, loss=1.9073212146759033
I0210 08:20:06.078282 140050979010304 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.23451487720012665, loss=1.8846381902694702
I0210 08:20:41.479618 140050897565440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.19476650655269623, loss=1.8885746002197266
I0210 08:21:16.853819 140050979010304 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.1951412409543991, loss=1.9219650030136108
I0210 08:21:52.229820 140050897565440 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.21556665003299713, loss=1.8477158546447754
I0210 08:22:27.642254 140050979010304 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2143307775259018, loss=1.922000765800476
I0210 08:23:03.043936 140050897565440 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2582595646381378, loss=1.8737860918045044
I0210 08:23:38.442491 140050979010304 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.23115530610084534, loss=1.894237756729126
I0210 08:24:13.816264 140050897565440 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.25373682379722595, loss=1.9369382858276367
I0210 08:24:49.212725 140050979010304 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.21936525404453278, loss=1.8724592924118042
I0210 08:25:24.613283 140050897565440 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.20866891741752625, loss=1.8546133041381836
I0210 08:26:00.034967 140050979010304 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.1869853287935257, loss=1.901208519935608
I0210 08:26:35.459039 140050897565440 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.20681747794151306, loss=1.8811739683151245
I0210 08:27:10.871114 140050979010304 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.2971814274787903, loss=1.9233394861221313
I0210 08:27:46.238805 140050897565440 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.20021472871303558, loss=1.880842924118042
I0210 08:28:21.672468 140050979010304 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.22068016231060028, loss=1.9373042583465576
I0210 08:28:57.052260 140050897565440 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.21142563223838806, loss=1.9145814180374146
I0210 08:29:32.410565 140050979010304 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.21847529709339142, loss=1.916515827178955
I0210 08:30:07.810520 140050897565440 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.22382313013076782, loss=1.8525123596191406
I0210 08:30:43.189540 140050979010304 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.20495939254760742, loss=1.8466718196868896
I0210 08:31:18.579874 140050897565440 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.2593689262866974, loss=1.9622890949249268
I0210 08:31:54.001448 140050979010304 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.19860894978046417, loss=1.7725131511688232
I0210 08:32:27.341128 140225696298816 spec.py:321] Evaluating on the training split.
I0210 08:32:30.314935 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:37:01.848968 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 08:37:04.521129 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:41:27.168908 140225696298816 spec.py:349] Evaluating on the test split.
I0210 08:41:29.851306 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 08:45:42.103279 140225696298816 submission_runner.py:408] Time since start: 12137.91s, 	Step: 18996, 	{'train/accuracy': 0.6516609787940979, 'train/loss': 1.649769902229309, 'train/bleu': 31.447995353884966, 'validation/accuracy': 0.6500477194786072, 'validation/loss': 1.6590960025787354, 'validation/bleu': 26.475614046524264, 'validation/num_examples': 3000, 'test/accuracy': 0.6589855551719666, 'test/loss': 1.592655897140503, 'test/bleu': 26.80562274957389, 'test/num_examples': 3003, 'score': 6751.716594457626, 'total_duration': 12137.907361030579, 'accumulated_submission_time': 6751.716594457626, 'accumulated_eval_time': 5385.383625268936, 'accumulated_logging_time': 0.2077019214630127}
I0210 08:45:42.120785 140050897565440 logging_writer.py:48] [18996] accumulated_eval_time=5385.383625, accumulated_logging_time=0.207702, accumulated_submission_time=6751.716594, global_step=18996, preemption_count=0, score=6751.716594, test/accuracy=0.658986, test/bleu=26.805623, test/loss=1.592656, test/num_examples=3003, total_duration=12137.907361, train/accuracy=0.651661, train/bleu=31.447995, train/loss=1.649770, validation/accuracy=0.650048, validation/bleu=26.475614, validation/loss=1.659096, validation/num_examples=3000
I0210 08:45:43.904316 140050979010304 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.22426679730415344, loss=1.8461517095565796
I0210 08:46:19.142102 140050897565440 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.29742851853370667, loss=1.9011890888214111
I0210 08:46:54.514155 140050979010304 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.19443874061107635, loss=1.8630104064941406
I0210 08:47:29.924387 140050897565440 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.21442817151546478, loss=1.8456029891967773
I0210 08:48:05.293523 140050979010304 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.20407162606716156, loss=1.8940125703811646
I0210 08:48:40.702587 140050897565440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.214692160487175, loss=1.8625402450561523
I0210 08:49:16.111428 140050979010304 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.22793401777744293, loss=1.8352856636047363
I0210 08:49:51.527053 140050897565440 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2835388481616974, loss=1.8976715803146362
I0210 08:50:26.897000 140050979010304 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.1981631964445114, loss=1.9722832441329956
I0210 08:51:02.273620 140050897565440 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.1877312809228897, loss=1.8844305276870728
I0210 08:51:37.675929 140050979010304 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.22946804761886597, loss=1.8882578611373901
I0210 08:52:13.107081 140050897565440 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.2653220593929291, loss=1.8294047117233276
I0210 08:52:48.533237 140050979010304 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.2498246431350708, loss=2.172715902328491
I0210 08:53:23.944800 140050897565440 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.17536132037639618, loss=1.8794485330581665
I0210 08:53:59.335111 140050979010304 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.18859153985977173, loss=1.9222277402877808
I0210 08:54:34.747435 140050897565440 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.21469688415527344, loss=1.8686922788619995
I0210 08:55:10.179340 140050979010304 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.23351715505123138, loss=1.904789686203003
I0210 08:55:45.558589 140050897565440 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1803876757621765, loss=1.860484004020691
I0210 08:56:20.955217 140050979010304 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.2781100571155548, loss=1.8395953178405762
I0210 08:56:56.377759 140050897565440 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.188780277967453, loss=1.7803820371627808
I0210 08:57:31.792518 140050979010304 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.21924936771392822, loss=1.8092949390411377
I0210 08:58:07.196207 140050897565440 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.21100302040576935, loss=1.818063497543335
I0210 08:58:42.632698 140050979010304 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.28747978806495667, loss=1.8523215055465698
I0210 08:59:18.038770 140050897565440 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.22288529574871063, loss=1.8589040040969849
I0210 08:59:42.191658 140225696298816 spec.py:321] Evaluating on the training split.
I0210 08:59:45.155439 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:03:44.381741 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 09:03:47.062053 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:07:30.127801 140225696298816 spec.py:349] Evaluating on the test split.
I0210 09:07:32.802419 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:11:02.076022 140225696298816 submission_runner.py:408] Time since start: 13657.88s, 	Step: 21370, 	{'train/accuracy': 0.6328192353248596, 'train/loss': 1.773112177848816, 'train/bleu': 30.559165503155825, 'validation/accuracy': 0.6511512398719788, 'validation/loss': 1.6486619710922241, 'validation/bleu': 27.135931286020796, 'validation/num_examples': 3000, 'test/accuracy': 0.6615420579910278, 'test/loss': 1.5799243450164795, 'test/bleu': 26.81812092467978, 'test/num_examples': 3003, 'score': 7591.700333595276, 'total_duration': 13657.880095720291, 'accumulated_submission_time': 7591.700333595276, 'accumulated_eval_time': 6065.267931461334, 'accumulated_logging_time': 0.23696160316467285}
I0210 09:11:02.095795 140050979010304 logging_writer.py:48] [21370] accumulated_eval_time=6065.267931, accumulated_logging_time=0.236962, accumulated_submission_time=7591.700334, global_step=21370, preemption_count=0, score=7591.700334, test/accuracy=0.661542, test/bleu=26.818121, test/loss=1.579924, test/num_examples=3003, total_duration=13657.880096, train/accuracy=0.632819, train/bleu=30.559166, train/loss=1.773112, validation/accuracy=0.651151, validation/bleu=27.135931, validation/loss=1.648662, validation/num_examples=3000
I0210 09:11:13.019889 140050897565440 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.2274084836244583, loss=1.8702017068862915
I0210 09:11:48.285424 140050979010304 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.20291557908058167, loss=1.8722634315490723
I0210 09:12:23.629594 140050897565440 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.1983782947063446, loss=1.8394719362258911
I0210 09:12:59.044981 140050979010304 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.20055292546749115, loss=1.7966943979263306
I0210 09:13:34.425808 140050897565440 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1962137371301651, loss=1.8517035245895386
I0210 09:14:09.818588 140050979010304 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.20475919544696808, loss=1.9307796955108643
I0210 09:14:45.227390 140050897565440 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.20218396186828613, loss=1.9009573459625244
I0210 09:15:20.645228 140050979010304 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.25589579343795776, loss=1.8973314762115479
I0210 09:15:56.069566 140050897565440 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.28072643280029297, loss=1.9432792663574219
I0210 09:16:31.487964 140050979010304 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.23903600871562958, loss=1.759055733680725
I0210 09:17:06.879362 140050897565440 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.22390645742416382, loss=1.9041106700897217
I0210 09:17:42.269043 140050979010304 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.23986104130744934, loss=1.8933607339859009
I0210 09:18:17.693762 140050897565440 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.20807740092277527, loss=1.809863567352295
I0210 09:18:53.101233 140050979010304 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1885664314031601, loss=1.8858619928359985
I0210 09:19:28.517416 140050897565440 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.18674929440021515, loss=1.828279972076416
I0210 09:20:03.901138 140050979010304 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.19472266733646393, loss=1.849267601966858
I0210 09:20:39.330468 140050897565440 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.17982400953769684, loss=1.9392788410186768
I0210 09:21:14.749205 140050979010304 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.20266947150230408, loss=1.8720513582229614
I0210 09:21:50.141467 140050897565440 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.20676450431346893, loss=1.8187780380249023
I0210 09:22:25.549611 140050979010304 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.30888456106185913, loss=1.845589280128479
I0210 09:23:00.970772 140050897565440 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.18481341004371643, loss=1.8494728803634644
I0210 09:23:36.375943 140050979010304 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.27542755007743835, loss=1.8417394161224365
I0210 09:24:11.805623 140050897565440 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.2048138976097107, loss=1.8353662490844727
I0210 09:24:47.210027 140050979010304 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.21295273303985596, loss=1.7887091636657715
I0210 09:25:02.153555 140225696298816 spec.py:321] Evaluating on the training split.
I0210 09:25:05.133349 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:29:17.082506 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 09:29:19.743690 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:31:53.135957 140225696298816 spec.py:349] Evaluating on the test split.
I0210 09:31:55.823579 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:34:11.886228 140225696298816 submission_runner.py:408] Time since start: 15047.69s, 	Step: 23744, 	{'train/accuracy': 0.6332682967185974, 'train/loss': 1.783609390258789, 'train/bleu': 31.001136692097017, 'validation/accuracy': 0.6562844514846802, 'validation/loss': 1.6264350414276123, 'validation/bleu': 27.966466507153374, 'validation/num_examples': 3000, 'test/accuracy': 0.6650747060775757, 'test/loss': 1.5590084791183472, 'test/bleu': 27.07595711213196, 'test/num_examples': 3003, 'score': 8431.671695709229, 'total_duration': 15047.690311908722, 'accumulated_submission_time': 8431.671695709229, 'accumulated_eval_time': 6615.000554323196, 'accumulated_logging_time': 0.2675762176513672}
I0210 09:34:11.904378 140050897565440 logging_writer.py:48] [23744] accumulated_eval_time=6615.000554, accumulated_logging_time=0.267576, accumulated_submission_time=8431.671696, global_step=23744, preemption_count=0, score=8431.671696, test/accuracy=0.665075, test/bleu=27.075957, test/loss=1.559008, test/num_examples=3003, total_duration=15047.690312, train/accuracy=0.633268, train/bleu=31.001137, train/loss=1.783609, validation/accuracy=0.656284, validation/bleu=27.966467, validation/loss=1.626435, validation/num_examples=3000
I0210 09:34:31.983386 140050979010304 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.21968232095241547, loss=1.8655238151550293
I0210 09:35:07.241164 140050897565440 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.20509208738803864, loss=1.9396482706069946
I0210 09:35:42.600972 140050979010304 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.19819432497024536, loss=1.801560878753662
I0210 09:36:17.994060 140050897565440 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.2066526859998703, loss=1.8797309398651123
I0210 09:36:53.381279 140050979010304 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.24047426879405975, loss=1.8777021169662476
I0210 09:37:28.787629 140050897565440 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.20430727303028107, loss=1.873710036277771
I0210 09:38:04.174504 140050979010304 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.25596746802330017, loss=1.8597445487976074
I0210 09:38:39.570977 140050897565440 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.23653315007686615, loss=1.8508156538009644
I0210 09:39:14.980549 140050979010304 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.19060993194580078, loss=1.8646435737609863
I0210 09:39:50.389996 140050897565440 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.20976603031158447, loss=1.8448894023895264
I0210 09:40:25.777734 140050979010304 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.21218031644821167, loss=1.8629777431488037
I0210 09:41:01.182248 140050897565440 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.22125430405139923, loss=1.8153103590011597
I0210 09:41:36.604395 140050979010304 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.22208794951438904, loss=1.7943750619888306
I0210 09:42:12.010707 140050897565440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.22621646523475647, loss=1.822794795036316
I0210 09:42:47.427657 140050979010304 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.24241581559181213, loss=1.8445005416870117
I0210 09:43:22.838279 140050897565440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.22095981240272522, loss=1.930903673171997
I0210 09:43:58.245995 140050979010304 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2102709263563156, loss=1.8197247982025146
I0210 09:44:33.668800 140050897565440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.20898514986038208, loss=1.7389332056045532
I0210 09:45:09.086916 140050979010304 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2049739509820938, loss=1.7741549015045166
I0210 09:45:44.493326 140050897565440 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.21076993644237518, loss=1.8712353706359863
I0210 09:46:19.897722 140050979010304 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.218601793050766, loss=1.9756242036819458
I0210 09:46:55.317879 140050897565440 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.23321788012981415, loss=1.7525246143341064
I0210 09:47:30.726173 140050979010304 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.18924155831336975, loss=1.7532269954681396
I0210 09:48:06.125444 140050897565440 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.2078578621149063, loss=1.7809582948684692
I0210 09:48:12.220685 140225696298816 spec.py:321] Evaluating on the training split.
I0210 09:48:15.198813 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:50:49.385823 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 09:50:52.063423 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:53:17.466762 140225696298816 spec.py:349] Evaluating on the test split.
I0210 09:53:20.142493 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 09:55:41.000854 140225696298816 submission_runner.py:408] Time since start: 16336.80s, 	Step: 26119, 	{'train/accuracy': 0.6439311504364014, 'train/loss': 1.6942683458328247, 'train/bleu': 31.456285832045214, 'validation/accuracy': 0.6556025147438049, 'validation/loss': 1.624097228050232, 'validation/bleu': 27.50174142438485, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5462366342544556, 'test/bleu': 27.10684203663389, 'test/num_examples': 3003, 'score': 9271.90131354332, 'total_duration': 16336.804920434952, 'accumulated_submission_time': 9271.90131354332, 'accumulated_eval_time': 7063.780653953552, 'accumulated_logging_time': 0.2965688705444336}
I0210 09:55:41.019855 140050979010304 logging_writer.py:48] [26119] accumulated_eval_time=7063.780654, accumulated_logging_time=0.296569, accumulated_submission_time=9271.901314, global_step=26119, preemption_count=0, score=9271.901314, test/accuracy=0.667097, test/bleu=27.106842, test/loss=1.546237, test/num_examples=3003, total_duration=16336.804920, train/accuracy=0.643931, train/bleu=31.456286, train/loss=1.694268, validation/accuracy=0.655603, validation/bleu=27.501741, validation/loss=1.624097, validation/num_examples=3000
I0210 09:56:09.927702 140050897565440 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.17998385429382324, loss=1.869185209274292
I0210 09:56:45.293612 140050979010304 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.20336724817752838, loss=1.807016372680664
I0210 09:57:20.692973 140050897565440 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.19203922152519226, loss=1.8689227104187012
I0210 09:57:56.091069 140050979010304 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.21692325174808502, loss=1.831635594367981
I0210 09:58:31.482367 140050897565440 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.20756101608276367, loss=1.8883497714996338
I0210 09:59:06.912160 140050979010304 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.2083463966846466, loss=1.8306009769439697
I0210 09:59:42.300523 140050897565440 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.2199077308177948, loss=1.84919273853302
I0210 10:00:17.715415 140050979010304 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.21918486058712006, loss=1.897605538368225
I0210 10:00:53.128494 140050897565440 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2280602902173996, loss=1.7569745779037476
I0210 10:01:28.553166 140050979010304 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8347702622413635, loss=1.7868293523788452
I0210 10:02:03.977942 140050897565440 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.19053177535533905, loss=1.7834230661392212
I0210 10:02:39.405812 140050979010304 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2940811812877655, loss=1.8812984228134155
I0210 10:03:14.834612 140050897565440 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.24667850136756897, loss=1.7363557815551758
I0210 10:03:50.213014 140050979010304 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.21191814541816711, loss=1.8658941984176636
I0210 10:04:25.645314 140050897565440 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.18150682747364044, loss=1.8320627212524414
I0210 10:05:01.051882 140050979010304 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.21109451353549957, loss=1.8083168268203735
I0210 10:05:36.465996 140050897565440 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2185208946466446, loss=1.8497480154037476
I0210 10:06:11.892117 140050979010304 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.19956038892269135, loss=1.8102502822875977
I0210 10:06:47.326063 140050897565440 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.23654137551784515, loss=1.8415653705596924
I0210 10:07:22.735640 140050979010304 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.22297008335590363, loss=1.817155361175537
I0210 10:07:58.129917 140050897565440 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2570984363555908, loss=1.8222404718399048
I0210 10:08:33.548516 140050979010304 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.21632882952690125, loss=1.7972477674484253
I0210 10:09:08.976089 140050897565440 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.248516321182251, loss=1.9057356119155884
I0210 10:09:41.273336 140225696298816 spec.py:321] Evaluating on the training split.
I0210 10:09:44.243597 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:13:21.770041 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 10:13:24.442218 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:16:43.606381 140225696298816 spec.py:349] Evaluating on the test split.
I0210 10:16:46.278505 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:20:01.963863 140225696298816 submission_runner.py:408] Time since start: 17797.77s, 	Step: 28493, 	{'train/accuracy': 0.6390884518623352, 'train/loss': 1.7399277687072754, 'train/bleu': 31.07782237514248, 'validation/accuracy': 0.6567432284355164, 'validation/loss': 1.6084965467453003, 'validation/bleu': 27.803699121674846, 'validation/num_examples': 3000, 'test/accuracy': 0.6716286540031433, 'test/loss': 1.5281535387039185, 'test/bleu': 27.86535354891519, 'test/num_examples': 3003, 'score': 10112.068130731583, 'total_duration': 17797.767949342728, 'accumulated_submission_time': 10112.068130731583, 'accumulated_eval_time': 7684.471136808395, 'accumulated_logging_time': 0.3272538185119629}
I0210 10:20:01.985398 140050979010304 logging_writer.py:48] [28493] accumulated_eval_time=7684.471137, accumulated_logging_time=0.327254, accumulated_submission_time=10112.068131, global_step=28493, preemption_count=0, score=10112.068131, test/accuracy=0.671629, test/bleu=27.865354, test/loss=1.528154, test/num_examples=3003, total_duration=17797.767949, train/accuracy=0.639088, train/bleu=31.077822, train/loss=1.739928, validation/accuracy=0.656743, validation/bleu=27.803699, validation/loss=1.608497, validation/num_examples=3000
I0210 10:20:04.826941 140050897565440 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.18364374339580536, loss=1.8672329187393188
I0210 10:20:40.095816 140050979010304 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.1914711445569992, loss=1.8428568840026855
I0210 10:21:15.451052 140050897565440 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.20789778232574463, loss=1.856613278388977
I0210 10:21:50.828994 140050979010304 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.19931870698928833, loss=1.8490287065505981
I0210 10:22:26.225833 140050897565440 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.1933911293745041, loss=1.837790608406067
I0210 10:23:01.608083 140050979010304 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2269955277442932, loss=1.974867343902588
I0210 10:23:37.003037 140050897565440 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.2501487135887146, loss=1.7978160381317139
I0210 10:24:12.398717 140050979010304 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.23953132331371307, loss=1.798379898071289
I0210 10:24:47.812629 140050897565440 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.20952093601226807, loss=1.8257999420166016
I0210 10:25:23.225228 140050979010304 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.21437884867191315, loss=1.8252063989639282
I0210 10:25:58.625969 140050897565440 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.19239243865013123, loss=1.7785464525222778
I0210 10:26:34.042498 140050979010304 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.19404304027557373, loss=1.7866570949554443
I0210 10:27:09.459109 140050897565440 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.21001073718070984, loss=1.8628382682800293
I0210 10:27:44.874355 140050979010304 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3099406957626343, loss=1.8259738683700562
I0210 10:28:20.260966 140050897565440 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.21155263483524323, loss=1.7731550931930542
I0210 10:28:55.661372 140050979010304 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.20704878866672516, loss=1.7873209714889526
I0210 10:29:31.061188 140050897565440 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19316206872463226, loss=1.7426100969314575
I0210 10:30:06.485230 140050979010304 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.24835823476314545, loss=1.884019374847412
I0210 10:30:41.898970 140050897565440 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.283387690782547, loss=1.7799099683761597
I0210 10:31:17.305390 140050979010304 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.1979789435863495, loss=1.765687108039856
I0210 10:31:52.731941 140050897565440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.22778891026973724, loss=1.8446950912475586
I0210 10:32:28.163992 140050979010304 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2348184585571289, loss=1.8116905689239502
I0210 10:33:03.580856 140050897565440 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.22164294123649597, loss=1.7776217460632324
I0210 10:33:39.011258 140050979010304 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.24081988632678986, loss=1.823314905166626
I0210 10:34:02.100827 140225696298816 spec.py:321] Evaluating on the training split.
I0210 10:34:05.072680 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:37:11.783941 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 10:37:14.447222 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:39:47.135552 140225696298816 spec.py:349] Evaluating on the test split.
I0210 10:39:49.813838 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:42:07.357376 140225696298816 submission_runner.py:408] Time since start: 19123.16s, 	Step: 30867, 	{'train/accuracy': 0.6361994743347168, 'train/loss': 1.7502000331878662, 'train/bleu': 30.96582954608702, 'validation/accuracy': 0.6586031317710876, 'validation/loss': 1.5999562740325928, 'validation/bleu': 27.939569403470813, 'validation/num_examples': 3000, 'test/accuracy': 0.6713497638702393, 'test/loss': 1.5222526788711548, 'test/bleu': 27.451590240782444, 'test/num_examples': 3003, 'score': 10952.097707033157, 'total_duration': 19123.161445617676, 'accumulated_submission_time': 10952.097707033157, 'accumulated_eval_time': 8169.727640390396, 'accumulated_logging_time': 0.3598606586456299}
I0210 10:42:07.376932 140050897565440 logging_writer.py:48] [30867] accumulated_eval_time=8169.727640, accumulated_logging_time=0.359861, accumulated_submission_time=10952.097707, global_step=30867, preemption_count=0, score=10952.097707, test/accuracy=0.671350, test/bleu=27.451590, test/loss=1.522253, test/num_examples=3003, total_duration=19123.161446, train/accuracy=0.636199, train/bleu=30.965830, train/loss=1.750200, validation/accuracy=0.658603, validation/bleu=27.939569, validation/loss=1.599956, validation/num_examples=3000
I0210 10:42:19.375808 140050979010304 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.19905246794223785, loss=1.681528091430664
I0210 10:42:54.635419 140050897565440 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5853167176246643, loss=1.7845646142959595
I0210 10:43:30.001286 140050979010304 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.22377009689807892, loss=1.7655495405197144
I0210 10:44:05.403036 140050897565440 logging_writer.py:48] [31200] global_step=31200, grad_norm=5.365339279174805, loss=1.7542427778244019
I0210 10:44:40.795170 140050979010304 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3136735260486603, loss=1.8239495754241943
I0210 10:45:16.174922 140050897565440 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20051415264606476, loss=1.720569133758545
I0210 10:45:51.594764 140050979010304 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.29312559962272644, loss=1.7832441329956055
I0210 10:46:26.986737 140050897565440 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.2159726917743683, loss=1.8367620706558228
I0210 10:47:02.386633 140050979010304 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19919230043888092, loss=1.7970607280731201
I0210 10:47:37.796031 140050897565440 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.21247372031211853, loss=1.8369102478027344
I0210 10:48:13.193407 140050979010304 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.21576620638370514, loss=1.8331297636032104
I0210 10:48:48.581067 140050897565440 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2027128040790558, loss=1.810041069984436
I0210 10:49:23.975036 140050979010304 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.24223552644252777, loss=1.872715711593628
I0210 10:49:59.397474 140050897565440 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.18562541902065277, loss=1.770781397819519
I0210 10:50:34.805345 140050979010304 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.19631212949752808, loss=1.7279889583587646
I0210 10:51:10.231643 140050897565440 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.21995286643505096, loss=1.7799930572509766
I0210 10:51:45.623713 140050979010304 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.20691804587841034, loss=1.7568137645721436
I0210 10:52:21.035443 140050897565440 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.2626495659351349, loss=1.7204819917678833
I0210 10:52:56.467144 140050979010304 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.21106620132923126, loss=1.8491891622543335
I0210 10:53:31.863802 140050897565440 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2107468843460083, loss=1.8490958213806152
I0210 10:54:07.253206 140050979010304 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.18998399376869202, loss=1.838014006614685
I0210 10:54:42.650249 140050897565440 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.20185929536819458, loss=1.8027243614196777
I0210 10:55:18.063766 140050979010304 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.24408864974975586, loss=1.80672025680542
I0210 10:55:53.449521 140050897565440 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19071520864963531, loss=1.8272264003753662
I0210 10:56:07.677112 140225696298816 spec.py:321] Evaluating on the training split.
I0210 10:56:10.646579 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 10:59:42.462641 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 10:59:45.131106 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:02:18.559760 140225696298816 spec.py:349] Evaluating on the test split.
I0210 11:02:21.235352 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:04:40.344874 140225696298816 submission_runner.py:408] Time since start: 20476.15s, 	Step: 33242, 	{'train/accuracy': 0.6436529755592346, 'train/loss': 1.703749179840088, 'train/bleu': 31.702394127417445, 'validation/accuracy': 0.6596198081970215, 'validation/loss': 1.5879870653152466, 'validation/bleu': 28.288413599126006, 'validation/num_examples': 3000, 'test/accuracy': 0.671884298324585, 'test/loss': 1.5101147890090942, 'test/bleu': 27.42189268731702, 'test/num_examples': 3003, 'score': 11792.311067819595, 'total_duration': 20476.148945569992, 'accumulated_submission_time': 11792.311067819595, 'accumulated_eval_time': 8682.395342111588, 'accumulated_logging_time': 0.39069294929504395}
I0210 11:04:40.364506 140050979010304 logging_writer.py:48] [33242] accumulated_eval_time=8682.395342, accumulated_logging_time=0.390693, accumulated_submission_time=11792.311068, global_step=33242, preemption_count=0, score=11792.311068, test/accuracy=0.671884, test/bleu=27.421893, test/loss=1.510115, test/num_examples=3003, total_duration=20476.148946, train/accuracy=0.643653, train/bleu=31.702394, train/loss=1.703749, validation/accuracy=0.659620, validation/bleu=28.288414, validation/loss=1.587987, validation/num_examples=3000
I0210 11:05:01.191388 140050897565440 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.20863103866577148, loss=1.8046997785568237
I0210 11:05:36.491587 140050979010304 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2093692272901535, loss=1.8764855861663818
I0210 11:06:11.894466 140050897565440 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.1917763203382492, loss=1.8264068365097046
I0210 11:06:47.302054 140050979010304 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2293839454650879, loss=1.789391279220581
I0210 11:07:22.718695 140050897565440 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.20177634060382843, loss=1.790684461593628
I0210 11:07:58.126691 140050979010304 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2214844524860382, loss=1.8213160037994385
I0210 11:08:33.555601 140050897565440 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.19492845237255096, loss=1.7219716310501099
I0210 11:09:08.985169 140050979010304 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.228451207280159, loss=1.8235292434692383
I0210 11:09:44.388864 140050897565440 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.281390517950058, loss=1.8279954195022583
I0210 11:10:19.775963 140050979010304 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.22070157527923584, loss=1.8448271751403809
I0210 11:10:55.182196 140050897565440 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.21716390550136566, loss=1.7412663698196411
I0210 11:11:30.607348 140050979010304 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19072756171226501, loss=1.7209166288375854
I0210 11:12:06.015819 140050897565440 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.20071905851364136, loss=1.789971947669983
I0210 11:12:41.423444 140050979010304 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.21894437074661255, loss=1.7663229703903198
I0210 11:13:16.815660 140050897565440 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.19256988167762756, loss=1.8068034648895264
I0210 11:13:52.221264 140050979010304 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.35360145568847656, loss=1.7509195804595947
I0210 11:14:27.637345 140050897565440 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.4239814579486847, loss=1.811965823173523
I0210 11:15:03.054488 140050979010304 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.20386599004268646, loss=1.7431291341781616
I0210 11:15:38.463234 140050897565440 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.21017010509967804, loss=1.8894593715667725
I0210 11:16:13.867280 140050979010304 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.2917562425136566, loss=1.766123652458191
I0210 11:16:49.281199 140050897565440 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.22598977386951447, loss=1.8774471282958984
I0210 11:17:24.687649 140050979010304 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.20296214520931244, loss=1.7996090650558472
I0210 11:18:00.092409 140050897565440 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2087022066116333, loss=1.7993276119232178
I0210 11:18:35.509110 140050979010304 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.19670574367046356, loss=1.7394901514053345
I0210 11:18:40.543096 140225696298816 spec.py:321] Evaluating on the training split.
I0210 11:18:43.513777 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:21:29.927946 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 11:21:32.613672 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:23:59.469845 140225696298816 spec.py:349] Evaluating on the test split.
I0210 11:24:02.155571 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:26:17.852854 140225696298816 submission_runner.py:408] Time since start: 21773.66s, 	Step: 35616, 	{'train/accuracy': 0.641491711139679, 'train/loss': 1.723802089691162, 'train/bleu': 31.370909674220155, 'validation/accuracy': 0.6618516445159912, 'validation/loss': 1.5850014686584473, 'validation/bleu': 28.14534491270295, 'validation/num_examples': 3000, 'test/accuracy': 0.6744756698608398, 'test/loss': 1.5015976428985596, 'test/bleu': 27.731952103228185, 'test/num_examples': 3003, 'score': 12632.403628587723, 'total_duration': 21773.656921863556, 'accumulated_submission_time': 12632.403628587723, 'accumulated_eval_time': 9139.705032587051, 'accumulated_logging_time': 0.4213588237762451}
I0210 11:26:17.872464 140050897565440 logging_writer.py:48] [35616] accumulated_eval_time=9139.705033, accumulated_logging_time=0.421359, accumulated_submission_time=12632.403629, global_step=35616, preemption_count=0, score=12632.403629, test/accuracy=0.674476, test/bleu=27.731952, test/loss=1.501598, test/num_examples=3003, total_duration=21773.656922, train/accuracy=0.641492, train/bleu=31.370910, train/loss=1.723802, validation/accuracy=0.661852, validation/bleu=28.145345, validation/loss=1.585001, validation/num_examples=3000
I0210 11:26:47.834232 140050979010304 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.2322462499141693, loss=1.7940634489059448
I0210 11:27:23.151179 140050897565440 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.23106594383716583, loss=1.8155626058578491
I0210 11:27:58.557660 140050979010304 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.190199077129364, loss=1.7636470794677734
I0210 11:28:33.972766 140050897565440 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.20548783242702484, loss=1.8432350158691406
I0210 11:29:09.393370 140050979010304 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.22346636652946472, loss=1.8563259840011597
I0210 11:29:44.803805 140050897565440 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.20616045594215393, loss=1.7906237840652466
I0210 11:30:20.212850 140050979010304 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.20702701807022095, loss=1.7710456848144531
I0210 11:30:55.621818 140050897565440 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2072516679763794, loss=1.8569271564483643
I0210 11:31:31.039454 140050979010304 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.20385779440402985, loss=1.7216464281082153
I0210 11:32:06.482240 140050897565440 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.1988566815853119, loss=1.8269885778427124
I0210 11:32:41.898594 140050979010304 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.2238265872001648, loss=1.7440050840377808
I0210 11:33:17.309681 140050897565440 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.20004747807979584, loss=1.8342435359954834
I0210 11:33:52.712325 140050979010304 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.21900440752506256, loss=1.8215241432189941
I0210 11:34:28.091213 140050897565440 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.24445541203022003, loss=1.7605910301208496
I0210 11:35:03.482084 140050979010304 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.25235259532928467, loss=1.75497567653656
I0210 11:35:38.879638 140050897565440 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.23184451460838318, loss=1.8287341594696045
I0210 11:36:14.299636 140050979010304 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.25080370903015137, loss=1.7577053308486938
I0210 11:36:49.683134 140050897565440 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3018534183502197, loss=1.7978723049163818
I0210 11:37:25.110669 140050979010304 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.19521871209144592, loss=1.8160359859466553
I0210 11:38:00.508173 140050897565440 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.23258990049362183, loss=1.772901177406311
I0210 11:38:35.936007 140050979010304 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.20104217529296875, loss=1.8211283683776855
I0210 11:39:11.343967 140050897565440 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.21358303725719452, loss=1.7370167970657349
I0210 11:39:46.770867 140050979010304 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.20401014387607574, loss=1.8451683521270752
I0210 11:40:18.014256 140225696298816 spec.py:321] Evaluating on the training split.
I0210 11:40:20.980780 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:43:30.675976 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 11:43:33.340729 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:46:00.058890 140225696298816 spec.py:349] Evaluating on the test split.
I0210 11:46:02.742478 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 11:48:25.865895 140225696298816 submission_runner.py:408] Time since start: 23101.67s, 	Step: 37990, 	{'train/accuracy': 0.6566494703292847, 'train/loss': 1.6067266464233398, 'train/bleu': 32.589987116770835, 'validation/accuracy': 0.6636743545532227, 'validation/loss': 1.575439691543579, 'validation/bleu': 28.37420393676452, 'validation/num_examples': 3000, 'test/accuracy': 0.6746964454650879, 'test/loss': 1.4958630800247192, 'test/bleu': 27.57462241443689, 'test/num_examples': 3003, 'score': 13472.459535121918, 'total_duration': 23101.669969797134, 'accumulated_submission_time': 13472.459535121918, 'accumulated_eval_time': 9627.55661559105, 'accumulated_logging_time': 0.45209169387817383}
I0210 11:48:25.886637 140050897565440 logging_writer.py:48] [37990] accumulated_eval_time=9627.556616, accumulated_logging_time=0.452092, accumulated_submission_time=13472.459535, global_step=37990, preemption_count=0, score=13472.459535, test/accuracy=0.674696, test/bleu=27.574622, test/loss=1.495863, test/num_examples=3003, total_duration=23101.669970, train/accuracy=0.656649, train/bleu=32.589987, train/loss=1.606727, validation/accuracy=0.663674, validation/bleu=28.374204, validation/loss=1.575440, validation/num_examples=3000
I0210 11:48:29.786037 140050979010304 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.19491897523403168, loss=1.7618215084075928
I0210 11:49:05.032826 140050897565440 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.18813113868236542, loss=1.7446485757827759
I0210 11:49:40.386822 140050979010304 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.20581619441509247, loss=1.7953873872756958
I0210 11:50:15.793819 140050897565440 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.20526115596294403, loss=1.7716519832611084
I0210 11:50:51.188650 140050979010304 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.22855830192565918, loss=1.7363849878311157
I0210 11:51:26.569492 140050897565440 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.17439547181129456, loss=1.7626994848251343
I0210 11:52:01.976237 140050979010304 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.19649530947208405, loss=1.82667076587677
I0210 11:52:37.398205 140050897565440 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.19557268917560577, loss=1.7270877361297607
I0210 11:53:12.799239 140050979010304 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.2553216218948364, loss=1.761513590812683
I0210 11:53:48.203889 140050897565440 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.2541405260562897, loss=1.8364704847335815
I0210 11:54:23.617221 140050979010304 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.1950380504131317, loss=1.7001076936721802
I0210 11:54:59.006427 140050897565440 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.22731976211071014, loss=1.786035418510437
I0210 11:55:34.416664 140050979010304 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.21621257066726685, loss=1.8135130405426025
I0210 11:56:09.836027 140050897565440 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.27122682332992554, loss=1.9074066877365112
I0210 11:56:45.256802 140050979010304 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.25605854392051697, loss=1.7777425050735474
I0210 11:57:20.664170 140050897565440 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21348878741264343, loss=1.7504541873931885
I0210 11:57:56.057033 140050979010304 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.17833998799324036, loss=1.7609288692474365
I0210 11:58:31.428716 140050897565440 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2133064866065979, loss=1.7742624282836914
I0210 11:59:06.828516 140050979010304 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.20526330173015594, loss=1.752376675605774
I0210 11:59:42.231940 140050897565440 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.18628418445587158, loss=1.7054963111877441
I0210 12:00:17.653110 140050979010304 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.18992842733860016, loss=1.7396553754806519
I0210 12:00:53.085290 140050897565440 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.21679627895355225, loss=1.721372127532959
I0210 12:01:28.482152 140050979010304 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2256939709186554, loss=1.7441940307617188
I0210 12:02:03.897630 140050897565440 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20462463796138763, loss=1.824204444885254
I0210 12:02:25.921539 140225696298816 spec.py:321] Evaluating on the training split.
I0210 12:02:28.913223 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:06:32.523894 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 12:06:35.189238 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:10:05.835127 140225696298816 spec.py:349] Evaluating on the test split.
I0210 12:10:08.515495 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:13:30.244750 140225696298816 submission_runner.py:408] Time since start: 24606.05s, 	Step: 40364, 	{'train/accuracy': 0.6478399634361267, 'train/loss': 1.676770567893982, 'train/bleu': 31.48029070214986, 'validation/accuracy': 0.6622112393379211, 'validation/loss': 1.5716074705123901, 'validation/bleu': 28.142196735251872, 'validation/num_examples': 3000, 'test/accuracy': 0.6744988560676575, 'test/loss': 1.4857368469238281, 'test/bleu': 27.850430361041195, 'test/num_examples': 3003, 'score': 14312.40743112564, 'total_duration': 24606.04882979393, 'accumulated_submission_time': 14312.40743112564, 'accumulated_eval_time': 10291.879784822464, 'accumulated_logging_time': 0.4850766658782959}
I0210 12:13:30.265267 140050979010304 logging_writer.py:48] [40364] accumulated_eval_time=10291.879785, accumulated_logging_time=0.485077, accumulated_submission_time=14312.407431, global_step=40364, preemption_count=0, score=14312.407431, test/accuracy=0.674499, test/bleu=27.850430, test/loss=1.485737, test/num_examples=3003, total_duration=24606.048830, train/accuracy=0.647840, train/bleu=31.480291, train/loss=1.676771, validation/accuracy=0.662211, validation/bleu=28.142197, validation/loss=1.571607, validation/num_examples=3000
I0210 12:13:43.320343 140050897565440 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2762802839279175, loss=1.8536194562911987
I0210 12:14:18.557331 140050979010304 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.21666988730430603, loss=1.7701791524887085
I0210 12:14:53.918045 140050897565440 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20127622783184052, loss=1.7128493785858154
I0210 12:15:29.278149 140050979010304 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.20591968297958374, loss=1.7679669857025146
I0210 12:16:04.666691 140050897565440 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.21466654539108276, loss=1.8025262355804443
I0210 12:16:40.079629 140050979010304 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.20406505465507507, loss=1.8327809572219849
I0210 12:17:15.489432 140050897565440 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.20912964642047882, loss=1.7057281732559204
I0210 12:17:50.876513 140050979010304 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.20566466450691223, loss=1.8146154880523682
I0210 12:18:26.282835 140050897565440 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.20381271839141846, loss=1.7002825736999512
I0210 12:19:01.676149 140050979010304 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.20903147757053375, loss=1.781160831451416
I0210 12:19:37.087034 140050897565440 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2197166085243225, loss=1.783021092414856
I0210 12:20:12.525935 140050979010304 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.19622792303562164, loss=1.6320414543151855
I0210 12:20:47.914151 140050897565440 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.24010680615901947, loss=1.8237857818603516
I0210 12:21:23.354067 140050979010304 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.19494615495204926, loss=1.7065966129302979
I0210 12:21:58.752827 140050897565440 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1995197832584381, loss=1.8354358673095703
I0210 12:22:34.179733 140050979010304 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.18785321712493896, loss=1.8706445693969727
I0210 12:23:09.585035 140050897565440 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.21019133925437927, loss=1.7460306882858276
I0210 12:23:45.006319 140050979010304 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2177356630563736, loss=1.8712717294692993
I0210 12:24:20.407598 140050897565440 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.21136783063411713, loss=1.7850128412246704
I0210 12:24:55.825819 140050979010304 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.21553024649620056, loss=1.842073917388916
I0210 12:25:31.259944 140050897565440 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.1907973289489746, loss=1.721595287322998
I0210 12:26:06.682512 140050979010304 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.23762564361095428, loss=1.7550452947616577
I0210 12:26:42.101527 140050897565440 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.206830233335495, loss=1.7819187641143799
I0210 12:27:17.524674 140050979010304 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.20916572213172913, loss=1.7297767400741577
I0210 12:27:30.343129 140225696298816 spec.py:321] Evaluating on the training split.
I0210 12:27:33.309792 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:31:00.930386 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 12:31:03.616226 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:33:53.449477 140225696298816 spec.py:349] Evaluating on the test split.
I0210 12:33:56.138450 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:36:22.921434 140225696298816 submission_runner.py:408] Time since start: 25978.73s, 	Step: 42738, 	{'train/accuracy': 0.6446605920791626, 'train/loss': 1.6969213485717773, 'train/bleu': 31.174335951599705, 'validation/accuracy': 0.6645174622535706, 'validation/loss': 1.5625895261764526, 'validation/bleu': 28.429532815753745, 'validation/num_examples': 3000, 'test/accuracy': 0.6777177453041077, 'test/loss': 1.4755793809890747, 'test/bleu': 28.137474027499042, 'test/num_examples': 3003, 'score': 15152.398067235947, 'total_duration': 25978.725489377975, 'accumulated_submission_time': 15152.398067235947, 'accumulated_eval_time': 10824.458010911942, 'accumulated_logging_time': 0.5174057483673096}
I0210 12:36:22.942876 140050897565440 logging_writer.py:48] [42738] accumulated_eval_time=10824.458011, accumulated_logging_time=0.517406, accumulated_submission_time=15152.398067, global_step=42738, preemption_count=0, score=15152.398067, test/accuracy=0.677718, test/bleu=28.137474, test/loss=1.475579, test/num_examples=3003, total_duration=25978.725489, train/accuracy=0.644661, train/bleu=31.174336, train/loss=1.696921, validation/accuracy=0.664517, validation/bleu=28.429533, validation/loss=1.562590, validation/num_examples=3000
I0210 12:36:45.165476 140050979010304 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.19309376180171967, loss=1.7804920673370361
I0210 12:37:20.471754 140050897565440 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.206029012799263, loss=1.780479073524475
I0210 12:37:55.894006 140050979010304 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21970400214195251, loss=1.800704836845398
I0210 12:38:31.322432 140050897565440 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2152082622051239, loss=1.7685236930847168
I0210 12:39:06.740775 140050979010304 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.20090998709201813, loss=1.7117360830307007
I0210 12:39:42.119212 140050897565440 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.7156786322593689, loss=1.7485544681549072
I0210 12:40:17.505111 140050979010304 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.20924852788448334, loss=1.7296247482299805
I0210 12:40:52.886839 140050897565440 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1892593652009964, loss=1.7251182794570923
I0210 12:41:28.287621 140050979010304 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.23809576034545898, loss=1.7519079446792603
I0210 12:42:03.686252 140050897565440 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.21639305353164673, loss=1.74565851688385
I0210 12:42:39.102578 140050979010304 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1996685415506363, loss=1.8178449869155884
I0210 12:43:14.511975 140050897565440 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19616957008838654, loss=1.7083368301391602
I0210 12:43:49.912081 140050979010304 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.19513483345508575, loss=1.7424979209899902
I0210 12:44:25.354498 140050897565440 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1953171193599701, loss=1.8183759450912476
I0210 12:45:00.783474 140050979010304 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2037336677312851, loss=1.660394549369812
I0210 12:45:36.188767 140050897565440 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.24368852376937866, loss=1.815696358680725
I0210 12:46:11.614780 140050979010304 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2059343457221985, loss=1.729211449623108
I0210 12:46:47.037119 140050897565440 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.18005849421024323, loss=1.7773457765579224
I0210 12:47:22.463293 140050979010304 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19545944035053253, loss=1.7579621076583862
I0210 12:47:57.888761 140050897565440 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.2175534963607788, loss=1.8092036247253418
I0210 12:48:33.281396 140050979010304 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.208053857088089, loss=1.7322696447372437
I0210 12:49:08.691239 140050897565440 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.22034911811351776, loss=1.7129795551300049
I0210 12:49:44.089499 140050979010304 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.22696131467819214, loss=1.722636103630066
I0210 12:50:19.497256 140050897565440 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.19253748655319214, loss=1.7900420427322388
I0210 12:50:23.119056 140225696298816 spec.py:321] Evaluating on the training split.
I0210 12:50:26.085226 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:54:27.320358 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 12:54:29.984857 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:57:08.804398 140225696298816 spec.py:349] Evaluating on the test split.
I0210 12:57:11.483565 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 12:59:38.569452 140225696298816 submission_runner.py:408] Time since start: 27374.37s, 	Step: 45112, 	{'train/accuracy': 0.6525201201438904, 'train/loss': 1.646553874015808, 'train/bleu': 32.448058247862534, 'validation/accuracy': 0.6664517521858215, 'validation/loss': 1.550062656402588, 'validation/bleu': 28.67135209872841, 'validation/num_examples': 3000, 'test/accuracy': 0.6789379119873047, 'test/loss': 1.4708216190338135, 'test/bleu': 28.37878554052869, 'test/num_examples': 3003, 'score': 15992.489179372787, 'total_duration': 27374.3735370636, 'accumulated_submission_time': 15992.489179372787, 'accumulated_eval_time': 11379.908358573914, 'accumulated_logging_time': 0.5488555431365967}
I0210 12:59:38.590850 140050979010304 logging_writer.py:48] [45112] accumulated_eval_time=11379.908359, accumulated_logging_time=0.548856, accumulated_submission_time=15992.489179, global_step=45112, preemption_count=0, score=15992.489179, test/accuracy=0.678938, test/bleu=28.378786, test/loss=1.470822, test/num_examples=3003, total_duration=27374.373537, train/accuracy=0.652520, train/bleu=32.448058, train/loss=1.646554, validation/accuracy=0.666452, validation/bleu=28.671352, validation/loss=1.550063, validation/num_examples=3000
I0210 13:00:09.925048 140050897565440 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1824425756931305, loss=1.750032663345337
I0210 13:00:45.244347 140050979010304 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.19456158578395844, loss=1.7345319986343384
I0210 13:01:20.612568 140050897565440 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.21693840622901917, loss=1.7867248058319092
I0210 13:01:55.993158 140050979010304 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.22473442554473877, loss=1.7570686340332031
I0210 13:02:31.413655 140050897565440 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.19496221840381622, loss=1.7430516481399536
I0210 13:03:06.784702 140050979010304 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.22748833894729614, loss=1.7971267700195312
I0210 13:03:42.173300 140050897565440 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18633827567100525, loss=1.7590289115905762
I0210 13:04:17.572748 140050979010304 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.20149719715118408, loss=1.783733606338501
I0210 13:04:52.956895 140050897565440 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.21870972216129303, loss=1.749059796333313
I0210 13:05:28.348449 140050979010304 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22023609280586243, loss=1.8131110668182373
I0210 13:06:03.739831 140050897565440 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.612564206123352, loss=1.778417706489563
I0210 13:06:39.145437 140050979010304 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.21338029205799103, loss=1.6955934762954712
I0210 13:07:14.536660 140050897565440 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.21113306283950806, loss=1.681159496307373
I0210 13:07:49.924540 140050979010304 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19340908527374268, loss=1.8925830125808716
I0210 13:08:25.321457 140050897565440 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.21945153176784515, loss=1.769046664237976
I0210 13:09:00.703561 140050979010304 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2531388998031616, loss=1.7790216207504272
I0210 13:09:36.092300 140050897565440 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.2204882800579071, loss=1.74941885471344
I0210 13:10:11.476253 140050979010304 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.20394174754619598, loss=1.7105906009674072
I0210 13:10:46.868445 140050897565440 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.22927185893058777, loss=1.6795408725738525
I0210 13:11:22.258943 140050979010304 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.18639139831066132, loss=1.7667722702026367
I0210 13:11:57.666050 140050897565440 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.21600227057933807, loss=1.796222448348999
I0210 13:12:33.073222 140050979010304 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2892422676086426, loss=1.7068707942962646
I0210 13:13:08.483728 140050897565440 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20869088172912598, loss=1.8067247867584229
I0210 13:13:38.642189 140225696298816 spec.py:321] Evaluating on the training split.
I0210 13:13:41.617783 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:16:54.710437 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 13:16:57.381559 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:19:36.741684 140225696298816 spec.py:349] Evaluating on the test split.
I0210 13:19:39.422126 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:21:52.081708 140225696298816 submission_runner.py:408] Time since start: 28707.89s, 	Step: 47487, 	{'train/accuracy': 0.6500367522239685, 'train/loss': 1.6660417318344116, 'train/bleu': 32.292675817863426, 'validation/accuracy': 0.6654598116874695, 'validation/loss': 1.5579570531845093, 'validation/bleu': 28.594837733209264, 'validation/num_examples': 3000, 'test/accuracy': 0.6797629594802856, 'test/loss': 1.4692802429199219, 'test/bleu': 28.22441199348882, 'test/num_examples': 3003, 'score': 16832.455917835236, 'total_duration': 28707.885778665543, 'accumulated_submission_time': 16832.455917835236, 'accumulated_eval_time': 11873.347818136215, 'accumulated_logging_time': 0.5799081325531006}
I0210 13:21:52.103679 140050979010304 logging_writer.py:48] [47487] accumulated_eval_time=11873.347818, accumulated_logging_time=0.579908, accumulated_submission_time=16832.455918, global_step=47487, preemption_count=0, score=16832.455918, test/accuracy=0.679763, test/bleu=28.224412, test/loss=1.469280, test/num_examples=3003, total_duration=28707.885779, train/accuracy=0.650037, train/bleu=32.292676, train/loss=1.666042, validation/accuracy=0.665460, validation/bleu=28.594838, validation/loss=1.557957, validation/num_examples=3000
I0210 13:21:57.060019 140050897565440 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1850470006465912, loss=1.7455012798309326
I0210 13:22:32.332569 140050979010304 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2414974570274353, loss=1.6922966241836548
I0210 13:23:07.685654 140050897565440 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2162160724401474, loss=1.7270926237106323
I0210 13:23:43.069001 140050979010304 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2079395055770874, loss=1.7567107677459717
I0210 13:24:18.445710 140050897565440 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2265620231628418, loss=1.7594789266586304
I0210 13:24:53.845044 140050979010304 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.20040102303028107, loss=1.7400693893432617
I0210 13:25:29.244073 140050897565440 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.21950292587280273, loss=1.7903854846954346
I0210 13:26:04.622112 140050979010304 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.20489756762981415, loss=1.8164170980453491
I0210 13:26:40.001516 140050897565440 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.19108566641807556, loss=1.6813032627105713
I0210 13:27:15.404733 140050979010304 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.19569367170333862, loss=1.7031896114349365
I0210 13:27:50.801581 140050897565440 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2855704724788666, loss=1.751513123512268
I0210 13:28:26.205045 140050979010304 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2110249549150467, loss=1.8028044700622559
I0210 13:29:01.606954 140050897565440 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.26749131083488464, loss=1.7611922025680542
I0210 13:29:37.002156 140050979010304 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.2045714408159256, loss=1.7081917524337769
I0210 13:30:12.394970 140050897565440 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.18619675934314728, loss=1.7319046258926392
I0210 13:30:47.784375 140050979010304 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21540582180023193, loss=1.6739274263381958
I0210 13:31:23.159437 140050897565440 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.18857456743717194, loss=1.7820571660995483
I0210 13:31:58.556495 140050979010304 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19935393333435059, loss=1.6527328491210938
I0210 13:32:33.969766 140050897565440 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3680262267589569, loss=1.7904090881347656
I0210 13:33:09.374387 140050979010304 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2400159388780594, loss=1.734211802482605
I0210 13:33:44.776281 140050897565440 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19228792190551758, loss=1.7776453495025635
I0210 13:34:20.174849 140050979010304 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.22215062379837036, loss=1.7487692832946777
I0210 13:34:55.607086 140050897565440 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.19978837668895721, loss=1.7505823373794556
I0210 13:35:31.018841 140050979010304 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20809201896190643, loss=1.7599419355392456
I0210 13:35:52.346987 140225696298816 spec.py:321] Evaluating on the training split.
I0210 13:35:55.320264 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:39:46.921152 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 13:39:49.581360 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:42:31.890198 140225696298816 spec.py:349] Evaluating on the test split.
I0210 13:42:34.564436 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 13:45:07.908686 140225696298816 submission_runner.py:408] Time since start: 30103.71s, 	Step: 49862, 	{'train/accuracy': 0.6516985297203064, 'train/loss': 1.6592146158218384, 'train/bleu': 31.26118472682359, 'validation/accuracy': 0.6688819527626038, 'validation/loss': 1.5371198654174805, 'validation/bleu': 29.092173467752627, 'validation/num_examples': 3000, 'test/accuracy': 0.6824705004692078, 'test/loss': 1.4507604837417603, 'test/bleu': 28.232030328801066, 'test/num_examples': 3003, 'score': 17672.611881494522, 'total_duration': 30103.71274662018, 'accumulated_submission_time': 17672.611881494522, 'accumulated_eval_time': 12428.909444093704, 'accumulated_logging_time': 0.6142618656158447}
I0210 13:45:07.930954 140050897565440 logging_writer.py:48] [49862] accumulated_eval_time=12428.909444, accumulated_logging_time=0.614262, accumulated_submission_time=17672.611881, global_step=49862, preemption_count=0, score=17672.611881, test/accuracy=0.682471, test/bleu=28.232030, test/loss=1.450760, test/num_examples=3003, total_duration=30103.712747, train/accuracy=0.651699, train/bleu=31.261185, train/loss=1.659215, validation/accuracy=0.668882, validation/bleu=29.092173, validation/loss=1.537120, validation/num_examples=3000
I0210 13:45:21.671907 140050979010304 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2230204939842224, loss=1.8227283954620361
I0210 13:45:56.929235 140050897565440 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2033648043870926, loss=1.722697138786316
I0210 13:46:32.286349 140050979010304 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.26451024413108826, loss=1.7667715549468994
I0210 13:47:07.671385 140050897565440 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2036965787410736, loss=1.6855851411819458
I0210 13:47:43.076182 140050979010304 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.23873184621334076, loss=1.694002628326416
I0210 13:48:18.459802 140050897565440 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2115761786699295, loss=1.787213683128357
I0210 13:48:53.845285 140050979010304 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21710090339183807, loss=1.7145873308181763
I0210 13:49:29.257634 140050897565440 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.19987310469150543, loss=1.6577149629592896
I0210 13:50:04.659923 140050979010304 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.21898527443408966, loss=1.7940378189086914
I0210 13:50:40.050955 140050897565440 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.21786922216415405, loss=1.7061691284179688
I0210 13:51:15.443974 140050979010304 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.26006147265434265, loss=1.8085561990737915
I0210 13:51:50.835093 140050897565440 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.21637290716171265, loss=1.7056632041931152
I0210 13:52:26.253033 140050979010304 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.23251773416996002, loss=1.7564737796783447
I0210 13:53:01.664510 140050897565440 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.23296061158180237, loss=1.7950832843780518
I0210 13:53:37.071516 140050979010304 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.21257248520851135, loss=1.7665494680404663
I0210 13:54:12.460962 140050897565440 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.1926475167274475, loss=1.7171621322631836
I0210 13:54:47.866872 140050979010304 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20022909343242645, loss=1.7111133337020874
I0210 13:55:23.266492 140050897565440 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.21233852207660675, loss=1.7315999269485474
I0210 13:55:58.641378 140050979010304 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.20776407420635223, loss=1.708467721939087
I0210 13:56:34.026128 140050897565440 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.23207540810108185, loss=1.8332526683807373
I0210 13:57:09.390563 140050979010304 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.210976243019104, loss=1.6729891300201416
I0210 13:57:44.781409 140050897565440 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.23494361340999603, loss=1.6525284051895142
I0210 13:58:20.160712 140050979010304 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.21778684854507446, loss=1.7550944089889526
I0210 13:58:55.536705 140050897565440 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20174390077590942, loss=1.7437200546264648
I0210 13:59:08.000546 140225696298816 spec.py:321] Evaluating on the training split.
I0210 13:59:10.971134 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:03:33.245675 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 14:03:35.929718 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:06:23.016936 140225696298816 spec.py:349] Evaluating on the test split.
I0210 14:06:25.695508 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:09:16.154956 140225696298816 submission_runner.py:408] Time since start: 31551.96s, 	Step: 52237, 	{'train/accuracy': 0.651114284992218, 'train/loss': 1.6532764434814453, 'train/bleu': 32.15875856369312, 'validation/accuracy': 0.670059859752655, 'validation/loss': 1.5286074876785278, 'validation/bleu': 29.083560310137855, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032332420349, 'test/loss': 1.4496662616729736, 'test/bleu': 28.419362421745703, 'test/num_examples': 3003, 'score': 18512.59501695633, 'total_duration': 31551.95904159546, 'accumulated_submission_time': 18512.59501695633, 'accumulated_eval_time': 13037.063804626465, 'accumulated_logging_time': 0.6480474472045898}
I0210 14:09:16.176836 140050979010304 logging_writer.py:48] [52237] accumulated_eval_time=13037.063805, accumulated_logging_time=0.648047, accumulated_submission_time=18512.595017, global_step=52237, preemption_count=0, score=18512.595017, test/accuracy=0.682203, test/bleu=28.419362, test/loss=1.449666, test/num_examples=3003, total_duration=31551.959042, train/accuracy=0.651114, train/bleu=32.158759, train/loss=1.653276, validation/accuracy=0.670060, validation/bleu=29.083560, validation/loss=1.528607, validation/num_examples=3000
I0210 14:09:38.710152 140050897565440 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.21743641793727875, loss=1.7072675228118896
I0210 14:10:13.966733 140050979010304 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.19675609469413757, loss=1.751404881477356
I0210 14:10:49.371801 140050897565440 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.084259271621704, loss=1.7625527381896973
I0210 14:11:24.765422 140050979010304 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.6683059930801392, loss=2.366262197494507
I0210 14:12:00.157419 140050897565440 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2052900791168213, loss=1.7241986989974976
I0210 14:12:35.562450 140050979010304 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1941947042942047, loss=1.7879891395568848
I0210 14:13:10.984443 140050897565440 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.20338352024555206, loss=1.787209391593933
I0210 14:13:46.359367 140050979010304 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.19381120800971985, loss=1.7503447532653809
I0210 14:14:21.743336 140050897565440 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.22328539192676544, loss=1.7311047315597534
I0210 14:14:57.120783 140050979010304 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.19590704143047333, loss=1.700433611869812
I0210 14:15:32.523798 140050897565440 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.19881278276443481, loss=1.7079746723175049
I0210 14:16:07.915423 140050979010304 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19517391920089722, loss=1.7004231214523315
I0210 14:16:43.313836 140050897565440 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.21778911352157593, loss=1.7565146684646606
I0210 14:17:18.721307 140050979010304 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2123815268278122, loss=1.7762168645858765
I0210 14:17:54.119998 140050897565440 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2936882972717285, loss=1.8002492189407349
I0210 14:18:29.522510 140050979010304 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19137564301490784, loss=1.7326313257217407
I0210 14:19:04.924525 140050897565440 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.19074852764606476, loss=1.6670535802841187
I0210 14:19:40.315617 140050979010304 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2063169628381729, loss=1.725639820098877
I0210 14:20:15.736193 140050897565440 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.21197813749313354, loss=1.6618826389312744
I0210 14:20:51.084316 140050979010304 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2730131149291992, loss=3.822359085083008
I0210 14:21:26.428163 140050897565440 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.1851751804351807, loss=3.4998703002929688
I0210 14:22:01.830910 140050979010304 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.18764039874076843, loss=1.7235665321350098
I0210 14:22:37.213842 140050897565440 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.19049903750419617, loss=1.6955407857894897
I0210 14:23:12.624995 140050979010304 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.19800728559494019, loss=1.7631443738937378
I0210 14:23:16.241979 140225696298816 spec.py:321] Evaluating on the training split.
I0210 14:23:19.211432 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:26:42.704187 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 14:26:45.380264 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:29:44.775801 140225696298816 spec.py:349] Evaluating on the test split.
I0210 14:29:47.448929 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:32:19.557495 140225696298816 submission_runner.py:408] Time since start: 32935.36s, 	Step: 54612, 	{'train/accuracy': 0.6498084664344788, 'train/loss': 1.666841983795166, 'train/bleu': 31.615262520847377, 'validation/accuracy': 0.6697251200675964, 'validation/loss': 1.5325440168380737, 'validation/bleu': 28.794014542916006, 'validation/num_examples': 3000, 'test/accuracy': 0.6830283403396606, 'test/loss': 1.4475644826889038, 'test/bleu': 28.666378067688967, 'test/num_examples': 3003, 'score': 19352.575419425964, 'total_duration': 32935.361577510834, 'accumulated_submission_time': 19352.575419425964, 'accumulated_eval_time': 13580.379266500473, 'accumulated_logging_time': 0.6796233654022217}
I0210 14:32:19.579617 140050897565440 logging_writer.py:48] [54612] accumulated_eval_time=13580.379267, accumulated_logging_time=0.679623, accumulated_submission_time=19352.575419, global_step=54612, preemption_count=0, score=19352.575419, test/accuracy=0.683028, test/bleu=28.666378, test/loss=1.447564, test/num_examples=3003, total_duration=32935.361578, train/accuracy=0.649808, train/bleu=31.615263, train/loss=1.666842, validation/accuracy=0.669725, validation/bleu=28.794015, validation/loss=1.532544, validation/num_examples=3000
I0210 14:32:50.943638 140050979010304 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.18984156847000122, loss=1.7214630842208862
I0210 14:33:26.238840 140050897565440 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.21288977563381195, loss=1.7179133892059326
I0210 14:34:01.626870 140050979010304 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.21230390667915344, loss=1.7743350267410278
I0210 14:34:37.010065 140050897565440 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.1919577568769455, loss=1.7066700458526611
I0210 14:35:12.405317 140050979010304 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.24253888428211212, loss=1.7881550788879395
I0210 14:35:47.801950 140050897565440 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.24766506254673004, loss=1.7500334978103638
I0210 14:36:23.218324 140050979010304 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1950952112674713, loss=1.724927306175232
I0210 14:36:58.603050 140050897565440 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.18746918439865112, loss=1.6949719190597534
I0210 14:37:33.991639 140050979010304 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2052983194589615, loss=1.6997624635696411
I0210 14:38:09.364166 140050897565440 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.21321938931941986, loss=1.73445725440979
I0210 14:38:44.768367 140050979010304 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.20662450790405273, loss=1.7073941230773926
I0210 14:39:20.145126 140050897565440 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.1965869963169098, loss=1.7855318784713745
I0210 14:39:55.531015 140050979010304 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2090771198272705, loss=1.7302627563476562
I0210 14:40:30.922457 140050897565440 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.20013023912906647, loss=1.750294804573059
I0210 14:41:06.348824 140050979010304 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.19597314298152924, loss=1.6078824996948242
I0210 14:41:41.759436 140050897565440 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.22072230279445648, loss=1.7117435932159424
I0210 14:42:17.165325 140050979010304 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.269277423620224, loss=1.7682390213012695
I0210 14:42:52.564752 140050897565440 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.19679850339889526, loss=1.7122656106948853
I0210 14:43:27.986406 140050979010304 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.21081025898456573, loss=1.742638349533081
I0210 14:44:03.368698 140050897565440 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.18093524873256683, loss=1.6397626399993896
I0210 14:44:38.762266 140050979010304 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.2240598350763321, loss=1.7721842527389526
I0210 14:45:14.151743 140050897565440 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2144404947757721, loss=1.7108534574508667
I0210 14:45:49.549939 140050979010304 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1927724927663803, loss=1.7237178087234497
I0210 14:46:19.705963 140225696298816 spec.py:321] Evaluating on the training split.
I0210 14:46:22.675526 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:50:01.041182 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 14:50:03.720301 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:52:24.254921 140225696298816 spec.py:349] Evaluating on the test split.
I0210 14:52:26.964812 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 14:55:03.875671 140225696298816 submission_runner.py:408] Time since start: 34299.68s, 	Step: 56987, 	{'train/accuracy': 0.663896381855011, 'train/loss': 1.5628291368484497, 'train/bleu': 32.592228093350805, 'validation/accuracy': 0.6697623133659363, 'validation/loss': 1.524965763092041, 'validation/bleu': 28.77605624550842, 'validation/num_examples': 3000, 'test/accuracy': 0.6824240684509277, 'test/loss': 1.442415714263916, 'test/bleu': 28.528350110436556, 'test/num_examples': 3003, 'score': 20192.615739822388, 'total_duration': 34299.67973899841, 'accumulated_submission_time': 20192.615739822388, 'accumulated_eval_time': 14104.548913478851, 'accumulated_logging_time': 0.7127649784088135}
I0210 14:55:03.898436 140050897565440 logging_writer.py:48] [56987] accumulated_eval_time=14104.548913, accumulated_logging_time=0.712765, accumulated_submission_time=20192.615740, global_step=56987, preemption_count=0, score=20192.615740, test/accuracy=0.682424, test/bleu=28.528350, test/loss=1.442416, test/num_examples=3003, total_duration=34299.679739, train/accuracy=0.663896, train/bleu=32.592228, train/loss=1.562829, validation/accuracy=0.669762, validation/bleu=28.776056, validation/loss=1.524966, validation/num_examples=3000
I0210 14:55:08.846379 140050979010304 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.19901834428310394, loss=1.690840721130371
I0210 14:55:44.089628 140050897565440 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.21372616291046143, loss=1.8119211196899414
I0210 14:56:19.424294 140050979010304 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.21279937028884888, loss=1.6493871212005615
I0210 14:56:54.814797 140050897565440 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20126956701278687, loss=1.7346668243408203
I0210 14:57:30.180001 140050979010304 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19700202345848083, loss=1.703838586807251
I0210 14:58:05.563860 140050897565440 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.19672489166259766, loss=1.7593047618865967
I0210 14:58:40.953662 140050979010304 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.18353085219860077, loss=1.7163501977920532
I0210 14:59:16.361029 140050897565440 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.19199813902378082, loss=1.6668026447296143
I0210 14:59:51.761722 140050979010304 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19432644546031952, loss=1.671128511428833
I0210 15:00:27.196098 140050897565440 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.1938582956790924, loss=1.704376459121704
I0210 15:01:02.622695 140050979010304 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.20069526135921478, loss=1.663020133972168
I0210 15:01:38.002468 140050897565440 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.18836185336112976, loss=1.7305881977081299
I0210 15:02:13.438355 140050979010304 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.22181375324726105, loss=1.683516025543213
I0210 15:02:48.857710 140050897565440 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2041742503643036, loss=1.690673589706421
I0210 15:03:24.254467 140050979010304 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.1852962076663971, loss=1.6907893419265747
I0210 15:03:59.675070 140050897565440 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.2076786756515503, loss=1.7410615682601929
I0210 15:04:35.059823 140050979010304 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2108924239873886, loss=1.7203774452209473
I0210 15:05:10.486607 140050897565440 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2069854587316513, loss=1.7532682418823242
I0210 15:05:45.880329 140050979010304 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.20900779962539673, loss=1.763664960861206
I0210 15:06:21.277172 140050897565440 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.1748521775007248, loss=1.6883025169372559
I0210 15:06:56.663692 140050979010304 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.23212455213069916, loss=1.6429033279418945
I0210 15:07:32.087175 140050897565440 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.21677128970623016, loss=1.742898941040039
I0210 15:08:07.498043 140050979010304 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.19838687777519226, loss=1.7146210670471191
I0210 15:08:42.891883 140050897565440 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.19958777725696564, loss=1.8185219764709473
I0210 15:09:04.216718 140225696298816 spec.py:321] Evaluating on the training split.
I0210 15:09:07.190307 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:13:28.472981 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 15:13:31.142527 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:17:45.495196 140225696298816 spec.py:349] Evaluating on the test split.
I0210 15:17:48.173541 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:22:04.953200 140225696298816 submission_runner.py:408] Time since start: 35920.76s, 	Step: 59362, 	{'train/accuracy': 0.6547658443450928, 'train/loss': 1.639319896697998, 'train/bleu': 32.32660768039863, 'validation/accuracy': 0.6719445586204529, 'validation/loss': 1.517433762550354, 'validation/bleu': 28.431021789399217, 'validation/num_examples': 3000, 'test/accuracy': 0.6865028142929077, 'test/loss': 1.4255280494689941, 'test/bleu': 27.129961050076798, 'test/num_examples': 3003, 'score': 21032.84838962555, 'total_duration': 35920.757283210754, 'accumulated_submission_time': 21032.84838962555, 'accumulated_eval_time': 14885.285350084305, 'accumulated_logging_time': 0.7453715801239014}
I0210 15:22:04.976462 140050979010304 logging_writer.py:48] [59362] accumulated_eval_time=14885.285350, accumulated_logging_time=0.745372, accumulated_submission_time=21032.848390, global_step=59362, preemption_count=0, score=21032.848390, test/accuracy=0.686503, test/bleu=27.129961, test/loss=1.425528, test/num_examples=3003, total_duration=35920.757283, train/accuracy=0.654766, train/bleu=32.326608, train/loss=1.639320, validation/accuracy=0.671945, validation/bleu=28.431022, validation/loss=1.517434, validation/num_examples=3000
I0210 15:22:18.718889 140050897565440 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.19506536424160004, loss=1.7002677917480469
I0210 15:22:53.988485 140050979010304 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.19623862206935883, loss=1.7040519714355469
I0210 15:23:29.345966 140050897565440 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.1978689283132553, loss=1.6407158374786377
I0210 15:24:04.743579 140050979010304 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.21140144765377045, loss=1.7463557720184326
I0210 15:24:40.130540 140050897565440 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.19001275300979614, loss=1.6860740184783936
I0210 15:25:15.515347 140050979010304 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19507960975170135, loss=1.6677147150039673
I0210 15:25:50.912919 140050897565440 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2147248536348343, loss=1.698713779449463
I0210 15:26:26.298412 140050979010304 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.20585553348064423, loss=1.7399159669876099
I0210 15:27:01.691132 140050897565440 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.18442007899284363, loss=1.7022678852081299
I0210 15:27:37.127191 140050979010304 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.18925833702087402, loss=1.7467641830444336
I0210 15:28:12.538455 140050897565440 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.21639330685138702, loss=1.7631992101669312
I0210 15:28:47.945889 140050979010304 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.20551423728466034, loss=1.7434059381484985
I0210 15:29:23.365254 140050897565440 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1808052659034729, loss=1.719459891319275
I0210 15:29:58.770823 140050979010304 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.22863498330116272, loss=1.6530704498291016
I0210 15:30:34.174496 140050897565440 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2092464417219162, loss=1.7997153997421265
I0210 15:31:09.582531 140050979010304 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.19599825143814087, loss=1.7414147853851318
I0210 15:31:44.988808 140050897565440 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.1920519471168518, loss=1.7276774644851685
I0210 15:32:20.383083 140050979010304 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.23331795632839203, loss=1.6940850019454956
I0210 15:32:55.808483 140050897565440 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.20649048686027527, loss=1.697332501411438
I0210 15:33:31.239433 140050979010304 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.19484329223632812, loss=1.7296013832092285
I0210 15:34:06.636600 140050897565440 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20692850649356842, loss=1.651930809020996
I0210 15:34:42.041817 140050979010304 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.19286110997200012, loss=1.6486324071884155
I0210 15:35:17.457031 140050897565440 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.21500837802886963, loss=1.7098796367645264
I0210 15:35:52.859059 140050979010304 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.19400830566883087, loss=1.6893572807312012
I0210 15:36:04.975448 140225696298816 spec.py:321] Evaluating on the training split.
I0210 15:36:07.949728 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:39:48.832982 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 15:39:51.500525 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:42:18.092690 140225696298816 spec.py:349] Evaluating on the test split.
I0210 15:42:20.777284 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 15:44:39.243131 140225696298816 submission_runner.py:408] Time since start: 37275.05s, 	Step: 61736, 	{'train/accuracy': 0.6519191861152649, 'train/loss': 1.6469298601150513, 'train/bleu': 32.12458710119513, 'validation/accuracy': 0.673072874546051, 'validation/loss': 1.5103808641433716, 'validation/bleu': 29.256527875615028, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.4241111278533936, 'test/bleu': 28.778837217794965, 'test/num_examples': 3003, 'score': 21872.760452747345, 'total_duration': 37275.04721617699, 'accumulated_submission_time': 21872.760452747345, 'accumulated_eval_time': 15399.552985191345, 'accumulated_logging_time': 0.7798454761505127}
I0210 15:44:39.266054 140050897565440 logging_writer.py:48] [61736] accumulated_eval_time=15399.552985, accumulated_logging_time=0.779845, accumulated_submission_time=21872.760453, global_step=61736, preemption_count=0, score=21872.760453, test/accuracy=0.685062, test/bleu=28.778837, test/loss=1.424111, test/num_examples=3003, total_duration=37275.047216, train/accuracy=0.651919, train/bleu=32.124587, train/loss=1.646930, validation/accuracy=0.673073, validation/bleu=29.256528, validation/loss=1.510381, validation/num_examples=3000
I0210 15:45:02.162489 140050979010304 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.21451784670352936, loss=1.7395018339157104
I0210 15:45:37.481868 140050897565440 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.19930502772331238, loss=1.7397944927215576
I0210 15:46:12.877072 140050979010304 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.24088987708091736, loss=1.7533953189849854
I0210 15:46:48.270611 140050897565440 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.18772374093532562, loss=1.657095193862915
I0210 15:47:23.680405 140050979010304 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2054859697818756, loss=1.7443715333938599
I0210 15:47:59.081921 140050897565440 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.18826603889465332, loss=1.7209181785583496
I0210 15:48:34.468244 140050979010304 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.22242261469364166, loss=1.6793030500411987
I0210 15:49:09.865091 140050897565440 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2187589406967163, loss=1.690219521522522
I0210 15:49:45.266865 140050979010304 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.19802634418010712, loss=1.6493526697158813
I0210 15:50:20.652290 140050897565440 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.20367220044136047, loss=1.6953147649765015
I0210 15:50:56.022744 140050979010304 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.1899101436138153, loss=1.7681151628494263
I0210 15:51:31.408975 140050897565440 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2151099443435669, loss=1.7247158288955688
I0210 15:52:06.800720 140050979010304 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.19430075585842133, loss=1.7308238744735718
I0210 15:52:42.213043 140050897565440 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.18665479123592377, loss=1.6749942302703857
I0210 15:53:17.632251 140050979010304 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.20046114921569824, loss=1.6492034196853638
I0210 15:53:53.030192 140050897565440 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.21874399483203888, loss=1.6551016569137573
I0210 15:54:28.444381 140050979010304 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.21942050755023956, loss=1.737876296043396
I0210 15:55:03.850158 140050897565440 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.1964489072561264, loss=1.7215051651000977
I0210 15:55:39.265480 140050979010304 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.209570974111557, loss=1.7213722467422485
I0210 15:56:14.679002 140050897565440 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20484058558940887, loss=1.651299238204956
I0210 15:56:50.175662 140050979010304 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20926131308078766, loss=1.7055902481079102
I0210 15:57:25.578330 140050897565440 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.20343652367591858, loss=1.7018135786056519
I0210 15:58:00.998828 140050979010304 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.19282396137714386, loss=1.7177352905273438
I0210 15:58:36.416382 140050897565440 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2149110585451126, loss=1.683192253112793
I0210 15:58:39.330557 140225696298816 spec.py:321] Evaluating on the training split.
I0210 15:58:42.303449 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:02:32.378023 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 16:02:35.053445 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:06:14.119154 140225696298816 spec.py:349] Evaluating on the test split.
I0210 16:06:16.793293 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:09:55.054708 140225696298816 submission_runner.py:408] Time since start: 38790.86s, 	Step: 64110, 	{'train/accuracy': 0.6602867245674133, 'train/loss': 1.5844924449920654, 'train/bleu': 32.69674771030805, 'validation/accuracy': 0.6743127703666687, 'validation/loss': 1.5003536939620972, 'validation/bleu': 29.04694521750138, 'validation/num_examples': 3000, 'test/accuracy': 0.6873162388801575, 'test/loss': 1.4133727550506592, 'test/bleu': 28.827913304890142, 'test/num_examples': 3003, 'score': 22712.739403009415, 'total_duration': 38790.858786821365, 'accumulated_submission_time': 22712.739403009415, 'accumulated_eval_time': 16075.27707862854, 'accumulated_logging_time': 0.812446117401123}
I0210 16:09:55.080247 140050979010304 logging_writer.py:48] [64110] accumulated_eval_time=16075.277079, accumulated_logging_time=0.812446, accumulated_submission_time=22712.739403, global_step=64110, preemption_count=0, score=22712.739403, test/accuracy=0.687316, test/bleu=28.827913, test/loss=1.413373, test/num_examples=3003, total_duration=38790.858787, train/accuracy=0.660287, train/bleu=32.696748, train/loss=1.584492, validation/accuracy=0.674313, validation/bleu=29.046945, validation/loss=1.500354, validation/num_examples=3000
I0210 16:10:27.149049 140050897565440 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.19478943943977356, loss=1.7302254438400269
I0210 16:11:02.493897 140050979010304 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.201011061668396, loss=1.7152544260025024
I0210 16:11:37.874589 140050897565440 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2119816392660141, loss=1.7352137565612793
I0210 16:12:13.293970 140050979010304 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.20804455876350403, loss=1.7535954713821411
I0210 16:12:48.697743 140050897565440 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.19453799724578857, loss=1.7258858680725098
I0210 16:13:24.080311 140050979010304 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.20829394459724426, loss=1.7233269214630127
I0210 16:13:59.453605 140050897565440 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.19628359377384186, loss=1.7007592916488647
I0210 16:14:34.850359 140050979010304 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2078382521867752, loss=1.6714109182357788
I0210 16:15:10.248588 140050897565440 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19410178065299988, loss=1.6891210079193115
I0210 16:15:45.640406 140050979010304 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19869916141033173, loss=1.6603673696517944
I0210 16:16:21.048256 140050897565440 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.19766470789909363, loss=1.709788203239441
I0210 16:16:56.457578 140050979010304 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.19398950040340424, loss=1.7009319067001343
I0210 16:17:31.840729 140050897565440 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.20972517132759094, loss=1.736928105354309
I0210 16:18:07.236205 140050979010304 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2067689150571823, loss=1.724023461341858
I0210 16:18:42.632621 140050897565440 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.2229810357093811, loss=1.7124762535095215
I0210 16:19:18.027363 140050979010304 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.19261081516742706, loss=1.639398455619812
I0210 16:19:53.400024 140050897565440 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.21365511417388916, loss=1.7283527851104736
I0210 16:20:28.793021 140050979010304 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.20053733885288239, loss=1.6793633699417114
I0210 16:21:04.203659 140050897565440 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2036270946264267, loss=1.707118272781372
I0210 16:21:39.594530 140050979010304 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.21492865681648254, loss=1.6426270008087158
I0210 16:22:14.989470 140050897565440 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.19447195529937744, loss=1.652060866355896
I0210 16:22:50.384177 140050979010304 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2351398766040802, loss=1.6213390827178955
I0210 16:23:25.763717 140050897565440 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.18352185189723969, loss=1.5994696617126465
I0210 16:23:55.214312 140225696298816 spec.py:321] Evaluating on the training split.
I0210 16:23:58.183935 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:27:55.140021 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 16:27:57.807459 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:31:34.581901 140225696298816 spec.py:349] Evaluating on the test split.
I0210 16:31:37.265016 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:35:25.540703 140225696298816 submission_runner.py:408] Time since start: 40321.34s, 	Step: 66485, 	{'train/accuracy': 0.6578103303909302, 'train/loss': 1.6158467531204224, 'train/bleu': 32.83270106756466, 'validation/accuracy': 0.6757386922836304, 'validation/loss': 1.4923642873764038, 'validation/bleu': 29.64027073611295, 'validation/num_examples': 3000, 'test/accuracy': 0.691046416759491, 'test/loss': 1.3991174697875977, 'test/bleu': 29.41162720824399, 'test/num_examples': 3003, 'score': 23552.78756380081, 'total_duration': 40321.3447868824, 'accumulated_submission_time': 23552.78756380081, 'accumulated_eval_time': 16765.60342478752, 'accumulated_logging_time': 0.8476648330688477}
I0210 16:35:25.565608 140050979010304 logging_writer.py:48] [66485] accumulated_eval_time=16765.603425, accumulated_logging_time=0.847665, accumulated_submission_time=23552.787564, global_step=66485, preemption_count=0, score=23552.787564, test/accuracy=0.691046, test/bleu=29.411627, test/loss=1.399117, test/num_examples=3003, total_duration=40321.344787, train/accuracy=0.657810, train/bleu=32.832701, train/loss=1.615847, validation/accuracy=0.675739, validation/bleu=29.640271, validation/loss=1.492364, validation/num_examples=3000
I0210 16:35:31.213362 140050897565440 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.213545024394989, loss=1.6790211200714111
I0210 16:36:06.462977 140050979010304 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.21784818172454834, loss=1.7802375555038452
I0210 16:36:41.805775 140050897565440 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.22713446617126465, loss=1.6583878993988037
I0210 16:37:17.162972 140050979010304 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2226668745279312, loss=1.6983264684677124
I0210 16:37:52.548550 140050897565440 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.23156386613845825, loss=1.654049277305603
I0210 16:38:27.941386 140050979010304 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.22162795066833496, loss=1.734106183052063
I0210 16:39:03.360073 140050897565440 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.23360517621040344, loss=1.7145419120788574
I0210 16:39:38.731910 140050979010304 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.19685839116573334, loss=1.6581637859344482
I0210 16:40:14.110000 140050897565440 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.20758318901062012, loss=1.5993056297302246
I0210 16:40:49.515920 140050979010304 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.1911941021680832, loss=1.7222477197647095
I0210 16:41:24.890388 140050897565440 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.1932460218667984, loss=1.5959035158157349
I0210 16:42:00.248938 140050979010304 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2089373618364334, loss=1.5881556272506714
I0210 16:42:35.675302 140050897565440 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.186686173081398, loss=1.6696451902389526
I0210 16:43:11.070856 140050979010304 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.19983385503292084, loss=1.697117567062378
I0210 16:43:46.470224 140050897565440 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.21824921667575836, loss=1.7282830476760864
I0210 16:44:21.866006 140050979010304 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.19585631787776947, loss=1.6280831098556519
I0210 16:44:57.252790 140050897565440 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.24653999507427216, loss=1.597923755645752
I0210 16:45:32.644399 140050979010304 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.21145470440387726, loss=1.7431957721710205
I0210 16:46:08.041023 140050897565440 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.19943039119243622, loss=1.7382080554962158
I0210 16:46:43.457928 140050979010304 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.20035479962825775, loss=1.7106893062591553
I0210 16:47:18.855325 140050897565440 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.19809286296367645, loss=1.6843701601028442
I0210 16:47:54.242486 140050979010304 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.20448695123195648, loss=1.7207083702087402
I0210 16:48:29.656414 140050897565440 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.21105007827281952, loss=1.6811943054199219
I0210 16:49:05.077816 140050979010304 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.20952774584293365, loss=1.7426269054412842
I0210 16:49:25.707569 140225696298816 spec.py:321] Evaluating on the training split.
I0210 16:49:28.687965 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:52:48.598474 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 16:52:51.270720 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:55:40.063742 140225696298816 spec.py:349] Evaluating on the test split.
I0210 16:55:42.731820 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 16:58:04.610414 140225696298816 submission_runner.py:408] Time since start: 41680.41s, 	Step: 68860, 	{'train/accuracy': 0.6925715208053589, 'train/loss': 1.3943202495574951, 'train/bleu': 35.164799300747305, 'validation/accuracy': 0.675986647605896, 'validation/loss': 1.4907090663909912, 'validation/bleu': 29.37226423439722, 'validation/num_examples': 3000, 'test/accuracy': 0.6896519660949707, 'test/loss': 1.3980993032455444, 'test/bleu': 29.145196225741316, 'test/num_examples': 3003, 'score': 24392.84364748001, 'total_duration': 41680.414498806, 'accumulated_submission_time': 24392.84364748001, 'accumulated_eval_time': 17284.506219387054, 'accumulated_logging_time': 0.8824207782745361}
I0210 16:58:04.634977 140050897565440 logging_writer.py:48] [68860] accumulated_eval_time=17284.506219, accumulated_logging_time=0.882421, accumulated_submission_time=24392.843647, global_step=68860, preemption_count=0, score=24392.843647, test/accuracy=0.689652, test/bleu=29.145196, test/loss=1.398099, test/num_examples=3003, total_duration=41680.414499, train/accuracy=0.692572, train/bleu=35.164799, train/loss=1.394320, validation/accuracy=0.675987, validation/bleu=29.372264, validation/loss=1.490709, validation/num_examples=3000
I0210 16:58:19.085853 140050979010304 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.21261407434940338, loss=1.66264009475708
I0210 16:58:54.349488 140050897565440 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.21162106096744537, loss=1.6786808967590332
I0210 16:59:29.735852 140050979010304 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.20107588171958923, loss=1.7143580913543701
I0210 17:00:05.121907 140050897565440 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.24122673273086548, loss=1.7036750316619873
I0210 17:00:40.517075 140050979010304 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.20665650069713593, loss=1.597402811050415
I0210 17:01:15.918782 140050897565440 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.18327096104621887, loss=1.637406587600708
I0210 17:01:51.317723 140050979010304 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.19887250661849976, loss=1.6267476081848145
I0210 17:02:26.700118 140050897565440 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19077883660793304, loss=1.6540803909301758
I0210 17:03:02.114558 140050979010304 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.1835823506116867, loss=1.6964384317398071
I0210 17:03:37.527462 140050897565440 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.18824152648448944, loss=1.5857219696044922
I0210 17:04:12.937752 140050979010304 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19741323590278625, loss=1.6509760618209839
I0210 17:04:48.354323 140050897565440 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.19096368551254272, loss=1.617264986038208
I0210 17:05:23.765466 140050979010304 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.20470623672008514, loss=1.6486685276031494
I0210 17:05:59.181820 140050897565440 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4540942907333374, loss=1.6101793050765991
I0210 17:06:34.573692 140050979010304 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.19438351690769196, loss=1.5930064916610718
I0210 17:07:09.950740 140050897565440 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.19607943296432495, loss=1.6720534563064575
I0210 17:07:45.347565 140050979010304 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.19633473455905914, loss=1.6660237312316895
I0210 17:08:20.750935 140050897565440 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.23602741956710815, loss=1.7185730934143066
I0210 17:08:56.126466 140050979010304 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.21835635602474213, loss=1.61361825466156
I0210 17:09:31.530646 140050897565440 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.20902635157108307, loss=1.731615662574768
I0210 17:10:06.937878 140050979010304 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.2057304084300995, loss=1.6500598192214966
I0210 17:10:42.350480 140050897565440 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.20256823301315308, loss=1.745847225189209
I0210 17:11:17.752454 140050979010304 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.199105367064476, loss=1.684954285621643
I0210 17:11:53.129184 140050897565440 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.20748783648014069, loss=1.65708589553833
I0210 17:12:04.905800 140225696298816 spec.py:321] Evaluating on the training split.
I0210 17:12:07.876562 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:16:14.137273 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 17:16:16.815381 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:20:06.960574 140225696298816 spec.py:349] Evaluating on the test split.
I0210 17:20:09.629459 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:23:50.018335 140225696298816 submission_runner.py:408] Time since start: 43225.82s, 	Step: 71235, 	{'train/accuracy': 0.663612425327301, 'train/loss': 1.5702091455459595, 'train/bleu': 33.04904270814784, 'validation/accuracy': 0.6764578223228455, 'validation/loss': 1.4874941110610962, 'validation/bleu': 29.29003691272471, 'validation/num_examples': 3000, 'test/accuracy': 0.6922782063484192, 'test/loss': 1.3883384466171265, 'test/bleu': 29.602084105542623, 'test/num_examples': 3003, 'score': 25233.028274297714, 'total_duration': 43225.82240843773, 'accumulated_submission_time': 25233.028274297714, 'accumulated_eval_time': 17989.61869287491, 'accumulated_logging_time': 0.9181523323059082}
I0210 17:23:50.044435 140050979010304 logging_writer.py:48] [71235] accumulated_eval_time=17989.618693, accumulated_logging_time=0.918152, accumulated_submission_time=25233.028274, global_step=71235, preemption_count=0, score=25233.028274, test/accuracy=0.692278, test/bleu=29.602084, test/loss=1.388338, test/num_examples=3003, total_duration=43225.822408, train/accuracy=0.663612, train/bleu=33.049043, train/loss=1.570209, validation/accuracy=0.676458, validation/bleu=29.290037, validation/loss=1.487494, validation/num_examples=3000
I0210 17:24:13.314869 140050897565440 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.21988900005817413, loss=1.7026278972625732
I0210 17:24:48.584297 140050979010304 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2405630350112915, loss=1.6921052932739258
I0210 17:25:23.954968 140050897565440 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.22419020533561707, loss=1.7165313959121704
I0210 17:25:59.346538 140050979010304 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.1929142028093338, loss=1.641915202140808
I0210 17:26:34.714685 140050897565440 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.20899707078933716, loss=1.7010424137115479
I0210 17:27:10.109805 140050979010304 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.20678560435771942, loss=1.6892226934432983
I0210 17:27:45.514225 140050897565440 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.19689592719078064, loss=1.6359820365905762
I0210 17:28:20.910308 140050979010304 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20133370161056519, loss=1.6806795597076416
I0210 17:28:56.320103 140050897565440 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.19742323458194733, loss=1.6052582263946533
I0210 17:29:31.762772 140050979010304 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.20369000732898712, loss=1.702598214149475
I0210 17:30:07.154027 140050897565440 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.20412907004356384, loss=1.6400891542434692
I0210 17:30:42.562685 140050979010304 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.19613048434257507, loss=1.6389641761779785
I0210 17:31:17.969013 140050897565440 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.18026494979858398, loss=1.6220974922180176
I0210 17:31:53.381131 140050979010304 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.20065732300281525, loss=1.6012203693389893
I0210 17:32:28.775387 140050897565440 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.18991918861865997, loss=1.5662412643432617
I0210 17:33:04.188020 140050979010304 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2525792419910431, loss=1.6808264255523682
I0210 17:33:39.620766 140050897565440 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.22611986100673676, loss=1.559071660041809
I0210 17:34:15.040099 140050979010304 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.20551680028438568, loss=1.703639268875122
I0210 17:34:50.438307 140050897565440 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.20120969414710999, loss=1.5952732563018799
I0210 17:35:25.817382 140050979010304 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.206475168466568, loss=1.6372358798980713
I0210 17:36:01.220526 140050897565440 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.20588433742523193, loss=1.608206868171692
I0210 17:36:36.632234 140050979010304 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2086252123117447, loss=1.6487796306610107
I0210 17:37:12.042704 140050897565440 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1859883964061737, loss=1.593360185623169
I0210 17:37:47.437628 140050979010304 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.20524229109287262, loss=1.6388075351715088
I0210 17:37:50.346970 140225696298816 spec.py:321] Evaluating on the training split.
I0210 17:37:53.309676 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:42:17.179774 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 17:42:19.864193 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:45:10.818540 140225696298816 spec.py:349] Evaluating on the test split.
I0210 17:45:13.497109 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 17:48:07.115351 140225696298816 submission_runner.py:408] Time since start: 44682.92s, 	Step: 73610, 	{'train/accuracy': 0.6637750864028931, 'train/loss': 1.5700886249542236, 'train/bleu': 32.387814931952754, 'validation/accuracy': 0.6769413948059082, 'validation/loss': 1.4775288105010986, 'validation/bleu': 29.41112830125952, 'validation/num_examples': 3000, 'test/accuracy': 0.6927198171615601, 'test/loss': 1.3803156614303589, 'test/bleu': 29.329502689351866, 'test/num_examples': 3003, 'score': 26073.243137598038, 'total_duration': 44682.91942191124, 'accumulated_submission_time': 26073.243137598038, 'accumulated_eval_time': 18606.387011051178, 'accumulated_logging_time': 0.9560637474060059}
I0210 17:48:07.141546 140050897565440 logging_writer.py:48] [73610] accumulated_eval_time=18606.387011, accumulated_logging_time=0.956064, accumulated_submission_time=26073.243138, global_step=73610, preemption_count=0, score=26073.243138, test/accuracy=0.692720, test/bleu=29.329503, test/loss=1.380316, test/num_examples=3003, total_duration=44682.919422, train/accuracy=0.663775, train/bleu=32.387815, train/loss=1.570089, validation/accuracy=0.676941, validation/bleu=29.411128, validation/loss=1.477529, validation/num_examples=3000
I0210 17:48:39.163929 140050979010304 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.20041966438293457, loss=1.654172658920288
I0210 17:49:14.481735 140050897565440 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.1919924020767212, loss=1.667176365852356
I0210 17:49:49.865786 140050979010304 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.22317104041576385, loss=1.6032525300979614
I0210 17:50:25.267018 140050897565440 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.26039668917655945, loss=1.6552428007125854
I0210 17:51:00.648492 140050979010304 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20674490928649902, loss=1.6095818281173706
I0210 17:51:36.058565 140050897565440 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.21060457825660706, loss=1.6891887187957764
I0210 17:52:11.451501 140050979010304 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.20266738533973694, loss=1.6039756536483765
I0210 17:52:46.877134 140050897565440 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.21563638746738434, loss=1.660918116569519
I0210 17:53:22.289637 140050979010304 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.20351964235305786, loss=1.704673409461975
I0210 17:53:57.687397 140050897565440 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.20313842594623566, loss=1.617598533630371
I0210 17:54:33.084263 140050979010304 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.20163659751415253, loss=1.607149362564087
I0210 17:55:08.492415 140050897565440 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.1896243691444397, loss=1.6179994344711304
I0210 17:55:43.898452 140050979010304 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.19046472012996674, loss=1.6308327913284302
I0210 17:56:19.289939 140050897565440 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.1919395476579666, loss=1.644437551498413
I0210 17:56:54.706585 140050979010304 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2159518003463745, loss=1.6530262231826782
I0210 17:57:30.120294 140050897565440 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3306061327457428, loss=1.6872351169586182
I0210 17:58:05.556589 140050979010304 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.21283245086669922, loss=1.5532677173614502
I0210 17:58:40.961750 140050897565440 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.20198914408683777, loss=1.5901027917861938
I0210 17:59:16.384751 140050979010304 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.18377292156219482, loss=1.5707659721374512
I0210 17:59:51.795271 140050897565440 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20593887567520142, loss=1.6100914478302002
I0210 18:00:27.201100 140050979010304 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.21018870174884796, loss=1.673109531402588
I0210 18:01:02.602018 140050897565440 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2075873762369156, loss=1.7183113098144531
I0210 18:01:37.997633 140050979010304 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.20674823224544525, loss=1.6941195726394653
I0210 18:02:07.455100 140225696298816 spec.py:321] Evaluating on the training split.
I0210 18:02:10.422132 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:05:36.938250 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 18:05:39.608417 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:08:49.581381 140225696298816 spec.py:349] Evaluating on the test split.
I0210 18:08:52.265751 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:11:53.333738 140225696298816 submission_runner.py:408] Time since start: 46109.14s, 	Step: 75985, 	{'train/accuracy': 0.6715718507766724, 'train/loss': 1.5168931484222412, 'train/bleu': 33.4930723722878, 'validation/accuracy': 0.6797311902046204, 'validation/loss': 1.4644559621810913, 'validation/bleu': 29.64138507890422, 'validation/num_examples': 3000, 'test/accuracy': 0.6943234205245972, 'test/loss': 1.3734365701675415, 'test/bleu': 29.62003987298333, 'test/num_examples': 3003, 'score': 26913.46995139122, 'total_duration': 46109.13782072067, 'accumulated_submission_time': 26913.46995139122, 'accumulated_eval_time': 19192.265601158142, 'accumulated_logging_time': 0.9935698509216309}
I0210 18:11:53.361033 140050897565440 logging_writer.py:48] [75985] accumulated_eval_time=19192.265601, accumulated_logging_time=0.993570, accumulated_submission_time=26913.469951, global_step=75985, preemption_count=0, score=26913.469951, test/accuracy=0.694323, test/bleu=29.620040, test/loss=1.373437, test/num_examples=3003, total_duration=46109.137821, train/accuracy=0.671572, train/bleu=33.493072, train/loss=1.516893, validation/accuracy=0.679731, validation/bleu=29.641385, validation/loss=1.464456, validation/num_examples=3000
I0210 18:11:59.012935 140050979010304 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.19512493908405304, loss=1.6574794054031372
I0210 18:12:34.325663 140050897565440 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.21152375638484955, loss=1.6382920742034912
I0210 18:13:09.717826 140050979010304 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21379698812961578, loss=1.6543986797332764
I0210 18:13:45.100214 140050897565440 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.20350737869739532, loss=1.6475056409835815
I0210 18:14:20.505433 140050979010304 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2006182223558426, loss=1.6591887474060059
I0210 18:14:55.927705 140050897565440 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2069050371646881, loss=1.6552470922470093
I0210 18:15:31.337373 140050979010304 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.23086629807949066, loss=1.5605422258377075
I0210 18:16:06.746155 140050897565440 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.25880563259124756, loss=1.7315127849578857
I0210 18:16:42.141734 140050979010304 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.19353243708610535, loss=1.7501434087753296
I0210 18:17:17.541576 140050897565440 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.18058647215366364, loss=1.5787279605865479
I0210 18:17:52.930062 140050979010304 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.20168736577033997, loss=1.5611708164215088
I0210 18:18:28.339264 140050897565440 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.21278168261051178, loss=1.6530400514602661
I0210 18:19:03.765871 140050979010304 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.19091396033763885, loss=1.604941487312317
I0210 18:19:39.184092 140050897565440 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21071380376815796, loss=1.6844053268432617
I0210 18:20:14.587875 140050979010304 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.1929837465286255, loss=1.6235979795455933
I0210 18:20:49.993715 140050897565440 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.20606018602848053, loss=1.6134265661239624
I0210 18:21:25.403916 140050979010304 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2162620574235916, loss=1.625627875328064
I0210 18:22:00.836305 140050897565440 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.19524279236793518, loss=1.5559149980545044
I0210 18:22:36.272387 140050979010304 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.208841010928154, loss=1.616928219795227
I0210 18:23:11.660423 140050897565440 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.26452046632766724, loss=1.653451681137085
I0210 18:23:47.066580 140050979010304 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.23362338542938232, loss=1.7083210945129395
I0210 18:24:22.493689 140050897565440 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.2200382500886917, loss=1.6779019832611084
I0210 18:24:57.911978 140050979010304 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.20669761300086975, loss=1.7361299991607666
I0210 18:25:33.323274 140050897565440 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.20780585706233978, loss=1.622732162475586
I0210 18:25:53.612086 140225696298816 spec.py:321] Evaluating on the training split.
I0210 18:25:56.587708 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:29:24.900229 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 18:29:27.572715 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:31:57.315388 140225696298816 spec.py:349] Evaluating on the test split.
I0210 18:31:59.989166 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:34:15.305499 140225696298816 submission_runner.py:408] Time since start: 47451.11s, 	Step: 78359, 	{'train/accuracy': 0.6638250946998596, 'train/loss': 1.5705655813217163, 'train/bleu': 32.52902754344788, 'validation/accuracy': 0.6803511381149292, 'validation/loss': 1.4611443281173706, 'validation/bleu': 29.507351771603417, 'validation/num_examples': 3000, 'test/accuracy': 0.6962175369262695, 'test/loss': 1.3679414987564087, 'test/bleu': 29.716893139832724, 'test/num_examples': 3003, 'score': 27753.634783506393, 'total_duration': 47451.10957407951, 'accumulated_submission_time': 27753.634783506393, 'accumulated_eval_time': 19693.95895600319, 'accumulated_logging_time': 1.03102707862854}
I0210 18:34:15.332194 140050979010304 logging_writer.py:48] [78359] accumulated_eval_time=19693.958956, accumulated_logging_time=1.031027, accumulated_submission_time=27753.634784, global_step=78359, preemption_count=0, score=27753.634784, test/accuracy=0.696218, test/bleu=29.716893, test/loss=1.367941, test/num_examples=3003, total_duration=47451.109574, train/accuracy=0.663825, train/bleu=32.529028, train/loss=1.570566, validation/accuracy=0.680351, validation/bleu=29.507352, validation/loss=1.461144, validation/num_examples=3000
I0210 18:34:30.144538 140050897565440 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.22512255609035492, loss=1.535353183746338
I0210 18:35:05.417504 140050979010304 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2026659995317459, loss=1.5790475606918335
I0210 18:35:40.771142 140050897565440 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2409971058368683, loss=1.6525464057922363
I0210 18:36:16.168320 140050979010304 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.20676559209823608, loss=1.6888501644134521
I0210 18:36:51.574973 140050897565440 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.19137361645698547, loss=1.6093833446502686
I0210 18:37:26.978035 140050979010304 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19576981663703918, loss=1.5788254737854004
I0210 18:38:02.390483 140050897565440 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21246594190597534, loss=1.616183876991272
I0210 18:38:37.791948 140050979010304 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.1990499496459961, loss=1.658231496810913
I0210 18:39:13.201774 140050897565440 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.18987761437892914, loss=1.597631812095642
I0210 18:39:48.588298 140050979010304 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20214320719242096, loss=1.6201163530349731
I0210 18:40:24.004496 140050897565440 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2266424596309662, loss=1.7033942937850952
I0210 18:40:59.398768 140050979010304 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.20627480745315552, loss=1.633057713508606
I0210 18:41:34.824250 140050897565440 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.21133457124233246, loss=1.6432955265045166
I0210 18:42:10.233230 140050979010304 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.20228874683380127, loss=1.6778514385223389
I0210 18:42:45.649034 140050897565440 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.3217889368534088, loss=1.6067475080490112
I0210 18:43:21.037946 140050979010304 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.21521522104740143, loss=1.5257922410964966
I0210 18:43:56.434759 140050897565440 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.19457845389842987, loss=1.6622527837753296
I0210 18:44:31.821984 140050979010304 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2069675773382187, loss=1.6584787368774414
I0210 18:45:07.236161 140050897565440 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.2195989489555359, loss=1.550523042678833
I0210 18:45:42.672965 140050979010304 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20306062698364258, loss=1.6531462669372559
I0210 18:46:18.099920 140050897565440 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20324862003326416, loss=1.607631802558899
I0210 18:46:53.501780 140050979010304 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.19680622220039368, loss=1.617720603942871
I0210 18:47:28.906129 140050897565440 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.21679383516311646, loss=1.5979987382888794
I0210 18:48:04.311154 140050979010304 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20419661700725555, loss=1.6409125328063965
I0210 18:48:15.364904 140225696298816 spec.py:321] Evaluating on the training split.
I0210 18:48:18.328879 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:52:07.154639 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 18:52:09.830230 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:54:40.486239 140225696298816 spec.py:349] Evaluating on the test split.
I0210 18:54:43.167462 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 18:56:58.162128 140225696298816 submission_runner.py:408] Time since start: 48813.97s, 	Step: 80733, 	{'train/accuracy': 0.6645106077194214, 'train/loss': 1.572019338607788, 'train/bleu': 33.2519705355767, 'validation/accuracy': 0.6811570525169373, 'validation/loss': 1.4543503522872925, 'validation/bleu': 30.089135107250858, 'validation/num_examples': 3000, 'test/accuracy': 0.6961246132850647, 'test/loss': 1.3614065647125244, 'test/bleu': 29.882335007945937, 'test/num_examples': 3003, 'score': 28593.58155298233, 'total_duration': 48813.9661796093, 'accumulated_submission_time': 28593.58155298233, 'accumulated_eval_time': 20216.756098031998, 'accumulated_logging_time': 1.0682952404022217}
I0210 18:56:58.188484 140050897565440 logging_writer.py:48] [80733] accumulated_eval_time=20216.756098, accumulated_logging_time=1.068295, accumulated_submission_time=28593.581553, global_step=80733, preemption_count=0, score=28593.581553, test/accuracy=0.696125, test/bleu=29.882335, test/loss=1.361407, test/num_examples=3003, total_duration=48813.966180, train/accuracy=0.664511, train/bleu=33.251971, train/loss=1.572019, validation/accuracy=0.681157, validation/bleu=30.089135, validation/loss=1.454350, validation/num_examples=3000
I0210 18:57:22.150408 140050979010304 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.000563859939575, loss=1.707642674446106
I0210 18:57:57.430078 140050897565440 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.19732803106307983, loss=1.6264077425003052
I0210 18:58:32.808894 140050979010304 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.19466029107570648, loss=1.6066813468933105
I0210 18:59:08.217833 140050897565440 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.21275325119495392, loss=1.6574840545654297
I0210 18:59:43.590142 140050979010304 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.20742906630039215, loss=1.6010284423828125
I0210 19:00:18.979176 140050897565440 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.19736424088478088, loss=1.6011037826538086
I0210 19:00:54.371251 140050979010304 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.1991579383611679, loss=1.6369919776916504
I0210 19:01:29.775750 140050897565440 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.1905379742383957, loss=1.655402660369873
I0210 19:02:05.175953 140050979010304 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21070076525211334, loss=1.6846915483474731
I0210 19:02:40.570045 140050897565440 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2175581455230713, loss=1.6142243146896362
I0210 19:03:15.960789 140050979010304 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.21472887694835663, loss=1.6368415355682373
I0210 19:03:51.347524 140050897565440 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.22763068974018097, loss=1.6651190519332886
I0210 19:04:26.764997 140050979010304 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21408961713314056, loss=1.672809362411499
I0210 19:05:02.169607 140050897565440 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2105795294046402, loss=1.640292763710022
I0210 19:05:37.570326 140050979010304 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.22706486284732819, loss=1.7158997058868408
I0210 19:06:12.994169 140050897565440 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.21939082443714142, loss=1.6663795709609985
I0210 19:06:48.391321 140050979010304 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2009868323802948, loss=1.626493215560913
I0210 19:07:23.797129 140050897565440 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20547203719615936, loss=1.5751190185546875
I0210 19:07:59.191259 140050979010304 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.21947339177131653, loss=1.668831467628479
I0210 19:08:34.564188 140050897565440 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.21273571252822876, loss=1.6682006120681763
I0210 19:09:09.942226 140050979010304 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.2134108990430832, loss=1.6532313823699951
I0210 19:09:45.337165 140050897565440 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.19454023241996765, loss=1.6048651933670044
I0210 19:10:20.755121 140050979010304 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.21993237733840942, loss=1.5433064699172974
I0210 19:10:56.149659 140050897565440 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.29447925090789795, loss=1.6698676347732544
I0210 19:10:58.351077 140225696298816 spec.py:321] Evaluating on the training split.
I0210 19:11:01.319559 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:15:11.019756 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 19:15:13.702594 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:17:47.337114 140225696298816 spec.py:349] Evaluating on the test split.
I0210 19:17:50.019081 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:20:10.448758 140225696298816 submission_runner.py:408] Time since start: 50206.25s, 	Step: 83108, 	{'train/accuracy': 0.6732177138328552, 'train/loss': 1.5030677318572998, 'train/bleu': 33.89015855149095, 'validation/accuracy': 0.6820250153541565, 'validation/loss': 1.4465928077697754, 'validation/bleu': 29.565130484491412, 'validation/num_examples': 3000, 'test/accuracy': 0.6982976198196411, 'test/loss': 1.349581003189087, 'test/bleu': 30.016736440843538, 'test/num_examples': 3003, 'score': 29433.65891289711, 'total_duration': 50206.2528424263, 'accumulated_submission_time': 29433.65891289711, 'accumulated_eval_time': 20768.853723526, 'accumulated_logging_time': 1.1046974658966064}
I0210 19:20:10.475326 140050979010304 logging_writer.py:48] [83108] accumulated_eval_time=20768.853724, accumulated_logging_time=1.104697, accumulated_submission_time=29433.658913, global_step=83108, preemption_count=0, score=29433.658913, test/accuracy=0.698298, test/bleu=30.016736, test/loss=1.349581, test/num_examples=3003, total_duration=50206.252842, train/accuracy=0.673218, train/bleu=33.890159, train/loss=1.503068, validation/accuracy=0.682025, validation/bleu=29.565130, validation/loss=1.446593, validation/num_examples=3000
I0210 19:20:43.238220 140050897565440 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20312272012233734, loss=1.6724051237106323
I0210 19:21:18.585431 140050979010304 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2005591094493866, loss=1.6141811609268188
I0210 19:21:53.990420 140050897565440 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21633516252040863, loss=1.6694161891937256
I0210 19:22:29.401631 140050979010304 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3352825343608856, loss=1.6530628204345703
I0210 19:23:04.811721 140050897565440 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.19589512050151825, loss=1.5902845859527588
I0210 19:23:40.207794 140050979010304 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.1927369236946106, loss=1.5973396301269531
I0210 19:24:15.597078 140050897565440 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2063833624124527, loss=1.5506304502487183
I0210 19:24:50.985442 140050979010304 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20650245249271393, loss=1.6470849514007568
I0210 19:25:26.378708 140050897565440 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.20758169889450073, loss=1.5988786220550537
I0210 19:26:01.771515 140050979010304 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.22332945466041565, loss=1.6314142942428589
I0210 19:26:37.147233 140050897565440 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.1993284970521927, loss=1.6458135843276978
I0210 19:27:12.528969 140050979010304 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20810842514038086, loss=1.6381535530090332
I0210 19:27:47.928074 140050897565440 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.21636752784252167, loss=1.6887962818145752
I0210 19:28:23.323973 140050979010304 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20925064384937286, loss=1.620692253112793
I0210 19:28:58.720431 140050897565440 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.20053081214427948, loss=1.609768033027649
I0210 19:29:34.119205 140050979010304 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.20359724760055542, loss=1.5530706644058228
I0210 19:30:09.516115 140050897565440 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.20809732377529144, loss=1.589950442314148
I0210 19:30:44.918807 140050979010304 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.2313278615474701, loss=1.5867866277694702
I0210 19:31:20.309184 140050897565440 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2326822131872177, loss=1.6121203899383545
I0210 19:31:55.709752 140050979010304 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.20836278796195984, loss=1.6814559698104858
I0210 19:32:31.114005 140050897565440 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.223857581615448, loss=1.5567152500152588
I0210 19:33:06.526960 140050979010304 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2196333259344101, loss=1.6096861362457275
I0210 19:33:41.906362 140050897565440 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.20242708921432495, loss=1.5438930988311768
I0210 19:34:10.633450 140225696298816 spec.py:321] Evaluating on the training split.
I0210 19:34:13.605139 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:37:38.355714 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 19:37:41.028977 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:40:33.167318 140225696298816 spec.py:349] Evaluating on the test split.
I0210 19:40:35.840308 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 19:43:18.656841 140225696298816 submission_runner.py:408] Time since start: 51594.46s, 	Step: 85483, 	{'train/accuracy': 0.6708388328552246, 'train/loss': 1.520539402961731, 'train/bleu': 33.3104744951593, 'validation/accuracy': 0.6837112903594971, 'validation/loss': 1.4417012929916382, 'validation/bleu': 30.142586435709703, 'validation/num_examples': 3000, 'test/accuracy': 0.698448657989502, 'test/loss': 1.3438206911087036, 'test/bleu': 29.82206997277747, 'test/num_examples': 3003, 'score': 30273.729954242706, 'total_duration': 51594.460906744, 'accumulated_submission_time': 30273.729954242706, 'accumulated_eval_time': 21316.877049922943, 'accumulated_logging_time': 1.142566204071045}
I0210 19:43:18.683937 140050979010304 logging_writer.py:48] [85483] accumulated_eval_time=21316.877050, accumulated_logging_time=1.142566, accumulated_submission_time=30273.729954, global_step=85483, preemption_count=0, score=30273.729954, test/accuracy=0.698449, test/bleu=29.822070, test/loss=1.343821, test/num_examples=3003, total_duration=51594.460907, train/accuracy=0.670839, train/bleu=33.310474, train/loss=1.520539, validation/accuracy=0.683711, validation/bleu=30.142586, validation/loss=1.441701, validation/num_examples=3000
I0210 19:43:25.045353 140050897565440 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.20277878642082214, loss=1.5984002351760864
I0210 19:44:00.266041 140050979010304 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.24701537191867828, loss=1.5720058679580688
I0210 19:44:35.604245 140050897565440 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.20139363408088684, loss=1.6423836946487427
I0210 19:45:10.971068 140050979010304 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2087307870388031, loss=1.6966354846954346
I0210 19:45:46.374059 140050897565440 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.23307198286056519, loss=1.622085452079773
I0210 19:46:21.769077 140050979010304 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.2153773009777069, loss=1.6288042068481445
I0210 19:46:57.133648 140050897565440 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.2286531776189804, loss=1.5517048835754395
I0210 19:47:32.508766 140050979010304 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.22694545984268188, loss=1.6248830556869507
I0210 19:48:07.905813 140050897565440 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.20861195027828217, loss=1.5980764627456665
I0210 19:48:43.295185 140050979010304 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.21259386837482452, loss=1.6571011543273926
I0210 19:49:18.709753 140050897565440 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.21373075246810913, loss=1.637684941291809
I0210 19:49:54.099160 140050979010304 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.21666280925273895, loss=1.6637760400772095
I0210 19:50:29.499321 140050897565440 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.21120980381965637, loss=1.601757287979126
I0210 19:51:04.892832 140050979010304 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20658063888549805, loss=1.6365041732788086
I0210 19:51:40.264947 140050897565440 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.20391373336315155, loss=1.5261142253875732
I0210 19:52:15.686398 140050979010304 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2147337794303894, loss=1.6875358819961548
I0210 19:52:51.105233 140050897565440 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.2116364985704422, loss=1.5998947620391846
I0210 19:53:26.502126 140050979010304 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.21658122539520264, loss=1.6136337518692017
I0210 19:54:01.912245 140050897565440 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.22649985551834106, loss=1.6051098108291626
I0210 19:54:37.308643 140050979010304 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.2037404179573059, loss=1.561362862586975
I0210 19:55:12.686024 140050897565440 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.20765836536884308, loss=1.5980466604232788
I0210 19:55:48.087742 140050979010304 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2207387089729309, loss=1.6136246919631958
I0210 19:56:23.467133 140050897565440 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20995600521564484, loss=1.571137547492981
I0210 19:56:58.871183 140050979010304 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.21431998908519745, loss=1.5709137916564941
I0210 19:57:18.783723 140225696298816 spec.py:321] Evaluating on the training split.
I0210 19:57:21.746886 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:01:18.139423 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 20:01:20.807193 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:04:25.948385 140225696298816 spec.py:349] Evaluating on the test split.
I0210 20:04:28.626887 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:07:24.393388 140225696298816 submission_runner.py:408] Time since start: 53040.20s, 	Step: 87858, 	{'train/accuracy': 0.6888561248779297, 'train/loss': 1.4054269790649414, 'train/bleu': 34.55736403714124, 'validation/accuracy': 0.6858067512512207, 'validation/loss': 1.4331176280975342, 'validation/bleu': 29.992162601588777, 'validation/num_examples': 3000, 'test/accuracy': 0.698506772518158, 'test/loss': 1.3392562866210938, 'test/bleu': 29.978959453072076, 'test/num_examples': 3003, 'score': 31113.744668483734, 'total_duration': 53040.19746589661, 'accumulated_submission_time': 31113.744668483734, 'accumulated_eval_time': 21922.486659526825, 'accumulated_logging_time': 1.1804225444793701}
I0210 20:07:24.420949 140050897565440 logging_writer.py:48] [87858] accumulated_eval_time=21922.486660, accumulated_logging_time=1.180423, accumulated_submission_time=31113.744668, global_step=87858, preemption_count=0, score=31113.744668, test/accuracy=0.698507, test/bleu=29.978959, test/loss=1.339256, test/num_examples=3003, total_duration=53040.197466, train/accuracy=0.688856, train/bleu=34.557364, train/loss=1.405427, validation/accuracy=0.685807, validation/bleu=29.992163, validation/loss=1.433118, validation/num_examples=3000
I0210 20:07:39.567102 140050979010304 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.2033383548259735, loss=1.5593478679656982
I0210 20:08:14.798611 140050897565440 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.21903933584690094, loss=1.6055991649627686
I0210 20:08:50.148874 140050979010304 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.19912688434123993, loss=1.5578563213348389
I0210 20:09:25.527127 140050897565440 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.19060549139976501, loss=1.6481801271438599
I0210 20:10:00.871422 140050979010304 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.2065669745206833, loss=1.6046695709228516
I0210 20:10:36.255937 140050897565440 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.1935267597436905, loss=1.6088523864746094
I0210 20:11:11.628661 140050979010304 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.19776083528995514, loss=1.6015822887420654
I0210 20:11:47.013194 140050897565440 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.20897309482097626, loss=1.5687808990478516
I0210 20:12:22.424729 140050979010304 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21654942631721497, loss=1.6019126176834106
I0210 20:12:57.830113 140050897565440 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.21033881604671478, loss=1.7082502841949463
I0210 20:13:33.201084 140050979010304 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20055396854877472, loss=1.5577175617218018
I0210 20:14:08.594939 140050897565440 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.19639460742473602, loss=1.5535244941711426
I0210 20:14:43.992982 140050979010304 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.22747747600078583, loss=1.6378905773162842
I0210 20:15:19.361840 140050897565440 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.22019925713539124, loss=1.672554612159729
I0210 20:15:54.749998 140050979010304 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.20234672725200653, loss=1.6293301582336426
I0210 20:16:30.137660 140050897565440 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.22750453650951385, loss=1.5663329362869263
I0210 20:17:05.527134 140050979010304 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.198726624250412, loss=1.5507278442382812
I0210 20:17:40.947327 140050897565440 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.23566685616970062, loss=1.549715518951416
I0210 20:18:16.349699 140050979010304 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.213870108127594, loss=1.565769910812378
I0210 20:18:51.751666 140050897565440 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.20448702573776245, loss=1.5844767093658447
I0210 20:19:27.137375 140050979010304 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.21110843122005463, loss=1.6206622123718262
I0210 20:20:02.522292 140050897565440 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20535515248775482, loss=1.6157922744750977
I0210 20:20:37.917094 140050979010304 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.22556552290916443, loss=1.5996795892715454
I0210 20:21:13.308194 140050897565440 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.20395709574222565, loss=1.4924721717834473
I0210 20:21:24.704722 140225696298816 spec.py:321] Evaluating on the training split.
I0210 20:21:27.680725 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:25:43.099435 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 20:25:45.773539 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:29:24.060009 140225696298816 spec.py:349] Evaluating on the test split.
I0210 20:29:26.727939 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:32:58.518812 140225696298816 submission_runner.py:408] Time since start: 54574.32s, 	Step: 90234, 	{'train/accuracy': 0.6757356524467468, 'train/loss': 1.4885623455047607, 'train/bleu': 33.67554884277297, 'validation/accuracy': 0.68563312292099, 'validation/loss': 1.4297630786895752, 'validation/bleu': 29.696829275004344, 'validation/num_examples': 3000, 'test/accuracy': 0.7016443014144897, 'test/loss': 1.3300366401672363, 'test/bleu': 29.90019334184747, 'test/num_examples': 3003, 'score': 31953.942955493927, 'total_duration': 54574.322892427444, 'accumulated_submission_time': 31953.942955493927, 'accumulated_eval_time': 22616.300694704056, 'accumulated_logging_time': 1.2178633213043213}
I0210 20:32:58.547090 140050979010304 logging_writer.py:48] [90234] accumulated_eval_time=22616.300695, accumulated_logging_time=1.217863, accumulated_submission_time=31953.942955, global_step=90234, preemption_count=0, score=31953.942955, test/accuracy=0.701644, test/bleu=29.900193, test/loss=1.330037, test/num_examples=3003, total_duration=54574.322892, train/accuracy=0.675736, train/bleu=33.675549, train/loss=1.488562, validation/accuracy=0.685633, validation/bleu=29.696829, validation/loss=1.429763, validation/num_examples=3000
I0210 20:33:22.153356 140050897565440 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.22308069467544556, loss=1.6156151294708252
I0210 20:33:57.428439 140050979010304 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.21491551399230957, loss=1.5857261419296265
I0210 20:34:32.771120 140050897565440 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2090766429901123, loss=1.469779372215271
I0210 20:35:08.143970 140050979010304 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.1947292983531952, loss=1.6065726280212402
I0210 20:35:43.549699 140050897565440 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20182360708713531, loss=1.540895938873291
I0210 20:36:18.916172 140050979010304 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.21258719265460968, loss=1.551767349243164
I0210 20:36:54.296029 140050897565440 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.21435287594795227, loss=1.6324023008346558
I0210 20:37:29.670309 140050979010304 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.20799556374549866, loss=1.5852038860321045
I0210 20:38:05.059742 140050897565440 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.21076709032058716, loss=1.5789248943328857
I0210 20:38:40.429924 140050979010304 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.24857167899608612, loss=1.684525728225708
I0210 20:39:15.812020 140050897565440 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.2154024839401245, loss=1.5411007404327393
I0210 20:39:51.199254 140050979010304 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.21369165182113647, loss=1.5497852563858032
I0210 20:40:26.596562 140050897565440 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.20154869556427002, loss=1.6403590440750122
I0210 20:41:01.975041 140050979010304 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.20939673483371735, loss=1.599900245666504
I0210 20:41:37.355175 140050897565440 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.21950890123844147, loss=1.6107815504074097
I0210 20:42:12.747918 140050979010304 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.22519521415233612, loss=1.6870654821395874
I0210 20:42:48.157473 140050897565440 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.22117792069911957, loss=1.5419158935546875
I0210 20:43:23.564704 140050979010304 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.20814374089241028, loss=1.5867096185684204
I0210 20:43:58.950926 140050897565440 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.23552550375461578, loss=1.5194157361984253
I0210 20:44:34.333016 140050979010304 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.2168651968240738, loss=1.583107352256775
I0210 20:45:09.702476 140050897565440 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21466493606567383, loss=1.5553513765335083
I0210 20:45:45.105947 140050979010304 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.2139482945203781, loss=1.640131950378418
I0210 20:46:20.508499 140050897565440 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.21336421370506287, loss=1.6195231676101685
I0210 20:46:55.914545 140050979010304 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.2007921040058136, loss=1.6123671531677246
I0210 20:46:58.828592 140225696298816 spec.py:321] Evaluating on the training split.
I0210 20:47:01.807974 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:50:58.090035 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 20:51:00.766443 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:53:26.956437 140225696298816 spec.py:349] Evaluating on the test split.
I0210 20:53:29.657138 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 20:55:56.554368 140225696298816 submission_runner.py:408] Time since start: 55952.36s, 	Step: 92610, 	{'train/accuracy': 0.6737697720527649, 'train/loss': 1.50662362575531, 'train/bleu': 33.71377058019944, 'validation/accuracy': 0.6864514946937561, 'validation/loss': 1.4230599403381348, 'validation/bleu': 30.020258720494507, 'validation/num_examples': 3000, 'test/accuracy': 0.7028412222862244, 'test/loss': 1.3216743469238281, 'test/bleu': 30.292554115791944, 'test/num_examples': 3003, 'score': 32794.13788151741, 'total_duration': 55952.358438014984, 'accumulated_submission_time': 32794.13788151741, 'accumulated_eval_time': 23154.026401281357, 'accumulated_logging_time': 1.2572076320648193}
I0210 20:55:56.583422 140050897565440 logging_writer.py:48] [92610] accumulated_eval_time=23154.026401, accumulated_logging_time=1.257208, accumulated_submission_time=32794.137882, global_step=92610, preemption_count=0, score=32794.137882, test/accuracy=0.702841, test/bleu=30.292554, test/loss=1.321674, test/num_examples=3003, total_duration=55952.358438, train/accuracy=0.673770, train/bleu=33.713771, train/loss=1.506624, validation/accuracy=0.686451, validation/bleu=30.020259, validation/loss=1.423060, validation/num_examples=3000
I0210 20:56:28.679042 140050979010304 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.21148353815078735, loss=1.5807167291641235
I0210 20:57:04.003497 140050897565440 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.2099263221025467, loss=1.5155723094940186
I0210 20:57:39.360364 140050979010304 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.21225270628929138, loss=1.5323280096054077
I0210 20:58:14.735416 140050897565440 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.24666520953178406, loss=1.5735177993774414
I0210 20:58:50.122514 140050979010304 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.23630821704864502, loss=1.5410972833633423
I0210 20:59:25.483328 140050897565440 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.20121456682682037, loss=1.5047760009765625
I0210 21:00:00.850356 140050979010304 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.20555445551872253, loss=1.5159962177276611
I0210 21:00:36.239178 140050897565440 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.22130198776721954, loss=1.5663642883300781
I0210 21:01:11.625339 140050979010304 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.23812028765678406, loss=1.634592890739441
I0210 21:01:47.015269 140050897565440 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2129436433315277, loss=1.5729804039001465
I0210 21:02:22.404124 140050979010304 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.23253661394119263, loss=1.5718610286712646
I0210 21:02:57.801601 140050897565440 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2132447361946106, loss=1.5657414197921753
I0210 21:03:33.206020 140050979010304 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.2020202875137329, loss=1.6269721984863281
I0210 21:04:08.617921 140050897565440 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.2137717604637146, loss=1.528668999671936
I0210 21:04:44.064321 140050979010304 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2112380564212799, loss=1.5817869901657104
I0210 21:05:19.479737 140050897565440 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.20138244330883026, loss=1.546700358390808
I0210 21:05:54.896890 140050979010304 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.20721520483493805, loss=1.6161848306655884
I0210 21:06:30.276103 140050897565440 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21791884303092957, loss=1.5721865892410278
I0210 21:07:05.697219 140050979010304 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.21303747594356537, loss=1.6136531829833984
I0210 21:07:41.110837 140050897565440 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.2122759371995926, loss=1.5730713605880737
I0210 21:08:16.508163 140050979010304 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.21549510955810547, loss=1.5608173608779907
I0210 21:08:51.909937 140050897565440 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.21391557157039642, loss=1.5017902851104736
I0210 21:09:27.312520 140050979010304 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.20906789600849152, loss=1.5336573123931885
I0210 21:09:56.795698 140225696298816 spec.py:321] Evaluating on the training split.
I0210 21:09:59.767757 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:13:32.630430 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 21:13:35.313346 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:16:43.393039 140225696298816 spec.py:349] Evaluating on the test split.
I0210 21:16:46.076589 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:19:39.888823 140225696298816 submission_runner.py:408] Time since start: 57375.69s, 	Step: 94985, 	{'train/accuracy': 0.6820225715637207, 'train/loss': 1.4462394714355469, 'train/bleu': 34.13055095861968, 'validation/accuracy': 0.6877161860466003, 'validation/loss': 1.4159722328186035, 'validation/bleu': 30.236845114728684, 'validation/num_examples': 3000, 'test/accuracy': 0.7035384774208069, 'test/loss': 1.316325306892395, 'test/bleu': 30.436514797787588, 'test/num_examples': 3003, 'score': 33634.26413846016, 'total_duration': 57375.692895412445, 'accumulated_submission_time': 33634.26413846016, 'accumulated_eval_time': 23737.11946773529, 'accumulated_logging_time': 1.2959558963775635}
I0210 21:19:39.918060 140050897565440 logging_writer.py:48] [94985] accumulated_eval_time=23737.119468, accumulated_logging_time=1.295956, accumulated_submission_time=33634.264138, global_step=94985, preemption_count=0, score=33634.264138, test/accuracy=0.703538, test/bleu=30.436515, test/loss=1.316325, test/num_examples=3003, total_duration=57375.692895, train/accuracy=0.682023, train/bleu=34.130551, train/loss=1.446239, validation/accuracy=0.687716, validation/bleu=30.236845, validation/loss=1.415972, validation/num_examples=3000
I0210 21:19:45.570198 140050979010304 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.2091287076473236, loss=1.5611302852630615
I0210 21:20:20.781427 140050897565440 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2102825939655304, loss=1.5323885679244995
I0210 21:20:56.130329 140050979010304 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.22331489622592926, loss=1.603878140449524
I0210 21:21:31.497279 140050897565440 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21143068373203278, loss=1.5856153964996338
I0210 21:22:06.871360 140050979010304 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2223907858133316, loss=1.4530631303787231
I0210 21:22:42.271115 140050897565440 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21539628505706787, loss=1.621214747428894
I0210 21:23:17.670195 140050979010304 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.22580964863300323, loss=1.5679774284362793
I0210 21:23:53.054455 140050897565440 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.20979560911655426, loss=1.5179604291915894
I0210 21:24:28.438993 140050979010304 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21001988649368286, loss=1.5120435953140259
I0210 21:25:03.826104 140050897565440 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.21598705649375916, loss=1.5284448862075806
I0210 21:25:39.226349 140050979010304 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21323291957378387, loss=1.5775549411773682
I0210 21:26:14.607900 140050897565440 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.20911411941051483, loss=1.5648890733718872
I0210 21:26:50.004499 140050979010304 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.20202048122882843, loss=1.533722996711731
I0210 21:27:25.413089 140050897565440 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.21889089047908783, loss=1.636497974395752
I0210 21:28:00.799767 140050979010304 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.23605486750602722, loss=1.5554566383361816
I0210 21:28:36.195431 140050897565440 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.2134382128715515, loss=1.6002044677734375
I0210 21:29:11.591325 140050979010304 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.22498711943626404, loss=1.6000956296920776
I0210 21:29:46.977093 140050897565440 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.2119729220867157, loss=1.517957091331482
I0210 21:30:22.374117 140050979010304 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22522450983524323, loss=1.6379755735397339
I0210 21:30:57.810444 140050897565440 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.20797260105609894, loss=1.5521221160888672
I0210 21:31:33.192296 140050979010304 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.22286398708820343, loss=1.604370355606079
I0210 21:32:08.598414 140050897565440 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.22093544900417328, loss=1.4887852668762207
I0210 21:32:44.007609 140050979010304 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.20835350453853607, loss=1.5570865869522095
I0210 21:33:19.383105 140050897565440 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.22424644231796265, loss=1.6158764362335205
I0210 21:33:39.982321 140225696298816 spec.py:321] Evaluating on the training split.
I0210 21:33:42.950759 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:37:44.920163 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 21:37:47.586736 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:40:54.949893 140225696298816 spec.py:349] Evaluating on the test split.
I0210 21:40:57.625317 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 21:44:00.971821 140225696298816 submission_runner.py:408] Time since start: 58836.78s, 	Step: 97360, 	{'train/accuracy': 0.6778927445411682, 'train/loss': 1.4817765951156616, 'train/bleu': 33.92635535254963, 'validation/accuracy': 0.6891545057296753, 'validation/loss': 1.410452127456665, 'validation/bleu': 30.186419436898866, 'validation/num_examples': 3000, 'test/accuracy': 0.7068619132041931, 'test/loss': 1.306337833404541, 'test/bleu': 30.644997144646442, 'test/num_examples': 3003, 'score': 34474.2428150177, 'total_duration': 58836.77590274811, 'accumulated_submission_time': 34474.2428150177, 'accumulated_eval_time': 24358.108916282654, 'accumulated_logging_time': 1.3352117538452148}
I0210 21:44:01.000697 140050979010304 logging_writer.py:48] [97360] accumulated_eval_time=24358.108916, accumulated_logging_time=1.335212, accumulated_submission_time=34474.242815, global_step=97360, preemption_count=0, score=34474.242815, test/accuracy=0.706862, test/bleu=30.644997, test/loss=1.306338, test/num_examples=3003, total_duration=58836.775903, train/accuracy=0.677893, train/bleu=33.926355, train/loss=1.481777, validation/accuracy=0.689155, validation/bleu=30.186419, validation/loss=1.410452, validation/num_examples=3000
I0210 21:44:15.454853 140050897565440 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.21879538893699646, loss=1.5272276401519775
I0210 21:44:50.712484 140050979010304 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.21999548375606537, loss=1.6309927701950073
I0210 21:45:26.068057 140050897565440 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.20565971732139587, loss=1.5421184301376343
I0210 21:46:01.427226 140050979010304 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.20524199306964874, loss=1.5488154888153076
I0210 21:46:36.818812 140050897565440 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.21491079032421112, loss=1.507678508758545
I0210 21:47:12.192097 140050979010304 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.2035813182592392, loss=1.5882028341293335
I0210 21:47:47.554797 140050897565440 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.23395580053329468, loss=1.6001901626586914
I0210 21:48:22.934904 140050979010304 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.20665793120861053, loss=1.5190385580062866
I0210 21:48:58.306511 140050897565440 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2257550209760666, loss=1.5880035161972046
I0210 21:49:33.700646 140050979010304 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.2136596292257309, loss=1.566308617591858
I0210 21:50:09.075488 140050897565440 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.22110365331172943, loss=1.5466103553771973
I0210 21:50:44.495428 140050979010304 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21055865287780762, loss=1.5136301517486572
I0210 21:51:19.885823 140050897565440 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21123887598514557, loss=1.5412352085113525
I0210 21:51:55.267154 140050979010304 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.22117756307125092, loss=1.4749552011489868
I0210 21:52:30.661581 140050897565440 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.22150911390781403, loss=1.636871099472046
I0210 21:53:06.077089 140050979010304 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.20449377596378326, loss=1.51524019241333
I0210 21:53:41.465475 140050897565440 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.21927843987941742, loss=1.5368430614471436
I0210 21:54:16.840682 140050979010304 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21494214236736298, loss=1.6030722856521606
I0210 21:54:52.226619 140050897565440 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.2020450383424759, loss=1.490549921989441
I0210 21:55:27.614763 140050979010304 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.2210685759782791, loss=1.5606733560562134
I0210 21:56:03.030057 140050897565440 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.20978513360023499, loss=1.5281541347503662
I0210 21:56:38.442835 140050979010304 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.23567207157611847, loss=1.5569926500320435
I0210 21:57:13.838984 140050897565440 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.2106742560863495, loss=1.50143563747406
I0210 21:57:49.251622 140050979010304 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.20708413422107697, loss=1.5385245084762573
I0210 21:58:00.998597 140225696298816 spec.py:321] Evaluating on the training split.
I0210 21:58:03.979202 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:01:50.627993 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 22:01:53.307902 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:04:28.688293 140225696298816 spec.py:349] Evaluating on the test split.
I0210 22:04:31.368999 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:06:46.994764 140225696298816 submission_runner.py:408] Time since start: 60202.80s, 	Step: 99735, 	{'train/accuracy': 0.6772194504737854, 'train/loss': 1.4831637144088745, 'train/bleu': 33.94047766861321, 'validation/accuracy': 0.6899604201316833, 'validation/loss': 1.4045439958572388, 'validation/bleu': 30.348874012448068, 'validation/num_examples': 3000, 'test/accuracy': 0.7069200277328491, 'test/loss': 1.2959305047988892, 'test/bleu': 30.6400977790549, 'test/num_examples': 3003, 'score': 35314.15435934067, 'total_duration': 60202.79884767532, 'accumulated_submission_time': 35314.15435934067, 'accumulated_eval_time': 24884.105031967163, 'accumulated_logging_time': 1.375361442565918}
I0210 22:06:47.023987 140050897565440 logging_writer.py:48] [99735] accumulated_eval_time=24884.105032, accumulated_logging_time=1.375361, accumulated_submission_time=35314.154359, global_step=99735, preemption_count=0, score=35314.154359, test/accuracy=0.706920, test/bleu=30.640098, test/loss=1.295931, test/num_examples=3003, total_duration=60202.798848, train/accuracy=0.677219, train/bleu=33.940478, train/loss=1.483164, validation/accuracy=0.689960, validation/bleu=30.348874, validation/loss=1.404544, validation/num_examples=3000
I0210 22:07:10.260463 140050979010304 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.21485938131809235, loss=1.5337746143341064
I0210 22:07:45.498838 140050897565440 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.22014503180980682, loss=1.597489595413208
I0210 22:08:20.862851 140050979010304 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.21802403032779694, loss=1.6037367582321167
I0210 22:08:56.246718 140050897565440 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.20477117598056793, loss=1.5384230613708496
I0210 22:09:31.627875 140050979010304 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.22907069325447083, loss=1.5292233228683472
I0210 22:10:07.026244 140050897565440 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.20393753051757812, loss=1.5255439281463623
I0210 22:10:42.423629 140050979010304 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.20834942162036896, loss=1.547569990158081
I0210 22:11:17.789171 140050897565440 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2094995528459549, loss=1.5808894634246826
I0210 22:11:53.182612 140050979010304 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.22696933150291443, loss=1.5370124578475952
I0210 22:12:28.580233 140050897565440 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.23424075543880463, loss=1.5784724950790405
I0210 22:13:03.960944 140050979010304 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.23630130290985107, loss=1.4892197847366333
I0210 22:13:39.363833 140050897565440 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.21340271830558777, loss=1.5030709505081177
I0210 22:14:14.749060 140050979010304 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22111789882183075, loss=1.5480351448059082
I0210 22:14:50.130351 140050897565440 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.24396611750125885, loss=1.4902814626693726
I0210 22:15:25.495434 140050979010304 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.22241942584514618, loss=1.5711448192596436
I0210 22:16:00.876949 140050897565440 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.22882255911827087, loss=1.625124454498291
I0210 22:16:36.285398 140050979010304 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.21215029060840607, loss=1.5309898853302002
I0210 22:17:11.691768 140050897565440 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.24884356558322906, loss=1.5955179929733276
I0210 22:17:47.101225 140050979010304 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2372976392507553, loss=1.6430922746658325
I0210 22:18:22.487951 140050897565440 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.20997510850429535, loss=1.544783115386963
I0210 22:18:57.886443 140050979010304 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.21024267375469208, loss=1.564587950706482
I0210 22:19:33.280513 140050897565440 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22421230375766754, loss=1.5646758079528809
I0210 22:20:08.670247 140050979010304 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.22325126826763153, loss=1.5187792778015137
I0210 22:20:44.084907 140050897565440 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.21802353858947754, loss=1.4560720920562744
I0210 22:20:47.002776 140225696298816 spec.py:321] Evaluating on the training split.
I0210 22:20:49.971637 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:25:05.666009 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 22:25:08.337847 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:27:49.767018 140225696298816 spec.py:349] Evaluating on the test split.
I0210 22:27:52.450460 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:30:07.097647 140225696298816 submission_runner.py:408] Time since start: 61602.90s, 	Step: 102110, 	{'train/accuracy': 0.6823187470436096, 'train/loss': 1.446167230606079, 'train/bleu': 34.765234065491775, 'validation/accuracy': 0.6899108290672302, 'validation/loss': 1.3995331525802612, 'validation/bleu': 30.400901601894276, 'validation/num_examples': 3000, 'test/accuracy': 0.7071989178657532, 'test/loss': 1.2938815355300903, 'test/bleu': 30.771823174514257, 'test/num_examples': 3003, 'score': 36154.047691106796, 'total_duration': 61602.90172743797, 'accumulated_submission_time': 36154.047691106796, 'accumulated_eval_time': 25444.199846982956, 'accumulated_logging_time': 1.414492130279541}
I0210 22:30:07.127934 140050979010304 logging_writer.py:48] [102110] accumulated_eval_time=25444.199847, accumulated_logging_time=1.414492, accumulated_submission_time=36154.047691, global_step=102110, preemption_count=0, score=36154.047691, test/accuracy=0.707199, test/bleu=30.771823, test/loss=1.293882, test/num_examples=3003, total_duration=61602.901727, train/accuracy=0.682319, train/bleu=34.765234, train/loss=1.446167, validation/accuracy=0.689911, validation/bleu=30.400902, validation/loss=1.399533, validation/num_examples=3000
I0210 22:30:39.179175 140050897565440 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2248847931623459, loss=1.5328090190887451
I0210 22:31:14.469907 140050979010304 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.2241588532924652, loss=1.5229851007461548
I0210 22:31:49.826915 140050897565440 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.2201559990644455, loss=1.4655810594558716
I0210 22:32:25.234874 140050979010304 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.21363231539726257, loss=1.5751736164093018
I0210 22:33:00.615914 140050897565440 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.21365714073181152, loss=1.5393954515457153
I0210 22:33:35.992931 140050979010304 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.22758400440216064, loss=1.643872618675232
I0210 22:34:11.356325 140050897565440 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.23030595481395721, loss=1.59397292137146
I0210 22:34:46.742090 140050979010304 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.2090918868780136, loss=1.5037810802459717
I0210 22:35:22.127048 140050897565440 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.22092704474925995, loss=1.5719839334487915
I0210 22:35:57.520735 140050979010304 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.24899210035800934, loss=1.5803961753845215
I0210 22:36:32.900528 140050897565440 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.21874049305915833, loss=1.5186183452606201
I0210 22:37:08.278768 140050979010304 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22720253467559814, loss=1.568176507949829
I0210 22:37:43.666580 140050897565440 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.22468537092208862, loss=1.5337296724319458
I0210 22:38:19.043867 140050979010304 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23435069620609283, loss=1.514635682106018
I0210 22:38:54.412019 140050897565440 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.22263020277023315, loss=1.5477296113967896
I0210 22:39:29.792571 140050979010304 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.23433874547481537, loss=1.537304401397705
I0210 22:40:05.187180 140050897565440 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.20720213651657104, loss=1.4702876806259155
I0210 22:40:40.575590 140050979010304 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.2067389041185379, loss=1.560395359992981
I0210 22:41:15.951519 140050897565440 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.21422041952610016, loss=1.510939598083496
I0210 22:41:51.345146 140050979010304 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.2263074666261673, loss=1.562703013420105
I0210 22:42:26.754782 140050897565440 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.22051312029361725, loss=1.5340763330459595
I0210 22:43:02.117046 140050979010304 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22041121125221252, loss=1.5563545227050781
I0210 22:43:37.502424 140050897565440 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.2245345264673233, loss=1.5509415864944458
I0210 22:44:07.299196 140225696298816 spec.py:321] Evaluating on the training split.
I0210 22:44:10.265891 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:48:06.702624 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 22:48:09.373207 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:51:14.980560 140225696298816 spec.py:349] Evaluating on the test split.
I0210 22:51:17.655499 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 22:54:37.918924 140225696298816 submission_runner.py:408] Time since start: 63073.72s, 	Step: 104486, 	{'train/accuracy': 0.6826587319374084, 'train/loss': 1.4512836933135986, 'train/bleu': 34.46536235091862, 'validation/accuracy': 0.6910763382911682, 'validation/loss': 1.3928955793380737, 'validation/bleu': 30.427187187595266, 'validation/num_examples': 3000, 'test/accuracy': 0.70707106590271, 'test/loss': 1.2888002395629883, 'test/bleu': 30.534439506816717, 'test/num_examples': 3003, 'score': 36994.13355565071, 'total_duration': 63073.723007678986, 'accumulated_submission_time': 36994.13355565071, 'accumulated_eval_time': 26074.819528579712, 'accumulated_logging_time': 1.4544637203216553}
I0210 22:54:37.949559 140050979010304 logging_writer.py:48] [104486] accumulated_eval_time=26074.819529, accumulated_logging_time=1.454464, accumulated_submission_time=36994.133556, global_step=104486, preemption_count=0, score=36994.133556, test/accuracy=0.707071, test/bleu=30.534440, test/loss=1.288800, test/num_examples=3003, total_duration=63073.723008, train/accuracy=0.682659, train/bleu=34.465362, train/loss=1.451284, validation/accuracy=0.691076, validation/bleu=30.427187, validation/loss=1.392896, validation/num_examples=3000
I0210 22:54:43.250334 140050897565440 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.22085723280906677, loss=1.6031489372253418
I0210 22:55:18.454090 140050979010304 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.2152707725763321, loss=1.4886821508407593
I0210 22:55:53.754366 140050897565440 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.21025635302066803, loss=1.5445764064788818
I0210 22:56:29.104785 140050979010304 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22375185787677765, loss=1.5329197645187378
I0210 22:57:04.453601 140050897565440 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.2256769984960556, loss=1.5253140926361084
I0210 22:57:39.833753 140050979010304 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.2207389771938324, loss=1.5329824686050415
I0210 22:58:15.198875 140050897565440 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.22747856378555298, loss=1.5011907815933228
I0210 22:58:50.557171 140050979010304 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.2284342646598816, loss=1.5704350471496582
I0210 22:59:25.947114 140050897565440 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.21243253350257874, loss=1.5429801940917969
I0210 23:00:01.351370 140050979010304 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.2101130187511444, loss=1.4892704486846924
I0210 23:00:36.731408 140050897565440 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.22496922314167023, loss=1.5085701942443848
I0210 23:01:12.134696 140050979010304 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.2197563648223877, loss=1.5238052606582642
I0210 23:01:47.532252 140050897565440 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21525688469409943, loss=1.5089207887649536
I0210 23:02:22.948210 140050979010304 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.21611352264881134, loss=1.5216431617736816
I0210 23:02:58.332608 140050897565440 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.2229650318622589, loss=1.487533450126648
I0210 23:03:33.723881 140050979010304 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.21247747540473938, loss=1.4432483911514282
I0210 23:04:09.087594 140050897565440 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.22612108290195465, loss=1.4785360097885132
I0210 23:04:44.473263 140050979010304 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.23668818175792694, loss=1.5627567768096924
I0210 23:05:19.867608 140050897565440 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.21914005279541016, loss=1.5688881874084473
I0210 23:05:55.228057 140050979010304 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.2329726219177246, loss=1.5364460945129395
I0210 23:06:30.623656 140050897565440 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.21735452115535736, loss=1.496485948562622
I0210 23:07:06.010604 140050979010304 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22879183292388916, loss=1.5483912229537964
I0210 23:07:41.355294 140050897565440 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.22594855725765228, loss=1.5625420808792114
I0210 23:08:16.747908 140050979010304 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22626277804374695, loss=1.52316153049469
I0210 23:08:38.071201 140225696298816 spec.py:321] Evaluating on the training split.
I0210 23:08:41.033403 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:12:33.164801 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 23:12:35.848935 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:16:28.800572 140225696298816 spec.py:349] Evaluating on the test split.
I0210 23:16:31.485184 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:20:22.147476 140225696298816 submission_runner.py:408] Time since start: 64617.95s, 	Step: 106862, 	{'train/accuracy': 0.6929945349693298, 'train/loss': 1.3875455856323242, 'train/bleu': 35.6936682202158, 'validation/accuracy': 0.6926262378692627, 'validation/loss': 1.3891054391860962, 'validation/bleu': 30.321860371372377, 'validation/num_examples': 3000, 'test/accuracy': 0.7095810770988464, 'test/loss': 1.2826364040374756, 'test/bleu': 30.553318604965003, 'test/num_examples': 3003, 'score': 37834.16900777817, 'total_duration': 64617.951558828354, 'accumulated_submission_time': 37834.16900777817, 'accumulated_eval_time': 26778.895754098892, 'accumulated_logging_time': 1.4950628280639648}
I0210 23:20:22.178620 140050897565440 logging_writer.py:48] [106862] accumulated_eval_time=26778.895754, accumulated_logging_time=1.495063, accumulated_submission_time=37834.169008, global_step=106862, preemption_count=0, score=37834.169008, test/accuracy=0.709581, test/bleu=30.553319, test/loss=1.282636, test/num_examples=3003, total_duration=64617.951559, train/accuracy=0.692995, train/bleu=35.693668, train/loss=1.387546, validation/accuracy=0.692626, validation/bleu=30.321860, validation/loss=1.389105, validation/num_examples=3000
I0210 23:20:35.912906 140050979010304 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.21270501613616943, loss=1.477209448814392
I0210 23:21:11.127191 140050897565440 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.22280722856521606, loss=1.579922080039978
I0210 23:21:46.478842 140050979010304 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.2305527925491333, loss=1.542945146560669
I0210 23:22:21.834875 140050897565440 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.2364317625761032, loss=1.5511350631713867
I0210 23:22:57.230476 140050979010304 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.2337234765291214, loss=1.5081596374511719
I0210 23:23:32.612606 140050897565440 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.21706214547157288, loss=1.5207583904266357
I0210 23:24:07.979331 140050979010304 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.2431904822587967, loss=1.4385192394256592
I0210 23:24:43.334563 140050897565440 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.23742344975471497, loss=1.4684579372406006
I0210 23:25:18.699189 140050979010304 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.23897048830986023, loss=1.4695627689361572
I0210 23:25:54.083184 140050897565440 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.22405670583248138, loss=1.5059211254119873
I0210 23:26:29.487115 140050979010304 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.2280053347349167, loss=1.4371155500411987
I0210 23:27:04.893488 140050897565440 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.21872013807296753, loss=1.518265724182129
I0210 23:27:40.296305 140050979010304 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22884123027324677, loss=1.5052173137664795
I0210 23:28:15.693302 140050897565440 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.24533946812152863, loss=1.511229395866394
I0210 23:28:51.068648 140050979010304 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.2362271547317505, loss=1.562453269958496
I0210 23:29:26.445729 140050897565440 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.2186865657567978, loss=1.479758858680725
I0210 23:30:01.842303 140050979010304 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.23520460724830627, loss=1.4978643655776978
I0210 23:30:37.209942 140050897565440 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.2278994917869568, loss=1.5370224714279175
I0210 23:31:12.587679 140050979010304 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.23613111674785614, loss=1.5397956371307373
I0210 23:31:47.975442 140050897565440 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22411313652992249, loss=1.476091742515564
I0210 23:32:23.356068 140050979010304 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.2308114767074585, loss=1.4621716737747192
I0210 23:32:58.747596 140050897565440 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.22886574268341064, loss=1.525935411453247
I0210 23:33:34.117645 140050979010304 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.22602783143520355, loss=1.463088035583496
I0210 23:34:09.504635 140050897565440 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.22463835775852203, loss=1.535368800163269
I0210 23:34:22.323655 140225696298816 spec.py:321] Evaluating on the training split.
I0210 23:34:25.295995 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:38:22.421035 140225696298816 spec.py:333] Evaluating on the validation split.
I0210 23:38:25.094864 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:41:27.712451 140225696298816 spec.py:349] Evaluating on the test split.
I0210 23:41:30.400231 140225696298816 workload.py:181] Translating evaluation dataset.
I0210 23:44:39.819678 140225696298816 submission_runner.py:408] Time since start: 66075.62s, 	Step: 109238, 	{'train/accuracy': 0.6881809234619141, 'train/loss': 1.4110008478164673, 'train/bleu': 34.792418856647195, 'validation/accuracy': 0.6915723085403442, 'validation/loss': 1.3885592222213745, 'validation/bleu': 30.53146184710506, 'validation/num_examples': 3000, 'test/accuracy': 0.7090697884559631, 'test/loss': 1.2797530889511108, 'test/bleu': 30.7142415724196, 'test/num_examples': 3003, 'score': 38674.227446079254, 'total_duration': 66075.62375807762, 'accumulated_submission_time': 38674.227446079254, 'accumulated_eval_time': 27396.391726732254, 'accumulated_logging_time': 1.5361199378967285}
I0210 23:44:39.849810 140050979010304 logging_writer.py:48] [109238] accumulated_eval_time=27396.391727, accumulated_logging_time=1.536120, accumulated_submission_time=38674.227446, global_step=109238, preemption_count=0, score=38674.227446, test/accuracy=0.709070, test/bleu=30.714242, test/loss=1.279753, test/num_examples=3003, total_duration=66075.623758, train/accuracy=0.688181, train/bleu=34.792419, train/loss=1.411001, validation/accuracy=0.691572, validation/bleu=30.531462, validation/loss=1.388559, validation/num_examples=3000
I0210 23:45:02.027629 140050897565440 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.2368181049823761, loss=1.4758294820785522
I0210 23:45:37.295261 140050979010304 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.2412114292383194, loss=1.4782829284667969
I0210 23:46:12.626218 140050897565440 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22420138120651245, loss=1.5694477558135986
I0210 23:46:48.004830 140050979010304 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.24015271663665771, loss=1.4368696212768555
I0210 23:47:23.401161 140050897565440 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.215834841132164, loss=1.5209314823150635
I0210 23:47:58.790613 140050979010304 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22483520209789276, loss=1.4224748611450195
I0210 23:48:34.166903 140050897565440 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.2298535704612732, loss=1.5395634174346924
I0210 23:49:09.534176 140050979010304 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.22871945798397064, loss=1.5019983053207397
I0210 23:49:44.927482 140050897565440 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.21700893342494965, loss=1.5136226415634155
I0210 23:50:20.315249 140050979010304 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.22614577412605286, loss=1.5376559495925903
I0210 23:50:55.695900 140050897565440 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.2314269244670868, loss=1.490363359451294
I0210 23:51:31.115149 140050979010304 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.23291511833667755, loss=1.6016141176223755
I0210 23:52:06.493120 140050897565440 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.2231554090976715, loss=1.4906435012817383
I0210 23:52:41.889908 140050979010304 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.22065900266170502, loss=1.4437603950500488
I0210 23:53:17.295684 140050897565440 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.2372465282678604, loss=1.5572761297225952
I0210 23:53:52.672188 140050979010304 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.22192710638046265, loss=1.5550490617752075
I0210 23:54:28.050014 140050897565440 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22527214884757996, loss=1.5751087665557861
I0210 23:55:03.440434 140050979010304 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.22924520075321198, loss=1.5027320384979248
I0210 23:55:38.856301 140050897565440 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.22498533129692078, loss=1.5269063711166382
I0210 23:56:14.234235 140050979010304 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.21908490359783173, loss=1.4518465995788574
I0210 23:56:49.622448 140050897565440 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.2265360802412033, loss=1.5125943422317505
I0210 23:57:25.010303 140050979010304 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.2455286830663681, loss=1.5441856384277344
I0210 23:58:00.406188 140050897565440 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.22708170115947723, loss=1.5403889417648315
I0210 23:58:35.779758 140050979010304 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.23451270163059235, loss=1.514777660369873
I0210 23:58:40.110813 140225696298816 spec.py:321] Evaluating on the training split.
I0210 23:58:43.081630 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:02:32.550437 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 00:02:35.238585 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:05:06.659863 140225696298816 spec.py:349] Evaluating on the test split.
I0211 00:05:09.340592 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:07:37.884523 140225696298816 submission_runner.py:408] Time since start: 67453.69s, 	Step: 111614, 	{'train/accuracy': 0.6850574612617493, 'train/loss': 1.43190598487854, 'train/bleu': 34.64420152808263, 'validation/accuracy': 0.6938413381576538, 'validation/loss': 1.3793723583221436, 'validation/bleu': 30.713396510455496, 'validation/num_examples': 3000, 'test/accuracy': 0.7106850147247314, 'test/loss': 1.2727055549621582, 'test/bleu': 30.786169768048005, 'test/num_examples': 3003, 'score': 39514.40298914909, 'total_duration': 67453.68859291077, 'accumulated_submission_time': 39514.40298914909, 'accumulated_eval_time': 27934.165376663208, 'accumulated_logging_time': 1.5760877132415771}
I0211 00:07:37.916185 140050897565440 logging_writer.py:48] [111614] accumulated_eval_time=27934.165377, accumulated_logging_time=1.576088, accumulated_submission_time=39514.402989, global_step=111614, preemption_count=0, score=39514.402989, test/accuracy=0.710685, test/bleu=30.786170, test/loss=1.272706, test/num_examples=3003, total_duration=67453.688593, train/accuracy=0.685057, train/bleu=34.644202, train/loss=1.431906, validation/accuracy=0.693841, validation/bleu=30.713397, validation/loss=1.379372, validation/num_examples=3000
I0211 00:08:08.539194 140050979010304 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.23454387485980988, loss=1.4879717826843262
I0211 00:08:43.812944 140050897565440 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.2405611127614975, loss=1.4794889688491821
I0211 00:09:19.180112 140050979010304 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.23984505236148834, loss=1.556374430656433
I0211 00:09:54.526613 140050897565440 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.24234095215797424, loss=1.5428454875946045
I0211 00:10:29.878522 140050979010304 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.235246479511261, loss=1.4577265977859497
I0211 00:11:05.256783 140050897565440 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.22645851969718933, loss=1.4622523784637451
I0211 00:11:40.617197 140050979010304 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.2318875938653946, loss=1.4712328910827637
I0211 00:12:15.998383 140050897565440 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.2389288991689682, loss=1.4655011892318726
I0211 00:12:51.416486 140050979010304 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.2290794402360916, loss=1.4982635974884033
I0211 00:13:26.808256 140050897565440 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.24337588250637054, loss=1.5129878520965576
I0211 00:14:02.199141 140050979010304 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.23507259786128998, loss=1.4336930513381958
I0211 00:14:37.583304 140050897565440 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.23317161202430725, loss=1.4775375127792358
I0211 00:15:12.962557 140050979010304 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.2254907786846161, loss=1.4720276594161987
I0211 00:15:48.362858 140050897565440 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.21173308789730072, loss=1.3968658447265625
I0211 00:16:23.744478 140050979010304 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.22267934679985046, loss=1.4107471704483032
I0211 00:16:59.126013 140050897565440 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.21912941336631775, loss=1.4030317068099976
I0211 00:17:34.529849 140050979010304 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.219075009226799, loss=1.5097171068191528
I0211 00:18:09.928301 140050897565440 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.25396066904067993, loss=1.4716206789016724
I0211 00:18:45.329948 140050979010304 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.23425133526325226, loss=1.4831677675247192
I0211 00:19:20.701500 140050897565440 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.24180783331394196, loss=1.467862844467163
I0211 00:19:56.097988 140050979010304 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.22946907579898834, loss=1.4779541492462158
I0211 00:20:31.496574 140050897565440 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.22687238454818726, loss=1.4512923955917358
I0211 00:21:06.872482 140050979010304 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.23009265959262848, loss=1.435768723487854
I0211 00:21:38.096770 140225696298816 spec.py:321] Evaluating on the training split.
I0211 00:21:41.067348 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:25:48.650875 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 00:25:51.328052 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:29:03.676751 140225696298816 spec.py:349] Evaluating on the test split.
I0211 00:29:06.344913 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:32:34.327023 140225696298816 submission_runner.py:408] Time since start: 68950.13s, 	Step: 113990, 	{'train/accuracy': 0.6922797560691833, 'train/loss': 1.3891639709472656, 'train/bleu': 35.342772251239296, 'validation/accuracy': 0.6943125128746033, 'validation/loss': 1.3756449222564697, 'validation/bleu': 30.757281576648342, 'validation/num_examples': 3000, 'test/accuracy': 0.711661159992218, 'test/loss': 1.2692705392837524, 'test/bleu': 31.040662341172386, 'test/num_examples': 3003, 'score': 40354.49650359154, 'total_duration': 68950.13110136986, 'accumulated_submission_time': 40354.49650359154, 'accumulated_eval_time': 28590.395575761795, 'accumulated_logging_time': 1.6191487312316895}
I0211 00:32:34.357469 140050897565440 logging_writer.py:48] [113990] accumulated_eval_time=28590.395576, accumulated_logging_time=1.619149, accumulated_submission_time=40354.496504, global_step=113990, preemption_count=0, score=40354.496504, test/accuracy=0.711661, test/bleu=31.040662, test/loss=1.269271, test/num_examples=3003, total_duration=68950.131101, train/accuracy=0.692280, train/bleu=35.342772, train/loss=1.389164, validation/accuracy=0.694313, validation/bleu=30.757282, validation/loss=1.375645, validation/num_examples=3000
I0211 00:32:38.251250 140050979010304 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.2307254821062088, loss=1.4737752676010132
I0211 00:33:13.482930 140050897565440 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.2347898632287979, loss=1.4724962711334229
I0211 00:33:48.811708 140050979010304 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22874495387077332, loss=1.5043801069259644
I0211 00:34:24.187979 140050897565440 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.23260197043418884, loss=1.4945892095565796
I0211 00:34:59.546038 140050979010304 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.2406369149684906, loss=1.4826507568359375
I0211 00:35:34.928186 140050897565440 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.23817266523838043, loss=1.5641835927963257
I0211 00:36:10.294542 140050979010304 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.2594625949859619, loss=1.4653126001358032
I0211 00:36:45.674094 140050897565440 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.23787246644496918, loss=1.4289790391921997
I0211 00:37:21.085532 140050979010304 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.2284935563802719, loss=1.4559004306793213
I0211 00:37:56.467740 140050897565440 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.2244487851858139, loss=1.4722580909729004
I0211 00:38:31.858822 140050979010304 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.22746580839157104, loss=1.4437288045883179
I0211 00:39:07.246390 140050897565440 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.22033420205116272, loss=1.4341446161270142
I0211 00:39:42.636563 140050979010304 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.23104624450206757, loss=1.5037037134170532
I0211 00:40:17.995220 140050897565440 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.23549610376358032, loss=1.4998857975006104
I0211 00:40:53.380854 140050979010304 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.23137886822223663, loss=1.5106621980667114
I0211 00:41:28.767791 140050897565440 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.23236753046512604, loss=1.540832757949829
I0211 00:42:04.152770 140050979010304 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.22182923555374146, loss=1.5033372640609741
I0211 00:42:39.557339 140050897565440 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.24791134893894196, loss=1.516049861907959
I0211 00:43:14.956485 140050979010304 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.2437213510274887, loss=1.4790183305740356
I0211 00:43:50.333537 140050897565440 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.21623675525188446, loss=1.4801937341690063
I0211 00:44:25.735340 140050979010304 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.24856679141521454, loss=1.4904199838638306
I0211 00:45:01.129420 140050897565440 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.22648607194423676, loss=1.4529627561569214
I0211 00:45:36.526129 140050979010304 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.2286001741886139, loss=1.4628381729125977
I0211 00:46:11.931093 140050897565440 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.22460345923900604, loss=1.4655667543411255
I0211 00:46:34.656185 140225696298816 spec.py:321] Evaluating on the training split.
I0211 00:46:37.630149 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:50:36.802055 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 00:50:39.480607 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:53:13.480705 140225696298816 spec.py:349] Evaluating on the test split.
I0211 00:53:16.152689 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 00:56:04.109142 140225696298816 submission_runner.py:408] Time since start: 70359.91s, 	Step: 116366, 	{'train/accuracy': 0.6906039714813232, 'train/loss': 1.3968476057052612, 'train/bleu': 34.79750781778418, 'validation/accuracy': 0.6947464942932129, 'validation/loss': 1.373445987701416, 'validation/bleu': 30.66973494550261, 'validation/num_examples': 3000, 'test/accuracy': 0.7128929495811462, 'test/loss': 1.264074683189392, 'test/bleu': 31.160097992334794, 'test/num_examples': 3003, 'score': 41194.70844531059, 'total_duration': 70359.9132270813, 'accumulated_submission_time': 41194.70844531059, 'accumulated_eval_time': 29159.84848332405, 'accumulated_logging_time': 1.660917043685913}
I0211 00:56:04.140929 140050979010304 logging_writer.py:48] [116366] accumulated_eval_time=29159.848483, accumulated_logging_time=1.660917, accumulated_submission_time=41194.708445, global_step=116366, preemption_count=0, score=41194.708445, test/accuracy=0.712893, test/bleu=31.160098, test/loss=1.264075, test/num_examples=3003, total_duration=70359.913227, train/accuracy=0.690604, train/bleu=34.797508, train/loss=1.396848, validation/accuracy=0.694746, validation/bleu=30.669735, validation/loss=1.373446, validation/num_examples=3000
I0211 00:56:16.480277 140050897565440 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.22928951680660248, loss=1.3889052867889404
I0211 00:56:51.727930 140050979010304 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.23701336979866028, loss=1.4782747030258179
I0211 00:57:27.181385 140050897565440 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.2383008599281311, loss=1.4925732612609863
I0211 00:58:02.569390 140050979010304 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.21794776618480682, loss=1.4148411750793457
I0211 00:58:37.946624 140050897565440 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.2259492576122284, loss=1.472525954246521
I0211 00:59:13.325600 140050979010304 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.2425553798675537, loss=1.468544363975525
I0211 00:59:48.705960 140050897565440 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.23530027270317078, loss=1.5156892538070679
I0211 01:00:24.068681 140050979010304 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.24237671494483948, loss=1.4920302629470825
I0211 01:00:59.436403 140050897565440 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.23951812088489532, loss=1.4796351194381714
I0211 01:01:34.843579 140050979010304 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.22998207807540894, loss=1.508653163909912
I0211 01:02:10.249839 140050897565440 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.23241600394248962, loss=1.5174235105514526
I0211 01:02:45.645531 140050979010304 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.23206979036331177, loss=1.5386991500854492
I0211 01:03:21.018106 140050897565440 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.22969232499599457, loss=1.412142276763916
I0211 01:03:56.388975 140050979010304 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.2234472632408142, loss=1.4285104274749756
I0211 01:04:31.781899 140050897565440 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.23771381378173828, loss=1.460344910621643
I0211 01:05:07.175498 140050979010304 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.22597923874855042, loss=1.4585851430892944
I0211 01:05:42.540187 140050897565440 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.23197686672210693, loss=1.3812469244003296
I0211 01:06:17.952468 140050979010304 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.23653897643089294, loss=1.4848430156707764
I0211 01:06:53.339225 140050897565440 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.22654661536216736, loss=1.4343820810317993
I0211 01:07:28.738303 140050979010304 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.23169472813606262, loss=1.4960365295410156
I0211 01:08:04.167742 140050897565440 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.23559677600860596, loss=1.470875859260559
I0211 01:08:39.557191 140050979010304 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.232746884226799, loss=1.4827685356140137
I0211 01:09:14.960330 140050897565440 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.23550879955291748, loss=1.4376442432403564
I0211 01:09:50.349608 140050979010304 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.2379075288772583, loss=1.5368990898132324
I0211 01:10:04.240864 140225696298816 spec.py:321] Evaluating on the training split.
I0211 01:10:07.218287 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:13:46.283981 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 01:13:48.965493 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:16:18.437750 140225696298816 spec.py:349] Evaluating on the test split.
I0211 01:16:21.113534 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:18:55.496325 140225696298816 submission_runner.py:408] Time since start: 71731.30s, 	Step: 118741, 	{'train/accuracy': 0.6908093690872192, 'train/loss': 1.4008818864822388, 'train/bleu': 34.89803540192062, 'validation/accuracy': 0.6953044533729553, 'validation/loss': 1.3693207502365112, 'validation/bleu': 30.95589799049646, 'validation/num_examples': 3000, 'test/accuracy': 0.7127767205238342, 'test/loss': 1.262351393699646, 'test/bleu': 30.769333545080798, 'test/num_examples': 3003, 'score': 42034.721598386765, 'total_duration': 71731.30040001869, 'accumulated_submission_time': 42034.721598386765, 'accumulated_eval_time': 29691.103921175003, 'accumulated_logging_time': 1.7029705047607422}
I0211 01:18:55.528532 140050897565440 logging_writer.py:48] [118741] accumulated_eval_time=29691.103921, accumulated_logging_time=1.702971, accumulated_submission_time=42034.721598, global_step=118741, preemption_count=0, score=42034.721598, test/accuracy=0.712777, test/bleu=30.769334, test/loss=1.262351, test/num_examples=3003, total_duration=71731.300400, train/accuracy=0.690809, train/bleu=34.898035, train/loss=1.400882, validation/accuracy=0.695304, validation/bleu=30.955898, validation/loss=1.369321, validation/num_examples=3000
I0211 01:19:16.635412 140050979010304 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.23280537128448486, loss=1.438829779624939
I0211 01:19:51.877017 140050897565440 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.22779551148414612, loss=1.4209529161453247
I0211 01:20:27.214359 140050979010304 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.2508924603462219, loss=1.538111925125122
I0211 01:21:02.601226 140050897565440 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.23475374281406403, loss=1.442747712135315
I0211 01:21:37.953783 140050979010304 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.23534424602985382, loss=1.5710304975509644
I0211 01:22:13.340617 140050897565440 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.22439467906951904, loss=1.444782018661499
I0211 01:22:48.767217 140050979010304 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.23369838297367096, loss=1.5005741119384766
I0211 01:23:24.166647 140050897565440 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23981955647468567, loss=1.4854191541671753
I0211 01:23:59.575354 140050979010304 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.22761057317256927, loss=1.4793994426727295
I0211 01:24:34.959131 140050897565440 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.24420560896396637, loss=1.488931655883789
I0211 01:25:10.353279 140050979010304 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.21672864258289337, loss=1.4136205911636353
I0211 01:25:45.737199 140050897565440 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.2449873685836792, loss=1.48288094997406
I0211 01:26:21.095781 140050979010304 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.24266961216926575, loss=1.450588345527649
I0211 01:26:56.484858 140050897565440 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.23963354527950287, loss=1.4903925657272339
I0211 01:27:31.880840 140050979010304 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.22958062589168549, loss=1.4390981197357178
I0211 01:28:07.273384 140050897565440 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.2395317554473877, loss=1.5194612741470337
I0211 01:28:42.665481 140050979010304 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.2412094622850418, loss=1.5159430503845215
I0211 01:29:18.044799 140050897565440 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.23030602931976318, loss=1.4784597158432007
I0211 01:29:53.430447 140050979010304 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.2295801341533661, loss=1.5373910665512085
I0211 01:30:28.836509 140050897565440 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.22692768275737762, loss=1.4381952285766602
I0211 01:31:04.251775 140050979010304 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.2366979420185089, loss=1.4544366598129272
I0211 01:31:39.634603 140050897565440 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.2309034764766693, loss=1.4798972606658936
I0211 01:32:15.040259 140050979010304 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.23006299138069153, loss=1.408007025718689
I0211 01:32:50.461599 140050897565440 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.23294328153133392, loss=1.4373259544372559
I0211 01:32:55.501402 140225696298816 spec.py:321] Evaluating on the training split.
I0211 01:32:58.477159 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:37:08.474494 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 01:37:11.154673 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:39:59.855735 140225696298816 spec.py:349] Evaluating on the test split.
I0211 01:40:02.545222 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 01:43:08.574583 140225696298816 submission_runner.py:408] Time since start: 73184.38s, 	Step: 121116, 	{'train/accuracy': 0.6932882070541382, 'train/loss': 1.388006567955017, 'train/bleu': 35.52039251433238, 'validation/accuracy': 0.6962592005729675, 'validation/loss': 1.367598533630371, 'validation/bleu': 30.874046124594287, 'validation/num_examples': 3000, 'test/accuracy': 0.7138457894325256, 'test/loss': 1.2584335803985596, 'test/bleu': 31.123645584563633, 'test/num_examples': 3003, 'score': 42874.60907793045, 'total_duration': 73184.37866711617, 'accumulated_submission_time': 42874.60907793045, 'accumulated_eval_time': 30304.17706489563, 'accumulated_logging_time': 1.744511365890503}
I0211 01:43:08.607213 140050979010304 logging_writer.py:48] [121116] accumulated_eval_time=30304.177065, accumulated_logging_time=1.744511, accumulated_submission_time=42874.609078, global_step=121116, preemption_count=0, score=42874.609078, test/accuracy=0.713846, test/bleu=31.123646, test/loss=1.258434, test/num_examples=3003, total_duration=73184.378667, train/accuracy=0.693288, train/bleu=35.520393, train/loss=1.388007, validation/accuracy=0.696259, validation/bleu=30.874046, validation/loss=1.367599, validation/num_examples=3000
I0211 01:43:38.527061 140050897565440 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.2340368628501892, loss=1.47728431224823
I0211 01:44:13.833632 140050979010304 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.2296322137117386, loss=1.3859413862228394
I0211 01:44:49.193139 140050897565440 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.239693745970726, loss=1.4452496767044067
I0211 01:45:24.608113 140050979010304 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.2438165843486786, loss=1.455786943435669
I0211 01:45:59.996881 140050897565440 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.24243654310703278, loss=1.3681772947311401
I0211 01:46:35.409513 140050979010304 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.2327905148267746, loss=1.4822877645492554
I0211 01:47:10.807287 140050897565440 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23478613793849945, loss=1.536704182624817
I0211 01:47:46.200224 140050979010304 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.23833870887756348, loss=1.5013422966003418
I0211 01:48:21.590476 140050897565440 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.228949636220932, loss=1.4284420013427734
I0211 01:48:57.009035 140050979010304 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.25131502747535706, loss=1.3818283081054688
I0211 01:49:32.403150 140050897565440 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.22837120294570923, loss=1.4903326034545898
I0211 01:50:07.816594 140050979010304 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.23256994783878326, loss=1.427716851234436
I0211 01:50:43.222939 140050897565440 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.23322825133800507, loss=1.4172146320343018
I0211 01:51:18.627896 140050979010304 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.22315746545791626, loss=1.410361886024475
I0211 01:51:54.005603 140050897565440 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.2309194654226303, loss=1.4491500854492188
I0211 01:52:29.402482 140050979010304 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.24076734483242035, loss=1.4751886129379272
I0211 01:53:04.790150 140050897565440 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.23421145975589752, loss=1.4990363121032715
I0211 01:53:40.208168 140050979010304 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.23418396711349487, loss=1.4341914653778076
I0211 01:54:15.618533 140050897565440 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.25089988112449646, loss=1.5152456760406494
I0211 01:54:51.020708 140050979010304 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.2260282337665558, loss=1.464343547821045
I0211 01:55:26.426804 140050897565440 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.23408569395542145, loss=1.401878833770752
I0211 01:56:01.833255 140050979010304 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.23801609873771667, loss=1.4639369249343872
I0211 01:56:37.251886 140050897565440 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.2412077784538269, loss=1.4633170366287231
I0211 01:57:08.841739 140225696298816 spec.py:321] Evaluating on the training split.
I0211 01:57:11.831487 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:00:52.430621 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 02:00:55.113149 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:03:27.566367 140225696298816 spec.py:349] Evaluating on the test split.
I0211 02:03:30.243502 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:06:01.984745 140225696298816 submission_runner.py:408] Time since start: 74557.79s, 	Step: 123491, 	{'train/accuracy': 0.6912440061569214, 'train/loss': 1.395629644393921, 'train/bleu': 35.35323909995653, 'validation/accuracy': 0.6952672600746155, 'validation/loss': 1.367558479309082, 'validation/bleu': 30.844249592137086, 'validation/num_examples': 3000, 'test/accuracy': 0.7139503955841064, 'test/loss': 1.255860447883606, 'test/bleu': 31.129922784276456, 'test/num_examples': 3003, 'score': 43714.75734376907, 'total_duration': 74557.78880643845, 'accumulated_submission_time': 43714.75734376907, 'accumulated_eval_time': 30837.32000207901, 'accumulated_logging_time': 1.7871100902557373}
I0211 02:06:02.018987 140050979010304 logging_writer.py:48] [123491] accumulated_eval_time=30837.320002, accumulated_logging_time=1.787110, accumulated_submission_time=43714.757344, global_step=123491, preemption_count=0, score=43714.757344, test/accuracy=0.713950, test/bleu=31.129923, test/loss=1.255860, test/num_examples=3003, total_duration=74557.788806, train/accuracy=0.691244, train/bleu=35.353239, train/loss=1.395630, validation/accuracy=0.695267, validation/bleu=30.844250, validation/loss=1.367558, validation/num_examples=3000
I0211 02:06:05.553914 140050897565440 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.23635733127593994, loss=1.4239914417266846
I0211 02:06:40.753394 140050979010304 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.23993198573589325, loss=1.5167584419250488
I0211 02:07:16.088369 140050897565440 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.22909969091415405, loss=1.4529716968536377
I0211 02:07:51.463440 140050979010304 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.2333173155784607, loss=1.4336740970611572
I0211 02:08:26.829810 140050897565440 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.23370172083377838, loss=1.4568787813186646
I0211 02:09:02.205704 140050979010304 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.2336144596338272, loss=1.3897980451583862
I0211 02:09:37.603146 140050897565440 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.2403552532196045, loss=1.415337324142456
I0211 02:10:13.023910 140050979010304 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.22871094942092896, loss=1.4481093883514404
I0211 02:10:48.427308 140050897565440 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.24783316254615784, loss=1.4071522951126099
I0211 02:11:23.806924 140050979010304 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.23956198990345, loss=1.4728490114212036
I0211 02:11:59.222122 140050897565440 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22487670183181763, loss=1.4916363954544067
I0211 02:12:34.613463 140050979010304 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.23340199887752533, loss=1.4708025455474854
I0211 02:13:10.025964 140050897565440 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.23376040160655975, loss=1.4564988613128662
I0211 02:13:45.433266 140050979010304 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.23169949650764465, loss=1.482210636138916
I0211 02:14:20.826219 140050897565440 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.24186895787715912, loss=1.4748599529266357
I0211 02:14:56.226754 140050979010304 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.22958502173423767, loss=1.4681342840194702
I0211 02:15:31.598790 140050897565440 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.22944001853466034, loss=1.4459855556488037
I0211 02:16:07.011850 140050979010304 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.22970989346504211, loss=1.408733606338501
I0211 02:16:42.383069 140050897565440 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.23205019533634186, loss=1.4451786279678345
I0211 02:17:17.771760 140050979010304 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.23931163549423218, loss=1.5317902565002441
I0211 02:17:53.154350 140050897565440 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.23679953813552856, loss=1.4652516841888428
I0211 02:18:28.550954 140050979010304 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.2422727793455124, loss=1.4995994567871094
I0211 02:19:03.931521 140050897565440 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.22849923372268677, loss=1.4313321113586426
I0211 02:19:39.310104 140050979010304 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.23384948074817657, loss=1.4867323637008667
I0211 02:20:02.030643 140225696298816 spec.py:321] Evaluating on the training split.
I0211 02:20:04.989835 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:23:33.662044 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 02:23:36.335765 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:26:15.661095 140225696298816 spec.py:349] Evaluating on the test split.
I0211 02:26:18.332112 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:29:07.612138 140225696298816 submission_runner.py:408] Time since start: 75943.42s, 	Step: 125866, 	{'train/accuracy': 0.697185218334198, 'train/loss': 1.3619694709777832, 'train/bleu': 35.78074525428276, 'validation/accuracy': 0.695651650428772, 'validation/loss': 1.3671724796295166, 'validation/bleu': 30.911010119255785, 'validation/num_examples': 3000, 'test/accuracy': 0.7137179970741272, 'test/loss': 1.25685453414917, 'test/bleu': 30.930439140655892, 'test/num_examples': 3003, 'score': 44554.68297743797, 'total_duration': 75943.4162247181, 'accumulated_submission_time': 44554.68297743797, 'accumulated_eval_time': 31382.90145254135, 'accumulated_logging_time': 1.8318068981170654}
I0211 02:29:07.644859 140050897565440 logging_writer.py:48] [125866] accumulated_eval_time=31382.901453, accumulated_logging_time=1.831807, accumulated_submission_time=44554.682977, global_step=125866, preemption_count=0, score=44554.682977, test/accuracy=0.713718, test/bleu=30.930439, test/loss=1.256855, test/num_examples=3003, total_duration=75943.416225, train/accuracy=0.697185, train/bleu=35.780745, train/loss=1.361969, validation/accuracy=0.695652, validation/bleu=30.911010, validation/loss=1.367172, validation/num_examples=3000
I0211 02:29:19.986864 140050979010304 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.23558780550956726, loss=1.4625192880630493
I0211 02:29:55.257570 140050897565440 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.22801275551319122, loss=1.4635518789291382
I0211 02:30:30.633098 140050979010304 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.23101189732551575, loss=1.4881759881973267
I0211 02:31:06.020016 140050897565440 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.22850045561790466, loss=1.4115203619003296
I0211 02:31:41.403462 140050979010304 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.23836925625801086, loss=1.553002119064331
I0211 02:32:16.806393 140050897565440 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.22965869307518005, loss=1.4778157472610474
I0211 02:32:52.252694 140050979010304 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.229648619890213, loss=1.4456969499588013
I0211 02:33:27.666513 140050897565440 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.2367745339870453, loss=1.3901067972183228
I0211 02:34:03.054365 140050979010304 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.2316325455904007, loss=1.368031620979309
I0211 02:34:38.464566 140050897565440 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.24034908413887024, loss=1.449802041053772
I0211 02:35:13.872298 140050979010304 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.22309225797653198, loss=1.3948997259140015
I0211 02:35:49.250122 140050897565440 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.23015736043453217, loss=1.3894391059875488
I0211 02:36:24.628045 140050979010304 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.34609749913215637, loss=1.4203941822052002
I0211 02:36:59.990472 140050897565440 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.22981542348861694, loss=1.4105501174926758
I0211 02:37:35.389513 140050979010304 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.22919462621212006, loss=1.4670628309249878
I0211 02:38:10.796338 140050897565440 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.23780234158039093, loss=1.3855217695236206
I0211 02:38:46.193515 140050979010304 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.22447994351387024, loss=1.5039125680923462
I0211 02:39:21.600404 140050897565440 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.22452516853809357, loss=1.4550684690475464
I0211 02:39:57.002374 140050979010304 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.2528277039527893, loss=1.463619351387024
I0211 02:40:32.396188 140050897565440 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.23341287672519684, loss=1.4286729097366333
I0211 02:41:07.785172 140050979010304 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.21914052963256836, loss=1.4166260957717896
I0211 02:41:43.184034 140050897565440 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23174777626991272, loss=1.4642891883850098
I0211 02:42:18.601651 140050979010304 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.23524966835975647, loss=1.495072603225708
I0211 02:42:54.004116 140050897565440 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.23397350311279297, loss=1.4763904809951782
I0211 02:43:07.884093 140225696298816 spec.py:321] Evaluating on the training split.
I0211 02:43:10.862135 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:47:00.767712 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 02:47:03.448271 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:49:47.856180 140225696298816 spec.py:349] Evaluating on the test split.
I0211 02:49:50.523065 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 02:52:35.465862 140225696298816 submission_runner.py:408] Time since start: 77351.27s, 	Step: 128241, 	{'train/accuracy': 0.6922347545623779, 'train/loss': 1.3835301399230957, 'train/bleu': 35.30423628524667, 'validation/accuracy': 0.6954284310340881, 'validation/loss': 1.3652565479278564, 'validation/bleu': 30.88262735410851, 'validation/num_examples': 3000, 'test/accuracy': 0.7137528657913208, 'test/loss': 1.2547988891601562, 'test/bleu': 31.126286255052236, 'test/num_examples': 3003, 'score': 45394.836584329605, 'total_duration': 77351.26991295815, 'accumulated_submission_time': 45394.836584329605, 'accumulated_eval_time': 31950.483147382736, 'accumulated_logging_time': 1.8745992183685303}
I0211 02:52:35.508598 140050979010304 logging_writer.py:48] [128241] accumulated_eval_time=31950.483147, accumulated_logging_time=1.874599, accumulated_submission_time=45394.836584, global_step=128241, preemption_count=0, score=45394.836584, test/accuracy=0.713753, test/bleu=31.126286, test/loss=1.254799, test/num_examples=3003, total_duration=77351.269913, train/accuracy=0.692235, train/bleu=35.304236, train/loss=1.383530, validation/accuracy=0.695428, validation/bleu=30.882627, validation/loss=1.365257, validation/num_examples=3000
I0211 02:52:56.652859 140050897565440 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.23335552215576172, loss=1.5177397727966309
I0211 02:53:31.950484 140050979010304 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.22370462119579315, loss=1.4356181621551514
I0211 02:54:07.327469 140050897565440 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.23285868763923645, loss=1.4542222023010254
I0211 02:54:42.683280 140050979010304 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.2316812425851822, loss=1.4874449968338013
I0211 02:55:18.059850 140050897565440 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.2417794167995453, loss=1.46792471408844
I0211 02:55:53.458288 140050979010304 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.24795754253864288, loss=1.4867035150527954
I0211 02:56:28.845561 140050897565440 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.23083579540252686, loss=1.4656970500946045
I0211 02:57:04.252286 140050979010304 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.2510210871696472, loss=1.4734220504760742
I0211 02:57:39.625858 140050897565440 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.22742430865764618, loss=1.4035260677337646
I0211 02:58:15.036131 140050979010304 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.24214650690555573, loss=1.466455340385437
I0211 02:58:50.423942 140050897565440 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.2328348606824875, loss=1.4523794651031494
I0211 02:59:25.823764 140050979010304 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.22457119822502136, loss=1.4248418807983398
I0211 03:00:01.217107 140050897565440 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.23460078239440918, loss=1.4125055074691772
I0211 03:00:36.630154 140050979010304 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.22977402806282043, loss=1.4226008653640747
I0211 03:01:12.022302 140050897565440 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.2297668755054474, loss=1.4686148166656494
I0211 03:01:47.431155 140050979010304 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.2352805882692337, loss=1.4086122512817383
I0211 03:02:22.836501 140050897565440 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.23183554410934448, loss=1.507328987121582
I0211 03:02:58.243396 140050979010304 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.23362906277179718, loss=1.3950436115264893
I0211 03:03:33.659388 140050897565440 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.23375292122364044, loss=1.474947452545166
I0211 03:04:09.054165 140050979010304 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.23205327987670898, loss=1.4811110496520996
I0211 03:04:44.451870 140050897565440 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.24580077826976776, loss=1.5186861753463745
I0211 03:05:19.866929 140050979010304 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.23040173947811127, loss=1.4712804555892944
I0211 03:05:55.278594 140050897565440 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.2500952184200287, loss=1.4644486904144287
I0211 03:06:30.647512 140050979010304 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.23969627916812897, loss=1.4556571245193481
I0211 03:06:35.681133 140225696298816 spec.py:321] Evaluating on the training split.
I0211 03:06:38.655415 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:10:35.835976 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 03:10:38.503456 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:13:11.624070 140225696298816 spec.py:349] Evaluating on the test split.
I0211 03:13:14.292618 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:15:58.688937 140225696298816 submission_runner.py:408] Time since start: 78754.49s, 	Step: 130616, 	{'train/accuracy': 0.6963204741477966, 'train/loss': 1.366992473602295, 'train/bleu': 35.57011387339867, 'validation/accuracy': 0.6954284310340881, 'validation/loss': 1.3648079633712769, 'validation/bleu': 30.946973319770205, 'validation/num_examples': 3000, 'test/accuracy': 0.7141479253768921, 'test/loss': 1.2541664838790894, 'test/bleu': 31.103261886181247, 'test/num_examples': 3003, 'score': 46234.92179393768, 'total_duration': 78754.49300599098, 'accumulated_submission_time': 46234.92179393768, 'accumulated_eval_time': 32513.490881443024, 'accumulated_logging_time': 1.928779125213623}
I0211 03:15:58.722443 140050897565440 logging_writer.py:48] [130616] accumulated_eval_time=32513.490881, accumulated_logging_time=1.928779, accumulated_submission_time=46234.921794, global_step=130616, preemption_count=0, score=46234.921794, test/accuracy=0.714148, test/bleu=31.103262, test/loss=1.254166, test/num_examples=3003, total_duration=78754.493006, train/accuracy=0.696320, train/bleu=35.570114, train/loss=1.366992, validation/accuracy=0.695428, validation/bleu=30.946973, validation/loss=1.364808, validation/num_examples=3000
I0211 03:16:28.675364 140050979010304 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.23246094584465027, loss=1.4404414892196655
I0211 03:17:03.997832 140050897565440 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.23409664630889893, loss=1.49946129322052
I0211 03:17:39.362117 140050979010304 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.22678406536579132, loss=1.4215543270111084
I0211 03:18:14.719962 140050897565440 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.2274232804775238, loss=1.4047603607177734
I0211 03:18:50.102878 140050979010304 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.2356622815132141, loss=1.491438388824463
I0211 03:19:25.494292 140050897565440 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.24230632185935974, loss=1.5053831338882446
I0211 03:20:00.892649 140050979010304 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.23255044221878052, loss=1.5218926668167114
I0211 03:20:36.297589 140050897565440 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.22710426151752472, loss=1.4457147121429443
I0211 03:21:11.704664 140050979010304 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.22621294856071472, loss=1.4426040649414062
I0211 03:21:47.110139 140050897565440 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.22484125196933746, loss=1.4517953395843506
I0211 03:22:22.492692 140050979010304 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.24254904687404633, loss=1.4661686420440674
I0211 03:22:57.900296 140050897565440 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.23381386697292328, loss=1.4755818843841553
I0211 03:23:33.300079 140050979010304 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.22802633047103882, loss=1.4022061824798584
I0211 03:24:08.689417 140050897565440 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.23582302033901215, loss=1.4961649179458618
I0211 03:24:44.083715 140050979010304 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.2332116663455963, loss=1.3860597610473633
I0211 03:25:19.468556 140050897565440 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.22735272347927094, loss=1.462364912033081
I0211 03:25:54.849770 140050979010304 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.23870906233787537, loss=1.4578325748443604
I0211 03:26:30.253482 140050897565440 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.24156363308429718, loss=1.5072181224822998
I0211 03:27:05.645456 140050979010304 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.2198874056339264, loss=1.4673991203308105
I0211 03:27:41.044719 140050897565440 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.23394045233726501, loss=1.4333926439285278
I0211 03:28:16.414547 140050979010304 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.22914926707744598, loss=1.4704363346099854
I0211 03:28:51.777066 140050897565440 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.22864145040512085, loss=1.5156432390213013
I0211 03:29:27.178290 140050979010304 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2374097853899002, loss=1.535918951034546
I0211 03:29:58.739500 140225696298816 spec.py:321] Evaluating on the training split.
I0211 03:30:01.707441 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:34:03.021824 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 03:34:05.702507 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:36:38.420063 140225696298816 spec.py:349] Evaluating on the test split.
I0211 03:36:41.103656 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:39:24.140473 140225696298816 submission_runner.py:408] Time since start: 80159.94s, 	Step: 132991, 	{'train/accuracy': 0.69627445936203, 'train/loss': 1.36464262008667, 'train/bleu': 35.58865724493143, 'validation/accuracy': 0.6954532265663147, 'validation/loss': 1.3649661540985107, 'validation/bleu': 31.040501645749234, 'validation/num_examples': 3000, 'test/accuracy': 0.7140085101127625, 'test/loss': 1.2541321516036987, 'test/bleu': 31.145809433459892, 'test/num_examples': 3003, 'score': 47074.85367107391, 'total_duration': 80159.94454312325, 'accumulated_submission_time': 47074.85367107391, 'accumulated_eval_time': 33078.891793727875, 'accumulated_logging_time': 1.9719855785369873}
I0211 03:39:24.174161 140050897565440 logging_writer.py:48] [132991] accumulated_eval_time=33078.891794, accumulated_logging_time=1.971986, accumulated_submission_time=47074.853671, global_step=132991, preemption_count=0, score=47074.853671, test/accuracy=0.714009, test/bleu=31.145809, test/loss=1.254132, test/num_examples=3003, total_duration=80159.944543, train/accuracy=0.696274, train/bleu=35.588657, train/loss=1.364643, validation/accuracy=0.695453, validation/bleu=31.040502, validation/loss=1.364966, validation/num_examples=3000
I0211 03:39:27.717376 140050979010304 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.22903764247894287, loss=1.4892503023147583
I0211 03:40:02.951929 140050897565440 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.23099899291992188, loss=1.4192283153533936
I0211 03:40:38.281530 140050979010304 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.23541845381259918, loss=1.4475507736206055
I0211 03:41:13.636583 140050897565440 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.23296061158180237, loss=1.5276901721954346
I0211 03:41:24.673486 140225696298816 spec.py:321] Evaluating on the training split.
I0211 03:41:27.640183 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:45:15.922749 140225696298816 spec.py:333] Evaluating on the validation split.
I0211 03:45:18.598223 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:47:52.125591 140225696298816 spec.py:349] Evaluating on the test split.
I0211 03:47:54.808866 140225696298816 workload.py:181] Translating evaluation dataset.
I0211 03:50:38.521364 140225696298816 submission_runner.py:408] Time since start: 80834.33s, 	Step: 133333, 	{'train/accuracy': 0.6945627331733704, 'train/loss': 1.3744174242019653, 'train/bleu': 35.631271648808266, 'validation/accuracy': 0.6954532265663147, 'validation/loss': 1.3649758100509644, 'validation/bleu': 31.040501645749234, 'validation/num_examples': 3000, 'test/accuracy': 0.7140317559242249, 'test/loss': 1.2541394233703613, 'test/bleu': 31.13090825160175, 'test/num_examples': 3003, 'score': 47195.332033634186, 'total_duration': 80834.32544207573, 'accumulated_submission_time': 47195.332033634186, 'accumulated_eval_time': 33632.739612579346, 'accumulated_logging_time': 2.015644073486328}
I0211 03:50:38.555250 140050979010304 logging_writer.py:48] [133333] accumulated_eval_time=33632.739613, accumulated_logging_time=2.015644, accumulated_submission_time=47195.332034, global_step=133333, preemption_count=0, score=47195.332034, test/accuracy=0.714032, test/bleu=31.130908, test/loss=1.254139, test/num_examples=3003, total_duration=80834.325442, train/accuracy=0.694563, train/bleu=35.631272, train/loss=1.374417, validation/accuracy=0.695453, validation/bleu=31.040502, validation/loss=1.364976, validation/num_examples=3000
I0211 03:50:38.589885 140050897565440 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47195.332034
I0211 03:50:39.814715 140225696298816 checkpoints.py:490] Saving checkpoint at step: 133333
I0211 03:50:43.904216 140225696298816 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5/checkpoint_133333
I0211 03:50:43.909097 140225696298816 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/wmt_jax/trial_5/checkpoint_133333.
I0211 03:50:43.970906 140225696298816 submission_runner.py:583] Tuning trial 5/5
I0211 03:50:43.971085 140225696298816 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0211 03:50:43.983705 140225696298816 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005945574957877398, 'train/loss': 11.024234771728516, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.047277450561523, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.036273956298828, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.910383462905884, 'total_duration': 887.1384572982788, 'accumulated_submission_time': 30.910383462905884, 'accumulated_eval_time': 856.2280375957489, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2373, {'train/accuracy': 0.5159180164337158, 'train/loss': 2.8420560359954834, 'train/bleu': 23.051630204159792, 'validation/accuracy': 0.5170301795005798, 'validation/loss': 2.82480525970459, 'validation/bleu': 18.71438425767787, 'validation/num_examples': 3000, 'test/accuracy': 0.5161815285682678, 'test/loss': 2.86415958404541, 'test/bleu': 17.2187217021128, 'test/num_examples': 3003, 'score': 870.9926633834839, 'total_duration': 2228.3837530612946, 'accumulated_submission_time': 870.9926633834839, 'accumulated_eval_time': 1357.2976994514465, 'accumulated_logging_time': 0.018973827362060547, 'global_step': 2373, 'preemption_count': 0}), (4747, {'train/accuracy': 0.5766600966453552, 'train/loss': 2.2446744441986084, 'train/bleu': 26.829642974541517, 'validation/accuracy': 0.5912140011787415, 'validation/loss': 2.1382405757904053, 'validation/bleu': 23.472348577549567, 'validation/num_examples': 3000, 'test/accuracy': 0.5934344530105591, 'test/loss': 2.123737335205078, 'test/bleu': 22.196164092048317, 'test/num_examples': 3003, 'score': 1710.9601809978485, 'total_duration': 3671.4114067554474, 'accumulated_submission_time': 1710.9601809978485, 'accumulated_eval_time': 1960.2570941448212, 'accumulated_logging_time': 0.04469037055969238, 'global_step': 4747, 'preemption_count': 0}), (7122, {'train/accuracy': 0.604808509349823, 'train/loss': 2.0004196166992188, 'train/bleu': 29.179246592431355, 'validation/accuracy': 0.6162725687026978, 'validation/loss': 1.923214077949524, 'validation/bleu': 25.082469012091778, 'validation/num_examples': 3000, 'test/accuracy': 0.6211028099060059, 'test/loss': 1.8899255990982056, 'test/bleu': 24.172361442868134, 'test/num_examples': 3003, 'score': 2551.1123156547546, 'total_duration': 4931.874216079712, 'accumulated_submission_time': 2551.1123156547546, 'accumulated_eval_time': 2380.467398405075, 'accumulated_logging_time': 0.07047128677368164, 'global_step': 7122, 'preemption_count': 0}), (9497, {'train/accuracy': 0.6149968504905701, 'train/loss': 1.941268801689148, 'train/bleu': 29.185273045229128, 'validation/accuracy': 0.6318706274032593, 'validation/loss': 1.8118021488189697, 'validation/bleu': 26.343849434747742, 'validation/num_examples': 3000, 'test/accuracy': 0.6391029357910156, 'test/loss': 1.7555793523788452, 'test/bleu': 25.708341384430515, 'test/num_examples': 3003, 'score': 3391.2784502506256, 'total_duration': 6243.961631298065, 'accumulated_submission_time': 3391.2784502506256, 'accumulated_eval_time': 2852.2883291244507, 'accumulated_logging_time': 0.09632468223571777, 'global_step': 9497, 'preemption_count': 0}), (11872, {'train/accuracy': 0.6193379759788513, 'train/loss': 1.8914982080459595, 'train/bleu': 29.845714220736607, 'validation/accuracy': 0.6386777758598328, 'validation/loss': 1.7426763772964478, 'validation/bleu': 26.896069609013942, 'validation/num_examples': 3000, 'test/accuracy': 0.6488640904426575, 'test/loss': 1.6925368309020996, 'test/bleu': 25.705053088847816, 'test/num_examples': 3003, 'score': 4231.514441490173, 'total_duration': 7524.716588258743, 'accumulated_submission_time': 4231.514441490173, 'accumulated_eval_time': 3292.704610824585, 'accumulated_logging_time': 0.12389349937438965, 'global_step': 11872, 'preemption_count': 0}), (14247, {'train/accuracy': 0.6294394135475159, 'train/loss': 1.81944739818573, 'train/bleu': 30.450921466562807, 'validation/accuracy': 0.6437985897064209, 'validation/loss': 1.7072787284851074, 'validation/bleu': 27.1843550621261, 'validation/num_examples': 3000, 'test/accuracy': 0.6537911891937256, 'test/loss': 1.640441656112671, 'test/bleu': 26.303994280825968, 'test/num_examples': 3003, 'score': 5071.745926856995, 'total_duration': 8857.325303792953, 'accumulated_submission_time': 5071.745926856995, 'accumulated_eval_time': 3784.9792997837067, 'accumulated_logging_time': 0.1510317325592041, 'global_step': 14247, 'preemption_count': 0}), (16621, {'train/accuracy': 0.6266187429428101, 'train/loss': 1.8285259008407593, 'train/bleu': 30.356420384125588, 'validation/accuracy': 0.6463403701782227, 'validation/loss': 1.6791563034057617, 'validation/bleu': 27.10367908047491, 'validation/num_examples': 3000, 'test/accuracy': 0.6579513549804688, 'test/loss': 1.6053942441940308, 'test/bleu': 26.439084023361506, 'test/num_examples': 3003, 'score': 5911.688598394394, 'total_duration': 10503.013573646545, 'accumulated_submission_time': 5911.688598394394, 'accumulated_eval_time': 4590.6215217113495, 'accumulated_logging_time': 0.17937827110290527, 'global_step': 16621, 'preemption_count': 0}), (18996, {'train/accuracy': 0.6516609787940979, 'train/loss': 1.649769902229309, 'train/bleu': 31.447995353884966, 'validation/accuracy': 0.6500477194786072, 'validation/loss': 1.6590960025787354, 'validation/bleu': 26.475614046524264, 'validation/num_examples': 3000, 'test/accuracy': 0.6589855551719666, 'test/loss': 1.592655897140503, 'test/bleu': 26.80562274957389, 'test/num_examples': 3003, 'score': 6751.716594457626, 'total_duration': 12137.907361030579, 'accumulated_submission_time': 6751.716594457626, 'accumulated_eval_time': 5385.383625268936, 'accumulated_logging_time': 0.2077019214630127, 'global_step': 18996, 'preemption_count': 0}), (21370, {'train/accuracy': 0.6328192353248596, 'train/loss': 1.773112177848816, 'train/bleu': 30.559165503155825, 'validation/accuracy': 0.6511512398719788, 'validation/loss': 1.6486619710922241, 'validation/bleu': 27.135931286020796, 'validation/num_examples': 3000, 'test/accuracy': 0.6615420579910278, 'test/loss': 1.5799243450164795, 'test/bleu': 26.81812092467978, 'test/num_examples': 3003, 'score': 7591.700333595276, 'total_duration': 13657.880095720291, 'accumulated_submission_time': 7591.700333595276, 'accumulated_eval_time': 6065.267931461334, 'accumulated_logging_time': 0.23696160316467285, 'global_step': 21370, 'preemption_count': 0}), (23744, {'train/accuracy': 0.6332682967185974, 'train/loss': 1.783609390258789, 'train/bleu': 31.001136692097017, 'validation/accuracy': 0.6562844514846802, 'validation/loss': 1.6264350414276123, 'validation/bleu': 27.966466507153374, 'validation/num_examples': 3000, 'test/accuracy': 0.6650747060775757, 'test/loss': 1.5590084791183472, 'test/bleu': 27.07595711213196, 'test/num_examples': 3003, 'score': 8431.671695709229, 'total_duration': 15047.690311908722, 'accumulated_submission_time': 8431.671695709229, 'accumulated_eval_time': 6615.000554323196, 'accumulated_logging_time': 0.2675762176513672, 'global_step': 23744, 'preemption_count': 0}), (26119, {'train/accuracy': 0.6439311504364014, 'train/loss': 1.6942683458328247, 'train/bleu': 31.456285832045214, 'validation/accuracy': 0.6556025147438049, 'validation/loss': 1.624097228050232, 'validation/bleu': 27.50174142438485, 'validation/num_examples': 3000, 'test/accuracy': 0.6670966148376465, 'test/loss': 1.5462366342544556, 'test/bleu': 27.10684203663389, 'test/num_examples': 3003, 'score': 9271.90131354332, 'total_duration': 16336.804920434952, 'accumulated_submission_time': 9271.90131354332, 'accumulated_eval_time': 7063.780653953552, 'accumulated_logging_time': 0.2965688705444336, 'global_step': 26119, 'preemption_count': 0}), (28493, {'train/accuracy': 0.6390884518623352, 'train/loss': 1.7399277687072754, 'train/bleu': 31.07782237514248, 'validation/accuracy': 0.6567432284355164, 'validation/loss': 1.6084965467453003, 'validation/bleu': 27.803699121674846, 'validation/num_examples': 3000, 'test/accuracy': 0.6716286540031433, 'test/loss': 1.5281535387039185, 'test/bleu': 27.86535354891519, 'test/num_examples': 3003, 'score': 10112.068130731583, 'total_duration': 17797.767949342728, 'accumulated_submission_time': 10112.068130731583, 'accumulated_eval_time': 7684.471136808395, 'accumulated_logging_time': 0.3272538185119629, 'global_step': 28493, 'preemption_count': 0}), (30867, {'train/accuracy': 0.6361994743347168, 'train/loss': 1.7502000331878662, 'train/bleu': 30.96582954608702, 'validation/accuracy': 0.6586031317710876, 'validation/loss': 1.5999562740325928, 'validation/bleu': 27.939569403470813, 'validation/num_examples': 3000, 'test/accuracy': 0.6713497638702393, 'test/loss': 1.5222526788711548, 'test/bleu': 27.451590240782444, 'test/num_examples': 3003, 'score': 10952.097707033157, 'total_duration': 19123.161445617676, 'accumulated_submission_time': 10952.097707033157, 'accumulated_eval_time': 8169.727640390396, 'accumulated_logging_time': 0.3598606586456299, 'global_step': 30867, 'preemption_count': 0}), (33242, {'train/accuracy': 0.6436529755592346, 'train/loss': 1.703749179840088, 'train/bleu': 31.702394127417445, 'validation/accuracy': 0.6596198081970215, 'validation/loss': 1.5879870653152466, 'validation/bleu': 28.288413599126006, 'validation/num_examples': 3000, 'test/accuracy': 0.671884298324585, 'test/loss': 1.5101147890090942, 'test/bleu': 27.42189268731702, 'test/num_examples': 3003, 'score': 11792.311067819595, 'total_duration': 20476.148945569992, 'accumulated_submission_time': 11792.311067819595, 'accumulated_eval_time': 8682.395342111588, 'accumulated_logging_time': 0.39069294929504395, 'global_step': 33242, 'preemption_count': 0}), (35616, {'train/accuracy': 0.641491711139679, 'train/loss': 1.723802089691162, 'train/bleu': 31.370909674220155, 'validation/accuracy': 0.6618516445159912, 'validation/loss': 1.5850014686584473, 'validation/bleu': 28.14534491270295, 'validation/num_examples': 3000, 'test/accuracy': 0.6744756698608398, 'test/loss': 1.5015976428985596, 'test/bleu': 27.731952103228185, 'test/num_examples': 3003, 'score': 12632.403628587723, 'total_duration': 21773.656921863556, 'accumulated_submission_time': 12632.403628587723, 'accumulated_eval_time': 9139.705032587051, 'accumulated_logging_time': 0.4213588237762451, 'global_step': 35616, 'preemption_count': 0}), (37990, {'train/accuracy': 0.6566494703292847, 'train/loss': 1.6067266464233398, 'train/bleu': 32.589987116770835, 'validation/accuracy': 0.6636743545532227, 'validation/loss': 1.575439691543579, 'validation/bleu': 28.37420393676452, 'validation/num_examples': 3000, 'test/accuracy': 0.6746964454650879, 'test/loss': 1.4958630800247192, 'test/bleu': 27.57462241443689, 'test/num_examples': 3003, 'score': 13472.459535121918, 'total_duration': 23101.669969797134, 'accumulated_submission_time': 13472.459535121918, 'accumulated_eval_time': 9627.55661559105, 'accumulated_logging_time': 0.45209169387817383, 'global_step': 37990, 'preemption_count': 0}), (40364, {'train/accuracy': 0.6478399634361267, 'train/loss': 1.676770567893982, 'train/bleu': 31.48029070214986, 'validation/accuracy': 0.6622112393379211, 'validation/loss': 1.5716074705123901, 'validation/bleu': 28.142196735251872, 'validation/num_examples': 3000, 'test/accuracy': 0.6744988560676575, 'test/loss': 1.4857368469238281, 'test/bleu': 27.850430361041195, 'test/num_examples': 3003, 'score': 14312.40743112564, 'total_duration': 24606.04882979393, 'accumulated_submission_time': 14312.40743112564, 'accumulated_eval_time': 10291.879784822464, 'accumulated_logging_time': 0.4850766658782959, 'global_step': 40364, 'preemption_count': 0}), (42738, {'train/accuracy': 0.6446605920791626, 'train/loss': 1.6969213485717773, 'train/bleu': 31.174335951599705, 'validation/accuracy': 0.6645174622535706, 'validation/loss': 1.5625895261764526, 'validation/bleu': 28.429532815753745, 'validation/num_examples': 3000, 'test/accuracy': 0.6777177453041077, 'test/loss': 1.4755793809890747, 'test/bleu': 28.137474027499042, 'test/num_examples': 3003, 'score': 15152.398067235947, 'total_duration': 25978.725489377975, 'accumulated_submission_time': 15152.398067235947, 'accumulated_eval_time': 10824.458010911942, 'accumulated_logging_time': 0.5174057483673096, 'global_step': 42738, 'preemption_count': 0}), (45112, {'train/accuracy': 0.6525201201438904, 'train/loss': 1.646553874015808, 'train/bleu': 32.448058247862534, 'validation/accuracy': 0.6664517521858215, 'validation/loss': 1.550062656402588, 'validation/bleu': 28.67135209872841, 'validation/num_examples': 3000, 'test/accuracy': 0.6789379119873047, 'test/loss': 1.4708216190338135, 'test/bleu': 28.37878554052869, 'test/num_examples': 3003, 'score': 15992.489179372787, 'total_duration': 27374.3735370636, 'accumulated_submission_time': 15992.489179372787, 'accumulated_eval_time': 11379.908358573914, 'accumulated_logging_time': 0.5488555431365967, 'global_step': 45112, 'preemption_count': 0}), (47487, {'train/accuracy': 0.6500367522239685, 'train/loss': 1.6660417318344116, 'train/bleu': 32.292675817863426, 'validation/accuracy': 0.6654598116874695, 'validation/loss': 1.5579570531845093, 'validation/bleu': 28.594837733209264, 'validation/num_examples': 3000, 'test/accuracy': 0.6797629594802856, 'test/loss': 1.4692802429199219, 'test/bleu': 28.22441199348882, 'test/num_examples': 3003, 'score': 16832.455917835236, 'total_duration': 28707.885778665543, 'accumulated_submission_time': 16832.455917835236, 'accumulated_eval_time': 11873.347818136215, 'accumulated_logging_time': 0.5799081325531006, 'global_step': 47487, 'preemption_count': 0}), (49862, {'train/accuracy': 0.6516985297203064, 'train/loss': 1.6592146158218384, 'train/bleu': 31.26118472682359, 'validation/accuracy': 0.6688819527626038, 'validation/loss': 1.5371198654174805, 'validation/bleu': 29.092173467752627, 'validation/num_examples': 3000, 'test/accuracy': 0.6824705004692078, 'test/loss': 1.4507604837417603, 'test/bleu': 28.232030328801066, 'test/num_examples': 3003, 'score': 17672.611881494522, 'total_duration': 30103.71274662018, 'accumulated_submission_time': 17672.611881494522, 'accumulated_eval_time': 12428.909444093704, 'accumulated_logging_time': 0.6142618656158447, 'global_step': 49862, 'preemption_count': 0}), (52237, {'train/accuracy': 0.651114284992218, 'train/loss': 1.6532764434814453, 'train/bleu': 32.15875856369312, 'validation/accuracy': 0.670059859752655, 'validation/loss': 1.5286074876785278, 'validation/bleu': 29.083560310137855, 'validation/num_examples': 3000, 'test/accuracy': 0.6822032332420349, 'test/loss': 1.4496662616729736, 'test/bleu': 28.419362421745703, 'test/num_examples': 3003, 'score': 18512.59501695633, 'total_duration': 31551.95904159546, 'accumulated_submission_time': 18512.59501695633, 'accumulated_eval_time': 13037.063804626465, 'accumulated_logging_time': 0.6480474472045898, 'global_step': 52237, 'preemption_count': 0}), (54612, {'train/accuracy': 0.6498084664344788, 'train/loss': 1.666841983795166, 'train/bleu': 31.615262520847377, 'validation/accuracy': 0.6697251200675964, 'validation/loss': 1.5325440168380737, 'validation/bleu': 28.794014542916006, 'validation/num_examples': 3000, 'test/accuracy': 0.6830283403396606, 'test/loss': 1.4475644826889038, 'test/bleu': 28.666378067688967, 'test/num_examples': 3003, 'score': 19352.575419425964, 'total_duration': 32935.361577510834, 'accumulated_submission_time': 19352.575419425964, 'accumulated_eval_time': 13580.379266500473, 'accumulated_logging_time': 0.6796233654022217, 'global_step': 54612, 'preemption_count': 0}), (56987, {'train/accuracy': 0.663896381855011, 'train/loss': 1.5628291368484497, 'train/bleu': 32.592228093350805, 'validation/accuracy': 0.6697623133659363, 'validation/loss': 1.524965763092041, 'validation/bleu': 28.77605624550842, 'validation/num_examples': 3000, 'test/accuracy': 0.6824240684509277, 'test/loss': 1.442415714263916, 'test/bleu': 28.528350110436556, 'test/num_examples': 3003, 'score': 20192.615739822388, 'total_duration': 34299.67973899841, 'accumulated_submission_time': 20192.615739822388, 'accumulated_eval_time': 14104.548913478851, 'accumulated_logging_time': 0.7127649784088135, 'global_step': 56987, 'preemption_count': 0}), (59362, {'train/accuracy': 0.6547658443450928, 'train/loss': 1.639319896697998, 'train/bleu': 32.32660768039863, 'validation/accuracy': 0.6719445586204529, 'validation/loss': 1.517433762550354, 'validation/bleu': 28.431021789399217, 'validation/num_examples': 3000, 'test/accuracy': 0.6865028142929077, 'test/loss': 1.4255280494689941, 'test/bleu': 27.129961050076798, 'test/num_examples': 3003, 'score': 21032.84838962555, 'total_duration': 35920.757283210754, 'accumulated_submission_time': 21032.84838962555, 'accumulated_eval_time': 14885.285350084305, 'accumulated_logging_time': 0.7453715801239014, 'global_step': 59362, 'preemption_count': 0}), (61736, {'train/accuracy': 0.6519191861152649, 'train/loss': 1.6469298601150513, 'train/bleu': 32.12458710119513, 'validation/accuracy': 0.673072874546051, 'validation/loss': 1.5103808641433716, 'validation/bleu': 29.256527875615028, 'validation/num_examples': 3000, 'test/accuracy': 0.6850618720054626, 'test/loss': 1.4241111278533936, 'test/bleu': 28.778837217794965, 'test/num_examples': 3003, 'score': 21872.760452747345, 'total_duration': 37275.04721617699, 'accumulated_submission_time': 21872.760452747345, 'accumulated_eval_time': 15399.552985191345, 'accumulated_logging_time': 0.7798454761505127, 'global_step': 61736, 'preemption_count': 0}), (64110, {'train/accuracy': 0.6602867245674133, 'train/loss': 1.5844924449920654, 'train/bleu': 32.69674771030805, 'validation/accuracy': 0.6743127703666687, 'validation/loss': 1.5003536939620972, 'validation/bleu': 29.04694521750138, 'validation/num_examples': 3000, 'test/accuracy': 0.6873162388801575, 'test/loss': 1.4133727550506592, 'test/bleu': 28.827913304890142, 'test/num_examples': 3003, 'score': 22712.739403009415, 'total_duration': 38790.858786821365, 'accumulated_submission_time': 22712.739403009415, 'accumulated_eval_time': 16075.27707862854, 'accumulated_logging_time': 0.812446117401123, 'global_step': 64110, 'preemption_count': 0}), (66485, {'train/accuracy': 0.6578103303909302, 'train/loss': 1.6158467531204224, 'train/bleu': 32.83270106756466, 'validation/accuracy': 0.6757386922836304, 'validation/loss': 1.4923642873764038, 'validation/bleu': 29.64027073611295, 'validation/num_examples': 3000, 'test/accuracy': 0.691046416759491, 'test/loss': 1.3991174697875977, 'test/bleu': 29.41162720824399, 'test/num_examples': 3003, 'score': 23552.78756380081, 'total_duration': 40321.3447868824, 'accumulated_submission_time': 23552.78756380081, 'accumulated_eval_time': 16765.60342478752, 'accumulated_logging_time': 0.8476648330688477, 'global_step': 66485, 'preemption_count': 0}), (68860, {'train/accuracy': 0.6925715208053589, 'train/loss': 1.3943202495574951, 'train/bleu': 35.164799300747305, 'validation/accuracy': 0.675986647605896, 'validation/loss': 1.4907090663909912, 'validation/bleu': 29.37226423439722, 'validation/num_examples': 3000, 'test/accuracy': 0.6896519660949707, 'test/loss': 1.3980993032455444, 'test/bleu': 29.145196225741316, 'test/num_examples': 3003, 'score': 24392.84364748001, 'total_duration': 41680.414498806, 'accumulated_submission_time': 24392.84364748001, 'accumulated_eval_time': 17284.506219387054, 'accumulated_logging_time': 0.8824207782745361, 'global_step': 68860, 'preemption_count': 0}), (71235, {'train/accuracy': 0.663612425327301, 'train/loss': 1.5702091455459595, 'train/bleu': 33.04904270814784, 'validation/accuracy': 0.6764578223228455, 'validation/loss': 1.4874941110610962, 'validation/bleu': 29.29003691272471, 'validation/num_examples': 3000, 'test/accuracy': 0.6922782063484192, 'test/loss': 1.3883384466171265, 'test/bleu': 29.602084105542623, 'test/num_examples': 3003, 'score': 25233.028274297714, 'total_duration': 43225.82240843773, 'accumulated_submission_time': 25233.028274297714, 'accumulated_eval_time': 17989.61869287491, 'accumulated_logging_time': 0.9181523323059082, 'global_step': 71235, 'preemption_count': 0}), (73610, {'train/accuracy': 0.6637750864028931, 'train/loss': 1.5700886249542236, 'train/bleu': 32.387814931952754, 'validation/accuracy': 0.6769413948059082, 'validation/loss': 1.4775288105010986, 'validation/bleu': 29.41112830125952, 'validation/num_examples': 3000, 'test/accuracy': 0.6927198171615601, 'test/loss': 1.3803156614303589, 'test/bleu': 29.329502689351866, 'test/num_examples': 3003, 'score': 26073.243137598038, 'total_duration': 44682.91942191124, 'accumulated_submission_time': 26073.243137598038, 'accumulated_eval_time': 18606.387011051178, 'accumulated_logging_time': 0.9560637474060059, 'global_step': 73610, 'preemption_count': 0}), (75985, {'train/accuracy': 0.6715718507766724, 'train/loss': 1.5168931484222412, 'train/bleu': 33.4930723722878, 'validation/accuracy': 0.6797311902046204, 'validation/loss': 1.4644559621810913, 'validation/bleu': 29.64138507890422, 'validation/num_examples': 3000, 'test/accuracy': 0.6943234205245972, 'test/loss': 1.3734365701675415, 'test/bleu': 29.62003987298333, 'test/num_examples': 3003, 'score': 26913.46995139122, 'total_duration': 46109.13782072067, 'accumulated_submission_time': 26913.46995139122, 'accumulated_eval_time': 19192.265601158142, 'accumulated_logging_time': 0.9935698509216309, 'global_step': 75985, 'preemption_count': 0}), (78359, {'train/accuracy': 0.6638250946998596, 'train/loss': 1.5705655813217163, 'train/bleu': 32.52902754344788, 'validation/accuracy': 0.6803511381149292, 'validation/loss': 1.4611443281173706, 'validation/bleu': 29.507351771603417, 'validation/num_examples': 3000, 'test/accuracy': 0.6962175369262695, 'test/loss': 1.3679414987564087, 'test/bleu': 29.716893139832724, 'test/num_examples': 3003, 'score': 27753.634783506393, 'total_duration': 47451.10957407951, 'accumulated_submission_time': 27753.634783506393, 'accumulated_eval_time': 19693.95895600319, 'accumulated_logging_time': 1.03102707862854, 'global_step': 78359, 'preemption_count': 0}), (80733, {'train/accuracy': 0.6645106077194214, 'train/loss': 1.572019338607788, 'train/bleu': 33.2519705355767, 'validation/accuracy': 0.6811570525169373, 'validation/loss': 1.4543503522872925, 'validation/bleu': 30.089135107250858, 'validation/num_examples': 3000, 'test/accuracy': 0.6961246132850647, 'test/loss': 1.3614065647125244, 'test/bleu': 29.882335007945937, 'test/num_examples': 3003, 'score': 28593.58155298233, 'total_duration': 48813.9661796093, 'accumulated_submission_time': 28593.58155298233, 'accumulated_eval_time': 20216.756098031998, 'accumulated_logging_time': 1.0682952404022217, 'global_step': 80733, 'preemption_count': 0}), (83108, {'train/accuracy': 0.6732177138328552, 'train/loss': 1.5030677318572998, 'train/bleu': 33.89015855149095, 'validation/accuracy': 0.6820250153541565, 'validation/loss': 1.4465928077697754, 'validation/bleu': 29.565130484491412, 'validation/num_examples': 3000, 'test/accuracy': 0.6982976198196411, 'test/loss': 1.349581003189087, 'test/bleu': 30.016736440843538, 'test/num_examples': 3003, 'score': 29433.65891289711, 'total_duration': 50206.2528424263, 'accumulated_submission_time': 29433.65891289711, 'accumulated_eval_time': 20768.853723526, 'accumulated_logging_time': 1.1046974658966064, 'global_step': 83108, 'preemption_count': 0}), (85483, {'train/accuracy': 0.6708388328552246, 'train/loss': 1.520539402961731, 'train/bleu': 33.3104744951593, 'validation/accuracy': 0.6837112903594971, 'validation/loss': 1.4417012929916382, 'validation/bleu': 30.142586435709703, 'validation/num_examples': 3000, 'test/accuracy': 0.698448657989502, 'test/loss': 1.3438206911087036, 'test/bleu': 29.82206997277747, 'test/num_examples': 3003, 'score': 30273.729954242706, 'total_duration': 51594.460906744, 'accumulated_submission_time': 30273.729954242706, 'accumulated_eval_time': 21316.877049922943, 'accumulated_logging_time': 1.142566204071045, 'global_step': 85483, 'preemption_count': 0}), (87858, {'train/accuracy': 0.6888561248779297, 'train/loss': 1.4054269790649414, 'train/bleu': 34.55736403714124, 'validation/accuracy': 0.6858067512512207, 'validation/loss': 1.4331176280975342, 'validation/bleu': 29.992162601588777, 'validation/num_examples': 3000, 'test/accuracy': 0.698506772518158, 'test/loss': 1.3392562866210938, 'test/bleu': 29.978959453072076, 'test/num_examples': 3003, 'score': 31113.744668483734, 'total_duration': 53040.19746589661, 'accumulated_submission_time': 31113.744668483734, 'accumulated_eval_time': 21922.486659526825, 'accumulated_logging_time': 1.1804225444793701, 'global_step': 87858, 'preemption_count': 0}), (90234, {'train/accuracy': 0.6757356524467468, 'train/loss': 1.4885623455047607, 'train/bleu': 33.67554884277297, 'validation/accuracy': 0.68563312292099, 'validation/loss': 1.4297630786895752, 'validation/bleu': 29.696829275004344, 'validation/num_examples': 3000, 'test/accuracy': 0.7016443014144897, 'test/loss': 1.3300366401672363, 'test/bleu': 29.90019334184747, 'test/num_examples': 3003, 'score': 31953.942955493927, 'total_duration': 54574.322892427444, 'accumulated_submission_time': 31953.942955493927, 'accumulated_eval_time': 22616.300694704056, 'accumulated_logging_time': 1.2178633213043213, 'global_step': 90234, 'preemption_count': 0}), (92610, {'train/accuracy': 0.6737697720527649, 'train/loss': 1.50662362575531, 'train/bleu': 33.71377058019944, 'validation/accuracy': 0.6864514946937561, 'validation/loss': 1.4230599403381348, 'validation/bleu': 30.020258720494507, 'validation/num_examples': 3000, 'test/accuracy': 0.7028412222862244, 'test/loss': 1.3216743469238281, 'test/bleu': 30.292554115791944, 'test/num_examples': 3003, 'score': 32794.13788151741, 'total_duration': 55952.358438014984, 'accumulated_submission_time': 32794.13788151741, 'accumulated_eval_time': 23154.026401281357, 'accumulated_logging_time': 1.2572076320648193, 'global_step': 92610, 'preemption_count': 0}), (94985, {'train/accuracy': 0.6820225715637207, 'train/loss': 1.4462394714355469, 'train/bleu': 34.13055095861968, 'validation/accuracy': 0.6877161860466003, 'validation/loss': 1.4159722328186035, 'validation/bleu': 30.236845114728684, 'validation/num_examples': 3000, 'test/accuracy': 0.7035384774208069, 'test/loss': 1.316325306892395, 'test/bleu': 30.436514797787588, 'test/num_examples': 3003, 'score': 33634.26413846016, 'total_duration': 57375.692895412445, 'accumulated_submission_time': 33634.26413846016, 'accumulated_eval_time': 23737.11946773529, 'accumulated_logging_time': 1.2959558963775635, 'global_step': 94985, 'preemption_count': 0}), (97360, {'train/accuracy': 0.6778927445411682, 'train/loss': 1.4817765951156616, 'train/bleu': 33.92635535254963, 'validation/accuracy': 0.6891545057296753, 'validation/loss': 1.410452127456665, 'validation/bleu': 30.186419436898866, 'validation/num_examples': 3000, 'test/accuracy': 0.7068619132041931, 'test/loss': 1.306337833404541, 'test/bleu': 30.644997144646442, 'test/num_examples': 3003, 'score': 34474.2428150177, 'total_duration': 58836.77590274811, 'accumulated_submission_time': 34474.2428150177, 'accumulated_eval_time': 24358.108916282654, 'accumulated_logging_time': 1.3352117538452148, 'global_step': 97360, 'preemption_count': 0}), (99735, {'train/accuracy': 0.6772194504737854, 'train/loss': 1.4831637144088745, 'train/bleu': 33.94047766861321, 'validation/accuracy': 0.6899604201316833, 'validation/loss': 1.4045439958572388, 'validation/bleu': 30.348874012448068, 'validation/num_examples': 3000, 'test/accuracy': 0.7069200277328491, 'test/loss': 1.2959305047988892, 'test/bleu': 30.6400977790549, 'test/num_examples': 3003, 'score': 35314.15435934067, 'total_duration': 60202.79884767532, 'accumulated_submission_time': 35314.15435934067, 'accumulated_eval_time': 24884.105031967163, 'accumulated_logging_time': 1.375361442565918, 'global_step': 99735, 'preemption_count': 0}), (102110, {'train/accuracy': 0.6823187470436096, 'train/loss': 1.446167230606079, 'train/bleu': 34.765234065491775, 'validation/accuracy': 0.6899108290672302, 'validation/loss': 1.3995331525802612, 'validation/bleu': 30.400901601894276, 'validation/num_examples': 3000, 'test/accuracy': 0.7071989178657532, 'test/loss': 1.2938815355300903, 'test/bleu': 30.771823174514257, 'test/num_examples': 3003, 'score': 36154.047691106796, 'total_duration': 61602.90172743797, 'accumulated_submission_time': 36154.047691106796, 'accumulated_eval_time': 25444.199846982956, 'accumulated_logging_time': 1.414492130279541, 'global_step': 102110, 'preemption_count': 0}), (104486, {'train/accuracy': 0.6826587319374084, 'train/loss': 1.4512836933135986, 'train/bleu': 34.46536235091862, 'validation/accuracy': 0.6910763382911682, 'validation/loss': 1.3928955793380737, 'validation/bleu': 30.427187187595266, 'validation/num_examples': 3000, 'test/accuracy': 0.70707106590271, 'test/loss': 1.2888002395629883, 'test/bleu': 30.534439506816717, 'test/num_examples': 3003, 'score': 36994.13355565071, 'total_duration': 63073.723007678986, 'accumulated_submission_time': 36994.13355565071, 'accumulated_eval_time': 26074.819528579712, 'accumulated_logging_time': 1.4544637203216553, 'global_step': 104486, 'preemption_count': 0}), (106862, {'train/accuracy': 0.6929945349693298, 'train/loss': 1.3875455856323242, 'train/bleu': 35.6936682202158, 'validation/accuracy': 0.6926262378692627, 'validation/loss': 1.3891054391860962, 'validation/bleu': 30.321860371372377, 'validation/num_examples': 3000, 'test/accuracy': 0.7095810770988464, 'test/loss': 1.2826364040374756, 'test/bleu': 30.553318604965003, 'test/num_examples': 3003, 'score': 37834.16900777817, 'total_duration': 64617.951558828354, 'accumulated_submission_time': 37834.16900777817, 'accumulated_eval_time': 26778.895754098892, 'accumulated_logging_time': 1.4950628280639648, 'global_step': 106862, 'preemption_count': 0}), (109238, {'train/accuracy': 0.6881809234619141, 'train/loss': 1.4110008478164673, 'train/bleu': 34.792418856647195, 'validation/accuracy': 0.6915723085403442, 'validation/loss': 1.3885592222213745, 'validation/bleu': 30.53146184710506, 'validation/num_examples': 3000, 'test/accuracy': 0.7090697884559631, 'test/loss': 1.2797530889511108, 'test/bleu': 30.7142415724196, 'test/num_examples': 3003, 'score': 38674.227446079254, 'total_duration': 66075.62375807762, 'accumulated_submission_time': 38674.227446079254, 'accumulated_eval_time': 27396.391726732254, 'accumulated_logging_time': 1.5361199378967285, 'global_step': 109238, 'preemption_count': 0}), (111614, {'train/accuracy': 0.6850574612617493, 'train/loss': 1.43190598487854, 'train/bleu': 34.64420152808263, 'validation/accuracy': 0.6938413381576538, 'validation/loss': 1.3793723583221436, 'validation/bleu': 30.713396510455496, 'validation/num_examples': 3000, 'test/accuracy': 0.7106850147247314, 'test/loss': 1.2727055549621582, 'test/bleu': 30.786169768048005, 'test/num_examples': 3003, 'score': 39514.40298914909, 'total_duration': 67453.68859291077, 'accumulated_submission_time': 39514.40298914909, 'accumulated_eval_time': 27934.165376663208, 'accumulated_logging_time': 1.5760877132415771, 'global_step': 111614, 'preemption_count': 0}), (113990, {'train/accuracy': 0.6922797560691833, 'train/loss': 1.3891639709472656, 'train/bleu': 35.342772251239296, 'validation/accuracy': 0.6943125128746033, 'validation/loss': 1.3756449222564697, 'validation/bleu': 30.757281576648342, 'validation/num_examples': 3000, 'test/accuracy': 0.711661159992218, 'test/loss': 1.2692705392837524, 'test/bleu': 31.040662341172386, 'test/num_examples': 3003, 'score': 40354.49650359154, 'total_duration': 68950.13110136986, 'accumulated_submission_time': 40354.49650359154, 'accumulated_eval_time': 28590.395575761795, 'accumulated_logging_time': 1.6191487312316895, 'global_step': 113990, 'preemption_count': 0}), (116366, {'train/accuracy': 0.6906039714813232, 'train/loss': 1.3968476057052612, 'train/bleu': 34.79750781778418, 'validation/accuracy': 0.6947464942932129, 'validation/loss': 1.373445987701416, 'validation/bleu': 30.66973494550261, 'validation/num_examples': 3000, 'test/accuracy': 0.7128929495811462, 'test/loss': 1.264074683189392, 'test/bleu': 31.160097992334794, 'test/num_examples': 3003, 'score': 41194.70844531059, 'total_duration': 70359.9132270813, 'accumulated_submission_time': 41194.70844531059, 'accumulated_eval_time': 29159.84848332405, 'accumulated_logging_time': 1.660917043685913, 'global_step': 116366, 'preemption_count': 0}), (118741, {'train/accuracy': 0.6908093690872192, 'train/loss': 1.4008818864822388, 'train/bleu': 34.89803540192062, 'validation/accuracy': 0.6953044533729553, 'validation/loss': 1.3693207502365112, 'validation/bleu': 30.95589799049646, 'validation/num_examples': 3000, 'test/accuracy': 0.7127767205238342, 'test/loss': 1.262351393699646, 'test/bleu': 30.769333545080798, 'test/num_examples': 3003, 'score': 42034.721598386765, 'total_duration': 71731.30040001869, 'accumulated_submission_time': 42034.721598386765, 'accumulated_eval_time': 29691.103921175003, 'accumulated_logging_time': 1.7029705047607422, 'global_step': 118741, 'preemption_count': 0}), (121116, {'train/accuracy': 0.6932882070541382, 'train/loss': 1.388006567955017, 'train/bleu': 35.52039251433238, 'validation/accuracy': 0.6962592005729675, 'validation/loss': 1.367598533630371, 'validation/bleu': 30.874046124594287, 'validation/num_examples': 3000, 'test/accuracy': 0.7138457894325256, 'test/loss': 1.2584335803985596, 'test/bleu': 31.123645584563633, 'test/num_examples': 3003, 'score': 42874.60907793045, 'total_duration': 73184.37866711617, 'accumulated_submission_time': 42874.60907793045, 'accumulated_eval_time': 30304.17706489563, 'accumulated_logging_time': 1.744511365890503, 'global_step': 121116, 'preemption_count': 0}), (123491, {'train/accuracy': 0.6912440061569214, 'train/loss': 1.395629644393921, 'train/bleu': 35.35323909995653, 'validation/accuracy': 0.6952672600746155, 'validation/loss': 1.367558479309082, 'validation/bleu': 30.844249592137086, 'validation/num_examples': 3000, 'test/accuracy': 0.7139503955841064, 'test/loss': 1.255860447883606, 'test/bleu': 31.129922784276456, 'test/num_examples': 3003, 'score': 43714.75734376907, 'total_duration': 74557.78880643845, 'accumulated_submission_time': 43714.75734376907, 'accumulated_eval_time': 30837.32000207901, 'accumulated_logging_time': 1.7871100902557373, 'global_step': 123491, 'preemption_count': 0}), (125866, {'train/accuracy': 0.697185218334198, 'train/loss': 1.3619694709777832, 'train/bleu': 35.78074525428276, 'validation/accuracy': 0.695651650428772, 'validation/loss': 1.3671724796295166, 'validation/bleu': 30.911010119255785, 'validation/num_examples': 3000, 'test/accuracy': 0.7137179970741272, 'test/loss': 1.25685453414917, 'test/bleu': 30.930439140655892, 'test/num_examples': 3003, 'score': 44554.68297743797, 'total_duration': 75943.4162247181, 'accumulated_submission_time': 44554.68297743797, 'accumulated_eval_time': 31382.90145254135, 'accumulated_logging_time': 1.8318068981170654, 'global_step': 125866, 'preemption_count': 0}), (128241, {'train/accuracy': 0.6922347545623779, 'train/loss': 1.3835301399230957, 'train/bleu': 35.30423628524667, 'validation/accuracy': 0.6954284310340881, 'validation/loss': 1.3652565479278564, 'validation/bleu': 30.88262735410851, 'validation/num_examples': 3000, 'test/accuracy': 0.7137528657913208, 'test/loss': 1.2547988891601562, 'test/bleu': 31.126286255052236, 'test/num_examples': 3003, 'score': 45394.836584329605, 'total_duration': 77351.26991295815, 'accumulated_submission_time': 45394.836584329605, 'accumulated_eval_time': 31950.483147382736, 'accumulated_logging_time': 1.8745992183685303, 'global_step': 128241, 'preemption_count': 0}), (130616, {'train/accuracy': 0.6963204741477966, 'train/loss': 1.366992473602295, 'train/bleu': 35.57011387339867, 'validation/accuracy': 0.6954284310340881, 'validation/loss': 1.3648079633712769, 'validation/bleu': 30.946973319770205, 'validation/num_examples': 3000, 'test/accuracy': 0.7141479253768921, 'test/loss': 1.2541664838790894, 'test/bleu': 31.103261886181247, 'test/num_examples': 3003, 'score': 46234.92179393768, 'total_duration': 78754.49300599098, 'accumulated_submission_time': 46234.92179393768, 'accumulated_eval_time': 32513.490881443024, 'accumulated_logging_time': 1.928779125213623, 'global_step': 130616, 'preemption_count': 0}), (132991, {'train/accuracy': 0.69627445936203, 'train/loss': 1.36464262008667, 'train/bleu': 35.58865724493143, 'validation/accuracy': 0.6954532265663147, 'validation/loss': 1.3649661540985107, 'validation/bleu': 31.040501645749234, 'validation/num_examples': 3000, 'test/accuracy': 0.7140085101127625, 'test/loss': 1.2541321516036987, 'test/bleu': 31.145809433459892, 'test/num_examples': 3003, 'score': 47074.85367107391, 'total_duration': 80159.94454312325, 'accumulated_submission_time': 47074.85367107391, 'accumulated_eval_time': 33078.891793727875, 'accumulated_logging_time': 1.9719855785369873, 'global_step': 132991, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6945627331733704, 'train/loss': 1.3744174242019653, 'train/bleu': 35.631271648808266, 'validation/accuracy': 0.6954532265663147, 'validation/loss': 1.3649758100509644, 'validation/bleu': 31.040501645749234, 'validation/num_examples': 3000, 'test/accuracy': 0.7140317559242249, 'test/loss': 1.2541394233703613, 'test/bleu': 31.13090825160175, 'test/num_examples': 3003, 'score': 47195.332033634186, 'total_duration': 80834.32544207573, 'accumulated_submission_time': 47195.332033634186, 'accumulated_eval_time': 33632.739612579346, 'accumulated_logging_time': 2.015644073486328, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0211 03:50:43.983927 140225696298816 submission_runner.py:586] Timing: 47195.332033634186
I0211 03:50:43.983979 140225696298816 submission_runner.py:588] Total number of evals: 58
I0211 03:50:43.984019 140225696298816 submission_runner.py:589] ====================
I0211 03:50:43.985085 140225696298816 submission_runner.py:673] Final wmt score: 46060.09185528755
