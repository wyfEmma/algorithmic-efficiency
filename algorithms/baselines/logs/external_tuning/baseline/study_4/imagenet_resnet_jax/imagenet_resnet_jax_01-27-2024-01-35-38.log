python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3614520272 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_01-27-2024-01-35-38.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0127 01:36:02.826349 140169137129280 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax.
I0127 01:36:03.987496 140169137129280 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0127 01:36:03.988358 140169137129280 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0127 01:36:03.988492 140169137129280 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0127 01:36:03.989644 140169137129280 submission_runner.py:542] Using RNG seed 3614520272
I0127 01:36:10.274229 140169137129280 submission_runner.py:551] --- Tuning run 1/5 ---
I0127 01:36:10.274441 140169137129280 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1.
I0127 01:36:10.274615 140169137129280 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1/hparams.json.
I0127 01:36:10.455862 140169137129280 submission_runner.py:206] Initializing dataset.
I0127 01:36:10.472371 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:36:10.482613 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 01:36:10.864146 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:36:12.066549 140169137129280 submission_runner.py:213] Initializing model.
I0127 01:36:22.055039 140169137129280 submission_runner.py:255] Initializing optimizer.
I0127 01:36:23.731436 140169137129280 submission_runner.py:262] Initializing metrics bundle.
I0127 01:36:23.731616 140169137129280 submission_runner.py:280] Initializing checkpoint and logger.
I0127 01:36:23.732705 140169137129280 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0127 01:36:23.732867 140169137129280 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0127 01:36:24.140002 140169137129280 logger_utils.py:220] Unable to record git information. Continuing without it.
I0127 01:36:24.518108 140169137129280 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1/flags_0.json.
I0127 01:36:24.528529 140169137129280 submission_runner.py:314] Starting training loop.
2024-01-27 01:37:30.050566: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2024-01-27 01:37:32.608135: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
I0127 01:37:34.177611 139996725995264 logging_writer.py:48] [0] global_step=0, grad_norm=0.5981332659721375, loss=6.92013692855835
I0127 01:37:34.195035 140169137129280 spec.py:321] Evaluating on the training split.
I0127 01:37:35.206250 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:37:35.216354 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 01:37:35.301970 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:37:48.686816 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 01:37:50.445777 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:37:50.455273 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 01:37:50.497014 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 01:38:07.034181 140169137129280 spec.py:349] Evaluating on the test split.
I0127 01:38:07.833976 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0127 01:38:07.840276 140169137129280 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0127 01:38:07.878273 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0127 01:38:11.866910 140169137129280 submission_runner.py:408] Time since start: 107.34s, 	Step: 1, 	{'train/accuracy': 0.0007573341717943549, 'train/loss': 6.910542011260986, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 69.66642546653748, 'total_duration': 107.33833169937134, 'accumulated_submission_time': 69.66642546653748, 'accumulated_eval_time': 37.67181849479675, 'accumulated_logging_time': 0}
I0127 01:38:11.886567 139976700307200 logging_writer.py:48] [1] accumulated_eval_time=37.671818, accumulated_logging_time=0, accumulated_submission_time=69.666425, global_step=1, preemption_count=0, score=69.666425, test/accuracy=0.000600, test/loss=6.910250, test/num_examples=10000, total_duration=107.338332, train/accuracy=0.000757, train/loss=6.910542, validation/accuracy=0.000960, validation/loss=6.910244, validation/num_examples=50000
I0127 01:38:45.459033 139976691914496 logging_writer.py:48] [100] global_step=100, grad_norm=0.5784755945205688, loss=6.902760028839111
I0127 01:39:19.132485 139976700307200 logging_writer.py:48] [200] global_step=200, grad_norm=0.5876134634017944, loss=6.86502742767334
I0127 01:39:52.822512 139976691914496 logging_writer.py:48] [300] global_step=300, grad_norm=0.6389508843421936, loss=6.787942886352539
I0127 01:40:26.563796 139976700307200 logging_writer.py:48] [400] global_step=400, grad_norm=0.6632044911384583, loss=6.685791015625
I0127 01:41:00.281399 139976691914496 logging_writer.py:48] [500] global_step=500, grad_norm=0.711142897605896, loss=6.5861616134643555
I0127 01:41:34.051924 139976700307200 logging_writer.py:48] [600] global_step=600, grad_norm=0.7550148963928223, loss=6.546924591064453
I0127 01:42:07.794723 139976691914496 logging_writer.py:48] [700] global_step=700, grad_norm=0.8050522208213806, loss=6.420192718505859
I0127 01:42:41.554238 139976700307200 logging_writer.py:48] [800] global_step=800, grad_norm=0.8635852932929993, loss=6.26979923248291
I0127 01:43:15.298031 139976691914496 logging_writer.py:48] [900] global_step=900, grad_norm=1.2339413166046143, loss=6.202386856079102
I0127 01:43:49.175522 139976700307200 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.612676739692688, loss=6.1813459396362305
I0127 01:44:22.929189 139976691914496 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.245516300201416, loss=6.081448078155518
I0127 01:44:56.704722 139976700307200 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.7211617231369019, loss=6.047825336456299
I0127 01:45:30.459543 139976691914496 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.2765564918518066, loss=5.907381534576416
I0127 01:46:04.231181 139976700307200 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.095381498336792, loss=5.835407257080078
I0127 01:46:37.993053 139976691914496 logging_writer.py:48] [1500] global_step=1500, grad_norm=3.1859354972839355, loss=5.796683311462402
I0127 01:46:42.144515 140169137129280 spec.py:321] Evaluating on the training split.
I0127 01:46:49.378988 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 01:46:57.801282 140169137129280 spec.py:349] Evaluating on the test split.
I0127 01:47:00.066009 140169137129280 submission_runner.py:408] Time since start: 635.54s, 	Step: 1514, 	{'train/accuracy': 0.07354113459587097, 'train/loss': 5.336194038391113, 'validation/accuracy': 0.06785999983549118, 'validation/loss': 5.403800964355469, 'validation/num_examples': 50000, 'test/accuracy': 0.04960000142455101, 'test/loss': 5.629796504974365, 'test/num_examples': 10000, 'score': 579.8638372421265, 'total_duration': 635.5374145507812, 'accumulated_submission_time': 579.8638372421265, 'accumulated_eval_time': 55.593273878097534, 'accumulated_logging_time': 0.029314041137695312}
I0127 01:47:00.082832 139976708699904 logging_writer.py:48] [1514] accumulated_eval_time=55.593274, accumulated_logging_time=0.029314, accumulated_submission_time=579.863837, global_step=1514, preemption_count=0, score=579.863837, test/accuracy=0.049600, test/loss=5.629797, test/num_examples=10000, total_duration=635.537415, train/accuracy=0.073541, train/loss=5.336194, validation/accuracy=0.067860, validation/loss=5.403801, validation/num_examples=50000
I0127 01:47:29.456302 139976717092608 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.3478713035583496, loss=5.733814716339111
I0127 01:48:03.192340 139976708699904 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.9511139392852783, loss=5.678197383880615
I0127 01:48:36.944776 139976717092608 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.910611391067505, loss=5.58250617980957
I0127 01:49:10.712134 139976708699904 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.1299734115600586, loss=5.535990238189697
I0127 01:49:44.582821 139976717092608 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.685263633728027, loss=5.428093433380127
I0127 01:50:18.343025 139976708699904 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.380779266357422, loss=5.401987075805664
I0127 01:50:52.082368 139976717092608 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.381535053253174, loss=5.359907150268555
I0127 01:51:25.846023 139976708699904 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.170131206512451, loss=5.367110729217529
I0127 01:51:59.622238 139976717092608 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.9330976009368896, loss=5.245477676391602
I0127 01:52:33.376356 139976708699904 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.5452044010162354, loss=5.249812602996826
I0127 01:53:07.110246 139976717092608 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.946106433868408, loss=5.195734977722168
I0127 01:53:40.849115 139976708699904 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.6976447105407715, loss=5.182199001312256
I0127 01:54:14.605979 139976717092608 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.1785807609558105, loss=5.124695777893066
I0127 01:54:48.375881 139976708699904 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.169508457183838, loss=5.097527503967285
I0127 01:55:22.137030 139976717092608 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.755920648574829, loss=4.911209583282471
I0127 01:55:30.151255 140169137129280 spec.py:321] Evaluating on the training split.
I0127 01:55:37.488861 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 01:55:46.052417 140169137129280 spec.py:349] Evaluating on the test split.
I0127 01:55:48.388292 140169137129280 submission_runner.py:408] Time since start: 1163.86s, 	Step: 3025, 	{'train/accuracy': 0.1801857352256775, 'train/loss': 4.261962890625, 'validation/accuracy': 0.16099999845027924, 'validation/loss': 4.37910270690918, 'validation/num_examples': 50000, 'test/accuracy': 0.12120000272989273, 'test/loss': 4.7993268966674805, 'test/num_examples': 10000, 'score': 1089.8733115196228, 'total_duration': 1163.8596937656403, 'accumulated_submission_time': 1089.8733115196228, 'accumulated_eval_time': 73.83026790618896, 'accumulated_logging_time': 0.05568432807922363}
I0127 01:55:48.406881 140005842306816 logging_writer.py:48] [3025] accumulated_eval_time=73.830268, accumulated_logging_time=0.055684, accumulated_submission_time=1089.873312, global_step=3025, preemption_count=0, score=1089.873312, test/accuracy=0.121200, test/loss=4.799327, test/num_examples=10000, total_duration=1163.859694, train/accuracy=0.180186, train/loss=4.261963, validation/accuracy=0.161000, validation/loss=4.379103, validation/num_examples=50000
I0127 01:56:14.023869 140005850699520 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5064384937286377, loss=4.966598987579346
I0127 01:56:47.744388 140005842306816 logging_writer.py:48] [3200] global_step=3200, grad_norm=8.116687774658203, loss=4.887415885925293
I0127 01:57:21.498626 140005850699520 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.777766704559326, loss=4.916378974914551
I0127 01:57:55.271459 140005842306816 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.327110767364502, loss=4.885592460632324
I0127 01:58:29.060008 140005850699520 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.672607421875, loss=4.750488758087158
I0127 01:59:02.826621 140005842306816 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.565009117126465, loss=4.725878715515137
I0127 01:59:36.577388 140005850699520 logging_writer.py:48] [3700] global_step=3700, grad_norm=5.705304145812988, loss=4.714261054992676
I0127 02:00:10.348721 140005842306816 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.639308452606201, loss=4.583737373352051
I0127 02:00:44.116585 140005850699520 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.773007869720459, loss=4.588313102722168
I0127 02:01:17.883779 140005842306816 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.557863235473633, loss=4.619786262512207
I0127 02:01:51.699917 140005850699520 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.62069034576416, loss=4.591747283935547
I0127 02:02:25.460693 140005842306816 logging_writer.py:48] [4200] global_step=4200, grad_norm=7.86076545715332, loss=4.514934539794922
I0127 02:02:59.239312 140005850699520 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.506117343902588, loss=4.533120155334473
I0127 02:03:33.009691 140005842306816 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.28261137008667, loss=4.4632568359375
I0127 02:04:06.793377 140005850699520 logging_writer.py:48] [4500] global_step=4500, grad_norm=6.539878845214844, loss=4.374695777893066
I0127 02:04:18.715715 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:04:26.063029 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:04:34.721145 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:04:36.949845 140169137129280 submission_runner.py:408] Time since start: 1692.42s, 	Step: 4537, 	{'train/accuracy': 0.28413981199264526, 'train/loss': 3.504873514175415, 'validation/accuracy': 0.2563000023365021, 'validation/loss': 3.6541614532470703, 'validation/num_examples': 50000, 'test/accuracy': 0.19180001318454742, 'test/loss': 4.191176891326904, 'test/num_examples': 10000, 'score': 1600.1244082450867, 'total_duration': 1692.4212460517883, 'accumulated_submission_time': 1600.1244082450867, 'accumulated_eval_time': 92.06435537338257, 'accumulated_logging_time': 0.08449053764343262}
I0127 02:04:36.966522 140005389362944 logging_writer.py:48] [4537] accumulated_eval_time=92.064355, accumulated_logging_time=0.084491, accumulated_submission_time=1600.124408, global_step=4537, preemption_count=0, score=1600.124408, test/accuracy=0.191800, test/loss=4.191177, test/num_examples=10000, total_duration=1692.421246, train/accuracy=0.284140, train/loss=3.504874, validation/accuracy=0.256300, validation/loss=3.654161, validation/num_examples=50000
I0127 02:04:58.580847 140005397755648 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.568394660949707, loss=4.323918342590332
I0127 02:05:32.321554 140005389362944 logging_writer.py:48] [4700] global_step=4700, grad_norm=7.697589874267578, loss=4.351049900054932
I0127 02:06:06.082452 140005397755648 logging_writer.py:48] [4800] global_step=4800, grad_norm=7.027551174163818, loss=4.254675388336182
I0127 02:06:39.816986 140005389362944 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.056701183319092, loss=4.305759429931641
I0127 02:07:13.588756 140005397755648 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.247304439544678, loss=4.21111536026001
I0127 02:07:47.499673 140005389362944 logging_writer.py:48] [5100] global_step=5100, grad_norm=14.415817260742188, loss=4.1594719886779785
I0127 02:08:21.241615 140005397755648 logging_writer.py:48] [5200] global_step=5200, grad_norm=7.919807434082031, loss=4.157466888427734
I0127 02:08:54.996689 140005389362944 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.521769046783447, loss=4.227570533752441
I0127 02:09:28.743053 140005397755648 logging_writer.py:48] [5400] global_step=5400, grad_norm=8.319543838500977, loss=4.141225814819336
I0127 02:10:02.508247 140005389362944 logging_writer.py:48] [5500] global_step=5500, grad_norm=9.417938232421875, loss=4.080438613891602
I0127 02:10:36.262794 140005397755648 logging_writer.py:48] [5600] global_step=5600, grad_norm=8.642918586730957, loss=4.115400791168213
I0127 02:11:09.979573 140005389362944 logging_writer.py:48] [5700] global_step=5700, grad_norm=6.314146041870117, loss=4.015321731567383
I0127 02:11:43.722251 140005397755648 logging_writer.py:48] [5800] global_step=5800, grad_norm=5.866004943847656, loss=4.057621002197266
I0127 02:12:17.496330 140005389362944 logging_writer.py:48] [5900] global_step=5900, grad_norm=4.334531307220459, loss=3.9822590351104736
I0127 02:12:51.260580 140005397755648 logging_writer.py:48] [6000] global_step=6000, grad_norm=6.505324363708496, loss=3.910442590713501
I0127 02:13:07.235010 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:13:14.721621 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:13:23.196393 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:13:25.483711 140169137129280 submission_runner.py:408] Time since start: 2220.96s, 	Step: 6049, 	{'train/accuracy': 0.36937978863716125, 'train/loss': 2.9464893341064453, 'validation/accuracy': 0.34289997816085815, 'validation/loss': 3.0969789028167725, 'validation/num_examples': 50000, 'test/accuracy': 0.2526000142097473, 'test/loss': 3.744076728820801, 'test/num_examples': 10000, 'score': 2110.33571267128, 'total_duration': 2220.955129146576, 'accumulated_submission_time': 2110.33571267128, 'accumulated_eval_time': 110.31303834915161, 'accumulated_logging_time': 0.11001849174499512}
I0127 02:13:25.500493 140005833914112 logging_writer.py:48] [6049] accumulated_eval_time=110.313038, accumulated_logging_time=0.110018, accumulated_submission_time=2110.335713, global_step=6049, preemption_count=0, score=2110.335713, test/accuracy=0.252600, test/loss=3.744077, test/num_examples=10000, total_duration=2220.955129, train/accuracy=0.369380, train/loss=2.946489, validation/accuracy=0.342900, validation/loss=3.096979, validation/num_examples=50000
I0127 02:13:43.045955 140005842306816 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.3293538093566895, loss=3.8790206909179688
I0127 02:14:16.798057 140005833914112 logging_writer.py:48] [6200] global_step=6200, grad_norm=10.139266014099121, loss=3.8215441703796387
I0127 02:14:50.541941 140005842306816 logging_writer.py:48] [6300] global_step=6300, grad_norm=5.658329963684082, loss=3.923800468444824
I0127 02:15:24.290839 140005833914112 logging_writer.py:48] [6400] global_step=6400, grad_norm=5.300354957580566, loss=3.9027528762817383
I0127 02:15:58.059151 140005842306816 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.109261512756348, loss=3.944443702697754
I0127 02:16:31.789875 140005833914112 logging_writer.py:48] [6600] global_step=6600, grad_norm=5.530078887939453, loss=3.7791855335235596
I0127 02:17:05.546468 140005842306816 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.290716648101807, loss=3.845628261566162
I0127 02:17:39.298060 140005833914112 logging_writer.py:48] [6800] global_step=6800, grad_norm=6.796193599700928, loss=3.7654876708984375
I0127 02:18:13.055632 140005842306816 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.963301181793213, loss=3.765538454055786
I0127 02:18:46.820340 140005833914112 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.933741569519043, loss=3.7876312732696533
I0127 02:19:20.549741 140005842306816 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.758420467376709, loss=3.77095890045166
I0127 02:19:54.253287 140005833914112 logging_writer.py:48] [7200] global_step=7200, grad_norm=7.029714584350586, loss=3.7816073894500732
I0127 02:20:28.000076 140005842306816 logging_writer.py:48] [7300] global_step=7300, grad_norm=5.856439590454102, loss=3.6987359523773193
I0127 02:21:01.770415 140005833914112 logging_writer.py:48] [7400] global_step=7400, grad_norm=8.228339195251465, loss=3.7312536239624023
I0127 02:21:35.496591 140005842306816 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.583555698394775, loss=3.694295644760132
I0127 02:21:55.504673 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:22:02.998760 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:22:11.483162 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:22:13.808045 140169137129280 submission_runner.py:408] Time since start: 2749.28s, 	Step: 7561, 	{'train/accuracy': 0.44345900416374207, 'train/loss': 2.543766975402832, 'validation/accuracy': 0.4113999903202057, 'validation/loss': 2.7178561687469482, 'validation/num_examples': 50000, 'test/accuracy': 0.3183000087738037, 'test/loss': 3.374671220779419, 'test/num_examples': 10000, 'score': 2620.2774546146393, 'total_duration': 2749.279436826706, 'accumulated_submission_time': 2620.2774546146393, 'accumulated_eval_time': 128.61637711524963, 'accumulated_logging_time': 0.14015746116638184}
I0127 02:22:13.829492 140005380970240 logging_writer.py:48] [7561] accumulated_eval_time=128.616377, accumulated_logging_time=0.140157, accumulated_submission_time=2620.277455, global_step=7561, preemption_count=0, score=2620.277455, test/accuracy=0.318300, test/loss=3.374671, test/num_examples=10000, total_duration=2749.279437, train/accuracy=0.443459, train/loss=2.543767, validation/accuracy=0.411400, validation/loss=2.717856, validation/num_examples=50000
I0127 02:22:27.338771 140005389362944 logging_writer.py:48] [7600] global_step=7600, grad_norm=4.8575239181518555, loss=3.675046920776367
I0127 02:23:01.046505 140005380970240 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.861529350280762, loss=3.6108627319335938
I0127 02:23:34.778776 140005389362944 logging_writer.py:48] [7800] global_step=7800, grad_norm=7.140058517456055, loss=3.5724024772644043
I0127 02:24:08.490289 140005380970240 logging_writer.py:48] [7900] global_step=7900, grad_norm=7.496475696563721, loss=3.6447296142578125
I0127 02:24:42.233835 140005389362944 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.476444244384766, loss=3.570248603820801
I0127 02:25:15.989790 140005380970240 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.517152786254883, loss=3.545178174972534
I0127 02:25:49.725654 140005389362944 logging_writer.py:48] [8200] global_step=8200, grad_norm=4.555913925170898, loss=3.5575294494628906
I0127 02:26:23.636359 140005380970240 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.208028316497803, loss=3.535673141479492
I0127 02:26:57.389593 140005389362944 logging_writer.py:48] [8400] global_step=8400, grad_norm=4.553493976593018, loss=3.56573748588562
I0127 02:27:31.145152 140005380970240 logging_writer.py:48] [8500] global_step=8500, grad_norm=4.123766899108887, loss=3.476672887802124
I0127 02:28:04.878981 140005389362944 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.7190420627593994, loss=3.5087924003601074
I0127 02:28:38.632486 140005380970240 logging_writer.py:48] [8700] global_step=8700, grad_norm=8.396598815917969, loss=3.4645519256591797
I0127 02:29:12.388670 140005389362944 logging_writer.py:48] [8800] global_step=8800, grad_norm=5.2227301597595215, loss=3.482044219970703
I0127 02:29:46.143700 140005380970240 logging_writer.py:48] [8900] global_step=8900, grad_norm=6.425182342529297, loss=3.460986852645874
I0127 02:30:19.874720 140005389362944 logging_writer.py:48] [9000] global_step=9000, grad_norm=7.482987403869629, loss=3.512913227081299
I0127 02:30:43.929945 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:30:51.344533 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:31:00.122933 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:31:02.465611 140169137129280 submission_runner.py:408] Time since start: 3277.94s, 	Step: 9073, 	{'train/accuracy': 0.5113998651504517, 'train/loss': 2.2248189449310303, 'validation/accuracy': 0.4605399966239929, 'validation/loss': 2.471454620361328, 'validation/num_examples': 50000, 'test/accuracy': 0.35850000381469727, 'test/loss': 3.0965747833251953, 'test/num_examples': 10000, 'score': 3130.3148624897003, 'total_duration': 3277.937031984329, 'accumulated_submission_time': 3130.3148624897003, 'accumulated_eval_time': 147.15200424194336, 'accumulated_logging_time': 0.17615604400634766}
I0127 02:31:02.482313 140005842306816 logging_writer.py:48] [9073] accumulated_eval_time=147.152004, accumulated_logging_time=0.176156, accumulated_submission_time=3130.314862, global_step=9073, preemption_count=0, score=3130.314862, test/accuracy=0.358500, test/loss=3.096575, test/num_examples=10000, total_duration=3277.937032, train/accuracy=0.511400, train/loss=2.224819, validation/accuracy=0.460540, validation/loss=2.471455, validation/num_examples=50000
I0127 02:31:11.936137 140005850699520 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.442668914794922, loss=3.330390214920044
I0127 02:31:45.635612 140005842306816 logging_writer.py:48] [9200] global_step=9200, grad_norm=6.70035457611084, loss=3.3748013973236084
I0127 02:32:19.342574 140005850699520 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.0501227378845215, loss=3.465204954147339
I0127 02:32:53.133036 140005842306816 logging_writer.py:48] [9400] global_step=9400, grad_norm=7.131802082061768, loss=3.4227256774902344
I0127 02:33:26.865362 140005850699520 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.388283729553223, loss=3.3223509788513184
I0127 02:34:00.617440 140005842306816 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.927008152008057, loss=3.395998001098633
I0127 02:34:34.357909 140005850699520 logging_writer.py:48] [9700] global_step=9700, grad_norm=7.452096939086914, loss=3.4393973350524902
I0127 02:35:08.089794 140005842306816 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.56898307800293, loss=3.335817337036133
I0127 02:35:41.803603 140005850699520 logging_writer.py:48] [9900] global_step=9900, grad_norm=5.522243022918701, loss=3.35628080368042
I0127 02:36:15.531108 140005842306816 logging_writer.py:48] [10000] global_step=10000, grad_norm=5.59677791595459, loss=3.360685110092163
I0127 02:36:49.258343 140005850699520 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.557775974273682, loss=3.2858896255493164
I0127 02:37:22.941242 140005842306816 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.6334292888641357, loss=3.414249897003174
I0127 02:37:56.683686 140005850699520 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.526923179626465, loss=3.334156036376953
I0127 02:38:30.428534 140005842306816 logging_writer.py:48] [10400] global_step=10400, grad_norm=5.626438617706299, loss=3.3522520065307617
I0127 02:39:04.129764 140005850699520 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.316931247711182, loss=3.2968010902404785
I0127 02:39:32.549030 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:39:40.137264 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:39:49.132385 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:39:51.396684 140169137129280 submission_runner.py:408] Time since start: 3806.87s, 	Step: 10586, 	{'train/accuracy': 0.5440847873687744, 'train/loss': 2.0609612464904785, 'validation/accuracy': 0.49281999468803406, 'validation/loss': 2.328207492828369, 'validation/num_examples': 50000, 'test/accuracy': 0.3758000135421753, 'test/loss': 3.0121326446533203, 'test/num_examples': 10000, 'score': 3640.3234446048737, 'total_duration': 3806.8680925369263, 'accumulated_submission_time': 3640.3234446048737, 'accumulated_eval_time': 165.99961352348328, 'accumulated_logging_time': 0.20357942581176758}
I0127 02:39:51.424166 140005817128704 logging_writer.py:48] [10586] accumulated_eval_time=165.999614, accumulated_logging_time=0.203579, accumulated_submission_time=3640.323445, global_step=10586, preemption_count=0, score=3640.323445, test/accuracy=0.375800, test/loss=3.012133, test/num_examples=10000, total_duration=3806.868093, train/accuracy=0.544085, train/loss=2.060961, validation/accuracy=0.492820, validation/loss=2.328207, validation/num_examples=50000
I0127 02:39:56.481089 140005825521408 logging_writer.py:48] [10600] global_step=10600, grad_norm=7.150444030761719, loss=3.2222681045532227
I0127 02:40:30.141383 140005817128704 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.3143815994262695, loss=3.267199993133545
I0127 02:41:03.832738 140005825521408 logging_writer.py:48] [10800] global_step=10800, grad_norm=6.4480299949646, loss=3.3309545516967773
I0127 02:41:37.497952 140005817128704 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.594354152679443, loss=3.2648813724517822
I0127 02:42:11.202937 140005825521408 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.791562080383301, loss=3.193612813949585
I0127 02:42:44.910320 140005817128704 logging_writer.py:48] [11100] global_step=11100, grad_norm=5.017348766326904, loss=3.25112247467041
I0127 02:43:18.630648 140005825521408 logging_writer.py:48] [11200] global_step=11200, grad_norm=4.166161060333252, loss=3.144587755203247
I0127 02:43:52.336137 140005817128704 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.5438168048858643, loss=3.215996265411377
I0127 02:44:26.031276 140005825521408 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.532037734985352, loss=3.2705821990966797
I0127 02:44:59.765622 140005817128704 logging_writer.py:48] [11500] global_step=11500, grad_norm=8.766890525817871, loss=3.2284417152404785
I0127 02:45:33.442352 140005825521408 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.1683223247528076, loss=3.3576064109802246
I0127 02:46:07.131793 140005817128704 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.3057451248168945, loss=3.197270631790161
I0127 02:46:40.808784 140005825521408 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.436513900756836, loss=3.151087760925293
I0127 02:47:14.519300 140005817128704 logging_writer.py:48] [11900] global_step=11900, grad_norm=5.897553443908691, loss=3.2156295776367188
I0127 02:47:48.212563 140005825521408 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.6742262840271, loss=3.0982253551483154
I0127 02:48:21.913228 140005817128704 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.440184593200684, loss=3.07116436958313
I0127 02:48:21.919449 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:48:29.405825 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:48:40.389796 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:48:42.658666 140169137129280 submission_runner.py:408] Time since start: 4338.13s, 	Step: 12101, 	{'train/accuracy': 0.5639150142669678, 'train/loss': 1.9292712211608887, 'validation/accuracy': 0.521619975566864, 'validation/loss': 2.1376399993896484, 'validation/num_examples': 50000, 'test/accuracy': 0.41050001978874207, 'test/loss': 2.809180498123169, 'test/num_examples': 10000, 'score': 4150.755066394806, 'total_duration': 4338.130095720291, 'accumulated_submission_time': 4150.755066394806, 'accumulated_eval_time': 186.73877835273743, 'accumulated_logging_time': 0.2456672191619873}
I0127 02:48:42.676212 140005322254080 logging_writer.py:48] [12101] accumulated_eval_time=186.738778, accumulated_logging_time=0.245667, accumulated_submission_time=4150.755066, global_step=12101, preemption_count=0, score=4150.755066, test/accuracy=0.410500, test/loss=2.809180, test/num_examples=10000, total_duration=4338.130096, train/accuracy=0.563915, train/loss=1.929271, validation/accuracy=0.521620, validation/loss=2.137640, validation/num_examples=50000
I0127 02:49:16.327083 140005330646784 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.535552978515625, loss=3.0356364250183105
I0127 02:49:49.971409 140005322254080 logging_writer.py:48] [12300] global_step=12300, grad_norm=5.64260721206665, loss=3.0231316089630127
I0127 02:50:23.638592 140005330646784 logging_writer.py:48] [12400] global_step=12400, grad_norm=7.075564384460449, loss=3.1943416595458984
I0127 02:50:57.345189 140005322254080 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.794612169265747, loss=3.142279863357544
I0127 02:51:31.015705 140005330646784 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.629423141479492, loss=3.1680657863616943
I0127 02:52:04.721003 140005322254080 logging_writer.py:48] [12700] global_step=12700, grad_norm=6.51055908203125, loss=3.104130744934082
I0127 02:52:38.399289 140005330646784 logging_writer.py:48] [12800] global_step=12800, grad_norm=4.21491003036499, loss=3.120114326477051
I0127 02:53:12.119612 140005322254080 logging_writer.py:48] [12900] global_step=12900, grad_norm=6.27653694152832, loss=3.149958372116089
I0127 02:53:45.791771 140005330646784 logging_writer.py:48] [13000] global_step=13000, grad_norm=5.120803356170654, loss=3.210106134414673
I0127 02:54:19.459079 140005322254080 logging_writer.py:48] [13100] global_step=13100, grad_norm=4.40364408493042, loss=3.169980764389038
I0127 02:54:53.147970 140005330646784 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.465131759643555, loss=3.016728162765503
I0127 02:55:26.846348 140005322254080 logging_writer.py:48] [13300] global_step=13300, grad_norm=4.840334892272949, loss=3.076244354248047
I0127 02:56:00.534836 140005330646784 logging_writer.py:48] [13400] global_step=13400, grad_norm=4.863167762756348, loss=3.143932580947876
I0127 02:56:34.232922 140005322254080 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.1010661125183105, loss=3.008026599884033
I0127 02:57:07.902767 140005330646784 logging_writer.py:48] [13600] global_step=13600, grad_norm=9.281346321105957, loss=3.06264328956604
I0127 02:57:12.711958 140169137129280 spec.py:321] Evaluating on the training split.
I0127 02:57:21.013602 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 02:57:29.974525 140169137129280 spec.py:349] Evaluating on the test split.
I0127 02:57:32.238809 140169137129280 submission_runner.py:408] Time since start: 4867.71s, 	Step: 13616, 	{'train/accuracy': 0.5889668464660645, 'train/loss': 1.8186978101730347, 'validation/accuracy': 0.5448799729347229, 'validation/loss': 2.03045392036438, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.7065205574035645, 'test/num_examples': 10000, 'score': 4660.733189105988, 'total_duration': 4867.710218667984, 'accumulated_submission_time': 4660.733189105988, 'accumulated_eval_time': 206.2655758857727, 'accumulated_logging_time': 0.27249741554260254}
I0127 02:57:32.271191 140005322254080 logging_writer.py:48] [13616] accumulated_eval_time=206.265576, accumulated_logging_time=0.272497, accumulated_submission_time=4660.733189, global_step=13616, preemption_count=0, score=4660.733189, test/accuracy=0.419300, test/loss=2.706521, test/num_examples=10000, total_duration=4867.710219, train/accuracy=0.588967, train/loss=1.818698, validation/accuracy=0.544880, validation/loss=2.030454, validation/num_examples=50000
I0127 02:58:00.824344 140005330646784 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.675462245941162, loss=3.106860637664795
I0127 02:58:34.469829 140005322254080 logging_writer.py:48] [13800] global_step=13800, grad_norm=4.759748935699463, loss=3.011012315750122
I0127 02:59:08.159741 140005330646784 logging_writer.py:48] [13900] global_step=13900, grad_norm=6.905990123748779, loss=3.136641263961792
I0127 02:59:41.795999 140005322254080 logging_writer.py:48] [14000] global_step=14000, grad_norm=7.304080009460449, loss=3.1802940368652344
I0127 03:00:15.471287 140005330646784 logging_writer.py:48] [14100] global_step=14100, grad_norm=6.185815334320068, loss=3.211732864379883
I0127 03:00:49.131667 140005322254080 logging_writer.py:48] [14200] global_step=14200, grad_norm=7.296192169189453, loss=3.1931800842285156
I0127 03:01:22.811172 140005330646784 logging_writer.py:48] [14300] global_step=14300, grad_norm=6.017561435699463, loss=3.110515832901001
I0127 03:01:56.479468 140005322254080 logging_writer.py:48] [14400] global_step=14400, grad_norm=4.545891761779785, loss=2.9797921180725098
I0127 03:02:30.155673 140005330646784 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.285814046859741, loss=3.01381516456604
I0127 03:03:03.837196 140005322254080 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.293021202087402, loss=3.1265549659729004
I0127 03:03:37.520502 140005330646784 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.064038276672363, loss=3.1914217472076416
I0127 03:04:11.187403 140005322254080 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.026854991912842, loss=2.989421844482422
I0127 03:04:44.857572 140005330646784 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.303098201751709, loss=2.9950265884399414
I0127 03:05:18.503052 140005322254080 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.939918518066406, loss=3.022498369216919
I0127 03:05:52.154041 140005330646784 logging_writer.py:48] [15100] global_step=15100, grad_norm=8.017793655395508, loss=3.1622064113616943
I0127 03:06:02.349921 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:06:10.129684 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:06:21.991482 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:06:24.247408 140169137129280 submission_runner.py:408] Time since start: 5399.72s, 	Step: 15132, 	{'train/accuracy': 0.5876913070678711, 'train/loss': 1.8149843215942383, 'validation/accuracy': 0.5457000136375427, 'validation/loss': 2.0031583309173584, 'validation/num_examples': 50000, 'test/accuracy': 0.4246000349521637, 'test/loss': 2.713749647140503, 'test/num_examples': 10000, 'score': 5170.75431728363, 'total_duration': 5399.7187423706055, 'accumulated_submission_time': 5170.75431728363, 'accumulated_eval_time': 228.16293787956238, 'accumulated_logging_time': 0.3139839172363281}
I0127 03:06:24.270569 140005817128704 logging_writer.py:48] [15132] accumulated_eval_time=228.162938, accumulated_logging_time=0.313984, accumulated_submission_time=5170.754317, global_step=15132, preemption_count=0, score=5170.754317, test/accuracy=0.424600, test/loss=2.713750, test/num_examples=10000, total_duration=5399.718742, train/accuracy=0.587691, train/loss=1.814984, validation/accuracy=0.545700, validation/loss=2.003158, validation/num_examples=50000
I0127 03:06:47.452148 140005833914112 logging_writer.py:48] [15200] global_step=15200, grad_norm=5.229207992553711, loss=3.052354335784912
I0127 03:07:21.081971 140005817128704 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.653693675994873, loss=3.0645477771759033
I0127 03:07:54.745721 140005833914112 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.934172630310059, loss=3.001631259918213
I0127 03:08:28.376102 140005817128704 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.013448238372803, loss=3.0839016437530518
I0127 03:09:02.079535 140005833914112 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.139910697937012, loss=3.1005938053131104
I0127 03:09:35.730910 140005817128704 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.123535633087158, loss=2.941267251968384
I0127 03:10:09.347438 140005833914112 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.751747131347656, loss=3.0372393131256104
I0127 03:10:43.018060 140005817128704 logging_writer.py:48] [15900] global_step=15900, grad_norm=4.210514068603516, loss=3.0393247604370117
I0127 03:11:16.712034 140005833914112 logging_writer.py:48] [16000] global_step=16000, grad_norm=4.429537296295166, loss=3.038681745529175
I0127 03:11:50.376554 140005817128704 logging_writer.py:48] [16100] global_step=16100, grad_norm=4.268867492675781, loss=3.0357422828674316
I0127 03:12:24.070250 140005833914112 logging_writer.py:48] [16200] global_step=16200, grad_norm=7.7006731033325195, loss=3.1786000728607178
I0127 03:12:57.712281 140005817128704 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.896332740783691, loss=3.0563716888427734
I0127 03:13:31.414837 140005833914112 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.82232403755188, loss=3.019712448120117
I0127 03:14:05.028707 140005817128704 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.544411659240723, loss=3.024933338165283
I0127 03:14:38.723104 140005833914112 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.865044593811035, loss=2.936375141143799
I0127 03:14:54.307136 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:15:02.562801 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:15:13.426714 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:15:16.051656 140169137129280 submission_runner.py:408] Time since start: 5931.52s, 	Step: 16648, 	{'train/accuracy': 0.5989118218421936, 'train/loss': 1.7425923347473145, 'validation/accuracy': 0.5619199872016907, 'validation/loss': 1.9272526502609253, 'validation/num_examples': 50000, 'test/accuracy': 0.44030001759529114, 'test/loss': 2.5987136363983154, 'test/num_examples': 10000, 'score': 5680.732532739639, 'total_duration': 5931.523034095764, 'accumulated_submission_time': 5680.732532739639, 'accumulated_eval_time': 249.90737676620483, 'accumulated_logging_time': 0.3474881649017334}
I0127 03:15:16.089183 140005330646784 logging_writer.py:48] [16648] accumulated_eval_time=249.907377, accumulated_logging_time=0.347488, accumulated_submission_time=5680.732533, global_step=16648, preemption_count=0, score=5680.732533, test/accuracy=0.440300, test/loss=2.598714, test/num_examples=10000, total_duration=5931.523034, train/accuracy=0.598912, train/loss=1.742592, validation/accuracy=0.561920, validation/loss=1.927253, validation/num_examples=50000
I0127 03:15:33.915006 140005817128704 logging_writer.py:48] [16700] global_step=16700, grad_norm=4.1770405769348145, loss=3.0415003299713135
I0127 03:16:07.459505 140005330646784 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.203094482421875, loss=3.0214290618896484
I0127 03:16:41.046252 140005817128704 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.596819877624512, loss=2.9474174976348877
I0127 03:17:14.682316 140005330646784 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.3692145347595215, loss=3.0884299278259277
I0127 03:17:48.361642 140005817128704 logging_writer.py:48] [17100] global_step=17100, grad_norm=4.419428825378418, loss=2.969273805618286
I0127 03:18:21.987925 140005330646784 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.422788143157959, loss=3.022703170776367
I0127 03:18:55.744482 140005817128704 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.2975566387176514, loss=2.958197832107544
I0127 03:19:29.349911 140005330646784 logging_writer.py:48] [17400] global_step=17400, grad_norm=4.512965679168701, loss=3.035160541534424
I0127 03:20:03.061121 140005817128704 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.2522149085998535, loss=2.9794015884399414
I0127 03:20:36.687076 140005330646784 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.2010111808776855, loss=3.014404296875
I0127 03:21:10.484834 140005817128704 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.054560661315918, loss=3.0217649936676025
I0127 03:21:44.108507 140005330646784 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.2706751823425293, loss=3.048366069793701
I0127 03:22:17.722551 140005817128704 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.29909610748291, loss=3.0035603046417236
I0127 03:22:51.339622 140005330646784 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.330460071563721, loss=2.973505735397339
I0127 03:23:24.995559 140005817128704 logging_writer.py:48] [18100] global_step=18100, grad_norm=3.5759928226470947, loss=2.9204657077789307
I0127 03:23:46.316683 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:23:54.207611 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:24:08.389786 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:24:10.584371 140169137129280 submission_runner.py:408] Time since start: 6466.06s, 	Step: 18165, 	{'train/accuracy': 0.6433154940605164, 'train/loss': 1.562894344329834, 'validation/accuracy': 0.5627399682998657, 'validation/loss': 1.9308120012283325, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.6102218627929688, 'test/num_examples': 10000, 'score': 6190.90416431427, 'total_duration': 6466.05579328537, 'accumulated_submission_time': 6190.90416431427, 'accumulated_eval_time': 274.1750280857086, 'accumulated_logging_time': 0.39433765411376953}
I0127 03:24:10.601286 140005850699520 logging_writer.py:48] [18165] accumulated_eval_time=274.175028, accumulated_logging_time=0.394338, accumulated_submission_time=6190.904164, global_step=18165, preemption_count=0, score=6190.904164, test/accuracy=0.445600, test/loss=2.610222, test/num_examples=10000, total_duration=6466.055793, train/accuracy=0.643315, train/loss=1.562894, validation/accuracy=0.562740, validation/loss=1.930812, validation/num_examples=50000
I0127 03:24:22.691225 140005859092224 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.697023868560791, loss=3.046835422515869
I0127 03:24:56.298886 140005850699520 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.190150737762451, loss=2.943434000015259
I0127 03:25:29.932946 140005859092224 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.342159271240234, loss=2.862069606781006
I0127 03:26:03.621669 140005850699520 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.443902969360352, loss=3.0099854469299316
I0127 03:26:37.280653 140005859092224 logging_writer.py:48] [18600] global_step=18600, grad_norm=5.792394161224365, loss=3.052976608276367
I0127 03:27:10.935108 140005850699520 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.0746009349823, loss=2.936655044555664
I0127 03:27:44.622932 140005859092224 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.9545645713806152, loss=3.0412638187408447
I0127 03:28:18.272573 140005850699520 logging_writer.py:48] [18900] global_step=18900, grad_norm=5.332441329956055, loss=2.973301887512207
I0127 03:28:51.951852 140005859092224 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.545516490936279, loss=2.8937392234802246
I0127 03:29:25.634294 140005850699520 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.949542760848999, loss=2.9096102714538574
I0127 03:29:59.231236 140005859092224 logging_writer.py:48] [19200] global_step=19200, grad_norm=5.942277908325195, loss=2.9758450984954834
I0127 03:30:32.880744 140005850699520 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.551961898803711, loss=3.0455102920532227
I0127 03:31:06.560182 140005859092224 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.041520118713379, loss=2.919851779937744
I0127 03:31:40.220711 140005850699520 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.736427307128906, loss=2.9545016288757324
I0127 03:32:13.891935 140005859092224 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.4539217948913574, loss=2.9084279537200928
I0127 03:32:40.607000 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:32:49.004253 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:33:03.730437 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:33:05.875651 140169137129280 submission_runner.py:408] Time since start: 7001.35s, 	Step: 19681, 	{'train/accuracy': 0.6237842440605164, 'train/loss': 1.6315232515335083, 'validation/accuracy': 0.5619800090789795, 'validation/loss': 1.9117379188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.569241523742676, 'test/num_examples': 10000, 'score': 6700.854817867279, 'total_duration': 7001.347071886063, 'accumulated_submission_time': 6700.854817867279, 'accumulated_eval_time': 299.4436390399933, 'accumulated_logging_time': 0.42169928550720215}
I0127 03:33:05.915839 140005313861376 logging_writer.py:48] [19681] accumulated_eval_time=299.443639, accumulated_logging_time=0.421699, accumulated_submission_time=6700.854818, global_step=19681, preemption_count=0, score=6700.854818, test/accuracy=0.446700, test/loss=2.569242, test/num_examples=10000, total_duration=7001.347072, train/accuracy=0.623784, train/loss=1.631523, validation/accuracy=0.561980, validation/loss=1.911738, validation/num_examples=50000
I0127 03:33:12.651316 140005322254080 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.485538959503174, loss=3.0156893730163574
I0127 03:33:46.241537 140005313861376 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.41748309135437, loss=2.994065761566162
I0127 03:34:19.890408 140005322254080 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.666079044342041, loss=2.906214952468872
I0127 03:34:53.494302 140005313861376 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.858011484146118, loss=3.0357697010040283
I0127 03:35:27.122998 140005322254080 logging_writer.py:48] [20100] global_step=20100, grad_norm=4.320413589477539, loss=2.989961624145508
I0127 03:36:00.813556 140005313861376 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.1506097316741943, loss=3.126873254776001
I0127 03:36:34.453806 140005322254080 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.0956292152404785, loss=2.908629894256592
I0127 03:37:08.111580 140005313861376 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.4649500846862793, loss=2.948432207107544
I0127 03:37:41.780245 140005322254080 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.4443416595458984, loss=2.9595963954925537
I0127 03:38:15.451356 140005313861376 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.03169846534729, loss=2.8859901428222656
I0127 03:38:49.107395 140005322254080 logging_writer.py:48] [20700] global_step=20700, grad_norm=4.06453275680542, loss=2.93994140625
I0127 03:39:22.782335 140005313861376 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.603591203689575, loss=3.036005973815918
I0127 03:39:56.428077 140005322254080 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.348907232284546, loss=2.9389703273773193
I0127 03:40:30.077661 140005313861376 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.6330060958862305, loss=3.016481637954712
I0127 03:41:03.698470 140005322254080 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.572614908218384, loss=2.983707904815674
I0127 03:41:36.096366 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:41:44.769013 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:41:59.603800 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:42:01.766033 140169137129280 submission_runner.py:408] Time since start: 7537.24s, 	Step: 21198, 	{'train/accuracy': 0.6259167790412903, 'train/loss': 1.6084551811218262, 'validation/accuracy': 0.5781800150871277, 'validation/loss': 1.8428733348846436, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.527494430541992, 'test/num_examples': 10000, 'score': 7210.977495670319, 'total_duration': 7537.237438201904, 'accumulated_submission_time': 7210.977495670319, 'accumulated_eval_time': 325.11325001716614, 'accumulated_logging_time': 0.47275590896606445}
I0127 03:42:01.799440 140005833914112 logging_writer.py:48] [21198] accumulated_eval_time=325.113250, accumulated_logging_time=0.472756, accumulated_submission_time=7210.977496, global_step=21198, preemption_count=0, score=7210.977496, test/accuracy=0.450900, test/loss=2.527494, test/num_examples=10000, total_duration=7537.237438, train/accuracy=0.625917, train/loss=1.608455, validation/accuracy=0.578180, validation/loss=1.842873, validation/num_examples=50000
I0127 03:42:02.824193 140005842306816 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.372119665145874, loss=2.8960700035095215
I0127 03:42:36.382017 140005833914112 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.9601352214813232, loss=2.8917932510375977
I0127 03:43:09.945891 140005842306816 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.3887643814086914, loss=2.8456499576568604
I0127 03:43:43.562509 140005833914112 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.963041067123413, loss=2.9495694637298584
I0127 03:44:17.212181 140005842306816 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.442819356918335, loss=2.9072203636169434
I0127 03:44:50.861045 140005833914112 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.3016157150268555, loss=2.925649404525757
I0127 03:45:24.486570 140005842306816 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.516162157058716, loss=2.8657236099243164
I0127 03:45:58.164845 140005833914112 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.50957989692688, loss=2.832388162612915
I0127 03:46:31.794299 140005842306816 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.3627371788024902, loss=2.83528995513916
I0127 03:47:05.388016 140005833914112 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.0587058067321777, loss=2.8185043334960938
I0127 03:47:39.070544 140005842306816 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.192311763763428, loss=2.922215700149536
I0127 03:48:12.721151 140005833914112 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.000060796737671, loss=2.883467197418213
I0127 03:48:46.353402 140005842306816 logging_writer.py:48] [22400] global_step=22400, grad_norm=5.1023173332214355, loss=2.9050087928771973
I0127 03:49:19.988311 140005833914112 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.500246286392212, loss=2.8976616859436035
I0127 03:49:53.640570 140005842306816 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.8444526195526123, loss=2.868140459060669
I0127 03:50:27.310003 140005833914112 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.9320929050445557, loss=2.9307851791381836
I0127 03:50:31.798479 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:50:39.675092 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:50:52.753998 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:50:55.128202 140169137129280 submission_runner.py:408] Time since start: 8070.60s, 	Step: 22715, 	{'train/accuracy': 0.6239436864852905, 'train/loss': 1.6318671703338623, 'validation/accuracy': 0.5826199650764465, 'validation/loss': 1.8319292068481445, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.526982307434082, 'test/num_examples': 10000, 'score': 7720.921734571457, 'total_duration': 8070.599623918533, 'accumulated_submission_time': 7720.921734571457, 'accumulated_eval_time': 348.44293189048767, 'accumulated_logging_time': 0.514392614364624}
I0127 03:50:55.146836 140005817128704 logging_writer.py:48] [22715] accumulated_eval_time=348.442932, accumulated_logging_time=0.514393, accumulated_submission_time=7720.921735, global_step=22715, preemption_count=0, score=7720.921735, test/accuracy=0.455400, test/loss=2.526982, test/num_examples=10000, total_duration=8070.599624, train/accuracy=0.623944, train/loss=1.631867, validation/accuracy=0.582620, validation/loss=1.831929, validation/num_examples=50000
I0127 03:51:24.040974 140005825521408 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.0135719776153564, loss=2.8146491050720215
I0127 03:51:57.674482 140005817128704 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.0310046672821045, loss=2.829789400100708
I0127 03:52:31.331335 140005825521408 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.958432197570801, loss=2.918452262878418
I0127 03:53:04.969445 140005817128704 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.9807019233703613, loss=3.0659377574920654
I0127 03:53:38.644855 140005825521408 logging_writer.py:48] [23200] global_step=23200, grad_norm=4.157486915588379, loss=2.992852210998535
I0127 03:54:12.412370 140005817128704 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.158062696456909, loss=2.8785452842712402
I0127 03:54:46.056782 140005825521408 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.033043622970581, loss=2.882497787475586
I0127 03:55:19.698693 140005817128704 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.8794310092926025, loss=2.8886404037475586
I0127 03:55:53.302511 140005825521408 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.6800315380096436, loss=3.0118987560272217
I0127 03:56:26.874436 140005817128704 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.017265558242798, loss=2.8903279304504395
I0127 03:57:00.481218 140005825521408 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.956712245941162, loss=2.871508836746216
I0127 03:57:34.134325 140005817128704 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.8848462104797363, loss=2.8969247341156006
I0127 03:58:07.770789 140005825521408 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.9254441261291504, loss=2.837663412094116
I0127 03:58:41.571009 140005817128704 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.5572640895843506, loss=2.9278430938720703
I0127 03:59:15.212733 140005825521408 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.812467336654663, loss=2.9842147827148438
I0127 03:59:25.428246 140169137129280 spec.py:321] Evaluating on the training split.
I0127 03:59:33.364481 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 03:59:47.752480 140169137129280 spec.py:349] Evaluating on the test split.
I0127 03:59:49.946910 140169137129280 submission_runner.py:408] Time since start: 8605.42s, 	Step: 24232, 	{'train/accuracy': 0.6275310516357422, 'train/loss': 1.646264910697937, 'validation/accuracy': 0.5839399695396423, 'validation/loss': 1.8555593490600586, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.4928462505340576, 'test/num_examples': 10000, 'score': 8231.1480448246, 'total_duration': 8605.418276548386, 'accumulated_submission_time': 8231.1480448246, 'accumulated_eval_time': 372.9615008831024, 'accumulated_logging_time': 0.5425989627838135}
I0127 03:59:49.964617 140005842306816 logging_writer.py:48] [24232] accumulated_eval_time=372.961501, accumulated_logging_time=0.542599, accumulated_submission_time=8231.148045, global_step=24232, preemption_count=0, score=8231.148045, test/accuracy=0.467500, test/loss=2.492846, test/num_examples=10000, total_duration=8605.418277, train/accuracy=0.627531, train/loss=1.646265, validation/accuracy=0.583940, validation/loss=1.855559, validation/num_examples=50000
I0127 04:00:13.129984 140005850699520 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.742593765258789, loss=2.9486498832702637
I0127 04:00:46.750525 140005842306816 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.2823362350463867, loss=2.8997979164123535
I0127 04:01:20.396496 140005850699520 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.580402135848999, loss=2.9181113243103027
I0127 04:01:54.045116 140005842306816 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.7794559001922607, loss=2.8163163661956787
I0127 04:02:27.702552 140005850699520 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.673689365386963, loss=2.9145946502685547
I0127 04:03:01.334175 140005842306816 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.6983654499053955, loss=2.8827695846557617
I0127 04:03:35.006219 140005850699520 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.5426156520843506, loss=2.8416519165039062
I0127 04:04:08.671336 140005842306816 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.724316358566284, loss=2.940951347351074
I0127 04:04:42.314757 140005850699520 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.866905927658081, loss=2.8005378246307373
I0127 04:05:15.976654 140005842306816 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.0256500244140625, loss=2.862086772918701
I0127 04:05:49.633376 140005850699520 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.7103967666625977, loss=2.82798433303833
I0127 04:06:23.287598 140005842306816 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.741020917892456, loss=2.8927738666534424
I0127 04:06:56.937552 140005850699520 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.001502513885498, loss=2.815051794052124
I0127 04:07:30.587023 140005842306816 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.8149147033691406, loss=2.8147635459899902
I0127 04:08:04.249233 140005850699520 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.619474172592163, loss=2.782651901245117
I0127 04:08:20.199754 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:08:28.313492 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:08:40.762316 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:08:43.125737 140169137129280 submission_runner.py:408] Time since start: 9138.60s, 	Step: 25749, 	{'train/accuracy': 0.6228276491165161, 'train/loss': 1.6504992246627808, 'validation/accuracy': 0.5827800035476685, 'validation/loss': 1.8330410718917847, 'validation/num_examples': 50000, 'test/accuracy': 0.46470001339912415, 'test/loss': 2.5192012786865234, 'test/num_examples': 10000, 'score': 8741.327996253967, 'total_duration': 9138.597157239914, 'accumulated_submission_time': 8741.327996253967, 'accumulated_eval_time': 395.8874454498291, 'accumulated_logging_time': 0.5697648525238037}
I0127 04:08:43.145490 140005817128704 logging_writer.py:48] [25749] accumulated_eval_time=395.887445, accumulated_logging_time=0.569765, accumulated_submission_time=8741.327996, global_step=25749, preemption_count=0, score=8741.327996, test/accuracy=0.464700, test/loss=2.519201, test/num_examples=10000, total_duration=9138.597157, train/accuracy=0.622828, train/loss=1.650499, validation/accuracy=0.582780, validation/loss=1.833041, validation/num_examples=50000
I0127 04:09:00.588019 140005825521408 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.593435287475586, loss=2.936310052871704
I0127 04:09:34.106706 140005817128704 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.8797714710235596, loss=2.9151389598846436
I0127 04:10:07.692980 140005825521408 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.962639093399048, loss=2.86136531829834
I0127 04:10:41.281144 140005817128704 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.070852041244507, loss=2.8985435962677
I0127 04:11:14.918919 140005825521408 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.5745058059692383, loss=2.8089613914489746
I0127 04:11:48.579824 140005817128704 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.2028846740722656, loss=2.9198057651519775
I0127 04:12:22.220277 140005825521408 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.487673282623291, loss=2.89424467086792
I0127 04:12:55.878524 140005817128704 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.668323278427124, loss=2.89635968208313
I0127 04:13:29.539932 140005825521408 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.2021586894989014, loss=2.966214179992676
I0127 04:14:03.197806 140005817128704 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.623448610305786, loss=2.9182958602905273
I0127 04:14:36.829619 140005825521408 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.523531198501587, loss=2.879760265350342
I0127 04:15:10.473108 140005817128704 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.1068975925445557, loss=2.8350207805633545
I0127 04:15:44.084678 140005825521408 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.8249013423919678, loss=2.699359178543091
I0127 04:16:17.723067 140005817128704 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.9842891693115234, loss=3.05181884765625
I0127 04:16:51.406776 140005825521408 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.9913132190704346, loss=2.754851818084717
I0127 04:17:13.297467 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:17:21.494345 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:17:33.667047 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:17:35.959122 140169137129280 submission_runner.py:408] Time since start: 9671.43s, 	Step: 27266, 	{'train/accuracy': 0.6548548936843872, 'train/loss': 1.5148216485977173, 'validation/accuracy': 0.581820011138916, 'validation/loss': 1.8570882081985474, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.5271754264831543, 'test/num_examples': 10000, 'score': 9251.424172401428, 'total_duration': 9671.430490970612, 'accumulated_submission_time': 9251.424172401428, 'accumulated_eval_time': 418.5490050315857, 'accumulated_logging_time': 0.6001632213592529}
I0127 04:17:35.983013 140005313861376 logging_writer.py:48] [27266] accumulated_eval_time=418.549005, accumulated_logging_time=0.600163, accumulated_submission_time=9251.424172, global_step=27266, preemption_count=0, score=9251.424172, test/accuracy=0.460100, test/loss=2.527175, test/num_examples=10000, total_duration=9671.430491, train/accuracy=0.654855, train/loss=1.514822, validation/accuracy=0.581820, validation/loss=1.857088, validation/num_examples=50000
I0127 04:17:47.743318 140005322254080 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.8112285137176514, loss=2.805828094482422
I0127 04:18:21.328088 140005313861376 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.715177297592163, loss=2.843512773513794
I0127 04:18:54.970500 140005322254080 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.6946444511413574, loss=2.8750579357147217
I0127 04:19:28.596677 140005313861376 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.8457655906677246, loss=2.8611552715301514
I0127 04:20:02.222934 140005322254080 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.766184091567993, loss=2.8312172889709473
I0127 04:20:35.821547 140005313861376 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.1202778816223145, loss=2.8850889205932617
I0127 04:21:09.485641 140005322254080 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.8766562938690186, loss=2.8225295543670654
I0127 04:21:43.143676 140005313861376 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.1223270893096924, loss=2.845010995864868
I0127 04:22:16.743813 140005322254080 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.7005345821380615, loss=2.9471731185913086
I0127 04:22:50.375408 140005313861376 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.3821234703063965, loss=2.8102059364318848
I0127 04:23:23.998138 140005322254080 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.2734010219573975, loss=2.815633535385132
I0127 04:23:57.603504 140005313861376 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.665083169937134, loss=2.927550792694092
I0127 04:24:31.282804 140005322254080 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2535974979400635, loss=2.7786412239074707
I0127 04:25:04.889484 140005313861376 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.946807861328125, loss=2.7662792205810547
I0127 04:25:38.508328 140005322254080 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.9061832427978516, loss=2.865633487701416
I0127 04:26:06.189383 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:26:14.283861 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:26:26.023540 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:26:28.271974 140169137129280 submission_runner.py:408] Time since start: 10203.74s, 	Step: 28784, 	{'train/accuracy': 0.6424983739852905, 'train/loss': 1.578094244003296, 'validation/accuracy': 0.5882999897003174, 'validation/loss': 1.832597017288208, 'validation/num_examples': 50000, 'test/accuracy': 0.45910000801086426, 'test/loss': 2.5145013332366943, 'test/num_examples': 10000, 'score': 9761.575938463211, 'total_duration': 10203.743408441544, 'accumulated_submission_time': 9761.575938463211, 'accumulated_eval_time': 440.63156938552856, 'accumulated_logging_time': 0.6332635879516602}
I0127 04:26:28.294132 140005842306816 logging_writer.py:48] [28784] accumulated_eval_time=440.631569, accumulated_logging_time=0.633264, accumulated_submission_time=9761.575938, global_step=28784, preemption_count=0, score=9761.575938, test/accuracy=0.459100, test/loss=2.514501, test/num_examples=10000, total_duration=10203.743408, train/accuracy=0.642498, train/loss=1.578094, validation/accuracy=0.588300, validation/loss=1.832597, validation/num_examples=50000
I0127 04:26:33.997473 140005850699520 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.288560152053833, loss=2.8455309867858887
I0127 04:27:07.522553 140005842306816 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.5543019771575928, loss=2.896260976791382
I0127 04:27:41.054073 140005850699520 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.229444980621338, loss=2.801407814025879
I0127 04:28:14.639974 140005842306816 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.0415172576904297, loss=2.8052401542663574
I0127 04:28:48.256650 140005850699520 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.1106410026550293, loss=2.8352150917053223
I0127 04:29:21.903412 140005842306816 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.7168867588043213, loss=2.738311767578125
I0127 04:29:55.530729 140005850699520 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.005796432495117, loss=2.824453115463257
I0127 04:30:29.175458 140005842306816 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.1590380668640137, loss=2.8759334087371826
I0127 04:31:02.808114 140005850699520 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.4764351844787598, loss=2.7946197986602783
I0127 04:31:36.448196 140005842306816 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.7221922874450684, loss=2.8445167541503906
I0127 04:32:10.088922 140005850699520 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.279118537902832, loss=2.8273940086364746
I0127 04:32:43.685976 140005842306816 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.065312623977661, loss=2.8674585819244385
I0127 04:33:17.243654 140005850699520 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.2921524047851562, loss=2.879255771636963
I0127 04:33:50.822740 140005842306816 logging_writer.py:48] [30100] global_step=30100, grad_norm=4.04529333114624, loss=2.7890279293060303
I0127 04:34:24.457925 140005850699520 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.6357626914978027, loss=2.7766942977905273
I0127 04:34:58.069518 140005842306816 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.2058069705963135, loss=2.9355266094207764
I0127 04:34:58.501510 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:35:06.689904 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:35:20.097352 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:35:22.595397 140169137129280 submission_runner.py:408] Time since start: 10738.07s, 	Step: 30303, 	{'train/accuracy': 0.6371771097183228, 'train/loss': 1.5771594047546387, 'validation/accuracy': 0.5897799730300903, 'validation/loss': 1.7975454330444336, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4995713233947754, 'test/num_examples': 10000, 'score': 10271.727610588074, 'total_duration': 10738.066817998886, 'accumulated_submission_time': 10271.727610588074, 'accumulated_eval_time': 464.72541093826294, 'accumulated_logging_time': 0.6648633480072021}
I0127 04:35:22.615847 140005322254080 logging_writer.py:48] [30303] accumulated_eval_time=464.725411, accumulated_logging_time=0.664863, accumulated_submission_time=10271.727611, global_step=30303, preemption_count=0, score=10271.727611, test/accuracy=0.458900, test/loss=2.499571, test/num_examples=10000, total_duration=10738.066818, train/accuracy=0.637177, train/loss=1.577159, validation/accuracy=0.589780, validation/loss=1.797545, validation/num_examples=50000
I0127 04:35:55.481327 140005330646784 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.9733030796051025, loss=2.813098907470703
I0127 04:36:29.066875 140005322254080 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.029966115951538, loss=2.8545420169830322
I0127 04:37:02.705443 140005330646784 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.6326444149017334, loss=2.7731850147247314
I0127 04:37:36.350958 140005322254080 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.852663040161133, loss=2.915109634399414
I0127 04:38:09.993615 140005330646784 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.8181607723236084, loss=2.8423514366149902
I0127 04:38:43.611905 140005322254080 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.048774242401123, loss=2.8747599124908447
I0127 04:39:17.236365 140005330646784 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.9006764888763428, loss=2.804170608520508
I0127 04:39:50.878613 140005322254080 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.9844858646392822, loss=2.8934690952301025
I0127 04:40:24.502700 140005330646784 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.0387864112854004, loss=2.785216808319092
I0127 04:40:58.147319 140005322254080 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.701568841934204, loss=2.748220205307007
I0127 04:41:31.759690 140005330646784 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.5056393146514893, loss=2.811763286590576
I0127 04:42:05.397278 140005322254080 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.6037821769714355, loss=2.7581400871276855
I0127 04:42:39.015720 140005330646784 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.245548963546753, loss=2.772315740585327
I0127 04:43:12.674796 140005322254080 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.912830352783203, loss=2.822718381881714
I0127 04:43:46.267662 140005330646784 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.6039342880249023, loss=2.9582135677337646
I0127 04:43:52.767801 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:44:01.009840 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:44:14.551855 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:44:16.821162 140169137129280 submission_runner.py:408] Time since start: 11272.29s, 	Step: 31821, 	{'train/accuracy': 0.6344068646430969, 'train/loss': 1.5880506038665771, 'validation/accuracy': 0.5906800031661987, 'validation/loss': 1.8002487421035767, 'validation/num_examples': 50000, 'test/accuracy': 0.4662000238895416, 'test/loss': 2.477463960647583, 'test/num_examples': 10000, 'score': 10781.824187994003, 'total_duration': 11272.292582035065, 'accumulated_submission_time': 10781.824187994003, 'accumulated_eval_time': 488.77872920036316, 'accumulated_logging_time': 0.6953849792480469}
I0127 04:44:16.840166 140004667934464 logging_writer.py:48] [31821] accumulated_eval_time=488.778729, accumulated_logging_time=0.695385, accumulated_submission_time=10781.824188, global_step=31821, preemption_count=0, score=10781.824188, test/accuracy=0.466200, test/loss=2.477464, test/num_examples=10000, total_duration=11272.292582, train/accuracy=0.634407, train/loss=1.588051, validation/accuracy=0.590680, validation/loss=1.800249, validation/num_examples=50000
I0127 04:44:43.681000 140004676327168 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.6142687797546387, loss=2.893354892730713
I0127 04:45:17.249849 140004667934464 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.731928825378418, loss=2.9122657775878906
I0127 04:45:50.871603 140004676327168 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.9643120765686035, loss=2.770777940750122
I0127 04:46:24.511661 140004667934464 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.0364208221435547, loss=2.7947068214416504
I0127 04:46:58.142510 140004676327168 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.985175132751465, loss=2.727952241897583
I0127 04:47:31.792923 140004667934464 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.883126735687256, loss=2.772672653198242
I0127 04:48:05.409068 140004676327168 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.0549745559692383, loss=2.8607187271118164
I0127 04:48:39.029263 140004667934464 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.948498249053955, loss=2.8358535766601562
I0127 04:49:12.669162 140004676327168 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.3274526596069336, loss=2.779820442199707
I0127 04:49:46.334491 140004667934464 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.1667609214782715, loss=2.861572265625
I0127 04:50:19.975075 140004676327168 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.6429810523986816, loss=2.7488036155700684
I0127 04:50:53.579937 140004667934464 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.320521593093872, loss=2.8119661808013916
I0127 04:51:27.205521 140004676327168 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.7077224254608154, loss=2.8137948513031006
I0127 04:52:00.873793 140004667934464 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.7773709297180176, loss=2.837214469909668
I0127 04:52:34.501610 140004676327168 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.29756236076355, loss=2.871624231338501
I0127 04:52:47.067560 140169137129280 spec.py:321] Evaluating on the training split.
I0127 04:52:55.294587 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 04:53:08.726918 140169137129280 spec.py:349] Evaluating on the test split.
I0127 04:53:10.971125 140169137129280 submission_runner.py:408] Time since start: 11806.44s, 	Step: 33339, 	{'train/accuracy': 0.6326330900192261, 'train/loss': 1.577653169631958, 'validation/accuracy': 0.5985000133514404, 'validation/loss': 1.7677619457244873, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.437567949295044, 'test/num_examples': 10000, 'score': 11291.99487566948, 'total_duration': 11806.442516088486, 'accumulated_submission_time': 11291.99487566948, 'accumulated_eval_time': 512.6822199821472, 'accumulated_logging_time': 0.7257883548736572}
I0127 04:53:10.992016 140005313861376 logging_writer.py:48] [33339] accumulated_eval_time=512.682220, accumulated_logging_time=0.725788, accumulated_submission_time=11291.994876, global_step=33339, preemption_count=0, score=11291.994876, test/accuracy=0.471400, test/loss=2.437568, test/num_examples=10000, total_duration=11806.442516, train/accuracy=0.632633, train/loss=1.577653, validation/accuracy=0.598500, validation/loss=1.767762, validation/num_examples=50000
I0127 04:53:31.761615 140005322254080 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.6637394428253174, loss=2.7417800426483154
I0127 04:54:05.332028 140005313861376 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.7066991329193115, loss=2.76567006111145
I0127 04:54:38.884579 140005322254080 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.6194145679473877, loss=2.7865939140319824
I0127 04:55:12.474758 140005313861376 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.842777967453003, loss=2.899839401245117
I0127 04:55:46.124437 140005322254080 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.356051206588745, loss=2.7843406200408936
I0127 04:56:19.738986 140005313861376 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.035998821258545, loss=2.8657243251800537
I0127 04:56:53.379020 140005322254080 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.823162078857422, loss=2.874696969985962
I0127 04:57:26.971502 140005313861376 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.1852023601531982, loss=2.8701791763305664
I0127 04:58:00.555167 140005322254080 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.0422005653381348, loss=2.858323097229004
I0127 04:58:34.206325 140005313861376 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.9364497661590576, loss=2.8956756591796875
I0127 04:59:07.845245 140005322254080 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.943650484085083, loss=2.891446352005005
I0127 04:59:41.467659 140005313861376 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.9238126277923584, loss=2.8662943840026855
I0127 05:00:15.124228 140005322254080 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.066770315170288, loss=2.8183999061584473
I0127 05:00:48.738944 140005313861376 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.9246208667755127, loss=2.786789655685425
I0127 05:01:22.370685 140005322254080 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.3937971591949463, loss=2.8277742862701416
I0127 05:01:41.316627 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:01:49.634817 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:02:03.312697 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:02:05.626127 140169137129280 submission_runner.py:408] Time since start: 12341.10s, 	Step: 34858, 	{'train/accuracy': 0.6404655575752258, 'train/loss': 1.569533348083496, 'validation/accuracy': 0.5995999574661255, 'validation/loss': 1.7648115158081055, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.4060091972351074, 'test/num_examples': 10000, 'score': 11802.263848781586, 'total_duration': 12341.097544431686, 'accumulated_submission_time': 11802.263848781586, 'accumulated_eval_time': 536.9916772842407, 'accumulated_logging_time': 0.756098747253418}
I0127 05:02:05.646043 140004659541760 logging_writer.py:48] [34858] accumulated_eval_time=536.991677, accumulated_logging_time=0.756099, accumulated_submission_time=11802.263849, global_step=34858, preemption_count=0, score=11802.263849, test/accuracy=0.478400, test/loss=2.406009, test/num_examples=10000, total_duration=12341.097544, train/accuracy=0.640466, train/loss=1.569533, validation/accuracy=0.599600, validation/loss=1.764812, validation/num_examples=50000
I0127 05:02:20.083049 140004667934464 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.0742428302764893, loss=2.8651602268218994
I0127 05:02:53.612607 140004659541760 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.7804925441741943, loss=2.873223066329956
I0127 05:03:27.150958 140004667934464 logging_writer.py:48] [35100] global_step=35100, grad_norm=4.031745910644531, loss=2.840360164642334
I0127 05:04:00.746173 140004659541760 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.630831718444824, loss=2.775534152984619
I0127 05:04:34.375008 140004667934464 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.097900152206421, loss=2.778918743133545
I0127 05:05:08.007288 140004659541760 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.846733570098877, loss=2.848482370376587
I0127 05:05:41.644679 140004667934464 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.603623867034912, loss=2.76939058303833
I0127 05:06:15.323448 140004659541760 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.9816784858703613, loss=2.779430389404297
I0127 05:06:48.967751 140004667934464 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.384798526763916, loss=2.7882819175720215
I0127 05:07:22.566994 140004659541760 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.0653138160705566, loss=2.8155181407928467
I0127 05:07:56.180297 140004667934464 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.1314713954925537, loss=2.7918496131896973
I0127 05:08:29.803690 140004659541760 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.028353214263916, loss=2.8330748081207275
I0127 05:09:03.434122 140004667934464 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.213848114013672, loss=2.7741918563842773
I0127 05:09:37.078674 140004659541760 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.87684965133667, loss=2.770535707473755
I0127 05:10:10.730751 140004667934464 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.5106494426727295, loss=2.8176207542419434
I0127 05:10:35.737024 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:10:44.011024 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:10:58.336674 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:11:00.463663 140169137129280 submission_runner.py:408] Time since start: 12875.93s, 	Step: 36376, 	{'train/accuracy': 0.670340359210968, 'train/loss': 1.3972032070159912, 'validation/accuracy': 0.6022799611091614, 'validation/loss': 1.7204241752624512, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.39719295501709, 'test/num_examples': 10000, 'score': 12312.29682803154, 'total_duration': 12875.93498826027, 'accumulated_submission_time': 12312.29682803154, 'accumulated_eval_time': 561.7181787490845, 'accumulated_logging_time': 0.7879447937011719}
I0127 05:11:00.484810 140005313861376 logging_writer.py:48] [36376] accumulated_eval_time=561.718179, accumulated_logging_time=0.787945, accumulated_submission_time=12312.296828, global_step=36376, preemption_count=0, score=12312.296828, test/accuracy=0.479100, test/loss=2.397193, test/num_examples=10000, total_duration=12875.934988, train/accuracy=0.670340, train/loss=1.397203, validation/accuracy=0.602280, validation/loss=1.720424, validation/num_examples=50000
I0127 05:11:08.871105 140005322254080 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.535637617111206, loss=2.8369226455688477
I0127 05:11:42.438400 140005313861376 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.7145936489105225, loss=2.905299425125122
I0127 05:12:15.958307 140005322254080 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.589083433151245, loss=2.660098075866699
I0127 05:12:49.589935 140005313861376 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.8842504024505615, loss=2.8041534423828125
I0127 05:13:23.226978 140005322254080 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.6819589138031006, loss=2.7119085788726807
I0127 05:13:56.865620 140005313861376 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.453911304473877, loss=2.8024027347564697
I0127 05:14:30.512511 140005322254080 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.0448498725891113, loss=2.8154258728027344
I0127 05:15:04.141335 140005313861376 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.63433837890625, loss=2.743784189224243
I0127 05:15:37.770975 140005322254080 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.2680907249450684, loss=2.8178110122680664
I0127 05:16:11.387953 140005313861376 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.885000228881836, loss=2.8054447174072266
I0127 05:16:45.002326 140005322254080 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.1058714389801025, loss=2.748652935028076
I0127 05:17:18.642048 140005313861376 logging_writer.py:48] [37500] global_step=37500, grad_norm=2.9694178104400635, loss=2.8210055828094482
I0127 05:17:52.291417 140005322254080 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.993579149246216, loss=2.765258312225342
I0127 05:18:25.958971 140005313861376 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.139784336090088, loss=2.835845947265625
I0127 05:18:59.545952 140005322254080 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.3675425052642822, loss=2.749723434448242
I0127 05:19:30.579500 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:19:37.745037 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:19:51.404305 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:19:53.576021 140169137129280 submission_runner.py:408] Time since start: 13409.05s, 	Step: 37894, 	{'train/accuracy': 0.6550741195678711, 'train/loss': 1.4708644151687622, 'validation/accuracy': 0.6022599935531616, 'validation/loss': 1.7340105772018433, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.4191813468933105, 'test/num_examples': 10000, 'score': 12822.334090471268, 'total_duration': 13409.047441959381, 'accumulated_submission_time': 12822.334090471268, 'accumulated_eval_time': 584.714658498764, 'accumulated_logging_time': 0.8208250999450684}
I0127 05:19:53.596616 140004667934464 logging_writer.py:48] [37894] accumulated_eval_time=584.714658, accumulated_logging_time=0.820825, accumulated_submission_time=12822.334090, global_step=37894, preemption_count=0, score=12822.334090, test/accuracy=0.477900, test/loss=2.419181, test/num_examples=10000, total_duration=13409.047442, train/accuracy=0.655074, train/loss=1.470864, validation/accuracy=0.602260, validation/loss=1.734011, validation/num_examples=50000
I0127 05:19:55.951787 140004676327168 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.815680980682373, loss=2.8477907180786133
I0127 05:20:29.474814 140004667934464 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.891841173171997, loss=2.7924437522888184
I0127 05:21:03.006930 140004676327168 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.943769693374634, loss=2.7000560760498047
I0127 05:21:36.570705 140004667934464 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.0125060081481934, loss=2.8101749420166016
I0127 05:22:10.199262 140004676327168 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.170180559158325, loss=2.8699026107788086
I0127 05:22:43.828032 140004667934464 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.915053367614746, loss=2.7501893043518066
I0127 05:23:17.475275 140004676327168 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.337637424468994, loss=2.766148805618286
I0127 05:23:51.083293 140004667934464 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.7934460639953613, loss=2.7805795669555664
I0127 05:24:24.698549 140004676327168 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.079796552658081, loss=2.7341785430908203
I0127 05:24:58.316605 140004667934464 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.051851272583008, loss=2.7212836742401123
I0127 05:25:31.930220 140004676327168 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.929548740386963, loss=2.81461238861084
I0127 05:26:05.537274 140004667934464 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.928187847137451, loss=2.654714345932007
I0127 05:26:39.202795 140004676327168 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.2488036155700684, loss=2.857095241546631
I0127 05:27:12.827288 140004667934464 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.1393942832946777, loss=2.811849355697632
I0127 05:27:46.473443 140004676327168 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.5872015953063965, loss=2.7617571353912354
I0127 05:28:20.098470 140004667934464 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.321255922317505, loss=2.726844310760498
I0127 05:28:23.580029 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:28:30.781231 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:28:44.277674 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:28:46.447763 140169137129280 submission_runner.py:408] Time since start: 13941.92s, 	Step: 39412, 	{'train/accuracy': 0.6563097834587097, 'train/loss': 1.4915953874588013, 'validation/accuracy': 0.6089400053024292, 'validation/loss': 1.7161346673965454, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.380843162536621, 'test/num_examples': 10000, 'score': 13332.26151394844, 'total_duration': 13941.919185638428, 'accumulated_submission_time': 13332.26151394844, 'accumulated_eval_time': 607.5823495388031, 'accumulated_logging_time': 0.8514549732208252}
I0127 05:28:46.467681 140005313861376 logging_writer.py:48] [39412] accumulated_eval_time=607.582350, accumulated_logging_time=0.851455, accumulated_submission_time=13332.261514, global_step=39412, preemption_count=0, score=13332.261514, test/accuracy=0.488600, test/loss=2.380843, test/num_examples=10000, total_duration=13941.919186, train/accuracy=0.656310, train/loss=1.491595, validation/accuracy=0.608940, validation/loss=1.716135, validation/num_examples=50000
I0127 05:29:16.333729 140005322254080 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.5166568756103516, loss=2.784993886947632
I0127 05:29:49.880773 140005313861376 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.847921848297119, loss=2.781827211380005
I0127 05:30:23.412880 140005322254080 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.1261048316955566, loss=2.797940969467163
I0127 05:30:56.998372 140005313861376 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.040940761566162, loss=2.8739917278289795
I0127 05:31:30.591598 140005322254080 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.839185953140259, loss=2.7745676040649414
I0127 05:32:04.217413 140005313861376 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.4124045372009277, loss=2.852505683898926
I0127 05:32:37.869001 140005322254080 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.1805834770202637, loss=2.6950628757476807
I0127 05:33:11.481076 140005313861376 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.7189996242523193, loss=2.748396873474121
I0127 05:33:45.122555 140005322254080 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.6872100830078125, loss=2.762082576751709
I0127 05:34:18.780217 140005313861376 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.4188952445983887, loss=2.870135545730591
I0127 05:34:52.386602 140005322254080 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.8002097606658936, loss=2.6770312786102295
I0127 05:35:26.024340 140005313861376 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.0950958728790283, loss=2.810631275177002
I0127 05:35:59.664718 140005322254080 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.6289937496185303, loss=2.912788152694702
I0127 05:36:33.274890 140005313861376 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.2786130905151367, loss=2.7820186614990234
I0127 05:37:06.911433 140005322254080 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.1459543704986572, loss=2.8084893226623535
I0127 05:37:16.450892 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:37:23.424060 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:37:36.142292 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:37:38.483867 140169137129280 submission_runner.py:408] Time since start: 14473.96s, 	Step: 40930, 	{'train/accuracy': 0.6541174650192261, 'train/loss': 1.5210903882980347, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.733443260192871, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3924968242645264, 'test/num_examples': 10000, 'score': 13842.186959505081, 'total_duration': 14473.955300807953, 'accumulated_submission_time': 13842.186959505081, 'accumulated_eval_time': 629.6153049468994, 'accumulated_logging_time': 0.883293628692627}
I0127 05:37:38.507251 140004676327168 logging_writer.py:48] [40930] accumulated_eval_time=629.615305, accumulated_logging_time=0.883294, accumulated_submission_time=13842.186960, global_step=40930, preemption_count=0, score=13842.186960, test/accuracy=0.481700, test/loss=2.392497, test/num_examples=10000, total_duration=14473.955301, train/accuracy=0.654117, train/loss=1.521090, validation/accuracy=0.605020, validation/loss=1.733443, validation/num_examples=50000
I0127 05:38:02.332572 140005288683264 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.834089517593384, loss=2.8169026374816895
I0127 05:38:35.855961 140004676327168 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.212820529937744, loss=2.741750955581665
I0127 05:39:09.429937 140005288683264 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.550379753112793, loss=2.8443803787231445
I0127 05:39:43.043138 140004676327168 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.7584540843963623, loss=2.810025453567505
I0127 05:40:16.653408 140005288683264 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.5597152709960938, loss=2.718430519104004
I0127 05:40:50.277313 140004676327168 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.32566237449646, loss=2.854391098022461
I0127 05:41:23.949439 140005288683264 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.633237600326538, loss=2.7926762104034424
I0127 05:41:57.569208 140004676327168 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.876420021057129, loss=2.8095695972442627
I0127 05:42:31.187862 140005288683264 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.318392753601074, loss=2.6777071952819824
I0127 05:43:04.812566 140004676327168 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.574150800704956, loss=2.807725191116333
I0127 05:43:38.460136 140005288683264 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.2628636360168457, loss=2.772202491760254
I0127 05:44:12.065531 140004676327168 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.5445635318756104, loss=2.7207236289978027
I0127 05:44:45.679056 140005288683264 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.6823220252990723, loss=2.778198719024658
I0127 05:45:19.319470 140004676327168 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.821878433227539, loss=2.6714532375335693
I0127 05:45:52.958848 140005288683264 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.2408628463745117, loss=2.791785478591919
I0127 05:46:08.547618 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:46:15.464898 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:46:28.658957 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:46:30.798432 140169137129280 submission_runner.py:408] Time since start: 15006.27s, 	Step: 42448, 	{'train/accuracy': 0.6384526491165161, 'train/loss': 1.568603515625, 'validation/accuracy': 0.5953999757766724, 'validation/loss': 1.7692608833312988, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4440882205963135, 'test/num_examples': 10000, 'score': 14352.171369075775, 'total_duration': 15006.269856929779, 'accumulated_submission_time': 14352.171369075775, 'accumulated_eval_time': 651.8660788536072, 'accumulated_logging_time': 0.916776180267334}
I0127 05:46:30.819246 140005305468672 logging_writer.py:48] [42448] accumulated_eval_time=651.866079, accumulated_logging_time=0.916776, accumulated_submission_time=14352.171369, global_step=42448, preemption_count=0, score=14352.171369, test/accuracy=0.472000, test/loss=2.444088, test/num_examples=10000, total_duration=15006.269857, train/accuracy=0.638453, train/loss=1.568604, validation/accuracy=0.595400, validation/loss=1.769261, validation/num_examples=50000
I0127 05:46:48.557426 140005313861376 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.4639132022857666, loss=2.740450620651245
I0127 05:47:22.106442 140005305468672 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.1990857124328613, loss=2.770977735519409
I0127 05:47:55.663640 140005313861376 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.0987906455993652, loss=2.7400014400482178
I0127 05:48:29.203104 140005305468672 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.2564730644226074, loss=2.7531778812408447
I0127 05:49:02.741271 140005313861376 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.1989431381225586, loss=2.7525784969329834
I0127 05:49:36.302431 140005305468672 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.262110948562622, loss=2.77954363822937
I0127 05:50:09.917818 140005313861376 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.0311405658721924, loss=2.7731971740722656
I0127 05:50:43.528951 140005305468672 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.8399317264556885, loss=2.757795810699463
I0127 05:51:17.169842 140005313861376 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.779308319091797, loss=2.737457275390625
I0127 05:51:50.794634 140005305468672 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.9346811771392822, loss=2.7064874172210693
I0127 05:52:24.428482 140005313861376 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.594609022140503, loss=2.7712433338165283
I0127 05:52:58.058066 140005305468672 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.912224292755127, loss=2.837769031524658
I0127 05:53:31.645849 140005313861376 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.893697500228882, loss=2.729738712310791
I0127 05:54:05.289983 140005305468672 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.805802583694458, loss=2.8003032207489014
I0127 05:54:38.910202 140005313861376 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.825514554977417, loss=2.775667190551758
I0127 05:55:00.906050 140169137129280 spec.py:321] Evaluating on the training split.
I0127 05:55:07.944201 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 05:55:21.640049 140169137129280 spec.py:349] Evaluating on the test split.
I0127 05:55:23.749218 140169137129280 submission_runner.py:408] Time since start: 15539.22s, 	Step: 43967, 	{'train/accuracy': 0.6627271771430969, 'train/loss': 1.4629147052764893, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6995658874511719, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.402573585510254, 'test/num_examples': 10000, 'score': 14862.203121185303, 'total_duration': 15539.220652580261, 'accumulated_submission_time': 14862.203121185303, 'accumulated_eval_time': 674.709219455719, 'accumulated_logging_time': 0.9466893672943115}
I0127 05:55:23.769253 140005313861376 logging_writer.py:48] [43967] accumulated_eval_time=674.709219, accumulated_logging_time=0.946689, accumulated_submission_time=14862.203121, global_step=43967, preemption_count=0, score=14862.203121, test/accuracy=0.482800, test/loss=2.402574, test/num_examples=10000, total_duration=15539.220653, train/accuracy=0.662727, train/loss=1.462915, validation/accuracy=0.610960, validation/loss=1.699566, validation/num_examples=50000
I0127 05:55:35.189548 140005322254080 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.2775046825408936, loss=2.814465045928955
I0127 05:56:08.768010 140005313861376 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.2480978965759277, loss=2.783694267272949
I0127 05:56:42.352123 140005322254080 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.790839672088623, loss=2.799333333969116
I0127 05:57:15.872332 140005313861376 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.8194468021392822, loss=2.764464855194092
I0127 05:57:49.403222 140005322254080 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.10040283203125, loss=2.8931679725646973
I0127 05:58:22.974425 140005313861376 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.8259615898132324, loss=2.8492794036865234
I0127 05:58:56.576948 140005322254080 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.639763832092285, loss=2.7618513107299805
I0127 05:59:30.193466 140005313861376 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.342738628387451, loss=2.7778990268707275
I0127 06:00:03.825452 140005322254080 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.8281469345092773, loss=2.744906187057495
I0127 06:00:37.460758 140005313861376 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.9465808868408203, loss=2.775181293487549
I0127 06:01:11.077571 140005322254080 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.82075572013855, loss=2.709012269973755
I0127 06:01:44.709918 140005313861376 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.1974997520446777, loss=2.793715476989746
I0127 06:02:18.329530 140005322254080 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.6213321685791016, loss=2.674426317214966
I0127 06:02:51.963971 140005313861376 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.449396848678589, loss=2.8249411582946777
I0127 06:03:25.546384 140005322254080 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.0557162761688232, loss=2.6272552013397217
I0127 06:03:53.821653 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:04:00.703454 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:04:13.104792 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:04:15.575981 140169137129280 submission_runner.py:408] Time since start: 16071.05s, 	Step: 45486, 	{'train/accuracy': 0.674246609210968, 'train/loss': 1.4069665670394897, 'validation/accuracy': 0.6114199757575989, 'validation/loss': 1.696818232536316, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.347254753112793, 'test/num_examples': 10000, 'score': 15372.199818134308, 'total_duration': 16071.047409534454, 'accumulated_submission_time': 15372.199818134308, 'accumulated_eval_time': 696.4635140895844, 'accumulated_logging_time': 0.9763743877410889}
I0127 06:04:15.596551 140005288683264 logging_writer.py:48] [45486] accumulated_eval_time=696.463514, accumulated_logging_time=0.976374, accumulated_submission_time=15372.199818, global_step=45486, preemption_count=0, score=15372.199818, test/accuracy=0.494500, test/loss=2.347255, test/num_examples=10000, total_duration=16071.047410, train/accuracy=0.674247, train/loss=1.406967, validation/accuracy=0.611420, validation/loss=1.696818, validation/num_examples=50000
I0127 06:04:20.635253 140005297075968 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.0272305011749268, loss=2.664250135421753
I0127 06:04:54.220549 140005288683264 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.0164029598236084, loss=2.7958498001098633
I0127 06:05:27.844655 140005297075968 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.910623788833618, loss=2.7663984298706055
I0127 06:06:01.497316 140005288683264 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.1762189865112305, loss=2.721456527709961
I0127 06:06:35.127578 140005297075968 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.4841110706329346, loss=2.8061752319335938
I0127 06:07:08.772658 140005288683264 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.2024850845336914, loss=2.766310214996338
I0127 06:07:42.389305 140005297075968 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.407256841659546, loss=2.7367122173309326
I0127 06:08:16.030787 140005288683264 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.8961732387542725, loss=2.8799571990966797
I0127 06:08:49.654014 140005297075968 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.8696203231811523, loss=2.6312568187713623
I0127 06:09:23.188735 140005288683264 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.4000747203826904, loss=2.660902976989746
I0127 06:09:56.737529 140005297075968 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.975131034851074, loss=2.74776291847229
I0127 06:10:30.350715 140005288683264 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.2380247116088867, loss=2.797248125076294
I0127 06:11:03.990177 140005297075968 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.030135154724121, loss=2.7237133979797363
I0127 06:11:37.618237 140005288683264 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.0791380405426025, loss=2.7470500469207764
I0127 06:12:11.239730 140005297075968 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.985135793685913, loss=2.743042469024658
I0127 06:12:44.856824 140005288683264 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.190837860107422, loss=2.8111064434051514
I0127 06:12:45.637866 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:12:52.411489 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:13:04.310670 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:13:06.619277 140169137129280 submission_runner.py:408] Time since start: 16602.09s, 	Step: 47004, 	{'train/accuracy': 0.6636040806770325, 'train/loss': 1.4506380558013916, 'validation/accuracy': 0.6128799915313721, 'validation/loss': 1.6783645153045654, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3279991149902344, 'test/num_examples': 10000, 'score': 15882.18303823471, 'total_duration': 16602.090711593628, 'accumulated_submission_time': 15882.18303823471, 'accumulated_eval_time': 717.4448945522308, 'accumulated_logging_time': 1.0088527202606201}
I0127 06:13:06.640268 140004667934464 logging_writer.py:48] [47004] accumulated_eval_time=717.444895, accumulated_logging_time=1.008853, accumulated_submission_time=15882.183038, global_step=47004, preemption_count=0, score=15882.183038, test/accuracy=0.491700, test/loss=2.327999, test/num_examples=10000, total_duration=16602.090712, train/accuracy=0.663604, train/loss=1.450638, validation/accuracy=0.612880, validation/loss=1.678365, validation/num_examples=50000
I0127 06:13:39.125436 140004676327168 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.9464762210845947, loss=2.7099828720092773
I0127 06:14:12.720910 140004667934464 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.094174385070801, loss=2.794461965560913
I0127 06:14:46.251518 140004676327168 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.8696231842041016, loss=2.7587451934814453
I0127 06:15:19.843181 140004667934464 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.6521964073181152, loss=2.800121307373047
I0127 06:15:53.454756 140004676327168 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.6127700805664062, loss=2.82063889503479
I0127 06:16:27.079930 140004667934464 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.146996021270752, loss=2.642164468765259
I0127 06:17:00.650479 140004676327168 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.362053871154785, loss=2.726606845855713
I0127 06:17:34.195729 140004667934464 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.3202974796295166, loss=2.813708543777466
I0127 06:18:07.783498 140004676327168 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.727813482284546, loss=2.688237428665161
I0127 06:18:41.408720 140004667934464 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.1605982780456543, loss=2.7185258865356445
I0127 06:19:15.061228 140004676327168 logging_writer.py:48] [48100] global_step=48100, grad_norm=2.811755418777466, loss=2.642669439315796
I0127 06:19:48.665100 140004667934464 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.8253886699676514, loss=2.7748186588287354
I0127 06:20:22.322650 140004676327168 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.355832815170288, loss=2.8566746711730957
I0127 06:20:55.942936 140004667934464 logging_writer.py:48] [48400] global_step=48400, grad_norm=2.9183175563812256, loss=2.885216474533081
I0127 06:21:29.524426 140004676327168 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.0825295448303223, loss=2.715156316757202
I0127 06:21:36.719946 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:21:43.312563 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:21:56.046800 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:21:58.202357 140169137129280 submission_runner.py:408] Time since start: 17133.67s, 	Step: 48523, 	{'train/accuracy': 0.6721540093421936, 'train/loss': 1.4366706609725952, 'validation/accuracy': 0.6180399656295776, 'validation/loss': 1.684572458267212, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.3448281288146973, 'test/num_examples': 10000, 'score': 16392.207607269287, 'total_duration': 17133.673773765564, 'accumulated_submission_time': 16392.207607269287, 'accumulated_eval_time': 738.927268743515, 'accumulated_logging_time': 1.0390520095825195}
I0127 06:21:58.224967 140004676327168 logging_writer.py:48] [48523] accumulated_eval_time=738.927269, accumulated_logging_time=1.039052, accumulated_submission_time=16392.207607, global_step=48523, preemption_count=0, score=16392.207607, test/accuracy=0.499100, test/loss=2.344828, test/num_examples=10000, total_duration=17133.673774, train/accuracy=0.672154, train/loss=1.436671, validation/accuracy=0.618040, validation/loss=1.684572, validation/num_examples=50000
I0127 06:22:24.372305 140005288683264 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.8723063468933105, loss=2.726101875305176
I0127 06:22:57.908735 140004676327168 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.5971052646636963, loss=2.6949121952056885
I0127 06:23:31.472848 140005288683264 logging_writer.py:48] [48800] global_step=48800, grad_norm=2.8820109367370605, loss=2.7222108840942383
I0127 06:24:05.025916 140004676327168 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.2409706115722656, loss=2.7996935844421387
I0127 06:24:38.570844 140005288683264 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.587063789367676, loss=2.71008038520813
I0127 06:25:12.101024 140004676327168 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.3111066818237305, loss=2.7233364582061768
I0127 06:25:45.650547 140005288683264 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.265226125717163, loss=2.6355648040771484
I0127 06:26:19.293862 140004676327168 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.0427982807159424, loss=2.7705297470092773
I0127 06:26:52.885661 140005288683264 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.109130859375, loss=2.80405330657959
I0127 06:27:26.505119 140004676327168 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.32420015335083, loss=2.6781935691833496
I0127 06:28:00.143852 140005288683264 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.154242515563965, loss=2.7516961097717285
I0127 06:28:33.773649 140004676327168 logging_writer.py:48] [49700] global_step=49700, grad_norm=2.8563528060913086, loss=2.8090522289276123
I0127 06:29:07.413268 140005288683264 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.2501485347747803, loss=2.7671334743499756
I0127 06:29:41.043956 140004676327168 logging_writer.py:48] [49900] global_step=49900, grad_norm=2.7000653743743896, loss=2.7352471351623535
I0127 06:30:14.675176 140005288683264 logging_writer.py:48] [50000] global_step=50000, grad_norm=2.855412244796753, loss=2.6630234718322754
I0127 06:30:28.240293 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:30:34.793438 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:30:46.051784 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:30:48.344135 140169137129280 submission_runner.py:408] Time since start: 17663.82s, 	Step: 50042, 	{'train/accuracy': 0.6578244566917419, 'train/loss': 1.4930152893066406, 'validation/accuracy': 0.6173999905586243, 'validation/loss': 1.6958266496658325, 'validation/num_examples': 50000, 'test/accuracy': 0.491100013256073, 'test/loss': 2.3817460536956787, 'test/num_examples': 10000, 'score': 16902.16853904724, 'total_duration': 17663.81556916237, 'accumulated_submission_time': 16902.16853904724, 'accumulated_eval_time': 759.0310838222504, 'accumulated_logging_time': 1.070474624633789}
I0127 06:30:48.368180 140005313861376 logging_writer.py:48] [50042] accumulated_eval_time=759.031084, accumulated_logging_time=1.070475, accumulated_submission_time=16902.168539, global_step=50042, preemption_count=0, score=16902.168539, test/accuracy=0.491100, test/loss=2.381746, test/num_examples=10000, total_duration=17663.815569, train/accuracy=0.657824, train/loss=1.493015, validation/accuracy=0.617400, validation/loss=1.695827, validation/num_examples=50000
I0127 06:31:08.135570 140005322254080 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.3317840099334717, loss=2.7362098693847656
I0127 06:31:41.660748 140005313861376 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.047197103500366, loss=2.813337802886963
I0127 06:32:15.257445 140005322254080 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.0887959003448486, loss=2.7646164894104004
I0127 06:32:48.896055 140005313861376 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.1154632568359375, loss=2.6728572845458984
I0127 06:33:22.531120 140005322254080 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.536942720413208, loss=2.8349502086639404
I0127 06:33:56.168409 140005313861376 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.448807716369629, loss=2.7641353607177734
I0127 06:34:29.792609 140005322254080 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.4099385738372803, loss=2.740201473236084
I0127 06:35:03.348143 140005313861376 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.2975687980651855, loss=2.8139915466308594
I0127 06:35:36.919117 140005322254080 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.2714035511016846, loss=2.7539498805999756
I0127 06:36:10.522845 140005313861376 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.1643481254577637, loss=2.7692713737487793
I0127 06:36:44.121013 140005322254080 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.854142189025879, loss=2.7461090087890625
I0127 06:37:17.781412 140005313861376 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.275006055831909, loss=2.7563300132751465
I0127 06:37:51.416528 140005322254080 logging_writer.py:48] [51300] global_step=51300, grad_norm=2.8821685314178467, loss=2.850172281265259
I0127 06:38:25.062287 140005313861376 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.0464401245117188, loss=2.7549707889556885
I0127 06:38:58.678644 140005322254080 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.458763837814331, loss=2.7786455154418945
I0127 06:39:18.629965 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:39:25.251016 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:39:35.300576 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:39:37.640128 140169137129280 submission_runner.py:408] Time since start: 18193.11s, 	Step: 51561, 	{'train/accuracy': 0.6567083597183228, 'train/loss': 1.477152943611145, 'validation/accuracy': 0.613099992275238, 'validation/loss': 1.6816418170928955, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.355665445327759, 'test/num_examples': 10000, 'score': 17412.375111818314, 'total_duration': 18193.11154937744, 'accumulated_submission_time': 17412.375111818314, 'accumulated_eval_time': 778.0412209033966, 'accumulated_logging_time': 1.1035346984863281}
I0127 06:39:37.668645 140004667934464 logging_writer.py:48] [51561] accumulated_eval_time=778.041221, accumulated_logging_time=1.103535, accumulated_submission_time=17412.375112, global_step=51561, preemption_count=0, score=17412.375112, test/accuracy=0.490200, test/loss=2.355665, test/num_examples=10000, total_duration=18193.111549, train/accuracy=0.656708, train/loss=1.477153, validation/accuracy=0.613100, validation/loss=1.681642, validation/num_examples=50000
I0127 06:39:51.094700 140004676327168 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.485304832458496, loss=2.6955268383026123
I0127 06:40:24.587255 140004667934464 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.8809587955474854, loss=2.7508623600006104
I0127 06:40:58.078594 140004676327168 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.2237653732299805, loss=2.7389893531799316
I0127 06:41:31.685785 140004667934464 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.082510471343994, loss=2.8501808643341064
I0127 06:42:05.322936 140004676327168 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.8335673809051514, loss=2.776097297668457
I0127 06:42:38.907454 140004667934464 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.1878585815429688, loss=2.660966634750366
I0127 06:43:12.428370 140004676327168 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.7719335556030273, loss=2.6288745403289795
I0127 06:43:45.958043 140004667934464 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.0262370109558105, loss=2.7557809352874756
I0127 06:44:19.539861 140004676327168 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.0845770835876465, loss=2.73483943939209
I0127 06:44:53.188609 140004667934464 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.029414653778076, loss=2.7155375480651855
I0127 06:45:26.788111 140004676327168 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.3533427715301514, loss=2.6272165775299072
I0127 06:46:00.412674 140004667934464 logging_writer.py:48] [52700] global_step=52700, grad_norm=2.8821704387664795, loss=2.7539901733398438
I0127 06:46:34.041640 140004676327168 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.9192659854888916, loss=2.6949546337127686
I0127 06:47:07.647000 140004667934464 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.188897132873535, loss=2.7337589263916016
I0127 06:47:41.269035 140004676327168 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.15250563621521, loss=2.692262649536133
I0127 06:48:07.936290 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:48:14.414835 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:48:23.267936 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:48:25.688083 140169137129280 submission_runner.py:408] Time since start: 18721.16s, 	Step: 53081, 	{'train/accuracy': 0.7010921239852905, 'train/loss': 1.2759617567062378, 'validation/accuracy': 0.6177399754524231, 'validation/loss': 1.6607871055603027, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.3438918590545654, 'test/num_examples': 10000, 'score': 17922.586621522903, 'total_duration': 18721.159491062164, 'accumulated_submission_time': 17922.586621522903, 'accumulated_eval_time': 795.7929601669312, 'accumulated_logging_time': 1.1418347358703613}
I0127 06:48:25.713891 140004667934464 logging_writer.py:48] [53081] accumulated_eval_time=795.792960, accumulated_logging_time=1.141835, accumulated_submission_time=17922.586622, global_step=53081, preemption_count=0, score=17922.586622, test/accuracy=0.496800, test/loss=2.343892, test/num_examples=10000, total_duration=18721.159491, train/accuracy=0.701092, train/loss=1.275962, validation/accuracy=0.617740, validation/loss=1.660787, validation/num_examples=50000
I0127 06:48:32.418147 140005313861376 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.115571975708008, loss=2.858375310897827
I0127 06:49:05.926969 140004667934464 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.053332805633545, loss=2.6903252601623535
I0127 06:49:39.530563 140005313861376 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.221870183944702, loss=2.7572872638702393
I0127 06:50:13.171150 140004667934464 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.1269290447235107, loss=2.631844997406006
I0127 06:50:46.805935 140005313861376 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.188716173171997, loss=2.70467472076416
I0127 06:51:20.403384 140004667934464 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.366046667098999, loss=2.7273778915405273
I0127 06:51:54.039647 140005313861376 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.523456573486328, loss=2.75886607170105
I0127 06:52:27.662064 140004667934464 logging_writer.py:48] [53800] global_step=53800, grad_norm=2.942324638366699, loss=2.8023784160614014
I0127 06:53:01.204308 140005313861376 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.1104555130004883, loss=2.662564754486084
I0127 06:53:34.737024 140004667934464 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.8908119201660156, loss=2.747476816177368
I0127 06:54:08.281424 140005313861376 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.1874170303344727, loss=2.794074773788452
I0127 06:54:41.866359 140004667934464 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.5176239013671875, loss=2.7345104217529297
I0127 06:55:15.495456 140005313861376 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.0241434574127197, loss=2.8346991539001465
I0127 06:55:49.118451 140004667934464 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.3429036140441895, loss=2.713042736053467
I0127 06:56:22.759964 140005313861376 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.4282801151275635, loss=2.715285062789917
I0127 06:56:55.847777 140169137129280 spec.py:321] Evaluating on the training split.
I0127 06:57:02.323676 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 06:57:11.081137 140169137129280 spec.py:349] Evaluating on the test split.
I0127 06:57:13.380566 140169137129280 submission_runner.py:408] Time since start: 19248.85s, 	Step: 54600, 	{'train/accuracy': 0.6756815910339355, 'train/loss': 1.4029406309127808, 'validation/accuracy': 0.6170799732208252, 'validation/loss': 1.6762901544570923, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.347243547439575, 'test/num_examples': 10000, 'score': 18432.664180994034, 'total_duration': 19248.851968050003, 'accumulated_submission_time': 18432.664180994034, 'accumulated_eval_time': 813.3257002830505, 'accumulated_logging_time': 1.1776843070983887}
I0127 06:57:13.415508 140004659541760 logging_writer.py:48] [54600] accumulated_eval_time=813.325700, accumulated_logging_time=1.177684, accumulated_submission_time=18432.664181, global_step=54600, preemption_count=0, score=18432.664181, test/accuracy=0.495800, test/loss=2.347244, test/num_examples=10000, total_duration=19248.851968, train/accuracy=0.675682, train/loss=1.402941, validation/accuracy=0.617080, validation/loss=1.676290, validation/num_examples=50000
I0127 06:57:13.778917 140004667934464 logging_writer.py:48] [54600] global_step=54600, grad_norm=2.968297004699707, loss=2.6660187244415283
I0127 06:57:47.347442 140004659541760 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.9988386631011963, loss=2.6588423252105713
I0127 06:58:20.862462 140004667934464 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.024940013885498, loss=2.7819652557373047
I0127 06:58:54.439851 140004659541760 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.09175705909729, loss=2.730616569519043
I0127 06:59:28.029781 140004667934464 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.907015323638916, loss=2.7307965755462646
I0127 07:00:01.673261 140004659541760 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.1194491386413574, loss=2.7247910499572754
I0127 07:00:35.312678 140004667934464 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.7527575492858887, loss=2.7787108421325684
I0127 07:01:08.939015 140004659541760 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.740602493286133, loss=2.6442019939422607
I0127 07:01:42.540665 140004667934464 logging_writer.py:48] [55400] global_step=55400, grad_norm=2.7507967948913574, loss=2.652195453643799
I0127 07:02:16.168129 140004659541760 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.0897085666656494, loss=2.6325416564941406
I0127 07:02:49.769418 140004667934464 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.4088380336761475, loss=2.626837968826294
I0127 07:03:23.287350 140004659541760 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.8142201900482178, loss=2.758030414581299
I0127 07:03:56.889367 140004667934464 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.940592050552368, loss=2.7143516540527344
I0127 07:04:30.519678 140004659541760 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.3953840732574463, loss=2.7265217304229736
I0127 07:05:04.131004 140004667934464 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.832554578781128, loss=2.6543192863464355
I0127 07:05:37.758260 140004659541760 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.2942874431610107, loss=2.5949313640594482
I0127 07:05:43.618320 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:05:50.087760 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:06:00.580954 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:06:02.846153 140169137129280 submission_runner.py:408] Time since start: 19778.32s, 	Step: 56119, 	{'train/accuracy': 0.6758211255073547, 'train/loss': 1.4040615558624268, 'validation/accuracy': 0.6243000030517578, 'validation/loss': 1.6517601013183594, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.3165369033813477, 'test/num_examples': 10000, 'score': 18942.804827928543, 'total_duration': 19778.317579746246, 'accumulated_submission_time': 18942.804827928543, 'accumulated_eval_time': 832.5534996986389, 'accumulated_logging_time': 1.2281639575958252}
I0127 07:06:02.870017 140004659541760 logging_writer.py:48] [56119] accumulated_eval_time=832.553500, accumulated_logging_time=1.228164, accumulated_submission_time=18942.804828, global_step=56119, preemption_count=0, score=18942.804828, test/accuracy=0.501500, test/loss=2.316537, test/num_examples=10000, total_duration=19778.317580, train/accuracy=0.675821, train/loss=1.404062, validation/accuracy=0.624300, validation/loss=1.651760, validation/num_examples=50000
I0127 07:06:30.331476 140004667934464 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.8628010749816895, loss=2.7184343338012695
I0127 07:07:03.832714 140004659541760 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.3707425594329834, loss=2.6519508361816406
I0127 07:07:37.379157 140004667934464 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.1097757816314697, loss=2.7419354915618896
I0127 07:08:10.979027 140004659541760 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.8601953983306885, loss=2.786175489425659
I0127 07:08:44.599440 140004667934464 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.835963487625122, loss=2.85886812210083
I0127 07:09:18.225801 140004659541760 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.298611640930176, loss=2.7850286960601807
I0127 07:09:51.849914 140004667934464 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.7151172161102295, loss=2.8288023471832275
I0127 07:10:25.472797 140004659541760 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.774965763092041, loss=2.7212491035461426
I0127 07:10:59.098468 140004667934464 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.0480258464813232, loss=2.800199270248413
I0127 07:11:32.691235 140004659541760 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.200749158859253, loss=2.6722030639648438
I0127 07:12:06.241684 140004667934464 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.2758800983428955, loss=2.738281726837158
I0127 07:12:39.781646 140004659541760 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.8738982677459717, loss=2.7608108520507812
I0127 07:13:13.367599 140004667934464 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.371554136276245, loss=2.7239394187927246
I0127 07:13:46.966586 140004659541760 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.270095109939575, loss=2.7087316513061523
I0127 07:14:20.586666 140004667934464 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.247544765472412, loss=2.697150230407715
I0127 07:14:33.154704 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:14:39.528588 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:14:48.779432 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:14:51.077177 140169137129280 submission_runner.py:408] Time since start: 20306.55s, 	Step: 57639, 	{'train/accuracy': 0.6713966727256775, 'train/loss': 1.409226417541504, 'validation/accuracy': 0.6262199878692627, 'validation/loss': 1.636522889137268, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.2848803997039795, 'test/num_examples': 10000, 'score': 19453.03369998932, 'total_duration': 20306.5486035347, 'accumulated_submission_time': 19453.03369998932, 'accumulated_eval_time': 850.4759373664856, 'accumulated_logging_time': 1.2610328197479248}
I0127 07:14:51.104424 140004667934464 logging_writer.py:48] [57639] accumulated_eval_time=850.475937, accumulated_logging_time=1.261033, accumulated_submission_time=19453.033700, global_step=57639, preemption_count=0, score=19453.033700, test/accuracy=0.505000, test/loss=2.284880, test/num_examples=10000, total_duration=20306.548604, train/accuracy=0.671397, train/loss=1.409226, validation/accuracy=0.626220, validation/loss=1.636523, validation/num_examples=50000
I0127 07:15:11.965099 140004676327168 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.057326555252075, loss=2.6210951805114746
I0127 07:15:45.580987 140004667934464 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.249951124191284, loss=2.6416587829589844
I0127 07:16:19.231492 140004676327168 logging_writer.py:48] [57900] global_step=57900, grad_norm=2.8917293548583984, loss=2.707162857055664
I0127 07:16:52.847411 140004667934464 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.4559905529022217, loss=2.745959520339966
I0127 07:17:26.425026 140004676327168 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.2019526958465576, loss=2.5576796531677246
I0127 07:17:59.968491 140004667934464 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.2349236011505127, loss=2.744591236114502
I0127 07:18:33.567289 140004676327168 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.778216600418091, loss=2.6068902015686035
I0127 07:19:07.182539 140004667934464 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.868734836578369, loss=2.6526219844818115
I0127 07:19:40.813098 140004676327168 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.100200891494751, loss=2.629420757293701
I0127 07:20:14.420923 140004667934464 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.073761224746704, loss=2.6926586627960205
I0127 07:20:48.042935 140004676327168 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.7582881450653076, loss=2.687204360961914
I0127 07:21:21.653103 140004667934464 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.4604086875915527, loss=2.7298643589019775
I0127 07:21:55.263772 140004676327168 logging_writer.py:48] [58900] global_step=58900, grad_norm=4.118968963623047, loss=2.629324436187744
I0127 07:22:28.889634 140004667934464 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.2283287048339844, loss=2.773205041885376
I0127 07:23:02.509867 140004676327168 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.8142054080963135, loss=2.728018283843994
I0127 07:23:21.125491 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:23:27.494308 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:23:36.160731 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:23:38.472244 140169137129280 submission_runner.py:408] Time since start: 20833.94s, 	Step: 59157, 	{'train/accuracy': 0.6717753410339355, 'train/loss': 1.4401389360427856, 'validation/accuracy': 0.6253199577331543, 'validation/loss': 1.6600350141525269, 'validation/num_examples': 50000, 'test/accuracy': 0.49870002269744873, 'test/loss': 2.3283464908599854, 'test/num_examples': 10000, 'score': 19962.99881720543, 'total_duration': 20833.943662643433, 'accumulated_submission_time': 19962.99881720543, 'accumulated_eval_time': 867.8226597309113, 'accumulated_logging_time': 1.2980821132659912}
I0127 07:23:38.500264 140004667934464 logging_writer.py:48] [59157] accumulated_eval_time=867.822660, accumulated_logging_time=1.298082, accumulated_submission_time=19962.998817, global_step=59157, preemption_count=0, score=19962.998817, test/accuracy=0.498700, test/loss=2.328346, test/num_examples=10000, total_duration=20833.943663, train/accuracy=0.671775, train/loss=1.440139, validation/accuracy=0.625320, validation/loss=1.660035, validation/num_examples=50000
I0127 07:23:53.256869 140005305468672 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.502357006072998, loss=2.6949923038482666
I0127 07:24:26.788928 140004667934464 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.2215301990509033, loss=2.6635313034057617
I0127 07:25:00.373142 140005305468672 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.0971810817718506, loss=2.6413822174072266
I0127 07:25:33.943025 140004667934464 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.9338550567626953, loss=2.641970634460449
I0127 07:26:07.584825 140005305468672 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.1532859802246094, loss=2.779062509536743
I0127 07:26:41.209315 140004667934464 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.9917304515838623, loss=2.7195088863372803
I0127 07:27:14.864058 140005305468672 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.9062485694885254, loss=2.6397619247436523
I0127 07:27:48.463913 140004667934464 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.5060224533081055, loss=2.709167242050171
I0127 07:28:22.060983 140005305468672 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.535280466079712, loss=2.7887163162231445
I0127 07:28:55.637258 140004667934464 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.2954306602478027, loss=2.74670672416687
I0127 07:29:29.239249 140005305468672 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.9821505546569824, loss=2.6852059364318848
I0127 07:30:02.756094 140004667934464 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.0709502696990967, loss=2.687861919403076
I0127 07:30:36.279047 140005305468672 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.3240878582000732, loss=2.656282901763916
I0127 07:31:09.836925 140004667934464 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.926661252975464, loss=2.6763343811035156
I0127 07:31:43.420776 140005305468672 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.931057929992676, loss=2.5629405975341797
I0127 07:32:08.781621 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:32:15.147801 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:32:23.825864 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:32:26.084576 140169137129280 submission_runner.py:408] Time since start: 21361.56s, 	Step: 60677, 	{'train/accuracy': 0.6564891338348389, 'train/loss': 1.4987339973449707, 'validation/accuracy': 0.614579975605011, 'validation/loss': 1.6961729526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.3575313091278076, 'test/num_examples': 10000, 'score': 20473.223114967346, 'total_duration': 21361.555997371674, 'accumulated_submission_time': 20473.223114967346, 'accumulated_eval_time': 885.1255774497986, 'accumulated_logging_time': 1.3364100456237793}
I0127 07:32:26.116460 140004676327168 logging_writer.py:48] [60677] accumulated_eval_time=885.125577, accumulated_logging_time=1.336410, accumulated_submission_time=20473.223115, global_step=60677, preemption_count=0, score=20473.223115, test/accuracy=0.495600, test/loss=2.357531, test/num_examples=10000, total_duration=21361.555997, train/accuracy=0.656489, train/loss=1.498734, validation/accuracy=0.614580, validation/loss=1.696173, validation/num_examples=50000
I0127 07:32:34.169160 140005288683264 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.121474266052246, loss=2.669978380203247
I0127 07:33:07.713487 140004676327168 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.345186471939087, loss=2.806826114654541
I0127 07:33:41.349351 140005288683264 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.0333359241485596, loss=2.680788516998291
I0127 07:34:14.900571 140004676327168 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.153977394104004, loss=2.6514763832092285
I0127 07:34:48.478152 140005288683264 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.851611375808716, loss=2.6621041297912598
I0127 07:35:22.007067 140004676327168 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.3925397396087646, loss=2.684791088104248
I0127 07:35:55.544226 140005288683264 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.968327760696411, loss=2.592468738555908
I0127 07:36:29.132993 140004676327168 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.225397825241089, loss=2.6932950019836426
I0127 07:37:02.761096 140005288683264 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.2161757946014404, loss=2.840296983718872
I0127 07:37:36.414815 140004676327168 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.217292547225952, loss=2.665557384490967
I0127 07:38:10.022931 140005288683264 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.623034715652466, loss=2.7318596839904785
I0127 07:38:43.606270 140004676327168 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.05704927444458, loss=2.7491698265075684
I0127 07:39:17.281693 140005288683264 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.169114351272583, loss=2.775893449783325
I0127 07:39:50.876456 140004676327168 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.0847315788269043, loss=2.731696367263794
I0127 07:40:24.488227 140005288683264 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.316267251968384, loss=2.691955804824829
I0127 07:40:56.193124 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:41:02.593825 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:41:11.322035 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:41:13.689780 140169137129280 submission_runner.py:408] Time since start: 21889.16s, 	Step: 62196, 	{'train/accuracy': 0.7053770422935486, 'train/loss': 1.2767313718795776, 'validation/accuracy': 0.6273599863052368, 'validation/loss': 1.6219308376312256, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.3041574954986572, 'test/num_examples': 10000, 'score': 20983.24218249321, 'total_duration': 21889.161187648773, 'accumulated_submission_time': 20983.24218249321, 'accumulated_eval_time': 902.6221942901611, 'accumulated_logging_time': 1.3790311813354492}
I0127 07:41:13.723081 140004659541760 logging_writer.py:48] [62196] accumulated_eval_time=902.622194, accumulated_logging_time=1.379031, accumulated_submission_time=20983.242182, global_step=62196, preemption_count=0, score=20983.242182, test/accuracy=0.507400, test/loss=2.304157, test/num_examples=10000, total_duration=21889.161188, train/accuracy=0.705377, train/loss=1.276731, validation/accuracy=0.627360, validation/loss=1.621931, validation/num_examples=50000
I0127 07:41:15.403224 140004667934464 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.3214828968048096, loss=2.8244969844818115
I0127 07:41:48.943179 140004659541760 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.8062331676483154, loss=2.6317782402038574
I0127 07:42:22.425368 140004667934464 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.1738390922546387, loss=2.695164203643799
I0127 07:42:56.000769 140004659541760 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.3029770851135254, loss=2.7014732360839844
I0127 07:43:29.635857 140004667934464 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.282449960708618, loss=2.709597587585449
I0127 07:44:03.244766 140004659541760 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.104094982147217, loss=2.7880232334136963
I0127 07:44:36.859878 140004667934464 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.080540418624878, loss=2.663944721221924
I0127 07:45:10.447652 140004659541760 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.039710760116577, loss=2.641594409942627
I0127 07:45:44.081912 140004667934464 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.0782623291015625, loss=2.647820234298706
I0127 07:46:17.690086 140004659541760 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.2392449378967285, loss=2.7904787063598633
I0127 07:46:51.290844 140004667934464 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.5452146530151367, loss=2.659621477127075
I0127 07:47:24.895202 140004659541760 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.0511343479156494, loss=2.668937921524048
I0127 07:47:58.516381 140004667934464 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.1170737743377686, loss=2.6898229122161865
I0127 07:48:32.107485 140004659541760 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.137995958328247, loss=2.6299362182617188
I0127 07:49:05.726400 140004667934464 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.236506700515747, loss=2.685044288635254
I0127 07:49:39.314785 140004659541760 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.3751578330993652, loss=2.5986273288726807
I0127 07:49:43.821600 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:49:50.177910 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:49:58.938507 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:50:01.244274 140169137129280 submission_runner.py:408] Time since start: 22416.72s, 	Step: 63715, 	{'train/accuracy': 0.686922013759613, 'train/loss': 1.3271760940551758, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.617149829864502, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.267906904220581, 'test/num_examples': 10000, 'score': 21493.278660058975, 'total_duration': 22416.715700864792, 'accumulated_submission_time': 21493.278660058975, 'accumulated_eval_time': 920.0448322296143, 'accumulated_logging_time': 1.4266955852508545}
I0127 07:50:01.274277 140004676327168 logging_writer.py:48] [63715] accumulated_eval_time=920.044832, accumulated_logging_time=1.426696, accumulated_submission_time=21493.278660, global_step=63715, preemption_count=0, score=21493.278660, test/accuracy=0.506400, test/loss=2.267907, test/num_examples=10000, total_duration=22416.715701, train/accuracy=0.686922, train/loss=1.327176, validation/accuracy=0.626800, validation/loss=1.617150, validation/num_examples=50000
I0127 07:50:30.063129 140005297075968 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.9320545196533203, loss=2.650989294052124
I0127 07:51:03.647241 140004676327168 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.1622753143310547, loss=2.590261459350586
I0127 07:51:37.302700 140005297075968 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.229412078857422, loss=2.7374768257141113
I0127 07:52:10.925917 140004676327168 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.9212841987609863, loss=2.595322608947754
I0127 07:52:44.537834 140005297075968 logging_writer.py:48] [64200] global_step=64200, grad_norm=2.982011556625366, loss=2.6971142292022705
I0127 07:53:18.115664 140004676327168 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.9450185298919678, loss=2.702385425567627
I0127 07:53:51.743516 140005297075968 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.169029712677002, loss=2.7155990600585938
I0127 07:54:25.383278 140004676327168 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.5915234088897705, loss=2.652024745941162
I0127 07:54:58.997184 140005297075968 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.0553605556488037, loss=2.6010923385620117
I0127 07:55:32.582962 140004676327168 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.092763662338257, loss=2.6925551891326904
I0127 07:56:06.206716 140005297075968 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.31380295753479, loss=2.660104751586914
I0127 07:56:39.749902 140004676327168 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.3358232975006104, loss=2.7779011726379395
I0127 07:57:13.273317 140005297075968 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.1119489669799805, loss=2.592949151992798
I0127 07:57:46.926015 140004676327168 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.0285513401031494, loss=2.6596429347991943
I0127 07:58:20.537883 140005297075968 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.9431307315826416, loss=2.7586147785186768
I0127 07:58:31.407254 140169137129280 spec.py:321] Evaluating on the training split.
I0127 07:58:37.767368 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 07:58:46.251298 140169137129280 spec.py:349] Evaluating on the test split.
I0127 07:58:48.558783 140169137129280 submission_runner.py:408] Time since start: 22944.03s, 	Step: 65234, 	{'train/accuracy': 0.6711774468421936, 'train/loss': 1.4122507572174072, 'validation/accuracy': 0.6250199675559998, 'validation/loss': 1.6446616649627686, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.329805850982666, 'test/num_examples': 10000, 'score': 22003.35453414917, 'total_duration': 22944.030207157135, 'accumulated_submission_time': 22003.35453414917, 'accumulated_eval_time': 937.196320772171, 'accumulated_logging_time': 1.4666364192962646}
I0127 07:58:48.587252 140004667934464 logging_writer.py:48] [65234] accumulated_eval_time=937.196321, accumulated_logging_time=1.466636, accumulated_submission_time=22003.354534, global_step=65234, preemption_count=0, score=22003.354534, test/accuracy=0.498900, test/loss=2.329806, test/num_examples=10000, total_duration=22944.030207, train/accuracy=0.671177, train/loss=1.412251, validation/accuracy=0.625020, validation/loss=1.644662, validation/num_examples=50000
I0127 07:59:11.063087 140004676327168 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.6340603828430176, loss=2.6957974433898926
I0127 07:59:44.567997 140004667934464 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.0307040214538574, loss=2.622783660888672
I0127 08:00:18.108675 140004676327168 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.1658568382263184, loss=2.6815292835235596
I0127 08:00:51.693147 140004667934464 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.523345470428467, loss=2.658033609390259
I0127 08:01:25.320178 140004676327168 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.0761468410491943, loss=2.5817627906799316
I0127 08:01:58.957518 140004667934464 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.992769241333008, loss=2.7043724060058594
I0127 08:02:32.564393 140004676327168 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.418083429336548, loss=2.6861977577209473
I0127 08:03:06.182809 140004667934464 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.3604416847229004, loss=2.67317533493042
I0127 08:03:39.839503 140004676327168 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.2625925540924072, loss=2.655207633972168
I0127 08:04:13.415217 140004667934464 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.4059226512908936, loss=2.617096185684204
I0127 08:04:47.022454 140004676327168 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.2338736057281494, loss=2.6731741428375244
I0127 08:05:20.625256 140004667934464 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.462067127227783, loss=2.7413272857666016
I0127 08:05:54.209461 140004676327168 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.184901475906372, loss=2.7106642723083496
I0127 08:06:27.857864 140004667934464 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.117788076400757, loss=2.6598496437072754
I0127 08:07:01.468228 140004676327168 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.549996852874756, loss=2.6865506172180176
I0127 08:07:18.756412 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:07:25.124620 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:07:33.753562 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:07:35.990669 140169137129280 submission_runner.py:408] Time since start: 23471.46s, 	Step: 66753, 	{'train/accuracy': 0.6834542155265808, 'train/loss': 1.3994112014770508, 'validation/accuracy': 0.635159969329834, 'validation/loss': 1.6157879829406738, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2873687744140625, 'test/num_examples': 10000, 'score': 22513.463595867157, 'total_duration': 23471.462081432343, 'accumulated_submission_time': 22513.463595867157, 'accumulated_eval_time': 954.4305288791656, 'accumulated_logging_time': 1.5082590579986572}
I0127 08:07:36.019025 140004659541760 logging_writer.py:48] [66753] accumulated_eval_time=954.430529, accumulated_logging_time=1.508259, accumulated_submission_time=22513.463596, global_step=66753, preemption_count=0, score=22513.463596, test/accuracy=0.510500, test/loss=2.287369, test/num_examples=10000, total_duration=23471.462081, train/accuracy=0.683454, train/loss=1.399411, validation/accuracy=0.635160, validation/loss=1.615788, validation/num_examples=50000
I0127 08:07:52.092018 140005305468672 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.9101674556732178, loss=2.6955785751342773
I0127 08:08:25.643100 140004659541760 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.13455867767334, loss=2.6985714435577393
I0127 08:08:59.158579 140005305468672 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.647181987762451, loss=2.704792022705078
I0127 08:09:32.692141 140004659541760 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.1340525150299072, loss=2.687077045440674
I0127 08:10:06.300255 140005305468672 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.6778857707977295, loss=2.691497802734375
I0127 08:10:39.852128 140004659541760 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.0395307540893555, loss=2.6541833877563477
I0127 08:11:13.446750 140005305468672 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.042649507522583, loss=2.6568844318389893
I0127 08:11:47.071751 140004659541760 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.374013662338257, loss=2.6365904808044434
I0127 08:12:20.724474 140005305468672 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.2159550189971924, loss=2.6729066371917725
I0127 08:12:54.326944 140004659541760 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.31988263130188, loss=2.653215169906616
I0127 08:13:27.871221 140005305468672 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.2918434143066406, loss=2.6969449520111084
I0127 08:14:01.422867 140004659541760 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.6058826446533203, loss=2.5984578132629395
I0127 08:14:35.032036 140005305468672 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.167933940887451, loss=2.7744271755218506
I0127 08:15:08.669040 140004659541760 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.172952175140381, loss=2.687633991241455
I0127 08:15:42.318509 140005305468672 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.260139226913452, loss=2.654444932937622
I0127 08:16:06.129399 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:16:12.680341 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:16:20.986082 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:16:23.298645 140169137129280 submission_runner.py:408] Time since start: 23998.77s, 	Step: 68272, 	{'train/accuracy': 0.6808832883834839, 'train/loss': 1.3870117664337158, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.601508378982544, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2549126148223877, 'test/num_examples': 10000, 'score': 23023.515419483185, 'total_duration': 23998.77006816864, 'accumulated_submission_time': 23023.515419483185, 'accumulated_eval_time': 971.5997366905212, 'accumulated_logging_time': 1.5481698513031006}
I0127 08:16:23.329252 140005322254080 logging_writer.py:48] [68272] accumulated_eval_time=971.599737, accumulated_logging_time=1.548170, accumulated_submission_time=23023.515419, global_step=68272, preemption_count=0, score=23023.515419, test/accuracy=0.511300, test/loss=2.254913, test/num_examples=10000, total_duration=23998.770068, train/accuracy=0.680883, train/loss=1.387012, validation/accuracy=0.635020, validation/loss=1.601508, validation/num_examples=50000
I0127 08:16:33.061918 140005330646784 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.2294692993164062, loss=2.6653687953948975
I0127 08:17:06.614294 140005322254080 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.36395263671875, loss=2.674412727355957
I0127 08:17:40.169936 140005330646784 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.3894994258880615, loss=2.7684266567230225
I0127 08:18:13.697278 140005322254080 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.0799245834350586, loss=2.6164045333862305
I0127 08:18:47.234306 140005330646784 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.521913766860962, loss=2.6394734382629395
I0127 08:19:20.766169 140005322254080 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.498746395111084, loss=2.653545379638672
I0127 08:19:54.359328 140005330646784 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.190685749053955, loss=2.704967737197876
I0127 08:20:27.990587 140005322254080 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.964370012283325, loss=2.668541431427002
I0127 08:21:01.595214 140005330646784 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.8255443572998047, loss=2.697845935821533
I0127 08:21:35.216466 140005322254080 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.833378553390503, loss=2.6697371006011963
I0127 08:22:08.885980 140005330646784 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.931447982788086, loss=2.665792465209961
I0127 08:22:42.491133 140005322254080 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.024362802505493, loss=2.647874355316162
I0127 08:23:16.084844 140005330646784 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.503727912902832, loss=2.625247001647949
I0127 08:23:49.704513 140005322254080 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.339134931564331, loss=2.567305326461792
I0127 08:24:23.313919 140005330646784 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.4862678050994873, loss=2.629031181335449
I0127 08:24:53.383583 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:24:59.787911 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:25:08.413069 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:25:10.693161 140169137129280 submission_runner.py:408] Time since start: 24526.16s, 	Step: 69791, 	{'train/accuracy': 0.6853276491165161, 'train/loss': 1.3273032903671265, 'validation/accuracy': 0.6344199776649475, 'validation/loss': 1.5748940706253052, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.2334415912628174, 'test/num_examples': 10000, 'score': 23533.512261867523, 'total_duration': 24526.164582252502, 'accumulated_submission_time': 23533.512261867523, 'accumulated_eval_time': 988.9092743396759, 'accumulated_logging_time': 1.5890882015228271}
I0127 08:25:10.725028 140005288683264 logging_writer.py:48] [69791] accumulated_eval_time=988.909274, accumulated_logging_time=1.589088, accumulated_submission_time=23533.512262, global_step=69791, preemption_count=0, score=23533.512262, test/accuracy=0.514600, test/loss=2.233442, test/num_examples=10000, total_duration=24526.164582, train/accuracy=0.685328, train/loss=1.327303, validation/accuracy=0.634420, validation/loss=1.574894, validation/num_examples=50000
I0127 08:25:14.077884 140005297075968 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.7517144680023193, loss=2.672898769378662
I0127 08:25:47.609995 140005288683264 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.0789644718170166, loss=2.6291396617889404
I0127 08:26:21.166155 140005297075968 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.1696431636810303, loss=2.6717398166656494
I0127 08:26:54.682025 140005288683264 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.3445355892181396, loss=2.731590986251831
I0127 08:27:28.227163 140005297075968 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.4812674522399902, loss=2.679133415222168
I0127 08:28:01.906820 140005288683264 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.15427827835083, loss=2.550980806350708
I0127 08:28:35.468204 140005297075968 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.2314321994781494, loss=2.549506664276123
I0127 08:29:09.092300 140005288683264 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.042351484298706, loss=2.682466745376587
I0127 08:29:42.696674 140005297075968 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.402984619140625, loss=2.6532557010650635
I0127 08:30:16.323901 140005288683264 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.589729070663452, loss=2.7194273471832275
I0127 08:30:49.924298 140005297075968 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.0763516426086426, loss=2.67520809173584
I0127 08:31:23.511873 140005288683264 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.099113941192627, loss=2.6813745498657227
I0127 08:31:57.134103 140005297075968 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.5503592491149902, loss=2.5730628967285156
I0127 08:32:30.706836 140005288683264 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.2484705448150635, loss=2.587552785873413
I0127 08:33:04.330019 140005297075968 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.8366806507110596, loss=2.7943503856658936
I0127 08:33:37.949496 140005288683264 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.917930841445923, loss=2.6362526416778564
I0127 08:33:40.786938 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:33:47.271591 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:33:55.807353 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:33:58.096634 140169137129280 submission_runner.py:408] Time since start: 25053.57s, 	Step: 71310, 	{'train/accuracy': 0.7115951776504517, 'train/loss': 1.2286536693572998, 'validation/accuracy': 0.6373599767684937, 'validation/loss': 1.564759612083435, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.210066556930542, 'test/num_examples': 10000, 'score': 24043.516547441483, 'total_duration': 25053.56804537773, 'accumulated_submission_time': 24043.516547441483, 'accumulated_eval_time': 1006.218918800354, 'accumulated_logging_time': 1.6315371990203857}
I0127 08:33:58.128642 140004667934464 logging_writer.py:48] [71310] accumulated_eval_time=1006.218919, accumulated_logging_time=1.631537, accumulated_submission_time=24043.516547, global_step=71310, preemption_count=0, score=24043.516547, test/accuracy=0.520900, test/loss=2.210067, test/num_examples=10000, total_duration=25053.568045, train/accuracy=0.711595, train/loss=1.228654, validation/accuracy=0.637360, validation/loss=1.564760, validation/num_examples=50000
I0127 08:34:28.692691 140004676327168 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.307387113571167, loss=2.6284337043762207
I0127 08:35:02.296973 140004667934464 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.1961326599121094, loss=2.579608201980591
I0127 08:35:35.865044 140004676327168 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.101916551589966, loss=2.6647613048553467
I0127 08:36:09.416492 140004667934464 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.802133560180664, loss=2.719454765319824
I0127 08:36:42.969721 140004676327168 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.496605157852173, loss=2.6579859256744385
I0127 08:37:16.487905 140004667934464 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.2107980251312256, loss=2.704482078552246
I0127 08:37:50.038342 140004676327168 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.226344108581543, loss=2.59794282913208
I0127 08:38:23.555870 140004667934464 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.136578321456909, loss=2.6833648681640625
I0127 08:38:57.155572 140004676327168 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.3122200965881348, loss=2.6760122776031494
I0127 08:39:30.772768 140004667934464 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.2833352088928223, loss=2.6750400066375732
I0127 08:40:04.377988 140004676327168 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.919203758239746, loss=2.6299209594726562
I0127 08:40:37.986843 140004667934464 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.355292558670044, loss=2.607750177383423
I0127 08:41:11.575763 140004676327168 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.4058420658111572, loss=2.7101476192474365
I0127 08:41:45.201367 140004667934464 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.411579132080078, loss=2.754102945327759
I0127 08:42:18.797283 140004676327168 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.4222731590270996, loss=2.6128671169281006
I0127 08:42:28.365800 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:42:34.753698 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:42:43.170214 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:42:45.496989 140169137129280 submission_runner.py:408] Time since start: 25580.97s, 	Step: 72830, 	{'train/accuracy': 0.6828762888908386, 'train/loss': 1.3698737621307373, 'validation/accuracy': 0.6262800097465515, 'validation/loss': 1.6336623430252075, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.260319232940674, 'test/num_examples': 10000, 'score': 24553.690885543823, 'total_duration': 25580.968412399292, 'accumulated_submission_time': 24553.690885543823, 'accumulated_eval_time': 1023.3500711917877, 'accumulated_logging_time': 1.6788830757141113}
I0127 08:42:45.526207 140004667934464 logging_writer.py:48] [72830] accumulated_eval_time=1023.350071, accumulated_logging_time=1.678883, accumulated_submission_time=24553.690886, global_step=72830, preemption_count=0, score=24553.690886, test/accuracy=0.509600, test/loss=2.260319, test/num_examples=10000, total_duration=25580.968412, train/accuracy=0.682876, train/loss=1.369874, validation/accuracy=0.626280, validation/loss=1.633662, validation/num_examples=50000
I0127 08:43:09.359322 140005288683264 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.212491512298584, loss=2.715188503265381
I0127 08:43:42.963344 140004667934464 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.611785888671875, loss=2.630171537399292
I0127 08:44:16.572706 140005288683264 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.4298665523529053, loss=2.615415096282959
I0127 08:44:50.194326 140004667934464 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.3000073432922363, loss=2.584329128265381
I0127 08:45:23.813448 140005288683264 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.367940902709961, loss=2.6483302116394043
I0127 08:45:57.361607 140004667934464 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.135554790496826, loss=2.4568467140197754
I0127 08:46:31.005405 140005288683264 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.1693549156188965, loss=2.5322089195251465
I0127 08:47:04.636080 140004667934464 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.483062267303467, loss=2.600315570831299
I0127 08:47:38.240468 140005288683264 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.4314866065979004, loss=2.7064788341522217
I0127 08:48:11.781724 140004667934464 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.2084619998931885, loss=2.629580020904541
I0127 08:48:45.310104 140005288683264 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.118712902069092, loss=2.642705202102661
I0127 08:49:18.891843 140004667934464 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.9287281036376953, loss=2.5458452701568604
I0127 08:49:52.521029 140005288683264 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.216615915298462, loss=2.587721586227417
I0127 08:50:26.092254 140004667934464 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.1711151599884033, loss=2.62496018409729
I0127 08:50:59.730259 140005288683264 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.4321160316467285, loss=2.674159288406372
I0127 08:51:15.677497 140169137129280 spec.py:321] Evaluating on the training split.
I0127 08:51:22.097014 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 08:51:30.716083 140169137129280 spec.py:349] Evaluating on the test split.
I0127 08:51:33.011814 140169137129280 submission_runner.py:408] Time since start: 26108.48s, 	Step: 74349, 	{'train/accuracy': 0.6959502696990967, 'train/loss': 1.3203473091125488, 'validation/accuracy': 0.6404399871826172, 'validation/loss': 1.5740586519241333, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.2236592769622803, 'test/num_examples': 10000, 'score': 25063.78478884697, 'total_duration': 26108.48323750496, 'accumulated_submission_time': 25063.78478884697, 'accumulated_eval_time': 1040.6843490600586, 'accumulated_logging_time': 1.7180449962615967}
I0127 08:51:33.043807 140004676327168 logging_writer.py:48] [74349] accumulated_eval_time=1040.684349, accumulated_logging_time=1.718045, accumulated_submission_time=25063.784789, global_step=74349, preemption_count=0, score=25063.784789, test/accuracy=0.515400, test/loss=2.223659, test/num_examples=10000, total_duration=26108.483238, train/accuracy=0.695950, train/loss=1.320347, validation/accuracy=0.640440, validation/loss=1.574059, validation/num_examples=50000
I0127 08:51:50.470903 140005305468672 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.9054553508758545, loss=2.5973057746887207
I0127 08:52:23.991021 140004676327168 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.2745065689086914, loss=2.6687631607055664
I0127 08:52:57.630061 140005305468672 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.54518985748291, loss=2.657782554626465
I0127 08:53:31.236181 140004676327168 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.46917986869812, loss=2.6096303462982178
I0127 08:54:04.848929 140005305468672 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.5463056564331055, loss=2.5782670974731445
I0127 08:54:38.420395 140004676327168 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.5627477169036865, loss=2.680107831954956
I0127 08:55:11.957572 140005305468672 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.548886775970459, loss=2.594914197921753
I0127 08:55:45.481760 140004676327168 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.4997341632843018, loss=2.626194477081299
I0127 08:56:19.050852 140005305468672 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.013261079788208, loss=2.5664942264556885
I0127 08:56:52.687669 140004676327168 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.4282851219177246, loss=2.6442105770111084
I0127 08:57:26.307837 140005305468672 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.1240859031677246, loss=2.5427441596984863
I0127 08:57:59.947174 140004676327168 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.659872531890869, loss=2.634669065475464
I0127 08:58:33.623691 140005305468672 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.3971574306488037, loss=2.589353322982788
I0127 08:59:07.197550 140004676327168 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.368743896484375, loss=2.6651182174682617
I0127 08:59:40.808914 140005305468672 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.1272644996643066, loss=2.5804219245910645
I0127 09:00:03.146886 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:00:10.205281 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:00:18.803751 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:00:21.106802 140169137129280 submission_runner.py:408] Time since start: 26636.58s, 	Step: 75868, 	{'train/accuracy': 0.6812818646430969, 'train/loss': 1.3611277341842651, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.6080148220062256, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.291550636291504, 'test/num_examples': 10000, 'score': 25573.829756498337, 'total_duration': 26636.57822918892, 'accumulated_submission_time': 25573.829756498337, 'accumulated_eval_time': 1058.6442294120789, 'accumulated_logging_time': 1.7606401443481445}
I0127 09:00:21.139173 140004659541760 logging_writer.py:48] [75868] accumulated_eval_time=1058.644229, accumulated_logging_time=1.760640, accumulated_submission_time=25573.829756, global_step=75868, preemption_count=0, score=25573.829756, test/accuracy=0.504300, test/loss=2.291551, test/num_examples=10000, total_duration=26636.578229, train/accuracy=0.681282, train/loss=1.361128, validation/accuracy=0.627960, validation/loss=1.608015, validation/num_examples=50000
I0127 09:00:32.229150 140004667934464 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.235647678375244, loss=2.58660888671875
I0127 09:01:05.791299 140004659541760 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.2451202869415283, loss=2.5476255416870117
I0127 09:01:39.370246 140004667934464 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.5382766723632812, loss=2.6801373958587646
I0127 09:02:13.011460 140004659541760 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.608812093734741, loss=2.7032840251922607
I0127 09:02:46.629068 140004667934464 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.1476292610168457, loss=2.602790355682373
I0127 09:03:20.242454 140004659541760 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.64370059967041, loss=2.562685012817383
I0127 09:03:53.846370 140004667934464 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.6642661094665527, loss=2.7159011363983154
I0127 09:04:27.459178 140004659541760 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.832305908203125, loss=2.615339994430542
I0127 09:05:01.097409 140004667934464 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.952404260635376, loss=2.6281418800354004
I0127 09:05:34.700361 140004659541760 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.836686134338379, loss=2.5852982997894287
I0127 09:06:08.291634 140004667934464 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.6754226684570312, loss=2.7143044471740723
I0127 09:06:41.929526 140004659541760 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.287566661834717, loss=2.6117584705352783
I0127 09:07:15.556196 140004667934464 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.7108314037323, loss=2.601353645324707
I0127 09:07:49.178381 140004659541760 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.1075847148895264, loss=2.6219592094421387
I0127 09:08:22.725535 140004667934464 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.3704614639282227, loss=2.665635108947754
I0127 09:08:51.386032 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:08:57.712138 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:09:06.178018 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:09:08.511631 140169137129280 submission_runner.py:408] Time since start: 27163.98s, 	Step: 77387, 	{'train/accuracy': 0.6804049611091614, 'train/loss': 1.39091157913208, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.6200053691864014, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2878735065460205, 'test/num_examples': 10000, 'score': 26084.01960873604, 'total_duration': 27163.98302912712, 'accumulated_submission_time': 26084.01960873604, 'accumulated_eval_time': 1075.7697627544403, 'accumulated_logging_time': 1.8028457164764404}
I0127 09:09:08.543946 140004659541760 logging_writer.py:48] [77387] accumulated_eval_time=1075.769763, accumulated_logging_time=1.802846, accumulated_submission_time=26084.019609, global_step=77387, preemption_count=0, score=26084.019609, test/accuracy=0.511200, test/loss=2.287874, test/num_examples=10000, total_duration=27163.983029, train/accuracy=0.680405, train/loss=1.390912, validation/accuracy=0.629880, validation/loss=1.620005, validation/num_examples=50000
I0127 09:09:14.172559 140005305468672 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.1851446628570557, loss=2.6036832332611084
I0127 09:09:47.720044 140004659541760 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.565204381942749, loss=2.6928296089172363
I0127 09:10:21.237420 140005305468672 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.2292792797088623, loss=2.600222110748291
I0127 09:10:54.824068 140004659541760 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.2894930839538574, loss=2.6426515579223633
I0127 09:11:28.377996 140005305468672 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.212653398513794, loss=2.6486034393310547
I0127 09:12:01.962405 140004659541760 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.538686752319336, loss=2.665731430053711
I0127 09:12:35.585740 140005305468672 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.147334337234497, loss=2.6006643772125244
I0127 09:13:09.210160 140004659541760 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.297023057937622, loss=2.593301296234131
I0127 09:13:42.839860 140005305468672 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.783874273300171, loss=2.5320191383361816
I0127 09:14:16.453190 140004659541760 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.481138229370117, loss=2.641716718673706
I0127 09:14:50.040108 140005305468672 logging_writer.py:48] [78400] global_step=78400, grad_norm=4.127418041229248, loss=2.5756468772888184
I0127 09:15:23.666669 140004659541760 logging_writer.py:48] [78500] global_step=78500, grad_norm=4.062807559967041, loss=2.625786781311035
I0127 09:15:57.265246 140005305468672 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.578366756439209, loss=2.5793800354003906
I0127 09:16:30.886222 140004659541760 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.6299779415130615, loss=2.6658875942230225
I0127 09:17:04.539593 140005305468672 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.355229139328003, loss=2.651395797729492
I0127 09:17:38.118199 140004659541760 logging_writer.py:48] [78900] global_step=78900, grad_norm=4.126092433929443, loss=2.600346565246582
I0127 09:17:38.601785 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:17:44.972649 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:17:53.572508 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:17:55.898488 140169137129280 submission_runner.py:408] Time since start: 27691.37s, 	Step: 78903, 	{'train/accuracy': 0.7225167155265808, 'train/loss': 1.2051185369491577, 'validation/accuracy': 0.6467399597167969, 'validation/loss': 1.5430620908737183, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.2047617435455322, 'test/num_examples': 10000, 'score': 26593.105378627777, 'total_duration': 27691.369906663895, 'accumulated_submission_time': 26593.105378627777, 'accumulated_eval_time': 1093.0664336681366, 'accumulated_logging_time': 2.7603235244750977}
I0127 09:17:55.929395 140004659541760 logging_writer.py:48] [78903] accumulated_eval_time=1093.066434, accumulated_logging_time=2.760324, accumulated_submission_time=26593.105379, global_step=78903, preemption_count=0, score=26593.105379, test/accuracy=0.519500, test/loss=2.204762, test/num_examples=10000, total_duration=27691.369907, train/accuracy=0.722517, train/loss=1.205119, validation/accuracy=0.646740, validation/loss=1.543062, validation/num_examples=50000
I0127 09:18:28.784935 140004676327168 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.457871437072754, loss=2.643526554107666
I0127 09:19:02.302013 140004659541760 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.7602481842041016, loss=2.5946598052978516
I0127 09:19:35.832296 140004676327168 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.292492151260376, loss=2.671077013015747
I0127 09:20:09.408968 140004659541760 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.4654288291931152, loss=2.615161895751953
I0127 09:20:43.036396 140004676327168 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.267991781234741, loss=2.6648917198181152
I0127 09:21:16.663630 140004659541760 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.6579997539520264, loss=2.7110133171081543
I0127 09:21:50.285296 140004676327168 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.6300642490386963, loss=2.617703914642334
I0127 09:22:23.898777 140004659541760 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.4291069507598877, loss=2.4778952598571777
I0127 09:22:57.486050 140004676327168 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.2435243129730225, loss=2.64579701423645
I0127 09:23:31.068493 140004659541760 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.1837100982666016, loss=2.4867117404937744
I0127 09:24:04.606876 140004676327168 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.410935401916504, loss=2.670581817626953
I0127 09:24:38.128363 140004659541760 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.6431210041046143, loss=2.624617338180542
I0127 09:25:11.666067 140004676327168 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.6993377208709717, loss=2.650601863861084
I0127 09:25:45.256787 140004659541760 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.25209641456604, loss=2.590604305267334
I0127 09:26:18.864798 140004676327168 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.5286762714385986, loss=2.5990774631500244
I0127 09:26:26.063294 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:26:32.407718 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:26:40.937299 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:26:43.305112 140169137129280 submission_runner.py:408] Time since start: 28218.78s, 	Step: 80423, 	{'train/accuracy': 0.6974050998687744, 'train/loss': 1.309516429901123, 'validation/accuracy': 0.6312400102615356, 'validation/loss': 1.6141302585601807, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.290923833847046, 'test/num_examples': 10000, 'score': 27103.18165063858, 'total_duration': 28218.77651834488, 'accumulated_submission_time': 27103.18165063858, 'accumulated_eval_time': 1110.3081967830658, 'accumulated_logging_time': 2.8014683723449707}
I0127 09:26:43.335794 140004667934464 logging_writer.py:48] [80423] accumulated_eval_time=1110.308197, accumulated_logging_time=2.801468, accumulated_submission_time=27103.181651, global_step=80423, preemption_count=0, score=27103.181651, test/accuracy=0.508800, test/loss=2.290924, test/num_examples=10000, total_duration=28218.776518, train/accuracy=0.697405, train/loss=1.309516, validation/accuracy=0.631240, validation/loss=1.614130, validation/num_examples=50000
I0127 09:27:09.497092 140004676327168 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.7710442543029785, loss=2.7173690795898438
I0127 09:27:43.008386 140004667934464 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.2062580585479736, loss=2.589958906173706
I0127 09:28:16.541553 140004676327168 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.4155352115631104, loss=2.5923690795898438
I0127 09:28:50.098107 140004667934464 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.142360210418701, loss=2.6919283866882324
I0127 09:29:23.724540 140004676327168 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.6819539070129395, loss=2.614753007888794
I0127 09:29:57.279137 140004667934464 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.490891218185425, loss=2.5095224380493164
I0127 09:30:30.891325 140004676327168 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.1166770458221436, loss=2.644789934158325
I0127 09:31:04.503624 140004667934464 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.796116352081299, loss=2.543879508972168
I0127 09:31:38.090203 140004676327168 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.297117233276367, loss=2.6459474563598633
I0127 09:32:11.713956 140004667934464 logging_writer.py:48] [81400] global_step=81400, grad_norm=4.4108967781066895, loss=2.572636365890503
I0127 09:32:45.268721 140004676327168 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.192225933074951, loss=2.5624945163726807
I0127 09:33:18.787206 140004667934464 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.3580613136291504, loss=2.5212655067443848
I0127 09:33:52.314184 140004676327168 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.571458101272583, loss=2.536201238632202
I0127 09:34:25.863808 140004667934464 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.719464063644409, loss=2.6621949672698975
I0127 09:34:59.473640 140004676327168 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.5739805698394775, loss=2.5659754276275635
I0127 09:35:13.431148 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:35:19.940394 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:35:28.445477 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:35:30.751158 140169137129280 submission_runner.py:408] Time since start: 28746.22s, 	Step: 81943, 	{'train/accuracy': 0.7071707248687744, 'train/loss': 1.2736690044403076, 'validation/accuracy': 0.6467799544334412, 'validation/loss': 1.539712905883789, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.181778907775879, 'test/num_examples': 10000, 'score': 27613.21877503395, 'total_duration': 28746.222467899323, 'accumulated_submission_time': 27613.21877503395, 'accumulated_eval_time': 1127.628051996231, 'accumulated_logging_time': 2.843165397644043}
I0127 09:35:30.786403 140005288683264 logging_writer.py:48] [81943] accumulated_eval_time=1127.628052, accumulated_logging_time=2.843165, accumulated_submission_time=27613.218775, global_step=81943, preemption_count=0, score=27613.218775, test/accuracy=0.527900, test/loss=2.181779, test/num_examples=10000, total_duration=28746.222468, train/accuracy=0.707171, train/loss=1.273669, validation/accuracy=0.646780, validation/loss=1.539713, validation/num_examples=50000
I0127 09:35:50.231872 140005297075968 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.3400497436523438, loss=2.6081960201263428
I0127 09:36:23.773956 140005288683264 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.726148843765259, loss=2.555995225906372
I0127 09:36:57.389231 140005297075968 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.5466272830963135, loss=2.567012310028076
I0127 09:37:30.999039 140005288683264 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.205110549926758, loss=2.65014386177063
I0127 09:38:04.610187 140005297075968 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.6934776306152344, loss=2.7137742042541504
I0127 09:38:38.203703 140005288683264 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.3487069606781006, loss=2.579861640930176
I0127 09:39:11.837783 140005297075968 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.5584192276000977, loss=2.7057344913482666
I0127 09:39:45.463127 140005288683264 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.7914509773254395, loss=2.636791229248047
I0127 09:40:19.070204 140005297075968 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.855370044708252, loss=2.620821475982666
I0127 09:40:52.653549 140005288683264 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.858497381210327, loss=2.526285171508789
I0127 09:41:26.323336 140005297075968 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.214519023895264, loss=2.559741973876953
I0127 09:41:59.926380 140005288683264 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.5471174716949463, loss=2.5195295810699463
I0127 09:42:33.525457 140005297075968 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.740666627883911, loss=2.6503801345825195
I0127 09:43:07.026974 140005288683264 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.4109013080596924, loss=2.5651650428771973
I0127 09:43:40.546058 140005297075968 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.2842795848846436, loss=2.6155147552490234
I0127 09:44:00.854490 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:44:07.223143 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:44:15.836167 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:44:18.105988 140169137129280 submission_runner.py:408] Time since start: 29273.58s, 	Step: 83462, 	{'train/accuracy': 0.7016302347183228, 'train/loss': 1.2925941944122314, 'validation/accuracy': 0.6476199626922607, 'validation/loss': 1.5447707176208496, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.181201457977295, 'test/num_examples': 10000, 'score': 28123.213754415512, 'total_duration': 29273.577413082123, 'accumulated_submission_time': 28123.213754415512, 'accumulated_eval_time': 1144.8795185089111, 'accumulated_logging_time': 2.9040091037750244}
I0127 09:44:18.137801 140004676327168 logging_writer.py:48] [83462] accumulated_eval_time=1144.879519, accumulated_logging_time=2.904009, accumulated_submission_time=28123.213754, global_step=83462, preemption_count=0, score=28123.213754, test/accuracy=0.525400, test/loss=2.181201, test/num_examples=10000, total_duration=29273.577413, train/accuracy=0.701630, train/loss=1.292594, validation/accuracy=0.647620, validation/loss=1.544771, validation/num_examples=50000
I0127 09:44:31.250042 140005288683264 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.362283229827881, loss=2.601799249649048
I0127 09:45:04.780628 140004676327168 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.952087163925171, loss=2.640228033065796
I0127 09:45:38.275553 140005288683264 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.651529550552368, loss=2.5424275398254395
I0127 09:46:11.852766 140004676327168 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.077260971069336, loss=2.667797565460205
I0127 09:46:45.462343 140005288683264 logging_writer.py:48] [83900] global_step=83900, grad_norm=3.970811367034912, loss=2.6246490478515625
I0127 09:47:19.044714 140004676327168 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.6266489028930664, loss=2.544818878173828
I0127 09:47:52.675554 140005288683264 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.388821601867676, loss=2.5562422275543213
I0127 09:48:26.297219 140004676327168 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.893442392349243, loss=2.6098473072052
I0127 09:48:59.918661 140005288683264 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.5883491039276123, loss=2.554567337036133
I0127 09:49:33.480343 140004676327168 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.3756375312805176, loss=2.619127035140991
I0127 09:50:07.057675 140005288683264 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.4054667949676514, loss=2.6030707359313965
I0127 09:50:40.610315 140004676327168 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.7248342037200928, loss=2.6060240268707275
I0127 09:51:14.129765 140005288683264 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.090623617172241, loss=2.5825631618499756
I0127 09:51:47.660795 140004676327168 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.285703182220459, loss=2.6589436531066895
I0127 09:52:21.246395 140005288683264 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.4563181400299072, loss=2.5680289268493652
I0127 09:52:48.265079 140169137129280 spec.py:321] Evaluating on the training split.
I0127 09:52:54.514264 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 09:53:03.180400 140169137129280 spec.py:349] Evaluating on the test split.
I0127 09:53:05.464868 140169137129280 submission_runner.py:408] Time since start: 29800.94s, 	Step: 84982, 	{'train/accuracy': 0.6917649507522583, 'train/loss': 1.3234705924987793, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.566019058227539, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.230949878692627, 'test/num_examples': 10000, 'score': 28633.28370141983, 'total_duration': 29800.936242103577, 'accumulated_submission_time': 28633.28370141983, 'accumulated_eval_time': 1162.0792186260223, 'accumulated_logging_time': 2.9459140300750732}
I0127 09:53:05.495903 140005288683264 logging_writer.py:48] [84982] accumulated_eval_time=1162.079219, accumulated_logging_time=2.945914, accumulated_submission_time=28633.283701, global_step=84982, preemption_count=0, score=28633.283701, test/accuracy=0.520100, test/loss=2.230950, test/num_examples=10000, total_duration=29800.936242, train/accuracy=0.691765, train/loss=1.323471, validation/accuracy=0.639860, validation/loss=1.566019, validation/num_examples=50000
I0127 09:53:11.860706 140005297075968 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.9239206314086914, loss=2.6314878463745117
I0127 09:53:45.467119 140005288683264 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.712549924850464, loss=2.5421509742736816
I0127 09:54:19.063968 140005297075968 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.5931646823883057, loss=2.568535566329956
I0127 09:54:52.693854 140005288683264 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.2430458068847656, loss=2.546281337738037
I0127 09:55:26.333071 140005297075968 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.392268657684326, loss=2.576796770095825
I0127 09:55:59.938490 140005288683264 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.4934167861938477, loss=2.6415748596191406
I0127 09:56:33.571690 140005297075968 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.5675208568573, loss=2.5343689918518066
I0127 09:57:07.173468 140005288683264 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.89951491355896, loss=2.606766700744629
I0127 09:57:40.705965 140005297075968 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.723112106323242, loss=2.557929515838623
I0127 09:58:14.254171 140005288683264 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.4457006454467773, loss=2.547901153564453
I0127 09:58:47.847321 140005297075968 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.3022778034210205, loss=2.537196636199951
I0127 09:59:21.499596 140005288683264 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.4258828163146973, loss=2.5359530448913574
I0127 09:59:55.129124 140005297075968 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.967221260070801, loss=2.5530290603637695
I0127 10:00:28.715114 140005288683264 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.707298994064331, loss=2.5951790809631348
I0127 10:01:02.341015 140005297075968 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.446810722351074, loss=2.547724485397339
I0127 10:01:35.924181 140005288683264 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.314533233642578, loss=2.6982152462005615
I0127 10:01:35.931061 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:01:42.165300 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:01:50.712698 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:01:53.025332 140169137129280 submission_runner.py:408] Time since start: 30328.50s, 	Step: 86501, 	{'train/accuracy': 0.7071906924247742, 'train/loss': 1.2525839805603027, 'validation/accuracy': 0.6568399667739868, 'validation/loss': 1.4887808561325073, 'validation/num_examples': 50000, 'test/accuracy': 0.5342000126838684, 'test/loss': 2.135103940963745, 'test/num_examples': 10000, 'score': 29143.66135954857, 'total_duration': 30328.49675798416, 'accumulated_submission_time': 29143.66135954857, 'accumulated_eval_time': 1179.1734466552734, 'accumulated_logging_time': 2.9869160652160645}
I0127 10:01:53.058870 140005313861376 logging_writer.py:48] [86501] accumulated_eval_time=1179.173447, accumulated_logging_time=2.986916, accumulated_submission_time=29143.661360, global_step=86501, preemption_count=0, score=29143.661360, test/accuracy=0.534200, test/loss=2.135104, test/num_examples=10000, total_duration=30328.496758, train/accuracy=0.707191, train/loss=1.252584, validation/accuracy=0.656840, validation/loss=1.488781, validation/num_examples=50000
I0127 10:02:26.653818 140005322254080 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.9230639934539795, loss=2.5504045486450195
I0127 10:03:00.151957 140005313861376 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.8352012634277344, loss=2.5528512001037598
I0127 10:03:33.686898 140005322254080 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.012415885925293, loss=2.6378426551818848
I0127 10:04:07.297575 140005313861376 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.5095791816711426, loss=2.559549570083618
I0127 10:04:40.909038 140005322254080 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.026113271713257, loss=2.638667106628418
I0127 10:05:14.498133 140005313861376 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.6205670833587646, loss=2.5652689933776855
I0127 10:05:48.155642 140005322254080 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.8127171993255615, loss=2.570566415786743
I0127 10:06:21.766981 140005313861376 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.803419589996338, loss=2.636728286743164
I0127 10:06:55.366635 140005322254080 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.2588045597076416, loss=2.608520030975342
I0127 10:07:28.970113 140005313861376 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.618086099624634, loss=2.601682662963867
I0127 10:08:02.593322 140005322254080 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.6888134479522705, loss=2.607814073562622
I0127 10:08:36.191997 140005313861376 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.4586355686187744, loss=2.5182337760925293
I0127 10:09:09.811720 140005322254080 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.463711977005005, loss=2.519451379776001
I0127 10:09:43.446655 140005313861376 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.9119510650634766, loss=2.615251302719116
I0127 10:10:17.049283 140005322254080 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.6703999042510986, loss=2.5432872772216797
I0127 10:10:23.244940 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:10:29.426342 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:10:38.242667 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:10:40.521412 140169137129280 submission_runner.py:408] Time since start: 30855.99s, 	Step: 88020, 	{'train/accuracy': 0.7352319955825806, 'train/loss': 1.1445754766464233, 'validation/accuracy': 0.649459958076477, 'validation/loss': 1.5333545207977295, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.18676495552063, 'test/num_examples': 10000, 'score': 29653.78978037834, 'total_duration': 30855.992782831192, 'accumulated_submission_time': 29653.78978037834, 'accumulated_eval_time': 1196.4498274326324, 'accumulated_logging_time': 3.030723810195923}
I0127 10:10:40.554568 140004659541760 logging_writer.py:48] [88020] accumulated_eval_time=1196.449827, accumulated_logging_time=3.030724, accumulated_submission_time=29653.789780, global_step=88020, preemption_count=0, score=29653.789780, test/accuracy=0.527000, test/loss=2.186765, test/num_examples=10000, total_duration=30855.992783, train/accuracy=0.735232, train/loss=1.144575, validation/accuracy=0.649460, validation/loss=1.533355, validation/num_examples=50000
I0127 10:11:07.691082 140004667934464 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.7940971851348877, loss=2.483370542526245
I0127 10:11:41.196745 140004659541760 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.6314852237701416, loss=2.5992519855499268
I0127 10:12:14.802826 140004667934464 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.3212382793426514, loss=2.6088061332702637
I0127 10:12:48.336823 140004659541760 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.1303277015686035, loss=2.4916114807128906
I0127 10:13:21.865836 140004667934464 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.5610318183898926, loss=2.5623538494110107
I0127 10:13:55.409116 140004659541760 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.5074334144592285, loss=2.5458309650421143
I0127 10:14:28.956796 140004667934464 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.9864017963409424, loss=2.598198652267456
I0127 10:15:02.544839 140004659541760 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.8212051391601562, loss=2.4216041564941406
I0127 10:15:36.097025 140004667934464 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.806791305541992, loss=2.498828887939453
I0127 10:16:09.618801 140004659541760 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.108177661895752, loss=2.5538363456726074
I0127 10:16:43.165333 140004667934464 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.058083534240723, loss=2.5606508255004883
I0127 10:17:16.754312 140004659541760 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.192866325378418, loss=2.545319080352783
I0127 10:17:50.354155 140004667934464 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.261751651763916, loss=2.497220039367676
I0127 10:18:23.985354 140004659541760 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.4976766109466553, loss=2.4546573162078857
I0127 10:18:57.597405 140004667934464 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.2974555492401123, loss=2.416140556335449
I0127 10:19:10.852895 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:19:17.157787 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:19:25.853055 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:19:28.221434 140169137129280 submission_runner.py:408] Time since start: 31383.69s, 	Step: 89541, 	{'train/accuracy': 0.7146444320678711, 'train/loss': 1.2600167989730835, 'validation/accuracy': 0.6505399942398071, 'validation/loss': 1.539999008178711, 'validation/num_examples': 50000, 'test/accuracy': 0.523900032043457, 'test/loss': 2.197510004043579, 'test/num_examples': 10000, 'score': 30164.02828192711, 'total_duration': 31383.692828655243, 'accumulated_submission_time': 30164.02828192711, 'accumulated_eval_time': 1213.8182995319366, 'accumulated_logging_time': 3.0758121013641357}
I0127 10:19:28.257040 140005297075968 logging_writer.py:48] [89541] accumulated_eval_time=1213.818300, accumulated_logging_time=3.075812, accumulated_submission_time=30164.028282, global_step=89541, preemption_count=0, score=30164.028282, test/accuracy=0.523900, test/loss=2.197510, test/num_examples=10000, total_duration=31383.692829, train/accuracy=0.714644, train/loss=1.260017, validation/accuracy=0.650540, validation/loss=1.539999, validation/num_examples=50000
I0127 10:19:48.363145 140005305468672 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.740280866622925, loss=2.6342673301696777
I0127 10:20:21.914234 140005297075968 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.4550180435180664, loss=2.648139476776123
I0127 10:20:55.440111 140005305468672 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.7787985801696777, loss=2.6184682846069336
I0127 10:21:28.944526 140005297075968 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.4262900352478027, loss=2.519401788711548
I0127 10:22:02.564710 140005305468672 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.6497445106506348, loss=2.535442352294922
I0127 10:22:36.172588 140005297075968 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.598264694213867, loss=2.557386875152588
I0127 10:23:09.803874 140005305468672 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.722896099090576, loss=2.536503314971924
I0127 10:23:43.424619 140005297075968 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.5244219303131104, loss=2.615269660949707
I0127 10:24:17.113307 140005305468672 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.678513765335083, loss=2.6045329570770264
I0127 10:24:50.627269 140005297075968 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.867628335952759, loss=2.6233088970184326
I0127 10:25:24.203815 140005305468672 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.063204288482666, loss=2.5456833839416504
I0127 10:25:57.784351 140005297075968 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.346883773803711, loss=2.4422335624694824
I0127 10:26:31.376441 140005305468672 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.6471877098083496, loss=2.5496296882629395
I0127 10:27:05.001524 140005297075968 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.9885659217834473, loss=2.4950931072235107
I0127 10:27:38.535986 140005305468672 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.949120283126831, loss=2.554893970489502
I0127 10:27:58.451687 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:28:04.762516 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:28:13.358294 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:28:15.913349 140169137129280 submission_runner.py:408] Time since start: 31911.38s, 	Step: 91061, 	{'train/accuracy': 0.7093231678009033, 'train/loss': 1.2750592231750488, 'validation/accuracy': 0.6477000117301941, 'validation/loss': 1.54591703414917, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.186326503753662, 'test/num_examples': 10000, 'score': 30674.164899349213, 'total_duration': 31911.384786367416, 'accumulated_submission_time': 30674.164899349213, 'accumulated_eval_time': 1231.2799394130707, 'accumulated_logging_time': 3.1218960285186768}
I0127 10:28:15.943235 140005288683264 logging_writer.py:48] [91061] accumulated_eval_time=1231.279939, accumulated_logging_time=3.121896, accumulated_submission_time=30674.164899, global_step=91061, preemption_count=0, score=30674.164899, test/accuracy=0.530800, test/loss=2.186327, test/num_examples=10000, total_duration=31911.384786, train/accuracy=0.709323, train/loss=1.275059, validation/accuracy=0.647700, validation/loss=1.545917, validation/num_examples=50000
I0127 10:28:29.362344 140005297075968 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.5516610145568848, loss=2.5791192054748535
I0127 10:29:02.845993 140005288683264 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.4917781352996826, loss=2.639110803604126
I0127 10:29:36.410685 140005297075968 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.704502582550049, loss=2.540405035018921
I0127 10:30:10.042825 140005288683264 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.744286060333252, loss=2.5004727840423584
I0127 10:30:43.622038 140005297075968 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.753117084503174, loss=2.6218838691711426
I0127 10:31:17.235033 140005288683264 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.150304794311523, loss=2.596522092819214
I0127 10:31:50.869853 140005297075968 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.0744540691375732, loss=2.5038442611694336
I0127 10:32:24.488417 140005288683264 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.846503257751465, loss=2.575709104537964
I0127 10:32:58.069626 140005297075968 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.113621234893799, loss=2.585574150085449
I0127 10:33:31.609423 140005288683264 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.0867133140563965, loss=2.562835693359375
I0127 10:34:05.143917 140005297075968 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.540431976318359, loss=2.639235019683838
I0127 10:34:38.670621 140005288683264 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.9052491188049316, loss=2.5812582969665527
I0127 10:35:12.259429 140005297075968 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.9582533836364746, loss=2.50941801071167
I0127 10:35:45.890716 140005288683264 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.3530497550964355, loss=2.545260429382324
I0127 10:36:19.542061 140005297075968 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.851612091064453, loss=2.5513715744018555
I0127 10:36:46.188151 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:36:52.369403 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:37:00.974138 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:37:03.308105 140169137129280 submission_runner.py:408] Time since start: 32438.78s, 	Step: 92581, 	{'train/accuracy': 0.720723032951355, 'train/loss': 1.2018429040908813, 'validation/accuracy': 0.6581999659538269, 'validation/loss': 1.4694814682006836, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.1427531242370605, 'test/num_examples': 10000, 'score': 31184.35217189789, 'total_duration': 32438.77952504158, 'accumulated_submission_time': 31184.35217189789, 'accumulated_eval_time': 1248.3998510837555, 'accumulated_logging_time': 3.161928653717041}
I0127 10:37:03.343921 140004667934464 logging_writer.py:48] [92581] accumulated_eval_time=1248.399851, accumulated_logging_time=3.161929, accumulated_submission_time=31184.352172, global_step=92581, preemption_count=0, score=31184.352172, test/accuracy=0.535100, test/loss=2.142753, test/num_examples=10000, total_duration=32438.779525, train/accuracy=0.720723, train/loss=1.201843, validation/accuracy=0.658200, validation/loss=1.469481, validation/num_examples=50000
I0127 10:37:10.068801 140004676327168 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.519850969314575, loss=2.4776978492736816
I0127 10:37:43.646520 140004667934464 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.8768832683563232, loss=2.5395662784576416
I0127 10:38:17.265673 140004676327168 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.9824411869049072, loss=2.5445938110351562
I0127 10:38:50.904603 140004667934464 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.7533071041107178, loss=2.515259265899658
I0127 10:39:24.519791 140004676327168 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.4849395751953125, loss=2.5001399517059326
I0127 10:39:58.162569 140004667934464 logging_writer.py:48] [93100] global_step=93100, grad_norm=3.601768970489502, loss=2.5443084239959717
I0127 10:40:31.765902 140004676327168 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.353090286254883, loss=2.4698309898376465
I0127 10:41:05.375153 140004667934464 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.112112522125244, loss=2.5526366233825684
I0127 10:41:38.992634 140004676327168 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.688838005065918, loss=2.4466075897216797
I0127 10:42:12.680972 140004667934464 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.01297664642334, loss=2.5248730182647705
I0127 10:42:46.216845 140004676327168 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.384864091873169, loss=2.51511287689209
I0127 10:43:19.761992 140004667934464 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.8899848461151123, loss=2.5925111770629883
I0127 10:43:53.338110 140004676327168 logging_writer.py:48] [93800] global_step=93800, grad_norm=3.6468043327331543, loss=2.532473087310791
I0127 10:44:26.835885 140004667934464 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.8679723739624023, loss=2.4562907218933105
I0127 10:45:00.401080 140004676327168 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.7344276905059814, loss=2.5401904582977295
I0127 10:45:33.464265 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:45:39.735482 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:45:48.506086 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:45:50.876352 140169137129280 submission_runner.py:408] Time since start: 32966.35s, 	Step: 94100, 	{'train/accuracy': 0.711355984210968, 'train/loss': 1.227055311203003, 'validation/accuracy': 0.6619600057601929, 'validation/loss': 1.455871343612671, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.120694875717163, 'test/num_examples': 10000, 'score': 31694.415167331696, 'total_duration': 32966.347737550735, 'accumulated_submission_time': 31694.415167331696, 'accumulated_eval_time': 1265.8118696212769, 'accumulated_logging_time': 3.2076263427734375}
I0127 10:45:50.924278 140004659541760 logging_writer.py:48] [94100] accumulated_eval_time=1265.811870, accumulated_logging_time=3.207626, accumulated_submission_time=31694.415167, global_step=94100, preemption_count=0, score=31694.415167, test/accuracy=0.531400, test/loss=2.120695, test/num_examples=10000, total_duration=32966.347738, train/accuracy=0.711356, train/loss=1.227055, validation/accuracy=0.661960, validation/loss=1.455871, validation/num_examples=50000
I0127 10:45:51.284259 140005288683264 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.994553565979004, loss=2.5817089080810547
I0127 10:46:24.814629 140004659541760 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.066097259521484, loss=2.536212205886841
I0127 10:46:58.324051 140005288683264 logging_writer.py:48] [94300] global_step=94300, grad_norm=3.574708938598633, loss=2.53279447555542
I0127 10:47:31.909511 140004659541760 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.293808937072754, loss=2.5483932495117188
I0127 10:48:05.509056 140005288683264 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.7354466915130615, loss=2.512817621231079
I0127 10:48:39.157683 140004659541760 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.8578619956970215, loss=2.531916618347168
I0127 10:49:12.746696 140005288683264 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.8120505809783936, loss=2.5034730434417725
I0127 10:49:46.322787 140004659541760 logging_writer.py:48] [94800] global_step=94800, grad_norm=3.623983860015869, loss=2.473740577697754
I0127 10:50:19.938628 140005288683264 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.039973258972168, loss=2.6259944438934326
I0127 10:50:53.526872 140004659541760 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.6182150840759277, loss=2.543403148651123
I0127 10:51:27.047430 140005288683264 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.691640853881836, loss=2.5372815132141113
I0127 10:52:00.564542 140004659541760 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.19356107711792, loss=2.5830886363983154
I0127 10:52:34.112429 140005288683264 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.971440553665161, loss=2.4980180263519287
I0127 10:53:07.709913 140004659541760 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.79858136177063, loss=2.5920634269714355
I0127 10:53:41.322289 140005288683264 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.7093889713287354, loss=2.5681211948394775
I0127 10:54:14.947690 140004659541760 logging_writer.py:48] [95600] global_step=95600, grad_norm=3.966707229614258, loss=2.5676915645599365
I0127 10:54:21.150082 140169137129280 spec.py:321] Evaluating on the training split.
I0127 10:54:27.640282 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 10:54:36.357950 140169137129280 spec.py:349] Evaluating on the test split.
I0127 10:54:38.738735 140169137129280 submission_runner.py:408] Time since start: 33494.21s, 	Step: 95620, 	{'train/accuracy': 0.7209023833274841, 'train/loss': 1.197264552116394, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.4429244995117188, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.099045753479004, 'test/num_examples': 10000, 'score': 32204.58331465721, 'total_duration': 33494.21009898186, 'accumulated_submission_time': 32204.58331465721, 'accumulated_eval_time': 1283.4004225730896, 'accumulated_logging_time': 3.2659912109375}
I0127 10:54:38.773213 140004659541760 logging_writer.py:48] [95620] accumulated_eval_time=1283.400423, accumulated_logging_time=3.265991, accumulated_submission_time=32204.583315, global_step=95620, preemption_count=0, score=32204.583315, test/accuracy=0.545800, test/loss=2.099046, test/num_examples=10000, total_duration=33494.210099, train/accuracy=0.720902, train/loss=1.197265, validation/accuracy=0.665800, validation/loss=1.442924, validation/num_examples=50000
I0127 10:55:05.989504 140004667934464 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.919426202774048, loss=2.5683720111846924
I0127 10:55:39.470550 140004659541760 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.792623519897461, loss=2.4451992511749268
I0127 10:56:13.032362 140004667934464 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.343872308731079, loss=2.5786728858947754
I0127 10:56:46.555747 140004659541760 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.193049907684326, loss=2.503885507583618
I0127 10:57:20.080957 140004667934464 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.267880916595459, loss=2.521634101867676
I0127 10:57:53.618339 140004659541760 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.85052490234375, loss=2.5312747955322266
I0127 10:58:27.176150 140004667934464 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.064325332641602, loss=2.555669069290161
I0127 10:59:00.700686 140004659541760 logging_writer.py:48] [96400] global_step=96400, grad_norm=3.838284969329834, loss=2.4203970432281494
I0127 10:59:34.214891 140004667934464 logging_writer.py:48] [96500] global_step=96500, grad_norm=5.353830337524414, loss=2.5615999698638916
I0127 11:00:07.788638 140004659541760 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.7776529788970947, loss=2.548245668411255
I0127 11:00:41.446052 140004667934464 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.9907612800598145, loss=2.4933507442474365
I0127 11:01:15.014233 140004659541760 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.698633909225464, loss=2.4640908241271973
I0127 11:01:48.608522 140004667934464 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.956921100616455, loss=2.5119941234588623
I0127 11:02:22.119154 140004659541760 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.745516777038574, loss=2.5484373569488525
I0127 11:02:55.657046 140004667934464 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.8032262325286865, loss=2.547971487045288
I0127 11:03:08.887042 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:03:15.179685 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:03:23.951922 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:03:26.274263 140169137129280 submission_runner.py:408] Time since start: 34021.75s, 	Step: 97141, 	{'train/accuracy': 0.7374441623687744, 'train/loss': 1.1343486309051514, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.4939663410186768, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.161627769470215, 'test/num_examples': 10000, 'score': 32714.638087511063, 'total_duration': 34021.7456908226, 'accumulated_submission_time': 32714.638087511063, 'accumulated_eval_time': 1300.787608385086, 'accumulated_logging_time': 3.3119215965270996}
I0127 11:03:26.307146 140005305468672 logging_writer.py:48] [97141] accumulated_eval_time=1300.787608, accumulated_logging_time=3.311922, accumulated_submission_time=32714.638088, global_step=97141, preemption_count=0, score=32714.638088, test/accuracy=0.530400, test/loss=2.161628, test/num_examples=10000, total_duration=34021.745691, train/accuracy=0.737444, train/loss=1.134349, validation/accuracy=0.657640, validation/loss=1.493966, validation/num_examples=50000
I0127 11:03:46.418989 140005313861376 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.446448802947998, loss=2.5246012210845947
I0127 11:04:19.930676 140005305468672 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.720823049545288, loss=2.5392351150512695
I0127 11:04:53.472461 140005313861376 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.847668170928955, loss=2.4727847576141357
I0127 11:05:27.006223 140005305468672 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.643191337585449, loss=2.4639580249786377
I0127 11:06:00.549072 140005313861376 logging_writer.py:48] [97600] global_step=97600, grad_norm=4.518396854400635, loss=2.574230432510376
I0127 11:06:34.152260 140005305468672 logging_writer.py:48] [97700] global_step=97700, grad_norm=3.699897050857544, loss=2.521693468093872
I0127 11:07:07.756400 140005313861376 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.6192708015441895, loss=2.509963274002075
I0127 11:07:41.340969 140005305468672 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.6321170330047607, loss=2.4637413024902344
I0127 11:08:14.955876 140005313861376 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.805449962615967, loss=2.464478015899658
I0127 11:08:48.571963 140005305468672 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.7955026626586914, loss=2.465038776397705
I0127 11:09:22.196120 140005313861376 logging_writer.py:48] [98200] global_step=98200, grad_norm=4.027547359466553, loss=2.503171682357788
I0127 11:09:55.763483 140005305468672 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.535037517547607, loss=2.544846773147583
I0127 11:10:29.299980 140005313861376 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.401247024536133, loss=2.474170446395874
I0127 11:11:02.855047 140005305468672 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.055637359619141, loss=2.4907565116882324
I0127 11:11:36.453538 140005313861376 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.4091246128082275, loss=2.46649169921875
I0127 11:11:56.408545 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:12:02.695334 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:12:11.281110 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:12:13.593495 140169137129280 submission_runner.py:408] Time since start: 34549.06s, 	Step: 98661, 	{'train/accuracy': 0.7361487150192261, 'train/loss': 1.1169815063476562, 'validation/accuracy': 0.6668399572372437, 'validation/loss': 1.4288816452026367, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.060675621032715, 'test/num_examples': 10000, 'score': 33224.680389881134, 'total_duration': 34549.06488656998, 'accumulated_submission_time': 33224.680389881134, 'accumulated_eval_time': 1317.972489118576, 'accumulated_logging_time': 3.3565704822540283}
I0127 11:12:13.645890 140004659541760 logging_writer.py:48] [98661] accumulated_eval_time=1317.972489, accumulated_logging_time=3.356570, accumulated_submission_time=33224.680390, global_step=98661, preemption_count=0, score=33224.680390, test/accuracy=0.550800, test/loss=2.060676, test/num_examples=10000, total_duration=34549.064887, train/accuracy=0.736149, train/loss=1.116982, validation/accuracy=0.666840, validation/loss=1.428882, validation/num_examples=50000
I0127 11:12:27.069453 140004667934464 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.654707431793213, loss=2.4164774417877197
I0127 11:13:00.669578 140004659541760 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.736818790435791, loss=2.500222682952881
I0127 11:13:34.244090 140004667934464 logging_writer.py:48] [98900] global_step=98900, grad_norm=3.5701050758361816, loss=2.483798027038574
I0127 11:14:07.856223 140004659541760 logging_writer.py:48] [99000] global_step=99000, grad_norm=4.36841344833374, loss=2.523414134979248
I0127 11:14:41.401595 140004667934464 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.5279884338378906, loss=2.5091114044189453
I0127 11:15:14.943737 140004659541760 logging_writer.py:48] [99200] global_step=99200, grad_norm=3.6652987003326416, loss=2.5330328941345215
I0127 11:15:48.444900 140004667934464 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.733586072921753, loss=2.48577880859375
I0127 11:16:22.018276 140004659541760 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.381075382232666, loss=2.457612991333008
I0127 11:16:55.518041 140004667934464 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.908003091812134, loss=2.6340012550354004
I0127 11:17:29.055718 140004659541760 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.963855743408203, loss=2.5382323265075684
I0127 11:18:02.658915 140004667934464 logging_writer.py:48] [99700] global_step=99700, grad_norm=3.7092676162719727, loss=2.563084840774536
I0127 11:18:36.273284 140004659541760 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.6186070442199707, loss=2.3491129875183105
I0127 11:19:09.905605 140004667934464 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.143678665161133, loss=2.4535443782806396
I0127 11:19:43.469497 140004659541760 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.86971378326416, loss=2.463336229324341
I0127 11:20:17.069009 140004667934464 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.8645362854003906, loss=2.528311252593994
I0127 11:20:43.756052 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:20:49.979531 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:20:58.597546 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:21:00.952006 140169137129280 submission_runner.py:408] Time since start: 35076.42s, 	Step: 100181, 	{'train/accuracy': 0.7201650142669678, 'train/loss': 1.1991384029388428, 'validation/accuracy': 0.6631199717521667, 'validation/loss': 1.4586812257766724, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.1067442893981934, 'test/num_examples': 10000, 'score': 33734.73139023781, 'total_duration': 35076.42337989807, 'accumulated_submission_time': 33734.73139023781, 'accumulated_eval_time': 1335.168357372284, 'accumulated_logging_time': 3.4204201698303223}
I0127 11:21:00.985454 140004659541760 logging_writer.py:48] [100181] accumulated_eval_time=1335.168357, accumulated_logging_time=3.420420, accumulated_submission_time=33734.731390, global_step=100181, preemption_count=0, score=33734.731390, test/accuracy=0.543200, test/loss=2.106744, test/num_examples=10000, total_duration=35076.423380, train/accuracy=0.720165, train/loss=1.199138, validation/accuracy=0.663120, validation/loss=1.458681, validation/num_examples=50000
I0127 11:21:07.713838 140005297075968 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.788855791091919, loss=2.4819273948669434
I0127 11:21:41.237182 140004659541760 logging_writer.py:48] [100300] global_step=100300, grad_norm=3.7328617572784424, loss=2.530867576599121
I0127 11:22:14.788518 140005297075968 logging_writer.py:48] [100400] global_step=100400, grad_norm=3.6258931159973145, loss=2.550722599029541
I0127 11:22:48.375214 140004659541760 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.7913262844085693, loss=2.529801607131958
I0127 11:23:21.966497 140005297075968 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.4614057540893555, loss=2.3911373615264893
I0127 11:23:55.560713 140004659541760 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.105507850646973, loss=2.4886722564697266
I0127 11:24:29.190580 140005297075968 logging_writer.py:48] [100800] global_step=100800, grad_norm=3.6582071781158447, loss=2.440096616744995
I0127 11:25:02.886569 140004659541760 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.105464458465576, loss=2.5200772285461426
I0127 11:25:36.437262 140005297075968 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.127303123474121, loss=2.437654972076416
I0127 11:26:10.062240 140004659541760 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.9032833576202393, loss=2.4522290229797363
I0127 11:26:43.701552 140005297075968 logging_writer.py:48] [101200] global_step=101200, grad_norm=3.9996256828308105, loss=2.3826212882995605
I0127 11:27:17.311681 140004659541760 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.062743186950684, loss=2.4945740699768066
I0127 11:27:50.910454 140005297075968 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.865656614303589, loss=2.4636354446411133
I0127 11:28:24.532756 140004659541760 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.632175922393799, loss=2.499894380569458
I0127 11:28:58.115626 140005297075968 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.224720001220703, loss=2.431770086288452
I0127 11:29:31.190065 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:29:37.427110 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:29:46.116157 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:29:48.418717 140169137129280 submission_runner.py:408] Time since start: 35603.89s, 	Step: 101700, 	{'train/accuracy': 0.7339365482330322, 'train/loss': 1.128197193145752, 'validation/accuracy': 0.6735199689865112, 'validation/loss': 1.4085602760314941, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.062584638595581, 'test/num_examples': 10000, 'score': 34244.87643456459, 'total_duration': 35603.890127658844, 'accumulated_submission_time': 34244.87643456459, 'accumulated_eval_time': 1352.396959066391, 'accumulated_logging_time': 3.4652857780456543}
I0127 11:29:48.463346 140004676327168 logging_writer.py:48] [101700] accumulated_eval_time=1352.396959, accumulated_logging_time=3.465286, accumulated_submission_time=34244.876435, global_step=101700, preemption_count=0, score=34244.876435, test/accuracy=0.547400, test/loss=2.062585, test/num_examples=10000, total_duration=35603.890128, train/accuracy=0.733937, train/loss=1.128197, validation/accuracy=0.673520, validation/loss=1.408560, validation/num_examples=50000
I0127 11:29:48.811697 140005288683264 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.782660722732544, loss=2.4881198406219482
I0127 11:30:22.278947 140004676327168 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.002223968505859, loss=2.490724563598633
I0127 11:30:55.858318 140005288683264 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.201103210449219, loss=2.5749595165252686
I0127 11:31:29.498812 140004676327168 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.091953277587891, loss=2.4783177375793457
I0127 11:32:03.114715 140005288683264 logging_writer.py:48] [102100] global_step=102100, grad_norm=3.9796032905578613, loss=2.5209364891052246
I0127 11:32:36.662491 140004676327168 logging_writer.py:48] [102200] global_step=102200, grad_norm=3.9105660915374756, loss=2.4989078044891357
I0127 11:33:10.180886 140005288683264 logging_writer.py:48] [102300] global_step=102300, grad_norm=3.836653709411621, loss=2.5280745029449463
I0127 11:33:43.713582 140004676327168 logging_writer.py:48] [102400] global_step=102400, grad_norm=3.5385971069335938, loss=2.414731025695801
I0127 11:34:17.280781 140005288683264 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.108100414276123, loss=2.5043394565582275
I0127 11:34:50.902833 140004676327168 logging_writer.py:48] [102600] global_step=102600, grad_norm=3.8271429538726807, loss=2.478466510772705
I0127 11:35:24.519252 140005288683264 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.290072441101074, loss=2.436704635620117
I0127 11:35:58.049971 140004676327168 logging_writer.py:48] [102800] global_step=102800, grad_norm=5.466830730438232, loss=2.5198025703430176
I0127 11:36:31.600473 140005288683264 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.150539398193359, loss=2.4710731506347656
I0127 11:37:05.132471 140004676327168 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.068418025970459, loss=2.563788414001465
I0127 11:37:38.766231 140005288683264 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.0966267585754395, loss=2.5340359210968018
I0127 11:38:12.302835 140004676327168 logging_writer.py:48] [103200] global_step=103200, grad_norm=3.976236581802368, loss=2.516420841217041
I0127 11:38:18.477268 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:38:24.675725 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:38:33.445987 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:38:35.743779 140169137129280 submission_runner.py:408] Time since start: 36131.22s, 	Step: 103220, 	{'train/accuracy': 0.7251275181770325, 'train/loss': 1.1850506067276, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.444494366645813, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.114496946334839, 'test/num_examples': 10000, 'score': 34754.832845926285, 'total_duration': 36131.21515202522, 'accumulated_submission_time': 34754.832845926285, 'accumulated_eval_time': 1369.6633830070496, 'accumulated_logging_time': 3.5199079513549805}
I0127 11:38:35.781435 140004667934464 logging_writer.py:48] [103220] accumulated_eval_time=1369.663383, accumulated_logging_time=3.519908, accumulated_submission_time=34754.832846, global_step=103220, preemption_count=0, score=34754.832846, test/accuracy=0.539900, test/loss=2.114497, test/num_examples=10000, total_duration=36131.215152, train/accuracy=0.725128, train/loss=1.185051, validation/accuracy=0.668180, validation/loss=1.444494, validation/num_examples=50000
I0127 11:39:02.910397 140004676327168 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.759793281555176, loss=2.441399097442627
I0127 11:39:36.459288 140004667934464 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.8012490272521973, loss=2.5022878646850586
I0127 11:40:10.006614 140004676327168 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.8514904975891113, loss=2.390202522277832
I0127 11:40:43.591401 140004667934464 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.720424175262451, loss=2.443230152130127
I0127 11:41:17.110686 140004676327168 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.369379043579102, loss=2.3937342166900635
I0127 11:41:50.636480 140004667934464 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.065406799316406, loss=2.4394209384918213
I0127 11:42:24.238440 140004676327168 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.1959452629089355, loss=2.463068962097168
I0127 11:42:57.847727 140004667934464 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.21433687210083, loss=2.4855856895446777
I0127 11:43:31.504315 140004676327168 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.280576229095459, loss=2.5214548110961914
I0127 11:44:05.114535 140004667934464 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.065220832824707, loss=2.3593902587890625
I0127 11:44:38.738150 140004676327168 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.362821578979492, loss=2.571647882461548
I0127 11:45:12.352408 140004667934464 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.017950534820557, loss=2.5026378631591797
I0127 11:45:45.971972 140004676327168 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.6360318660736084, loss=2.421077251434326
I0127 11:46:19.589272 140004667934464 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.378493785858154, loss=2.4068918228149414
I0127 11:46:53.167811 140004676327168 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.22399377822876, loss=2.434577465057373
I0127 11:47:06.064704 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:47:12.339399 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:47:21.114496 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:47:23.428061 140169137129280 submission_runner.py:408] Time since start: 36658.90s, 	Step: 104740, 	{'train/accuracy': 0.7435427308082581, 'train/loss': 1.0935951471328735, 'validation/accuracy': 0.6726999878883362, 'validation/loss': 1.40591561794281, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.052886724472046, 'test/num_examples': 10000, 'score': 35265.055674791336, 'total_duration': 36658.89939570427, 'accumulated_submission_time': 35265.055674791336, 'accumulated_eval_time': 1387.0266127586365, 'accumulated_logging_time': 3.5705230236053467}
I0127 11:47:23.466304 140005313861376 logging_writer.py:48] [104740] accumulated_eval_time=1387.026613, accumulated_logging_time=3.570523, accumulated_submission_time=35265.055675, global_step=104740, preemption_count=0, score=35265.055675, test/accuracy=0.552200, test/loss=2.052887, test/num_examples=10000, total_duration=36658.899396, train/accuracy=0.743543, train/loss=1.093595, validation/accuracy=0.672700, validation/loss=1.405916, validation/num_examples=50000
I0127 11:47:43.906817 140005322254080 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.295171737670898, loss=2.4158248901367188
I0127 11:48:17.480441 140005313861376 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.133155822753906, loss=2.4341487884521484
I0127 11:48:51.096141 140005322254080 logging_writer.py:48] [105000] global_step=105000, grad_norm=3.900843858718872, loss=2.4244446754455566
I0127 11:49:24.661987 140005313861376 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.348022937774658, loss=2.5737454891204834
I0127 11:49:58.283067 140005322254080 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.164002418518066, loss=2.400850296020508
I0127 11:50:31.783388 140005313861376 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.059875011444092, loss=2.5079727172851562
I0127 11:51:05.292641 140005322254080 logging_writer.py:48] [105400] global_step=105400, grad_norm=3.7000014781951904, loss=2.494323253631592
I0127 11:51:38.898793 140005313861376 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.772980213165283, loss=2.452247142791748
I0127 11:52:12.499237 140005322254080 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.439690589904785, loss=2.4332492351531982
I0127 11:52:46.067553 140005313861376 logging_writer.py:48] [105700] global_step=105700, grad_norm=3.568718433380127, loss=2.492436408996582
I0127 11:53:19.625120 140005322254080 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.109090805053711, loss=2.4159300327301025
I0127 11:53:53.132673 140005313861376 logging_writer.py:48] [105900] global_step=105900, grad_norm=3.7177672386169434, loss=2.431295394897461
I0127 11:54:26.713992 140005322254080 logging_writer.py:48] [106000] global_step=106000, grad_norm=3.9342970848083496, loss=2.427753210067749
I0127 11:55:00.314845 140005313861376 logging_writer.py:48] [106100] global_step=106100, grad_norm=3.8956217765808105, loss=2.5090043544769287
I0127 11:55:33.969908 140005322254080 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.571521759033203, loss=2.4215824604034424
I0127 11:55:53.598024 140169137129280 spec.py:321] Evaluating on the training split.
I0127 11:55:59.855216 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 11:56:08.525150 140169137129280 spec.py:349] Evaluating on the test split.
I0127 11:56:10.842355 140169137129280 submission_runner.py:408] Time since start: 37186.31s, 	Step: 106260, 	{'train/accuracy': 0.7564970850944519, 'train/loss': 1.039839744567871, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.3940984010696411, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.0571823120117188, 'test/num_examples': 10000, 'score': 35775.128962278366, 'total_duration': 37186.31377243996, 'accumulated_submission_time': 35775.128962278366, 'accumulated_eval_time': 1404.2709031105042, 'accumulated_logging_time': 3.6195414066314697}
I0127 11:56:10.880991 140004676327168 logging_writer.py:48] [106260] accumulated_eval_time=1404.270903, accumulated_logging_time=3.619541, accumulated_submission_time=35775.128962, global_step=106260, preemption_count=0, score=35775.128962, test/accuracy=0.551000, test/loss=2.057182, test/num_examples=10000, total_duration=37186.313772, train/accuracy=0.756497, train/loss=1.039840, validation/accuracy=0.677420, validation/loss=1.394098, validation/num_examples=50000
I0127 11:56:24.633464 140005288683264 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.3614888191223145, loss=2.502274513244629
I0127 11:56:58.107862 140004676327168 logging_writer.py:48] [106400] global_step=106400, grad_norm=3.99337100982666, loss=2.4387366771698
I0127 11:57:31.674766 140005288683264 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.056972980499268, loss=2.4777748584747314
I0127 11:58:05.243438 140004676327168 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.005857944488525, loss=2.5105719566345215
I0127 11:58:38.825860 140005288683264 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.035037994384766, loss=2.388533592224121
I0127 11:59:12.339312 140004676327168 logging_writer.py:48] [106800] global_step=106800, grad_norm=3.8931024074554443, loss=2.402561664581299
I0127 11:59:45.898865 140005288683264 logging_writer.py:48] [106900] global_step=106900, grad_norm=4.082906246185303, loss=2.523430585861206
I0127 12:00:19.438422 140004676327168 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.7307560443878174, loss=2.3564460277557373
I0127 12:00:52.946082 140005288683264 logging_writer.py:48] [107100] global_step=107100, grad_norm=3.876687526702881, loss=2.3568506240844727
I0127 12:01:26.488412 140004676327168 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.064325332641602, loss=2.4233362674713135
I0127 12:02:00.108536 140005288683264 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.522266387939453, loss=2.5141398906707764
I0127 12:02:33.635854 140004676327168 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.843006610870361, loss=2.491041898727417
I0127 12:03:07.148165 140005288683264 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.20551061630249, loss=2.4755122661590576
I0127 12:03:40.700853 140004676327168 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.595587730407715, loss=2.5461931228637695
I0127 12:04:14.311179 140005288683264 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.361283302307129, loss=2.4827964305877686
I0127 12:04:40.984723 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:04:47.236481 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:04:55.782033 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:04:58.116424 140169137129280 submission_runner.py:408] Time since start: 37713.59s, 	Step: 107781, 	{'train/accuracy': 0.7531289458274841, 'train/loss': 1.063181757926941, 'validation/accuracy': 0.6829400062561035, 'validation/loss': 1.3601897954940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.0226991176605225, 'test/num_examples': 10000, 'score': 36285.17335796356, 'total_duration': 37713.587841272354, 'accumulated_submission_time': 36285.17335796356, 'accumulated_eval_time': 1421.402559518814, 'accumulated_logging_time': 3.66951847076416}
I0127 12:04:58.152809 140004667934464 logging_writer.py:48] [107781] accumulated_eval_time=1421.402560, accumulated_logging_time=3.669518, accumulated_submission_time=36285.173358, global_step=107781, preemption_count=0, score=36285.173358, test/accuracy=0.561600, test/loss=2.022699, test/num_examples=10000, total_duration=37713.587841, train/accuracy=0.753129, train/loss=1.063182, validation/accuracy=0.682940, validation/loss=1.360190, validation/num_examples=50000
I0127 12:05:04.869522 140004676327168 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.441686153411865, loss=2.549288511276245
I0127 12:05:38.357397 140004667934464 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.56358528137207, loss=2.4845283031463623
I0127 12:06:11.908337 140004676327168 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.252349853515625, loss=2.4817118644714355
I0127 12:06:45.497813 140004667934464 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.347503185272217, loss=2.512277364730835
I0127 12:07:19.043638 140004676327168 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.320312976837158, loss=2.4459943771362305
I0127 12:07:52.666308 140004667934464 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.0123419761657715, loss=2.3788177967071533
I0127 12:08:26.238782 140004676327168 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.072154998779297, loss=2.4736831188201904
I0127 12:08:59.822407 140004667934464 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.080698490142822, loss=2.430241346359253
I0127 12:09:33.356784 140004676327168 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.23840856552124, loss=2.386425495147705
I0127 12:10:06.866039 140004667934464 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.0333027839660645, loss=2.445557117462158
I0127 12:10:40.423037 140004676327168 logging_writer.py:48] [108800] global_step=108800, grad_norm=3.877633571624756, loss=2.461658239364624
I0127 12:11:13.999461 140004667934464 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.352315902709961, loss=2.544323205947876
I0127 12:11:47.607105 140004676327168 logging_writer.py:48] [109000] global_step=109000, grad_norm=3.860762596130371, loss=2.423163414001465
I0127 12:12:21.215006 140004667934464 logging_writer.py:48] [109100] global_step=109100, grad_norm=3.854871988296509, loss=2.4078779220581055
I0127 12:12:54.791303 140004676327168 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.370511054992676, loss=2.4270403385162354
I0127 12:13:28.410567 140004667934464 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.094281196594238, loss=2.4338083267211914
I0127 12:13:28.417140 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:13:34.630679 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:13:43.257767 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:13:45.964840 140169137129280 submission_runner.py:408] Time since start: 38241.44s, 	Step: 109301, 	{'train/accuracy': 0.7407127022743225, 'train/loss': 1.1150754690170288, 'validation/accuracy': 0.6787599921226501, 'validation/loss': 1.402061939239502, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.055934190750122, 'test/num_examples': 10000, 'score': 36795.37970161438, 'total_duration': 38241.43623971939, 'accumulated_submission_time': 36795.37970161438, 'accumulated_eval_time': 1438.9501745700836, 'accumulated_logging_time': 3.7160933017730713}
I0127 12:13:46.002420 140004659541760 logging_writer.py:48] [109301] accumulated_eval_time=1438.950175, accumulated_logging_time=3.716093, accumulated_submission_time=36795.379702, global_step=109301, preemption_count=0, score=36795.379702, test/accuracy=0.555600, test/loss=2.055934, test/num_examples=10000, total_duration=38241.436240, train/accuracy=0.740713, train/loss=1.115075, validation/accuracy=0.678760, validation/loss=1.402062, validation/num_examples=50000
I0127 12:14:19.471923 140005297075968 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.323042392730713, loss=2.3611721992492676
I0127 12:14:53.058748 140004659541760 logging_writer.py:48] [109500] global_step=109500, grad_norm=3.9346489906311035, loss=2.427612543106079
I0127 12:15:26.669259 140005297075968 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.581504821777344, loss=2.42881441116333
I0127 12:16:00.223555 140004659541760 logging_writer.py:48] [109700] global_step=109700, grad_norm=3.8817906379699707, loss=2.4653167724609375
I0127 12:16:33.796329 140005297075968 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.723576068878174, loss=2.3980164527893066
I0127 12:17:07.406260 140004659541760 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.8145623207092285, loss=2.51625394821167
I0127 12:17:41.039339 140005297075968 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.522945404052734, loss=2.5537965297698975
I0127 12:18:14.636645 140004659541760 logging_writer.py:48] [110100] global_step=110100, grad_norm=3.9650790691375732, loss=2.408658027648926
I0127 12:18:48.259207 140005297075968 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.276528835296631, loss=2.3475959300994873
I0127 12:19:21.876699 140004659541760 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.113537311553955, loss=2.5083794593811035
I0127 12:19:55.533826 140005297075968 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.31072473526001, loss=2.3803768157958984
I0127 12:20:29.069752 140004659541760 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.231164932250977, loss=2.373542308807373
I0127 12:21:02.660194 140005297075968 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.37136173248291, loss=2.494192123413086
I0127 12:21:36.273220 140004659541760 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.057434558868408, loss=2.3351991176605225
I0127 12:22:09.899285 140005297075968 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.321992874145508, loss=2.2790753841400146
I0127 12:22:16.093691 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:22:22.294633 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:22:31.108292 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:22:33.433945 140169137129280 submission_runner.py:408] Time since start: 38768.91s, 	Step: 110820, 	{'train/accuracy': 0.746113657951355, 'train/loss': 1.0706056356430054, 'validation/accuracy': 0.685479998588562, 'validation/loss': 1.3558672666549683, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 2.0043318271636963, 'test/num_examples': 10000, 'score': 37305.41358447075, 'total_duration': 38768.90533566475, 'accumulated_submission_time': 37305.41358447075, 'accumulated_eval_time': 1456.2903575897217, 'accumulated_logging_time': 3.7633721828460693}
I0127 12:22:33.471441 140004659541760 logging_writer.py:48] [110820] accumulated_eval_time=1456.290358, accumulated_logging_time=3.763372, accumulated_submission_time=37305.413584, global_step=110820, preemption_count=0, score=37305.413584, test/accuracy=0.563100, test/loss=2.004332, test/num_examples=10000, total_duration=38768.905336, train/accuracy=0.746114, train/loss=1.070606, validation/accuracy=0.685480, validation/loss=1.355867, validation/num_examples=50000
I0127 12:23:00.630326 140004667934464 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.619422912597656, loss=2.4417312145233154
I0127 12:23:34.157692 140004659541760 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.47260046005249, loss=2.4210238456726074
I0127 12:24:07.643541 140004667934464 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.296420574188232, loss=2.3940625190734863
I0127 12:24:41.213167 140004659541760 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.343530654907227, loss=2.3529367446899414
I0127 12:25:14.827143 140004667934464 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.000729084014893, loss=2.4309210777282715
I0127 12:25:48.455206 140004659541760 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.376124382019043, loss=2.4859588146209717
I0127 12:26:22.064902 140004667934464 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.619927406311035, loss=2.431675434112549
I0127 12:26:55.668564 140004659541760 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.072749137878418, loss=2.3994388580322266
I0127 12:27:29.281579 140004667934464 logging_writer.py:48] [111700] global_step=111700, grad_norm=5.282598972320557, loss=2.4588115215301514
I0127 12:28:02.902501 140004659541760 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.350536823272705, loss=2.479013204574585
I0127 12:28:36.525576 140004667934464 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.35286808013916, loss=2.3579964637756348
I0127 12:29:10.114167 140004659541760 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.083568572998047, loss=2.4228174686431885
I0127 12:29:43.622896 140004667934464 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.259002208709717, loss=2.4731898307800293
I0127 12:30:17.165014 140004659541760 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.268332004547119, loss=2.451460838317871
I0127 12:30:50.776912 140004667934464 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.479492664337158, loss=2.3774518966674805
I0127 12:31:03.691356 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:31:09.917081 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:31:18.575409 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:31:20.896945 140169137129280 submission_runner.py:408] Time since start: 39296.37s, 	Step: 112340, 	{'train/accuracy': 0.7463727593421936, 'train/loss': 1.0985363721847534, 'validation/accuracy': 0.6815400123596191, 'validation/loss': 1.3779244422912598, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 2.016084909439087, 'test/num_examples': 10000, 'score': 37815.575922966, 'total_duration': 39296.36831307411, 'accumulated_submission_time': 37815.575922966, 'accumulated_eval_time': 1473.4958517551422, 'accumulated_logging_time': 3.8107833862304688}
I0127 12:31:20.933089 140005305468672 logging_writer.py:48] [112340] accumulated_eval_time=1473.495852, accumulated_logging_time=3.810783, accumulated_submission_time=37815.575923, global_step=112340, preemption_count=0, score=37815.575923, test/accuracy=0.562700, test/loss=2.016085, test/num_examples=10000, total_duration=39296.368313, train/accuracy=0.746373, train/loss=1.098536, validation/accuracy=0.681540, validation/loss=1.377924, validation/num_examples=50000
I0127 12:31:41.369582 140005313861376 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.382138252258301, loss=2.4543464183807373
I0127 12:32:14.959315 140005305468672 logging_writer.py:48] [112500] global_step=112500, grad_norm=3.8235583305358887, loss=2.3737974166870117
I0127 12:32:48.499651 140005313861376 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.7649641036987305, loss=2.4709346294403076
I0127 12:33:22.010096 140005305468672 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.101064205169678, loss=2.4733405113220215
I0127 12:33:55.539946 140005313861376 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.334200859069824, loss=2.424961805343628
I0127 12:34:29.119347 140005305468672 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.262885570526123, loss=2.4515328407287598
I0127 12:35:02.672710 140005313861376 logging_writer.py:48] [113000] global_step=113000, grad_norm=3.673023223876953, loss=2.43711519241333
I0127 12:35:36.169129 140005305468672 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.926870822906494, loss=2.4119224548339844
I0127 12:36:09.742477 140005313861376 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.621248245239258, loss=2.3850884437561035
I0127 12:36:43.343339 140005305468672 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.434844970703125, loss=2.512131690979004
I0127 12:37:16.928955 140005313861376 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.02816104888916, loss=2.4502532482147217
I0127 12:37:50.552128 140005305468672 logging_writer.py:48] [113500] global_step=113500, grad_norm=3.8339483737945557, loss=2.415384292602539
I0127 12:38:24.209008 140005313861376 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.268855571746826, loss=2.433292865753174
I0127 12:38:57.799619 140005305468672 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.0989990234375, loss=2.398339033126831
I0127 12:39:31.397774 140005313861376 logging_writer.py:48] [113800] global_step=113800, grad_norm=3.7617931365966797, loss=2.3641717433929443
I0127 12:39:51.038356 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:39:57.370987 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:40:06.071391 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:40:08.405880 140169137129280 submission_runner.py:408] Time since start: 39823.88s, 	Step: 113860, 	{'train/accuracy': 0.7833625674247742, 'train/loss': 0.9273353815078735, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.3640166521072388, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.0014493465423584, 'test/num_examples': 10000, 'score': 38325.62386679649, 'total_duration': 39823.877307891846, 'accumulated_submission_time': 38325.62386679649, 'accumulated_eval_time': 1490.8633544445038, 'accumulated_logging_time': 3.8569798469543457}
I0127 12:40:08.443180 140004676327168 logging_writer.py:48] [113860] accumulated_eval_time=1490.863354, accumulated_logging_time=3.856980, accumulated_submission_time=38325.623867, global_step=113860, preemption_count=0, score=38325.623867, test/accuracy=0.561600, test/loss=2.001449, test/num_examples=10000, total_duration=39823.877308, train/accuracy=0.783363, train/loss=0.927335, validation/accuracy=0.684520, validation/loss=1.364017, validation/num_examples=50000
I0127 12:40:22.174641 140005288683264 logging_writer.py:48] [113900] global_step=113900, grad_norm=3.9714267253875732, loss=2.424381971359253
I0127 12:40:55.726201 140004676327168 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.299342632293701, loss=2.4611587524414062
I0127 12:41:29.328499 140005288683264 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.223846912384033, loss=2.4391930103302
I0127 12:42:02.924774 140004676327168 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.196725845336914, loss=2.371870994567871
I0127 12:42:36.549696 140005288683264 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.7868733406066895, loss=2.3773741722106934
I0127 12:43:10.172122 140004676327168 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.027531623840332, loss=2.44193959236145
I0127 12:43:43.774694 140005288683264 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.666713237762451, loss=2.419910192489624
I0127 12:44:17.407706 140004676327168 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.381970405578613, loss=2.4290435314178467
I0127 12:44:50.987247 140005288683264 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.62265682220459, loss=2.3689723014831543
I0127 12:45:24.597895 140004676327168 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.67363977432251, loss=2.3798668384552
I0127 12:45:58.191140 140005288683264 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.461263179779053, loss=2.3998825550079346
I0127 12:46:31.804333 140004676327168 logging_writer.py:48] [115000] global_step=115000, grad_norm=3.8156979084014893, loss=2.2675652503967285
I0127 12:47:05.419327 140005288683264 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.524545192718506, loss=2.4882900714874268
I0127 12:47:39.003635 140004676327168 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.250173568725586, loss=2.397521734237671
I0127 12:48:12.598701 140005288683264 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.064057350158691, loss=2.334681510925293
I0127 12:48:38.594698 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:48:44.865002 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:48:53.415015 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:48:55.709655 140169137129280 submission_runner.py:408] Time since start: 40351.18s, 	Step: 115379, 	{'train/accuracy': 0.7703882455825806, 'train/loss': 1.0078136920928955, 'validation/accuracy': 0.6881399750709534, 'validation/loss': 1.3621941804885864, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9929922819137573, 'test/num_examples': 10000, 'score': 38835.71640062332, 'total_duration': 40351.181077718735, 'accumulated_submission_time': 38835.71640062332, 'accumulated_eval_time': 1507.9782707691193, 'accumulated_logging_time': 3.9053003787994385}
I0127 12:48:55.749106 140005305468672 logging_writer.py:48] [115379] accumulated_eval_time=1507.978271, accumulated_logging_time=3.905300, accumulated_submission_time=38835.716401, global_step=115379, preemption_count=0, score=38835.716401, test/accuracy=0.565200, test/loss=1.992992, test/num_examples=10000, total_duration=40351.181078, train/accuracy=0.770388, train/loss=1.007814, validation/accuracy=0.688140, validation/loss=1.362194, validation/num_examples=50000
I0127 12:49:03.150214 140005313861376 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.043722152709961, loss=2.3766632080078125
I0127 12:49:36.681172 140005305468672 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.98224401473999, loss=2.3975703716278076
I0127 12:50:10.165749 140005313861376 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.458791732788086, loss=2.3632452487945557
I0127 12:50:43.754510 140005305468672 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.211184501647949, loss=2.4031381607055664
I0127 12:51:17.344400 140005313861376 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.098812103271484, loss=2.302757740020752
I0127 12:51:50.928642 140005305468672 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.549872875213623, loss=2.433112859725952
I0127 12:52:24.550640 140005313861376 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.290168285369873, loss=2.320072650909424
I0127 12:52:58.076172 140005305468672 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.831729888916016, loss=2.3386378288269043
I0127 12:53:31.570718 140005313861376 logging_writer.py:48] [116200] global_step=116200, grad_norm=3.9761667251586914, loss=2.349184989929199
I0127 12:54:05.173626 140005305468672 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.176572322845459, loss=2.323540449142456
I0127 12:54:38.789766 140005313861376 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.266648292541504, loss=2.433070659637451
I0127 12:55:12.340828 140005305468672 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.051758766174316, loss=2.348829507827759
I0127 12:55:45.835231 140005313861376 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.590129375457764, loss=2.3820786476135254
I0127 12:56:19.450268 140005305468672 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.585826873779297, loss=2.3726813793182373
I0127 12:56:53.000104 140005313861376 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.030534744262695, loss=2.3744404315948486
I0127 12:57:25.988578 140169137129280 spec.py:321] Evaluating on the training split.
I0127 12:57:32.247553 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 12:57:40.788549 140169137129280 spec.py:349] Evaluating on the test split.
I0127 12:57:43.098417 140169137129280 submission_runner.py:408] Time since start: 40878.57s, 	Step: 116900, 	{'train/accuracy': 0.7623764276504517, 'train/loss': 1.0332750082015991, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.3372550010681152, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.9591028690338135, 'test/num_examples': 10000, 'score': 39345.8971452713, 'total_duration': 40878.56984305382, 'accumulated_submission_time': 39345.8971452713, 'accumulated_eval_time': 1525.088080406189, 'accumulated_logging_time': 3.955566167831421}
I0127 12:57:43.135138 140004659541760 logging_writer.py:48] [116900] accumulated_eval_time=1525.088080, accumulated_logging_time=3.955566, accumulated_submission_time=39345.897145, global_step=116900, preemption_count=0, score=39345.897145, test/accuracy=0.577700, test/loss=1.959103, test/num_examples=10000, total_duration=40878.569843, train/accuracy=0.762376, train/loss=1.033275, validation/accuracy=0.694600, validation/loss=1.337255, validation/num_examples=50000
I0127 12:57:43.496535 140004667934464 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.3354926109313965, loss=2.401155710220337
I0127 12:58:16.992688 140004659541760 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.468593597412109, loss=2.360808849334717
I0127 12:58:50.539706 140004667934464 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.276094436645508, loss=2.381535530090332
I0127 12:59:24.071326 140004659541760 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.384129524230957, loss=2.3952057361602783
I0127 12:59:57.608865 140004667934464 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.2549357414245605, loss=2.4188222885131836
I0127 13:00:31.181592 140004659541760 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.972659587860107, loss=2.4247188568115234
I0127 13:01:04.753844 140004667934464 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.191159248352051, loss=2.3257334232330322
I0127 13:01:38.336745 140004659541760 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.193880081176758, loss=2.446629524230957
I0127 13:02:11.855040 140004667934464 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.702115535736084, loss=2.397496461868286
I0127 13:02:45.527648 140004659541760 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.131381511688232, loss=2.354292869567871
I0127 13:03:19.142514 140004667934464 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.314403533935547, loss=2.373687505722046
I0127 13:03:52.749341 140004659541760 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.2384257316589355, loss=2.3822121620178223
I0127 13:04:26.369986 140004667934464 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.647047996520996, loss=2.3983407020568848
I0127 13:04:59.969217 140004659541760 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.364155292510986, loss=2.4470150470733643
I0127 13:05:33.591383 140004667934464 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.27830171585083, loss=2.346261501312256
I0127 13:06:07.183451 140004659541760 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.334118366241455, loss=2.2878377437591553
I0127 13:06:13.379080 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:06:19.625001 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:06:28.256278 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:06:30.542553 140169137129280 submission_runner.py:408] Time since start: 41406.01s, 	Step: 118420, 	{'train/accuracy': 0.7677973508834839, 'train/loss': 0.9986758232116699, 'validation/accuracy': 0.6958000063896179, 'validation/loss': 1.3079572916030884, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.957592248916626, 'test/num_examples': 10000, 'score': 39856.08269357681, 'total_duration': 41406.01397848129, 'accumulated_submission_time': 39856.08269357681, 'accumulated_eval_time': 1542.2515261173248, 'accumulated_logging_time': 4.002415657043457}
I0127 13:06:30.586530 140005322254080 logging_writer.py:48] [118420] accumulated_eval_time=1542.251526, accumulated_logging_time=4.002416, accumulated_submission_time=39856.082694, global_step=118420, preemption_count=0, score=39856.082694, test/accuracy=0.567300, test/loss=1.957592, test/num_examples=10000, total_duration=41406.013978, train/accuracy=0.767797, train/loss=0.998676, validation/accuracy=0.695800, validation/loss=1.307957, validation/num_examples=50000
I0127 13:06:57.736168 140005330646784 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.326777935028076, loss=2.359215021133423
I0127 13:07:31.262066 140005322254080 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.324834823608398, loss=2.305210590362549
I0127 13:08:04.744749 140005330646784 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.653505325317383, loss=2.1903271675109863
I0127 13:08:38.377313 140005322254080 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.44045352935791, loss=2.325766086578369
I0127 13:09:11.922078 140005330646784 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.296357154846191, loss=2.3673884868621826
I0127 13:09:45.543230 140005322254080 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.392502784729004, loss=2.2562601566314697
I0127 13:10:19.157907 140005330646784 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.220456600189209, loss=2.388977527618408
I0127 13:10:52.725350 140005322254080 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.52794075012207, loss=2.3995912075042725
I0127 13:11:26.341648 140005330646784 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.379543304443359, loss=2.345127582550049
I0127 13:11:59.951029 140005322254080 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.368912696838379, loss=2.370774269104004
I0127 13:12:33.572804 140005330646784 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.173585414886475, loss=2.368497848510742
I0127 13:13:07.149605 140005322254080 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.37408447265625, loss=2.4693565368652344
I0127 13:13:40.747155 140005330646784 logging_writer.py:48] [119700] global_step=119700, grad_norm=4.135878562927246, loss=2.3566477298736572
I0127 13:14:14.386449 140005322254080 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.4560065269470215, loss=2.41731595993042
I0127 13:14:48.031825 140005330646784 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.613803863525391, loss=2.313887357711792
I0127 13:15:00.588216 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:15:06.911753 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:15:15.575168 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:15:17.877328 140169137129280 submission_runner.py:408] Time since start: 41933.35s, 	Step: 119939, 	{'train/accuracy': 0.760184109210968, 'train/loss': 1.0345245599746704, 'validation/accuracy': 0.6921399831771851, 'validation/loss': 1.3318723440170288, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9610300064086914, 'test/num_examples': 10000, 'score': 40366.02552604675, 'total_duration': 41933.3487534523, 'accumulated_submission_time': 40366.02552604675, 'accumulated_eval_time': 1559.540601491928, 'accumulated_logging_time': 4.057379245758057}
I0127 13:15:17.917239 140005313861376 logging_writer.py:48] [119939] accumulated_eval_time=1559.540601, accumulated_logging_time=4.057379, accumulated_submission_time=40366.025526, global_step=119939, preemption_count=0, score=40366.025526, test/accuracy=0.569900, test/loss=1.961030, test/num_examples=10000, total_duration=41933.348753, train/accuracy=0.760184, train/loss=1.034525, validation/accuracy=0.692140, validation/loss=1.331872, validation/num_examples=50000
I0127 13:15:38.740090 140005322254080 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.060296535491943, loss=2.3907535076141357
I0127 13:16:12.258055 140005313861376 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.433898448944092, loss=2.322875499725342
I0127 13:16:45.819663 140005322254080 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.268497943878174, loss=2.352963924407959
I0127 13:17:19.402660 140005313861376 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.5199151039123535, loss=2.34911847114563
I0127 13:17:52.916256 140005322254080 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.461141109466553, loss=2.353738307952881
I0127 13:18:26.443679 140005313861376 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.537351131439209, loss=2.3234047889709473
I0127 13:18:59.987435 140005322254080 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.694886684417725, loss=2.364926815032959
I0127 13:19:33.571420 140005313861376 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.164514541625977, loss=2.3919763565063477
I0127 13:20:07.186437 140005322254080 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.532566070556641, loss=2.355006694793701
I0127 13:20:40.863668 140005313861376 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.953908443450928, loss=2.4942736625671387
I0127 13:21:14.407745 140005322254080 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.971193313598633, loss=2.353260040283203
I0127 13:21:47.991040 140005313861376 logging_writer.py:48] [121100] global_step=121100, grad_norm=4.277135372161865, loss=2.2926254272460938
I0127 13:22:21.603699 140005322254080 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.7150044441223145, loss=2.324748992919922
I0127 13:22:55.228671 140005313861376 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.265976905822754, loss=2.315932035446167
I0127 13:23:28.849440 140005322254080 logging_writer.py:48] [121400] global_step=121400, grad_norm=4.710177421569824, loss=2.361274480819702
I0127 13:23:48.158649 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:23:54.393312 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:24:03.167978 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:24:05.483203 140169137129280 submission_runner.py:408] Time since start: 42460.95s, 	Step: 121459, 	{'train/accuracy': 0.7603236436843872, 'train/loss': 1.0235108137130737, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.3048481941223145, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 1.9369956254959106, 'test/num_examples': 10000, 'score': 40876.209104537964, 'total_duration': 42460.95462989807, 'accumulated_submission_time': 40876.209104537964, 'accumulated_eval_time': 1576.865121126175, 'accumulated_logging_time': 4.10741400718689}
I0127 13:24:05.521116 140004676327168 logging_writer.py:48] [121459] accumulated_eval_time=1576.865121, accumulated_logging_time=4.107414, accumulated_submission_time=40876.209105, global_step=121459, preemption_count=0, score=40876.209105, test/accuracy=0.573100, test/loss=1.936996, test/num_examples=10000, total_duration=42460.954630, train/accuracy=0.760324, train/loss=1.023511, validation/accuracy=0.696700, validation/loss=1.304848, validation/num_examples=50000
I0127 13:24:19.625583 140005288683264 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.839973449707031, loss=2.3908164501190186
I0127 13:24:53.173751 140004676327168 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.824112415313721, loss=2.3426661491394043
I0127 13:25:26.727492 140005288683264 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.682563304901123, loss=2.3457016944885254
I0127 13:26:00.343672 140004676327168 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.9653730392456055, loss=2.4400393962860107
I0127 13:26:33.977518 140005288683264 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.486461162567139, loss=2.362555980682373
I0127 13:27:07.580617 140004676327168 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.477390766143799, loss=2.3045527935028076
I0127 13:27:41.172492 140005288683264 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.688568592071533, loss=2.3588225841522217
I0127 13:28:14.787998 140004676327168 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.465575218200684, loss=2.320587158203125
I0127 13:28:48.325572 140005288683264 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.3280229568481445, loss=2.358330011367798
I0127 13:29:21.868639 140004676327168 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.556774139404297, loss=2.2771637439727783
I0127 13:29:55.389940 140005288683264 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.748189449310303, loss=2.3823390007019043
I0127 13:30:28.955206 140004676327168 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.983710289001465, loss=2.3398330211639404
I0127 13:31:02.496531 140005288683264 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.776748180389404, loss=2.381934642791748
I0127 13:31:36.013935 140004676327168 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.916698932647705, loss=2.3315882682800293
I0127 13:32:09.549341 140005288683264 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.944430351257324, loss=2.3275885581970215
I0127 13:32:35.545068 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:32:41.987365 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:32:50.533244 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:32:52.863663 140169137129280 submission_runner.py:408] Time since start: 42988.34s, 	Step: 122979, 	{'train/accuracy': 0.7931680083274841, 'train/loss': 0.8888539671897888, 'validation/accuracy': 0.6993399858474731, 'validation/loss': 1.2950785160064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.9291913509368896, 'test/num_examples': 10000, 'score': 41386.174983263016, 'total_duration': 42988.33508872986, 'accumulated_submission_time': 41386.174983263016, 'accumulated_eval_time': 1594.183688402176, 'accumulated_logging_time': 4.156118869781494}
I0127 13:32:52.903601 140004676327168 logging_writer.py:48] [122979] accumulated_eval_time=1594.183688, accumulated_logging_time=4.156119, accumulated_submission_time=41386.174983, global_step=122979, preemption_count=0, score=41386.174983, test/accuracy=0.580100, test/loss=1.929191, test/num_examples=10000, total_duration=42988.335089, train/accuracy=0.793168, train/loss=0.888854, validation/accuracy=0.699340, validation/loss=1.295079, validation/num_examples=50000
I0127 13:33:00.293260 140005305468672 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.606162071228027, loss=2.4195945262908936
I0127 13:33:33.826445 140004676327168 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.783288478851318, loss=2.2241227626800537
I0127 13:34:07.386327 140005305468672 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.3212761878967285, loss=2.2188847064971924
I0127 13:34:40.894557 140004676327168 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.084003448486328, loss=2.303581476211548
I0127 13:35:14.426285 140005305468672 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.608667850494385, loss=2.2887699604034424
I0127 13:35:48.027294 140004676327168 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.772955894470215, loss=2.3702476024627686
I0127 13:36:21.614994 140005305468672 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.431037425994873, loss=2.2772836685180664
I0127 13:36:55.239002 140004676327168 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.792237758636475, loss=2.356128215789795
I0127 13:37:28.835086 140005305468672 logging_writer.py:48] [123800] global_step=123800, grad_norm=4.822565078735352, loss=2.388939619064331
I0127 13:38:02.435619 140004676327168 logging_writer.py:48] [123900] global_step=123900, grad_norm=4.617231369018555, loss=2.2961912155151367
I0127 13:38:36.060328 140005305468672 logging_writer.py:48] [124000] global_step=124000, grad_norm=4.766501426696777, loss=2.358867883682251
I0127 13:39:09.694860 140004676327168 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.074288845062256, loss=2.3512797355651855
I0127 13:39:43.301791 140005305468672 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.209639072418213, loss=2.3381035327911377
I0127 13:40:16.918451 140004676327168 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.300107002258301, loss=2.316331624984741
I0127 13:40:50.508190 140005305468672 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.531811714172363, loss=2.3114500045776367
I0127 13:41:22.920286 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:41:29.187680 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:41:37.903150 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:41:40.219432 140169137129280 submission_runner.py:408] Time since start: 43515.69s, 	Step: 124498, 	{'train/accuracy': 0.7772042155265808, 'train/loss': 0.9544240832328796, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.3114979267120361, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9389946460723877, 'test/num_examples': 10000, 'score': 41896.13381576538, 'total_duration': 43515.690836429596, 'accumulated_submission_time': 41896.13381576538, 'accumulated_eval_time': 1611.4827795028687, 'accumulated_logging_time': 4.206348896026611}
I0127 13:41:40.264405 140004676327168 logging_writer.py:48] [124498] accumulated_eval_time=1611.482780, accumulated_logging_time=4.206349, accumulated_submission_time=41896.133816, global_step=124498, preemption_count=0, score=41896.133816, test/accuracy=0.575100, test/loss=1.938995, test/num_examples=10000, total_duration=43515.690836, train/accuracy=0.777204, train/loss=0.954424, validation/accuracy=0.694940, validation/loss=1.311498, validation/num_examples=50000
I0127 13:41:41.270913 140005288683264 logging_writer.py:48] [124500] global_step=124500, grad_norm=4.6252522468566895, loss=2.3455309867858887
I0127 13:42:14.849558 140004676327168 logging_writer.py:48] [124600] global_step=124600, grad_norm=4.384786605834961, loss=2.294299364089966
I0127 13:42:48.458396 140005288683264 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.969297409057617, loss=2.3538498878479004
I0127 13:43:22.078120 140004676327168 logging_writer.py:48] [124800] global_step=124800, grad_norm=4.91312837600708, loss=2.373931407928467
I0127 13:43:55.699748 140005288683264 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.29503059387207, loss=2.2725768089294434
I0127 13:44:29.316588 140004676327168 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.069813251495361, loss=2.2302041053771973
I0127 13:45:02.933616 140005288683264 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.95568323135376, loss=2.391083002090454
I0127 13:45:36.551836 140004676327168 logging_writer.py:48] [125200] global_step=125200, grad_norm=4.892997741699219, loss=2.339850425720215
I0127 13:46:10.176923 140005288683264 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.686682224273682, loss=2.364715814590454
I0127 13:46:43.810187 140004676327168 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.252465724945068, loss=2.2745418548583984
I0127 13:47:17.426376 140005288683264 logging_writer.py:48] [125500] global_step=125500, grad_norm=4.965012550354004, loss=2.4135184288024902
I0127 13:47:50.970418 140004676327168 logging_writer.py:48] [125600] global_step=125600, grad_norm=4.952874183654785, loss=2.309080123901367
I0127 13:48:24.579475 140005288683264 logging_writer.py:48] [125700] global_step=125700, grad_norm=4.671825885772705, loss=2.3071393966674805
I0127 13:48:58.206481 140004676327168 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.086074352264404, loss=2.3023643493652344
I0127 13:49:31.812902 140005288683264 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.089101314544678, loss=2.3080646991729736
I0127 13:50:05.419932 140004676327168 logging_writer.py:48] [126000] global_step=126000, grad_norm=4.584364414215088, loss=2.2319083213806152
I0127 13:50:10.274782 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:50:16.492553 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:50:25.070730 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:50:27.383923 140169137129280 submission_runner.py:408] Time since start: 44042.86s, 	Step: 126016, 	{'train/accuracy': 0.7820471525192261, 'train/loss': 0.9358399510383606, 'validation/accuracy': 0.7050999999046326, 'validation/loss': 1.2812731266021729, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.9185949563980103, 'test/num_examples': 10000, 'score': 42406.0859875679, 'total_duration': 44042.85533833504, 'accumulated_submission_time': 42406.0859875679, 'accumulated_eval_time': 1628.5918712615967, 'accumulated_logging_time': 4.261731863021851}
I0127 13:50:27.423964 140005305468672 logging_writer.py:48] [126016] accumulated_eval_time=1628.591871, accumulated_logging_time=4.261732, accumulated_submission_time=42406.085988, global_step=126016, preemption_count=0, score=42406.085988, test/accuracy=0.583400, test/loss=1.918595, test/num_examples=10000, total_duration=44042.855338, train/accuracy=0.782047, train/loss=0.935840, validation/accuracy=0.705100, validation/loss=1.281273, validation/num_examples=50000
I0127 13:50:56.003612 140005313861376 logging_writer.py:48] [126100] global_step=126100, grad_norm=4.805734634399414, loss=2.2728474140167236
I0127 13:51:29.565569 140005305468672 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.049076080322266, loss=2.3819243907928467
I0127 13:52:03.152018 140005313861376 logging_writer.py:48] [126300] global_step=126300, grad_norm=4.543945789337158, loss=2.2688300609588623
I0127 13:52:36.765480 140005305468672 logging_writer.py:48] [126400] global_step=126400, grad_norm=4.487829208374023, loss=2.2578272819519043
I0127 13:53:10.372183 140005313861376 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.035154819488525, loss=2.299119472503662
I0127 13:53:43.990920 140005305468672 logging_writer.py:48] [126600] global_step=126600, grad_norm=4.611148834228516, loss=2.281331777572632
I0127 13:54:17.562429 140005313861376 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.103840351104736, loss=2.2507076263427734
I0127 13:54:51.131065 140005305468672 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.201977252960205, loss=2.2237019538879395
I0127 13:55:24.704063 140005313861376 logging_writer.py:48] [126900] global_step=126900, grad_norm=4.715142726898193, loss=2.3561110496520996
I0127 13:55:58.184461 140005305468672 logging_writer.py:48] [127000] global_step=127000, grad_norm=4.928256511688232, loss=2.308812379837036
I0127 13:56:31.747934 140005313861376 logging_writer.py:48] [127100] global_step=127100, grad_norm=4.735311985015869, loss=2.27636456489563
I0127 13:57:05.401359 140005305468672 logging_writer.py:48] [127200] global_step=127200, grad_norm=4.7890944480896, loss=2.318019390106201
I0127 13:57:38.927523 140005313861376 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.552704334259033, loss=2.307023763656616
I0127 13:58:12.443502 140005305468672 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.010650634765625, loss=2.244101047515869
I0127 13:58:45.977701 140005313861376 logging_writer.py:48] [127500] global_step=127500, grad_norm=4.732182025909424, loss=2.2250449657440186
I0127 13:58:57.511876 140169137129280 spec.py:321] Evaluating on the training split.
I0127 13:59:03.844406 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 13:59:12.603887 140169137129280 spec.py:349] Evaluating on the test split.
I0127 13:59:14.923933 140169137129280 submission_runner.py:408] Time since start: 44570.40s, 	Step: 127536, 	{'train/accuracy': 0.7782206535339355, 'train/loss': 0.9608622193336487, 'validation/accuracy': 0.7054600119590759, 'validation/loss': 1.2842296361923218, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.9385350942611694, 'test/num_examples': 10000, 'score': 42916.115965127945, 'total_duration': 44570.39530873299, 'accumulated_submission_time': 42916.115965127945, 'accumulated_eval_time': 1646.0038397312164, 'accumulated_logging_time': 4.311935186386108}
I0127 13:59:14.988055 140005288683264 logging_writer.py:48] [127536] accumulated_eval_time=1646.003840, accumulated_logging_time=4.311935, accumulated_submission_time=42916.115965, global_step=127536, preemption_count=0, score=42916.115965, test/accuracy=0.579200, test/loss=1.938535, test/num_examples=10000, total_duration=44570.395309, train/accuracy=0.778221, train/loss=0.960862, validation/accuracy=0.705460, validation/loss=1.284230, validation/num_examples=50000
I0127 13:59:37.115947 140005297075968 logging_writer.py:48] [127600] global_step=127600, grad_norm=4.694718837738037, loss=2.212811231613159
I0127 14:00:10.635772 140005288683264 logging_writer.py:48] [127700] global_step=127700, grad_norm=4.545786380767822, loss=2.304725170135498
I0127 14:00:44.212325 140005297075968 logging_writer.py:48] [127800] global_step=127800, grad_norm=4.9917120933532715, loss=2.340463638305664
I0127 14:01:17.719355 140005288683264 logging_writer.py:48] [127900] global_step=127900, grad_norm=4.792203426361084, loss=2.2872414588928223
I0127 14:01:51.272189 140005297075968 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.03338623046875, loss=2.2871127128601074
I0127 14:02:24.869042 140005288683264 logging_writer.py:48] [128100] global_step=128100, grad_norm=4.8062639236450195, loss=2.273432970046997
I0127 14:02:58.483461 140005297075968 logging_writer.py:48] [128200] global_step=128200, grad_norm=4.984546661376953, loss=2.3186135292053223
I0127 14:03:32.138412 140005288683264 logging_writer.py:48] [128300] global_step=128300, grad_norm=4.92013692855835, loss=2.287379741668701
I0127 14:04:05.750335 140005297075968 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.567263603210449, loss=2.274143695831299
I0127 14:04:39.361693 140005288683264 logging_writer.py:48] [128500] global_step=128500, grad_norm=4.818190574645996, loss=2.171071767807007
I0127 14:05:12.976875 140005297075968 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.839080810546875, loss=2.3107571601867676
I0127 14:05:46.612743 140005288683264 logging_writer.py:48] [128700] global_step=128700, grad_norm=4.743947505950928, loss=2.2664029598236084
I0127 14:06:20.228092 140005297075968 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.1464009284973145, loss=2.2764785289764404
I0127 14:06:53.849694 140005288683264 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.012848854064941, loss=2.31027889251709
I0127 14:07:27.453110 140005297075968 logging_writer.py:48] [129000] global_step=129000, grad_norm=4.635490894317627, loss=2.2077836990356445
I0127 14:07:45.054505 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:07:51.466737 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:08:00.046355 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:08:02.364438 140169137129280 submission_runner.py:408] Time since start: 45097.84s, 	Step: 129054, 	{'train/accuracy': 0.7837212681770325, 'train/loss': 0.9352360963821411, 'validation/accuracy': 0.712179958820343, 'validation/loss': 1.254599690437317, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.8815577030181885, 'test/num_examples': 10000, 'score': 43425.82639288902, 'total_duration': 45097.83585691452, 'accumulated_submission_time': 43425.82639288902, 'accumulated_eval_time': 1663.313729763031, 'accumulated_logging_time': 4.684342861175537}
I0127 14:08:02.405973 140004667934464 logging_writer.py:48] [129054] accumulated_eval_time=1663.313730, accumulated_logging_time=4.684343, accumulated_submission_time=43425.826393, global_step=129054, preemption_count=0, score=43425.826393, test/accuracy=0.587300, test/loss=1.881558, test/num_examples=10000, total_duration=45097.835857, train/accuracy=0.783721, train/loss=0.935236, validation/accuracy=0.712180, validation/loss=1.254600, validation/num_examples=50000
I0127 14:08:18.188524 140004676327168 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.521640777587891, loss=2.247206926345825
I0127 14:08:51.727566 140004667934464 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.679424285888672, loss=2.2926461696624756
I0127 14:09:25.366555 140004676327168 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.223592281341553, loss=2.3624277114868164
I0127 14:09:58.954730 140004667934464 logging_writer.py:48] [129400] global_step=129400, grad_norm=4.936046123504639, loss=2.2216949462890625
I0127 14:10:32.569599 140004676327168 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.1108012199401855, loss=2.2641701698303223
I0127 14:11:06.189158 140004667934464 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.448273181915283, loss=2.2044920921325684
I0127 14:11:39.810994 140004676327168 logging_writer.py:48] [129700] global_step=129700, grad_norm=4.682421684265137, loss=2.262930393218994
I0127 14:12:13.391641 140004667934464 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.0407938957214355, loss=2.3034799098968506
I0127 14:12:46.950175 140004676327168 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.447017669677734, loss=2.262991189956665
I0127 14:13:20.578605 140004667934464 logging_writer.py:48] [130000] global_step=130000, grad_norm=6.006014347076416, loss=2.291616201400757
I0127 14:13:54.198745 140004676327168 logging_writer.py:48] [130100] global_step=130100, grad_norm=4.969081878662109, loss=2.1672251224517822
I0127 14:14:27.779367 140004667934464 logging_writer.py:48] [130200] global_step=130200, grad_norm=4.790355682373047, loss=2.231511354446411
I0127 14:15:01.314647 140004676327168 logging_writer.py:48] [130300] global_step=130300, grad_norm=4.9055376052856445, loss=2.300610303878784
I0127 14:15:34.982894 140004667934464 logging_writer.py:48] [130400] global_step=130400, grad_norm=4.845527172088623, loss=2.248455762863159
I0127 14:16:08.596852 140004676327168 logging_writer.py:48] [130500] global_step=130500, grad_norm=4.593290328979492, loss=2.2756409645080566
I0127 14:16:32.599669 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:16:38.809397 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:16:47.361955 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:16:49.676435 140169137129280 submission_runner.py:408] Time since start: 45625.15s, 	Step: 130573, 	{'train/accuracy': 0.7919722199440002, 'train/loss': 0.8858956694602966, 'validation/accuracy': 0.7138800024986267, 'validation/loss': 1.2260390520095825, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.860069990158081, 'test/num_examples': 10000, 'score': 43935.96074128151, 'total_duration': 45625.14785838127, 'accumulated_submission_time': 43935.96074128151, 'accumulated_eval_time': 1680.3904614448547, 'accumulated_logging_time': 4.7375593185424805}
I0127 14:16:49.718088 140005297075968 logging_writer.py:48] [130573] accumulated_eval_time=1680.390461, accumulated_logging_time=4.737559, accumulated_submission_time=43935.960741, global_step=130573, preemption_count=0, score=43935.960741, test/accuracy=0.593400, test/loss=1.860070, test/num_examples=10000, total_duration=45625.147858, train/accuracy=0.791972, train/loss=0.885896, validation/accuracy=0.713880, validation/loss=1.226039, validation/num_examples=50000
I0127 14:16:59.123883 140005322254080 logging_writer.py:48] [130600] global_step=130600, grad_norm=4.963957786560059, loss=2.23811411857605
I0127 14:17:32.677021 140005297075968 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.191378116607666, loss=2.230276584625244
I0127 14:18:06.175834 140005322254080 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.044520378112793, loss=2.2151200771331787
I0127 14:18:39.756558 140005297075968 logging_writer.py:48] [130900] global_step=130900, grad_norm=4.730966091156006, loss=2.338526487350464
I0127 14:19:13.371640 140005322254080 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.123180389404297, loss=2.252289056777954
I0127 14:19:46.987463 140005297075968 logging_writer.py:48] [131100] global_step=131100, grad_norm=4.703003406524658, loss=2.295252799987793
I0127 14:20:20.610904 140005322254080 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.537405967712402, loss=2.23878812789917
I0127 14:20:54.218031 140005297075968 logging_writer.py:48] [131300] global_step=131300, grad_norm=4.718814849853516, loss=2.2713375091552734
I0127 14:21:27.777434 140005322254080 logging_writer.py:48] [131400] global_step=131400, grad_norm=4.978920936584473, loss=2.2655014991760254
I0127 14:22:01.422682 140005297075968 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.306540489196777, loss=2.31105899810791
I0127 14:22:35.054190 140005322254080 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.6014933586120605, loss=2.239231824874878
I0127 14:23:08.666776 140005297075968 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.28069543838501, loss=2.2051143646240234
I0127 14:23:42.206570 140005322254080 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.156735897064209, loss=2.220989465713501
I0127 14:24:15.746264 140005297075968 logging_writer.py:48] [131900] global_step=131900, grad_norm=4.888586521148682, loss=2.1713223457336426
I0127 14:24:49.261782 140005322254080 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.031744003295898, loss=2.2385404109954834
I0127 14:25:19.937814 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:25:26.160797 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:25:34.757009 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:25:36.990443 140169137129280 submission_runner.py:408] Time since start: 46152.46s, 	Step: 132093, 	{'train/accuracy': 0.8088328838348389, 'train/loss': 0.8323812484741211, 'validation/accuracy': 0.7155199646949768, 'validation/loss': 1.2397068738937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.8860760927200317, 'test/num_examples': 10000, 'score': 44446.12138080597, 'total_duration': 46152.46186089516, 'accumulated_submission_time': 44446.12138080597, 'accumulated_eval_time': 1697.4430515766144, 'accumulated_logging_time': 4.790385007858276}
I0127 14:25:37.031607 140004667934464 logging_writer.py:48] [132093] accumulated_eval_time=1697.443052, accumulated_logging_time=4.790385, accumulated_submission_time=44446.121381, global_step=132093, preemption_count=0, score=44446.121381, test/accuracy=0.583600, test/loss=1.886076, test/num_examples=10000, total_duration=46152.461861, train/accuracy=0.808833, train/loss=0.832381, validation/accuracy=0.715520, validation/loss=1.239707, validation/num_examples=50000
I0127 14:25:39.720419 140004676327168 logging_writer.py:48] [132100] global_step=132100, grad_norm=4.927542686462402, loss=2.162609338760376
I0127 14:26:13.254276 140004667934464 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.324382781982422, loss=2.209700107574463
I0127 14:26:46.820021 140004676327168 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.096100807189941, loss=2.217700481414795
I0127 14:27:20.435749 140004667934464 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.026772499084473, loss=2.342637538909912
I0127 14:27:54.085758 140004676327168 logging_writer.py:48] [132500] global_step=132500, grad_norm=4.51455545425415, loss=2.2155399322509766
I0127 14:28:27.690773 140004667934464 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.379557132720947, loss=2.2209503650665283
I0127 14:29:01.267251 140004676327168 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.620187759399414, loss=2.192061424255371
I0127 14:29:34.799259 140004667934464 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.219213962554932, loss=2.2078680992126465
I0127 14:30:08.327901 140004676327168 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.629360198974609, loss=2.3561699390411377
I0127 14:30:41.900521 140004667934464 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.4990715980529785, loss=2.210721015930176
I0127 14:31:15.497424 140004676327168 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.298107624053955, loss=2.2326736450195312
I0127 14:31:49.120979 140004667934464 logging_writer.py:48] [133200] global_step=133200, grad_norm=5.302103519439697, loss=2.2911739349365234
I0127 14:32:22.737944 140004676327168 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.203640460968018, loss=2.2153472900390625
I0127 14:32:56.351605 140004667934464 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.089057922363281, loss=2.1841888427734375
I0127 14:33:29.963898 140004676327168 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.162952899932861, loss=2.2519614696502686
I0127 14:34:03.579991 140004667934464 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.9348626136779785, loss=2.2241086959838867
I0127 14:34:07.086536 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:34:13.358895 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:34:21.916550 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:34:24.223109 140169137129280 submission_runner.py:408] Time since start: 46679.69s, 	Step: 133612, 	{'train/accuracy': 0.7983298897743225, 'train/loss': 0.8808016180992126, 'validation/accuracy': 0.7109400033950806, 'validation/loss': 1.252920150756836, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.9056119918823242, 'test/num_examples': 10000, 'score': 44956.11803674698, 'total_duration': 46679.69448518753, 'accumulated_submission_time': 44956.11803674698, 'accumulated_eval_time': 1714.5795366764069, 'accumulated_logging_time': 4.842229604721069}
I0127 14:34:24.264639 140004659541760 logging_writer.py:48] [133612] accumulated_eval_time=1714.579537, accumulated_logging_time=4.842230, accumulated_submission_time=44956.118037, global_step=133612, preemption_count=0, score=44956.118037, test/accuracy=0.588200, test/loss=1.905612, test/num_examples=10000, total_duration=46679.694485, train/accuracy=0.798330, train/loss=0.880802, validation/accuracy=0.710940, validation/loss=1.252920, validation/num_examples=50000
I0127 14:34:54.100048 140004667934464 logging_writer.py:48] [133700] global_step=133700, grad_norm=4.728238582611084, loss=2.2346391677856445
I0127 14:35:27.693654 140004659541760 logging_writer.py:48] [133800] global_step=133800, grad_norm=4.95604133605957, loss=2.191286087036133
I0127 14:36:01.297560 140004667934464 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.3236284255981445, loss=2.256711959838867
I0127 14:36:34.922378 140004659541760 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.538313865661621, loss=2.1831274032592773
I0127 14:37:08.545013 140004667934464 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.173604488372803, loss=2.231873035430908
I0127 14:37:42.157415 140004659541760 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.384604454040527, loss=2.1429760456085205
I0127 14:38:15.675823 140004667934464 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.199468612670898, loss=2.2661678791046143
I0127 14:38:49.208146 140004659541760 logging_writer.py:48] [134400] global_step=134400, grad_norm=4.982840538024902, loss=2.2413294315338135
I0127 14:39:22.742576 140004667934464 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.915025234222412, loss=2.2547011375427246
I0127 14:39:56.359559 140004659541760 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.153371334075928, loss=2.240194797515869
I0127 14:40:29.900058 140004667934464 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.5764546394348145, loss=2.227130651473999
I0127 14:41:03.534332 140004659541760 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.499160289764404, loss=2.165114641189575
I0127 14:41:37.167083 140004667934464 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.570746898651123, loss=2.192532777786255
I0127 14:42:10.781063 140004659541760 logging_writer.py:48] [135000] global_step=135000, grad_norm=4.9568705558776855, loss=2.220468282699585
I0127 14:42:44.350321 140004667934464 logging_writer.py:48] [135100] global_step=135100, grad_norm=5.289268970489502, loss=2.270604133605957
I0127 14:42:54.258498 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:43:00.500611 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:43:09.029791 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:43:11.318805 140169137129280 submission_runner.py:408] Time since start: 47206.79s, 	Step: 135131, 	{'train/accuracy': 0.8057836294174194, 'train/loss': 0.8274676203727722, 'validation/accuracy': 0.7206000089645386, 'validation/loss': 1.196054220199585, 'validation/num_examples': 50000, 'test/accuracy': 0.5926000475883484, 'test/loss': 1.841191053390503, 'test/num_examples': 10000, 'score': 45466.051250457764, 'total_duration': 47206.79020857811, 'accumulated_submission_time': 45466.051250457764, 'accumulated_eval_time': 1731.6397836208344, 'accumulated_logging_time': 4.896440029144287}
I0127 14:43:11.361035 140004659541760 logging_writer.py:48] [135131] accumulated_eval_time=1731.639784, accumulated_logging_time=4.896440, accumulated_submission_time=45466.051250, global_step=135131, preemption_count=0, score=45466.051250, test/accuracy=0.592600, test/loss=1.841191, test/num_examples=10000, total_duration=47206.790209, train/accuracy=0.805784, train/loss=0.827468, validation/accuracy=0.720600, validation/loss=1.196054, validation/num_examples=50000
I0127 14:43:34.819459 140004667934464 logging_writer.py:48] [135200] global_step=135200, grad_norm=4.964822292327881, loss=2.152771234512329
I0127 14:44:08.322701 140004659541760 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.321403980255127, loss=2.182471752166748
I0127 14:44:41.910928 140004667934464 logging_writer.py:48] [135400] global_step=135400, grad_norm=5.485291957855225, loss=2.2132201194763184
I0127 14:45:15.526225 140004659541760 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.0850605964660645, loss=2.146975517272949
I0127 14:45:49.205315 140004667934464 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.5627217292785645, loss=2.1739566326141357
I0127 14:46:22.742062 140004659541760 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.254703521728516, loss=2.2951223850250244
I0127 14:46:56.328005 140004667934464 logging_writer.py:48] [135800] global_step=135800, grad_norm=4.946141242980957, loss=2.2010364532470703
I0127 14:47:29.937308 140004659541760 logging_writer.py:48] [135900] global_step=135900, grad_norm=5.128889083862305, loss=2.1962814331054688
I0127 14:48:03.526275 140004667934464 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.6306610107421875, loss=2.2463269233703613
I0127 14:48:37.135853 140004659541760 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.63545036315918, loss=2.2075390815734863
I0127 14:49:10.741195 140004667934464 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.741674900054932, loss=2.1450278759002686
I0127 14:49:44.351289 140004659541760 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.199582099914551, loss=2.192920446395874
I0127 14:50:17.997043 140004667934464 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.733859539031982, loss=2.2495036125183105
I0127 14:50:51.610360 140004659541760 logging_writer.py:48] [136500] global_step=136500, grad_norm=5.308541297912598, loss=2.2493276596069336
I0127 14:51:25.206501 140004667934464 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.876645565032959, loss=2.1965672969818115
I0127 14:51:41.470731 140169137129280 spec.py:321] Evaluating on the training split.
I0127 14:51:47.693845 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 14:51:56.178945 140169137129280 spec.py:349] Evaluating on the test split.
I0127 14:51:58.781664 140169137129280 submission_runner.py:408] Time since start: 47734.25s, 	Step: 136650, 	{'train/accuracy': 0.7958585619926453, 'train/loss': 0.875244677066803, 'validation/accuracy': 0.7117800116539001, 'validation/loss': 1.2368409633636475, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8613744974136353, 'test/num_examples': 10000, 'score': 45976.103113889694, 'total_duration': 47734.25307178497, 'accumulated_submission_time': 45976.103113889694, 'accumulated_eval_time': 1748.9506647586823, 'accumulated_logging_time': 4.948869228363037}
I0127 14:51:58.824624 140004659541760 logging_writer.py:48] [136650] accumulated_eval_time=1748.950665, accumulated_logging_time=4.948869, accumulated_submission_time=45976.103114, global_step=136650, preemption_count=0, score=45976.103114, test/accuracy=0.590600, test/loss=1.861374, test/num_examples=10000, total_duration=47734.253072, train/accuracy=0.795859, train/loss=0.875245, validation/accuracy=0.711780, validation/loss=1.236841, validation/num_examples=50000
I0127 14:52:15.942763 140004667934464 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.617618083953857, loss=2.207927942276001
I0127 14:52:49.422160 140004659541760 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.093630790710449, loss=2.179551124572754
I0127 14:53:22.948520 140004667934464 logging_writer.py:48] [136900] global_step=136900, grad_norm=6.083588123321533, loss=2.2498135566711426
I0127 14:53:56.562893 140004659541760 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.483582973480225, loss=2.153660774230957
I0127 14:54:30.161080 140004667934464 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.490230083465576, loss=2.1665213108062744
I0127 14:55:03.712602 140004659541760 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.367619514465332, loss=2.172175645828247
I0127 14:55:37.360691 140004667934464 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.1481804847717285, loss=2.118138074874878
I0127 14:56:10.987014 140004659541760 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.583739757537842, loss=2.121999740600586
I0127 14:56:44.601427 140004667934464 logging_writer.py:48] [137500] global_step=137500, grad_norm=5.9934282302856445, loss=2.3148953914642334
I0127 14:57:18.128674 140004659541760 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.113955497741699, loss=2.131678342819214
I0127 14:57:51.699479 140004667934464 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.687467575073242, loss=2.1576714515686035
I0127 14:58:25.325865 140004659541760 logging_writer.py:48] [137800] global_step=137800, grad_norm=5.5479302406311035, loss=2.1418840885162354
I0127 14:58:58.955906 140004667934464 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.551352024078369, loss=2.1328554153442383
I0127 14:59:32.578924 140004659541760 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.261847496032715, loss=2.1448397636413574
I0127 15:00:06.184091 140004667934464 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.083708763122559, loss=2.2051773071289062
I0127 15:00:28.825768 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:00:35.166998 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:00:43.823039 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:00:46.127501 140169137129280 submission_runner.py:408] Time since start: 48261.60s, 	Step: 138169, 	{'train/accuracy': 0.8053850531578064, 'train/loss': 0.8225789070129395, 'validation/accuracy': 0.723859965801239, 'validation/loss': 1.1837297677993774, 'validation/num_examples': 50000, 'test/accuracy': 0.6002000570297241, 'test/loss': 1.803093433380127, 'test/num_examples': 10000, 'score': 46486.0466632843, 'total_duration': 48261.59885954857, 'accumulated_submission_time': 46486.0466632843, 'accumulated_eval_time': 1766.2522943019867, 'accumulated_logging_time': 5.0018205642700195}
I0127 15:00:46.167555 140005288683264 logging_writer.py:48] [138169] accumulated_eval_time=1766.252294, accumulated_logging_time=5.001821, accumulated_submission_time=46486.046663, global_step=138169, preemption_count=0, score=46486.046663, test/accuracy=0.600200, test/loss=1.803093, test/num_examples=10000, total_duration=48261.598860, train/accuracy=0.805385, train/loss=0.822579, validation/accuracy=0.723860, validation/loss=1.183730, validation/num_examples=50000
I0127 15:00:56.920947 140005297075968 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.181248664855957, loss=2.156848669052124
I0127 15:01:30.416681 140005288683264 logging_writer.py:48] [138300] global_step=138300, grad_norm=5.794107437133789, loss=2.142381191253662
I0127 15:02:03.957348 140005297075968 logging_writer.py:48] [138400] global_step=138400, grad_norm=5.671799659729004, loss=2.178478479385376
I0127 15:02:37.490470 140005288683264 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.464675426483154, loss=2.1919896602630615
I0127 15:03:10.984962 140005297075968 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.697605133056641, loss=2.1626737117767334
I0127 15:03:44.588161 140005288683264 logging_writer.py:48] [138700] global_step=138700, grad_norm=5.795968055725098, loss=2.193669319152832
I0127 15:04:18.229747 140005297075968 logging_writer.py:48] [138800] global_step=138800, grad_norm=5.606086730957031, loss=2.193666934967041
I0127 15:04:51.799941 140005288683264 logging_writer.py:48] [138900] global_step=138900, grad_norm=5.364060878753662, loss=2.2161505222320557
I0127 15:05:25.399584 140005297075968 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.230961799621582, loss=2.2413549423217773
I0127 15:05:58.970169 140005288683264 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.490633964538574, loss=2.2019522190093994
I0127 15:06:32.590303 140005297075968 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.298152923583984, loss=2.159134864807129
I0127 15:07:06.230540 140005288683264 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.343691825866699, loss=2.1306087970733643
I0127 15:07:39.833512 140005297075968 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.727836608886719, loss=2.189891815185547
I0127 15:08:13.441221 140005288683264 logging_writer.py:48] [139500] global_step=139500, grad_norm=5.724422454833984, loss=2.1347999572753906
I0127 15:08:47.057927 140005297075968 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.179880619049072, loss=2.194507122039795
I0127 15:09:16.434340 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:09:22.902005 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:09:31.531761 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:09:33.909441 140169137129280 submission_runner.py:408] Time since start: 48789.38s, 	Step: 139689, 	{'train/accuracy': 0.8387476205825806, 'train/loss': 0.7008848190307617, 'validation/accuracy': 0.7271199822425842, 'validation/loss': 1.168049693107605, 'validation/num_examples': 50000, 'test/accuracy': 0.6009000539779663, 'test/loss': 1.8257156610488892, 'test/num_examples': 10000, 'score': 46996.255591869354, 'total_duration': 48789.38086462021, 'accumulated_submission_time': 46996.255591869354, 'accumulated_eval_time': 1783.7273569107056, 'accumulated_logging_time': 5.05206823348999}
I0127 15:09:33.950391 140004667934464 logging_writer.py:48] [139689] accumulated_eval_time=1783.727357, accumulated_logging_time=5.052068, accumulated_submission_time=46996.255592, global_step=139689, preemption_count=0, score=46996.255592, test/accuracy=0.600900, test/loss=1.825716, test/num_examples=10000, total_duration=48789.380865, train/accuracy=0.838748, train/loss=0.700885, validation/accuracy=0.727120, validation/loss=1.168050, validation/num_examples=50000
I0127 15:09:38.001399 140004676327168 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.630965232849121, loss=2.12304425239563
I0127 15:10:11.627132 140004667934464 logging_writer.py:48] [139800] global_step=139800, grad_norm=5.875054359436035, loss=2.128901958465576
I0127 15:10:45.182566 140004676327168 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.490785598754883, loss=2.130786180496216
I0127 15:11:18.761182 140004667934464 logging_writer.py:48] [140000] global_step=140000, grad_norm=5.007899284362793, loss=2.144503593444824
I0127 15:11:52.302879 140004676327168 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.769130706787109, loss=2.1562445163726807
I0127 15:12:25.851737 140004667934464 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.966233730316162, loss=2.178018808364868
I0127 15:12:59.359032 140004676327168 logging_writer.py:48] [140300] global_step=140300, grad_norm=5.6993842124938965, loss=2.123267889022827
I0127 15:13:32.913549 140004667934464 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.02023983001709, loss=2.189488410949707
I0127 15:14:06.421436 140004676327168 logging_writer.py:48] [140500] global_step=140500, grad_norm=5.407648086547852, loss=2.1437056064605713
I0127 15:14:39.970935 140004667934464 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.850319862365723, loss=2.177320718765259
I0127 15:15:13.491515 140004676327168 logging_writer.py:48] [140700] global_step=140700, grad_norm=5.7114577293396, loss=2.1072428226470947
I0127 15:15:47.075246 140004667934464 logging_writer.py:48] [140800] global_step=140800, grad_norm=5.546368598937988, loss=2.092984437942505
I0127 15:16:20.667204 140004676327168 logging_writer.py:48] [140900] global_step=140900, grad_norm=5.783397197723389, loss=2.141554594039917
I0127 15:16:54.283792 140004667934464 logging_writer.py:48] [141000] global_step=141000, grad_norm=5.485949993133545, loss=2.0179693698883057
I0127 15:17:27.905730 140004676327168 logging_writer.py:48] [141100] global_step=141100, grad_norm=5.687121391296387, loss=2.1626486778259277
I0127 15:18:01.513123 140004667934464 logging_writer.py:48] [141200] global_step=141200, grad_norm=5.986566066741943, loss=2.045144557952881
I0127 15:18:04.006327 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:18:10.294482 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:18:18.814340 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:18:21.144979 140169137129280 submission_runner.py:408] Time since start: 49316.62s, 	Step: 141209, 	{'train/accuracy': 0.8303571343421936, 'train/loss': 0.7429234385490417, 'validation/accuracy': 0.725600004196167, 'validation/loss': 1.1872261762619019, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7983014583587646, 'test/num_examples': 10000, 'score': 47506.24500584602, 'total_duration': 49316.61631822586, 'accumulated_submission_time': 47506.24500584602, 'accumulated_eval_time': 1800.8658833503723, 'accumulated_logging_time': 5.111589431762695}
I0127 15:18:21.190200 140004659541760 logging_writer.py:48] [141209] accumulated_eval_time=1800.865883, accumulated_logging_time=5.111589, accumulated_submission_time=47506.245006, global_step=141209, preemption_count=0, score=47506.245006, test/accuracy=0.607000, test/loss=1.798301, test/num_examples=10000, total_duration=49316.616318, train/accuracy=0.830357, train/loss=0.742923, validation/accuracy=0.725600, validation/loss=1.187226, validation/num_examples=50000
I0127 15:18:52.087750 140004667934464 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.006790637969971, loss=2.262446880340576
I0127 15:19:25.643240 140004659541760 logging_writer.py:48] [141400] global_step=141400, grad_norm=5.8354010581970215, loss=2.218742847442627
I0127 15:19:59.236687 140004667934464 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.654168128967285, loss=2.1886301040649414
I0127 15:20:32.840117 140004659541760 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.211353302001953, loss=2.1929562091827393
I0127 15:21:06.459521 140004667934464 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.716936111450195, loss=2.161893367767334
I0127 15:21:40.054824 140004659541760 logging_writer.py:48] [141800] global_step=141800, grad_norm=5.552116394042969, loss=2.1613430976867676
I0127 15:22:13.677548 140004667934464 logging_writer.py:48] [141900] global_step=141900, grad_norm=5.685634613037109, loss=2.140821695327759
I0127 15:22:47.441653 140004659541760 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.324605941772461, loss=2.145416021347046
I0127 15:23:20.972059 140004667934464 logging_writer.py:48] [142100] global_step=142100, grad_norm=5.815414905548096, loss=2.1423702239990234
I0127 15:23:54.551849 140004659541760 logging_writer.py:48] [142200] global_step=142200, grad_norm=5.676190376281738, loss=2.170516014099121
I0127 15:24:28.136967 140004667934464 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.9099273681640625, loss=2.094825267791748
I0127 15:25:01.793328 140004659541760 logging_writer.py:48] [142400] global_step=142400, grad_norm=5.927090644836426, loss=2.192689895629883
I0127 15:25:35.402632 140004667934464 logging_writer.py:48] [142500] global_step=142500, grad_norm=5.514322280883789, loss=2.0709381103515625
I0127 15:26:08.963176 140004659541760 logging_writer.py:48] [142600] global_step=142600, grad_norm=5.83546781539917, loss=2.0536181926727295
I0127 15:26:42.493645 140004667934464 logging_writer.py:48] [142700] global_step=142700, grad_norm=5.352053642272949, loss=2.1507275104522705
I0127 15:26:51.351624 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:26:57.566297 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:27:06.247504 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:27:08.537393 140169137129280 submission_runner.py:408] Time since start: 49844.01s, 	Step: 142728, 	{'train/accuracy': 0.8176219463348389, 'train/loss': 0.8000547885894775, 'validation/accuracy': 0.7247999906539917, 'validation/loss': 1.205077886581421, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.849668025970459, 'test/num_examples': 10000, 'score': 48016.3487842083, 'total_duration': 49844.00881743431, 'accumulated_submission_time': 48016.3487842083, 'accumulated_eval_time': 1818.0516149997711, 'accumulated_logging_time': 5.166984558105469}
I0127 15:27:08.579003 140005288683264 logging_writer.py:48] [142728] accumulated_eval_time=1818.051615, accumulated_logging_time=5.166985, accumulated_submission_time=48016.348784, global_step=142728, preemption_count=0, score=48016.348784, test/accuracy=0.599800, test/loss=1.849668, test/num_examples=10000, total_duration=49844.008817, train/accuracy=0.817622, train/loss=0.800055, validation/accuracy=0.724800, validation/loss=1.205078, validation/num_examples=50000
I0127 15:27:33.088901 140005313861376 logging_writer.py:48] [142800] global_step=142800, grad_norm=5.572998046875, loss=2.056300163269043
I0127 15:28:06.571705 140005288683264 logging_writer.py:48] [142900] global_step=142900, grad_norm=5.5050225257873535, loss=2.177795886993408
I0127 15:28:40.231965 140005313861376 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.059038162231445, loss=2.161945343017578
I0127 15:29:13.823712 140005288683264 logging_writer.py:48] [143100] global_step=143100, grad_norm=5.799712181091309, loss=2.0724105834960938
I0127 15:29:47.325146 140005313861376 logging_writer.py:48] [143200] global_step=143200, grad_norm=5.786951541900635, loss=2.1652748584747314
I0127 15:30:20.851732 140005288683264 logging_writer.py:48] [143300] global_step=143300, grad_norm=5.786190986633301, loss=2.136829137802124
I0127 15:30:54.466156 140005313861376 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.16949462890625, loss=2.0840816497802734
I0127 15:31:28.059997 140005288683264 logging_writer.py:48] [143500] global_step=143500, grad_norm=5.78898286819458, loss=2.097806215286255
I0127 15:32:01.591850 140005313861376 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.0376296043396, loss=2.073481559753418
I0127 15:32:35.144937 140005288683264 logging_writer.py:48] [143700] global_step=143700, grad_norm=5.854378700256348, loss=2.164470672607422
I0127 15:33:08.672527 140005313861376 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.415629863739014, loss=2.167245388031006
I0127 15:33:42.259311 140005288683264 logging_writer.py:48] [143900] global_step=143900, grad_norm=5.742250442504883, loss=2.1277780532836914
I0127 15:34:15.862491 140005313861376 logging_writer.py:48] [144000] global_step=144000, grad_norm=5.7472710609436035, loss=2.1500678062438965
I0127 15:34:49.522257 140005288683264 logging_writer.py:48] [144100] global_step=144100, grad_norm=5.701470375061035, loss=2.117753505706787
I0127 15:35:23.129766 140005313861376 logging_writer.py:48] [144200] global_step=144200, grad_norm=5.774714469909668, loss=2.107177972793579
I0127 15:35:38.729800 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:35:45.059275 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:35:53.707050 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:35:55.984375 140169137129280 submission_runner.py:408] Time since start: 50371.46s, 	Step: 144248, 	{'train/accuracy': 0.8232421875, 'train/loss': 0.7640501856803894, 'validation/accuracy': 0.7290999889373779, 'validation/loss': 1.1696761846542358, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7840397357940674, 'test/num_examples': 10000, 'score': 48526.44045686722, 'total_duration': 50371.45580005646, 'accumulated_submission_time': 48526.44045686722, 'accumulated_eval_time': 1835.3061537742615, 'accumulated_logging_time': 5.220117092132568}
I0127 15:35:56.026048 140004659541760 logging_writer.py:48] [144248] accumulated_eval_time=1835.306154, accumulated_logging_time=5.220117, accumulated_submission_time=48526.440457, global_step=144248, preemption_count=0, score=48526.440457, test/accuracy=0.607100, test/loss=1.784040, test/num_examples=10000, total_duration=50371.455800, train/accuracy=0.823242, train/loss=0.764050, validation/accuracy=0.729100, validation/loss=1.169676, validation/num_examples=50000
I0127 15:36:13.802701 140004667934464 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.396789073944092, loss=2.1765904426574707
I0127 15:36:47.282950 140004659541760 logging_writer.py:48] [144400] global_step=144400, grad_norm=6.039709091186523, loss=2.1050727367401123
I0127 15:37:20.869892 140004667934464 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.049261093139648, loss=2.1157116889953613
I0127 15:37:54.487651 140004659541760 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.062244892120361, loss=2.062450885772705
I0127 15:38:28.123471 140004667934464 logging_writer.py:48] [144700] global_step=144700, grad_norm=5.883344650268555, loss=2.1765756607055664
I0127 15:39:01.727184 140004659541760 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.498960494995117, loss=2.2131235599517822
I0127 15:39:35.354698 140004667934464 logging_writer.py:48] [144900] global_step=144900, grad_norm=5.538187503814697, loss=2.120582103729248
I0127 15:40:08.968624 140004659541760 logging_writer.py:48] [145000] global_step=145000, grad_norm=5.872467041015625, loss=2.1319074630737305
I0127 15:40:42.630131 140004667934464 logging_writer.py:48] [145100] global_step=145100, grad_norm=5.9802327156066895, loss=2.126854181289673
I0127 15:41:16.171862 140004659541760 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.121476650238037, loss=2.1266491413116455
I0127 15:41:49.726108 140004667934464 logging_writer.py:48] [145300] global_step=145300, grad_norm=5.840091705322266, loss=2.0749166011810303
I0127 15:42:23.303735 140004659541760 logging_writer.py:48] [145400] global_step=145400, grad_norm=5.739707946777344, loss=2.0578866004943848
I0127 15:42:56.922279 140004667934464 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.779468536376953, loss=2.1072537899017334
I0127 15:43:30.549188 140004659541760 logging_writer.py:48] [145600] global_step=145600, grad_norm=5.692925930023193, loss=2.019063949584961
I0127 15:44:04.188706 140004667934464 logging_writer.py:48] [145700] global_step=145700, grad_norm=5.601420879364014, loss=2.094963550567627
I0127 15:44:26.172374 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:44:32.430677 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:44:41.248193 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:44:43.552987 140169137129280 submission_runner.py:408] Time since start: 50899.02s, 	Step: 145767, 	{'train/accuracy': 0.8306760191917419, 'train/loss': 0.7338430881500244, 'validation/accuracy': 0.735040009021759, 'validation/loss': 1.146825909614563, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7603540420532227, 'test/num_examples': 10000, 'score': 49036.527978897095, 'total_duration': 50899.024411439896, 'accumulated_submission_time': 49036.527978897095, 'accumulated_eval_time': 1852.6867318153381, 'accumulated_logging_time': 5.2726123332977295}
I0127 15:44:43.596419 140004667934464 logging_writer.py:48] [145767] accumulated_eval_time=1852.686732, accumulated_logging_time=5.272612, accumulated_submission_time=49036.527979, global_step=145767, preemption_count=0, score=49036.527979, test/accuracy=0.612600, test/loss=1.760354, test/num_examples=10000, total_duration=50899.024411, train/accuracy=0.830676, train/loss=0.733843, validation/accuracy=0.735040, validation/loss=1.146826, validation/num_examples=50000
I0127 15:44:54.998098 140005305468672 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.322633743286133, loss=2.1465330123901367
I0127 15:45:28.498228 140004667934464 logging_writer.py:48] [145900] global_step=145900, grad_norm=5.854998588562012, loss=1.9798643589019775
I0127 15:46:02.079305 140005305468672 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.837796211242676, loss=2.1199657917022705
I0127 15:46:35.589741 140004667934464 logging_writer.py:48] [146100] global_step=146100, grad_norm=5.716549396514893, loss=2.1695969104766846
I0127 15:47:09.193633 140005305468672 logging_writer.py:48] [146200] global_step=146200, grad_norm=5.985003471374512, loss=2.111231803894043
I0127 15:47:42.685429 140004667934464 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.633581161499023, loss=2.085423231124878
I0127 15:48:16.226858 140005305468672 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.818362236022949, loss=2.110506296157837
I0127 15:48:49.830538 140004667934464 logging_writer.py:48] [146500] global_step=146500, grad_norm=5.929258823394775, loss=2.0752739906311035
I0127 15:49:23.424113 140005305468672 logging_writer.py:48] [146600] global_step=146600, grad_norm=6.04838228225708, loss=2.115384578704834
I0127 15:49:56.940504 140004667934464 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.190461158752441, loss=2.075779438018799
I0127 15:50:30.456039 140005305468672 logging_writer.py:48] [146800] global_step=146800, grad_norm=5.9727983474731445, loss=2.087139129638672
I0127 15:51:04.031501 140004667934464 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.95020866394043, loss=2.1523492336273193
I0127 15:51:37.619495 140005305468672 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.624789237976074, loss=2.1227595806121826
I0127 15:52:11.234361 140004667934464 logging_writer.py:48] [147100] global_step=147100, grad_norm=6.249555587768555, loss=2.150491714477539
I0127 15:52:44.856755 140005305468672 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.166158676147461, loss=2.0482332706451416
I0127 15:53:13.580948 140169137129280 spec.py:321] Evaluating on the training split.
I0127 15:53:19.922447 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 15:53:28.535123 140169137129280 spec.py:349] Evaluating on the test split.
I0127 15:53:30.841505 140169137129280 submission_runner.py:408] Time since start: 51426.31s, 	Step: 147287, 	{'train/accuracy': 0.8342036008834839, 'train/loss': 0.7212420701980591, 'validation/accuracy': 0.7377399802207947, 'validation/loss': 1.13205885887146, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7458066940307617, 'test/num_examples': 10000, 'score': 49546.45242190361, 'total_duration': 51426.31285953522, 'accumulated_submission_time': 49546.45242190361, 'accumulated_eval_time': 1869.9471807479858, 'accumulated_logging_time': 5.328448534011841}
I0127 15:53:30.885012 140005288683264 logging_writer.py:48] [147287] accumulated_eval_time=1869.947181, accumulated_logging_time=5.328449, accumulated_submission_time=49546.452422, global_step=147287, preemption_count=0, score=49546.452422, test/accuracy=0.618400, test/loss=1.745807, test/num_examples=10000, total_duration=51426.312860, train/accuracy=0.834204, train/loss=0.721242, validation/accuracy=0.737740, validation/loss=1.132059, validation/num_examples=50000
I0127 15:53:35.585908 140005297075968 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.046776294708252, loss=2.1111984252929688
I0127 15:54:09.158195 140005288683264 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.333241939544678, loss=2.166536331176758
I0127 15:54:42.771450 140005297075968 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.1882405281066895, loss=2.099470376968384
I0127 15:55:16.363945 140005288683264 logging_writer.py:48] [147600] global_step=147600, grad_norm=5.984386920928955, loss=2.031374931335449
I0127 15:55:49.988089 140005297075968 logging_writer.py:48] [147700] global_step=147700, grad_norm=5.786055564880371, loss=2.0833425521850586
I0127 15:56:23.615061 140005288683264 logging_writer.py:48] [147800] global_step=147800, grad_norm=5.937836647033691, loss=2.0485451221466064
I0127 15:56:57.236471 140005297075968 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.331355094909668, loss=2.0850141048431396
I0127 15:57:30.873923 140005288683264 logging_writer.py:48] [148000] global_step=148000, grad_norm=6.226876258850098, loss=2.0525963306427
I0127 15:58:04.481909 140005297075968 logging_writer.py:48] [148100] global_step=148100, grad_norm=5.995500087738037, loss=2.0905075073242188
I0127 15:58:38.068752 140005288683264 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.147116184234619, loss=2.0572943687438965
I0127 15:59:11.717528 140005297075968 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.396855354309082, loss=2.0606021881103516
I0127 15:59:45.317362 140005288683264 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.121328830718994, loss=2.106548309326172
I0127 16:00:18.931419 140005297075968 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.074504375457764, loss=2.045762777328491
I0127 16:00:52.501685 140005288683264 logging_writer.py:48] [148600] global_step=148600, grad_norm=5.935618877410889, loss=2.150250196456909
I0127 16:01:26.065521 140005297075968 logging_writer.py:48] [148700] global_step=148700, grad_norm=5.99802303314209, loss=2.069209337234497
I0127 16:01:59.579545 140005288683264 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.050649166107178, loss=2.1315081119537354
I0127 16:02:01.065696 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:02:07.347942 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:02:15.948623 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:02:18.276196 140169137129280 submission_runner.py:408] Time since start: 51953.75s, 	Step: 148806, 	{'train/accuracy': 0.8618662357330322, 'train/loss': 0.6142693758010864, 'validation/accuracy': 0.7396999597549438, 'validation/loss': 1.1240257024765015, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.7484245300292969, 'test/num_examples': 10000, 'score': 50056.575719833374, 'total_duration': 51953.74762201309, 'accumulated_submission_time': 50056.575719833374, 'accumulated_eval_time': 1887.1576430797577, 'accumulated_logging_time': 5.3819334506988525}
I0127 16:02:18.318871 140004667934464 logging_writer.py:48] [148806] accumulated_eval_time=1887.157643, accumulated_logging_time=5.381933, accumulated_submission_time=50056.575720, global_step=148806, preemption_count=0, score=50056.575720, test/accuracy=0.617800, test/loss=1.748425, test/num_examples=10000, total_duration=51953.747622, train/accuracy=0.861866, train/loss=0.614269, validation/accuracy=0.739700, validation/loss=1.124026, validation/num_examples=50000
I0127 16:02:50.187921 140004676327168 logging_writer.py:48] [148900] global_step=148900, grad_norm=6.053173542022705, loss=2.042877674102783
I0127 16:03:23.700039 140004667934464 logging_writer.py:48] [149000] global_step=149000, grad_norm=5.781563758850098, loss=2.043689250946045
I0127 16:03:57.239748 140004676327168 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.173334121704102, loss=1.9993785619735718
I0127 16:04:30.871348 140004667934464 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.409743309020996, loss=2.014833688735962
I0127 16:05:04.475621 140004676327168 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.502058029174805, loss=2.0804495811462402
I0127 16:05:38.090725 140004667934464 logging_writer.py:48] [149400] global_step=149400, grad_norm=6.479411602020264, loss=2.1006150245666504
I0127 16:06:11.692339 140004676327168 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.654972076416016, loss=2.0853006839752197
I0127 16:06:45.304355 140004667934464 logging_writer.py:48] [149600] global_step=149600, grad_norm=6.271857738494873, loss=2.1160717010498047
I0127 16:07:18.918315 140004676327168 logging_writer.py:48] [149700] global_step=149700, grad_norm=6.360131740570068, loss=2.0597708225250244
I0127 16:07:52.542036 140004667934464 logging_writer.py:48] [149800] global_step=149800, grad_norm=6.906918525695801, loss=2.1076443195343018
I0127 16:08:26.135531 140004676327168 logging_writer.py:48] [149900] global_step=149900, grad_norm=6.41662073135376, loss=2.0676608085632324
I0127 16:08:59.691449 140004667934464 logging_writer.py:48] [150000] global_step=150000, grad_norm=5.996240615844727, loss=2.104149580001831
I0127 16:09:33.312739 140004676327168 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.059481620788574, loss=2.017429828643799
I0127 16:10:06.931995 140004667934464 logging_writer.py:48] [150200] global_step=150200, grad_norm=7.308797836303711, loss=2.0933613777160645
I0127 16:10:40.548728 140004676327168 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.728309631347656, loss=2.1388814449310303
I0127 16:10:48.421816 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:10:54.616182 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:11:03.351774 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:11:05.717446 140169137129280 submission_runner.py:408] Time since start: 52481.19s, 	Step: 150325, 	{'train/accuracy': 0.8557676672935486, 'train/loss': 0.6431095004081726, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.1167047023773193, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7365473508834839, 'test/num_examples': 10000, 'score': 50566.62089514732, 'total_duration': 52481.188838005066, 'accumulated_submission_time': 50566.62089514732, 'accumulated_eval_time': 1904.453207731247, 'accumulated_logging_time': 5.434524297714233}
I0127 16:11:05.761564 140004676327168 logging_writer.py:48] [150325] accumulated_eval_time=1904.453208, accumulated_logging_time=5.434524, accumulated_submission_time=50566.620895, global_step=150325, preemption_count=0, score=50566.620895, test/accuracy=0.620300, test/loss=1.736547, test/num_examples=10000, total_duration=52481.188838, train/accuracy=0.855768, train/loss=0.643110, validation/accuracy=0.742440, validation/loss=1.116705, validation/num_examples=50000
I0127 16:11:31.286485 140005297075968 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.238796710968018, loss=2.053725481033325
I0127 16:12:04.879908 140004676327168 logging_writer.py:48] [150500] global_step=150500, grad_norm=5.765970230102539, loss=2.055967330932617
I0127 16:12:38.497712 140005297075968 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.141671180725098, loss=2.0716676712036133
I0127 16:13:12.108976 140004676327168 logging_writer.py:48] [150700] global_step=150700, grad_norm=5.915286540985107, loss=2.0202488899230957
I0127 16:13:45.729927 140005297075968 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.4578537940979, loss=2.0550217628479004
I0127 16:14:19.351519 140004676327168 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.516323566436768, loss=2.0591933727264404
I0127 16:14:52.965197 140005297075968 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.977799892425537, loss=1.9984536170959473
I0127 16:15:26.585855 140004676327168 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.706811904907227, loss=1.9960763454437256
I0127 16:16:00.205678 140005297075968 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.5938310623168945, loss=1.9992046356201172
I0127 16:16:33.789803 140004676327168 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.535370349884033, loss=2.000124454498291
I0127 16:17:07.398125 140005297075968 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.43615198135376, loss=2.0443789958953857
I0127 16:17:41.058898 140004676327168 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.774478912353516, loss=2.066502571105957
I0127 16:18:14.654068 140005297075968 logging_writer.py:48] [151600] global_step=151600, grad_norm=7.4791765213012695, loss=2.1287503242492676
I0127 16:18:48.205894 140004676327168 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.0463457107543945, loss=2.035046339035034
I0127 16:19:21.761386 140005297075968 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.609022617340088, loss=2.020387649536133
I0127 16:19:35.969950 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:19:42.218968 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:19:50.968177 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:19:53.269618 140169137129280 submission_runner.py:408] Time since start: 53008.74s, 	Step: 151844, 	{'train/accuracy': 0.8517019748687744, 'train/loss': 0.6508660316467285, 'validation/accuracy': 0.740399956703186, 'validation/loss': 1.1089304685592651, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.7190169095993042, 'test/num_examples': 10000, 'score': 51076.7715549469, 'total_duration': 53008.741042375565, 'accumulated_submission_time': 51076.7715549469, 'accumulated_eval_time': 1921.7528405189514, 'accumulated_logging_time': 5.488680601119995}
I0127 16:19:53.312558 140004667934464 logging_writer.py:48] [151844] accumulated_eval_time=1921.752841, accumulated_logging_time=5.488681, accumulated_submission_time=51076.771555, global_step=151844, preemption_count=0, score=51076.771555, test/accuracy=0.620600, test/loss=1.719017, test/num_examples=10000, total_duration=53008.741042, train/accuracy=0.851702, train/loss=0.650866, validation/accuracy=0.740400, validation/loss=1.108930, validation/num_examples=50000
I0127 16:20:12.389872 140004676327168 logging_writer.py:48] [151900] global_step=151900, grad_norm=6.511477947235107, loss=1.9877490997314453
I0127 16:20:45.903840 140004667934464 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.499239921569824, loss=2.0040395259857178
I0127 16:21:19.477198 140004676327168 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.616381645202637, loss=2.0152580738067627
I0127 16:21:53.026490 140004667934464 logging_writer.py:48] [152200] global_step=152200, grad_norm=6.042445182800293, loss=2.0029537677764893
I0127 16:22:26.533771 140004676327168 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.160984516143799, loss=2.000288724899292
I0127 16:23:00.101560 140004667934464 logging_writer.py:48] [152400] global_step=152400, grad_norm=6.77763032913208, loss=1.9715335369110107
I0127 16:23:33.747360 140004676327168 logging_writer.py:48] [152500] global_step=152500, grad_norm=5.819990158081055, loss=1.9618730545043945
I0127 16:24:07.331826 140004667934464 logging_writer.py:48] [152600] global_step=152600, grad_norm=6.355637550354004, loss=2.068544864654541
I0127 16:24:40.957170 140004676327168 logging_writer.py:48] [152700] global_step=152700, grad_norm=5.990957736968994, loss=2.0197176933288574
I0127 16:25:14.570654 140004667934464 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.851266860961914, loss=2.049126386642456
I0127 16:25:48.190458 140004676327168 logging_writer.py:48] [152900] global_step=152900, grad_norm=6.239209175109863, loss=2.038175344467163
I0127 16:26:21.798651 140004667934464 logging_writer.py:48] [153000] global_step=153000, grad_norm=6.721208095550537, loss=1.9991672039031982
I0127 16:26:55.406829 140004676327168 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.71260929107666, loss=2.0345706939697266
I0127 16:27:29.013281 140004667934464 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.8767523765563965, loss=2.007589817047119
I0127 16:28:02.634690 140004676327168 logging_writer.py:48] [153300] global_step=153300, grad_norm=6.4836812019348145, loss=2.117267608642578
I0127 16:28:23.281605 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:28:30.318838 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:28:39.040535 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:28:41.345576 140169137129280 submission_runner.py:408] Time since start: 53536.82s, 	Step: 153363, 	{'train/accuracy': 0.8570631146430969, 'train/loss': 0.6400169134140015, 'validation/accuracy': 0.7451599836349487, 'validation/loss': 1.0995455980300903, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.702033519744873, 'test/num_examples': 10000, 'score': 51586.682436704636, 'total_duration': 53536.81693482399, 'accumulated_submission_time': 51586.682436704636, 'accumulated_eval_time': 1939.816710472107, 'accumulated_logging_time': 5.542165279388428}
I0127 16:28:41.387346 140004667934464 logging_writer.py:48] [153363] accumulated_eval_time=1939.816710, accumulated_logging_time=5.542165, accumulated_submission_time=51586.682437, global_step=153363, preemption_count=0, score=51586.682437, test/accuracy=0.627600, test/loss=1.702034, test/num_examples=10000, total_duration=53536.816935, train/accuracy=0.857063, train/loss=0.640017, validation/accuracy=0.745160, validation/loss=1.099546, validation/num_examples=50000
I0127 16:28:54.107400 140005297075968 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.277578830718994, loss=2.0033304691314697
I0127 16:29:27.650500 140004667934464 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.102159023284912, loss=2.0582499504089355
I0127 16:30:01.267088 140005297075968 logging_writer.py:48] [153600] global_step=153600, grad_norm=6.334193706512451, loss=1.9724169969558716
I0127 16:30:34.869869 140004667934464 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.325296878814697, loss=2.0149033069610596
I0127 16:31:08.480546 140005297075968 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.303059101104736, loss=2.0292842388153076
I0127 16:31:42.092555 140004667934464 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.125492095947266, loss=1.9776585102081299
I0127 16:32:15.676237 140005297075968 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.563223361968994, loss=1.9991648197174072
I0127 16:32:49.306266 140004667934464 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.540890693664551, loss=2.039656400680542
I0127 16:33:22.932328 140005297075968 logging_writer.py:48] [154200] global_step=154200, grad_norm=7.179117679595947, loss=2.0982542037963867
I0127 16:33:56.546294 140004667934464 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.7904558181762695, loss=2.009005546569824
I0127 16:34:30.104261 140005297075968 logging_writer.py:48] [154400] global_step=154400, grad_norm=6.8585429191589355, loss=2.0005664825439453
I0127 16:35:03.673296 140004667934464 logging_writer.py:48] [154500] global_step=154500, grad_norm=6.5489115715026855, loss=2.011293649673462
I0127 16:35:37.252918 140005297075968 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.303144931793213, loss=2.023312568664551
I0127 16:36:10.873235 140004667934464 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.509035587310791, loss=2.072481393814087
I0127 16:36:44.510984 140005297075968 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.139081954956055, loss=1.965162992477417
I0127 16:37:11.539167 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:37:17.878902 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:37:26.409276 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:37:28.706820 140169137129280 submission_runner.py:408] Time since start: 54064.18s, 	Step: 154882, 	{'train/accuracy': 0.8557876348495483, 'train/loss': 0.6262343525886536, 'validation/accuracy': 0.7460199594497681, 'validation/loss': 1.0894845724105835, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6885766983032227, 'test/num_examples': 10000, 'score': 52096.776774168015, 'total_duration': 54064.17822790146, 'accumulated_submission_time': 52096.776774168015, 'accumulated_eval_time': 1956.984309911728, 'accumulated_logging_time': 5.594337701797485}
I0127 16:37:28.750103 140004667934464 logging_writer.py:48] [154882] accumulated_eval_time=1956.984310, accumulated_logging_time=5.594338, accumulated_submission_time=52096.776774, global_step=154882, preemption_count=0, score=52096.776774, test/accuracy=0.628300, test/loss=1.688577, test/num_examples=10000, total_duration=54064.178228, train/accuracy=0.855788, train/loss=0.626234, validation/accuracy=0.746020, validation/loss=1.089485, validation/num_examples=50000
I0127 16:37:35.112794 140004676327168 logging_writer.py:48] [154900] global_step=154900, grad_norm=6.585175514221191, loss=2.0127222537994385
I0127 16:38:08.673915 140004667934464 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.3553314208984375, loss=2.003922462463379
I0127 16:38:42.247661 140004676327168 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.067166805267334, loss=2.063815116882324
I0127 16:39:15.775282 140004667934464 logging_writer.py:48] [155200] global_step=155200, grad_norm=7.31401252746582, loss=2.0402448177337646
I0127 16:39:49.300910 140004676327168 logging_writer.py:48] [155300] global_step=155300, grad_norm=6.112193584442139, loss=1.9672108888626099
I0127 16:40:22.875998 140004667934464 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.7249836921691895, loss=2.007883071899414
I0127 16:40:56.499397 140004676327168 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.002647399902344, loss=1.9420510530471802
I0127 16:41:30.127148 140004667934464 logging_writer.py:48] [155600] global_step=155600, grad_norm=6.4703474044799805, loss=1.9605554342269897
I0127 16:42:03.734579 140004676327168 logging_writer.py:48] [155700] global_step=155700, grad_norm=6.663799285888672, loss=2.012897253036499
I0127 16:42:37.322644 140004667934464 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.3250346183776855, loss=1.982367753982544
I0127 16:43:10.913173 140004676327168 logging_writer.py:48] [155900] global_step=155900, grad_norm=6.988900184631348, loss=1.9908318519592285
I0127 16:43:44.530130 140004667934464 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.129820346832275, loss=2.0170443058013916
I0127 16:44:18.143010 140004676327168 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.561590671539307, loss=1.9986789226531982
I0127 16:44:51.775836 140004667934464 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.213984966278076, loss=2.013740301132202
I0127 16:45:25.377208 140004676327168 logging_writer.py:48] [156300] global_step=156300, grad_norm=6.513552188873291, loss=1.9813859462738037
I0127 16:45:59.005212 140004667934464 logging_writer.py:48] [156400] global_step=156400, grad_norm=6.678589820861816, loss=1.9685420989990234
I0127 16:45:59.012952 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:46:05.272227 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:46:14.001741 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:46:16.626271 140169137129280 submission_runner.py:408] Time since start: 54592.10s, 	Step: 156401, 	{'train/accuracy': 0.861348032951355, 'train/loss': 0.6097630858421326, 'validation/accuracy': 0.7497199773788452, 'validation/loss': 1.077207088470459, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.6781634092330933, 'test/num_examples': 10000, 'score': 52606.98158121109, 'total_duration': 54592.09770488739, 'accumulated_submission_time': 52606.98158121109, 'accumulated_eval_time': 1974.5975804328918, 'accumulated_logging_time': 5.648033142089844}
I0127 16:46:16.661129 140005313861376 logging_writer.py:48] [156401] accumulated_eval_time=1974.597580, accumulated_logging_time=5.648033, accumulated_submission_time=52606.981581, global_step=156401, preemption_count=0, score=52606.981581, test/accuracy=0.630100, test/loss=1.678163, test/num_examples=10000, total_duration=54592.097705, train/accuracy=0.861348, train/loss=0.609763, validation/accuracy=0.749720, validation/loss=1.077207, validation/num_examples=50000
I0127 16:46:50.235292 140005322254080 logging_writer.py:48] [156500] global_step=156500, grad_norm=6.839601039886475, loss=1.9042916297912598
I0127 16:47:23.843287 140005313861376 logging_writer.py:48] [156600] global_step=156600, grad_norm=6.504554748535156, loss=1.920516014099121
I0127 16:47:57.469957 140005322254080 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.804584980010986, loss=1.905110239982605
I0127 16:48:31.033653 140005313861376 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.147722244262695, loss=1.9692963361740112
I0127 16:49:04.651788 140005322254080 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.721579551696777, loss=1.9688467979431152
I0127 16:49:38.261744 140005313861376 logging_writer.py:48] [157000] global_step=157000, grad_norm=7.956109523773193, loss=2.0737216472625732
I0127 16:50:11.865268 140005322254080 logging_writer.py:48] [157100] global_step=157100, grad_norm=6.4061808586120605, loss=1.9250054359436035
I0127 16:50:45.491382 140005313861376 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.201020240783691, loss=1.9659655094146729
I0127 16:51:19.085449 140005322254080 logging_writer.py:48] [157300] global_step=157300, grad_norm=6.99298620223999, loss=2.0175094604492188
I0127 16:51:52.707439 140005313861376 logging_writer.py:48] [157400] global_step=157400, grad_norm=6.896511077880859, loss=1.9337413311004639
I0127 16:52:26.331504 140005322254080 logging_writer.py:48] [157500] global_step=157500, grad_norm=6.568652153015137, loss=1.8749017715454102
I0127 16:52:59.946358 140005313861376 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.03890323638916, loss=1.9975802898406982
I0127 16:53:33.475931 140005322254080 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.580392837524414, loss=1.8888192176818848
I0127 16:54:07.123558 140005313861376 logging_writer.py:48] [157800] global_step=157800, grad_norm=6.486025333404541, loss=2.0000367164611816
I0127 16:54:40.755830 140005322254080 logging_writer.py:48] [157900] global_step=157900, grad_norm=6.473326683044434, loss=1.9741185903549194
I0127 16:54:46.950809 140169137129280 spec.py:321] Evaluating on the training split.
I0127 16:54:53.216259 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 16:55:01.841832 140169137129280 spec.py:349] Evaluating on the test split.
I0127 16:55:04.150464 140169137129280 submission_runner.py:408] Time since start: 55119.62s, 	Step: 157920, 	{'train/accuracy': 0.8819953799247742, 'train/loss': 0.5402708649635315, 'validation/accuracy': 0.7523799538612366, 'validation/loss': 1.0711145401000977, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.692663550376892, 'test/num_examples': 10000, 'score': 53117.21468949318, 'total_duration': 55119.62186551094, 'accumulated_submission_time': 53117.21468949318, 'accumulated_eval_time': 1991.7971727848053, 'accumulated_logging_time': 5.691704273223877}
I0127 16:55:04.196722 140004667934464 logging_writer.py:48] [157920] accumulated_eval_time=1991.797173, accumulated_logging_time=5.691704, accumulated_submission_time=53117.214689, global_step=157920, preemption_count=0, score=53117.214689, test/accuracy=0.631600, test/loss=1.692664, test/num_examples=10000, total_duration=55119.621866, train/accuracy=0.881995, train/loss=0.540271, validation/accuracy=0.752380, validation/loss=1.071115, validation/num_examples=50000
I0127 16:55:31.342286 140004676327168 logging_writer.py:48] [158000] global_step=158000, grad_norm=6.68543004989624, loss=2.019589900970459
I0127 16:56:04.876009 140004667934464 logging_writer.py:48] [158100] global_step=158100, grad_norm=6.772814750671387, loss=1.9575051069259644
I0127 16:56:38.451309 140004676327168 logging_writer.py:48] [158200] global_step=158200, grad_norm=6.909254550933838, loss=2.026606321334839
I0127 16:57:12.038761 140004667934464 logging_writer.py:48] [158300] global_step=158300, grad_norm=6.393918037414551, loss=1.9310382604599
I0127 16:57:45.679909 140004676327168 logging_writer.py:48] [158400] global_step=158400, grad_norm=6.307842254638672, loss=1.9187179803848267
I0127 16:58:19.302624 140004667934464 logging_writer.py:48] [158500] global_step=158500, grad_norm=6.474617958068848, loss=1.9230542182922363
I0127 16:58:52.931133 140004676327168 logging_writer.py:48] [158600] global_step=158600, grad_norm=6.397806644439697, loss=1.9356635808944702
I0127 16:59:26.550237 140004667934464 logging_writer.py:48] [158700] global_step=158700, grad_norm=6.712446212768555, loss=1.9767614603042603
I0127 17:00:00.218988 140004676327168 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.193053245544434, loss=1.9829777479171753
I0127 17:00:33.775636 140004667934464 logging_writer.py:48] [158900] global_step=158900, grad_norm=6.849817752838135, loss=2.053050994873047
I0127 17:01:07.338006 140004676327168 logging_writer.py:48] [159000] global_step=159000, grad_norm=6.842329025268555, loss=1.9571248292922974
I0127 17:01:40.834435 140004667934464 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.562758922576904, loss=2.0069472789764404
I0127 17:02:14.376724 140004676327168 logging_writer.py:48] [159200] global_step=159200, grad_norm=6.763555526733398, loss=1.9680407047271729
I0127 17:02:47.929762 140004667934464 logging_writer.py:48] [159300] global_step=159300, grad_norm=6.535273551940918, loss=1.9132084846496582
I0127 17:03:21.536364 140004676327168 logging_writer.py:48] [159400] global_step=159400, grad_norm=6.358316421508789, loss=1.9536699056625366
I0127 17:03:34.447170 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:03:40.711261 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:03:49.446760 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:03:51.810360 140169137129280 submission_runner.py:408] Time since start: 55647.28s, 	Step: 159440, 	{'train/accuracy': 0.8802216053009033, 'train/loss': 0.5432620644569397, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.059921383857727, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.6828455924987793, 'test/num_examples': 10000, 'score': 53627.40317606926, 'total_duration': 55647.28178501129, 'accumulated_submission_time': 53627.40317606926, 'accumulated_eval_time': 2009.1603240966797, 'accumulated_logging_time': 5.752228260040283}
I0127 17:03:51.857242 140005305468672 logging_writer.py:48] [159440] accumulated_eval_time=2009.160324, accumulated_logging_time=5.752228, accumulated_submission_time=53627.403176, global_step=159440, preemption_count=0, score=53627.403176, test/accuracy=0.630700, test/loss=1.682846, test/num_examples=10000, total_duration=55647.281785, train/accuracy=0.880222, train/loss=0.543262, validation/accuracy=0.754200, validation/loss=1.059921, validation/num_examples=50000
I0127 17:04:12.285926 140005313861376 logging_writer.py:48] [159500] global_step=159500, grad_norm=6.828410625457764, loss=1.9426710605621338
I0127 17:04:45.837947 140005305468672 logging_writer.py:48] [159600] global_step=159600, grad_norm=6.982237339019775, loss=1.9236068725585938
I0127 17:05:19.460108 140005313861376 logging_writer.py:48] [159700] global_step=159700, grad_norm=6.934784889221191, loss=1.9333293437957764
I0127 17:05:53.079367 140005305468672 logging_writer.py:48] [159800] global_step=159800, grad_norm=6.757965564727783, loss=1.9408625364303589
I0127 17:06:26.685566 140005313861376 logging_writer.py:48] [159900] global_step=159900, grad_norm=6.859017372131348, loss=1.903294563293457
I0127 17:07:00.288415 140005305468672 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.300166606903076, loss=2.0017249584198
I0127 17:07:33.910004 140005313861376 logging_writer.py:48] [160100] global_step=160100, grad_norm=6.357571125030518, loss=1.896769404411316
I0127 17:08:07.525151 140005305468672 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.027849197387695, loss=2.0715794563293457
I0127 17:08:41.153130 140005313861376 logging_writer.py:48] [160300] global_step=160300, grad_norm=6.986272811889648, loss=1.9807136058807373
I0127 17:09:14.769656 140005305468672 logging_writer.py:48] [160400] global_step=160400, grad_norm=6.667827129364014, loss=1.9202258586883545
I0127 17:09:48.385265 140005313861376 logging_writer.py:48] [160500] global_step=160500, grad_norm=7.234167575836182, loss=1.9341005086898804
I0127 17:10:22.015152 140005305468672 logging_writer.py:48] [160600] global_step=160600, grad_norm=6.541723251342773, loss=1.9681456089019775
I0127 17:10:55.626657 140005313861376 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.250298023223877, loss=1.9989278316497803
I0127 17:11:29.164999 140005305468672 logging_writer.py:48] [160800] global_step=160800, grad_norm=6.707102298736572, loss=1.9652537107467651
I0127 17:12:02.758964 140005313861376 logging_writer.py:48] [160900] global_step=160900, grad_norm=6.534915447235107, loss=1.8974721431732178
I0127 17:12:22.039862 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:12:28.333530 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:12:36.935813 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:12:39.216048 140169137129280 submission_runner.py:408] Time since start: 56174.69s, 	Step: 160959, 	{'train/accuracy': 0.8743024468421936, 'train/loss': 0.5627148747444153, 'validation/accuracy': 0.7519999742507935, 'validation/loss': 1.0669498443603516, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.689836025238037, 'test/num_examples': 10000, 'score': 54137.527206897736, 'total_duration': 56174.68740797043, 'accumulated_submission_time': 54137.527206897736, 'accumulated_eval_time': 2026.33642411232, 'accumulated_logging_time': 5.809492588043213}
I0127 17:12:39.262673 140004676327168 logging_writer.py:48] [160959] accumulated_eval_time=2026.336424, accumulated_logging_time=5.809493, accumulated_submission_time=54137.527207, global_step=160959, preemption_count=0, score=54137.527207, test/accuracy=0.632600, test/loss=1.689836, test/num_examples=10000, total_duration=56174.687408, train/accuracy=0.874302, train/loss=0.562715, validation/accuracy=0.752000, validation/loss=1.066950, validation/num_examples=50000
I0127 17:12:53.331612 140005288683264 logging_writer.py:48] [161000] global_step=161000, grad_norm=6.501001834869385, loss=1.8407540321350098
I0127 17:13:26.834009 140004676327168 logging_writer.py:48] [161100] global_step=161100, grad_norm=6.788576126098633, loss=1.9274358749389648
I0127 17:14:00.408415 140005288683264 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.3991522789001465, loss=1.9564669132232666
I0127 17:14:33.952058 140004676327168 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.01539945602417, loss=1.9657716751098633
I0127 17:15:07.471065 140005288683264 logging_writer.py:48] [161400] global_step=161400, grad_norm=7.0055036544799805, loss=1.8960834741592407
I0127 17:15:41.007583 140004676327168 logging_writer.py:48] [161500] global_step=161500, grad_norm=6.479722499847412, loss=1.8902842998504639
I0127 17:16:14.562844 140005288683264 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.277195930480957, loss=1.8955553770065308
I0127 17:16:48.155893 140004676327168 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.480453014373779, loss=1.9799453020095825
I0127 17:17:21.775242 140005288683264 logging_writer.py:48] [161800] global_step=161800, grad_norm=7.309668064117432, loss=1.9725582599639893
I0127 17:17:55.363537 140004676327168 logging_writer.py:48] [161900] global_step=161900, grad_norm=6.673918724060059, loss=1.9103940725326538
I0127 17:18:28.996613 140005288683264 logging_writer.py:48] [162000] global_step=162000, grad_norm=7.251643180847168, loss=1.9345319271087646
I0127 17:19:02.572890 140004676327168 logging_writer.py:48] [162100] global_step=162100, grad_norm=6.988607883453369, loss=1.8854167461395264
I0127 17:19:36.113658 140005288683264 logging_writer.py:48] [162200] global_step=162200, grad_norm=6.762894153594971, loss=1.938431978225708
I0127 17:20:09.648811 140004676327168 logging_writer.py:48] [162300] global_step=162300, grad_norm=6.782808780670166, loss=1.9111205339431763
I0127 17:20:43.200562 140005288683264 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.468452453613281, loss=1.9272183179855347
I0127 17:21:09.545009 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:21:15.829292 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:21:24.341166 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:21:26.707544 140169137129280 submission_runner.py:408] Time since start: 56702.18s, 	Step: 162480, 	{'train/accuracy': 0.8794642686843872, 'train/loss': 0.5540726184844971, 'validation/accuracy': 0.7535199522972107, 'validation/loss': 1.0689977407455444, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.671195149421692, 'test/num_examples': 10000, 'score': 54647.75112223625, 'total_duration': 56702.178926467896, 'accumulated_submission_time': 54647.75112223625, 'accumulated_eval_time': 2043.4988808631897, 'accumulated_logging_time': 5.86617112159729}
I0127 17:21:26.751503 140004667934464 logging_writer.py:48] [162480] accumulated_eval_time=2043.498881, accumulated_logging_time=5.866171, accumulated_submission_time=54647.751122, global_step=162480, preemption_count=0, score=54647.751122, test/accuracy=0.635500, test/loss=1.671195, test/num_examples=10000, total_duration=56702.178926, train/accuracy=0.879464, train/loss=0.554073, validation/accuracy=0.753520, validation/loss=1.068998, validation/num_examples=50000
I0127 17:21:33.809272 140005305468672 logging_writer.py:48] [162500] global_step=162500, grad_norm=6.9772491455078125, loss=1.9651036262512207
I0127 17:22:07.314383 140004667934464 logging_writer.py:48] [162600] global_step=162600, grad_norm=6.335372447967529, loss=1.8720513582229614
I0127 17:22:40.890973 140005305468672 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.612234592437744, loss=1.9564883708953857
I0127 17:23:14.527003 140004667934464 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.454262733459473, loss=1.9175747632980347
I0127 17:23:48.158134 140005305468672 logging_writer.py:48] [162900] global_step=162900, grad_norm=7.692343711853027, loss=1.934554934501648
I0127 17:24:21.804020 140004667934464 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.318897247314453, loss=1.9277184009552002
I0127 17:24:55.401757 140005305468672 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.421398162841797, loss=1.8757022619247437
I0127 17:25:29.011991 140004667934464 logging_writer.py:48] [163200] global_step=163200, grad_norm=6.550403594970703, loss=1.8524832725524902
I0127 17:26:02.538455 140005305468672 logging_writer.py:48] [163300] global_step=163300, grad_norm=7.662947177886963, loss=1.8840818405151367
I0127 17:26:36.049201 140004667934464 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.314350128173828, loss=1.9670933485031128
I0127 17:27:09.592751 140005305468672 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.087873935699463, loss=1.8777267932891846
I0127 17:27:43.182050 140004667934464 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.2327399253845215, loss=1.8940424919128418
I0127 17:28:16.792618 140005305468672 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.503080368041992, loss=1.893351435661316
I0127 17:28:50.420869 140004667934464 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.483837604522705, loss=1.8813689947128296
I0127 17:29:24.049261 140005305468672 logging_writer.py:48] [163900] global_step=163900, grad_norm=6.99398136138916, loss=1.8842629194259644
I0127 17:29:56.794561 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:30:03.135140 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:30:11.882940 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:30:14.224500 140169137129280 submission_runner.py:408] Time since start: 57229.70s, 	Step: 163999, 	{'train/accuracy': 0.881257951259613, 'train/loss': 0.5390360951423645, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0575190782546997, 'validation/num_examples': 50000, 'test/accuracy': 0.6357000470161438, 'test/loss': 1.6739016771316528, 'test/num_examples': 10000, 'score': 55157.73432970047, 'total_duration': 57229.695892095566, 'accumulated_submission_time': 55157.73432970047, 'accumulated_eval_time': 2060.928759098053, 'accumulated_logging_time': 5.921685457229614}
I0127 17:30:14.280532 140004667934464 logging_writer.py:48] [163999] accumulated_eval_time=2060.928759, accumulated_logging_time=5.921685, accumulated_submission_time=55157.734330, global_step=163999, preemption_count=0, score=55157.734330, test/accuracy=0.635700, test/loss=1.673902, test/num_examples=10000, total_duration=57229.695892, train/accuracy=0.881258, train/loss=0.539036, validation/accuracy=0.756260, validation/loss=1.057519, validation/num_examples=50000
I0127 17:30:14.977063 140005288683264 logging_writer.py:48] [164000] global_step=164000, grad_norm=7.678018569946289, loss=1.9443615674972534
I0127 17:30:48.550239 140004667934464 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.861968517303467, loss=1.968153476715088
I0127 17:31:22.066973 140005288683264 logging_writer.py:48] [164200] global_step=164200, grad_norm=7.82295560836792, loss=1.958759069442749
I0127 17:31:55.619139 140004667934464 logging_writer.py:48] [164300] global_step=164300, grad_norm=6.803936004638672, loss=1.8367915153503418
I0127 17:32:29.133998 140005288683264 logging_writer.py:48] [164400] global_step=164400, grad_norm=7.415445327758789, loss=1.9254462718963623
I0127 17:33:02.718085 140004667934464 logging_writer.py:48] [164500] global_step=164500, grad_norm=6.784055233001709, loss=1.7746301889419556
I0127 17:33:36.296110 140005288683264 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.413942813873291, loss=1.8788626194000244
I0127 17:34:09.927098 140004667934464 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.079621315002441, loss=1.9661645889282227
I0127 17:34:43.557459 140005288683264 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.591150283813477, loss=1.9952112436294556
I0127 17:35:17.170464 140004667934464 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.243968486785889, loss=1.903101921081543
I0127 17:35:50.771804 140005288683264 logging_writer.py:48] [165000] global_step=165000, grad_norm=6.812582492828369, loss=1.8456676006317139
I0127 17:36:24.425123 140004667934464 logging_writer.py:48] [165100] global_step=165100, grad_norm=8.039652824401855, loss=1.9176656007766724
I0127 17:36:58.010638 140005288683264 logging_writer.py:48] [165200] global_step=165200, grad_norm=6.828432559967041, loss=1.792428970336914
I0127 17:37:31.622217 140004667934464 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.155533790588379, loss=1.9117825031280518
I0127 17:38:05.188264 140005288683264 logging_writer.py:48] [165400] global_step=165400, grad_norm=6.948283672332764, loss=1.923248052597046
I0127 17:38:38.745793 140004667934464 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.0987372398376465, loss=1.8739588260650635
I0127 17:38:44.274312 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:38:50.467885 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:38:59.246775 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:39:01.562512 140169137129280 submission_runner.py:408] Time since start: 57757.03s, 	Step: 165518, 	{'train/accuracy': 0.8997927308082581, 'train/loss': 0.4756694734096527, 'validation/accuracy': 0.7611799836158752, 'validation/loss': 1.0342700481414795, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.6526086330413818, 'test/num_examples': 10000, 'score': 55667.66546201706, 'total_duration': 57757.03380203247, 'accumulated_submission_time': 55667.66546201706, 'accumulated_eval_time': 2078.2167851924896, 'accumulated_logging_time': 5.992365121841431}
I0127 17:39:01.608714 140004676327168 logging_writer.py:48] [165518] accumulated_eval_time=2078.216785, accumulated_logging_time=5.992365, accumulated_submission_time=55667.665462, global_step=165518, preemption_count=0, score=55667.665462, test/accuracy=0.642000, test/loss=1.652609, test/num_examples=10000, total_duration=57757.033802, train/accuracy=0.899793, train/loss=0.475669, validation/accuracy=0.761180, validation/loss=1.034270, validation/num_examples=50000
I0127 17:39:29.449468 140005305468672 logging_writer.py:48] [165600] global_step=165600, grad_norm=7.939754486083984, loss=1.8358646631240845
I0127 17:40:02.961515 140004676327168 logging_writer.py:48] [165700] global_step=165700, grad_norm=7.29244327545166, loss=1.8444147109985352
I0127 17:40:36.515531 140005305468672 logging_writer.py:48] [165800] global_step=165800, grad_norm=6.701927661895752, loss=1.8824002742767334
I0127 17:41:10.117071 140004676327168 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.957178592681885, loss=1.8715927600860596
I0127 17:41:43.716751 140005305468672 logging_writer.py:48] [166000] global_step=166000, grad_norm=7.528191566467285, loss=1.9318974018096924
I0127 17:42:17.329586 140004676327168 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.609675407409668, loss=1.8420264720916748
I0127 17:42:50.957150 140005305468672 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.68148136138916, loss=1.9090821743011475
I0127 17:43:24.557565 140004676327168 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.4470295906066895, loss=1.8355250358581543
I0127 17:43:58.140516 140005305468672 logging_writer.py:48] [166400] global_step=166400, grad_norm=6.814377784729004, loss=1.8614557981491089
I0127 17:44:31.780370 140004676327168 logging_writer.py:48] [166500] global_step=166500, grad_norm=8.21694564819336, loss=1.8667680025100708
I0127 17:45:05.401144 140005305468672 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.058573246002197, loss=1.9076712131500244
I0127 17:45:39.008399 140004676327168 logging_writer.py:48] [166700] global_step=166700, grad_norm=7.70475959777832, loss=1.8787899017333984
I0127 17:46:12.591706 140005305468672 logging_writer.py:48] [166800] global_step=166800, grad_norm=7.683281421661377, loss=1.8854037523269653
I0127 17:46:46.223489 140004676327168 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.6131391525268555, loss=1.8400934934616089
I0127 17:47:19.849853 140005305468672 logging_writer.py:48] [167000] global_step=167000, grad_norm=8.24284839630127, loss=1.881501317024231
I0127 17:47:31.761093 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:47:38.009558 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:47:46.773629 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:47:49.083514 140169137129280 submission_runner.py:408] Time since start: 58284.55s, 	Step: 167037, 	{'train/accuracy': 0.9026426672935486, 'train/loss': 0.4636174738407135, 'validation/accuracy': 0.7644599676132202, 'validation/loss': 1.0262614488601685, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6404330730438232, 'test/num_examples': 10000, 'score': 56177.75947856903, 'total_duration': 58284.554938554764, 'accumulated_submission_time': 56177.75947856903, 'accumulated_eval_time': 2095.539171934128, 'accumulated_logging_time': 6.0488669872283936}
I0127 17:47:49.130823 140004667934464 logging_writer.py:48] [167037] accumulated_eval_time=2095.539172, accumulated_logging_time=6.048867, accumulated_submission_time=56177.759479, global_step=167037, preemption_count=0, score=56177.759479, test/accuracy=0.641300, test/loss=1.640433, test/num_examples=10000, total_duration=58284.554939, train/accuracy=0.902643, train/loss=0.463617, validation/accuracy=0.764460, validation/loss=1.026261, validation/num_examples=50000
I0127 17:48:10.594167 140004676327168 logging_writer.py:48] [167100] global_step=167100, grad_norm=7.439317226409912, loss=1.8498047590255737
I0127 17:48:44.221056 140004667934464 logging_writer.py:48] [167200] global_step=167200, grad_norm=7.074894428253174, loss=1.886745572090149
I0127 17:49:17.748352 140004676327168 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.50149917602539, loss=1.9334218502044678
I0127 17:49:51.258762 140004667934464 logging_writer.py:48] [167400] global_step=167400, grad_norm=6.65034294128418, loss=1.8354382514953613
I0127 17:50:24.784723 140004676327168 logging_writer.py:48] [167500] global_step=167500, grad_norm=7.431915760040283, loss=1.802372694015503
I0127 17:50:58.331549 140004667934464 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.335257530212402, loss=1.8471837043762207
I0127 17:51:31.945029 140004676327168 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.058201789855957, loss=1.8681961297988892
I0127 17:52:05.554823 140004667934464 logging_writer.py:48] [167800] global_step=167800, grad_norm=7.62361478805542, loss=1.8867915868759155
I0127 17:52:39.191844 140004676327168 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.62188720703125, loss=1.9013869762420654
I0127 17:53:12.797321 140004667934464 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.926488399505615, loss=1.8825860023498535
I0127 17:53:46.349497 140004676327168 logging_writer.py:48] [168100] global_step=168100, grad_norm=7.462792873382568, loss=1.826494574546814
I0127 17:54:19.915352 140004667934464 logging_writer.py:48] [168200] global_step=168200, grad_norm=7.6417741775512695, loss=1.8700709342956543
I0127 17:54:53.576069 140004676327168 logging_writer.py:48] [168300] global_step=168300, grad_norm=7.867367744445801, loss=1.8735384941101074
I0127 17:55:27.187076 140004667934464 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.704760551452637, loss=1.8088740110397339
I0127 17:56:00.767676 140004676327168 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.509371280670166, loss=1.8302727937698364
I0127 17:56:19.358418 140169137129280 spec.py:321] Evaluating on the training split.
I0127 17:56:25.686956 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 17:56:34.314502 140169137129280 spec.py:349] Evaluating on the test split.
I0127 17:56:36.620189 140169137129280 submission_runner.py:408] Time since start: 58812.09s, 	Step: 168557, 	{'train/accuracy': 0.9011479616165161, 'train/loss': 0.4678551256656647, 'validation/accuracy': 0.7643799781799316, 'validation/loss': 1.0218467712402344, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.6343672275543213, 'test/num_examples': 10000, 'score': 56687.92796278, 'total_duration': 58812.09150671959, 'accumulated_submission_time': 56687.92796278, 'accumulated_eval_time': 2112.800806760788, 'accumulated_logging_time': 6.1073596477508545}
I0127 17:56:36.668513 140004676327168 logging_writer.py:48] [168557] accumulated_eval_time=2112.800807, accumulated_logging_time=6.107360, accumulated_submission_time=56687.927963, global_step=168557, preemption_count=0, score=56687.927963, test/accuracy=0.644400, test/loss=1.634367, test/num_examples=10000, total_duration=58812.091507, train/accuracy=0.901148, train/loss=0.467855, validation/accuracy=0.764380, validation/loss=1.021847, validation/num_examples=50000
I0127 17:56:51.407174 140005288683264 logging_writer.py:48] [168600] global_step=168600, grad_norm=7.6277008056640625, loss=1.83506178855896
I0127 17:57:24.942642 140004676327168 logging_writer.py:48] [168700] global_step=168700, grad_norm=7.003668785095215, loss=1.7870585918426514
I0127 17:57:58.525114 140005288683264 logging_writer.py:48] [168800] global_step=168800, grad_norm=7.884220123291016, loss=1.8114746809005737
I0127 17:58:32.128839 140004676327168 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.5217132568359375, loss=1.8926116228103638
I0127 17:59:05.747169 140005288683264 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.297097682952881, loss=1.81967031955719
I0127 17:59:39.379853 140004676327168 logging_writer.py:48] [169100] global_step=169100, grad_norm=7.646091938018799, loss=1.8240309953689575
I0127 18:00:13.001741 140005288683264 logging_writer.py:48] [169200] global_step=169200, grad_norm=8.11839485168457, loss=1.875284194946289
I0127 18:00:46.671384 140004676327168 logging_writer.py:48] [169300] global_step=169300, grad_norm=7.4017534255981445, loss=1.8463666439056396
I0127 18:01:20.216608 140005288683264 logging_writer.py:48] [169400] global_step=169400, grad_norm=6.896088123321533, loss=1.8222432136535645
I0127 18:01:53.831286 140004676327168 logging_writer.py:48] [169500] global_step=169500, grad_norm=7.859507083892822, loss=1.7940309047698975
I0127 18:02:27.445791 140005288683264 logging_writer.py:48] [169600] global_step=169600, grad_norm=7.542710304260254, loss=1.8897590637207031
I0127 18:03:01.049604 140004676327168 logging_writer.py:48] [169700] global_step=169700, grad_norm=7.908179759979248, loss=1.8342101573944092
I0127 18:03:34.674905 140005288683264 logging_writer.py:48] [169800] global_step=169800, grad_norm=7.735321998596191, loss=1.9401639699935913
I0127 18:04:08.278938 140004676327168 logging_writer.py:48] [169900] global_step=169900, grad_norm=7.080763339996338, loss=1.8293898105621338
I0127 18:04:41.898886 140005288683264 logging_writer.py:48] [170000] global_step=170000, grad_norm=7.489664554595947, loss=1.8555793762207031
I0127 18:05:06.924068 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:05:13.207564 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:05:21.881581 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:05:24.216314 140169137129280 submission_runner.py:408] Time since start: 59339.69s, 	Step: 170076, 	{'train/accuracy': 0.9041174650192261, 'train/loss': 0.4593851864337921, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 1.023459792137146, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.6217212677001953, 'test/num_examples': 10000, 'score': 57198.12523698807, 'total_duration': 59339.6875641346, 'accumulated_submission_time': 57198.12523698807, 'accumulated_eval_time': 2130.0928435325623, 'accumulated_logging_time': 6.1661036014556885}
I0127 18:05:24.261708 140004667934464 logging_writer.py:48] [170076] accumulated_eval_time=2130.092844, accumulated_logging_time=6.166104, accumulated_submission_time=57198.125237, global_step=170076, preemption_count=0, score=57198.125237, test/accuracy=0.646100, test/loss=1.621721, test/num_examples=10000, total_duration=59339.687564, train/accuracy=0.904117, train/loss=0.459385, validation/accuracy=0.763560, validation/loss=1.023460, validation/num_examples=50000
I0127 18:05:32.648327 140004676327168 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.563604354858398, loss=1.8223824501037598
I0127 18:06:06.144745 140004667934464 logging_writer.py:48] [170200] global_step=170200, grad_norm=6.709278583526611, loss=1.758052945137024
I0127 18:06:39.658676 140004676327168 logging_writer.py:48] [170300] global_step=170300, grad_norm=7.617422580718994, loss=1.8765678405761719
I0127 18:07:13.306272 140004667934464 logging_writer.py:48] [170400] global_step=170400, grad_norm=8.518094062805176, loss=1.7764073610305786
I0127 18:07:46.937088 140004676327168 logging_writer.py:48] [170500] global_step=170500, grad_norm=7.884080410003662, loss=1.8808906078338623
I0127 18:08:20.578269 140004667934464 logging_writer.py:48] [170600] global_step=170600, grad_norm=7.587174415588379, loss=1.8619909286499023
I0127 18:08:54.183140 140004676327168 logging_writer.py:48] [170700] global_step=170700, grad_norm=8.131195068359375, loss=1.819146752357483
I0127 18:09:27.817522 140004667934464 logging_writer.py:48] [170800] global_step=170800, grad_norm=8.218409538269043, loss=1.8797169923782349
I0127 18:10:01.431692 140004676327168 logging_writer.py:48] [170900] global_step=170900, grad_norm=7.695859432220459, loss=1.809251070022583
I0127 18:10:35.059997 140004667934464 logging_writer.py:48] [171000] global_step=171000, grad_norm=7.634307384490967, loss=1.860946536064148
I0127 18:11:08.673506 140004676327168 logging_writer.py:48] [171100] global_step=171100, grad_norm=7.854898929595947, loss=1.828902006149292
I0127 18:11:42.231359 140004667934464 logging_writer.py:48] [171200] global_step=171200, grad_norm=7.792117595672607, loss=1.8289119005203247
I0127 18:12:15.783925 140004676327168 logging_writer.py:48] [171300] global_step=171300, grad_norm=7.835812091827393, loss=1.7792121171951294
I0127 18:12:49.342113 140004667934464 logging_writer.py:48] [171400] global_step=171400, grad_norm=7.01079797744751, loss=1.7749793529510498
I0127 18:13:23.012142 140004676327168 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.467029571533203, loss=1.7798296213150024
I0127 18:13:54.396286 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:14:00.751887 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:14:09.379801 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:14:11.700011 140169137129280 submission_runner.py:408] Time since start: 59867.17s, 	Step: 171595, 	{'train/accuracy': 0.906648576259613, 'train/loss': 0.4511556625366211, 'validation/accuracy': 0.7665199637413025, 'validation/loss': 1.014053463935852, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.6215829849243164, 'test/num_examples': 10000, 'score': 57708.20014286041, 'total_duration': 59867.17143726349, 'accumulated_submission_time': 57708.20014286041, 'accumulated_eval_time': 2147.3965339660645, 'accumulated_logging_time': 6.22303581237793}
I0127 18:14:11.749248 140005297075968 logging_writer.py:48] [171595] accumulated_eval_time=2147.396534, accumulated_logging_time=6.223036, accumulated_submission_time=57708.200143, global_step=171595, preemption_count=0, score=57708.200143, test/accuracy=0.644900, test/loss=1.621583, test/num_examples=10000, total_duration=59867.171437, train/accuracy=0.906649, train/loss=0.451156, validation/accuracy=0.766520, validation/loss=1.014053, validation/num_examples=50000
I0127 18:14:13.760856 140005305468672 logging_writer.py:48] [171600] global_step=171600, grad_norm=6.83953857421875, loss=1.7360285520553589
I0127 18:14:47.291022 140005297075968 logging_writer.py:48] [171700] global_step=171700, grad_norm=8.128483772277832, loss=1.8395788669586182
I0127 18:15:20.796107 140005305468672 logging_writer.py:48] [171800] global_step=171800, grad_norm=6.919894695281982, loss=1.7827818393707275
I0127 18:15:54.336499 140005297075968 logging_writer.py:48] [171900] global_step=171900, grad_norm=7.536596775054932, loss=1.80986487865448
I0127 18:16:27.895274 140005305468672 logging_writer.py:48] [172000] global_step=172000, grad_norm=7.196162223815918, loss=1.7650518417358398
I0127 18:17:01.453898 140005297075968 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.233695030212402, loss=1.8133916854858398
I0127 18:17:34.964549 140005305468672 logging_writer.py:48] [172200] global_step=172200, grad_norm=7.937017440795898, loss=1.7729392051696777
I0127 18:18:08.498339 140005297075968 logging_writer.py:48] [172300] global_step=172300, grad_norm=8.667827606201172, loss=1.8709499835968018
I0127 18:18:42.032121 140005305468672 logging_writer.py:48] [172400] global_step=172400, grad_norm=9.009291648864746, loss=1.865097999572754
I0127 18:19:15.675460 140005297075968 logging_writer.py:48] [172500] global_step=172500, grad_norm=7.635249137878418, loss=1.801178216934204
I0127 18:19:49.244336 140005305468672 logging_writer.py:48] [172600] global_step=172600, grad_norm=7.5517191886901855, loss=1.8144315481185913
I0127 18:20:22.801546 140005297075968 logging_writer.py:48] [172700] global_step=172700, grad_norm=7.416642189025879, loss=1.8196046352386475
I0127 18:20:56.309545 140005305468672 logging_writer.py:48] [172800] global_step=172800, grad_norm=7.637421131134033, loss=1.7545223236083984
I0127 18:21:29.853966 140005297075968 logging_writer.py:48] [172900] global_step=172900, grad_norm=8.10867691040039, loss=1.762580156326294
I0127 18:22:03.388109 140005305468672 logging_writer.py:48] [173000] global_step=173000, grad_norm=6.9870147705078125, loss=1.828364610671997
I0127 18:22:37.013955 140005297075968 logging_writer.py:48] [173100] global_step=173100, grad_norm=6.674898624420166, loss=1.7728756666183472
I0127 18:22:41.870660 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:22:48.116786 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:22:56.793631 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:22:59.086362 140169137129280 submission_runner.py:408] Time since start: 60394.56s, 	Step: 173116, 	{'train/accuracy': 0.9085220098495483, 'train/loss': 0.4437970519065857, 'validation/accuracy': 0.7672799825668335, 'validation/loss': 1.0095661878585815, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.6089009046554565, 'test/num_examples': 10000, 'score': 58218.26253461838, 'total_duration': 60394.55778956413, 'accumulated_submission_time': 58218.26253461838, 'accumulated_eval_time': 2164.6122002601624, 'accumulated_logging_time': 6.283660173416138}
I0127 18:22:59.134140 140004667934464 logging_writer.py:48] [173116] accumulated_eval_time=2164.612200, accumulated_logging_time=6.283660, accumulated_submission_time=58218.262535, global_step=173116, preemption_count=0, score=58218.262535, test/accuracy=0.649000, test/loss=1.608901, test/num_examples=10000, total_duration=60394.557790, train/accuracy=0.908522, train/loss=0.443797, validation/accuracy=0.767280, validation/loss=1.009566, validation/num_examples=50000
I0127 18:23:27.612157 140004676327168 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.042969703674316, loss=1.8316147327423096
I0127 18:24:01.157147 140004667934464 logging_writer.py:48] [173300] global_step=173300, grad_norm=7.840139389038086, loss=1.7965681552886963
I0127 18:24:34.771409 140004676327168 logging_writer.py:48] [173400] global_step=173400, grad_norm=7.739890098571777, loss=1.9006824493408203
I0127 18:25:08.375528 140004667934464 logging_writer.py:48] [173500] global_step=173500, grad_norm=7.931974411010742, loss=1.8065530061721802
I0127 18:25:41.979900 140004676327168 logging_writer.py:48] [173600] global_step=173600, grad_norm=8.20009994506836, loss=1.8280624151229858
I0127 18:26:15.583547 140004667934464 logging_writer.py:48] [173700] global_step=173700, grad_norm=8.09112548828125, loss=1.79575777053833
I0127 18:26:49.199059 140004676327168 logging_writer.py:48] [173800] global_step=173800, grad_norm=7.6908860206604, loss=1.8200433254241943
I0127 18:27:22.794479 140004667934464 logging_writer.py:48] [173900] global_step=173900, grad_norm=7.04960298538208, loss=1.809897780418396
I0127 18:27:56.422316 140004676327168 logging_writer.py:48] [174000] global_step=174000, grad_norm=7.751727104187012, loss=1.787226915359497
I0127 18:28:29.993011 140004667934464 logging_writer.py:48] [174100] global_step=174100, grad_norm=7.494133949279785, loss=1.7721232175827026
I0127 18:29:03.614071 140004676327168 logging_writer.py:48] [174200] global_step=174200, grad_norm=7.589690685272217, loss=1.7809021472930908
I0127 18:29:37.244898 140004667934464 logging_writer.py:48] [174300] global_step=174300, grad_norm=8.076385498046875, loss=1.8377400636672974
I0127 18:30:10.855268 140004676327168 logging_writer.py:48] [174400] global_step=174400, grad_norm=7.494980335235596, loss=1.8657417297363281
I0127 18:30:44.441321 140004667934464 logging_writer.py:48] [174500] global_step=174500, grad_norm=8.250185012817383, loss=1.8206111192703247
I0127 18:31:18.136330 140004676327168 logging_writer.py:48] [174600] global_step=174600, grad_norm=7.442976951599121, loss=1.8385330438613892
I0127 18:31:29.345306 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:31:35.706675 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:31:44.380252 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:31:46.689058 140169137129280 submission_runner.py:408] Time since start: 60922.16s, 	Step: 174635, 	{'train/accuracy': 0.9193837642669678, 'train/loss': 0.4103900194168091, 'validation/accuracy': 0.7671399712562561, 'validation/loss': 1.0063893795013428, 'validation/num_examples': 50000, 'test/accuracy': 0.6513000130653381, 'test/loss': 1.6108440160751343, 'test/num_examples': 10000, 'score': 58728.41574501991, 'total_duration': 60922.16036057472, 'accumulated_submission_time': 58728.41574501991, 'accumulated_eval_time': 2181.95579123497, 'accumulated_logging_time': 6.341675043106079}
I0127 18:31:46.736780 140004659541760 logging_writer.py:48] [174635] accumulated_eval_time=2181.955791, accumulated_logging_time=6.341675, accumulated_submission_time=58728.415745, global_step=174635, preemption_count=0, score=58728.415745, test/accuracy=0.651300, test/loss=1.610844, test/num_examples=10000, total_duration=60922.160361, train/accuracy=0.919384, train/loss=0.410390, validation/accuracy=0.767140, validation/loss=1.006389, validation/num_examples=50000
I0127 18:32:08.841636 140004667934464 logging_writer.py:48] [174700] global_step=174700, grad_norm=7.740952491760254, loss=1.8353464603424072
I0127 18:32:42.355162 140004659541760 logging_writer.py:48] [174800] global_step=174800, grad_norm=8.257767677307129, loss=1.8428947925567627
I0127 18:33:15.967169 140004667934464 logging_writer.py:48] [174900] global_step=174900, grad_norm=7.397555828094482, loss=1.7723945379257202
I0127 18:33:49.594817 140004659541760 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.681554794311523, loss=1.8614161014556885
I0127 18:34:23.222775 140004667934464 logging_writer.py:48] [175100] global_step=175100, grad_norm=7.501950263977051, loss=1.8477038145065308
I0127 18:34:56.815731 140004659541760 logging_writer.py:48] [175200] global_step=175200, grad_norm=8.125679016113281, loss=1.8768377304077148
I0127 18:35:30.360316 140004667934464 logging_writer.py:48] [175300] global_step=175300, grad_norm=8.013229370117188, loss=1.808339238166809
I0127 18:36:03.977316 140004659541760 logging_writer.py:48] [175400] global_step=175400, grad_norm=7.725950717926025, loss=1.8501017093658447
I0127 18:36:37.596395 140004667934464 logging_writer.py:48] [175500] global_step=175500, grad_norm=7.470000743865967, loss=1.7666009664535522
I0127 18:37:11.206050 140004659541760 logging_writer.py:48] [175600] global_step=175600, grad_norm=7.735416412353516, loss=1.7995409965515137
I0127 18:37:44.883569 140004667934464 logging_writer.py:48] [175700] global_step=175700, grad_norm=8.022113800048828, loss=1.8572673797607422
I0127 18:38:18.432441 140004659541760 logging_writer.py:48] [175800] global_step=175800, grad_norm=8.901297569274902, loss=1.898581624031067
I0127 18:38:51.983119 140004667934464 logging_writer.py:48] [175900] global_step=175900, grad_norm=7.521249771118164, loss=1.8843170404434204
I0127 18:39:25.581998 140004659541760 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.645847797393799, loss=1.7551538944244385
I0127 18:39:59.211801 140004667934464 logging_writer.py:48] [176100] global_step=176100, grad_norm=7.9353437423706055, loss=1.8074618577957153
I0127 18:40:16.829261 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:40:23.181174 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:40:31.837839 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:40:34.222573 140169137129280 submission_runner.py:408] Time since start: 61449.69s, 	Step: 176154, 	{'train/accuracy': 0.917390763759613, 'train/loss': 0.41718706488609314, 'validation/accuracy': 0.76801997423172, 'validation/loss': 1.0084744691848755, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6077901124954224, 'test/num_examples': 10000, 'score': 59238.44852399826, 'total_duration': 61449.693984270096, 'accumulated_submission_time': 59238.44852399826, 'accumulated_eval_time': 2199.349052429199, 'accumulated_logging_time': 6.401141405105591}
I0127 18:40:34.268317 140005305468672 logging_writer.py:48] [176154] accumulated_eval_time=2199.349052, accumulated_logging_time=6.401141, accumulated_submission_time=59238.448524, global_step=176154, preemption_count=0, score=59238.448524, test/accuracy=0.651900, test/loss=1.607790, test/num_examples=10000, total_duration=61449.693984, train/accuracy=0.917391, train/loss=0.417187, validation/accuracy=0.768020, validation/loss=1.008474, validation/num_examples=50000
I0127 18:40:50.020246 140005313861376 logging_writer.py:48] [176200] global_step=176200, grad_norm=8.838114738464355, loss=1.7976617813110352
I0127 18:41:23.499020 140005305468672 logging_writer.py:48] [176300] global_step=176300, grad_norm=7.747395992279053, loss=1.761298418045044
I0127 18:41:57.123139 140005313861376 logging_writer.py:48] [176400] global_step=176400, grad_norm=8.012154579162598, loss=1.7613804340362549
I0127 18:42:30.739753 140005305468672 logging_writer.py:48] [176500] global_step=176500, grad_norm=7.300069332122803, loss=1.7586679458618164
I0127 18:43:04.348300 140005313861376 logging_writer.py:48] [176600] global_step=176600, grad_norm=7.488167762756348, loss=1.7594468593597412
I0127 18:43:38.029136 140005305468672 logging_writer.py:48] [176700] global_step=176700, grad_norm=7.689415454864502, loss=1.7864446640014648
I0127 18:44:11.628789 140005313861376 logging_writer.py:48] [176800] global_step=176800, grad_norm=8.069611549377441, loss=1.7941917181015015
I0127 18:44:45.229701 140005305468672 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.049813270568848, loss=1.8627195358276367
I0127 18:45:18.862701 140005313861376 logging_writer.py:48] [177000] global_step=177000, grad_norm=8.70121955871582, loss=1.817448377609253
I0127 18:45:52.470848 140005305468672 logging_writer.py:48] [177100] global_step=177100, grad_norm=7.954060077667236, loss=1.7923083305358887
I0127 18:46:26.058014 140005313861376 logging_writer.py:48] [177200] global_step=177200, grad_norm=7.762639999389648, loss=1.8126273155212402
I0127 18:46:59.676761 140005305468672 logging_writer.py:48] [177300] global_step=177300, grad_norm=7.403955936431885, loss=1.7880661487579346
I0127 18:47:33.264301 140005313861376 logging_writer.py:48] [177400] global_step=177400, grad_norm=8.073952674865723, loss=1.8257663249969482
I0127 18:48:06.894600 140005305468672 logging_writer.py:48] [177500] global_step=177500, grad_norm=7.804849624633789, loss=1.7646543979644775
I0127 18:48:40.481772 140005313861376 logging_writer.py:48] [177600] global_step=177600, grad_norm=7.534610271453857, loss=1.751776933670044
I0127 18:49:04.505911 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:49:10.765044 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:49:19.385489 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:49:21.718331 140169137129280 submission_runner.py:408] Time since start: 61977.19s, 	Step: 177673, 	{'train/accuracy': 0.9159757494926453, 'train/loss': 0.4144531488418579, 'validation/accuracy': 0.7691400051116943, 'validation/loss': 1.002310037612915, 'validation/num_examples': 50000, 'test/accuracy': 0.6516000032424927, 'test/loss': 1.602932333946228, 'test/num_examples': 10000, 'score': 59748.627321243286, 'total_duration': 61977.189665555954, 'accumulated_submission_time': 59748.627321243286, 'accumulated_eval_time': 2216.56134724617, 'accumulated_logging_time': 6.457212448120117}
I0127 18:49:21.765201 140004667934464 logging_writer.py:48] [177673] accumulated_eval_time=2216.561347, accumulated_logging_time=6.457212, accumulated_submission_time=59748.627321, global_step=177673, preemption_count=0, score=59748.627321, test/accuracy=0.651600, test/loss=1.602932, test/num_examples=10000, total_duration=61977.189666, train/accuracy=0.915976, train/loss=0.414453, validation/accuracy=0.769140, validation/loss=1.002310, validation/num_examples=50000
I0127 18:49:31.959820 140004676327168 logging_writer.py:48] [177700] global_step=177700, grad_norm=7.668469429016113, loss=1.8095180988311768
I0127 18:50:05.514384 140004667934464 logging_writer.py:48] [177800] global_step=177800, grad_norm=7.428608417510986, loss=1.816988229751587
I0127 18:50:39.053648 140004676327168 logging_writer.py:48] [177900] global_step=177900, grad_norm=7.729918479919434, loss=1.811110496520996
I0127 18:51:12.589432 140004667934464 logging_writer.py:48] [178000] global_step=178000, grad_norm=7.819138050079346, loss=1.8236039876937866
I0127 18:51:46.124418 140004676327168 logging_writer.py:48] [178100] global_step=178100, grad_norm=7.871669292449951, loss=1.754746913909912
I0127 18:52:19.672710 140004667934464 logging_writer.py:48] [178200] global_step=178200, grad_norm=8.073553085327148, loss=1.7848892211914062
I0127 18:52:53.223804 140004676327168 logging_writer.py:48] [178300] global_step=178300, grad_norm=7.970077037811279, loss=1.8300377130508423
I0127 18:53:26.741531 140004667934464 logging_writer.py:48] [178400] global_step=178400, grad_norm=7.237823963165283, loss=1.7219492197036743
I0127 18:54:00.317271 140004676327168 logging_writer.py:48] [178500] global_step=178500, grad_norm=7.855592250823975, loss=1.837545394897461
I0127 18:54:33.904269 140004667934464 logging_writer.py:48] [178600] global_step=178600, grad_norm=7.373223304748535, loss=1.6994681358337402
I0127 18:55:07.541946 140004676327168 logging_writer.py:48] [178700] global_step=178700, grad_norm=7.983597755432129, loss=1.807133674621582
I0127 18:55:41.163704 140004667934464 logging_writer.py:48] [178800] global_step=178800, grad_norm=8.132243156433105, loss=1.7879602909088135
I0127 18:56:14.789336 140004676327168 logging_writer.py:48] [178900] global_step=178900, grad_norm=8.093951225280762, loss=1.7662783861160278
I0127 18:56:48.412922 140004667934464 logging_writer.py:48] [179000] global_step=179000, grad_norm=7.39988899230957, loss=1.8043094873428345
I0127 18:57:22.012484 140004676327168 logging_writer.py:48] [179100] global_step=179100, grad_norm=7.740108966827393, loss=1.830573558807373
I0127 18:57:51.758289 140169137129280 spec.py:321] Evaluating on the training split.
I0127 18:57:58.086649 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 18:58:06.643312 140169137129280 spec.py:349] Evaluating on the test split.
I0127 18:58:08.960598 140169137129280 submission_runner.py:408] Time since start: 62504.43s, 	Step: 179190, 	{'train/accuracy': 0.9170718789100647, 'train/loss': 0.40572771430015564, 'validation/accuracy': 0.7702599763870239, 'validation/loss': 0.9993435740470886, 'validation/num_examples': 50000, 'test/accuracy': 0.6502000093460083, 'test/loss': 1.6043004989624023, 'test/num_examples': 10000, 'score': 60257.7345225811, 'total_duration': 62504.43201804161, 'accumulated_submission_time': 60257.7345225811, 'accumulated_eval_time': 2233.763617515564, 'accumulated_logging_time': 7.341777801513672}
I0127 18:58:09.009582 140004667934464 logging_writer.py:48] [179190] accumulated_eval_time=2233.763618, accumulated_logging_time=7.341778, accumulated_submission_time=60257.734523, global_step=179190, preemption_count=0, score=60257.734523, test/accuracy=0.650200, test/loss=1.604300, test/num_examples=10000, total_duration=62504.432018, train/accuracy=0.917072, train/loss=0.405728, validation/accuracy=0.770260, validation/loss=0.999344, validation/num_examples=50000
I0127 18:58:12.724430 140004676327168 logging_writer.py:48] [179200] global_step=179200, grad_norm=7.883016109466553, loss=1.8554507493972778
I0127 18:58:46.230374 140004667934464 logging_writer.py:48] [179300] global_step=179300, grad_norm=7.86416482925415, loss=1.7229077816009521
I0127 18:59:19.824532 140004676327168 logging_writer.py:48] [179400] global_step=179400, grad_norm=8.112187385559082, loss=1.8005703687667847
I0127 18:59:53.416625 140004667934464 logging_writer.py:48] [179500] global_step=179500, grad_norm=7.910513877868652, loss=1.7620972394943237
I0127 19:00:27.053076 140004676327168 logging_writer.py:48] [179600] global_step=179600, grad_norm=7.841454982757568, loss=1.7891384363174438
I0127 19:01:00.676368 140004667934464 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.910571098327637, loss=1.7304155826568604
I0127 19:01:34.294689 140004676327168 logging_writer.py:48] [179800] global_step=179800, grad_norm=7.756988525390625, loss=1.779069185256958
I0127 19:02:07.957882 140004667934464 logging_writer.py:48] [179900] global_step=179900, grad_norm=7.793643951416016, loss=1.809823989868164
I0127 19:02:41.506500 140004676327168 logging_writer.py:48] [180000] global_step=180000, grad_norm=7.077109336853027, loss=1.7419257164001465
I0127 19:03:15.101454 140004667934464 logging_writer.py:48] [180100] global_step=180100, grad_norm=7.874743938446045, loss=1.7670668363571167
I0127 19:03:48.722587 140004676327168 logging_writer.py:48] [180200] global_step=180200, grad_norm=8.61433219909668, loss=1.878365159034729
I0127 19:04:22.355360 140004667934464 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.564838409423828, loss=1.8095487356185913
I0127 19:04:55.977189 140004676327168 logging_writer.py:48] [180400] global_step=180400, grad_norm=8.1350679397583, loss=1.7179853916168213
I0127 19:05:29.583487 140004667934464 logging_writer.py:48] [180500] global_step=180500, grad_norm=7.709474563598633, loss=1.755208969116211
I0127 19:06:03.188467 140004676327168 logging_writer.py:48] [180600] global_step=180600, grad_norm=8.761360168457031, loss=1.8199280500411987
I0127 19:06:36.731195 140004667934464 logging_writer.py:48] [180700] global_step=180700, grad_norm=7.91412353515625, loss=1.759515404701233
I0127 19:06:39.227249 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:06:45.592394 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:06:54.243758 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:06:56.613496 140169137129280 submission_runner.py:408] Time since start: 63032.08s, 	Step: 180709, 	{'train/accuracy': 0.9195631146430969, 'train/loss': 0.40906286239624023, 'validation/accuracy': 0.7701999545097351, 'validation/loss': 1.0007290840148926, 'validation/num_examples': 50000, 'test/accuracy': 0.6528000235557556, 'test/loss': 1.6050145626068115, 'test/num_examples': 10000, 'score': 60767.89223456383, 'total_duration': 63032.08485865593, 'accumulated_submission_time': 60767.89223456383, 'accumulated_eval_time': 2251.149762868881, 'accumulated_logging_time': 7.402913808822632}
I0127 19:06:56.662957 140004659541760 logging_writer.py:48] [180709] accumulated_eval_time=2251.149763, accumulated_logging_time=7.402914, accumulated_submission_time=60767.892235, global_step=180709, preemption_count=0, score=60767.892235, test/accuracy=0.652800, test/loss=1.605015, test/num_examples=10000, total_duration=63032.084859, train/accuracy=0.919563, train/loss=0.409063, validation/accuracy=0.770200, validation/loss=1.000729, validation/num_examples=50000
I0127 19:07:27.499364 140004667934464 logging_writer.py:48] [180800] global_step=180800, grad_norm=7.841622352600098, loss=1.7980389595031738
I0127 19:08:01.087877 140004659541760 logging_writer.py:48] [180900] global_step=180900, grad_norm=7.405635356903076, loss=1.8172105550765991
I0127 19:08:34.726936 140004667934464 logging_writer.py:48] [181000] global_step=181000, grad_norm=7.227350234985352, loss=1.7676737308502197
I0127 19:09:08.349236 140004659541760 logging_writer.py:48] [181100] global_step=181100, grad_norm=7.448197841644287, loss=1.7685924768447876
I0127 19:09:41.973407 140004667934464 logging_writer.py:48] [181200] global_step=181200, grad_norm=8.5061674118042, loss=1.7889411449432373
I0127 19:10:15.593401 140004659541760 logging_writer.py:48] [181300] global_step=181300, grad_norm=7.949965953826904, loss=1.7881956100463867
I0127 19:10:49.194969 140004667934464 logging_writer.py:48] [181400] global_step=181400, grad_norm=7.534079551696777, loss=1.7337675094604492
I0127 19:11:22.820436 140004659541760 logging_writer.py:48] [181500] global_step=181500, grad_norm=7.7740349769592285, loss=1.8473912477493286
I0127 19:11:56.436945 140004667934464 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.399173736572266, loss=1.7222647666931152
I0127 19:12:30.060039 140004659541760 logging_writer.py:48] [181700] global_step=181700, grad_norm=7.603586196899414, loss=1.8072782754898071
I0127 19:13:03.648966 140004667934464 logging_writer.py:48] [181800] global_step=181800, grad_norm=7.919949054718018, loss=1.827077865600586
I0127 19:13:37.220242 140004659541760 logging_writer.py:48] [181900] global_step=181900, grad_norm=7.099538803100586, loss=1.7383122444152832
I0127 19:14:10.871938 140004667934464 logging_writer.py:48] [182000] global_step=182000, grad_norm=7.423000335693359, loss=1.7960867881774902
I0127 19:14:44.491674 140004659541760 logging_writer.py:48] [182100] global_step=182100, grad_norm=7.912389278411865, loss=1.780391812324524
I0127 19:15:18.085238 140004667934464 logging_writer.py:48] [182200] global_step=182200, grad_norm=8.33172607421875, loss=1.7987239360809326
I0127 19:15:26.620618 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:15:32.903582 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:15:41.632004 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:15:43.949491 140169137129280 submission_runner.py:408] Time since start: 63559.42s, 	Step: 182227, 	{'train/accuracy': 0.9197823405265808, 'train/loss': 0.40754902362823486, 'validation/accuracy': 0.7706800103187561, 'validation/loss': 0.9976078867912292, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.602009892463684, 'test/num_examples': 10000, 'score': 61277.79031038284, 'total_duration': 63559.42090892792, 'accumulated_submission_time': 61277.79031038284, 'accumulated_eval_time': 2268.478595972061, 'accumulated_logging_time': 7.46382737159729}
I0127 19:15:44.000794 140005305468672 logging_writer.py:48] [182227] accumulated_eval_time=2268.478596, accumulated_logging_time=7.463827, accumulated_submission_time=61277.790310, global_step=182227, preemption_count=0, score=61277.790310, test/accuracy=0.651500, test/loss=1.602010, test/num_examples=10000, total_duration=63559.420909, train/accuracy=0.919782, train/loss=0.407549, validation/accuracy=0.770680, validation/loss=0.997608, validation/num_examples=50000
I0127 19:16:08.816925 140005313861376 logging_writer.py:48] [182300] global_step=182300, grad_norm=7.87467098236084, loss=1.8118242025375366
I0127 19:16:42.348642 140005305468672 logging_writer.py:48] [182400] global_step=182400, grad_norm=7.614064693450928, loss=1.7457621097564697
I0127 19:17:15.908014 140005313861376 logging_writer.py:48] [182500] global_step=182500, grad_norm=7.266965866088867, loss=1.723630666732788
I0127 19:17:49.425039 140005305468672 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.014105796813965, loss=1.7690813541412354
I0127 19:18:23.024090 140005313861376 logging_writer.py:48] [182700] global_step=182700, grad_norm=7.840306282043457, loss=1.8031541109085083
I0127 19:18:56.617552 140005305468672 logging_writer.py:48] [182800] global_step=182800, grad_norm=9.568517684936523, loss=1.8049781322479248
I0127 19:19:30.255240 140005313861376 logging_writer.py:48] [182900] global_step=182900, grad_norm=7.414927005767822, loss=1.7898447513580322
I0127 19:20:03.877221 140005305468672 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.454659461975098, loss=1.7766408920288086
I0127 19:20:37.499955 140005313861376 logging_writer.py:48] [183100] global_step=183100, grad_norm=7.516519069671631, loss=1.7369449138641357
I0127 19:21:11.109463 140005305468672 logging_writer.py:48] [183200] global_step=183200, grad_norm=7.447355270385742, loss=1.7459183931350708
I0127 19:21:44.710791 140005313861376 logging_writer.py:48] [183300] global_step=183300, grad_norm=7.790861129760742, loss=1.7780005931854248
I0127 19:22:18.241401 140005305468672 logging_writer.py:48] [183400] global_step=183400, grad_norm=7.771124839782715, loss=1.788591980934143
I0127 19:22:51.774879 140005313861376 logging_writer.py:48] [183500] global_step=183500, grad_norm=9.031929016113281, loss=1.760675072669983
I0127 19:23:25.358708 140005305468672 logging_writer.py:48] [183600] global_step=183600, grad_norm=7.879643440246582, loss=1.7581819295883179
I0127 19:23:58.993541 140005313861376 logging_writer.py:48] [183700] global_step=183700, grad_norm=7.0832953453063965, loss=1.676405429840088
I0127 19:24:14.270783 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:24:20.609862 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:24:29.325349 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:24:31.642249 140169137129280 submission_runner.py:408] Time since start: 64087.11s, 	Step: 183747, 	{'train/accuracy': 0.9213767051696777, 'train/loss': 0.40018025040626526, 'validation/accuracy': 0.7707799673080444, 'validation/loss': 0.9960906505584717, 'validation/num_examples': 50000, 'test/accuracy': 0.6534000039100647, 'test/loss': 1.599919080734253, 'test/num_examples': 10000, 'score': 61788.00178670883, 'total_duration': 64087.113674640656, 'accumulated_submission_time': 61788.00178670883, 'accumulated_eval_time': 2285.850029706955, 'accumulated_logging_time': 7.525523662567139}
I0127 19:24:31.691169 140004667934464 logging_writer.py:48] [183747] accumulated_eval_time=2285.850030, accumulated_logging_time=7.525524, accumulated_submission_time=61788.001787, global_step=183747, preemption_count=0, score=61788.001787, test/accuracy=0.653400, test/loss=1.599919, test/num_examples=10000, total_duration=64087.113675, train/accuracy=0.921377, train/loss=0.400180, validation/accuracy=0.770780, validation/loss=0.996091, validation/num_examples=50000
I0127 19:24:49.775849 140004676327168 logging_writer.py:48] [183800] global_step=183800, grad_norm=7.46398401260376, loss=1.7738280296325684
I0127 19:25:23.311422 140004667934464 logging_writer.py:48] [183900] global_step=183900, grad_norm=8.515172004699707, loss=1.7664026021957397
I0127 19:25:56.914375 140004676327168 logging_writer.py:48] [184000] global_step=184000, grad_norm=8.423304557800293, loss=1.7519360780715942
I0127 19:26:30.561969 140004667934464 logging_writer.py:48] [184100] global_step=184100, grad_norm=7.949738025665283, loss=1.8835054636001587
I0127 19:27:04.167717 140004676327168 logging_writer.py:48] [184200] global_step=184200, grad_norm=9.085538864135742, loss=1.8136407136917114
I0127 19:27:37.787239 140004667934464 logging_writer.py:48] [184300] global_step=184300, grad_norm=7.797827243804932, loss=1.8500359058380127
I0127 19:28:11.426771 140004676327168 logging_writer.py:48] [184400] global_step=184400, grad_norm=7.330956935882568, loss=1.7732418775558472
I0127 19:28:45.027004 140004667934464 logging_writer.py:48] [184500] global_step=184500, grad_norm=7.685371398925781, loss=1.7482538223266602
I0127 19:29:18.632927 140004676327168 logging_writer.py:48] [184600] global_step=184600, grad_norm=8.117433547973633, loss=1.7765977382659912
I0127 19:29:52.207592 140004667934464 logging_writer.py:48] [184700] global_step=184700, grad_norm=7.999034881591797, loss=1.780134916305542
I0127 19:30:25.727460 140004676327168 logging_writer.py:48] [184800] global_step=184800, grad_norm=7.4609808921813965, loss=1.7275880575180054
I0127 19:30:59.263559 140004667934464 logging_writer.py:48] [184900] global_step=184900, grad_norm=7.006933689117432, loss=1.7681124210357666
I0127 19:31:32.820595 140004676327168 logging_writer.py:48] [185000] global_step=185000, grad_norm=8.309967994689941, loss=1.7584388256072998
I0127 19:32:06.426755 140004667934464 logging_writer.py:48] [185100] global_step=185100, grad_norm=8.766921043395996, loss=1.7713866233825684
I0127 19:32:40.077450 140004676327168 logging_writer.py:48] [185200] global_step=185200, grad_norm=7.7581987380981445, loss=1.7633051872253418
I0127 19:33:01.675605 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:33:07.903182 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:33:16.552762 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:33:18.849359 140169137129280 submission_runner.py:408] Time since start: 64614.32s, 	Step: 185266, 	{'train/accuracy': 0.9223333597183228, 'train/loss': 0.39865994453430176, 'validation/accuracy': 0.7706599831581116, 'validation/loss': 0.9960277080535889, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.599553108215332, 'test/num_examples': 10000, 'score': 62297.926362752914, 'total_duration': 64614.320786714554, 'accumulated_submission_time': 62297.926362752914, 'accumulated_eval_time': 2303.0237517356873, 'accumulated_logging_time': 7.586014032363892}
I0127 19:33:18.899379 140004667934464 logging_writer.py:48] [185266] accumulated_eval_time=2303.023752, accumulated_logging_time=7.586014, accumulated_submission_time=62297.926363, global_step=185266, preemption_count=0, score=62297.926363, test/accuracy=0.654100, test/loss=1.599553, test/num_examples=10000, total_duration=64614.320787, train/accuracy=0.922333, train/loss=0.398660, validation/accuracy=0.770660, validation/loss=0.996028, validation/num_examples=50000
I0127 19:33:30.632641 140005313861376 logging_writer.py:48] [185300] global_step=185300, grad_norm=8.69248104095459, loss=1.7430415153503418
I0127 19:34:04.102427 140004667934464 logging_writer.py:48] [185400] global_step=185400, grad_norm=8.109518051147461, loss=1.8019376993179321
I0127 19:34:37.691417 140005313861376 logging_writer.py:48] [185500] global_step=185500, grad_norm=8.013169288635254, loss=1.8058133125305176
I0127 19:35:11.316766 140004667934464 logging_writer.py:48] [185600] global_step=185600, grad_norm=7.273545265197754, loss=1.7754544019699097
I0127 19:35:44.880053 140005313861376 logging_writer.py:48] [185700] global_step=185700, grad_norm=7.468111038208008, loss=1.8264094591140747
I0127 19:36:18.439921 140004667934464 logging_writer.py:48] [185800] global_step=185800, grad_norm=7.738556861877441, loss=1.7898292541503906
I0127 19:36:51.945212 140005313861376 logging_writer.py:48] [185900] global_step=185900, grad_norm=7.403099536895752, loss=1.7896748781204224
I0127 19:37:25.520474 140004667934464 logging_writer.py:48] [186000] global_step=186000, grad_norm=7.544460296630859, loss=1.7787814140319824
I0127 19:37:59.011639 140005313861376 logging_writer.py:48] [186100] global_step=186100, grad_norm=7.662560939788818, loss=1.8389919996261597
I0127 19:38:32.549625 140004667934464 logging_writer.py:48] [186200] global_step=186200, grad_norm=7.268826007843018, loss=1.7431080341339111
I0127 19:39:06.184725 140005313861376 logging_writer.py:48] [186300] global_step=186300, grad_norm=7.07927942276001, loss=1.8033485412597656
I0127 19:39:39.752247 140004667934464 logging_writer.py:48] [186400] global_step=186400, grad_norm=7.967802047729492, loss=1.8126929998397827
I0127 19:40:13.364603 140005313861376 logging_writer.py:48] [186500] global_step=186500, grad_norm=7.994514465332031, loss=1.7916041612625122
I0127 19:40:46.991832 140004667934464 logging_writer.py:48] [186600] global_step=186600, grad_norm=7.825072288513184, loss=1.738037109375
I0127 19:41:08.664767 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:41:14.917523 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:41:23.562861 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:41:25.940214 140169137129280 submission_runner.py:408] Time since start: 65101.41s, 	Step: 186666, 	{'train/accuracy': 0.9204400181770325, 'train/loss': 0.39730027318000793, 'validation/accuracy': 0.7709999680519104, 'validation/loss': 0.9936330318450928, 'validation/num_examples': 50000, 'test/accuracy': 0.6535000205039978, 'test/loss': 1.5979526042938232, 'test/num_examples': 10000, 'score': 62767.63660430908, 'total_duration': 65101.41163563728, 'accumulated_submission_time': 62767.63660430908, 'accumulated_eval_time': 2320.299160003662, 'accumulated_logging_time': 7.647452354431152}
I0127 19:41:25.992326 140004659541760 logging_writer.py:48] [186666] accumulated_eval_time=2320.299160, accumulated_logging_time=7.647452, accumulated_submission_time=62767.636604, global_step=186666, preemption_count=0, score=62767.636604, test/accuracy=0.653500, test/loss=1.597953, test/num_examples=10000, total_duration=65101.411636, train/accuracy=0.920440, train/loss=0.397300, validation/accuracy=0.771000, validation/loss=0.993633, validation/num_examples=50000
I0127 19:41:26.036576 140004667934464 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62767.636604
I0127 19:41:26.397139 140169137129280 checkpoints.py:490] Saving checkpoint at step: 186666
I0127 19:41:27.621001 140169137129280 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1/checkpoint_186666
I0127 19:41:27.650716 140169137129280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_1/checkpoint_186666.
I0127 19:41:28.424719 140169137129280 submission_runner.py:583] Tuning trial 1/5
I0127 19:41:28.424981 140169137129280 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0127 19:41:28.429110 140169137129280 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007573341717943549, 'train/loss': 6.910542011260986, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 69.66642546653748, 'total_duration': 107.33833169937134, 'accumulated_submission_time': 69.66642546653748, 'accumulated_eval_time': 37.67181849479675, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1514, {'train/accuracy': 0.07354113459587097, 'train/loss': 5.336194038391113, 'validation/accuracy': 0.06785999983549118, 'validation/loss': 5.403800964355469, 'validation/num_examples': 50000, 'test/accuracy': 0.04960000142455101, 'test/loss': 5.629796504974365, 'test/num_examples': 10000, 'score': 579.8638372421265, 'total_duration': 635.5374145507812, 'accumulated_submission_time': 579.8638372421265, 'accumulated_eval_time': 55.593273878097534, 'accumulated_logging_time': 0.029314041137695312, 'global_step': 1514, 'preemption_count': 0}), (3025, {'train/accuracy': 0.1801857352256775, 'train/loss': 4.261962890625, 'validation/accuracy': 0.16099999845027924, 'validation/loss': 4.37910270690918, 'validation/num_examples': 50000, 'test/accuracy': 0.12120000272989273, 'test/loss': 4.7993268966674805, 'test/num_examples': 10000, 'score': 1089.8733115196228, 'total_duration': 1163.8596937656403, 'accumulated_submission_time': 1089.8733115196228, 'accumulated_eval_time': 73.83026790618896, 'accumulated_logging_time': 0.05568432807922363, 'global_step': 3025, 'preemption_count': 0}), (4537, {'train/accuracy': 0.28413981199264526, 'train/loss': 3.504873514175415, 'validation/accuracy': 0.2563000023365021, 'validation/loss': 3.6541614532470703, 'validation/num_examples': 50000, 'test/accuracy': 0.19180001318454742, 'test/loss': 4.191176891326904, 'test/num_examples': 10000, 'score': 1600.1244082450867, 'total_duration': 1692.4212460517883, 'accumulated_submission_time': 1600.1244082450867, 'accumulated_eval_time': 92.06435537338257, 'accumulated_logging_time': 0.08449053764343262, 'global_step': 4537, 'preemption_count': 0}), (6049, {'train/accuracy': 0.36937978863716125, 'train/loss': 2.9464893341064453, 'validation/accuracy': 0.34289997816085815, 'validation/loss': 3.0969789028167725, 'validation/num_examples': 50000, 'test/accuracy': 0.2526000142097473, 'test/loss': 3.744076728820801, 'test/num_examples': 10000, 'score': 2110.33571267128, 'total_duration': 2220.955129146576, 'accumulated_submission_time': 2110.33571267128, 'accumulated_eval_time': 110.31303834915161, 'accumulated_logging_time': 0.11001849174499512, 'global_step': 6049, 'preemption_count': 0}), (7561, {'train/accuracy': 0.44345900416374207, 'train/loss': 2.543766975402832, 'validation/accuracy': 0.4113999903202057, 'validation/loss': 2.7178561687469482, 'validation/num_examples': 50000, 'test/accuracy': 0.3183000087738037, 'test/loss': 3.374671220779419, 'test/num_examples': 10000, 'score': 2620.2774546146393, 'total_duration': 2749.279436826706, 'accumulated_submission_time': 2620.2774546146393, 'accumulated_eval_time': 128.61637711524963, 'accumulated_logging_time': 0.14015746116638184, 'global_step': 7561, 'preemption_count': 0}), (9073, {'train/accuracy': 0.5113998651504517, 'train/loss': 2.2248189449310303, 'validation/accuracy': 0.4605399966239929, 'validation/loss': 2.471454620361328, 'validation/num_examples': 50000, 'test/accuracy': 0.35850000381469727, 'test/loss': 3.0965747833251953, 'test/num_examples': 10000, 'score': 3130.3148624897003, 'total_duration': 3277.937031984329, 'accumulated_submission_time': 3130.3148624897003, 'accumulated_eval_time': 147.15200424194336, 'accumulated_logging_time': 0.17615604400634766, 'global_step': 9073, 'preemption_count': 0}), (10586, {'train/accuracy': 0.5440847873687744, 'train/loss': 2.0609612464904785, 'validation/accuracy': 0.49281999468803406, 'validation/loss': 2.328207492828369, 'validation/num_examples': 50000, 'test/accuracy': 0.3758000135421753, 'test/loss': 3.0121326446533203, 'test/num_examples': 10000, 'score': 3640.3234446048737, 'total_duration': 3806.8680925369263, 'accumulated_submission_time': 3640.3234446048737, 'accumulated_eval_time': 165.99961352348328, 'accumulated_logging_time': 0.20357942581176758, 'global_step': 10586, 'preemption_count': 0}), (12101, {'train/accuracy': 0.5639150142669678, 'train/loss': 1.9292712211608887, 'validation/accuracy': 0.521619975566864, 'validation/loss': 2.1376399993896484, 'validation/num_examples': 50000, 'test/accuracy': 0.41050001978874207, 'test/loss': 2.809180498123169, 'test/num_examples': 10000, 'score': 4150.755066394806, 'total_duration': 4338.130095720291, 'accumulated_submission_time': 4150.755066394806, 'accumulated_eval_time': 186.73877835273743, 'accumulated_logging_time': 0.2456672191619873, 'global_step': 12101, 'preemption_count': 0}), (13616, {'train/accuracy': 0.5889668464660645, 'train/loss': 1.8186978101730347, 'validation/accuracy': 0.5448799729347229, 'validation/loss': 2.03045392036438, 'validation/num_examples': 50000, 'test/accuracy': 0.41930001974105835, 'test/loss': 2.7065205574035645, 'test/num_examples': 10000, 'score': 4660.733189105988, 'total_duration': 4867.710218667984, 'accumulated_submission_time': 4660.733189105988, 'accumulated_eval_time': 206.2655758857727, 'accumulated_logging_time': 0.27249741554260254, 'global_step': 13616, 'preemption_count': 0}), (15132, {'train/accuracy': 0.5876913070678711, 'train/loss': 1.8149843215942383, 'validation/accuracy': 0.5457000136375427, 'validation/loss': 2.0031583309173584, 'validation/num_examples': 50000, 'test/accuracy': 0.4246000349521637, 'test/loss': 2.713749647140503, 'test/num_examples': 10000, 'score': 5170.75431728363, 'total_duration': 5399.7187423706055, 'accumulated_submission_time': 5170.75431728363, 'accumulated_eval_time': 228.16293787956238, 'accumulated_logging_time': 0.3139839172363281, 'global_step': 15132, 'preemption_count': 0}), (16648, {'train/accuracy': 0.5989118218421936, 'train/loss': 1.7425923347473145, 'validation/accuracy': 0.5619199872016907, 'validation/loss': 1.9272526502609253, 'validation/num_examples': 50000, 'test/accuracy': 0.44030001759529114, 'test/loss': 2.5987136363983154, 'test/num_examples': 10000, 'score': 5680.732532739639, 'total_duration': 5931.523034095764, 'accumulated_submission_time': 5680.732532739639, 'accumulated_eval_time': 249.90737676620483, 'accumulated_logging_time': 0.3474881649017334, 'global_step': 16648, 'preemption_count': 0}), (18165, {'train/accuracy': 0.6433154940605164, 'train/loss': 1.562894344329834, 'validation/accuracy': 0.5627399682998657, 'validation/loss': 1.9308120012283325, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.6102218627929688, 'test/num_examples': 10000, 'score': 6190.90416431427, 'total_duration': 6466.05579328537, 'accumulated_submission_time': 6190.90416431427, 'accumulated_eval_time': 274.1750280857086, 'accumulated_logging_time': 0.39433765411376953, 'global_step': 18165, 'preemption_count': 0}), (19681, {'train/accuracy': 0.6237842440605164, 'train/loss': 1.6315232515335083, 'validation/accuracy': 0.5619800090789795, 'validation/loss': 1.9117379188537598, 'validation/num_examples': 50000, 'test/accuracy': 0.44670000672340393, 'test/loss': 2.569241523742676, 'test/num_examples': 10000, 'score': 6700.854817867279, 'total_duration': 7001.347071886063, 'accumulated_submission_time': 6700.854817867279, 'accumulated_eval_time': 299.4436390399933, 'accumulated_logging_time': 0.42169928550720215, 'global_step': 19681, 'preemption_count': 0}), (21198, {'train/accuracy': 0.6259167790412903, 'train/loss': 1.6084551811218262, 'validation/accuracy': 0.5781800150871277, 'validation/loss': 1.8428733348846436, 'validation/num_examples': 50000, 'test/accuracy': 0.45090001821517944, 'test/loss': 2.527494430541992, 'test/num_examples': 10000, 'score': 7210.977495670319, 'total_duration': 7537.237438201904, 'accumulated_submission_time': 7210.977495670319, 'accumulated_eval_time': 325.11325001716614, 'accumulated_logging_time': 0.47275590896606445, 'global_step': 21198, 'preemption_count': 0}), (22715, {'train/accuracy': 0.6239436864852905, 'train/loss': 1.6318671703338623, 'validation/accuracy': 0.5826199650764465, 'validation/loss': 1.8319292068481445, 'validation/num_examples': 50000, 'test/accuracy': 0.4554000198841095, 'test/loss': 2.526982307434082, 'test/num_examples': 10000, 'score': 7720.921734571457, 'total_duration': 8070.599623918533, 'accumulated_submission_time': 7720.921734571457, 'accumulated_eval_time': 348.44293189048767, 'accumulated_logging_time': 0.514392614364624, 'global_step': 22715, 'preemption_count': 0}), (24232, {'train/accuracy': 0.6275310516357422, 'train/loss': 1.646264910697937, 'validation/accuracy': 0.5839399695396423, 'validation/loss': 1.8555593490600586, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.4928462505340576, 'test/num_examples': 10000, 'score': 8231.1480448246, 'total_duration': 8605.418276548386, 'accumulated_submission_time': 8231.1480448246, 'accumulated_eval_time': 372.9615008831024, 'accumulated_logging_time': 0.5425989627838135, 'global_step': 24232, 'preemption_count': 0}), (25749, {'train/accuracy': 0.6228276491165161, 'train/loss': 1.6504992246627808, 'validation/accuracy': 0.5827800035476685, 'validation/loss': 1.8330410718917847, 'validation/num_examples': 50000, 'test/accuracy': 0.46470001339912415, 'test/loss': 2.5192012786865234, 'test/num_examples': 10000, 'score': 8741.327996253967, 'total_duration': 9138.597157239914, 'accumulated_submission_time': 8741.327996253967, 'accumulated_eval_time': 395.8874454498291, 'accumulated_logging_time': 0.5697648525238037, 'global_step': 25749, 'preemption_count': 0}), (27266, {'train/accuracy': 0.6548548936843872, 'train/loss': 1.5148216485977173, 'validation/accuracy': 0.581820011138916, 'validation/loss': 1.8570882081985474, 'validation/num_examples': 50000, 'test/accuracy': 0.4601000249385834, 'test/loss': 2.5271754264831543, 'test/num_examples': 10000, 'score': 9251.424172401428, 'total_duration': 9671.430490970612, 'accumulated_submission_time': 9251.424172401428, 'accumulated_eval_time': 418.5490050315857, 'accumulated_logging_time': 0.6001632213592529, 'global_step': 27266, 'preemption_count': 0}), (28784, {'train/accuracy': 0.6424983739852905, 'train/loss': 1.578094244003296, 'validation/accuracy': 0.5882999897003174, 'validation/loss': 1.832597017288208, 'validation/num_examples': 50000, 'test/accuracy': 0.45910000801086426, 'test/loss': 2.5145013332366943, 'test/num_examples': 10000, 'score': 9761.575938463211, 'total_duration': 10203.743408441544, 'accumulated_submission_time': 9761.575938463211, 'accumulated_eval_time': 440.63156938552856, 'accumulated_logging_time': 0.6332635879516602, 'global_step': 28784, 'preemption_count': 0}), (30303, {'train/accuracy': 0.6371771097183228, 'train/loss': 1.5771594047546387, 'validation/accuracy': 0.5897799730300903, 'validation/loss': 1.7975454330444336, 'validation/num_examples': 50000, 'test/accuracy': 0.4589000344276428, 'test/loss': 2.4995713233947754, 'test/num_examples': 10000, 'score': 10271.727610588074, 'total_duration': 10738.066817998886, 'accumulated_submission_time': 10271.727610588074, 'accumulated_eval_time': 464.72541093826294, 'accumulated_logging_time': 0.6648633480072021, 'global_step': 30303, 'preemption_count': 0}), (31821, {'train/accuracy': 0.6344068646430969, 'train/loss': 1.5880506038665771, 'validation/accuracy': 0.5906800031661987, 'validation/loss': 1.8002487421035767, 'validation/num_examples': 50000, 'test/accuracy': 0.4662000238895416, 'test/loss': 2.477463960647583, 'test/num_examples': 10000, 'score': 10781.824187994003, 'total_duration': 11272.292582035065, 'accumulated_submission_time': 10781.824187994003, 'accumulated_eval_time': 488.77872920036316, 'accumulated_logging_time': 0.6953849792480469, 'global_step': 31821, 'preemption_count': 0}), (33339, {'train/accuracy': 0.6326330900192261, 'train/loss': 1.577653169631958, 'validation/accuracy': 0.5985000133514404, 'validation/loss': 1.7677619457244873, 'validation/num_examples': 50000, 'test/accuracy': 0.47140002250671387, 'test/loss': 2.437567949295044, 'test/num_examples': 10000, 'score': 11291.99487566948, 'total_duration': 11806.442516088486, 'accumulated_submission_time': 11291.99487566948, 'accumulated_eval_time': 512.6822199821472, 'accumulated_logging_time': 0.7257883548736572, 'global_step': 33339, 'preemption_count': 0}), (34858, {'train/accuracy': 0.6404655575752258, 'train/loss': 1.569533348083496, 'validation/accuracy': 0.5995999574661255, 'validation/loss': 1.7648115158081055, 'validation/num_examples': 50000, 'test/accuracy': 0.47840002179145813, 'test/loss': 2.4060091972351074, 'test/num_examples': 10000, 'score': 11802.263848781586, 'total_duration': 12341.097544431686, 'accumulated_submission_time': 11802.263848781586, 'accumulated_eval_time': 536.9916772842407, 'accumulated_logging_time': 0.756098747253418, 'global_step': 34858, 'preemption_count': 0}), (36376, {'train/accuracy': 0.670340359210968, 'train/loss': 1.3972032070159912, 'validation/accuracy': 0.6022799611091614, 'validation/loss': 1.7204241752624512, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.39719295501709, 'test/num_examples': 10000, 'score': 12312.29682803154, 'total_duration': 12875.93498826027, 'accumulated_submission_time': 12312.29682803154, 'accumulated_eval_time': 561.7181787490845, 'accumulated_logging_time': 0.7879447937011719, 'global_step': 36376, 'preemption_count': 0}), (37894, {'train/accuracy': 0.6550741195678711, 'train/loss': 1.4708644151687622, 'validation/accuracy': 0.6022599935531616, 'validation/loss': 1.7340105772018433, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.4191813468933105, 'test/num_examples': 10000, 'score': 12822.334090471268, 'total_duration': 13409.047441959381, 'accumulated_submission_time': 12822.334090471268, 'accumulated_eval_time': 584.714658498764, 'accumulated_logging_time': 0.8208250999450684, 'global_step': 37894, 'preemption_count': 0}), (39412, {'train/accuracy': 0.6563097834587097, 'train/loss': 1.4915953874588013, 'validation/accuracy': 0.6089400053024292, 'validation/loss': 1.7161346673965454, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.380843162536621, 'test/num_examples': 10000, 'score': 13332.26151394844, 'total_duration': 13941.919185638428, 'accumulated_submission_time': 13332.26151394844, 'accumulated_eval_time': 607.5823495388031, 'accumulated_logging_time': 0.8514549732208252, 'global_step': 39412, 'preemption_count': 0}), (40930, {'train/accuracy': 0.6541174650192261, 'train/loss': 1.5210903882980347, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.733443260192871, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.3924968242645264, 'test/num_examples': 10000, 'score': 13842.186959505081, 'total_duration': 14473.955300807953, 'accumulated_submission_time': 13842.186959505081, 'accumulated_eval_time': 629.6153049468994, 'accumulated_logging_time': 0.883293628692627, 'global_step': 40930, 'preemption_count': 0}), (42448, {'train/accuracy': 0.6384526491165161, 'train/loss': 1.568603515625, 'validation/accuracy': 0.5953999757766724, 'validation/loss': 1.7692608833312988, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.4440882205963135, 'test/num_examples': 10000, 'score': 14352.171369075775, 'total_duration': 15006.269856929779, 'accumulated_submission_time': 14352.171369075775, 'accumulated_eval_time': 651.8660788536072, 'accumulated_logging_time': 0.916776180267334, 'global_step': 42448, 'preemption_count': 0}), (43967, {'train/accuracy': 0.6627271771430969, 'train/loss': 1.4629147052764893, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6995658874511719, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.402573585510254, 'test/num_examples': 10000, 'score': 14862.203121185303, 'total_duration': 15539.220652580261, 'accumulated_submission_time': 14862.203121185303, 'accumulated_eval_time': 674.709219455719, 'accumulated_logging_time': 0.9466893672943115, 'global_step': 43967, 'preemption_count': 0}), (45486, {'train/accuracy': 0.674246609210968, 'train/loss': 1.4069665670394897, 'validation/accuracy': 0.6114199757575989, 'validation/loss': 1.696818232536316, 'validation/num_examples': 50000, 'test/accuracy': 0.4945000112056732, 'test/loss': 2.347254753112793, 'test/num_examples': 10000, 'score': 15372.199818134308, 'total_duration': 16071.047409534454, 'accumulated_submission_time': 15372.199818134308, 'accumulated_eval_time': 696.4635140895844, 'accumulated_logging_time': 0.9763743877410889, 'global_step': 45486, 'preemption_count': 0}), (47004, {'train/accuracy': 0.6636040806770325, 'train/loss': 1.4506380558013916, 'validation/accuracy': 0.6128799915313721, 'validation/loss': 1.6783645153045654, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3279991149902344, 'test/num_examples': 10000, 'score': 15882.18303823471, 'total_duration': 16602.090711593628, 'accumulated_submission_time': 15882.18303823471, 'accumulated_eval_time': 717.4448945522308, 'accumulated_logging_time': 1.0088527202606201, 'global_step': 47004, 'preemption_count': 0}), (48523, {'train/accuracy': 0.6721540093421936, 'train/loss': 1.4366706609725952, 'validation/accuracy': 0.6180399656295776, 'validation/loss': 1.684572458267212, 'validation/num_examples': 50000, 'test/accuracy': 0.4991000294685364, 'test/loss': 2.3448281288146973, 'test/num_examples': 10000, 'score': 16392.207607269287, 'total_duration': 17133.673773765564, 'accumulated_submission_time': 16392.207607269287, 'accumulated_eval_time': 738.927268743515, 'accumulated_logging_time': 1.0390520095825195, 'global_step': 48523, 'preemption_count': 0}), (50042, {'train/accuracy': 0.6578244566917419, 'train/loss': 1.4930152893066406, 'validation/accuracy': 0.6173999905586243, 'validation/loss': 1.6958266496658325, 'validation/num_examples': 50000, 'test/accuracy': 0.491100013256073, 'test/loss': 2.3817460536956787, 'test/num_examples': 10000, 'score': 16902.16853904724, 'total_duration': 17663.81556916237, 'accumulated_submission_time': 16902.16853904724, 'accumulated_eval_time': 759.0310838222504, 'accumulated_logging_time': 1.070474624633789, 'global_step': 50042, 'preemption_count': 0}), (51561, {'train/accuracy': 0.6567083597183228, 'train/loss': 1.477152943611145, 'validation/accuracy': 0.613099992275238, 'validation/loss': 1.6816418170928955, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.355665445327759, 'test/num_examples': 10000, 'score': 17412.375111818314, 'total_duration': 18193.11154937744, 'accumulated_submission_time': 17412.375111818314, 'accumulated_eval_time': 778.0412209033966, 'accumulated_logging_time': 1.1035346984863281, 'global_step': 51561, 'preemption_count': 0}), (53081, {'train/accuracy': 0.7010921239852905, 'train/loss': 1.2759617567062378, 'validation/accuracy': 0.6177399754524231, 'validation/loss': 1.6607871055603027, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.3438918590545654, 'test/num_examples': 10000, 'score': 17922.586621522903, 'total_duration': 18721.159491062164, 'accumulated_submission_time': 17922.586621522903, 'accumulated_eval_time': 795.7929601669312, 'accumulated_logging_time': 1.1418347358703613, 'global_step': 53081, 'preemption_count': 0}), (54600, {'train/accuracy': 0.6756815910339355, 'train/loss': 1.4029406309127808, 'validation/accuracy': 0.6170799732208252, 'validation/loss': 1.6762901544570923, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.347243547439575, 'test/num_examples': 10000, 'score': 18432.664180994034, 'total_duration': 19248.851968050003, 'accumulated_submission_time': 18432.664180994034, 'accumulated_eval_time': 813.3257002830505, 'accumulated_logging_time': 1.1776843070983887, 'global_step': 54600, 'preemption_count': 0}), (56119, {'train/accuracy': 0.6758211255073547, 'train/loss': 1.4040615558624268, 'validation/accuracy': 0.6243000030517578, 'validation/loss': 1.6517601013183594, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.3165369033813477, 'test/num_examples': 10000, 'score': 18942.804827928543, 'total_duration': 19778.317579746246, 'accumulated_submission_time': 18942.804827928543, 'accumulated_eval_time': 832.5534996986389, 'accumulated_logging_time': 1.2281639575958252, 'global_step': 56119, 'preemption_count': 0}), (57639, {'train/accuracy': 0.6713966727256775, 'train/loss': 1.409226417541504, 'validation/accuracy': 0.6262199878692627, 'validation/loss': 1.636522889137268, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.2848803997039795, 'test/num_examples': 10000, 'score': 19453.03369998932, 'total_duration': 20306.5486035347, 'accumulated_submission_time': 19453.03369998932, 'accumulated_eval_time': 850.4759373664856, 'accumulated_logging_time': 1.2610328197479248, 'global_step': 57639, 'preemption_count': 0}), (59157, {'train/accuracy': 0.6717753410339355, 'train/loss': 1.4401389360427856, 'validation/accuracy': 0.6253199577331543, 'validation/loss': 1.6600350141525269, 'validation/num_examples': 50000, 'test/accuracy': 0.49870002269744873, 'test/loss': 2.3283464908599854, 'test/num_examples': 10000, 'score': 19962.99881720543, 'total_duration': 20833.943662643433, 'accumulated_submission_time': 19962.99881720543, 'accumulated_eval_time': 867.8226597309113, 'accumulated_logging_time': 1.2980821132659912, 'global_step': 59157, 'preemption_count': 0}), (60677, {'train/accuracy': 0.6564891338348389, 'train/loss': 1.4987339973449707, 'validation/accuracy': 0.614579975605011, 'validation/loss': 1.6961729526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.49560001492500305, 'test/loss': 2.3575313091278076, 'test/num_examples': 10000, 'score': 20473.223114967346, 'total_duration': 21361.555997371674, 'accumulated_submission_time': 20473.223114967346, 'accumulated_eval_time': 885.1255774497986, 'accumulated_logging_time': 1.3364100456237793, 'global_step': 60677, 'preemption_count': 0}), (62196, {'train/accuracy': 0.7053770422935486, 'train/loss': 1.2767313718795776, 'validation/accuracy': 0.6273599863052368, 'validation/loss': 1.6219308376312256, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.3041574954986572, 'test/num_examples': 10000, 'score': 20983.24218249321, 'total_duration': 21889.161187648773, 'accumulated_submission_time': 20983.24218249321, 'accumulated_eval_time': 902.6221942901611, 'accumulated_logging_time': 1.3790311813354492, 'global_step': 62196, 'preemption_count': 0}), (63715, {'train/accuracy': 0.686922013759613, 'train/loss': 1.3271760940551758, 'validation/accuracy': 0.626800000667572, 'validation/loss': 1.617149829864502, 'validation/num_examples': 50000, 'test/accuracy': 0.5064000487327576, 'test/loss': 2.267906904220581, 'test/num_examples': 10000, 'score': 21493.278660058975, 'total_duration': 22416.715700864792, 'accumulated_submission_time': 21493.278660058975, 'accumulated_eval_time': 920.0448322296143, 'accumulated_logging_time': 1.4266955852508545, 'global_step': 63715, 'preemption_count': 0}), (65234, {'train/accuracy': 0.6711774468421936, 'train/loss': 1.4122507572174072, 'validation/accuracy': 0.6250199675559998, 'validation/loss': 1.6446616649627686, 'validation/num_examples': 50000, 'test/accuracy': 0.49890002608299255, 'test/loss': 2.329805850982666, 'test/num_examples': 10000, 'score': 22003.35453414917, 'total_duration': 22944.030207157135, 'accumulated_submission_time': 22003.35453414917, 'accumulated_eval_time': 937.196320772171, 'accumulated_logging_time': 1.4666364192962646, 'global_step': 65234, 'preemption_count': 0}), (66753, {'train/accuracy': 0.6834542155265808, 'train/loss': 1.3994112014770508, 'validation/accuracy': 0.635159969329834, 'validation/loss': 1.6157879829406738, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2873687744140625, 'test/num_examples': 10000, 'score': 22513.463595867157, 'total_duration': 23471.462081432343, 'accumulated_submission_time': 22513.463595867157, 'accumulated_eval_time': 954.4305288791656, 'accumulated_logging_time': 1.5082590579986572, 'global_step': 66753, 'preemption_count': 0}), (68272, {'train/accuracy': 0.6808832883834839, 'train/loss': 1.3870117664337158, 'validation/accuracy': 0.6350199580192566, 'validation/loss': 1.601508378982544, 'validation/num_examples': 50000, 'test/accuracy': 0.5113000273704529, 'test/loss': 2.2549126148223877, 'test/num_examples': 10000, 'score': 23023.515419483185, 'total_duration': 23998.77006816864, 'accumulated_submission_time': 23023.515419483185, 'accumulated_eval_time': 971.5997366905212, 'accumulated_logging_time': 1.5481698513031006, 'global_step': 68272, 'preemption_count': 0}), (69791, {'train/accuracy': 0.6853276491165161, 'train/loss': 1.3273032903671265, 'validation/accuracy': 0.6344199776649475, 'validation/loss': 1.5748940706253052, 'validation/num_examples': 50000, 'test/accuracy': 0.5146000385284424, 'test/loss': 2.2334415912628174, 'test/num_examples': 10000, 'score': 23533.512261867523, 'total_duration': 24526.164582252502, 'accumulated_submission_time': 23533.512261867523, 'accumulated_eval_time': 988.9092743396759, 'accumulated_logging_time': 1.5890882015228271, 'global_step': 69791, 'preemption_count': 0}), (71310, {'train/accuracy': 0.7115951776504517, 'train/loss': 1.2286536693572998, 'validation/accuracy': 0.6373599767684937, 'validation/loss': 1.564759612083435, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.210066556930542, 'test/num_examples': 10000, 'score': 24043.516547441483, 'total_duration': 25053.56804537773, 'accumulated_submission_time': 24043.516547441483, 'accumulated_eval_time': 1006.218918800354, 'accumulated_logging_time': 1.6315371990203857, 'global_step': 71310, 'preemption_count': 0}), (72830, {'train/accuracy': 0.6828762888908386, 'train/loss': 1.3698737621307373, 'validation/accuracy': 0.6262800097465515, 'validation/loss': 1.6336623430252075, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.260319232940674, 'test/num_examples': 10000, 'score': 24553.690885543823, 'total_duration': 25580.968412399292, 'accumulated_submission_time': 24553.690885543823, 'accumulated_eval_time': 1023.3500711917877, 'accumulated_logging_time': 1.6788830757141113, 'global_step': 72830, 'preemption_count': 0}), (74349, {'train/accuracy': 0.6959502696990967, 'train/loss': 1.3203473091125488, 'validation/accuracy': 0.6404399871826172, 'validation/loss': 1.5740586519241333, 'validation/num_examples': 50000, 'test/accuracy': 0.5154000520706177, 'test/loss': 2.2236592769622803, 'test/num_examples': 10000, 'score': 25063.78478884697, 'total_duration': 26108.48323750496, 'accumulated_submission_time': 25063.78478884697, 'accumulated_eval_time': 1040.6843490600586, 'accumulated_logging_time': 1.7180449962615967, 'global_step': 74349, 'preemption_count': 0}), (75868, {'train/accuracy': 0.6812818646430969, 'train/loss': 1.3611277341842651, 'validation/accuracy': 0.6279599666595459, 'validation/loss': 1.6080148220062256, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.291550636291504, 'test/num_examples': 10000, 'score': 25573.829756498337, 'total_duration': 26636.57822918892, 'accumulated_submission_time': 25573.829756498337, 'accumulated_eval_time': 1058.6442294120789, 'accumulated_logging_time': 1.7606401443481445, 'global_step': 75868, 'preemption_count': 0}), (77387, {'train/accuracy': 0.6804049611091614, 'train/loss': 1.39091157913208, 'validation/accuracy': 0.6298800110816956, 'validation/loss': 1.6200053691864014, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.2878735065460205, 'test/num_examples': 10000, 'score': 26084.01960873604, 'total_duration': 27163.98302912712, 'accumulated_submission_time': 26084.01960873604, 'accumulated_eval_time': 1075.7697627544403, 'accumulated_logging_time': 1.8028457164764404, 'global_step': 77387, 'preemption_count': 0}), (78903, {'train/accuracy': 0.7225167155265808, 'train/loss': 1.2051185369491577, 'validation/accuracy': 0.6467399597167969, 'validation/loss': 1.5430620908737183, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.2047617435455322, 'test/num_examples': 10000, 'score': 26593.105378627777, 'total_duration': 27691.369906663895, 'accumulated_submission_time': 26593.105378627777, 'accumulated_eval_time': 1093.0664336681366, 'accumulated_logging_time': 2.7603235244750977, 'global_step': 78903, 'preemption_count': 0}), (80423, {'train/accuracy': 0.6974050998687744, 'train/loss': 1.309516429901123, 'validation/accuracy': 0.6312400102615356, 'validation/loss': 1.6141302585601807, 'validation/num_examples': 50000, 'test/accuracy': 0.5088000297546387, 'test/loss': 2.290923833847046, 'test/num_examples': 10000, 'score': 27103.18165063858, 'total_duration': 28218.77651834488, 'accumulated_submission_time': 27103.18165063858, 'accumulated_eval_time': 1110.3081967830658, 'accumulated_logging_time': 2.8014683723449707, 'global_step': 80423, 'preemption_count': 0}), (81943, {'train/accuracy': 0.7071707248687744, 'train/loss': 1.2736690044403076, 'validation/accuracy': 0.6467799544334412, 'validation/loss': 1.539712905883789, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.181778907775879, 'test/num_examples': 10000, 'score': 27613.21877503395, 'total_duration': 28746.222467899323, 'accumulated_submission_time': 27613.21877503395, 'accumulated_eval_time': 1127.628051996231, 'accumulated_logging_time': 2.843165397644043, 'global_step': 81943, 'preemption_count': 0}), (83462, {'train/accuracy': 0.7016302347183228, 'train/loss': 1.2925941944122314, 'validation/accuracy': 0.6476199626922607, 'validation/loss': 1.5447707176208496, 'validation/num_examples': 50000, 'test/accuracy': 0.5254000425338745, 'test/loss': 2.181201457977295, 'test/num_examples': 10000, 'score': 28123.213754415512, 'total_duration': 29273.577413082123, 'accumulated_submission_time': 28123.213754415512, 'accumulated_eval_time': 1144.8795185089111, 'accumulated_logging_time': 2.9040091037750244, 'global_step': 83462, 'preemption_count': 0}), (84982, {'train/accuracy': 0.6917649507522583, 'train/loss': 1.3234705924987793, 'validation/accuracy': 0.6398599743843079, 'validation/loss': 1.566019058227539, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.230949878692627, 'test/num_examples': 10000, 'score': 28633.28370141983, 'total_duration': 29800.936242103577, 'accumulated_submission_time': 28633.28370141983, 'accumulated_eval_time': 1162.0792186260223, 'accumulated_logging_time': 2.9459140300750732, 'global_step': 84982, 'preemption_count': 0}), (86501, {'train/accuracy': 0.7071906924247742, 'train/loss': 1.2525839805603027, 'validation/accuracy': 0.6568399667739868, 'validation/loss': 1.4887808561325073, 'validation/num_examples': 50000, 'test/accuracy': 0.5342000126838684, 'test/loss': 2.135103940963745, 'test/num_examples': 10000, 'score': 29143.66135954857, 'total_duration': 30328.49675798416, 'accumulated_submission_time': 29143.66135954857, 'accumulated_eval_time': 1179.1734466552734, 'accumulated_logging_time': 2.9869160652160645, 'global_step': 86501, 'preemption_count': 0}), (88020, {'train/accuracy': 0.7352319955825806, 'train/loss': 1.1445754766464233, 'validation/accuracy': 0.649459958076477, 'validation/loss': 1.5333545207977295, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.18676495552063, 'test/num_examples': 10000, 'score': 29653.78978037834, 'total_duration': 30855.992782831192, 'accumulated_submission_time': 29653.78978037834, 'accumulated_eval_time': 1196.4498274326324, 'accumulated_logging_time': 3.030723810195923, 'global_step': 88020, 'preemption_count': 0}), (89541, {'train/accuracy': 0.7146444320678711, 'train/loss': 1.2600167989730835, 'validation/accuracy': 0.6505399942398071, 'validation/loss': 1.539999008178711, 'validation/num_examples': 50000, 'test/accuracy': 0.523900032043457, 'test/loss': 2.197510004043579, 'test/num_examples': 10000, 'score': 30164.02828192711, 'total_duration': 31383.692828655243, 'accumulated_submission_time': 30164.02828192711, 'accumulated_eval_time': 1213.8182995319366, 'accumulated_logging_time': 3.0758121013641357, 'global_step': 89541, 'preemption_count': 0}), (91061, {'train/accuracy': 0.7093231678009033, 'train/loss': 1.2750592231750488, 'validation/accuracy': 0.6477000117301941, 'validation/loss': 1.54591703414917, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.186326503753662, 'test/num_examples': 10000, 'score': 30674.164899349213, 'total_duration': 31911.384786367416, 'accumulated_submission_time': 30674.164899349213, 'accumulated_eval_time': 1231.2799394130707, 'accumulated_logging_time': 3.1218960285186768, 'global_step': 91061, 'preemption_count': 0}), (92581, {'train/accuracy': 0.720723032951355, 'train/loss': 1.2018429040908813, 'validation/accuracy': 0.6581999659538269, 'validation/loss': 1.4694814682006836, 'validation/num_examples': 50000, 'test/accuracy': 0.5351000428199768, 'test/loss': 2.1427531242370605, 'test/num_examples': 10000, 'score': 31184.35217189789, 'total_duration': 32438.77952504158, 'accumulated_submission_time': 31184.35217189789, 'accumulated_eval_time': 1248.3998510837555, 'accumulated_logging_time': 3.161928653717041, 'global_step': 92581, 'preemption_count': 0}), (94100, {'train/accuracy': 0.711355984210968, 'train/loss': 1.227055311203003, 'validation/accuracy': 0.6619600057601929, 'validation/loss': 1.455871343612671, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.120694875717163, 'test/num_examples': 10000, 'score': 31694.415167331696, 'total_duration': 32966.347737550735, 'accumulated_submission_time': 31694.415167331696, 'accumulated_eval_time': 1265.8118696212769, 'accumulated_logging_time': 3.2076263427734375, 'global_step': 94100, 'preemption_count': 0}), (95620, {'train/accuracy': 0.7209023833274841, 'train/loss': 1.197264552116394, 'validation/accuracy': 0.6657999753952026, 'validation/loss': 1.4429244995117188, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.099045753479004, 'test/num_examples': 10000, 'score': 32204.58331465721, 'total_duration': 33494.21009898186, 'accumulated_submission_time': 32204.58331465721, 'accumulated_eval_time': 1283.4004225730896, 'accumulated_logging_time': 3.2659912109375, 'global_step': 95620, 'preemption_count': 0}), (97141, {'train/accuracy': 0.7374441623687744, 'train/loss': 1.1343486309051514, 'validation/accuracy': 0.6576399803161621, 'validation/loss': 1.4939663410186768, 'validation/num_examples': 50000, 'test/accuracy': 0.5304000377655029, 'test/loss': 2.161627769470215, 'test/num_examples': 10000, 'score': 32714.638087511063, 'total_duration': 34021.7456908226, 'accumulated_submission_time': 32714.638087511063, 'accumulated_eval_time': 1300.787608385086, 'accumulated_logging_time': 3.3119215965270996, 'global_step': 97141, 'preemption_count': 0}), (98661, {'train/accuracy': 0.7361487150192261, 'train/loss': 1.1169815063476562, 'validation/accuracy': 0.6668399572372437, 'validation/loss': 1.4288816452026367, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.060675621032715, 'test/num_examples': 10000, 'score': 33224.680389881134, 'total_duration': 34549.06488656998, 'accumulated_submission_time': 33224.680389881134, 'accumulated_eval_time': 1317.972489118576, 'accumulated_logging_time': 3.3565704822540283, 'global_step': 98661, 'preemption_count': 0}), (100181, {'train/accuracy': 0.7201650142669678, 'train/loss': 1.1991384029388428, 'validation/accuracy': 0.6631199717521667, 'validation/loss': 1.4586812257766724, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.1067442893981934, 'test/num_examples': 10000, 'score': 33734.73139023781, 'total_duration': 35076.42337989807, 'accumulated_submission_time': 33734.73139023781, 'accumulated_eval_time': 1335.168357372284, 'accumulated_logging_time': 3.4204201698303223, 'global_step': 100181, 'preemption_count': 0}), (101700, {'train/accuracy': 0.7339365482330322, 'train/loss': 1.128197193145752, 'validation/accuracy': 0.6735199689865112, 'validation/loss': 1.4085602760314941, 'validation/num_examples': 50000, 'test/accuracy': 0.5473999977111816, 'test/loss': 2.062584638595581, 'test/num_examples': 10000, 'score': 34244.87643456459, 'total_duration': 35603.890127658844, 'accumulated_submission_time': 34244.87643456459, 'accumulated_eval_time': 1352.396959066391, 'accumulated_logging_time': 3.4652857780456543, 'global_step': 101700, 'preemption_count': 0}), (103220, {'train/accuracy': 0.7251275181770325, 'train/loss': 1.1850506067276, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.444494366645813, 'validation/num_examples': 50000, 'test/accuracy': 0.539900004863739, 'test/loss': 2.114496946334839, 'test/num_examples': 10000, 'score': 34754.832845926285, 'total_duration': 36131.21515202522, 'accumulated_submission_time': 34754.832845926285, 'accumulated_eval_time': 1369.6633830070496, 'accumulated_logging_time': 3.5199079513549805, 'global_step': 103220, 'preemption_count': 0}), (104740, {'train/accuracy': 0.7435427308082581, 'train/loss': 1.0935951471328735, 'validation/accuracy': 0.6726999878883362, 'validation/loss': 1.40591561794281, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.052886724472046, 'test/num_examples': 10000, 'score': 35265.055674791336, 'total_duration': 36658.89939570427, 'accumulated_submission_time': 35265.055674791336, 'accumulated_eval_time': 1387.0266127586365, 'accumulated_logging_time': 3.5705230236053467, 'global_step': 104740, 'preemption_count': 0}), (106260, {'train/accuracy': 0.7564970850944519, 'train/loss': 1.039839744567871, 'validation/accuracy': 0.6774199604988098, 'validation/loss': 1.3940984010696411, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.0571823120117188, 'test/num_examples': 10000, 'score': 35775.128962278366, 'total_duration': 37186.31377243996, 'accumulated_submission_time': 35775.128962278366, 'accumulated_eval_time': 1404.2709031105042, 'accumulated_logging_time': 3.6195414066314697, 'global_step': 106260, 'preemption_count': 0}), (107781, {'train/accuracy': 0.7531289458274841, 'train/loss': 1.063181757926941, 'validation/accuracy': 0.6829400062561035, 'validation/loss': 1.3601897954940796, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.0226991176605225, 'test/num_examples': 10000, 'score': 36285.17335796356, 'total_duration': 37713.587841272354, 'accumulated_submission_time': 36285.17335796356, 'accumulated_eval_time': 1421.402559518814, 'accumulated_logging_time': 3.66951847076416, 'global_step': 107781, 'preemption_count': 0}), (109301, {'train/accuracy': 0.7407127022743225, 'train/loss': 1.1150754690170288, 'validation/accuracy': 0.6787599921226501, 'validation/loss': 1.402061939239502, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.055934190750122, 'test/num_examples': 10000, 'score': 36795.37970161438, 'total_duration': 38241.43623971939, 'accumulated_submission_time': 36795.37970161438, 'accumulated_eval_time': 1438.9501745700836, 'accumulated_logging_time': 3.7160933017730713, 'global_step': 109301, 'preemption_count': 0}), (110820, {'train/accuracy': 0.746113657951355, 'train/loss': 1.0706056356430054, 'validation/accuracy': 0.685479998588562, 'validation/loss': 1.3558672666549683, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 2.0043318271636963, 'test/num_examples': 10000, 'score': 37305.41358447075, 'total_duration': 38768.90533566475, 'accumulated_submission_time': 37305.41358447075, 'accumulated_eval_time': 1456.2903575897217, 'accumulated_logging_time': 3.7633721828460693, 'global_step': 110820, 'preemption_count': 0}), (112340, {'train/accuracy': 0.7463727593421936, 'train/loss': 1.0985363721847534, 'validation/accuracy': 0.6815400123596191, 'validation/loss': 1.3779244422912598, 'validation/num_examples': 50000, 'test/accuracy': 0.5627000331878662, 'test/loss': 2.016084909439087, 'test/num_examples': 10000, 'score': 37815.575922966, 'total_duration': 39296.36831307411, 'accumulated_submission_time': 37815.575922966, 'accumulated_eval_time': 1473.4958517551422, 'accumulated_logging_time': 3.8107833862304688, 'global_step': 112340, 'preemption_count': 0}), (113860, {'train/accuracy': 0.7833625674247742, 'train/loss': 0.9273353815078735, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.3640166521072388, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.0014493465423584, 'test/num_examples': 10000, 'score': 38325.62386679649, 'total_duration': 39823.877307891846, 'accumulated_submission_time': 38325.62386679649, 'accumulated_eval_time': 1490.8633544445038, 'accumulated_logging_time': 3.8569798469543457, 'global_step': 113860, 'preemption_count': 0}), (115379, {'train/accuracy': 0.7703882455825806, 'train/loss': 1.0078136920928955, 'validation/accuracy': 0.6881399750709534, 'validation/loss': 1.3621941804885864, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9929922819137573, 'test/num_examples': 10000, 'score': 38835.71640062332, 'total_duration': 40351.181077718735, 'accumulated_submission_time': 38835.71640062332, 'accumulated_eval_time': 1507.9782707691193, 'accumulated_logging_time': 3.9053003787994385, 'global_step': 115379, 'preemption_count': 0}), (116900, {'train/accuracy': 0.7623764276504517, 'train/loss': 1.0332750082015991, 'validation/accuracy': 0.694599986076355, 'validation/loss': 1.3372550010681152, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.9591028690338135, 'test/num_examples': 10000, 'score': 39345.8971452713, 'total_duration': 40878.56984305382, 'accumulated_submission_time': 39345.8971452713, 'accumulated_eval_time': 1525.088080406189, 'accumulated_logging_time': 3.955566167831421, 'global_step': 116900, 'preemption_count': 0}), (118420, {'train/accuracy': 0.7677973508834839, 'train/loss': 0.9986758232116699, 'validation/accuracy': 0.6958000063896179, 'validation/loss': 1.3079572916030884, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.957592248916626, 'test/num_examples': 10000, 'score': 39856.08269357681, 'total_duration': 41406.01397848129, 'accumulated_submission_time': 39856.08269357681, 'accumulated_eval_time': 1542.2515261173248, 'accumulated_logging_time': 4.002415657043457, 'global_step': 118420, 'preemption_count': 0}), (119939, {'train/accuracy': 0.760184109210968, 'train/loss': 1.0345245599746704, 'validation/accuracy': 0.6921399831771851, 'validation/loss': 1.3318723440170288, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.9610300064086914, 'test/num_examples': 10000, 'score': 40366.02552604675, 'total_duration': 41933.3487534523, 'accumulated_submission_time': 40366.02552604675, 'accumulated_eval_time': 1559.540601491928, 'accumulated_logging_time': 4.057379245758057, 'global_step': 119939, 'preemption_count': 0}), (121459, {'train/accuracy': 0.7603236436843872, 'train/loss': 1.0235108137130737, 'validation/accuracy': 0.6966999769210815, 'validation/loss': 1.3048481941223145, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 1.9369956254959106, 'test/num_examples': 10000, 'score': 40876.209104537964, 'total_duration': 42460.95462989807, 'accumulated_submission_time': 40876.209104537964, 'accumulated_eval_time': 1576.865121126175, 'accumulated_logging_time': 4.10741400718689, 'global_step': 121459, 'preemption_count': 0}), (122979, {'train/accuracy': 0.7931680083274841, 'train/loss': 0.8888539671897888, 'validation/accuracy': 0.6993399858474731, 'validation/loss': 1.2950785160064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.9291913509368896, 'test/num_examples': 10000, 'score': 41386.174983263016, 'total_duration': 42988.33508872986, 'accumulated_submission_time': 41386.174983263016, 'accumulated_eval_time': 1594.183688402176, 'accumulated_logging_time': 4.156118869781494, 'global_step': 122979, 'preemption_count': 0}), (124498, {'train/accuracy': 0.7772042155265808, 'train/loss': 0.9544240832328796, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.3114979267120361, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9389946460723877, 'test/num_examples': 10000, 'score': 41896.13381576538, 'total_duration': 43515.690836429596, 'accumulated_submission_time': 41896.13381576538, 'accumulated_eval_time': 1611.4827795028687, 'accumulated_logging_time': 4.206348896026611, 'global_step': 124498, 'preemption_count': 0}), (126016, {'train/accuracy': 0.7820471525192261, 'train/loss': 0.9358399510383606, 'validation/accuracy': 0.7050999999046326, 'validation/loss': 1.2812731266021729, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 1.9185949563980103, 'test/num_examples': 10000, 'score': 42406.0859875679, 'total_duration': 44042.85533833504, 'accumulated_submission_time': 42406.0859875679, 'accumulated_eval_time': 1628.5918712615967, 'accumulated_logging_time': 4.261731863021851, 'global_step': 126016, 'preemption_count': 0}), (127536, {'train/accuracy': 0.7782206535339355, 'train/loss': 0.9608622193336487, 'validation/accuracy': 0.7054600119590759, 'validation/loss': 1.2842296361923218, 'validation/num_examples': 50000, 'test/accuracy': 0.579200029373169, 'test/loss': 1.9385350942611694, 'test/num_examples': 10000, 'score': 42916.115965127945, 'total_duration': 44570.39530873299, 'accumulated_submission_time': 42916.115965127945, 'accumulated_eval_time': 1646.0038397312164, 'accumulated_logging_time': 4.311935186386108, 'global_step': 127536, 'preemption_count': 0}), (129054, {'train/accuracy': 0.7837212681770325, 'train/loss': 0.9352360963821411, 'validation/accuracy': 0.712179958820343, 'validation/loss': 1.254599690437317, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.8815577030181885, 'test/num_examples': 10000, 'score': 43425.82639288902, 'total_duration': 45097.83585691452, 'accumulated_submission_time': 43425.82639288902, 'accumulated_eval_time': 1663.313729763031, 'accumulated_logging_time': 4.684342861175537, 'global_step': 129054, 'preemption_count': 0}), (130573, {'train/accuracy': 0.7919722199440002, 'train/loss': 0.8858956694602966, 'validation/accuracy': 0.7138800024986267, 'validation/loss': 1.2260390520095825, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.860069990158081, 'test/num_examples': 10000, 'score': 43935.96074128151, 'total_duration': 45625.14785838127, 'accumulated_submission_time': 43935.96074128151, 'accumulated_eval_time': 1680.3904614448547, 'accumulated_logging_time': 4.7375593185424805, 'global_step': 130573, 'preemption_count': 0}), (132093, {'train/accuracy': 0.8088328838348389, 'train/loss': 0.8323812484741211, 'validation/accuracy': 0.7155199646949768, 'validation/loss': 1.2397068738937378, 'validation/num_examples': 50000, 'test/accuracy': 0.5836000442504883, 'test/loss': 1.8860760927200317, 'test/num_examples': 10000, 'score': 44446.12138080597, 'total_duration': 46152.46186089516, 'accumulated_submission_time': 44446.12138080597, 'accumulated_eval_time': 1697.4430515766144, 'accumulated_logging_time': 4.790385007858276, 'global_step': 132093, 'preemption_count': 0}), (133612, {'train/accuracy': 0.7983298897743225, 'train/loss': 0.8808016180992126, 'validation/accuracy': 0.7109400033950806, 'validation/loss': 1.252920150756836, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.9056119918823242, 'test/num_examples': 10000, 'score': 44956.11803674698, 'total_duration': 46679.69448518753, 'accumulated_submission_time': 44956.11803674698, 'accumulated_eval_time': 1714.5795366764069, 'accumulated_logging_time': 4.842229604721069, 'global_step': 133612, 'preemption_count': 0}), (135131, {'train/accuracy': 0.8057836294174194, 'train/loss': 0.8274676203727722, 'validation/accuracy': 0.7206000089645386, 'validation/loss': 1.196054220199585, 'validation/num_examples': 50000, 'test/accuracy': 0.5926000475883484, 'test/loss': 1.841191053390503, 'test/num_examples': 10000, 'score': 45466.051250457764, 'total_duration': 47206.79020857811, 'accumulated_submission_time': 45466.051250457764, 'accumulated_eval_time': 1731.6397836208344, 'accumulated_logging_time': 4.896440029144287, 'global_step': 135131, 'preemption_count': 0}), (136650, {'train/accuracy': 0.7958585619926453, 'train/loss': 0.875244677066803, 'validation/accuracy': 0.7117800116539001, 'validation/loss': 1.2368409633636475, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 1.8613744974136353, 'test/num_examples': 10000, 'score': 45976.103113889694, 'total_duration': 47734.25307178497, 'accumulated_submission_time': 45976.103113889694, 'accumulated_eval_time': 1748.9506647586823, 'accumulated_logging_time': 4.948869228363037, 'global_step': 136650, 'preemption_count': 0}), (138169, {'train/accuracy': 0.8053850531578064, 'train/loss': 0.8225789070129395, 'validation/accuracy': 0.723859965801239, 'validation/loss': 1.1837297677993774, 'validation/num_examples': 50000, 'test/accuracy': 0.6002000570297241, 'test/loss': 1.803093433380127, 'test/num_examples': 10000, 'score': 46486.0466632843, 'total_duration': 48261.59885954857, 'accumulated_submission_time': 46486.0466632843, 'accumulated_eval_time': 1766.2522943019867, 'accumulated_logging_time': 5.0018205642700195, 'global_step': 138169, 'preemption_count': 0}), (139689, {'train/accuracy': 0.8387476205825806, 'train/loss': 0.7008848190307617, 'validation/accuracy': 0.7271199822425842, 'validation/loss': 1.168049693107605, 'validation/num_examples': 50000, 'test/accuracy': 0.6009000539779663, 'test/loss': 1.8257156610488892, 'test/num_examples': 10000, 'score': 46996.255591869354, 'total_duration': 48789.38086462021, 'accumulated_submission_time': 46996.255591869354, 'accumulated_eval_time': 1783.7273569107056, 'accumulated_logging_time': 5.05206823348999, 'global_step': 139689, 'preemption_count': 0}), (141209, {'train/accuracy': 0.8303571343421936, 'train/loss': 0.7429234385490417, 'validation/accuracy': 0.725600004196167, 'validation/loss': 1.1872261762619019, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7983014583587646, 'test/num_examples': 10000, 'score': 47506.24500584602, 'total_duration': 49316.61631822586, 'accumulated_submission_time': 47506.24500584602, 'accumulated_eval_time': 1800.8658833503723, 'accumulated_logging_time': 5.111589431762695, 'global_step': 141209, 'preemption_count': 0}), (142728, {'train/accuracy': 0.8176219463348389, 'train/loss': 0.8000547885894775, 'validation/accuracy': 0.7247999906539917, 'validation/loss': 1.205077886581421, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.849668025970459, 'test/num_examples': 10000, 'score': 48016.3487842083, 'total_duration': 49844.00881743431, 'accumulated_submission_time': 48016.3487842083, 'accumulated_eval_time': 1818.0516149997711, 'accumulated_logging_time': 5.166984558105469, 'global_step': 142728, 'preemption_count': 0}), (144248, {'train/accuracy': 0.8232421875, 'train/loss': 0.7640501856803894, 'validation/accuracy': 0.7290999889373779, 'validation/loss': 1.1696761846542358, 'validation/num_examples': 50000, 'test/accuracy': 0.6071000099182129, 'test/loss': 1.7840397357940674, 'test/num_examples': 10000, 'score': 48526.44045686722, 'total_duration': 50371.45580005646, 'accumulated_submission_time': 48526.44045686722, 'accumulated_eval_time': 1835.3061537742615, 'accumulated_logging_time': 5.220117092132568, 'global_step': 144248, 'preemption_count': 0}), (145767, {'train/accuracy': 0.8306760191917419, 'train/loss': 0.7338430881500244, 'validation/accuracy': 0.735040009021759, 'validation/loss': 1.146825909614563, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.7603540420532227, 'test/num_examples': 10000, 'score': 49036.527978897095, 'total_duration': 50899.024411439896, 'accumulated_submission_time': 49036.527978897095, 'accumulated_eval_time': 1852.6867318153381, 'accumulated_logging_time': 5.2726123332977295, 'global_step': 145767, 'preemption_count': 0}), (147287, {'train/accuracy': 0.8342036008834839, 'train/loss': 0.7212420701980591, 'validation/accuracy': 0.7377399802207947, 'validation/loss': 1.13205885887146, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.7458066940307617, 'test/num_examples': 10000, 'score': 49546.45242190361, 'total_duration': 51426.31285953522, 'accumulated_submission_time': 49546.45242190361, 'accumulated_eval_time': 1869.9471807479858, 'accumulated_logging_time': 5.328448534011841, 'global_step': 147287, 'preemption_count': 0}), (148806, {'train/accuracy': 0.8618662357330322, 'train/loss': 0.6142693758010864, 'validation/accuracy': 0.7396999597549438, 'validation/loss': 1.1240257024765015, 'validation/num_examples': 50000, 'test/accuracy': 0.6178000569343567, 'test/loss': 1.7484245300292969, 'test/num_examples': 10000, 'score': 50056.575719833374, 'total_duration': 51953.74762201309, 'accumulated_submission_time': 50056.575719833374, 'accumulated_eval_time': 1887.1576430797577, 'accumulated_logging_time': 5.3819334506988525, 'global_step': 148806, 'preemption_count': 0}), (150325, {'train/accuracy': 0.8557676672935486, 'train/loss': 0.6431095004081726, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.1167047023773193, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.7365473508834839, 'test/num_examples': 10000, 'score': 50566.62089514732, 'total_duration': 52481.188838005066, 'accumulated_submission_time': 50566.62089514732, 'accumulated_eval_time': 1904.453207731247, 'accumulated_logging_time': 5.434524297714233, 'global_step': 150325, 'preemption_count': 0}), (151844, {'train/accuracy': 0.8517019748687744, 'train/loss': 0.6508660316467285, 'validation/accuracy': 0.740399956703186, 'validation/loss': 1.1089304685592651, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.7190169095993042, 'test/num_examples': 10000, 'score': 51076.7715549469, 'total_duration': 53008.741042375565, 'accumulated_submission_time': 51076.7715549469, 'accumulated_eval_time': 1921.7528405189514, 'accumulated_logging_time': 5.488680601119995, 'global_step': 151844, 'preemption_count': 0}), (153363, {'train/accuracy': 0.8570631146430969, 'train/loss': 0.6400169134140015, 'validation/accuracy': 0.7451599836349487, 'validation/loss': 1.0995455980300903, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.702033519744873, 'test/num_examples': 10000, 'score': 51586.682436704636, 'total_duration': 53536.81693482399, 'accumulated_submission_time': 51586.682436704636, 'accumulated_eval_time': 1939.816710472107, 'accumulated_logging_time': 5.542165279388428, 'global_step': 153363, 'preemption_count': 0}), (154882, {'train/accuracy': 0.8557876348495483, 'train/loss': 0.6262343525886536, 'validation/accuracy': 0.7460199594497681, 'validation/loss': 1.0894845724105835, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6885766983032227, 'test/num_examples': 10000, 'score': 52096.776774168015, 'total_duration': 54064.17822790146, 'accumulated_submission_time': 52096.776774168015, 'accumulated_eval_time': 1956.984309911728, 'accumulated_logging_time': 5.594337701797485, 'global_step': 154882, 'preemption_count': 0}), (156401, {'train/accuracy': 0.861348032951355, 'train/loss': 0.6097630858421326, 'validation/accuracy': 0.7497199773788452, 'validation/loss': 1.077207088470459, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.6781634092330933, 'test/num_examples': 10000, 'score': 52606.98158121109, 'total_duration': 54592.09770488739, 'accumulated_submission_time': 52606.98158121109, 'accumulated_eval_time': 1974.5975804328918, 'accumulated_logging_time': 5.648033142089844, 'global_step': 156401, 'preemption_count': 0}), (157920, {'train/accuracy': 0.8819953799247742, 'train/loss': 0.5402708649635315, 'validation/accuracy': 0.7523799538612366, 'validation/loss': 1.0711145401000977, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.692663550376892, 'test/num_examples': 10000, 'score': 53117.21468949318, 'total_duration': 55119.62186551094, 'accumulated_submission_time': 53117.21468949318, 'accumulated_eval_time': 1991.7971727848053, 'accumulated_logging_time': 5.691704273223877, 'global_step': 157920, 'preemption_count': 0}), (159440, {'train/accuracy': 0.8802216053009033, 'train/loss': 0.5432620644569397, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.059921383857727, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.6828455924987793, 'test/num_examples': 10000, 'score': 53627.40317606926, 'total_duration': 55647.28178501129, 'accumulated_submission_time': 53627.40317606926, 'accumulated_eval_time': 2009.1603240966797, 'accumulated_logging_time': 5.752228260040283, 'global_step': 159440, 'preemption_count': 0}), (160959, {'train/accuracy': 0.8743024468421936, 'train/loss': 0.5627148747444153, 'validation/accuracy': 0.7519999742507935, 'validation/loss': 1.0669498443603516, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.689836025238037, 'test/num_examples': 10000, 'score': 54137.527206897736, 'total_duration': 56174.68740797043, 'accumulated_submission_time': 54137.527206897736, 'accumulated_eval_time': 2026.33642411232, 'accumulated_logging_time': 5.809492588043213, 'global_step': 160959, 'preemption_count': 0}), (162480, {'train/accuracy': 0.8794642686843872, 'train/loss': 0.5540726184844971, 'validation/accuracy': 0.7535199522972107, 'validation/loss': 1.0689977407455444, 'validation/num_examples': 50000, 'test/accuracy': 0.6355000138282776, 'test/loss': 1.671195149421692, 'test/num_examples': 10000, 'score': 54647.75112223625, 'total_duration': 56702.178926467896, 'accumulated_submission_time': 54647.75112223625, 'accumulated_eval_time': 2043.4988808631897, 'accumulated_logging_time': 5.86617112159729, 'global_step': 162480, 'preemption_count': 0}), (163999, {'train/accuracy': 0.881257951259613, 'train/loss': 0.5390360951423645, 'validation/accuracy': 0.7562599778175354, 'validation/loss': 1.0575190782546997, 'validation/num_examples': 50000, 'test/accuracy': 0.6357000470161438, 'test/loss': 1.6739016771316528, 'test/num_examples': 10000, 'score': 55157.73432970047, 'total_duration': 57229.695892095566, 'accumulated_submission_time': 55157.73432970047, 'accumulated_eval_time': 2060.928759098053, 'accumulated_logging_time': 5.921685457229614, 'global_step': 163999, 'preemption_count': 0}), (165518, {'train/accuracy': 0.8997927308082581, 'train/loss': 0.4756694734096527, 'validation/accuracy': 0.7611799836158752, 'validation/loss': 1.0342700481414795, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.6526086330413818, 'test/num_examples': 10000, 'score': 55667.66546201706, 'total_duration': 57757.03380203247, 'accumulated_submission_time': 55667.66546201706, 'accumulated_eval_time': 2078.2167851924896, 'accumulated_logging_time': 5.992365121841431, 'global_step': 165518, 'preemption_count': 0}), (167037, {'train/accuracy': 0.9026426672935486, 'train/loss': 0.4636174738407135, 'validation/accuracy': 0.7644599676132202, 'validation/loss': 1.0262614488601685, 'validation/num_examples': 50000, 'test/accuracy': 0.6413000226020813, 'test/loss': 1.6404330730438232, 'test/num_examples': 10000, 'score': 56177.75947856903, 'total_duration': 58284.554938554764, 'accumulated_submission_time': 56177.75947856903, 'accumulated_eval_time': 2095.539171934128, 'accumulated_logging_time': 6.0488669872283936, 'global_step': 167037, 'preemption_count': 0}), (168557, {'train/accuracy': 0.9011479616165161, 'train/loss': 0.4678551256656647, 'validation/accuracy': 0.7643799781799316, 'validation/loss': 1.0218467712402344, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.6343672275543213, 'test/num_examples': 10000, 'score': 56687.92796278, 'total_duration': 58812.09150671959, 'accumulated_submission_time': 56687.92796278, 'accumulated_eval_time': 2112.800806760788, 'accumulated_logging_time': 6.1073596477508545, 'global_step': 168557, 'preemption_count': 0}), (170076, {'train/accuracy': 0.9041174650192261, 'train/loss': 0.4593851864337921, 'validation/accuracy': 0.7635599970817566, 'validation/loss': 1.023459792137146, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.6217212677001953, 'test/num_examples': 10000, 'score': 57198.12523698807, 'total_duration': 59339.6875641346, 'accumulated_submission_time': 57198.12523698807, 'accumulated_eval_time': 2130.0928435325623, 'accumulated_logging_time': 6.1661036014556885, 'global_step': 170076, 'preemption_count': 0}), (171595, {'train/accuracy': 0.906648576259613, 'train/loss': 0.4511556625366211, 'validation/accuracy': 0.7665199637413025, 'validation/loss': 1.014053463935852, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.6215829849243164, 'test/num_examples': 10000, 'score': 57708.20014286041, 'total_duration': 59867.17143726349, 'accumulated_submission_time': 57708.20014286041, 'accumulated_eval_time': 2147.3965339660645, 'accumulated_logging_time': 6.22303581237793, 'global_step': 171595, 'preemption_count': 0}), (173116, {'train/accuracy': 0.9085220098495483, 'train/loss': 0.4437970519065857, 'validation/accuracy': 0.7672799825668335, 'validation/loss': 1.0095661878585815, 'validation/num_examples': 50000, 'test/accuracy': 0.6490000486373901, 'test/loss': 1.6089009046554565, 'test/num_examples': 10000, 'score': 58218.26253461838, 'total_duration': 60394.55778956413, 'accumulated_submission_time': 58218.26253461838, 'accumulated_eval_time': 2164.6122002601624, 'accumulated_logging_time': 6.283660173416138, 'global_step': 173116, 'preemption_count': 0}), (174635, {'train/accuracy': 0.9193837642669678, 'train/loss': 0.4103900194168091, 'validation/accuracy': 0.7671399712562561, 'validation/loss': 1.0063893795013428, 'validation/num_examples': 50000, 'test/accuracy': 0.6513000130653381, 'test/loss': 1.6108440160751343, 'test/num_examples': 10000, 'score': 58728.41574501991, 'total_duration': 60922.16036057472, 'accumulated_submission_time': 58728.41574501991, 'accumulated_eval_time': 2181.95579123497, 'accumulated_logging_time': 6.341675043106079, 'global_step': 174635, 'preemption_count': 0}), (176154, {'train/accuracy': 0.917390763759613, 'train/loss': 0.41718706488609314, 'validation/accuracy': 0.76801997423172, 'validation/loss': 1.0084744691848755, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6077901124954224, 'test/num_examples': 10000, 'score': 59238.44852399826, 'total_duration': 61449.693984270096, 'accumulated_submission_time': 59238.44852399826, 'accumulated_eval_time': 2199.349052429199, 'accumulated_logging_time': 6.401141405105591, 'global_step': 176154, 'preemption_count': 0}), (177673, {'train/accuracy': 0.9159757494926453, 'train/loss': 0.4144531488418579, 'validation/accuracy': 0.7691400051116943, 'validation/loss': 1.002310037612915, 'validation/num_examples': 50000, 'test/accuracy': 0.6516000032424927, 'test/loss': 1.602932333946228, 'test/num_examples': 10000, 'score': 59748.627321243286, 'total_duration': 61977.189665555954, 'accumulated_submission_time': 59748.627321243286, 'accumulated_eval_time': 2216.56134724617, 'accumulated_logging_time': 6.457212448120117, 'global_step': 177673, 'preemption_count': 0}), (179190, {'train/accuracy': 0.9170718789100647, 'train/loss': 0.40572771430015564, 'validation/accuracy': 0.7702599763870239, 'validation/loss': 0.9993435740470886, 'validation/num_examples': 50000, 'test/accuracy': 0.6502000093460083, 'test/loss': 1.6043004989624023, 'test/num_examples': 10000, 'score': 60257.7345225811, 'total_duration': 62504.43201804161, 'accumulated_submission_time': 60257.7345225811, 'accumulated_eval_time': 2233.763617515564, 'accumulated_logging_time': 7.341777801513672, 'global_step': 179190, 'preemption_count': 0}), (180709, {'train/accuracy': 0.9195631146430969, 'train/loss': 0.40906286239624023, 'validation/accuracy': 0.7701999545097351, 'validation/loss': 1.0007290840148926, 'validation/num_examples': 50000, 'test/accuracy': 0.6528000235557556, 'test/loss': 1.6050145626068115, 'test/num_examples': 10000, 'score': 60767.89223456383, 'total_duration': 63032.08485865593, 'accumulated_submission_time': 60767.89223456383, 'accumulated_eval_time': 2251.149762868881, 'accumulated_logging_time': 7.402913808822632, 'global_step': 180709, 'preemption_count': 0}), (182227, {'train/accuracy': 0.9197823405265808, 'train/loss': 0.40754902362823486, 'validation/accuracy': 0.7706800103187561, 'validation/loss': 0.9976078867912292, 'validation/num_examples': 50000, 'test/accuracy': 0.6515000462532043, 'test/loss': 1.602009892463684, 'test/num_examples': 10000, 'score': 61277.79031038284, 'total_duration': 63559.42090892792, 'accumulated_submission_time': 61277.79031038284, 'accumulated_eval_time': 2268.478595972061, 'accumulated_logging_time': 7.46382737159729, 'global_step': 182227, 'preemption_count': 0}), (183747, {'train/accuracy': 0.9213767051696777, 'train/loss': 0.40018025040626526, 'validation/accuracy': 0.7707799673080444, 'validation/loss': 0.9960906505584717, 'validation/num_examples': 50000, 'test/accuracy': 0.6534000039100647, 'test/loss': 1.599919080734253, 'test/num_examples': 10000, 'score': 61788.00178670883, 'total_duration': 64087.113674640656, 'accumulated_submission_time': 61788.00178670883, 'accumulated_eval_time': 2285.850029706955, 'accumulated_logging_time': 7.525523662567139, 'global_step': 183747, 'preemption_count': 0}), (185266, {'train/accuracy': 0.9223333597183228, 'train/loss': 0.39865994453430176, 'validation/accuracy': 0.7706599831581116, 'validation/loss': 0.9960277080535889, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.599553108215332, 'test/num_examples': 10000, 'score': 62297.926362752914, 'total_duration': 64614.320786714554, 'accumulated_submission_time': 62297.926362752914, 'accumulated_eval_time': 2303.0237517356873, 'accumulated_logging_time': 7.586014032363892, 'global_step': 185266, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9204400181770325, 'train/loss': 0.39730027318000793, 'validation/accuracy': 0.7709999680519104, 'validation/loss': 0.9936330318450928, 'validation/num_examples': 50000, 'test/accuracy': 0.6535000205039978, 'test/loss': 1.5979526042938232, 'test/num_examples': 10000, 'score': 62767.63660430908, 'total_duration': 65101.41163563728, 'accumulated_submission_time': 62767.63660430908, 'accumulated_eval_time': 2320.299160003662, 'accumulated_logging_time': 7.647452354431152, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0127 19:41:28.429388 140169137129280 submission_runner.py:586] Timing: 62767.63660430908
I0127 19:41:28.429470 140169137129280 submission_runner.py:588] Total number of evals: 124
I0127 19:41:28.429520 140169137129280 submission_runner.py:589] ====================
I0127 19:41:28.429573 140169137129280 submission_runner.py:542] Using RNG seed 3614520272
I0127 19:41:28.431034 140169137129280 submission_runner.py:551] --- Tuning run 2/5 ---
I0127 19:41:28.431137 140169137129280 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2.
I0127 19:41:28.438513 140169137129280 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2/hparams.json.
I0127 19:41:28.440173 140169137129280 submission_runner.py:206] Initializing dataset.
I0127 19:41:28.454798 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 19:41:28.464396 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 19:41:28.664002 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 19:41:28.948533 140169137129280 submission_runner.py:213] Initializing model.
I0127 19:41:34.607305 140169137129280 submission_runner.py:255] Initializing optimizer.
I0127 19:41:35.009331 140169137129280 submission_runner.py:262] Initializing metrics bundle.
I0127 19:41:35.009497 140169137129280 submission_runner.py:280] Initializing checkpoint and logger.
I0127 19:41:35.105378 140169137129280 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0127 19:41:35.105548 140169137129280 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0127 19:41:47.003437 140169137129280 logger_utils.py:220] Unable to record git information. Continuing without it.
I0127 19:41:58.638144 140169137129280 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2/flags_0.json.
I0127 19:41:58.649763 140169137129280 submission_runner.py:314] Starting training loop.
I0127 19:42:30.284666 140004667934464 logging_writer.py:48] [0] global_step=0, grad_norm=0.5329928994178772, loss=6.920774936676025
I0127 19:42:30.295529 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:42:36.628269 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:42:45.055440 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:42:47.488102 140169137129280 submission_runner.py:408] Time since start: 48.84s, 	Step: 1, 	{'train/accuracy': 0.00047831633128225803, 'train/loss': 6.910229682922363, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 31.64558982849121, 'total_duration': 48.83828067779541, 'accumulated_submission_time': 31.64558982849121, 'accumulated_eval_time': 17.192532777786255, 'accumulated_logging_time': 0}
I0127 19:42:47.497650 140004676327168 logging_writer.py:48] [1] accumulated_eval_time=17.192533, accumulated_logging_time=0, accumulated_submission_time=31.645590, global_step=1, preemption_count=0, score=31.645590, test/accuracy=0.000600, test/loss=6.910250, test/num_examples=10000, total_duration=48.838281, train/accuracy=0.000478, train/loss=6.910230, validation/accuracy=0.000960, validation/loss=6.910244, validation/num_examples=50000
I0127 19:43:21.080645 140005305468672 logging_writer.py:48] [100] global_step=100, grad_norm=0.5142983794212341, loss=6.900108337402344
I0127 19:43:54.770647 140004676327168 logging_writer.py:48] [200] global_step=200, grad_norm=0.5290886163711548, loss=6.8554277420043945
I0127 19:44:28.435489 140005305468672 logging_writer.py:48] [300] global_step=300, grad_norm=0.5902806520462036, loss=6.764937400817871
I0127 19:45:02.144654 140004676327168 logging_writer.py:48] [400] global_step=400, grad_norm=0.6127716302871704, loss=6.668224334716797
I0127 19:45:35.852164 140005305468672 logging_writer.py:48] [500] global_step=500, grad_norm=0.6481290459632874, loss=6.579886436462402
I0127 19:46:09.584241 140004676327168 logging_writer.py:48] [600] global_step=600, grad_norm=0.7033886909484863, loss=6.543291091918945
I0127 19:46:43.339070 140005305468672 logging_writer.py:48] [700] global_step=700, grad_norm=1.5174622535705566, loss=6.413943767547607
I0127 19:47:17.080396 140004676327168 logging_writer.py:48] [800] global_step=800, grad_norm=1.3738876581192017, loss=6.302282810211182
I0127 19:47:50.838341 140005305468672 logging_writer.py:48] [900] global_step=900, grad_norm=2.0624992847442627, loss=6.2393693923950195
I0127 19:48:24.635486 140004676327168 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3498742580413818, loss=6.222570419311523
I0127 19:48:58.402802 140005305468672 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.9189479351043701, loss=6.1569294929504395
I0127 19:49:32.150128 140004676327168 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.4128161668777466, loss=6.111352443695068
I0127 19:50:05.911651 140005305468672 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.824114441871643, loss=5.992026329040527
I0127 19:50:39.636867 140004676327168 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.4546568393707275, loss=5.942293167114258
I0127 19:51:13.372424 140005305468672 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.077800989151001, loss=5.900854587554932
I0127 19:51:17.565187 140169137129280 spec.py:321] Evaluating on the training split.
I0127 19:51:24.045619 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 19:51:32.690899 140169137129280 spec.py:349] Evaluating on the test split.
I0127 19:51:35.219366 140169137129280 submission_runner.py:408] Time since start: 576.57s, 	Step: 1514, 	{'train/accuracy': 0.08402423560619354, 'train/loss': 5.227553844451904, 'validation/accuracy': 0.0806799978017807, 'validation/loss': 5.2884297370910645, 'validation/num_examples': 50000, 'test/accuracy': 0.055000003427267075, 'test/loss': 5.537538528442383, 'test/num_examples': 10000, 'score': 541.6560969352722, 'total_duration': 576.5695464611053, 'accumulated_submission_time': 541.6560969352722, 'accumulated_eval_time': 34.846673011779785, 'accumulated_logging_time': 0.01880931854248047}
I0127 19:51:35.237635 140004667934464 logging_writer.py:48] [1514] accumulated_eval_time=34.846673, accumulated_logging_time=0.018809, accumulated_submission_time=541.656097, global_step=1514, preemption_count=0, score=541.656097, test/accuracy=0.055000, test/loss=5.537539, test/num_examples=10000, total_duration=576.569546, train/accuracy=0.084024, train/loss=5.227554, validation/accuracy=0.080680, validation/loss=5.288430, validation/num_examples=50000
I0127 19:52:04.570732 140004676327168 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.5705738067626953, loss=5.850245475769043
I0127 19:52:38.280042 140004667934464 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.049616575241089, loss=5.83435583114624
I0127 19:53:12.003408 140004676327168 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.1022870540618896, loss=5.749349594116211
I0127 19:53:45.748784 140004667934464 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.65388560295105, loss=5.7278923988342285
I0127 19:54:19.530738 140004676327168 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.594700813293457, loss=5.61740255355835
I0127 19:54:53.368976 140004667934464 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.884854555130005, loss=5.576793670654297
I0127 19:55:27.107342 140004676327168 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.131197929382324, loss=5.547429084777832
I0127 19:56:00.863301 140004667934464 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.634824514389038, loss=5.571488380432129
I0127 19:56:34.625781 140004676327168 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.442887306213379, loss=5.441093921661377
I0127 19:57:08.370879 140004667934464 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.514089584350586, loss=5.44435977935791
I0127 19:57:42.126486 140004676327168 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.0546417236328125, loss=5.425423622131348
I0127 19:58:15.874719 140004667934464 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.2610321044921875, loss=5.420729637145996
I0127 19:58:49.612858 140004676327168 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.3500728607177734, loss=5.352999210357666
I0127 19:59:23.353337 140004667934464 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.4894447326660156, loss=5.317024230957031
I0127 19:59:57.110315 140004676327168 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.217849254608154, loss=5.151644229888916
I0127 20:00:05.391564 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:00:11.862242 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:00:20.757571 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:00:23.274930 140169137129280 submission_runner.py:408] Time since start: 1104.63s, 	Step: 3026, 	{'train/accuracy': 0.19959741830825806, 'train/loss': 4.143094062805176, 'validation/accuracy': 0.1790200024843216, 'validation/loss': 4.270390033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.1348000019788742, 'test/loss': 4.705352783203125, 'test/num_examples': 10000, 'score': 1051.752347946167, 'total_duration': 1104.6250972747803, 'accumulated_submission_time': 1051.752347946167, 'accumulated_eval_time': 52.729986906051636, 'accumulated_logging_time': 0.04701733589172363}
I0127 20:00:23.294801 140005305468672 logging_writer.py:48] [3026] accumulated_eval_time=52.729987, accumulated_logging_time=0.047017, accumulated_submission_time=1051.752348, global_step=3026, preemption_count=0, score=1051.752348, test/accuracy=0.134800, test/loss=4.705353, test/num_examples=10000, total_duration=1104.625097, train/accuracy=0.199597, train/loss=4.143094, validation/accuracy=0.179020, validation/loss=4.270390, validation/num_examples=50000
I0127 20:00:48.604112 140005313861376 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.390172481536865, loss=5.22822904586792
I0127 20:01:22.396667 140005305468672 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.245028495788574, loss=5.137744903564453
I0127 20:01:56.156101 140005313861376 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2279930114746094, loss=5.141033172607422
I0127 20:02:29.920133 140005305468672 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.763126373291016, loss=5.133513450622559
I0127 20:03:03.686634 140005313861376 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.164875507354736, loss=5.020752906799316
I0127 20:03:37.434567 140005305468672 logging_writer.py:48] [3600] global_step=3600, grad_norm=5.228586673736572, loss=4.994428634643555
I0127 20:04:11.201244 140005313861376 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2501022815704346, loss=4.939704895019531
I0127 20:04:44.972038 140005305468672 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.5702080726623535, loss=4.832488536834717
I0127 20:05:18.745170 140005313861376 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.776119709014893, loss=4.893100738525391
I0127 20:05:52.490650 140005305468672 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.125567436218262, loss=4.897491455078125
I0127 20:06:26.241731 140005313861376 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.214536666870117, loss=4.867344856262207
I0127 20:06:59.957080 140005305468672 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.34096622467041, loss=4.81776237487793
I0127 20:07:33.770083 140005313861376 logging_writer.py:48] [4300] global_step=4300, grad_norm=7.455495834350586, loss=4.856786727905273
I0127 20:08:07.508588 140005305468672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.883850574493408, loss=4.7786946296691895
I0127 20:08:41.276076 140005313861376 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.677103042602539, loss=4.689652919769287
I0127 20:08:53.568686 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:09:00.700974 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:09:09.448550 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:09:12.132024 140169137129280 submission_runner.py:408] Time since start: 1633.48s, 	Step: 4538, 	{'train/accuracy': 0.3047672212123871, 'train/loss': 3.462144136428833, 'validation/accuracy': 0.2767399847507477, 'validation/loss': 3.6103830337524414, 'validation/num_examples': 50000, 'test/accuracy': 0.20430001616477966, 'test/loss': 4.173789024353027, 'test/num_examples': 10000, 'score': 1561.9691922664642, 'total_duration': 1633.4822070598602, 'accumulated_submission_time': 1561.9691922664642, 'accumulated_eval_time': 71.29328966140747, 'accumulated_logging_time': 0.07619142532348633}
I0127 20:09:12.152750 140004659541760 logging_writer.py:48] [4538] accumulated_eval_time=71.293290, accumulated_logging_time=0.076191, accumulated_submission_time=1561.969192, global_step=4538, preemption_count=0, score=1561.969192, test/accuracy=0.204300, test/loss=4.173789, test/num_examples=10000, total_duration=1633.482207, train/accuracy=0.304767, train/loss=3.462144, validation/accuracy=0.276740, validation/loss=3.610383, validation/num_examples=50000
I0127 20:09:33.410286 140004667934464 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.304584503173828, loss=4.65236234664917
I0127 20:10:07.082874 140004659541760 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.397726058959961, loss=4.659616947174072
I0127 20:10:40.825347 140004667934464 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.768298864364624, loss=4.611351490020752
I0127 20:11:14.592965 140004659541760 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.6233973503112793, loss=4.6305131912231445
I0127 20:11:48.349022 140004667934464 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.9266679286956787, loss=4.53741455078125
I0127 20:12:22.073480 140004659541760 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.4813451766967773, loss=4.483537197113037
I0127 20:12:55.807910 140004667934464 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.002577304840088, loss=4.491422176361084
I0127 20:13:29.631870 140004659541760 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.0567626953125, loss=4.5430378913879395
I0127 20:14:03.347153 140004667934464 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.437725305557251, loss=4.515556335449219
I0127 20:14:37.105431 140004659541760 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.6448769569396973, loss=4.434929847717285
I0127 20:15:10.828015 140004667934464 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.454094886779785, loss=4.478875160217285
I0127 20:15:44.542843 140004659541760 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.838116407394409, loss=4.380796909332275
I0127 20:16:18.283726 140004667934464 logging_writer.py:48] [5800] global_step=5800, grad_norm=4.754166603088379, loss=4.434136867523193
I0127 20:16:52.039231 140004659541760 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.884570360183716, loss=4.385787487030029
I0127 20:17:25.788741 140004667934464 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.120787143707275, loss=4.330511569976807
I0127 20:17:42.468533 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:17:48.985543 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:17:57.747197 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:18:00.273441 140169137129280 submission_runner.py:408] Time since start: 2161.62s, 	Step: 6051, 	{'train/accuracy': 0.38402822613716125, 'train/loss': 2.9529519081115723, 'validation/accuracy': 0.3597399890422821, 'validation/loss': 3.0945801734924316, 'validation/num_examples': 50000, 'test/accuracy': 0.2770000100135803, 'test/loss': 3.678683042526245, 'test/num_examples': 10000, 'score': 2072.2266323566437, 'total_duration': 2161.6236131191254, 'accumulated_submission_time': 2072.2266323566437, 'accumulated_eval_time': 89.09815335273743, 'accumulated_logging_time': 0.10786223411560059}
I0127 20:18:00.293152 140004667934464 logging_writer.py:48] [6051] accumulated_eval_time=89.098153, accumulated_logging_time=0.107862, accumulated_submission_time=2072.226632, global_step=6051, preemption_count=0, score=2072.226632, test/accuracy=0.277000, test/loss=3.678683, test/num_examples=10000, total_duration=2161.623613, train/accuracy=0.384028, train/loss=2.952952, validation/accuracy=0.359740, validation/loss=3.094580, validation/num_examples=50000
I0127 20:18:17.161675 140005313861376 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.4832887649536133, loss=4.28971004486084
I0127 20:18:50.878709 140004667934464 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.997098684310913, loss=4.272220611572266
I0127 20:19:24.622803 140005313861376 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.385373115539551, loss=4.331168174743652
I0127 20:19:58.442833 140004667934464 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.6025304794311523, loss=4.322460651397705
I0127 20:20:32.183786 140005313861376 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.89141845703125, loss=4.318965911865234
I0127 20:21:05.956851 140004667934464 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.295045852661133, loss=4.243458271026611
I0127 20:21:39.696020 140005313861376 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.6455140113830566, loss=4.297214508056641
I0127 20:22:13.401014 140004667934464 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.500607967376709, loss=4.207087516784668
I0127 20:22:47.150410 140005313861376 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.3123788833618164, loss=4.197249412536621
I0127 20:23:20.872951 140004667934464 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.79538893699646, loss=4.205226898193359
I0127 20:23:54.624401 140005313861376 logging_writer.py:48] [7100] global_step=7100, grad_norm=4.512348175048828, loss=4.2011637687683105
I0127 20:24:28.366406 140004667934464 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.2093162536621094, loss=4.251354217529297
I0127 20:25:02.106802 140005313861376 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.9303345680236816, loss=4.133173942565918
I0127 20:25:35.835447 140004667934464 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.743750810623169, loss=4.157182216644287
I0127 20:26:09.650776 140005313861376 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.593872308731079, loss=4.147494316101074
I0127 20:26:30.379873 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:26:36.814655 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:26:45.664250 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:26:48.191555 140169137129280 submission_runner.py:408] Time since start: 2689.54s, 	Step: 7563, 	{'train/accuracy': 0.4412667453289032, 'train/loss': 2.6990714073181152, 'validation/accuracy': 0.40026000142097473, 'validation/loss': 2.899838447570801, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.5461227893829346, 'test/num_examples': 10000, 'score': 2582.2532529830933, 'total_duration': 2689.541732311249, 'accumulated_submission_time': 2582.2532529830933, 'accumulated_eval_time': 106.90979647636414, 'accumulated_logging_time': 0.140455961227417}
I0127 20:26:48.210126 140004667934464 logging_writer.py:48] [7563] accumulated_eval_time=106.909796, accumulated_logging_time=0.140456, accumulated_submission_time=2582.253253, global_step=7563, preemption_count=0, score=2582.253253, test/accuracy=0.303900, test/loss=3.546123, test/num_examples=10000, total_duration=2689.541732, train/accuracy=0.441267, train/loss=2.699071, validation/accuracy=0.400260, validation/loss=2.899838, validation/num_examples=50000
I0127 20:27:01.017268 140005288683264 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.9009997844696045, loss=4.128933906555176
I0127 20:27:34.657957 140004667934464 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.2752468585968018, loss=4.065674304962158
I0127 20:28:08.351830 140005288683264 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.146029472351074, loss=4.0581536293029785
I0127 20:28:42.080847 140004667934464 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.891442060470581, loss=4.095414638519287
I0127 20:29:15.782897 140005288683264 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.850616931915283, loss=4.042593479156494
I0127 20:29:49.504449 140004667934464 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.589648723602295, loss=4.0568695068359375
I0127 20:30:23.224178 140005288683264 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.638675570487976, loss=4.043368816375732
I0127 20:30:56.928274 140004667934464 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5398623943328857, loss=3.9973678588867188
I0127 20:31:30.676470 140005288683264 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.5266499519348145, loss=4.069364547729492
I0127 20:32:04.467390 140004667934464 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.339433193206787, loss=3.954817295074463
I0127 20:32:38.218779 140005288683264 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.8965835571289062, loss=3.9826369285583496
I0127 20:33:11.935740 140004667934464 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.213531970977783, loss=3.952474594116211
I0127 20:33:45.628802 140005288683264 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.380563974380493, loss=3.987926483154297
I0127 20:34:19.339009 140004667934464 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.4998390674591064, loss=3.9765238761901855
I0127 20:34:53.002127 140005288683264 logging_writer.py:48] [9000] global_step=9000, grad_norm=3.7905969619750977, loss=4.0251874923706055
I0127 20:35:18.454857 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:35:24.809035 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:35:33.553357 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:35:36.088206 140169137129280 submission_runner.py:408] Time since start: 3217.44s, 	Step: 9077, 	{'train/accuracy': 0.5027104616165161, 'train/loss': 2.3729019165039062, 'validation/accuracy': 0.45291998982429504, 'validation/loss': 2.6144769191741943, 'validation/num_examples': 50000, 'test/accuracy': 0.3500000238418579, 'test/loss': 3.250772476196289, 'test/num_examples': 10000, 'score': 3092.440190553665, 'total_duration': 3217.438376188278, 'accumulated_submission_time': 3092.440190553665, 'accumulated_eval_time': 124.54310297966003, 'accumulated_logging_time': 0.16915082931518555}
I0127 20:35:36.107153 140005305468672 logging_writer.py:48] [9077] accumulated_eval_time=124.543103, accumulated_logging_time=0.169151, accumulated_submission_time=3092.440191, global_step=9077, preemption_count=0, score=3092.440191, test/accuracy=0.350000, test/loss=3.250772, test/num_examples=10000, total_duration=3217.438376, train/accuracy=0.502710, train/loss=2.372902, validation/accuracy=0.452920, validation/loss=2.614477, validation/num_examples=50000
I0127 20:35:44.198164 140005313861376 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.617203712463379, loss=3.846837282180786
I0127 20:36:17.858877 140005305468672 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.405951738357544, loss=3.883260726928711
I0127 20:36:51.531723 140005313861376 logging_writer.py:48] [9300] global_step=9300, grad_norm=3.594449996948242, loss=3.969024181365967
I0127 20:37:25.237622 140005305468672 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.832181930541992, loss=3.9127848148345947
I0127 20:37:58.969276 140005313861376 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.6590611934661865, loss=3.8503170013427734
I0127 20:38:32.738327 140005305468672 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.4086813926696777, loss=3.9318790435791016
I0127 20:39:06.427459 140005313861376 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.6804697513580322, loss=3.908330202102661
I0127 20:39:40.133854 140005305468672 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.799565315246582, loss=3.827791690826416
I0127 20:40:13.837890 140005313861376 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.1640419960021973, loss=3.8256778717041016
I0127 20:40:47.548404 140005305468672 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.9430018663406372, loss=3.841681718826294
I0127 20:41:21.215806 140005313861376 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.3368093967437744, loss=3.8196520805358887
I0127 20:41:54.905885 140005305468672 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.6291172504425049, loss=3.9235215187072754
I0127 20:42:28.640443 140005313861376 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.072880744934082, loss=3.85673189163208
I0127 20:43:02.361792 140005305468672 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.1316914558410645, loss=3.8570241928100586
I0127 20:43:36.023148 140005313861376 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8680980205535889, loss=3.7980802059173584
I0127 20:44:06.144191 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:44:12.674650 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:44:21.473026 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:44:24.308523 140169137129280 submission_runner.py:408] Time since start: 3745.66s, 	Step: 10591, 	{'train/accuracy': 0.5700932741165161, 'train/loss': 2.06361722946167, 'validation/accuracy': 0.5237799882888794, 'validation/loss': 2.2730724811553955, 'validation/num_examples': 50000, 'test/accuracy': 0.4058000147342682, 'test/loss': 2.908501148223877, 'test/num_examples': 10000, 'score': 3602.4188084602356, 'total_duration': 3745.6586899757385, 'accumulated_submission_time': 3602.4188084602356, 'accumulated_eval_time': 142.70739150047302, 'accumulated_logging_time': 0.19852828979492188}
I0127 20:44:24.327517 140004667934464 logging_writer.py:48] [10591] accumulated_eval_time=142.707392, accumulated_logging_time=0.198528, accumulated_submission_time=3602.418808, global_step=10591, preemption_count=0, score=3602.418808, test/accuracy=0.405800, test/loss=2.908501, test/num_examples=10000, total_duration=3745.658690, train/accuracy=0.570093, train/loss=2.063617, validation/accuracy=0.523780, validation/loss=2.273072, validation/num_examples=50000
I0127 20:44:27.709121 140004676327168 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9822760820388794, loss=3.70503306388855
I0127 20:45:01.351197 140004667934464 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.7568999528884888, loss=3.7656242847442627
I0127 20:45:35.016947 140004676327168 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.8056644201278687, loss=3.8424949645996094
I0127 20:46:08.693266 140004667934464 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.578599214553833, loss=3.7162110805511475
I0127 20:46:42.416589 140004676327168 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.9918676614761353, loss=3.7156176567077637
I0127 20:47:16.135179 140004667934464 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.09570574760437, loss=3.7581145763397217
I0127 20:47:49.829792 140004676327168 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.04939603805542, loss=3.6425585746765137
I0127 20:48:23.546609 140004667934464 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.4761353731155396, loss=3.734703779220581
I0127 20:48:57.244027 140004676327168 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.8824708461761475, loss=3.7718992233276367
I0127 20:49:30.921934 140004667934464 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6955446004867554, loss=3.6968002319335938
I0127 20:50:04.582759 140004676327168 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.8025263547897339, loss=3.841069221496582
I0127 20:50:38.374317 140004667934464 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.111532688140869, loss=3.7056984901428223
I0127 20:51:12.081140 140004676327168 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7000954151153564, loss=3.651106834411621
I0127 20:51:45.755250 140004667934464 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6949331760406494, loss=3.685946464538574
I0127 20:52:19.454346 140004676327168 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.0966944694519043, loss=3.6167736053466797
I0127 20:52:53.160944 140004667934464 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.623875379562378, loss=3.5507075786590576
I0127 20:52:54.323782 140169137129280 spec.py:321] Evaluating on the training split.
I0127 20:53:00.747967 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 20:53:09.501565 140169137129280 spec.py:349] Evaluating on the test split.
I0127 20:53:11.984789 140169137129280 submission_runner.py:408] Time since start: 4273.33s, 	Step: 12105, 	{'train/accuracy': 0.5975366830825806, 'train/loss': 1.9064337015151978, 'validation/accuracy': 0.5470799803733826, 'validation/loss': 2.1341938972473145, 'validation/num_examples': 50000, 'test/accuracy': 0.42160001397132874, 'test/loss': 2.831441640853882, 'test/num_examples': 10000, 'score': 4112.355051517487, 'total_duration': 4273.3349623680115, 'accumulated_submission_time': 4112.355051517487, 'accumulated_eval_time': 160.36835479736328, 'accumulated_logging_time': 0.2296433448791504}
I0127 20:53:12.004425 140005322254080 logging_writer.py:48] [12105] accumulated_eval_time=160.368355, accumulated_logging_time=0.229643, accumulated_submission_time=4112.355052, global_step=12105, preemption_count=0, score=4112.355052, test/accuracy=0.421600, test/loss=2.831442, test/num_examples=10000, total_duration=4273.334962, train/accuracy=0.597537, train/loss=1.906434, validation/accuracy=0.547080, validation/loss=2.134194, validation/num_examples=50000
I0127 20:53:44.257619 140005330646784 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.3364317417144775, loss=3.5535409450531006
I0127 20:54:17.906535 140005322254080 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.5221011638641357, loss=3.554858922958374
I0127 20:54:51.587515 140005330646784 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.9035159349441528, loss=3.6783504486083984
I0127 20:55:25.230540 140005322254080 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.1811249256134033, loss=3.6672141551971436
I0127 20:55:58.915769 140005330646784 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.8005174398422241, loss=3.627068519592285
I0127 20:56:32.594005 140005322254080 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.754575490951538, loss=3.5810484886169434
I0127 20:57:06.304540 140005330646784 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.4178866147994995, loss=3.6021480560302734
I0127 20:57:40.002464 140005322254080 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.8234705924987793, loss=3.6316590309143066
I0127 20:58:13.722648 140005330646784 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.3985083103179932, loss=3.698904037475586
I0127 20:58:47.393908 140005322254080 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.006373643875122, loss=3.6698498725891113
I0127 20:59:21.100398 140005330646784 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.3381013870239258, loss=3.5283143520355225
I0127 20:59:54.763914 140005322254080 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.603880524635315, loss=3.573575019836426
I0127 21:00:28.481290 140005330646784 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5729442834854126, loss=3.6446683406829834
I0127 21:01:02.129495 140005322254080 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.478887915611267, loss=3.5243420600891113
I0127 21:01:35.772719 140005330646784 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6841188669204712, loss=3.544217586517334
I0127 21:01:42.001230 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:01:48.382840 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:01:56.953476 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:01:59.468046 140169137129280 submission_runner.py:408] Time since start: 4800.82s, 	Step: 13620, 	{'train/accuracy': 0.615632951259613, 'train/loss': 1.8004908561706543, 'validation/accuracy': 0.5709399580955505, 'validation/loss': 2.0190188884735107, 'validation/num_examples': 50000, 'test/accuracy': 0.44930002093315125, 'test/loss': 2.669410228729248, 'test/num_examples': 10000, 'score': 4622.293032407761, 'total_duration': 4800.818227529526, 'accumulated_submission_time': 4622.293032407761, 'accumulated_eval_time': 177.8351345062256, 'accumulated_logging_time': 0.2601957321166992}
I0127 21:01:59.487399 140005297075968 logging_writer.py:48] [13620] accumulated_eval_time=177.835135, accumulated_logging_time=0.260196, accumulated_submission_time=4622.293032, global_step=13620, preemption_count=0, score=4622.293032, test/accuracy=0.449300, test/loss=2.669410, test/num_examples=10000, total_duration=4800.818228, train/accuracy=0.615633, train/loss=1.800491, validation/accuracy=0.570940, validation/loss=2.019019, validation/num_examples=50000
I0127 21:02:26.763436 140005305468672 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5666579008102417, loss=3.589127540588379
I0127 21:03:00.380678 140005297075968 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.7900365591049194, loss=3.5310380458831787
I0127 21:03:34.118937 140005305468672 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.7016798257827759, loss=3.6001744270324707
I0127 21:04:07.777775 140005297075968 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.6363805532455444, loss=3.625136137008667
I0127 21:04:41.409547 140005305468672 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.8694508075714111, loss=3.675049066543579
I0127 21:05:15.075567 140005297075968 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.9246894121170044, loss=3.6913254261016846
I0127 21:05:48.770851 140005305468672 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.1325371265411377, loss=3.571305751800537
I0127 21:06:22.453058 140005297075968 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.6073534488677979, loss=3.5039865970611572
I0127 21:06:56.124127 140005305468672 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.5013304948806763, loss=3.501122236251831
I0127 21:07:29.816928 140005297075968 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.7415622472763062, loss=3.59213924407959
I0127 21:08:03.517531 140005305468672 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.6281988620758057, loss=3.684926986694336
I0127 21:08:37.204474 140005297075968 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.57486891746521, loss=3.468844413757324
I0127 21:09:10.904356 140005305468672 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.7221853733062744, loss=3.440279722213745
I0127 21:09:44.669328 140005297075968 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.4200892448425293, loss=3.5234670639038086
I0127 21:10:18.357855 140005305468672 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.3205806016921997, loss=3.5758607387542725
I0127 21:10:29.615582 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:10:36.022774 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:10:44.883169 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:10:47.406457 140169137129280 submission_runner.py:408] Time since start: 5328.76s, 	Step: 15135, 	{'train/accuracy': 0.6227080821990967, 'train/loss': 1.854886531829834, 'validation/accuracy': 0.5773000121116638, 'validation/loss': 2.06612229347229, 'validation/num_examples': 50000, 'test/accuracy': 0.44360002875328064, 'test/loss': 2.7721447944641113, 'test/num_examples': 10000, 'score': 5132.363230466843, 'total_duration': 5328.756629705429, 'accumulated_submission_time': 5132.363230466843, 'accumulated_eval_time': 195.62596201896667, 'accumulated_logging_time': 0.2902054786682129}
I0127 21:10:47.430109 140005288683264 logging_writer.py:48] [15135] accumulated_eval_time=195.625962, accumulated_logging_time=0.290205, accumulated_submission_time=5132.363230, global_step=15135, preemption_count=0, score=5132.363230, test/accuracy=0.443600, test/loss=2.772145, test/num_examples=10000, total_duration=5328.756630, train/accuracy=0.622708, train/loss=1.854887, validation/accuracy=0.577300, validation/loss=2.066122, validation/num_examples=50000
I0127 21:11:09.656392 140005322254080 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.5443031787872314, loss=3.563627243041992
I0127 21:11:43.292678 140005288683264 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.2453653812408447, loss=3.5266330242156982
I0127 21:12:16.946635 140005322254080 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.527320384979248, loss=3.472616195678711
I0127 21:12:50.632777 140005288683264 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5498677492141724, loss=3.558669090270996
I0127 21:13:24.281920 140005322254080 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5751404762268066, loss=3.594912528991699
I0127 21:13:57.940447 140005288683264 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.303158164024353, loss=3.438591718673706
I0127 21:14:31.624027 140005322254080 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.2004144191741943, loss=3.462895393371582
I0127 21:15:05.307359 140005288683264 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5575878620147705, loss=3.5135974884033203
I0127 21:15:39.069324 140005322254080 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.133150577545166, loss=3.4644479751586914
I0127 21:16:12.755887 140005288683264 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6732254028320312, loss=3.4755356311798096
I0127 21:16:46.447037 140005322254080 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.1034786701202393, loss=3.5464742183685303
I0127 21:17:20.155661 140005288683264 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.3755658864974976, loss=3.4930009841918945
I0127 21:17:53.834689 140005322254080 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4389986991882324, loss=3.4545462131500244
I0127 21:18:27.535832 140005288683264 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2508141994476318, loss=3.48960542678833
I0127 21:19:01.194775 140005322254080 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4075367450714111, loss=3.4437596797943115
I0127 21:19:17.533485 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:19:23.978724 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:19:32.664049 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:19:35.172473 140169137129280 submission_runner.py:408] Time since start: 5856.52s, 	Step: 16650, 	{'train/accuracy': 0.6803053021430969, 'train/loss': 1.539376974105835, 'validation/accuracy': 0.6041600108146667, 'validation/loss': 1.8682082891464233, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.5343410968780518, 'test/num_examples': 10000, 'score': 5642.407160997391, 'total_duration': 5856.522654294968, 'accumulated_submission_time': 5642.407160997391, 'accumulated_eval_time': 213.26491689682007, 'accumulated_logging_time': 0.3256824016571045}
I0127 21:19:35.192777 140004676327168 logging_writer.py:48] [16650] accumulated_eval_time=213.264917, accumulated_logging_time=0.325682, accumulated_submission_time=5642.407161, global_step=16650, preemption_count=0, score=5642.407161, test/accuracy=0.480200, test/loss=2.534341, test/num_examples=10000, total_duration=5856.522654, train/accuracy=0.680305, train/loss=1.539377, validation/accuracy=0.604160, validation/loss=1.868208, validation/num_examples=50000
I0127 21:19:52.359296 140005297075968 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.1861952543258667, loss=3.446204423904419
I0127 21:20:26.012128 140004676327168 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.383516550064087, loss=3.506084442138672
I0127 21:20:59.650557 140005297075968 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.7445966005325317, loss=3.4261627197265625
I0127 21:21:33.263086 140004676327168 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.7251417636871338, loss=3.526984214782715
I0127 21:22:07.003279 140005297075968 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.3732264041900635, loss=3.447614908218384
I0127 21:22:40.654206 140004676327168 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4014267921447754, loss=3.4750373363494873
I0127 21:23:14.334270 140005297075968 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.2181332111358643, loss=3.4292421340942383
I0127 21:23:48.004039 140004676327168 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.6514289379119873, loss=3.482544422149658
I0127 21:24:21.676254 140005297075968 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2629032135009766, loss=3.412750244140625
I0127 21:24:55.306915 140004676327168 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.1953363418579102, loss=3.4594435691833496
I0127 21:25:29.004430 140005297075968 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5579867362976074, loss=3.460357427597046
I0127 21:26:02.683079 140004676327168 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.1734894514083862, loss=3.493271827697754
I0127 21:26:36.378588 140005297075968 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.3974684476852417, loss=3.4009366035461426
I0127 21:27:10.052538 140004676327168 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.4012808799743652, loss=3.4228458404541016
I0127 21:27:43.738094 140005297075968 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.3150643110275269, loss=3.3499038219451904
I0127 21:28:05.189056 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:28:11.695565 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:28:20.173747 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:28:22.728325 140169137129280 submission_runner.py:408] Time since start: 6384.08s, 	Step: 18165, 	{'train/accuracy': 0.6703802347183228, 'train/loss': 1.531219720840454, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.8504778146743774, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.5441064834594727, 'test/num_examples': 10000, 'score': 6152.345567941666, 'total_duration': 6384.078463315964, 'accumulated_submission_time': 6152.345567941666, 'accumulated_eval_time': 230.80410766601562, 'accumulated_logging_time': 0.3559126853942871}
I0127 21:28:22.749776 140004676327168 logging_writer.py:48] [18165] accumulated_eval_time=230.804108, accumulated_logging_time=0.355913, accumulated_submission_time=6152.345568, global_step=18165, preemption_count=0, score=6152.345568, test/accuracy=0.469800, test/loss=2.544106, test/num_examples=10000, total_duration=6384.078463, train/accuracy=0.670380, train/loss=1.531220, validation/accuracy=0.599900, validation/loss=1.850478, validation/num_examples=50000
I0127 21:28:34.852998 140005288683264 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.6546787023544312, loss=3.452434539794922
I0127 21:29:08.426988 140004676327168 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.9533113241195679, loss=3.3808696269989014
I0127 21:29:42.107777 140005288683264 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.3579330444335938, loss=3.3343729972839355
I0127 21:30:15.768255 140004676327168 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.466814398765564, loss=3.427083969116211
I0127 21:30:49.406849 140005288683264 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.1781367063522339, loss=3.435859441757202
I0127 21:31:23.087510 140004676327168 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.2741503715515137, loss=3.3908514976501465
I0127 21:31:56.750066 140005288683264 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.2493999004364014, loss=3.4768755435943604
I0127 21:32:30.372195 140004676327168 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.7527838945388794, loss=3.4065005779266357
I0127 21:33:04.011889 140005288683264 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.2360317707061768, loss=3.3348262310028076
I0127 21:33:37.647246 140004676327168 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.419974684715271, loss=3.3555705547332764
I0127 21:34:11.415631 140005288683264 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.4449903964996338, loss=3.421740770339966
I0127 21:34:45.083199 140004676327168 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3546638488769531, loss=3.4555954933166504
I0127 21:35:18.758441 140005288683264 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.3413952589035034, loss=3.406139373779297
I0127 21:35:52.395635 140004676327168 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.229732871055603, loss=3.355637550354004
I0127 21:36:26.043432 140005288683264 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.235577940940857, loss=3.338449478149414
I0127 21:36:52.774365 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:36:59.254208 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:37:08.073573 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:37:10.629266 140169137129280 submission_runner.py:408] Time since start: 6911.98s, 	Step: 19681, 	{'train/accuracy': 0.6851084232330322, 'train/loss': 1.5148324966430664, 'validation/accuracy': 0.6193000078201294, 'validation/loss': 1.7937859296798706, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.440124988555908, 'test/num_examples': 10000, 'score': 6662.31097984314, 'total_duration': 6911.979278564453, 'accumulated_submission_time': 6662.31097984314, 'accumulated_eval_time': 248.65882778167725, 'accumulated_logging_time': 0.3883523941040039}
I0127 21:37:10.650387 140005322254080 logging_writer.py:48] [19681] accumulated_eval_time=248.658828, accumulated_logging_time=0.388352, accumulated_submission_time=6662.310980, global_step=19681, preemption_count=0, score=6662.310980, test/accuracy=0.490400, test/loss=2.440125, test/num_examples=10000, total_duration=6911.979279, train/accuracy=0.685108, train/loss=1.514832, validation/accuracy=0.619300, validation/loss=1.793786, validation/num_examples=50000
I0127 21:37:17.385180 140005330646784 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.3077667951583862, loss=3.453608512878418
I0127 21:37:50.957396 140005322254080 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.4239614009857178, loss=3.4218316078186035
I0127 21:38:24.566557 140005330646784 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5166280269622803, loss=3.359490156173706
I0127 21:38:58.252501 140005322254080 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1239125728607178, loss=3.4395830631256104
I0127 21:39:31.909663 140005330646784 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1641281843185425, loss=3.392918825149536
I0127 21:40:05.576715 140005322254080 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.3086349964141846, loss=3.5064537525177
I0127 21:40:39.326043 140005330646784 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.2951594591140747, loss=3.3458139896392822
I0127 21:41:12.999817 140005322254080 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.343099594116211, loss=3.3744804859161377
I0127 21:41:46.667525 140005330646784 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.7307305335998535, loss=3.3731603622436523
I0127 21:42:20.298163 140005322254080 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6239358186721802, loss=3.336972236633301
I0127 21:42:53.935205 140005330646784 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4288421869277954, loss=3.371800661087036
I0127 21:43:27.584924 140005322254080 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7475624084472656, loss=3.488295078277588
I0127 21:44:01.216918 140005330646784 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.5667171478271484, loss=3.365670680999756
I0127 21:44:34.886379 140005322254080 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2987723350524902, loss=3.4257514476776123
I0127 21:45:08.548573 140005330646784 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.5999175310134888, loss=3.414888381958008
I0127 21:45:40.656400 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:45:47.078753 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:45:56.075997 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:45:58.687149 140169137129280 submission_runner.py:408] Time since start: 7440.04s, 	Step: 21197, 	{'train/accuracy': 0.6892338991165161, 'train/loss': 1.42905855178833, 'validation/accuracy': 0.627020001411438, 'validation/loss': 1.7129013538360596, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.369382619857788, 'test/num_examples': 10000, 'score': 7172.2588493824005, 'total_duration': 7440.037328243256, 'accumulated_submission_time': 7172.2588493824005, 'accumulated_eval_time': 266.68954062461853, 'accumulated_logging_time': 0.4194791316986084}
I0127 21:45:58.710509 140005288683264 logging_writer.py:48] [21197] accumulated_eval_time=266.689541, accumulated_logging_time=0.419479, accumulated_submission_time=7172.258849, global_step=21197, preemption_count=0, score=7172.258849, test/accuracy=0.498800, test/loss=2.369383, test/num_examples=10000, total_duration=7440.037328, train/accuracy=0.689234, train/loss=1.429059, validation/accuracy=0.627020, validation/loss=1.712901, validation/num_examples=50000
I0127 21:46:00.080879 140005297075968 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.434201717376709, loss=3.325449228286743
I0127 21:46:33.686116 140005288683264 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.3957314491271973, loss=3.352736234664917
I0127 21:47:07.379799 140005297075968 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.2970051765441895, loss=3.2929649353027344
I0127 21:47:41.053451 140005288683264 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5190036296844482, loss=3.368948221206665
I0127 21:48:14.680984 140005297075968 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.3075783252716064, loss=3.3489465713500977
I0127 21:48:48.314528 140005288683264 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2385786771774292, loss=3.3687260150909424
I0127 21:49:21.929642 140005297075968 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.4260156154632568, loss=3.3168649673461914
I0127 21:49:55.565838 140005288683264 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.253859281539917, loss=3.2996599674224854
I0127 21:50:29.248255 140005297075968 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.3238924741744995, loss=3.246314764022827
I0127 21:51:02.931642 140005288683264 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.520166277885437, loss=3.3216378688812256
I0127 21:51:36.607553 140005297075968 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2907774448394775, loss=3.354904890060425
I0127 21:52:10.279734 140005288683264 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.3953264951705933, loss=3.3500571250915527
I0127 21:52:43.911051 140005297075968 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.5999659299850464, loss=3.358454704284668
I0127 21:53:17.590896 140005288683264 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.339761734008789, loss=3.295910120010376
I0127 21:53:51.262128 140005297075968 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.4702236652374268, loss=3.2951292991638184
I0127 21:54:24.946645 140005288683264 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.8104779720306396, loss=3.374088764190674
I0127 21:54:28.795220 140169137129280 spec.py:321] Evaluating on the training split.
I0127 21:54:35.203232 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 21:54:44.019327 140169137129280 spec.py:349] Evaluating on the test split.
I0127 21:54:46.504541 140169137129280 submission_runner.py:408] Time since start: 7967.85s, 	Step: 22713, 	{'train/accuracy': 0.684012234210968, 'train/loss': 1.5042533874511719, 'validation/accuracy': 0.6255599856376648, 'validation/loss': 1.766654372215271, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.4623095989227295, 'test/num_examples': 10000, 'score': 7682.2843725681305, 'total_duration': 7967.854699373245, 'accumulated_submission_time': 7682.2843725681305, 'accumulated_eval_time': 284.39879989624023, 'accumulated_logging_time': 0.4545462131500244}
I0127 21:54:46.527148 140004667934464 logging_writer.py:48] [22713] accumulated_eval_time=284.398800, accumulated_logging_time=0.454546, accumulated_submission_time=7682.284373, global_step=22713, preemption_count=0, score=7682.284373, test/accuracy=0.487200, test/loss=2.462310, test/num_examples=10000, total_duration=7967.854699, train/accuracy=0.684012, train/loss=1.504253, validation/accuracy=0.625560, validation/loss=1.766654, validation/num_examples=50000
I0127 21:55:16.148053 140004676327168 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.4939448833465576, loss=3.292757272720337
I0127 21:55:49.799349 140004667934464 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4341830015182495, loss=3.2796592712402344
I0127 21:56:23.430630 140004676327168 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.371848464012146, loss=3.347391128540039
I0127 21:56:57.081083 140004667934464 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.6246545314788818, loss=3.482356071472168
I0127 21:57:30.768340 140004676327168 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.2885596752166748, loss=3.4060866832733154
I0127 21:58:04.381045 140004667934464 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.7692210674285889, loss=3.331695556640625
I0127 21:58:38.041907 140004676327168 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.3510527610778809, loss=3.2816264629364014
I0127 21:59:11.771764 140004667934464 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.2163705825805664, loss=3.2899158000946045
I0127 21:59:45.482513 140004676327168 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.2806885242462158, loss=3.3880836963653564
I0127 22:00:19.107693 140004667934464 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6704539060592651, loss=3.3021581172943115
I0127 22:00:52.766016 140004676327168 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.4444243907928467, loss=3.3597898483276367
I0127 22:01:26.415458 140004667934464 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4875049591064453, loss=3.3294150829315186
I0127 22:02:00.056925 140004676327168 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2601919174194336, loss=3.2767751216888428
I0127 22:02:33.735629 140004667934464 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.3093665838241577, loss=3.3465261459350586
I0127 22:03:07.415963 140004676327168 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.4066945314407349, loss=3.393867015838623
I0127 22:03:16.653060 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:03:23.116869 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:03:31.828752 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:03:34.228572 140169137129280 submission_runner.py:408] Time since start: 8495.58s, 	Step: 24229, 	{'train/accuracy': 0.6905492544174194, 'train/loss': 1.4637855291366577, 'validation/accuracy': 0.6354599595069885, 'validation/loss': 1.7087171077728271, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.389516592025757, 'test/num_examples': 10000, 'score': 8192.348526477814, 'total_duration': 8495.57875084877, 'accumulated_submission_time': 8192.348526477814, 'accumulated_eval_time': 301.9742715358734, 'accumulated_logging_time': 0.4906270503997803}
I0127 22:03:34.252450 140005322254080 logging_writer.py:48] [24229] accumulated_eval_time=301.974272, accumulated_logging_time=0.490627, accumulated_submission_time=8192.348526, global_step=24229, preemption_count=0, score=8192.348526, test/accuracy=0.505000, test/loss=2.389517, test/num_examples=10000, total_duration=8495.578751, train/accuracy=0.690549, train/loss=1.463786, validation/accuracy=0.635460, validation/loss=1.708717, validation/num_examples=50000
I0127 22:03:58.489634 140005330646784 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.3609609603881836, loss=3.3258004188537598
I0127 22:04:32.124508 140005322254080 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.5101804733276367, loss=3.3429970741271973
I0127 22:05:05.759993 140005330646784 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.2572108507156372, loss=3.3431456089019775
I0127 22:05:39.519332 140005322254080 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1390395164489746, loss=3.2844252586364746
I0127 22:06:13.161251 140005330646784 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.501017451286316, loss=3.3788750171661377
I0127 22:06:46.828987 140005322254080 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.371118426322937, loss=3.3189187049865723
I0127 22:07:20.502780 140005330646784 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2491782903671265, loss=3.310641288757324
I0127 22:07:54.183619 140005322254080 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.4164092540740967, loss=3.3525028228759766
I0127 22:08:27.854710 140005330646784 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.1977543830871582, loss=3.240086317062378
I0127 22:09:01.516178 140005322254080 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2201207876205444, loss=3.2863261699676514
I0127 22:09:35.142913 140005330646784 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1966432332992554, loss=3.276514768600464
I0127 22:10:08.792234 140005322254080 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.6946632862091064, loss=3.303173065185547
I0127 22:10:42.447586 140005330646784 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5188772678375244, loss=3.2504990100860596
I0127 22:11:16.099362 140005322254080 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1453044414520264, loss=3.272843599319458
I0127 22:11:49.848643 140005330646784 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.2247227430343628, loss=3.2283713817596436
I0127 22:12:04.487676 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:12:10.902633 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:12:19.401245 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:12:21.887554 140169137129280 submission_runner.py:408] Time since start: 9023.24s, 	Step: 25745, 	{'train/accuracy': 0.7331194281578064, 'train/loss': 1.299314022064209, 'validation/accuracy': 0.6287800073623657, 'validation/loss': 1.7513171434402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.4090499877929688, 'test/num_examples': 10000, 'score': 8702.524030208588, 'total_duration': 9023.23773431778, 'accumulated_submission_time': 8702.524030208588, 'accumulated_eval_time': 319.3741111755371, 'accumulated_logging_time': 0.5262205600738525}
I0127 22:12:21.913865 140004667934464 logging_writer.py:48] [25745] accumulated_eval_time=319.374111, accumulated_logging_time=0.526221, accumulated_submission_time=8702.524030, global_step=25745, preemption_count=0, score=8702.524030, test/accuracy=0.507600, test/loss=2.409050, test/num_examples=10000, total_duration=9023.237734, train/accuracy=0.733119, train/loss=1.299314, validation/accuracy=0.628780, validation/loss=1.751317, validation/num_examples=50000
I0127 22:12:40.756293 140004676327168 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.4373290538787842, loss=3.3389534950256348
I0127 22:13:14.369585 140004667934464 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.4339492321014404, loss=3.32330584526062
I0127 22:13:48.031865 140004676327168 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.3497569561004639, loss=3.3118836879730225
I0127 22:14:21.702256 140004667934464 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.4631620645523071, loss=3.335955858230591
I0127 22:14:55.352549 140004676327168 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.3079575300216675, loss=3.234711170196533
I0127 22:15:29.037546 140004667934464 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.4514904022216797, loss=3.3361477851867676
I0127 22:16:02.657455 140004676327168 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3830530643463135, loss=3.323356866836548
I0127 22:16:36.276482 140004667934464 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.4366706609725952, loss=3.316331386566162
I0127 22:17:09.948967 140004676327168 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.4476783275604248, loss=3.3515233993530273
I0127 22:17:43.723609 140004667934464 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.2175709009170532, loss=3.3284761905670166
I0127 22:18:17.384597 140004676327168 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2346222400665283, loss=3.293626308441162
I0127 22:18:50.996609 140004667934464 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.4488526582717896, loss=3.2456843852996826
I0127 22:19:24.669878 140004676327168 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.173988699913025, loss=3.1607892513275146
I0127 22:19:58.338250 140004667934464 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3903090953826904, loss=3.3987927436828613
I0127 22:20:31.993294 140004676327168 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.2465442419052124, loss=3.2111287117004395
I0127 22:20:52.003096 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:20:58.455205 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:21:07.329819 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:21:09.770449 140169137129280 submission_runner.py:408] Time since start: 9551.12s, 	Step: 27261, 	{'train/accuracy': 0.7091438174247742, 'train/loss': 1.3814681768417358, 'validation/accuracy': 0.6342599987983704, 'validation/loss': 1.7121782302856445, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.3702125549316406, 'test/num_examples': 10000, 'score': 9212.554991006851, 'total_duration': 9551.120630264282, 'accumulated_submission_time': 9212.554991006851, 'accumulated_eval_time': 337.14146423339844, 'accumulated_logging_time': 0.562938928604126}
I0127 22:21:09.793223 140005313861376 logging_writer.py:48] [27261] accumulated_eval_time=337.141464, accumulated_logging_time=0.562939, accumulated_submission_time=9212.554991, global_step=27261, preemption_count=0, score=9212.554991, test/accuracy=0.510200, test/loss=2.370213, test/num_examples=10000, total_duration=9551.120630, train/accuracy=0.709144, train/loss=1.381468, validation/accuracy=0.634260, validation/loss=1.712178, validation/num_examples=50000
I0127 22:21:23.279749 140005322254080 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.311630368232727, loss=3.2321672439575195
I0127 22:21:56.911234 140005313861376 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.2247862815856934, loss=3.2879638671875
I0127 22:22:30.576209 140005322254080 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.419521450996399, loss=3.298727512359619
I0127 22:23:04.232420 140005313861376 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.2603302001953125, loss=3.2852470874786377
I0127 22:23:37.879607 140005322254080 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.3100725412368774, loss=3.2994322776794434
I0127 22:24:11.587418 140005313861376 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.527244210243225, loss=3.326608419418335
I0127 22:24:45.259154 140005322254080 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4277242422103882, loss=3.277970790863037
I0127 22:25:18.886327 140005313861376 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.337963581085205, loss=3.2899720668792725
I0127 22:25:52.547049 140005322254080 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3801437616348267, loss=3.3763792514801025
I0127 22:26:26.169443 140005313861376 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.224353313446045, loss=3.2643468379974365
I0127 22:26:59.830255 140005322254080 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.530232310295105, loss=3.2564663887023926
I0127 22:27:33.502432 140005313861376 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.7568140029907227, loss=3.330692768096924
I0127 22:28:07.169356 140005322254080 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.4058245420455933, loss=3.2460010051727295
I0127 22:28:40.865861 140005313861376 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2346826791763306, loss=3.2029595375061035
I0127 22:29:14.513312 140005322254080 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.2447559833526611, loss=3.2765395641326904
I0127 22:29:39.918721 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:29:46.274943 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:29:54.983382 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:29:57.498437 140169137129280 submission_runner.py:408] Time since start: 10078.85s, 	Step: 28777, 	{'train/accuracy': 0.7155014276504517, 'train/loss': 1.364587664604187, 'validation/accuracy': 0.6462999582290649, 'validation/loss': 1.6733760833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.340773820877075, 'test/num_examples': 10000, 'score': 9722.621646165848, 'total_duration': 10078.848618268967, 'accumulated_submission_time': 9722.621646165848, 'accumulated_eval_time': 354.72114634513855, 'accumulated_logging_time': 0.5966382026672363}
I0127 22:29:57.520499 140004667934464 logging_writer.py:48] [28777] accumulated_eval_time=354.721146, accumulated_logging_time=0.596638, accumulated_submission_time=9722.621646, global_step=28777, preemption_count=0, score=9722.621646, test/accuracy=0.517400, test/loss=2.340774, test/num_examples=10000, total_duration=10078.848618, train/accuracy=0.715501, train/loss=1.364588, validation/accuracy=0.646300, validation/loss=1.673376, validation/num_examples=50000
I0127 22:30:05.662414 140004676327168 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.456466794013977, loss=3.2890076637268066
I0127 22:30:39.289273 140004667934464 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.4423322677612305, loss=3.3012866973876953
I0127 22:31:12.927830 140004676327168 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.6250303983688354, loss=3.262477397918701
I0127 22:31:46.599259 140004667934464 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.2154743671417236, loss=3.2230610847473145
I0127 22:32:20.285458 140004676327168 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.3846482038497925, loss=3.232353448867798
I0127 22:32:53.948534 140004667934464 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.2463403940200806, loss=3.172532081604004
I0127 22:33:27.609500 140004676327168 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.812708854675293, loss=3.2527692317962646
I0127 22:34:01.274747 140004667934464 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.2690603733062744, loss=3.2997610569000244
I0127 22:34:34.936942 140004676327168 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2443448305130005, loss=3.251121997833252
I0127 22:35:08.588568 140004667934464 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3065241575241089, loss=3.271958589553833
I0127 22:35:42.248004 140004676327168 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.5257874727249146, loss=3.2546472549438477
I0127 22:36:15.991946 140004667934464 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.499651312828064, loss=3.262570381164551
I0127 22:36:49.695894 140004676327168 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.3123576641082764, loss=3.3122308254241943
I0127 22:37:23.315509 140004667934464 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.6772284507751465, loss=3.24534273147583
I0127 22:37:56.948983 140004676327168 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.5671818256378174, loss=3.2388253211975098
I0127 22:38:27.694921 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:38:34.156687 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:38:42.839370 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:38:45.391595 140169137129280 submission_runner.py:408] Time since start: 10606.74s, 	Step: 30293, 	{'train/accuracy': 0.7161391973495483, 'train/loss': 1.3571972846984863, 'validation/accuracy': 0.6464399695396423, 'validation/loss': 1.6608171463012695, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.338981866836548, 'test/num_examples': 10000, 'score': 10232.737368822098, 'total_duration': 10606.741770744324, 'accumulated_submission_time': 10232.737368822098, 'accumulated_eval_time': 372.41778016090393, 'accumulated_logging_time': 0.6293659210205078}
I0127 22:38:45.417383 140004667934464 logging_writer.py:48] [30293] accumulated_eval_time=372.417780, accumulated_logging_time=0.629366, accumulated_submission_time=10232.737369, global_step=30293, preemption_count=0, score=10232.737369, test/accuracy=0.514800, test/loss=2.338982, test/num_examples=10000, total_duration=10606.741771, train/accuracy=0.716139, train/loss=1.357197, validation/accuracy=0.646440, validation/loss=1.660817, validation/num_examples=50000
I0127 22:38:48.111849 140004676327168 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.5376334190368652, loss=3.332906723022461
I0127 22:39:21.713164 140004667934464 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4672863483428955, loss=3.2296175956726074
I0127 22:39:55.360221 140004676327168 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3045275211334229, loss=3.245856523513794
I0127 22:40:29.026084 140004667934464 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.4143264293670654, loss=3.233058214187622
I0127 22:41:02.684914 140004676327168 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.3136097192764282, loss=3.32051420211792
I0127 22:41:36.337900 140004667934464 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4036643505096436, loss=3.2385430335998535
I0127 22:42:09.956767 140004676327168 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2305829524993896, loss=3.286738634109497
I0127 22:42:43.650344 140004667934464 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4051605463027954, loss=3.2268967628479004
I0127 22:43:17.295430 140004676327168 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.3774363994598389, loss=3.3316211700439453
I0127 22:43:50.926312 140004667934464 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.435447335243225, loss=3.183695077896118
I0127 22:44:24.558423 140004676327168 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.3692700862884521, loss=3.209172487258911
I0127 22:44:58.558946 140004667934464 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.2719495296478271, loss=3.2622361183166504
I0127 22:45:32.222582 140004676327168 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.3789422512054443, loss=3.202117919921875
I0127 22:46:05.887778 140004667934464 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.6460511684417725, loss=3.199383497238159
I0127 22:46:39.513129 140004676327168 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2672370672225952, loss=3.249152421951294
I0127 22:47:13.178395 140004667934464 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.4527292251586914, loss=3.332165479660034
I0127 22:47:15.688953 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:47:22.090669 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:47:30.710530 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:47:33.209763 140169137129280 submission_runner.py:408] Time since start: 11134.56s, 	Step: 31809, 	{'train/accuracy': 0.70511794090271, 'train/loss': 1.4040005207061768, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.6982835531234741, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.3254666328430176, 'test/num_examples': 10000, 'score': 10742.950670957565, 'total_duration': 11134.55994296074, 'accumulated_submission_time': 10742.950670957565, 'accumulated_eval_time': 389.93855023384094, 'accumulated_logging_time': 0.665184497833252}
I0127 22:47:33.234229 140005297075968 logging_writer.py:48] [31809] accumulated_eval_time=389.938550, accumulated_logging_time=0.665184, accumulated_submission_time=10742.950671, global_step=31809, preemption_count=0, score=10742.950671, test/accuracy=0.518400, test/loss=2.325467, test/num_examples=10000, total_duration=11134.559943, train/accuracy=0.705118, train/loss=1.404001, validation/accuracy=0.640280, validation/loss=1.698284, validation/num_examples=50000
I0127 22:48:04.147109 140005305468672 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.5110054016113281, loss=3.2529220581054688
I0127 22:48:37.855689 140005297075968 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.4368468523025513, loss=3.3300020694732666
I0127 22:49:11.508835 140005305468672 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.269925832748413, loss=3.201070547103882
I0127 22:49:45.166172 140005297075968 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.5805507898330688, loss=3.2182958126068115
I0127 22:50:18.812587 140005305468672 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.5324161052703857, loss=3.1727218627929688
I0127 22:50:52.461262 140005297075968 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3531666994094849, loss=3.186565637588501
I0127 22:51:26.148459 140005305468672 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.4562816619873047, loss=3.276754856109619
I0127 22:51:59.797047 140005297075968 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5765687227249146, loss=3.221773624420166
I0127 22:52:33.477103 140005305468672 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.4789365530014038, loss=3.213292360305786
I0127 22:53:07.102463 140005297075968 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.4221689701080322, loss=3.261820077896118
I0127 22:53:40.753455 140005305468672 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.3413269519805908, loss=3.1728761196136475
I0127 22:54:14.436514 140005297075968 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.4446042776107788, loss=3.221214771270752
I0127 22:54:48.171157 140005305468672 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.412048578262329, loss=3.2770638465881348
I0127 22:55:21.836508 140005297075968 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.418820858001709, loss=3.2258591651916504
I0127 22:55:55.503174 140005305468672 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.3762840032577515, loss=3.2399492263793945
I0127 22:56:03.383910 140169137129280 spec.py:321] Evaluating on the training split.
I0127 22:56:09.818450 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 22:56:18.414401 140169137129280 spec.py:349] Evaluating on the test split.
I0127 22:56:20.939609 140169137129280 submission_runner.py:408] Time since start: 11662.29s, 	Step: 33325, 	{'train/accuracy': 0.7089444994926453, 'train/loss': 1.3775570392608643, 'validation/accuracy': 0.6474399566650391, 'validation/loss': 1.6512587070465088, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.3262481689453125, 'test/num_examples': 10000, 'score': 11253.040585517883, 'total_duration': 11662.289780378342, 'accumulated_submission_time': 11253.040585517883, 'accumulated_eval_time': 407.4942150115967, 'accumulated_logging_time': 0.7008495330810547}
I0127 22:56:20.963283 140004676327168 logging_writer.py:48] [33325] accumulated_eval_time=407.494215, accumulated_logging_time=0.700850, accumulated_submission_time=11253.040586, global_step=33325, preemption_count=0, score=11253.040586, test/accuracy=0.518500, test/loss=2.326248, test/num_examples=10000, total_duration=11662.289780, train/accuracy=0.708944, train/loss=1.377557, validation/accuracy=0.647440, validation/loss=1.651259, validation/num_examples=50000
I0127 22:56:46.514339 140005288683264 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.5264184474945068, loss=3.2032907009124756
I0127 22:57:20.149278 140004676327168 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.3822600841522217, loss=3.1974167823791504
I0127 22:57:53.808768 140005288683264 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.4331283569335938, loss=3.215353488922119
I0127 22:58:27.432000 140004676327168 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.478723406791687, loss=3.2879977226257324
I0127 22:59:01.085200 140005288683264 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.3889069557189941, loss=3.224602699279785
I0127 22:59:34.752125 140004676327168 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2957924604415894, loss=3.2221274375915527
I0127 23:00:08.401456 140005288683264 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.479719877243042, loss=3.282000780105591
I0127 23:00:42.078452 140004676327168 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.3006300926208496, loss=3.2515528202056885
I0127 23:01:15.800899 140005288683264 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.5805026292800903, loss=3.2804980278015137
I0127 23:01:49.465950 140004676327168 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.4639467000961304, loss=3.3202733993530273
I0127 23:02:23.154196 140005288683264 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.3824323415756226, loss=3.292477607727051
I0127 23:02:56.823772 140004676327168 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.3160561323165894, loss=3.290785789489746
I0127 23:03:30.484992 140005288683264 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3996881246566772, loss=3.241614580154419
I0127 23:04:04.136793 140004676327168 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.6215373277664185, loss=3.240297555923462
I0127 23:04:37.832974 140005288683264 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.4816313982009888, loss=3.272170066833496
I0127 23:04:51.087592 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:04:57.533962 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:05:06.355520 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:05:08.881699 140169137129280 submission_runner.py:408] Time since start: 12190.23s, 	Step: 34841, 	{'train/accuracy': 0.7420878410339355, 'train/loss': 1.2352484464645386, 'validation/accuracy': 0.6431399583816528, 'validation/loss': 1.672220230102539, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.30928897857666, 'test/num_examples': 10000, 'score': 11763.106231689453, 'total_duration': 12190.231878995895, 'accumulated_submission_time': 11763.106231689453, 'accumulated_eval_time': 425.2882845401764, 'accumulated_logging_time': 0.7346818447113037}
I0127 23:05:08.905964 140004676327168 logging_writer.py:48] [34841] accumulated_eval_time=425.288285, accumulated_logging_time=0.734682, accumulated_submission_time=11763.106232, global_step=34841, preemption_count=0, score=11763.106232, test/accuracy=0.517000, test/loss=2.309289, test/num_examples=10000, total_duration=12190.231879, train/accuracy=0.742088, train/loss=1.235248, validation/accuracy=0.643140, validation/loss=1.672220, validation/num_examples=50000
I0127 23:05:29.067722 140005297075968 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2961477041244507, loss=3.2706964015960693
I0127 23:06:02.639229 140004676327168 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.604588508605957, loss=3.2781190872192383
I0127 23:06:36.244037 140005297075968 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.515598177909851, loss=3.232675552368164
I0127 23:07:09.926305 140004676327168 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.3345190286636353, loss=3.1693286895751953
I0127 23:07:43.520215 140005297075968 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.4295334815979004, loss=3.226417303085327
I0127 23:08:17.164530 140004676327168 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.5447022914886475, loss=3.2410812377929688
I0127 23:08:50.796291 140005297075968 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.3071907758712769, loss=3.2057785987854004
I0127 23:09:24.419106 140004676327168 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.2937942743301392, loss=3.183966636657715
I0127 23:09:58.069892 140005297075968 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.3982148170471191, loss=3.2343428134918213
I0127 23:10:31.732463 140004676327168 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.4670608043670654, loss=3.235567092895508
I0127 23:11:05.412258 140005297075968 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.4272593259811401, loss=3.2270543575286865
I0127 23:11:39.065625 140004676327168 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4738719463348389, loss=3.2306911945343018
I0127 23:12:12.720046 140005297075968 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.5650904178619385, loss=3.1890780925750732
I0127 23:12:46.386072 140004676327168 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.5113381147384644, loss=3.2227511405944824
I0127 23:13:20.112812 140005297075968 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.4414665699005127, loss=3.225537061691284
I0127 23:13:39.061164 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:13:45.554883 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:13:54.405906 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:13:56.943987 140169137129280 submission_runner.py:408] Time since start: 12718.29s, 	Step: 36358, 	{'train/accuracy': 0.7210817933082581, 'train/loss': 1.3398000001907349, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.693834900856018, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.3678078651428223, 'test/num_examples': 10000, 'score': 12273.201929330826, 'total_duration': 12718.294162273407, 'accumulated_submission_time': 12273.201929330826, 'accumulated_eval_time': 443.1710669994354, 'accumulated_logging_time': 0.7699494361877441}
I0127 23:13:56.969863 140004676327168 logging_writer.py:48] [36358] accumulated_eval_time=443.171067, accumulated_logging_time=0.769949, accumulated_submission_time=12273.201929, global_step=36358, preemption_count=0, score=12273.201929, test/accuracy=0.514900, test/loss=2.367808, test/num_examples=10000, total_duration=12718.294162, train/accuracy=0.721082, train/loss=1.339800, validation/accuracy=0.641660, validation/loss=1.693835, validation/num_examples=50000
I0127 23:14:11.448329 140005288683264 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.4151798486709595, loss=3.225341320037842
I0127 23:14:45.053291 140004676327168 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.744828224182129, loss=3.302987813949585
I0127 23:15:18.688627 140005288683264 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.3932650089263916, loss=3.110260486602783
I0127 23:15:52.341521 140004676327168 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.550559163093567, loss=3.2334671020507812
I0127 23:16:26.005306 140005288683264 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.5153151750564575, loss=3.2021288871765137
I0127 23:16:59.650762 140004676327168 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.4192606210708618, loss=3.2190980911254883
I0127 23:17:33.292664 140005288683264 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.4620262384414673, loss=3.2198843955993652
I0127 23:18:06.937635 140004676327168 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.4518723487854004, loss=3.156435966491699
I0127 23:18:40.600326 140005288683264 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.5040701627731323, loss=3.238272190093994
I0127 23:19:14.289417 140004676327168 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.3640626668930054, loss=3.25566029548645
I0127 23:19:48.006143 140005288683264 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5992039442062378, loss=3.2024333477020264
I0127 23:20:21.660983 140004676327168 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4652420282363892, loss=3.2317941188812256
I0127 23:20:55.320448 140005288683264 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4431204795837402, loss=3.2084949016571045
I0127 23:21:28.977270 140004676327168 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.6535091400146484, loss=3.270423412322998
I0127 23:22:02.632855 140005288683264 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.4721111059188843, loss=3.1346676349639893
I0127 23:22:27.008023 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:22:33.364491 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:22:42.117914 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:22:44.627636 140169137129280 submission_runner.py:408] Time since start: 13245.98s, 	Step: 37874, 	{'train/accuracy': 0.7284956574440002, 'train/loss': 1.312329649925232, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.6255125999450684, 'validation/num_examples': 50000, 'test/accuracy': 0.5271000266075134, 'test/loss': 2.265986442565918, 'test/num_examples': 10000, 'score': 12783.18024611473, 'total_duration': 13245.977816104889, 'accumulated_submission_time': 12783.18024611473, 'accumulated_eval_time': 460.7906458377838, 'accumulated_logging_time': 0.8074545860290527}
I0127 23:22:44.655316 140005322254080 logging_writer.py:48] [37874] accumulated_eval_time=460.790646, accumulated_logging_time=0.807455, accumulated_submission_time=12783.180246, global_step=37874, preemption_count=0, score=12783.180246, test/accuracy=0.527100, test/loss=2.265986, test/num_examples=10000, total_duration=13245.977816, train/accuracy=0.728496, train/loss=1.312330, validation/accuracy=0.658220, validation/loss=1.625513, validation/num_examples=50000
I0127 23:22:53.756469 140005330646784 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.593955159187317, loss=3.26039981842041
I0127 23:23:27.392325 140005322254080 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.4854450225830078, loss=3.216420888900757
I0127 23:24:01.040229 140005330646784 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.3906283378601074, loss=3.158519744873047
I0127 23:24:34.678920 140005322254080 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.450474739074707, loss=3.2229275703430176
I0127 23:25:08.318684 140005330646784 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.7446929216384888, loss=3.2690563201904297
I0127 23:25:42.010954 140005322254080 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.0959255695343018, loss=3.2038116455078125
I0127 23:26:15.682564 140005330646784 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.5855282545089722, loss=3.226072311401367
I0127 23:26:49.352991 140005322254080 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5450758934020996, loss=3.2127139568328857
I0127 23:27:23.013772 140005330646784 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.4351478815078735, loss=3.1605136394500732
I0127 23:27:56.649327 140005322254080 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.568166971206665, loss=3.1819002628326416
I0127 23:28:30.306355 140005330646784 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.4804879426956177, loss=3.1993391513824463
I0127 23:29:03.950972 140005322254080 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.6268882751464844, loss=3.100825309753418
I0127 23:29:37.548665 140005330646784 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.479068398475647, loss=3.257232427597046
I0127 23:30:11.246026 140005322254080 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.4209107160568237, loss=3.201113224029541
I0127 23:30:44.866637 140005330646784 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.809201717376709, loss=3.221007823944092
I0127 23:31:14.921683 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:31:21.229756 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:31:30.056399 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:31:32.563160 140169137129280 submission_runner.py:408] Time since start: 13773.91s, 	Step: 39391, 	{'train/accuracy': 0.7284956574440002, 'train/loss': 1.298724889755249, 'validation/accuracy': 0.6593199968338013, 'validation/loss': 1.6137521266937256, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.272613525390625, 'test/num_examples': 10000, 'score': 13293.384312152863, 'total_duration': 13773.91310286522, 'accumulated_submission_time': 13293.384312152863, 'accumulated_eval_time': 478.4318549633026, 'accumulated_logging_time': 0.8491504192352295}
I0127 23:31:32.586916 140005288683264 logging_writer.py:48] [39391] accumulated_eval_time=478.431855, accumulated_logging_time=0.849150, accumulated_submission_time=13293.384312, global_step=39391, preemption_count=0, score=13293.384312, test/accuracy=0.528300, test/loss=2.272614, test/num_examples=10000, total_duration=13773.913103, train/accuracy=0.728496, train/loss=1.298725, validation/accuracy=0.659320, validation/loss=1.613752, validation/num_examples=50000
I0127 23:31:35.966495 140005297075968 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.681167483329773, loss=3.196437120437622
I0127 23:32:09.637702 140005288683264 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.7957897186279297, loss=3.192796230316162
I0127 23:32:43.311275 140005297075968 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.471531867980957, loss=3.200490951538086
I0127 23:33:16.938312 140005288683264 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.588904619216919, loss=3.1951346397399902
I0127 23:33:50.569745 140005297075968 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.5296142101287842, loss=3.2987260818481445
I0127 23:34:24.242765 140005288683264 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.7133547067642212, loss=3.1993532180786133
I0127 23:34:57.918116 140005297075968 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.5063673257827759, loss=3.2376132011413574
I0127 23:35:31.533945 140005288683264 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.7920641899108887, loss=3.1269900798797607
I0127 23:36:05.209270 140005297075968 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.456357479095459, loss=3.204047918319702
I0127 23:36:38.837034 140005288683264 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.5597848892211914, loss=3.1264264583587646
I0127 23:37:12.479367 140005297075968 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.4814910888671875, loss=3.1977663040161133
I0127 23:37:46.090819 140005288683264 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.5332527160644531, loss=3.122169256210327
I0127 23:38:19.809718 140005297075968 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.7724353075027466, loss=3.188471794128418
I0127 23:38:53.425359 140005288683264 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.5320764780044556, loss=3.3167202472686768
I0127 23:39:27.106634 140005297075968 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7444790601730347, loss=3.2400758266448975
I0127 23:40:00.776340 140005288683264 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7521483898162842, loss=3.209941864013672
I0127 23:40:02.616375 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:40:09.023382 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:40:17.827809 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:40:20.341406 140169137129280 submission_runner.py:408] Time since start: 14301.69s, 	Step: 40907, 	{'train/accuracy': 0.7203842401504517, 'train/loss': 1.3398540019989014, 'validation/accuracy': 0.6532599925994873, 'validation/loss': 1.6414886713027954, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.2787270545959473, 'test/num_examples': 10000, 'score': 13803.354566812515, 'total_duration': 14301.691583871841, 'accumulated_submission_time': 13803.354566812515, 'accumulated_eval_time': 496.1568441390991, 'accumulated_logging_time': 0.8838469982147217}
I0127 23:40:20.368707 140005305468672 logging_writer.py:48] [40907] accumulated_eval_time=496.156844, accumulated_logging_time=0.883847, accumulated_submission_time=13803.354567, global_step=40907, preemption_count=0, score=13803.354567, test/accuracy=0.524400, test/loss=2.278727, test/num_examples=10000, total_duration=14301.691584, train/accuracy=0.720384, train/loss=1.339854, validation/accuracy=0.653260, validation/loss=1.641489, validation/num_examples=50000
I0127 23:40:51.995288 140005313861376 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.619532823562622, loss=3.224844455718994
I0127 23:41:25.641924 140005305468672 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.6380573511123657, loss=3.19262957572937
I0127 23:41:59.296956 140005313861376 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.631313681602478, loss=3.276992082595825
I0127 23:42:32.913263 140005305468672 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.561379075050354, loss=3.2298474311828613
I0127 23:43:06.553332 140005313861376 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.5805808305740356, loss=3.1638026237487793
I0127 23:43:40.231113 140005305468672 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.5584183931350708, loss=3.265066623687744
I0127 23:44:13.957888 140005313861376 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.818306565284729, loss=3.2265193462371826
I0127 23:44:47.636440 140005305468672 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.4987616539001465, loss=3.195563316345215
I0127 23:45:21.284347 140005313861376 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7427130937576294, loss=3.13765811920166
I0127 23:45:54.943070 140005305468672 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8715440034866333, loss=3.2345781326293945
I0127 23:46:28.603443 140005313861376 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7215499877929688, loss=3.1844983100891113
I0127 23:47:02.266873 140005305468672 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.6478211879730225, loss=3.1364009380340576
I0127 23:47:35.911986 140005313861376 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.5727027654647827, loss=3.172762870788574
I0127 23:48:09.569841 140005305468672 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.5431264638900757, loss=3.130258321762085
I0127 23:48:43.237818 140005313861376 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.6943973302841187, loss=3.2119765281677246
I0127 23:48:50.445121 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:48:57.419847 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:49:06.515809 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:49:09.061414 140169137129280 submission_runner.py:408] Time since start: 14830.41s, 	Step: 42423, 	{'train/accuracy': 0.7283163070678711, 'train/loss': 1.3241567611694336, 'validation/accuracy': 0.6582599878311157, 'validation/loss': 1.6258599758148193, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.2608909606933594, 'test/num_examples': 10000, 'score': 14313.371778011322, 'total_duration': 14830.411584377289, 'accumulated_submission_time': 14313.371778011322, 'accumulated_eval_time': 514.7730877399445, 'accumulated_logging_time': 0.921715259552002}
I0127 23:49:09.086583 140004659541760 logging_writer.py:48] [42423] accumulated_eval_time=514.773088, accumulated_logging_time=0.921715, accumulated_submission_time=14313.371778, global_step=42423, preemption_count=0, score=14313.371778, test/accuracy=0.534800, test/loss=2.260891, test/num_examples=10000, total_duration=14830.411584, train/accuracy=0.728316, train/loss=1.324157, validation/accuracy=0.658260, validation/loss=1.625860, validation/num_examples=50000
I0127 23:49:35.320408 140004667934464 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.622645616531372, loss=3.1392571926116943
I0127 23:50:08.947471 140004659541760 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.5577600002288818, loss=3.1630301475524902
I0127 23:50:42.643592 140004667934464 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.9645037651062012, loss=3.132075786590576
I0127 23:51:16.245980 140004659541760 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5347365140914917, loss=3.1550331115722656
I0127 23:51:49.869585 140004667934464 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7619662284851074, loss=3.1734719276428223
I0127 23:52:23.554348 140004659541760 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6390682458877563, loss=3.212378740310669
I0127 23:52:57.186249 140004667934464 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.681915521621704, loss=3.167769193649292
I0127 23:53:30.830472 140004659541760 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.817918300628662, loss=3.186417818069458
I0127 23:54:04.487090 140004667934464 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.4862009286880493, loss=3.1494994163513184
I0127 23:54:38.145569 140004659541760 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.549080491065979, loss=3.1631522178649902
I0127 23:55:11.800309 140004667934464 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7300039529800415, loss=3.180764675140381
I0127 23:55:45.443015 140004659541760 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.6960934400558472, loss=3.2248589992523193
I0127 23:56:19.082025 140004667934464 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.7166563272476196, loss=3.1152255535125732
I0127 23:56:52.809679 140004659541760 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.6424341201782227, loss=3.221749782562256
I0127 23:57:26.481108 140004667934464 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7318565845489502, loss=3.2222180366516113
I0127 23:57:39.399778 140169137129280 spec.py:321] Evaluating on the training split.
I0127 23:57:45.696419 140169137129280 spec.py:333] Evaluating on the validation split.
I0127 23:57:54.543199 140169137129280 spec.py:349] Evaluating on the test split.
I0127 23:57:57.088100 140169137129280 submission_runner.py:408] Time since start: 15358.44s, 	Step: 43940, 	{'train/accuracy': 0.7344746589660645, 'train/loss': 1.301184892654419, 'validation/accuracy': 0.6468999981880188, 'validation/loss': 1.689677119255066, 'validation/num_examples': 50000, 'test/accuracy': 0.5181000232696533, 'test/loss': 2.357792377471924, 'test/num_examples': 10000, 'score': 14823.624761104584, 'total_duration': 15358.438279390335, 'accumulated_submission_time': 14823.624761104584, 'accumulated_eval_time': 532.4613721370697, 'accumulated_logging_time': 0.9581685066223145}
I0127 23:57:57.113846 140005313861376 logging_writer.py:48] [43940] accumulated_eval_time=532.461372, accumulated_logging_time=0.958169, accumulated_submission_time=14823.624761, global_step=43940, preemption_count=0, score=14823.624761, test/accuracy=0.518100, test/loss=2.357792, test/num_examples=10000, total_duration=15358.438279, train/accuracy=0.734475, train/loss=1.301185, validation/accuracy=0.646900, validation/loss=1.689677, validation/num_examples=50000
I0127 23:58:17.581298 140005322254080 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.5583250522613525, loss=3.2293994426727295
I0127 23:58:51.122163 140005313861376 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.7781007289886475, loss=3.163215160369873
I0127 23:59:24.699879 140005322254080 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.5568735599517822, loss=3.2257087230682373
I0127 23:59:58.259043 140005313861376 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.6204485893249512, loss=3.183357000350952
I0128 00:00:31.953639 140005322254080 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6009998321533203, loss=3.24070405960083
I0128 00:01:05.586841 140005313861376 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.763680100440979, loss=3.2617366313934326
I0128 00:01:39.213462 140005322254080 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7431864738464355, loss=3.1480350494384766
I0128 00:02:12.857103 140005313861376 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.5775126218795776, loss=3.1498355865478516
I0128 00:02:46.600200 140005322254080 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.4868582487106323, loss=3.1593923568725586
I0128 00:03:20.187164 140005313861376 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7021548748016357, loss=3.1469695568084717
I0128 00:03:53.802854 140005322254080 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7439004182815552, loss=3.1302201747894287
I0128 00:04:27.455226 140005313861376 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.8700060844421387, loss=3.186781406402588
I0128 00:05:01.082250 140005322254080 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.6698577404022217, loss=3.088697910308838
I0128 00:05:34.715646 140005313861376 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5446803569793701, loss=3.161797046661377
I0128 00:06:08.360978 140005322254080 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6130602359771729, loss=3.096987009048462
I0128 00:06:27.340623 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:06:33.662541 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:06:42.511171 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:06:45.034982 140169137129280 submission_runner.py:408] Time since start: 15886.39s, 	Step: 45458, 	{'train/accuracy': 0.7339963316917419, 'train/loss': 1.248931646347046, 'validation/accuracy': 0.6551799774169922, 'validation/loss': 1.5993354320526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2523021697998047, 'test/num_examples': 10000, 'score': 15333.792750835419, 'total_duration': 15886.385159492493, 'accumulated_submission_time': 15333.792750835419, 'accumulated_eval_time': 550.1557083129883, 'accumulated_logging_time': 0.9941191673278809}
I0128 00:06:45.062133 140004676327168 logging_writer.py:48] [45458] accumulated_eval_time=550.155708, accumulated_logging_time=0.994119, accumulated_submission_time=15333.792751, global_step=45458, preemption_count=0, score=15333.792751, test/accuracy=0.531400, test/loss=2.252302, test/num_examples=10000, total_duration=15886.385159, train/accuracy=0.733996, train/loss=1.248932, validation/accuracy=0.655180, validation/loss=1.599335, validation/num_examples=50000
I0128 00:06:59.495531 140005288683264 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.590049147605896, loss=3.084749221801758
I0128 00:07:33.064662 140004676327168 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.6739544868469238, loss=3.1892452239990234
I0128 00:08:06.740007 140005288683264 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5986008644104004, loss=3.179084062576294
I0128 00:08:40.404028 140004676327168 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.7173776626586914, loss=3.1414756774902344
I0128 00:09:14.116986 140005288683264 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.684353232383728, loss=3.1955204010009766
I0128 00:09:47.783018 140004676327168 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7263392210006714, loss=3.2222096920013428
I0128 00:10:21.410944 140005288683264 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.5932384729385376, loss=3.1584086418151855
I0128 00:10:55.059773 140004676327168 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.8554632663726807, loss=3.190392017364502
I0128 00:11:28.728660 140005288683264 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.7115888595581055, loss=3.0975520610809326
I0128 00:12:02.395843 140004676327168 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.726894497871399, loss=3.106630802154541
I0128 00:12:36.002427 140005288683264 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.783183217048645, loss=3.1793742179870605
I0128 00:13:09.581490 140004676327168 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6657949686050415, loss=3.1846795082092285
I0128 00:13:43.184707 140005288683264 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7197380065917969, loss=3.128326177597046
I0128 00:14:16.820472 140004676327168 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.6718894243240356, loss=3.1674864292144775
I0128 00:14:50.474251 140005288683264 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.706925630569458, loss=3.182345390319824
I0128 00:15:15.248747 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:15:21.489466 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:15:30.214279 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:15:32.742013 140169137129280 submission_runner.py:408] Time since start: 16414.09s, 	Step: 46975, 	{'train/accuracy': 0.7340162396430969, 'train/loss': 1.2588564157485962, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.5765480995178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.250394344329834, 'test/num_examples': 10000, 'score': 15843.919181346893, 'total_duration': 16414.092190027237, 'accumulated_submission_time': 15843.919181346893, 'accumulated_eval_time': 567.648931980133, 'accumulated_logging_time': 1.0327842235565186}
I0128 00:15:32.768366 140004667934464 logging_writer.py:48] [46975] accumulated_eval_time=567.648932, accumulated_logging_time=1.032784, accumulated_submission_time=15843.919181, global_step=46975, preemption_count=0, score=15843.919181, test/accuracy=0.532000, test/loss=2.250394, test/num_examples=10000, total_duration=16414.092190, train/accuracy=0.734016, train/loss=1.258856, validation/accuracy=0.662740, validation/loss=1.576548, validation/num_examples=50000
I0128 00:15:41.523166 140004676327168 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.5670621395111084, loss=3.188974380493164
I0128 00:16:15.155111 140004667934464 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.0670487880706787, loss=3.160447120666504
I0128 00:16:48.803912 140004676327168 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.8283129930496216, loss=3.1842503547668457
I0128 00:17:22.467386 140004667934464 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.9016222953796387, loss=3.167539358139038
I0128 00:17:56.112451 140004676327168 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.6207538843154907, loss=3.181399345397949
I0128 00:18:29.791363 140004667934464 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7551625967025757, loss=3.219205856323242
I0128 00:19:03.441374 140004676327168 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.7727240324020386, loss=3.129099130630493
I0128 00:19:37.094184 140004667934464 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7384122610092163, loss=3.127329111099243
I0128 00:20:10.696567 140004676327168 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.777127981185913, loss=3.212040424346924
I0128 00:20:44.333168 140004667934464 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.6702193021774292, loss=3.1336116790771484
I0128 00:21:17.992361 140004676327168 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.8629491329193115, loss=3.1534202098846436
I0128 00:21:51.718087 140004667934464 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.6739188432693481, loss=3.0926144123077393
I0128 00:22:25.371940 140004676327168 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7508881092071533, loss=3.1749234199523926
I0128 00:22:59.009132 140004667934464 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.8162736892700195, loss=3.2442517280578613
I0128 00:23:32.661626 140004676327168 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.607201099395752, loss=3.2754077911376953
I0128 00:24:02.758815 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:24:08.981842 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:24:18.051558 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:24:20.571478 140169137129280 submission_runner.py:408] Time since start: 16941.92s, 	Step: 48491, 	{'train/accuracy': 0.7350525856018066, 'train/loss': 1.2592726945877075, 'validation/accuracy': 0.665619969367981, 'validation/loss': 1.5760258436203003, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2234253883361816, 'test/num_examples': 10000, 'score': 16353.850271701813, 'total_duration': 16941.92165875435, 'accumulated_submission_time': 16353.850271701813, 'accumulated_eval_time': 585.4615631103516, 'accumulated_logging_time': 1.0696442127227783}
I0128 00:24:20.600362 140004676327168 logging_writer.py:48] [48491] accumulated_eval_time=585.461563, accumulated_logging_time=1.069644, accumulated_submission_time=16353.850272, global_step=48491, preemption_count=0, score=16353.850272, test/accuracy=0.538600, test/loss=2.223425, test/num_examples=10000, total_duration=16941.921659, train/accuracy=0.735053, train/loss=1.259273, validation/accuracy=0.665620, validation/loss=1.576026, validation/num_examples=50000
I0128 00:24:23.983919 140005288683264 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.7637689113616943, loss=3.1150894165039062
I0128 00:24:57.526080 140004676327168 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.713338851928711, loss=3.11488676071167
I0128 00:25:31.152135 140005288683264 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.8630763292312622, loss=3.1354727745056152
I0128 00:26:04.792352 140004676327168 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.703895926475525, loss=3.1477437019348145
I0128 00:26:38.456293 140005288683264 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7851618528366089, loss=3.1894524097442627
I0128 00:27:12.092878 140004676327168 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8024498224258423, loss=3.1681137084960938
I0128 00:27:45.814278 140005288683264 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.6702040433883667, loss=3.1446075439453125
I0128 00:28:19.393723 140004676327168 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.5418089628219604, loss=3.1023826599121094
I0128 00:28:53.064040 140005288683264 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.6567435264587402, loss=3.1784555912017822
I0128 00:29:26.701697 140004676327168 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.6558202505111694, loss=3.2109198570251465
I0128 00:30:00.341946 140005288683264 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.815283179283142, loss=3.097797393798828
I0128 00:30:33.998438 140004676327168 logging_writer.py:48] [49600] global_step=49600, grad_norm=2.0288498401641846, loss=3.1917920112609863
I0128 00:31:07.643400 140005288683264 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6636548042297363, loss=3.1935555934906006
I0128 00:31:41.248421 140004676327168 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8750277757644653, loss=3.1754977703094482
I0128 00:32:14.877362 140005288683264 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9781121015548706, loss=3.1468358039855957
I0128 00:32:48.523517 140004676327168 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7497330904006958, loss=3.119277000427246
I0128 00:32:50.696498 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:32:57.125859 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:33:06.127621 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:33:08.672474 140169137129280 submission_runner.py:408] Time since start: 17470.02s, 	Step: 50008, 	{'train/accuracy': 0.7184112071990967, 'train/loss': 1.2996251583099365, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.5904572010040283, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.27528715133667, 'test/num_examples': 10000, 'score': 16863.88741350174, 'total_duration': 17470.022649526596, 'accumulated_submission_time': 16863.88741350174, 'accumulated_eval_time': 603.4375021457672, 'accumulated_logging_time': 1.108870029449463}
I0128 00:33:08.701545 140004676327168 logging_writer.py:48] [50008] accumulated_eval_time=603.437502, accumulated_logging_time=1.108870, accumulated_submission_time=16863.887414, global_step=50008, preemption_count=0, score=16863.887414, test/accuracy=0.530800, test/loss=2.275287, test/num_examples=10000, total_duration=17470.022650, train/accuracy=0.718411, train/loss=1.299625, validation/accuracy=0.657780, validation/loss=1.590457, validation/num_examples=50000
I0128 00:33:39.953882 140005313861376 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.9633487462997437, loss=3.1905171871185303
I0128 00:34:13.600158 140004676327168 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.815175175666809, loss=3.23319935798645
I0128 00:34:47.187064 140005313861376 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.8872628211975098, loss=3.18646240234375
I0128 00:35:20.805207 140004676327168 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.7865021228790283, loss=3.101914167404175
I0128 00:35:54.447845 140005313861376 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8387656211853027, loss=3.2517504692077637
I0128 00:36:28.102859 140004676327168 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.7768689393997192, loss=3.1585333347320557
I0128 00:37:01.735270 140005313861376 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.6847484111785889, loss=3.1219308376312256
I0128 00:37:35.390353 140004676327168 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.9530301094055176, loss=3.201256513595581
I0128 00:38:09.046268 140005313861376 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.6740882396697998, loss=3.146811008453369
I0128 00:38:42.703184 140004676327168 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8378361463546753, loss=3.192117214202881
I0128 00:39:16.321534 140005313861376 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7796739339828491, loss=3.183605670928955
I0128 00:39:49.995277 140004676327168 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7870455980300903, loss=3.1479339599609375
I0128 00:40:23.734227 140005313861376 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.769999623298645, loss=3.257841110229492
I0128 00:40:57.399292 140004676327168 logging_writer.py:48] [51400] global_step=51400, grad_norm=2.1131155490875244, loss=3.1608450412750244
I0128 00:41:31.010849 140005313861376 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.7655929327011108, loss=3.1977527141571045
I0128 00:41:38.896526 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:41:45.150276 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:41:54.007141 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:41:56.539221 140169137129280 submission_runner.py:408] Time since start: 17997.89s, 	Step: 51525, 	{'train/accuracy': 0.7494618892669678, 'train/loss': 1.2071516513824463, 'validation/accuracy': 0.6755399703979492, 'validation/loss': 1.532850742340088, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.1815757751464844, 'test/num_examples': 10000, 'score': 17374.02089715004, 'total_duration': 17997.889188289642, 'accumulated_submission_time': 17374.02089715004, 'accumulated_eval_time': 621.0799465179443, 'accumulated_logging_time': 1.1506271362304688}
I0128 00:41:56.569530 140005288683264 logging_writer.py:48] [51525] accumulated_eval_time=621.079947, accumulated_logging_time=1.150627, accumulated_submission_time=17374.020897, global_step=51525, preemption_count=0, score=17374.020897, test/accuracy=0.542500, test/loss=2.181576, test/num_examples=10000, total_duration=17997.889188, train/accuracy=0.749462, train/loss=1.207152, validation/accuracy=0.675540, validation/loss=1.532851, validation/num_examples=50000
I0128 00:42:22.117447 140005297075968 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.832483172416687, loss=3.1032586097717285
I0128 00:42:55.733163 140005288683264 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.9996578693389893, loss=3.163151264190674
I0128 00:43:29.385397 140005297075968 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.093596935272217, loss=3.167992115020752
I0128 00:44:03.019075 140005288683264 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8541747331619263, loss=3.2300405502319336
I0128 00:44:36.690368 140005297075968 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.785683512687683, loss=3.178245782852173
I0128 00:45:10.338842 140005288683264 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.7805930376052856, loss=3.1132712364196777
I0128 00:45:43.939542 140005297075968 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.774971842765808, loss=3.0629475116729736
I0128 00:46:17.562458 140005288683264 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8785896301269531, loss=3.149970054626465
I0128 00:46:51.254666 140005297075968 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.6656019687652588, loss=3.1211812496185303
I0128 00:47:24.893978 140005288683264 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.750207781791687, loss=3.1111655235290527
I0128 00:47:58.534312 140005297075968 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9925702810287476, loss=3.0491936206817627
I0128 00:48:32.182253 140005288683264 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8914811611175537, loss=3.2075963020324707
I0128 00:49:05.840448 140005297075968 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8231991529464722, loss=3.1093175411224365
I0128 00:49:39.481287 140005288683264 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.6857560873031616, loss=3.1346917152404785
I0128 00:50:13.078791 140005297075968 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.7950141429901123, loss=3.0983567237854004
I0128 00:50:26.676563 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:50:32.956981 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:50:41.773658 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:50:44.324720 140169137129280 submission_runner.py:408] Time since start: 18525.67s, 	Step: 53042, 	{'train/accuracy': 0.7538065910339355, 'train/loss': 1.2099741697311401, 'validation/accuracy': 0.6650999784469604, 'validation/loss': 1.5931397676467896, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.2342357635498047, 'test/num_examples': 10000, 'score': 17884.068472623825, 'total_duration': 18525.674897909164, 'accumulated_submission_time': 17884.068472623825, 'accumulated_eval_time': 638.7280640602112, 'accumulated_logging_time': 1.1915946006774902}
I0128 00:50:44.352255 140004659541760 logging_writer.py:48] [53042] accumulated_eval_time=638.728064, accumulated_logging_time=1.191595, accumulated_submission_time=17884.068473, global_step=53042, preemption_count=0, score=17884.068473, test/accuracy=0.536300, test/loss=2.234236, test/num_examples=10000, total_duration=18525.674898, train/accuracy=0.753807, train/loss=1.209974, validation/accuracy=0.665100, validation/loss=1.593140, validation/num_examples=50000
I0128 00:51:04.193849 140004667934464 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8236562013626099, loss=3.2253777980804443
I0128 00:51:37.814101 140004659541760 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.9069055318832397, loss=3.088371753692627
I0128 00:52:11.410686 140004667934464 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.8725484609603882, loss=3.180408000946045
I0128 00:52:45.067701 140004659541760 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.863832712173462, loss=3.056706428527832
I0128 00:53:18.699243 140004667934464 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.0040059089660645, loss=3.0994391441345215
I0128 00:53:52.301372 140004659541760 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9672874212265015, loss=3.1386466026306152
I0128 00:54:25.928862 140004667934464 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.6947609186172485, loss=3.1637678146362305
I0128 00:54:59.521976 140004659541760 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8411140441894531, loss=3.1907577514648438
I0128 00:55:33.148548 140004667934464 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.785414457321167, loss=3.092959403991699
I0128 00:56:06.743269 140004659541760 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.873536467552185, loss=3.1413376331329346
I0128 00:56:40.369075 140004667934464 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8766844272613525, loss=3.1883091926574707
I0128 00:57:13.972136 140004659541760 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8941559791564941, loss=3.1539595127105713
I0128 00:57:47.606881 140004667934464 logging_writer.py:48] [54300] global_step=54300, grad_norm=2.0011544227600098, loss=3.2038025856018066
I0128 00:58:21.220792 140004659541760 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8578412532806396, loss=3.127851724624634
I0128 00:58:54.931480 140004667934464 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.8418521881103516, loss=3.1376190185546875
I0128 00:59:14.548562 140169137129280 spec.py:321] Evaluating on the training split.
I0128 00:59:20.921586 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 00:59:29.916093 140169137129280 spec.py:349] Evaluating on the test split.
I0128 00:59:32.490575 140169137129280 submission_runner.py:408] Time since start: 19053.84s, 	Step: 54560, 	{'train/accuracy': 0.7381616830825806, 'train/loss': 1.2561745643615723, 'validation/accuracy': 0.659559965133667, 'validation/loss': 1.598878264427185, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.2853755950927734, 'test/num_examples': 10000, 'score': 18394.205976486206, 'total_duration': 19053.840754270554, 'accumulated_submission_time': 18394.205976486206, 'accumulated_eval_time': 656.6700406074524, 'accumulated_logging_time': 1.2295169830322266}
I0128 00:59:32.522011 140005313861376 logging_writer.py:48] [54560] accumulated_eval_time=656.670041, accumulated_logging_time=1.229517, accumulated_submission_time=18394.205976, global_step=54560, preemption_count=0, score=18394.205976, test/accuracy=0.528100, test/loss=2.285376, test/num_examples=10000, total_duration=19053.840754, train/accuracy=0.738162, train/loss=1.256175, validation/accuracy=0.659560, validation/loss=1.598878, validation/num_examples=50000
I0128 00:59:46.286837 140005322254080 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.8302645683288574, loss=3.08575439453125
I0128 01:00:19.842664 140005313861376 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9434832334518433, loss=3.1117401123046875
I0128 01:00:53.498557 140005322254080 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.9094527959823608, loss=3.2002382278442383
I0128 01:01:27.140789 140005313861376 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.824330449104309, loss=3.154975175857544
I0128 01:02:00.773358 140005322254080 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.8155558109283447, loss=3.1534674167633057
I0128 01:02:34.425047 140005313861376 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8935444355010986, loss=3.1674017906188965
I0128 01:03:08.048042 140005322254080 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.809619665145874, loss=3.164827346801758
I0128 01:03:41.705569 140005313861376 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.0704948902130127, loss=3.08941650390625
I0128 01:04:15.354146 140005322254080 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.794975757598877, loss=3.122920036315918
I0128 01:04:49.004349 140005313861376 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.7699443101882935, loss=3.077143669128418
I0128 01:05:22.723602 140005322254080 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.9399443864822388, loss=3.0801589488983154
I0128 01:05:56.358707 140005313861376 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.832624912261963, loss=3.146623373031616
I0128 01:06:30.004840 140005322254080 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.0896012783050537, loss=3.1657657623291016
I0128 01:07:03.620337 140005313861376 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7328906059265137, loss=3.109286308288574
I0128 01:07:37.255657 140005322254080 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.9301741123199463, loss=3.1120569705963135
I0128 01:08:02.634527 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:08:08.843174 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:08:17.759354 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:08:20.259087 140169137129280 submission_runner.py:408] Time since start: 19581.61s, 	Step: 56077, 	{'train/accuracy': 0.7382214665412903, 'train/loss': 1.2343106269836426, 'validation/accuracy': 0.6646400094032288, 'validation/loss': 1.5662380456924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.216094732284546, 'test/num_examples': 10000, 'score': 18904.258904457092, 'total_duration': 19581.609267950058, 'accumulated_submission_time': 18904.258904457092, 'accumulated_eval_time': 674.2945744991302, 'accumulated_logging_time': 1.2716209888458252}
I0128 01:08:20.288295 140004676327168 logging_writer.py:48] [56077] accumulated_eval_time=674.294574, accumulated_logging_time=1.271621, accumulated_submission_time=18904.258904, global_step=56077, preemption_count=0, score=18904.258904, test/accuracy=0.537900, test/loss=2.216095, test/num_examples=10000, total_duration=19581.609268, train/accuracy=0.738221, train/loss=1.234311, validation/accuracy=0.664640, validation/loss=1.566238, validation/num_examples=50000
I0128 01:08:28.369553 140005288683264 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.7226773500442505, loss=3.043121576309204
I0128 01:09:01.971422 140004676327168 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.048067569732666, loss=3.121352195739746
I0128 01:09:35.612756 140005288683264 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.9687420129776, loss=3.043982982635498
I0128 01:10:09.267189 140004676327168 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8893663883209229, loss=3.156371831893921
I0128 01:10:42.901363 140005288683264 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.833350419998169, loss=3.1411056518554688
I0128 01:11:16.639482 140004676327168 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9247093200683594, loss=3.2197518348693848
I0128 01:11:50.324401 140005288683264 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.073136329650879, loss=3.1787478923797607
I0128 01:12:23.974837 140004676327168 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.0195353031158447, loss=3.2346720695495605
I0128 01:12:57.588655 140005288683264 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.805851697921753, loss=3.0984225273132324
I0128 01:13:31.264471 140004676327168 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.90670645236969, loss=3.208571195602417
I0128 01:14:04.886513 140005288683264 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.0571982860565186, loss=3.105048179626465
I0128 01:14:38.502444 140004676327168 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.9924092292785645, loss=3.1052496433258057
I0128 01:15:12.153263 140005288683264 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.994102954864502, loss=3.1842713356018066
I0128 01:15:45.809791 140004676327168 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9388017654418945, loss=3.0863938331604004
I0128 01:16:19.462258 140005288683264 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.9111264944076538, loss=3.1478986740112305
I0128 01:16:50.541765 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:16:56.864416 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:17:05.696459 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:17:08.193533 140169137129280 submission_runner.py:408] Time since start: 20109.54s, 	Step: 57594, 	{'train/accuracy': 0.7438416481018066, 'train/loss': 1.227565050125122, 'validation/accuracy': 0.6708399653434753, 'validation/loss': 1.5553630590438843, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.204932928085327, 'test/num_examples': 10000, 'score': 19414.452545642853, 'total_duration': 20109.543710708618, 'accumulated_submission_time': 19414.452545642853, 'accumulated_eval_time': 691.946305513382, 'accumulated_logging_time': 1.3120112419128418}
I0128 01:17:08.224248 140005288683264 logging_writer.py:48] [57594] accumulated_eval_time=691.946306, accumulated_logging_time=1.312011, accumulated_submission_time=19414.452546, global_step=57594, preemption_count=0, score=19414.452546, test/accuracy=0.541600, test/loss=2.204933, test/num_examples=10000, total_duration=20109.543711, train/accuracy=0.743842, train/loss=1.227565, validation/accuracy=0.670840, validation/loss=1.555363, validation/num_examples=50000
I0128 01:17:10.594216 140005313861376 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.896683692932129, loss=3.089581251144409
I0128 01:17:44.189168 140005288683264 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.7394850254058838, loss=3.0760955810546875
I0128 01:18:17.746126 140005313861376 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.854956865310669, loss=3.0970492362976074
I0128 01:18:51.297997 140005288683264 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.9176673889160156, loss=3.131516456604004
I0128 01:19:24.870055 140005313861376 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.1136536598205566, loss=3.1765966415405273
I0128 01:19:58.481641 140005288683264 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8207703828811646, loss=3.00948166847229
I0128 01:20:32.134647 140005313861376 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.851828694343567, loss=3.14103102684021
I0128 01:21:05.757859 140005288683264 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.89497709274292, loss=3.037517786026001
I0128 01:21:39.410870 140005313861376 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.0476410388946533, loss=3.091749668121338
I0128 01:22:13.047088 140005288683264 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.073881149291992, loss=3.1168179512023926
I0128 01:22:46.665435 140005313861376 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.9990185499191284, loss=3.0861587524414062
I0128 01:23:20.297551 140005288683264 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.8627099990844727, loss=3.1449472904205322
I0128 01:23:53.981545 140005313861376 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9544131755828857, loss=3.173311710357666
I0128 01:24:27.591284 140005288683264 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8855071067810059, loss=3.0475399494171143
I0128 01:25:01.222196 140005313861376 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8675496578216553, loss=3.1833975315093994
I0128 01:25:34.870638 140005288683264 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9088722467422485, loss=3.124680757522583
I0128 01:25:38.375842 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:25:44.678626 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:25:53.558483 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:25:56.078816 140169137129280 submission_runner.py:408] Time since start: 20637.43s, 	Step: 59112, 	{'train/accuracy': 0.7326809763908386, 'train/loss': 1.2575324773788452, 'validation/accuracy': 0.6616599559783936, 'validation/loss': 1.569987416267395, 'validation/num_examples': 50000, 'test/accuracy': 0.52920001745224, 'test/loss': 2.269146203994751, 'test/num_examples': 10000, 'score': 19924.543339967728, 'total_duration': 20637.42899608612, 'accumulated_submission_time': 19924.543339967728, 'accumulated_eval_time': 709.6492412090302, 'accumulated_logging_time': 1.3549623489379883}
I0128 01:25:56.107779 140004667934464 logging_writer.py:48] [59112] accumulated_eval_time=709.649241, accumulated_logging_time=1.354962, accumulated_submission_time=19924.543340, global_step=59112, preemption_count=0, score=19924.543340, test/accuracy=0.529200, test/loss=2.269146, test/num_examples=10000, total_duration=20637.428996, train/accuracy=0.732681, train/loss=1.257532, validation/accuracy=0.661660, validation/loss=1.569987, validation/num_examples=50000
I0128 01:26:25.967875 140004676327168 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.924347996711731, loss=3.109470844268799
I0128 01:26:59.604530 140004667934464 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.7655991315841675, loss=3.0284407138824463
I0128 01:27:33.230417 140004676327168 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9550269842147827, loss=3.097622871398926
I0128 01:28:06.890521 140004667934464 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.9063612222671509, loss=3.085561752319336
I0128 01:28:40.492128 140004676327168 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.0518877506256104, loss=3.1479604244232178
I0128 01:29:14.131713 140004667934464 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.969603419303894, loss=3.140294075012207
I0128 01:29:47.848006 140004676327168 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.8669830560684204, loss=3.087568521499634
I0128 01:30:21.437531 140004667934464 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.033254623413086, loss=3.1307601928710938
I0128 01:30:55.030374 140004676327168 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.0328433513641357, loss=3.1750290393829346
I0128 01:31:28.636622 140004667934464 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.1039066314697266, loss=3.156674861907959
I0128 01:32:02.231055 140004676327168 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.020432233810425, loss=3.1360278129577637
I0128 01:32:35.830349 140004667934464 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9418176412582397, loss=3.1026225090026855
I0128 01:33:09.460293 140004676327168 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9840409755706787, loss=3.067077875137329
I0128 01:33:43.104921 140004667934464 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.9973351955413818, loss=3.103640079498291
I0128 01:34:16.782702 140004676327168 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.8696264028549194, loss=3.0299646854400635
I0128 01:34:26.351503 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:34:32.578787 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:34:41.253430 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:34:43.765544 140169137129280 submission_runner.py:408] Time since start: 21165.12s, 	Step: 60630, 	{'train/accuracy': 0.7663623690605164, 'train/loss': 1.119907021522522, 'validation/accuracy': 0.6607800126075745, 'validation/loss': 1.5795252323150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.234541177749634, 'test/num_examples': 10000, 'score': 20434.725960969925, 'total_duration': 21165.11572289467, 'accumulated_submission_time': 20434.725960969925, 'accumulated_eval_time': 727.0632431507111, 'accumulated_logging_time': 1.3961291313171387}
I0128 01:34:43.793750 140005288683264 logging_writer.py:48] [60630] accumulated_eval_time=727.063243, accumulated_logging_time=1.396129, accumulated_submission_time=20434.725961, global_step=60630, preemption_count=0, score=20434.725961, test/accuracy=0.535700, test/loss=2.234541, test/num_examples=10000, total_duration=21165.115723, train/accuracy=0.766362, train/loss=1.119907, validation/accuracy=0.660780, validation/loss=1.579525, validation/num_examples=50000
I0128 01:35:07.678084 140005313861376 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.9515188932418823, loss=3.1063010692596436
I0128 01:35:41.248973 140005288683264 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.9776310920715332, loss=3.2224764823913574
I0128 01:36:14.912956 140005313861376 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8878697156906128, loss=3.096202850341797
I0128 01:36:48.540622 140005288683264 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.2047858238220215, loss=3.0765583515167236
I0128 01:37:22.161327 140005313861376 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.932384729385376, loss=3.0541727542877197
I0128 01:37:55.782291 140005288683264 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.345470905303955, loss=3.1448612213134766
I0128 01:38:29.407306 140005313861376 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9908554553985596, loss=3.050851345062256
I0128 01:39:03.065760 140005288683264 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.0786232948303223, loss=3.124103546142578
I0128 01:39:36.714485 140005313861376 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9358487129211426, loss=3.217146158218384
I0128 01:40:10.368550 140005288683264 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8737704753875732, loss=3.0471749305725098
I0128 01:40:43.981753 140005313861376 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.8432308435440063, loss=3.1236605644226074
I0128 01:41:17.614919 140005288683264 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.0997767448425293, loss=3.1539506912231445
I0128 01:41:51.253921 140005313861376 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.8946551084518433, loss=3.1505210399627686
I0128 01:42:24.949646 140005288683264 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.0914485454559326, loss=3.1279149055480957
I0128 01:42:58.522579 140005313861376 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.0467183589935303, loss=3.0982236862182617
I0128 01:43:14.109618 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:43:20.433719 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:43:29.222062 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:43:31.745530 140169137129280 submission_runner.py:408] Time since start: 21693.10s, 	Step: 62148, 	{'train/accuracy': 0.7565170526504517, 'train/loss': 1.1901875734329224, 'validation/accuracy': 0.6680799722671509, 'validation/loss': 1.5773533582687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.229222059249878, 'test/num_examples': 10000, 'score': 20944.98304605484, 'total_duration': 21693.09570145607, 'accumulated_submission_time': 20944.98304605484, 'accumulated_eval_time': 744.6991124153137, 'accumulated_logging_time': 1.4345617294311523}
I0128 01:43:31.775881 140004676327168 logging_writer.py:48] [62148] accumulated_eval_time=744.699112, accumulated_logging_time=1.434562, accumulated_submission_time=20944.983046, global_step=62148, preemption_count=0, score=20944.983046, test/accuracy=0.537000, test/loss=2.229222, test/num_examples=10000, total_duration=21693.095701, train/accuracy=0.756517, train/loss=1.190188, validation/accuracy=0.668080, validation/loss=1.577353, validation/num_examples=50000
I0128 01:43:49.592292 140005297075968 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1529769897460938, loss=3.1944682598114014
I0128 01:44:23.221801 140004676327168 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8610843420028687, loss=3.084815740585327
I0128 01:44:56.841496 140005297075968 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.036227226257324, loss=3.1162455081939697
I0128 01:45:30.472226 140004676327168 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9386252164840698, loss=3.117145538330078
I0128 01:46:04.122316 140005297075968 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.0928709506988525, loss=3.125701665878296
I0128 01:46:37.788964 140004676327168 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.1421167850494385, loss=3.186643600463867
I0128 01:47:11.415045 140005297075968 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.931342601776123, loss=3.108534336090088
I0128 01:47:45.065046 140004676327168 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.9449187517166138, loss=3.0747015476226807
I0128 01:48:18.733306 140005297075968 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.946489930152893, loss=3.0233654975891113
I0128 01:48:52.391143 140004676327168 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.1101980209350586, loss=3.18572735786438
I0128 01:49:25.952270 140005297075968 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9010982513427734, loss=3.062879800796509
I0128 01:49:59.570257 140004676327168 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9053339958190918, loss=3.077643632888794
I0128 01:50:33.173516 140005297075968 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.9701045751571655, loss=3.124810218811035
I0128 01:51:06.838430 140004676327168 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.0382721424102783, loss=3.064337730407715
I0128 01:51:40.493326 140005297075968 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.165484666824341, loss=3.1520090103149414
I0128 01:52:01.854823 140169137129280 spec.py:321] Evaluating on the training split.
I0128 01:52:08.196345 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 01:52:17.037250 140169137129280 spec.py:349] Evaluating on the test split.
I0128 01:52:19.552720 140169137129280 submission_runner.py:408] Time since start: 22220.90s, 	Step: 63665, 	{'train/accuracy': 0.7577726244926453, 'train/loss': 1.1620841026306152, 'validation/accuracy': 0.6750199794769287, 'validation/loss': 1.5228710174560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.200866222381592, 'test/num_examples': 10000, 'score': 21455.00278377533, 'total_duration': 22220.902873277664, 'accumulated_submission_time': 21455.00278377533, 'accumulated_eval_time': 762.3969528675079, 'accumulated_logging_time': 1.4752840995788574}
I0128 01:52:19.583926 140004667934464 logging_writer.py:48] [63665] accumulated_eval_time=762.396953, accumulated_logging_time=1.475284, accumulated_submission_time=21455.002784, global_step=63665, preemption_count=0, score=21455.002784, test/accuracy=0.543200, test/loss=2.200866, test/num_examples=10000, total_duration=22220.902873, train/accuracy=0.757773, train/loss=1.162084, validation/accuracy=0.675020, validation/loss=1.522871, validation/num_examples=50000
I0128 01:52:31.662312 140005288683264 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.830609679222107, loss=3.0120491981506348
I0128 01:53:05.147309 140004667934464 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.035451650619507, loss=3.085404872894287
I0128 01:53:38.743309 140005288683264 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.7555792331695557, loss=3.058654308319092
I0128 01:54:12.362975 140004667934464 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.0210516452789307, loss=3.139615535736084
I0128 01:54:46.059458 140005288683264 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9076346158981323, loss=3.0497114658355713
I0128 01:55:19.720351 140004667934464 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9653713703155518, loss=3.1200480461120605
I0128 01:55:53.357428 140005288683264 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.063095808029175, loss=3.111618757247925
I0128 01:56:26.994763 140004667934464 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.0240418910980225, loss=3.137460947036743
I0128 01:57:00.652554 140005288683264 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9576315879821777, loss=3.0993094444274902
I0128 01:57:34.283645 140004667934464 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.9879149198532104, loss=3.0537657737731934
I0128 01:58:07.924903 140005288683264 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.920460820198059, loss=3.0906283855438232
I0128 01:58:41.548724 140004667934464 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0858163833618164, loss=3.0829172134399414
I0128 01:59:15.182699 140005288683264 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.2203893661499023, loss=3.145857810974121
I0128 01:59:48.838811 140004667934464 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9391160011291504, loss=3.0289289951324463
I0128 02:00:22.483205 140005288683264 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.04473614692688, loss=3.0933361053466797
I0128 02:00:49.609540 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:00:55.852261 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:01:04.953069 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:01:07.563372 140169137129280 submission_runner.py:408] Time since start: 22748.91s, 	Step: 65182, 	{'train/accuracy': 0.7588488459587097, 'train/loss': 1.1486854553222656, 'validation/accuracy': 0.6837799549102783, 'validation/loss': 1.4862542152404785, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.1577084064483643, 'test/num_examples': 10000, 'score': 21964.96987080574, 'total_duration': 22748.91353034973, 'accumulated_submission_time': 21964.96987080574, 'accumulated_eval_time': 780.350729227066, 'accumulated_logging_time': 1.5167927742004395}
I0128 02:01:07.595342 140004667934464 logging_writer.py:48] [65182] accumulated_eval_time=780.350729, accumulated_logging_time=1.516793, accumulated_submission_time=21964.969871, global_step=65182, preemption_count=0, score=21964.969871, test/accuracy=0.553200, test/loss=2.157708, test/num_examples=10000, total_duration=22748.913530, train/accuracy=0.758849, train/loss=1.148685, validation/accuracy=0.683780, validation/loss=1.486254, validation/num_examples=50000
I0128 02:01:14.018674 140004676327168 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.947817087173462, loss=3.149691581726074
I0128 02:01:47.652174 140004667934464 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.105858564376831, loss=3.0976240634918213
I0128 02:02:21.306154 140004676327168 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9532945156097412, loss=3.0464909076690674
I0128 02:02:54.950294 140004667934464 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.107649326324463, loss=3.1105692386627197
I0128 02:03:28.571571 140004676327168 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.094364881515503, loss=3.1083593368530273
I0128 02:04:02.212840 140004667934464 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.0337283611297607, loss=3.0279712677001953
I0128 02:04:35.845359 140004676327168 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9996156692504883, loss=3.1128127574920654
I0128 02:05:09.449995 140004667934464 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.300166130065918, loss=3.1120970249176025
I0128 02:05:43.080229 140004676327168 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.037106513977051, loss=3.0795280933380127
I0128 02:06:16.690816 140004667934464 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.9990274906158447, loss=3.069519519805908
I0128 02:06:50.309651 140004676327168 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.029017210006714, loss=3.054462194442749
I0128 02:07:24.030718 140004667934464 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.1391172409057617, loss=3.123133897781372
I0128 02:07:57.706597 140004676327168 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.1767799854278564, loss=3.157824754714966
I0128 02:08:31.347954 140004667934464 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.1492433547973633, loss=3.1060829162597656
I0128 02:09:04.965786 140004676327168 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.980886697769165, loss=3.0633862018585205
I0128 02:09:37.742296 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:09:44.005187 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:09:52.811452 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:09:55.356651 140169137129280 submission_runner.py:408] Time since start: 23276.71s, 	Step: 66699, 	{'train/accuracy': 0.7502591013908386, 'train/loss': 1.2132987976074219, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.5442116260528564, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.189296245574951, 'test/num_examples': 10000, 'score': 22475.05553460121, 'total_duration': 23276.70662856102, 'accumulated_submission_time': 22475.05553460121, 'accumulated_eval_time': 797.9648485183716, 'accumulated_logging_time': 1.5609960556030273}
I0128 02:09:55.386375 140004667934464 logging_writer.py:48] [66699] accumulated_eval_time=797.964849, accumulated_logging_time=1.560996, accumulated_submission_time=22475.055535, global_step=66699, preemption_count=0, score=22475.055535, test/accuracy=0.554600, test/loss=2.189296, test/num_examples=10000, total_duration=23276.706629, train/accuracy=0.750259, train/loss=1.213299, validation/accuracy=0.675940, validation/loss=1.544212, validation/num_examples=50000
I0128 02:09:56.079565 140004676327168 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.2083580493927, loss=3.1615240573883057
I0128 02:10:29.669345 140004667934464 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.084648609161377, loss=3.0788474082946777
I0128 02:11:03.305546 140004676327168 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.108417272567749, loss=3.097886085510254
I0128 02:11:36.963880 140004667934464 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.2186808586120605, loss=3.1200931072235107
I0128 02:12:10.591079 140004676327168 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.970521330833435, loss=3.0993292331695557
I0128 02:12:44.196170 140004667934464 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.9980577230453491, loss=3.0982980728149414
I0128 02:13:17.882962 140004676327168 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.052359104156494, loss=3.092409610748291
I0128 02:13:51.533299 140004667934464 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.0320851802825928, loss=3.0620312690734863
I0128 02:14:25.187346 140004676327168 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.200866937637329, loss=3.0682520866394043
I0128 02:14:58.842749 140004667934464 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.2352042198181152, loss=3.1037986278533936
I0128 02:15:32.446752 140004676327168 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0107674598693848, loss=3.063333034515381
I0128 02:16:06.086997 140004667934464 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.968250036239624, loss=3.0617239475250244
I0128 02:16:39.704969 140004676327168 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.9328705072402954, loss=3.0077898502349854
I0128 02:17:13.352936 140004667934464 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.122673749923706, loss=3.13738751411438
I0128 02:17:46.981749 140004676327168 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.0333430767059326, loss=3.0926132202148438
I0128 02:18:20.618730 140004667934464 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0367789268493652, loss=3.066134214401245
I0128 02:18:25.473953 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:18:31.745544 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:18:40.591697 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:18:43.161900 140169137129280 submission_runner.py:408] Time since start: 23804.51s, 	Step: 68216, 	{'train/accuracy': 0.7473692297935486, 'train/loss': 1.19962477684021, 'validation/accuracy': 0.6743800044059753, 'validation/loss': 1.5287421941757202, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.196040153503418, 'test/num_examples': 10000, 'score': 22985.083032131195, 'total_duration': 23804.51207590103, 'accumulated_submission_time': 22985.083032131195, 'accumulated_eval_time': 815.652755022049, 'accumulated_logging_time': 1.6021060943603516}
I0128 02:18:43.191629 140004667934464 logging_writer.py:48] [68216] accumulated_eval_time=815.652755, accumulated_logging_time=1.602106, accumulated_submission_time=22985.083032, global_step=68216, preemption_count=0, score=22985.083032, test/accuracy=0.545200, test/loss=2.196040, test/num_examples=10000, total_duration=23804.512076, train/accuracy=0.747369, train/loss=1.199625, validation/accuracy=0.674380, validation/loss=1.528742, validation/num_examples=50000
I0128 02:19:11.737540 140005297075968 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.0825343132019043, loss=3.1186015605926514
I0128 02:19:45.438618 140004667934464 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.9759379625320435, loss=3.115993022918701
I0128 02:20:19.089934 140005297075968 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.3276476860046387, loss=3.1850242614746094
I0128 02:20:52.727416 140004667934464 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.1079516410827637, loss=3.038940668106079
I0128 02:21:26.350002 140005297075968 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.2602148056030273, loss=3.068429470062256
I0128 02:21:59.946800 140004667934464 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0956881046295166, loss=3.0724406242370605
I0128 02:22:33.563754 140005297075968 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.063652992248535, loss=3.1348040103912354
I0128 02:23:07.156028 140004667934464 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.0385921001434326, loss=3.065495729446411
I0128 02:23:40.797692 140005297075968 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.1245834827423096, loss=3.098181962966919
I0128 02:24:14.408481 140004667934464 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0182745456695557, loss=3.088599681854248
I0128 02:24:48.045234 140005297075968 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.0288712978363037, loss=3.0758485794067383
I0128 02:25:21.626482 140004667934464 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.0081067085266113, loss=3.074610471725464
I0128 02:25:55.295891 140005297075968 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.1554956436157227, loss=3.059483766555786
I0128 02:26:28.940979 140004667934464 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.9753015041351318, loss=2.972522258758545
I0128 02:27:02.582737 140005297075968 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9633797407150269, loss=3.0243279933929443
I0128 02:27:13.497561 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:27:19.842149 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:27:28.644123 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:27:31.165403 140169137129280 submission_runner.py:408] Time since start: 24332.52s, 	Step: 69734, 	{'train/accuracy': 0.7683553695678711, 'train/loss': 1.1409224271774292, 'validation/accuracy': 0.6607999801635742, 'validation/loss': 1.5949475765228271, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.243528127670288, 'test/num_examples': 10000, 'score': 23495.33017706871, 'total_duration': 24332.515582323074, 'accumulated_submission_time': 23495.33017706871, 'accumulated_eval_time': 833.3205726146698, 'accumulated_logging_time': 1.642003059387207}
I0128 02:27:31.197324 140004667934464 logging_writer.py:48] [69734] accumulated_eval_time=833.320573, accumulated_logging_time=1.642003, accumulated_submission_time=23495.330177, global_step=69734, preemption_count=0, score=23495.330177, test/accuracy=0.541800, test/loss=2.243528, test/num_examples=10000, total_duration=24332.515582, train/accuracy=0.768355, train/loss=1.140922, validation/accuracy=0.660800, validation/loss=1.594948, validation/num_examples=50000
I0128 02:27:53.668403 140005313861376 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.160628318786621, loss=3.0622284412384033
I0128 02:28:27.261226 140004667934464 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.131187915802002, loss=3.08536958694458
I0128 02:29:00.894738 140005313861376 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.9398049116134644, loss=3.0473222732543945
I0128 02:29:34.538831 140004667934464 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.052114725112915, loss=3.1511359214782715
I0128 02:30:08.162326 140005313861376 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.231140375137329, loss=3.092850923538208
I0128 02:30:41.809294 140004667934464 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.038975238800049, loss=3.0280494689941406
I0128 02:31:15.463643 140005313861376 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.0258145332336426, loss=3.0076615810394287
I0128 02:31:49.200067 140004667934464 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.0686707496643066, loss=3.136625051498413
I0128 02:32:22.778286 140005313861376 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0879225730895996, loss=3.0721747875213623
I0128 02:32:56.438619 140004667934464 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9454399347305298, loss=3.0781288146972656
I0128 02:33:30.065579 140005313861376 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9976396560668945, loss=3.0668559074401855
I0128 02:34:03.720435 140004667934464 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.113023042678833, loss=3.0669946670532227
I0128 02:34:37.339848 140005313861376 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.9606226682662964, loss=3.022143840789795
I0128 02:35:10.974301 140004667934464 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.019253730773926, loss=3.0311379432678223
I0128 02:35:44.567780 140005313861376 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.312808036804199, loss=3.1976635456085205
I0128 02:36:01.204366 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:36:07.511584 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:36:16.331926 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:36:18.851941 140169137129280 submission_runner.py:408] Time since start: 24860.20s, 	Step: 71251, 	{'train/accuracy': 0.7674585580825806, 'train/loss': 1.128574013710022, 'validation/accuracy': 0.6775799989700317, 'validation/loss': 1.5199021100997925, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.167146682739258, 'test/num_examples': 10000, 'score': 24005.278024673462, 'total_duration': 24860.202106952667, 'accumulated_submission_time': 24005.278024673462, 'accumulated_eval_time': 850.9681005477905, 'accumulated_logging_time': 1.6843080520629883}
I0128 02:36:18.882740 140005288683264 logging_writer.py:48] [71251] accumulated_eval_time=850.968101, accumulated_logging_time=1.684308, accumulated_submission_time=24005.278025, global_step=71251, preemption_count=0, score=24005.278025, test/accuracy=0.550800, test/loss=2.167147, test/num_examples=10000, total_duration=24860.202107, train/accuracy=0.767459, train/loss=1.128574, validation/accuracy=0.677580, validation/loss=1.519902, validation/num_examples=50000
I0128 02:36:35.647503 140005297075968 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.206613779067993, loss=3.0933713912963867
I0128 02:37:09.252907 140005288683264 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.087557554244995, loss=3.069000720977783
I0128 02:37:42.863487 140005297075968 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.166879653930664, loss=3.036998987197876
I0128 02:38:16.537701 140005288683264 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.9651325941085815, loss=3.099365472793579
I0128 02:38:50.114186 140005297075968 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.140810966491699, loss=3.1331889629364014
I0128 02:39:23.727505 140005288683264 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.1757171154022217, loss=3.0689520835876465
I0128 02:39:57.383941 140005297075968 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.1207070350646973, loss=3.1141197681427
I0128 02:40:31.018699 140005288683264 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.9563703536987305, loss=2.9955596923828125
I0128 02:41:04.680385 140005297075968 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.2510719299316406, loss=3.0921449661254883
I0128 02:41:38.337124 140005288683264 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.0658955574035645, loss=3.112696886062622
I0128 02:42:11.981063 140005297075968 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.1796653270721436, loss=3.101417064666748
I0128 02:42:45.597225 140005288683264 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.2058932781219482, loss=3.05082631111145
I0128 02:43:19.256139 140005297075968 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1141750812530518, loss=3.025834798812866
I0128 02:43:52.908711 140005288683264 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.182680368423462, loss=3.1439034938812256
I0128 02:44:26.579544 140005297075968 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.1939501762390137, loss=3.1695785522460938
I0128 02:44:48.920528 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:44:55.252629 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:45:04.156578 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:45:06.706431 140169137129280 submission_runner.py:408] Time since start: 25388.06s, 	Step: 72768, 	{'train/accuracy': 0.7587292790412903, 'train/loss': 1.159325361251831, 'validation/accuracy': 0.6759200096130371, 'validation/loss': 1.519149661064148, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.178276538848877, 'test/num_examples': 10000, 'score': 24515.25668501854, 'total_duration': 25388.056608200073, 'accumulated_submission_time': 24515.25668501854, 'accumulated_eval_time': 868.7539627552032, 'accumulated_logging_time': 1.7253928184509277}
I0128 02:45:06.736501 140005305468672 logging_writer.py:48] [72768] accumulated_eval_time=868.753963, accumulated_logging_time=1.725393, accumulated_submission_time=24515.256685, global_step=72768, preemption_count=0, score=24515.256685, test/accuracy=0.552100, test/loss=2.178277, test/num_examples=10000, total_duration=25388.056608, train/accuracy=0.758729, train/loss=1.159325, validation/accuracy=0.675920, validation/loss=1.519150, validation/num_examples=50000
I0128 02:45:17.819176 140005313861376 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.101365327835083, loss=3.0409932136535645
I0128 02:45:51.458113 140005305468672 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.227137327194214, loss=3.0875673294067383
I0128 02:46:25.101781 140005313861376 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.185345411300659, loss=3.012861728668213
I0128 02:46:58.739435 140005305468672 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.2346858978271484, loss=3.0251808166503906
I0128 02:47:32.384956 140005313861376 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.09735369682312, loss=3.0627236366271973
I0128 02:48:05.987085 140005305468672 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1587955951690674, loss=3.0413012504577637
I0128 02:48:39.627995 140005313861376 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.04947829246521, loss=2.9397196769714355
I0128 02:49:13.261024 140005305468672 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9872024059295654, loss=2.989483594894409
I0128 02:49:46.883748 140005313861376 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.046431064605713, loss=3.006402015686035
I0128 02:50:20.618227 140005305468672 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.1440207958221436, loss=3.1292927265167236
I0128 02:50:54.201227 140005313861376 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.248516798019409, loss=3.0069422721862793
I0128 02:51:27.786989 140005305468672 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.128453016281128, loss=3.0777945518493652
I0128 02:52:01.422770 140005313861376 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0876030921936035, loss=3.0033650398254395
I0128 02:52:35.029937 140005305468672 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0679218769073486, loss=3.0226404666900635
I0128 02:53:08.649771 140005313861376 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.354210615158081, loss=3.0622472763061523
I0128 02:53:36.716005 140169137129280 spec.py:321] Evaluating on the training split.
I0128 02:53:43.250461 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 02:53:52.117461 140169137129280 spec.py:349] Evaluating on the test split.
I0128 02:53:54.634639 140169137129280 submission_runner.py:408] Time since start: 25915.98s, 	Step: 74285, 	{'train/accuracy': 0.7687141299247742, 'train/loss': 1.109268307685852, 'validation/accuracy': 0.6859599947929382, 'validation/loss': 1.4653456211090088, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.108211040496826, 'test/num_examples': 10000, 'score': 25025.17741537094, 'total_duration': 25915.98461484909, 'accumulated_submission_time': 25025.17741537094, 'accumulated_eval_time': 886.672360420227, 'accumulated_logging_time': 1.7654447555541992}
I0128 02:53:54.665629 140005288683264 logging_writer.py:48] [74285] accumulated_eval_time=886.672360, accumulated_logging_time=1.765445, accumulated_submission_time=25025.177415, global_step=74285, preemption_count=0, score=25025.177415, test/accuracy=0.563700, test/loss=2.108211, test/num_examples=10000, total_duration=25915.984615, train/accuracy=0.768714, train/loss=1.109268, validation/accuracy=0.685960, validation/loss=1.465346, validation/num_examples=50000
I0128 02:54:00.096623 140005297075968 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.1905431747436523, loss=3.10300350189209
I0128 02:54:33.697075 140005288683264 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.118692398071289, loss=2.9947097301483154
I0128 02:55:07.289903 140005297075968 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.2259132862091064, loss=3.0808534622192383
I0128 02:55:40.873287 140005288683264 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.206308364868164, loss=3.0543437004089355
I0128 02:56:14.459928 140005297075968 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.0720901489257812, loss=3.010749101638794
I0128 02:56:48.111006 140005288683264 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0921239852905273, loss=3.0146942138671875
I0128 02:57:21.767581 140005297075968 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.317135810852051, loss=3.095020294189453
I0128 02:57:55.378898 140005288683264 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.180729627609253, loss=2.9906957149505615
I0128 02:58:29.002921 140005297075968 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.2625269889831543, loss=3.0451865196228027
I0128 02:59:02.650545 140005288683264 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.0510342121124268, loss=3.0118179321289062
I0128 02:59:36.280130 140005297075968 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.2283709049224854, loss=3.060683250427246
I0128 03:00:09.916241 140005288683264 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.9613388776779175, loss=3.015286445617676
I0128 03:00:43.526368 140005297075968 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.1009979248046875, loss=3.077178478240967
I0128 03:01:17.163172 140005288683264 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.37831449508667, loss=3.0268101692199707
I0128 03:01:50.815349 140005297075968 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.253974676132202, loss=3.11365008354187
I0128 03:02:24.446477 140005288683264 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.04754638671875, loss=3.018458843231201
I0128 03:02:24.932470 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:02:31.415685 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:02:40.502993 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:02:43.034590 140169137129280 submission_runner.py:408] Time since start: 26444.38s, 	Step: 75803, 	{'train/accuracy': 0.7549425959587097, 'train/loss': 1.1974339485168457, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5412228107452393, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.1966655254364014, 'test/num_examples': 10000, 'score': 25535.3845744133, 'total_duration': 26444.38476872444, 'accumulated_submission_time': 25535.3845744133, 'accumulated_eval_time': 904.7744419574738, 'accumulated_logging_time': 1.8068821430206299}
I0128 03:02:43.065124 140004659541760 logging_writer.py:48] [75803] accumulated_eval_time=904.774442, accumulated_logging_time=1.806882, accumulated_submission_time=25535.384574, global_step=75803, preemption_count=0, score=25535.384574, test/accuracy=0.557600, test/loss=2.196666, test/num_examples=10000, total_duration=26444.384769, train/accuracy=0.754943, train/loss=1.197434, validation/accuracy=0.680160, validation/loss=1.541223, validation/num_examples=50000
I0128 03:03:16.017441 140004667934464 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.0729730129241943, loss=3.0312936305999756
I0128 03:03:49.627524 140004659541760 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.0586702823638916, loss=3.001896381378174
I0128 03:04:23.285235 140004667934464 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.3614115715026855, loss=3.1314687728881836
I0128 03:04:56.894725 140004659541760 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.083956003189087, loss=3.0565974712371826
I0128 03:05:30.521125 140004667934464 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1027257442474365, loss=3.0564377307891846
I0128 03:06:04.144158 140004659541760 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.14199161529541, loss=2.969853401184082
I0128 03:06:37.801566 140004667934464 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.219358205795288, loss=3.121568202972412
I0128 03:07:11.446139 140004659541760 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1456735134124756, loss=3.053420305252075
I0128 03:07:45.066722 140004667934464 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.2965328693389893, loss=3.034982204437256
I0128 03:08:18.698950 140004659541760 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.111605167388916, loss=3.024712324142456
I0128 03:08:52.351829 140004667934464 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.176804780960083, loss=3.105797290802002
I0128 03:09:26.050992 140004659541760 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.060462713241577, loss=3.052037477493286
I0128 03:09:59.638369 140004667934464 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.414172649383545, loss=3.0113823413848877
I0128 03:10:33.243976 140004659541760 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.2317376136779785, loss=3.019711971282959
I0128 03:11:06.911263 140004667934464 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.273541212081909, loss=3.092348098754883
I0128 03:11:13.107424 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:11:19.420505 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:11:28.397629 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:11:30.913981 140169137129280 submission_runner.py:408] Time since start: 26972.26s, 	Step: 77320, 	{'train/accuracy': 0.7669403553009033, 'train/loss': 1.1282862424850464, 'validation/accuracy': 0.6887999773025513, 'validation/loss': 1.4690423011779785, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 2.1257145404815674, 'test/num_examples': 10000, 'score': 26045.367203950882, 'total_duration': 26972.264142274857, 'accumulated_submission_time': 26045.367203950882, 'accumulated_eval_time': 922.5809574127197, 'accumulated_logging_time': 1.8478496074676514}
I0128 03:11:30.946870 140004667934464 logging_writer.py:48] [77320] accumulated_eval_time=922.580957, accumulated_logging_time=1.847850, accumulated_submission_time=26045.367204, global_step=77320, preemption_count=0, score=26045.367204, test/accuracy=0.562800, test/loss=2.125715, test/num_examples=10000, total_duration=26972.264142, train/accuracy=0.766940, train/loss=1.128286, validation/accuracy=0.688800, validation/loss=1.469042, validation/num_examples=50000
I0128 03:11:58.141712 140005305468672 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1614296436309814, loss=3.0206563472747803
I0128 03:12:31.772647 140004667934464 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.14615797996521, loss=3.103864908218384
I0128 03:13:05.435254 140005305468672 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.2667133808135986, loss=3.015664577484131
I0128 03:13:39.043896 140004667934464 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.231473207473755, loss=3.0848677158355713
I0128 03:14:12.699723 140005305468672 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.2142984867095947, loss=3.070554256439209
I0128 03:14:46.348014 140004667934464 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.185178518295288, loss=3.0620789527893066
I0128 03:15:20.054668 140005305468672 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.2339510917663574, loss=3.0222034454345703
I0128 03:15:53.695994 140004667934464 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.321248769760132, loss=3.013853073120117
I0128 03:16:27.330176 140005305468672 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.0701396465301514, loss=2.9535441398620605
I0128 03:17:00.942642 140004667934464 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1629397869110107, loss=3.0385210514068604
I0128 03:17:34.571666 140005305468672 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.172971487045288, loss=3.008636951446533
I0128 03:18:08.218857 140004667934464 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.2216408252716064, loss=3.055760622024536
I0128 03:18:41.855710 140005305468672 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.1277873516082764, loss=2.9931039810180664
I0128 03:19:15.484942 140004667934464 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.2258718013763428, loss=3.0782394409179688
I0128 03:19:49.131098 140005305468672 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.235590696334839, loss=3.0676982402801514
I0128 03:20:01.050888 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:20:07.401906 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:20:16.533449 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:20:19.060136 140169137129280 submission_runner.py:408] Time since start: 27500.41s, 	Step: 78837, 	{'train/accuracy': 0.7891621589660645, 'train/loss': 1.0352303981781006, 'validation/accuracy': 0.684719979763031, 'validation/loss': 1.4883527755737305, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.133809804916382, 'test/num_examples': 10000, 'score': 26555.412185668945, 'total_duration': 27500.41028022766, 'accumulated_submission_time': 26555.412185668945, 'accumulated_eval_time': 940.5901341438293, 'accumulated_logging_time': 1.890838623046875}
I0128 03:20:19.106682 140005288683264 logging_writer.py:48] [78837] accumulated_eval_time=940.590134, accumulated_logging_time=1.890839, accumulated_submission_time=26555.412186, global_step=78837, preemption_count=0, score=26555.412186, test/accuracy=0.558600, test/loss=2.133810, test/num_examples=10000, total_duration=27500.410280, train/accuracy=0.789162, train/loss=1.035230, validation/accuracy=0.684720, validation/loss=1.488353, validation/num_examples=50000
I0128 03:20:40.551560 140005297075968 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.2173726558685303, loss=3.0038418769836426
I0128 03:21:14.108009 140005288683264 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.1613450050354004, loss=3.0896692276000977
I0128 03:21:47.753658 140005297075968 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.354968547821045, loss=3.022834539413452
I0128 03:22:21.320556 140005288683264 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.2981555461883545, loss=3.075223445892334
I0128 03:22:54.929714 140005297075968 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.089068651199341, loss=3.0511221885681152
I0128 03:23:28.564260 140005288683264 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.129429817199707, loss=3.0726280212402344
I0128 03:24:02.213752 140005297075968 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.2468221187591553, loss=3.11899471282959
I0128 03:24:35.861001 140005288683264 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.2911224365234375, loss=3.036843776702881
I0128 03:25:09.505655 140005297075968 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.0609021186828613, loss=2.9577674865722656
I0128 03:25:43.142812 140005288683264 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.229457139968872, loss=3.079824447631836
I0128 03:26:16.782370 140005297075968 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1226449012756348, loss=2.993103265762329
I0128 03:26:50.423098 140005288683264 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.252755641937256, loss=3.054349899291992
I0128 03:27:24.057287 140005297075968 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.1898205280303955, loss=3.047419548034668
I0128 03:27:57.764030 140005288683264 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.229372024536133, loss=3.0612549781799316
I0128 03:28:31.419514 140005297075968 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.2320191860198975, loss=3.021718740463257
I0128 03:28:49.065213 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:28:55.427077 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:29:04.529552 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:29:07.107316 140169137129280 submission_runner.py:408] Time since start: 28028.46s, 	Step: 80354, 	{'train/accuracy': 0.7771245241165161, 'train/loss': 1.0599044561386108, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.464746356010437, 'validation/num_examples': 50000, 'test/accuracy': 0.5583000183105469, 'test/loss': 2.1267716884613037, 'test/num_examples': 10000, 'score': 27065.312031030655, 'total_duration': 28028.457494974136, 'accumulated_submission_time': 27065.312031030655, 'accumulated_eval_time': 958.6321983337402, 'accumulated_logging_time': 1.9472503662109375}
I0128 03:29:07.142153 140004667934464 logging_writer.py:48] [80354] accumulated_eval_time=958.632198, accumulated_logging_time=1.947250, accumulated_submission_time=27065.312031, global_step=80354, preemption_count=0, score=27065.312031, test/accuracy=0.558300, test/loss=2.126772, test/num_examples=10000, total_duration=28028.457495, train/accuracy=0.777125, train/loss=1.059904, validation/accuracy=0.686740, validation/loss=1.464746, validation/num_examples=50000
I0128 03:29:22.978814 140004676327168 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.2496581077575684, loss=3.0584089756011963
I0128 03:29:56.576416 140004667934464 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.29889178276062, loss=3.065727710723877
I0128 03:30:30.220905 140004676327168 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.2826321125030518, loss=3.0069780349731445
I0128 03:31:03.872464 140004667934464 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.1796844005584717, loss=3.0519700050354004
I0128 03:31:37.488401 140004676327168 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.3184077739715576, loss=3.0901947021484375
I0128 03:32:11.152141 140004667934464 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.313803195953369, loss=3.029411554336548
I0128 03:32:44.767557 140004676327168 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.2746622562408447, loss=2.9806413650512695
I0128 03:33:18.419704 140004667934464 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.20829701423645, loss=3.031730890274048
I0128 03:33:52.130820 140004676327168 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.1820878982543945, loss=2.991785764694214
I0128 03:34:25.760554 140004667934464 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.173346519470215, loss=3.042069435119629
I0128 03:34:59.395493 140004676327168 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.3180205821990967, loss=2.9719974994659424
I0128 03:35:33.065346 140004667934464 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.274946689605713, loss=3.0072009563446045
I0128 03:36:06.983373 140004676327168 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.2023801803588867, loss=2.9971041679382324
I0128 03:36:40.614713 140004667934464 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.2957868576049805, loss=2.962510108947754
I0128 03:37:14.227267 140004676327168 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.31541109085083, loss=3.1026611328125
I0128 03:37:37.269651 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:37:44.233370 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:37:53.147344 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:37:55.675261 140169137129280 submission_runner.py:408] Time since start: 28557.03s, 	Step: 81870, 	{'train/accuracy': 0.7718032598495483, 'train/loss': 1.0625839233398438, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.4451771974563599, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.112879991531372, 'test/num_examples': 10000, 'score': 27575.380538225174, 'total_duration': 28557.02544283867, 'accumulated_submission_time': 27575.380538225174, 'accumulated_eval_time': 977.0377764701843, 'accumulated_logging_time': 1.9921598434448242}
I0128 03:37:55.710640 140004676327168 logging_writer.py:48] [81870] accumulated_eval_time=977.037776, accumulated_logging_time=1.992160, accumulated_submission_time=27575.380538, global_step=81870, preemption_count=0, score=27575.380538, test/accuracy=0.558400, test/loss=2.112880, test/num_examples=10000, total_duration=28557.025443, train/accuracy=0.771803, train/loss=1.062584, validation/accuracy=0.684520, validation/loss=1.445177, validation/num_examples=50000
I0128 03:38:06.148476 140005305468672 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1609714031219482, loss=2.975588083267212
I0128 03:38:39.760185 140004676327168 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.2453198432922363, loss=3.010207176208496
I0128 03:39:13.397933 140005305468672 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.2589669227600098, loss=2.978069305419922
I0128 03:39:47.043509 140004676327168 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.248197078704834, loss=3.026383638381958
I0128 03:40:20.699575 140005305468672 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.2116050720214844, loss=3.050109386444092
I0128 03:40:54.256685 140004676327168 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.462693214416504, loss=3.114241361618042
I0128 03:41:27.798534 140005305468672 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2292473316192627, loss=3.000492811203003
I0128 03:42:01.368058 140004676327168 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.238647222518921, loss=3.1254043579101562
I0128 03:42:34.948262 140005305468672 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.2421512603759766, loss=3.051602840423584
I0128 03:43:08.557587 140004676327168 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.2654242515563965, loss=3.0298404693603516
I0128 03:43:42.198041 140005305468672 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.1312429904937744, loss=2.9451475143432617
I0128 03:44:15.824918 140004676327168 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.3769659996032715, loss=2.967750072479248
I0128 03:44:49.433840 140005305468672 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.3794379234313965, loss=2.9642298221588135
I0128 03:45:23.121390 140004676327168 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.342318534851074, loss=3.0601658821105957
I0128 03:45:56.744531 140005305468672 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.3920793533325195, loss=3.0002877712249756
I0128 03:46:25.899002 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:46:32.169050 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:46:40.995270 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:46:43.627952 140169137129280 submission_runner.py:408] Time since start: 29084.98s, 	Step: 83388, 	{'train/accuracy': 0.7626355290412903, 'train/loss': 1.1467963457107544, 'validation/accuracy': 0.6807599663734436, 'validation/loss': 1.5086842775344849, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.1497886180877686, 'test/num_examples': 10000, 'score': 28085.509213924408, 'total_duration': 29084.97811937332, 'accumulated_submission_time': 28085.509213924408, 'accumulated_eval_time': 994.7666764259338, 'accumulated_logging_time': 2.0380008220672607}
I0128 03:46:43.662600 140004676327168 logging_writer.py:48] [83388] accumulated_eval_time=994.766676, accumulated_logging_time=2.038001, accumulated_submission_time=28085.509214, global_step=83388, preemption_count=0, score=28085.509214, test/accuracy=0.559300, test/loss=2.149789, test/num_examples=10000, total_duration=29084.978119, train/accuracy=0.762636, train/loss=1.146796, validation/accuracy=0.680760, validation/loss=1.508684, validation/num_examples=50000
I0128 03:46:48.028089 140005297075968 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.307835578918457, loss=3.0401077270507812
I0128 03:47:21.568642 140004676327168 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.3104074001312256, loss=3.030932903289795
I0128 03:47:55.122240 140005297075968 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.32832932472229, loss=3.0244991779327393
I0128 03:48:28.732113 140004676327168 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.1950490474700928, loss=3.001403570175171
I0128 03:49:02.383823 140005297075968 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.377926826477051, loss=3.0671706199645996
I0128 03:49:36.013368 140004676327168 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.368891954421997, loss=3.0357043743133545
I0128 03:50:09.671605 140005297075968 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.263746976852417, loss=2.995558023452759
I0128 03:50:43.305050 140004676327168 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.139643669128418, loss=2.9952099323272705
I0128 03:51:16.926571 140005297075968 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.3012237548828125, loss=3.053680419921875
I0128 03:51:50.565622 140004676327168 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.35129451751709, loss=2.9911532402038574
I0128 03:52:24.208866 140005297075968 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.3279812335968018, loss=3.0345053672790527
I0128 03:52:57.985054 140004676327168 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.2430708408355713, loss=3.035963535308838
I0128 03:53:31.637584 140005297075968 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.5482778549194336, loss=3.036264181137085
I0128 03:54:05.270814 140004676327168 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.310397148132324, loss=3.0403432846069336
I0128 03:54:38.916301 140005297075968 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.53373646736145, loss=3.0977976322174072
I0128 03:55:12.527999 140004676327168 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.383605718612671, loss=3.0710110664367676
I0128 03:55:13.685661 140169137129280 spec.py:321] Evaluating on the training split.
I0128 03:55:19.927156 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 03:55:28.736155 140169137129280 spec.py:349] Evaluating on the test split.
I0128 03:55:31.224293 140169137129280 submission_runner.py:408] Time since start: 29612.57s, 	Step: 84905, 	{'train/accuracy': 0.7693120241165161, 'train/loss': 1.1221765279769897, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.4688431024551392, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.1390833854675293, 'test/num_examples': 10000, 'score': 28595.469446659088, 'total_duration': 29612.574474573135, 'accumulated_submission_time': 28595.469446659088, 'accumulated_eval_time': 1012.3052713871002, 'accumulated_logging_time': 2.0864999294281006}
I0128 03:55:31.257433 140004676327168 logging_writer.py:48] [84905] accumulated_eval_time=1012.305271, accumulated_logging_time=2.086500, accumulated_submission_time=28595.469447, global_step=84905, preemption_count=0, score=28595.469447, test/accuracy=0.559800, test/loss=2.139083, test/num_examples=10000, total_duration=29612.574475, train/accuracy=0.769312, train/loss=1.122177, validation/accuracy=0.690100, validation/loss=1.468843, validation/num_examples=50000
I0128 03:56:03.445038 140005288683264 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.360652446746826, loss=3.023331642150879
I0128 03:56:36.989299 140004676327168 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.2537641525268555, loss=2.9926016330718994
I0128 03:57:10.555702 140005288683264 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.261443853378296, loss=3.000732898712158
I0128 03:57:44.180743 140004676327168 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.1725194454193115, loss=2.971109390258789
I0128 03:58:17.839496 140005288683264 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.4117014408111572, loss=3.035881519317627
I0128 03:58:51.522957 140004676327168 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.4182991981506348, loss=3.0255250930786133
I0128 03:59:25.113145 140005288683264 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.198272228240967, loss=2.9569380283355713
I0128 03:59:58.725193 140004676327168 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.3617007732391357, loss=3.0405187606811523
I0128 04:00:32.357970 140005288683264 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.2576403617858887, loss=3.0243794918060303
I0128 04:01:05.970122 140004676327168 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.213552474975586, loss=2.9806599617004395
I0128 04:01:39.627494 140005288683264 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.129786252975464, loss=2.9754252433776855
I0128 04:02:13.275873 140004676327168 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.5318727493286133, loss=2.9923129081726074
I0128 04:02:46.880891 140005288683264 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.211214303970337, loss=3.01557993888855
I0128 04:03:20.514662 140004676327168 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.9102001190185547, loss=3.0414257049560547
I0128 04:03:54.160075 140005288683264 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.478347063064575, loss=3.01896595954895
I0128 04:04:01.385809 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:04:07.673669 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:04:16.466934 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:04:18.960523 140169137129280 submission_runner.py:408] Time since start: 30140.31s, 	Step: 86423, 	{'train/accuracy': 0.78125, 'train/loss': 1.0466645956039429, 'validation/accuracy': 0.6961999535560608, 'validation/loss': 1.407152771949768, 'validation/num_examples': 50000, 'test/accuracy': 0.5702000260353088, 'test/loss': 2.054215669631958, 'test/num_examples': 10000, 'score': 29105.538024902344, 'total_duration': 30140.310687065125, 'accumulated_submission_time': 29105.538024902344, 'accumulated_eval_time': 1029.8799359798431, 'accumulated_logging_time': 2.1302878856658936}
I0128 04:04:18.992761 140004667934464 logging_writer.py:48] [86423] accumulated_eval_time=1029.879936, accumulated_logging_time=2.130288, accumulated_submission_time=29105.538025, global_step=86423, preemption_count=0, score=29105.538025, test/accuracy=0.570200, test/loss=2.054216, test/num_examples=10000, total_duration=30140.310687, train/accuracy=0.781250, train/loss=1.046665, validation/accuracy=0.696200, validation/loss=1.407153, validation/num_examples=50000
I0128 04:04:45.199863 140004676327168 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.5944488048553467, loss=3.088338851928711
I0128 04:05:18.845873 140004667934464 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.37032151222229, loss=3.031141757965088
I0128 04:05:52.461948 140004676327168 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.4116568565368652, loss=3.011857271194458
I0128 04:06:26.085795 140004667934464 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.5055887699127197, loss=3.0701048374176025
I0128 04:06:59.726716 140004676327168 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2749063968658447, loss=2.9948227405548096
I0128 04:07:33.372902 140004667934464 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.1426455974578857, loss=3.044661521911621
I0128 04:08:07.024271 140004676327168 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.1598098278045654, loss=2.9735593795776367
I0128 04:08:40.647671 140004667934464 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.276780605316162, loss=2.9725794792175293
I0128 04:09:14.308001 140004676327168 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.333353042602539, loss=3.081146717071533
I0128 04:09:47.940087 140004667934464 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.532741069793701, loss=2.982638120651245
I0128 04:10:21.577943 140004676327168 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.3973631858825684, loss=3.035863161087036
I0128 04:10:55.216207 140004667934464 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.4143850803375244, loss=3.016289710998535
I0128 04:11:28.945905 140004676327168 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.330179452896118, loss=2.9548773765563965
I0128 04:12:02.585509 140004667934464 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.313629627227783, loss=2.9679207801818848
I0128 04:12:36.263999 140004676327168 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.360133171081543, loss=3.0593883991241455
I0128 04:12:49.196070 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:12:55.471149 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:13:04.317013 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:13:06.824590 140169137129280 submission_runner.py:408] Time since start: 30668.17s, 	Step: 87940, 	{'train/accuracy': 0.7981704473495483, 'train/loss': 1.0359044075012207, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.4844554662704468, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.1412951946258545, 'test/num_examples': 10000, 'score': 29615.679981470108, 'total_duration': 30668.1747674942, 'accumulated_submission_time': 29615.679981470108, 'accumulated_eval_time': 1047.508416891098, 'accumulated_logging_time': 2.1748242378234863}
I0128 04:13:06.859139 140005313861376 logging_writer.py:48] [87940] accumulated_eval_time=1047.508417, accumulated_logging_time=2.174824, accumulated_submission_time=29615.679981, global_step=87940, preemption_count=0, score=29615.679981, test/accuracy=0.564800, test/loss=2.141295, test/num_examples=10000, total_duration=30668.174767, train/accuracy=0.798170, train/loss=1.035904, validation/accuracy=0.692420, validation/loss=1.484455, validation/num_examples=50000
I0128 04:13:27.352251 140005322254080 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.3380680084228516, loss=2.9910647869110107
I0128 04:14:00.960875 140005313861376 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.2644219398498535, loss=2.9253110885620117
I0128 04:14:34.612668 140005322254080 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.4477388858795166, loss=3.0345776081085205
I0128 04:15:08.253493 140005313861376 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.5174431800842285, loss=3.022156000137329
I0128 04:15:41.904043 140005322254080 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.3039166927337646, loss=2.9557998180389404
I0128 04:16:15.535414 140005313861376 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.3800270557403564, loss=2.9939756393432617
I0128 04:16:49.177253 140005322254080 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.229846239089966, loss=2.9502005577087402
I0128 04:17:22.863203 140005313861376 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.286109685897827, loss=3.0203044414520264
I0128 04:17:56.474133 140005322254080 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.268312692642212, loss=2.8489935398101807
I0128 04:18:30.135039 140005313861376 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.237459659576416, loss=2.9473929405212402
I0128 04:19:03.775048 140005322254080 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.437134027481079, loss=3.0379226207733154
I0128 04:19:37.399740 140005313861376 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.4632699489593506, loss=2.9747250080108643
I0128 04:20:11.052468 140005322254080 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.3545453548431396, loss=3.000365734100342
I0128 04:20:44.695085 140005313861376 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.3580002784729004, loss=2.958059310913086
I0128 04:21:18.312608 140005322254080 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.455151081085205, loss=2.9157536029815674
I0128 04:21:36.963988 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:21:43.185490 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:21:51.901946 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:21:54.462509 140169137129280 submission_runner.py:408] Time since start: 31195.81s, 	Step: 89457, 	{'train/accuracy': 0.7861925959587097, 'train/loss': 1.0689496994018555, 'validation/accuracy': 0.692359983921051, 'validation/loss': 1.4794917106628418, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.101494789123535, 'test/num_examples': 10000, 'score': 30125.72606277466, 'total_duration': 31195.812687158585, 'accumulated_submission_time': 30125.72606277466, 'accumulated_eval_time': 1065.0068988800049, 'accumulated_logging_time': 2.219353675842285}
I0128 04:21:54.496520 140004676327168 logging_writer.py:48] [89457] accumulated_eval_time=1065.006899, accumulated_logging_time=2.219354, accumulated_submission_time=30125.726063, global_step=89457, preemption_count=0, score=30125.726063, test/accuracy=0.568100, test/loss=2.101495, test/num_examples=10000, total_duration=31195.812687, train/accuracy=0.786193, train/loss=1.068950, validation/accuracy=0.692360, validation/loss=1.479492, validation/num_examples=50000
I0128 04:22:09.282655 140005288683264 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.2565174102783203, loss=2.8918566703796387
I0128 04:22:42.881122 140004676327168 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.4698238372802734, loss=3.01762056350708
I0128 04:23:16.528561 140005288683264 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.5190324783325195, loss=3.092708110809326
I0128 04:23:50.183647 140004676327168 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.392899513244629, loss=3.0099434852600098
I0128 04:24:23.753939 140005288683264 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.4313836097717285, loss=2.951725482940674
I0128 04:24:57.408370 140004676327168 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.3393070697784424, loss=2.954561233520508
I0128 04:25:31.056470 140005288683264 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.334587335586548, loss=2.974120855331421
I0128 04:26:04.686118 140004676327168 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.273890495300293, loss=2.9586997032165527
I0128 04:26:38.328420 140005288683264 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.3985440731048584, loss=3.0167529582977295
I0128 04:27:11.988722 140004676327168 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.4880003929138184, loss=3.055854320526123
I0128 04:27:45.614917 140005288683264 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.4139275550842285, loss=3.023270845413208
I0128 04:28:19.264893 140004676327168 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.4769227504730225, loss=2.9704298973083496
I0128 04:28:52.889550 140005288683264 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.321465492248535, loss=2.889949321746826
I0128 04:29:26.534117 140004676327168 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.317899703979492, loss=2.9721150398254395
I0128 04:30:00.240360 140005288683264 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2474963665008545, loss=2.899407148361206
I0128 04:30:24.622324 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:30:30.882899 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:30:39.667822 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:30:42.212129 140169137129280 submission_runner.py:408] Time since start: 31723.56s, 	Step: 90974, 	{'train/accuracy': 0.783621609210968, 'train/loss': 1.0466140508651733, 'validation/accuracy': 0.6930199861526489, 'validation/loss': 1.4299108982086182, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 2.0710158348083496, 'test/num_examples': 10000, 'score': 30635.79270672798, 'total_duration': 31723.562306404114, 'accumulated_submission_time': 30635.79270672798, 'accumulated_eval_time': 1082.5966680049896, 'accumulated_logging_time': 2.2636475563049316}
I0128 04:30:42.246945 140005313861376 logging_writer.py:48] [90974] accumulated_eval_time=1082.596668, accumulated_logging_time=2.263648, accumulated_submission_time=30635.792707, global_step=90974, preemption_count=0, score=30635.792707, test/accuracy=0.569900, test/loss=2.071016, test/num_examples=10000, total_duration=31723.562306, train/accuracy=0.783622, train/loss=1.046614, validation/accuracy=0.693020, validation/loss=1.429911, validation/num_examples=50000
I0128 04:30:51.337606 140005322254080 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.4772441387176514, loss=2.99059796333313
I0128 04:31:24.921146 140005313861376 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.3381240367889404, loss=3.006556272506714
I0128 04:31:58.547904 140005322254080 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.783247470855713, loss=3.0032854080200195
I0128 04:32:32.215535 140005313861376 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.4083213806152344, loss=3.0119614601135254
I0128 04:33:05.839128 140005322254080 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.3194687366485596, loss=2.8971803188323975
I0128 04:33:39.451139 140005313861376 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.5666909217834473, loss=3.042961835861206
I0128 04:34:13.061142 140005322254080 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.441927194595337, loss=2.982435941696167
I0128 04:34:46.687173 140005313861376 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.4044361114501953, loss=2.9949848651885986
I0128 04:35:20.336265 140005322254080 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.3555939197540283, loss=2.969494104385376
I0128 04:35:54.068353 140005313861376 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3546218872070312, loss=3.001091480255127
I0128 04:36:27.718988 140005322254080 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.4928526878356934, loss=2.9593887329101562
I0128 04:37:01.384563 140005313861376 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.6196095943450928, loss=3.0657761096954346
I0128 04:37:35.031112 140005322254080 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.2924795150756836, loss=2.974485158920288
I0128 04:38:08.656052 140005313861376 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.520515203475952, loss=2.916887044906616
I0128 04:38:42.310904 140005322254080 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.453273296356201, loss=2.9837164878845215
I0128 04:39:12.427999 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:39:18.754376 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:39:27.727338 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:39:30.230775 140169137129280 submission_runner.py:408] Time since start: 32251.58s, 	Step: 92491, 	{'train/accuracy': 0.7875877022743225, 'train/loss': 1.0378481149673462, 'validation/accuracy': 0.6992799639701843, 'validation/loss': 1.4238126277923584, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 2.0835328102111816, 'test/num_examples': 10000, 'score': 31145.915197372437, 'total_duration': 32251.58094215393, 'accumulated_submission_time': 31145.915197372437, 'accumulated_eval_time': 1100.3993997573853, 'accumulated_logging_time': 2.308422565460205}
I0128 04:39:30.263745 140004667934464 logging_writer.py:48] [92491] accumulated_eval_time=1100.399400, accumulated_logging_time=2.308423, accumulated_submission_time=31145.915197, global_step=92491, preemption_count=0, score=31145.915197, test/accuracy=0.573000, test/loss=2.083533, test/num_examples=10000, total_duration=32251.580942, train/accuracy=0.787588, train/loss=1.037848, validation/accuracy=0.699280, validation/loss=1.423813, validation/num_examples=50000
I0128 04:39:33.647757 140004676327168 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.3558847904205322, loss=2.97532320022583
I0128 04:40:07.220677 140004667934464 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.318068504333496, loss=2.93705153465271
I0128 04:40:40.850573 140004676327168 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4937450885772705, loss=3.024033546447754
I0128 04:41:14.487152 140004667934464 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.5634589195251465, loss=2.974431037902832
I0128 04:41:48.131250 140004676327168 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.3566384315490723, loss=2.925752639770508
I0128 04:42:21.872788 140004667934464 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.312690258026123, loss=2.938969135284424
I0128 04:42:55.521443 140004676327168 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.3910601139068604, loss=2.933943271636963
I0128 04:43:29.149901 140004667934464 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3072192668914795, loss=2.9341018199920654
I0128 04:44:02.796713 140004676327168 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.2571935653686523, loss=2.9714553356170654
I0128 04:44:36.441115 140004667934464 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.3568947315216064, loss=2.8940751552581787
I0128 04:45:10.090369 140004676327168 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.4687914848327637, loss=2.9865424633026123
I0128 04:45:43.728280 140004667934464 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.442554473876953, loss=2.9480202198028564
I0128 04:46:17.362321 140004676327168 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.6322193145751953, loss=3.0259156227111816
I0128 04:46:51.012181 140004667934464 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.605414867401123, loss=2.9568450450897217
I0128 04:47:24.647391 140004676327168 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.3866841793060303, loss=2.9224648475646973
I0128 04:47:58.292457 140004667934464 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.300748109817505, loss=3.03299617767334
I0128 04:48:00.456526 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:48:06.869038 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:48:15.690336 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:48:18.592914 140169137129280 submission_runner.py:408] Time since start: 32779.94s, 	Step: 94008, 	{'train/accuracy': 0.7824856638908386, 'train/loss': 1.0473655462265015, 'validation/accuracy': 0.6983399987220764, 'validation/loss': 1.4176557064056396, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 2.070194721221924, 'test/num_examples': 10000, 'score': 31656.0486536026, 'total_duration': 32779.9430668354, 'accumulated_submission_time': 31656.0486536026, 'accumulated_eval_time': 1118.535723209381, 'accumulated_logging_time': 2.3523941040039062}
I0128 04:48:18.630548 140005305468672 logging_writer.py:48] [94008] accumulated_eval_time=1118.535723, accumulated_logging_time=2.352394, accumulated_submission_time=31656.048654, global_step=94008, preemption_count=0, score=31656.048654, test/accuracy=0.573700, test/loss=2.070195, test/num_examples=10000, total_duration=32779.943067, train/accuracy=0.782486, train/loss=1.047366, validation/accuracy=0.698340, validation/loss=1.417656, validation/num_examples=50000
I0128 04:48:49.817388 140005313861376 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.6000354290008545, loss=3.0228025913238525
I0128 04:49:23.374775 140005305468672 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.527308464050293, loss=2.9598755836486816
I0128 04:49:56.941247 140005313861376 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4818646907806396, loss=2.9970264434814453
I0128 04:50:30.553758 140005305468672 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.928154706954956, loss=2.9795589447021484
I0128 04:51:04.211906 140005313861376 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.590054512023926, loss=2.9478983879089355
I0128 04:51:37.857437 140005305468672 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.3933675289154053, loss=2.946028709411621
I0128 04:52:11.471950 140005313861376 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.493175506591797, loss=2.9301891326904297
I0128 04:52:45.113296 140005305468672 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.6404354572296143, loss=2.9146077632904053
I0128 04:53:18.732645 140005313861376 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.799525022506714, loss=3.0265941619873047
I0128 04:53:52.356536 140005305468672 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.4236788749694824, loss=2.986525535583496
I0128 04:54:26.014622 140005313861376 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.5269243717193604, loss=2.9855294227600098
I0128 04:54:59.738218 140005305468672 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.4878311157226562, loss=2.987039566040039
I0128 04:55:33.396353 140005313861376 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.482390880584717, loss=2.936939001083374
I0128 04:56:06.975986 140005305468672 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.5945916175842285, loss=3.026409864425659
I0128 04:56:40.555752 140005313861376 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.462522029876709, loss=2.9913506507873535
I0128 04:56:48.775287 140169137129280 spec.py:321] Evaluating on the training split.
I0128 04:56:55.087075 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 04:57:04.231707 140169137129280 spec.py:349] Evaluating on the test split.
I0128 04:57:06.753102 140169137129280 submission_runner.py:408] Time since start: 33308.10s, 	Step: 95526, 	{'train/accuracy': 0.8123804330825806, 'train/loss': 0.9565143585205078, 'validation/accuracy': 0.6977799534797668, 'validation/loss': 1.4305815696716309, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 2.0659029483795166, 'test/num_examples': 10000, 'score': 32166.133952617645, 'total_duration': 33308.10327959061, 'accumulated_submission_time': 32166.133952617645, 'accumulated_eval_time': 1136.5134971141815, 'accumulated_logging_time': 2.4002532958984375}
I0128 04:57:06.789263 140005297075968 logging_writer.py:48] [95526] accumulated_eval_time=1136.513497, accumulated_logging_time=2.400253, accumulated_submission_time=32166.133953, global_step=95526, preemption_count=0, score=32166.133953, test/accuracy=0.576100, test/loss=2.065903, test/num_examples=10000, total_duration=33308.103280, train/accuracy=0.812380, train/loss=0.956514, validation/accuracy=0.697780, validation/loss=1.430582, validation/num_examples=50000
I0128 04:57:32.008604 140005330646784 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.32246470451355, loss=2.9597361087799072
I0128 04:58:05.602547 140005297075968 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.8698325157165527, loss=2.986237049102783
I0128 04:58:39.146685 140005330646784 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.346552848815918, loss=2.916703701019287
I0128 04:59:12.702173 140005297075968 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.452946186065674, loss=2.996638536453247
I0128 04:59:46.252482 140005330646784 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.4771711826324463, loss=2.92441463470459
I0128 05:00:19.825925 140005297075968 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.510516405105591, loss=2.990705966949463
I0128 05:00:53.525410 140005330646784 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.561235189437866, loss=2.983191728591919
I0128 05:01:27.118244 140005297075968 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.406437873840332, loss=2.9902729988098145
I0128 05:02:00.687408 140005330646784 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.450071334838867, loss=2.869849920272827
I0128 05:02:34.309368 140005297075968 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.666627883911133, loss=2.984766721725464
I0128 05:03:07.966418 140005330646784 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4226744174957275, loss=2.986773729324341
I0128 05:03:41.623344 140005297075968 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.595836639404297, loss=2.9212193489074707
I0128 05:04:15.237905 140005330646784 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.5059573650360107, loss=2.9137563705444336
I0128 05:04:48.860290 140005297075968 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.4118404388427734, loss=2.930018186569214
I0128 05:05:22.473883 140005330646784 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.4614267349243164, loss=2.9945807456970215
I0128 05:05:37.075198 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:05:43.386895 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:05:52.392575 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:05:54.904567 140169137129280 submission_runner.py:408] Time since start: 33836.25s, 	Step: 97045, 	{'train/accuracy': 0.7994260191917419, 'train/loss': 0.9897308945655823, 'validation/accuracy': 0.6978600025177002, 'validation/loss': 1.441440463066101, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 2.0669610500335693, 'test/num_examples': 10000, 'score': 32676.36093187332, 'total_duration': 33836.25459456444, 'accumulated_submission_time': 32676.36093187332, 'accumulated_eval_time': 1154.3426752090454, 'accumulated_logging_time': 2.4465677738189697}
I0128 05:05:54.941218 140005288683264 logging_writer.py:48] [97045] accumulated_eval_time=1154.342675, accumulated_logging_time=2.446568, accumulated_submission_time=32676.360932, global_step=97045, preemption_count=0, score=32676.360932, test/accuracy=0.576600, test/loss=2.066961, test/num_examples=10000, total_duration=33836.254595, train/accuracy=0.799426, train/loss=0.989731, validation/accuracy=0.697860, validation/loss=1.441440, validation/num_examples=50000
I0128 05:06:13.771689 140005297075968 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.538407564163208, loss=2.9665284156799316
I0128 05:06:47.374713 140005288683264 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.505406379699707, loss=2.981584072113037
I0128 05:07:21.117348 140005297075968 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.4881229400634766, loss=2.94350004196167
I0128 05:07:54.688646 140005288683264 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.5667800903320312, loss=2.911881923675537
I0128 05:08:28.294339 140005297075968 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.5239040851593018, loss=2.86606502532959
I0128 05:09:01.941416 140005288683264 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.506862163543701, loss=2.965290069580078
I0128 05:09:35.591080 140005297075968 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.6012775897979736, loss=2.963515281677246
I0128 05:10:09.221823 140005288683264 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.7249014377593994, loss=2.928103446960449
I0128 05:10:42.888941 140005297075968 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.5080230236053467, loss=2.926788568496704
I0128 05:11:16.528902 140005288683264 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.393203020095825, loss=2.9006245136260986
I0128 05:11:50.142138 140005297075968 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.4981319904327393, loss=2.93031644821167
I0128 05:12:23.790904 140005288683264 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.492690086364746, loss=2.920492649078369
I0128 05:12:57.457360 140005297075968 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.8621559143066406, loss=2.977513551712036
I0128 05:13:31.170876 140005288683264 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.421304941177368, loss=2.9136295318603516
I0128 05:14:04.760040 140005297075968 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.497718095779419, loss=2.950519323348999
I0128 05:14:25.110551 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:14:31.543912 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:14:40.150420 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:14:42.704733 140169137129280 submission_runner.py:408] Time since start: 34364.05s, 	Step: 98562, 	{'train/accuracy': 0.8053650856018066, 'train/loss': 0.9407090544700623, 'validation/accuracy': 0.7060999870300293, 'validation/loss': 1.369234323501587, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.9954557418823242, 'test/num_examples': 10000, 'score': 33186.46888136864, 'total_duration': 34364.05491042137, 'accumulated_submission_time': 33186.46888136864, 'accumulated_eval_time': 1171.936819076538, 'accumulated_logging_time': 2.4959654808044434}
I0128 05:14:42.742016 140005313861376 logging_writer.py:48] [98562] accumulated_eval_time=1171.936819, accumulated_logging_time=2.495965, accumulated_submission_time=33186.468881, global_step=98562, preemption_count=0, score=33186.468881, test/accuracy=0.581900, test/loss=1.995456, test/num_examples=10000, total_duration=34364.054910, train/accuracy=0.805365, train/loss=0.940709, validation/accuracy=0.706100, validation/loss=1.369234, validation/num_examples=50000
I0128 05:14:55.824287 140005322254080 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.5600128173828125, loss=2.956657886505127
I0128 05:15:29.343294 140005313861376 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.502981662750244, loss=2.8837785720825195
I0128 05:16:02.952378 140005322254080 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.7991199493408203, loss=2.96510910987854
I0128 05:16:36.568180 140005313861376 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4728405475616455, loss=2.9389824867248535
I0128 05:17:10.237095 140005322254080 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.479677438735962, loss=2.94140362739563
I0128 05:17:43.870964 140005313861376 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.5600128173828125, loss=2.9837968349456787
I0128 05:18:17.514833 140005322254080 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.36568021774292, loss=2.9687678813934326
I0128 05:18:51.150174 140005313861376 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.527583122253418, loss=2.9285926818847656
I0128 05:19:24.849971 140005322254080 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.6474711894989014, loss=2.913485288619995
I0128 05:19:58.424583 140005313861376 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.5328774452209473, loss=2.982792615890503
I0128 05:20:32.034449 140005322254080 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.6075167655944824, loss=2.9464449882507324
I0128 05:21:05.688806 140005313861376 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.4485738277435303, loss=3.0084729194641113
I0128 05:21:39.323226 140005322254080 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.47731614112854, loss=2.841585636138916
I0128 05:22:12.954657 140005313861376 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.6952967643737793, loss=2.94820499420166
I0128 05:22:46.592974 140005322254080 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.74029541015625, loss=2.9261720180511475
I0128 05:23:12.953125 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:23:19.189693 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:23:28.035808 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:23:30.562597 140169137129280 submission_runner.py:408] Time since start: 34891.91s, 	Step: 100080, 	{'train/accuracy': 0.7974131107330322, 'train/loss': 1.000417709350586, 'validation/accuracy': 0.7032999992370605, 'validation/loss': 1.4029895067214966, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 2.07080340385437, 'test/num_examples': 10000, 'score': 33696.61943101883, 'total_duration': 34891.91277551651, 'accumulated_submission_time': 33696.61943101883, 'accumulated_eval_time': 1189.5462565422058, 'accumulated_logging_time': 2.544489622116089}
I0128 05:23:30.600624 140004659541760 logging_writer.py:48] [100080] accumulated_eval_time=1189.546257, accumulated_logging_time=2.544490, accumulated_submission_time=33696.619431, global_step=100080, preemption_count=0, score=33696.619431, test/accuracy=0.573700, test/loss=2.070803, test/num_examples=10000, total_duration=34891.912776, train/accuracy=0.797413, train/loss=1.000418, validation/accuracy=0.703300, validation/loss=1.402990, validation/num_examples=50000
I0128 05:23:37.658654 140005288683264 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.528707265853882, loss=2.921060800552368
I0128 05:24:11.237828 140004659541760 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.718688488006592, loss=2.915339469909668
I0128 05:24:44.872737 140005288683264 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.454324245452881, loss=2.972090482711792
I0128 05:25:18.526901 140004659541760 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.5298683643341064, loss=2.9616997241973877
I0128 05:25:52.207689 140005288683264 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.505192279815674, loss=2.980217456817627
I0128 05:26:25.761772 140004659541760 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.7558839321136475, loss=2.8791391849517822
I0128 05:26:59.364571 140005288683264 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.48712158203125, loss=2.8936550617218018
I0128 05:27:32.969138 140004659541760 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.381032705307007, loss=2.8869376182556152
I0128 05:28:06.641559 140005288683264 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.6568117141723633, loss=2.9298007488250732
I0128 05:28:40.284901 140004659541760 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.7194814682006836, loss=2.918879270553589
I0128 05:29:13.913951 140005288683264 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.517883062362671, loss=2.9303598403930664
I0128 05:29:47.555049 140004659541760 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.4626965522766113, loss=2.81946063041687
I0128 05:30:21.168892 140005288683264 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.606292724609375, loss=2.9274654388427734
I0128 05:30:54.779756 140004659541760 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.5574951171875, loss=2.9219863414764404
I0128 05:31:28.416988 140005288683264 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.7992148399353027, loss=2.9544568061828613
I0128 05:32:00.565014 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:32:06.917072 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:32:15.933927 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:32:18.503328 140169137129280 submission_runner.py:408] Time since start: 35419.85s, 	Step: 101597, 	{'train/accuracy': 0.7987882494926453, 'train/loss': 0.9974828362464905, 'validation/accuracy': 0.7042399644851685, 'validation/loss': 1.3996078968048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5823000073432922, 'test/loss': 2.036078691482544, 'test/num_examples': 10000, 'score': 34206.52481389046, 'total_duration': 35419.85327458382, 'accumulated_submission_time': 34206.52481389046, 'accumulated_eval_time': 1207.4842991828918, 'accumulated_logging_time': 2.5927844047546387}
I0128 05:32:18.539965 140004667934464 logging_writer.py:48] [101597] accumulated_eval_time=1207.484299, accumulated_logging_time=2.592784, accumulated_submission_time=34206.524814, global_step=101597, preemption_count=0, score=34206.524814, test/accuracy=0.582300, test/loss=2.036079, test/num_examples=10000, total_duration=35419.853275, train/accuracy=0.798788, train/loss=0.997483, validation/accuracy=0.704240, validation/loss=1.399608, validation/num_examples=50000
I0128 05:32:19.884130 140004676327168 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.525130271911621, loss=2.868364095687866
I0128 05:32:53.409224 140004667934464 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.585770606994629, loss=2.9601025581359863
I0128 05:33:26.968965 140004676327168 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.8649892807006836, loss=2.9611663818359375
I0128 05:34:00.537081 140004667934464 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.810727834701538, loss=3.0259549617767334
I0128 05:34:34.120110 140004676327168 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.3903160095214844, loss=2.865349292755127
I0128 05:35:07.783381 140004667934464 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.5767784118652344, loss=2.974146842956543
I0128 05:35:41.413151 140004676327168 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.5268797874450684, loss=2.897885322570801
I0128 05:36:15.020612 140004667934464 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.607628107070923, loss=2.9791109561920166
I0128 05:36:48.678386 140004676327168 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.513385534286499, loss=2.8842830657958984
I0128 05:37:22.302840 140004667934464 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.6828949451446533, loss=2.934722900390625
I0128 05:37:55.921712 140004676327168 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.5720012187957764, loss=2.931292772293091
I0128 05:38:29.626487 140004667934464 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.5882816314697266, loss=2.889591693878174
I0128 05:39:03.257936 140004676327168 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.783186912536621, loss=2.9386487007141113
I0128 05:39:36.912259 140004667934464 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.6866867542266846, loss=2.9565019607543945
I0128 05:40:10.526351 140004676327168 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.765244245529175, loss=2.993412494659424
I0128 05:40:44.137330 140004667934464 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.490983247756958, loss=2.9784610271453857
I0128 05:40:48.645581 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:40:54.948369 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:41:03.815605 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:41:06.325348 140169137129280 submission_runner.py:408] Time since start: 35947.68s, 	Step: 103115, 	{'train/accuracy': 0.7995654940605164, 'train/loss': 0.9808319807052612, 'validation/accuracy': 0.7089200019836426, 'validation/loss': 1.3838368654251099, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0236704349517822, 'test/num_examples': 10000, 'score': 34716.570257902145, 'total_duration': 35947.67550730705, 'accumulated_submission_time': 34716.570257902145, 'accumulated_eval_time': 1225.1640086174011, 'accumulated_logging_time': 2.6405186653137207}
I0128 05:41:06.374880 140005313861376 logging_writer.py:48] [103115] accumulated_eval_time=1225.164009, accumulated_logging_time=2.640519, accumulated_submission_time=34716.570258, global_step=103115, preemption_count=0, score=34716.570258, test/accuracy=0.583500, test/loss=2.023670, test/num_examples=10000, total_duration=35947.675507, train/accuracy=0.799565, train/loss=0.980832, validation/accuracy=0.708920, validation/loss=1.383837, validation/num_examples=50000
I0128 05:41:35.252001 140005322254080 logging_writer.py:48] [103200] global_step=103200, grad_norm=3.037010908126831, loss=2.9535117149353027
I0128 05:42:08.876499 140005313861376 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.7155630588531494, loss=2.8809497356414795
I0128 05:42:42.514275 140005322254080 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.5828843116760254, loss=2.945357322692871
I0128 05:43:16.133874 140005313861376 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.6230647563934326, loss=2.866580009460449
I0128 05:43:49.771237 140005322254080 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.8083653450012207, loss=2.9201855659484863
I0128 05:44:23.494751 140005313861376 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.5096707344055176, loss=2.8456032276153564
I0128 05:44:57.160500 140005322254080 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.743438720703125, loss=2.877020835876465
I0128 05:45:30.806825 140005313861376 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.7108240127563477, loss=2.8804237842559814
I0128 05:46:04.425537 140005322254080 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.7904603481292725, loss=2.9579532146453857
I0128 05:46:38.079885 140005313861376 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.761425256729126, loss=2.9269797801971436
I0128 05:47:11.713398 140005322254080 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.820611000061035, loss=2.841114044189453
I0128 05:47:45.365639 140005313861376 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.772021770477295, loss=2.9836673736572266
I0128 05:48:18.965138 140005322254080 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.55903697013855, loss=2.926307201385498
I0128 05:48:52.630245 140005313861376 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.5687930583953857, loss=2.8692235946655273
I0128 05:49:26.261650 140005322254080 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.608818292617798, loss=2.8882389068603516
I0128 05:49:36.508538 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:49:42.783415 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:49:51.766183 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:49:54.178378 140169137129280 submission_runner.py:408] Time since start: 36475.53s, 	Step: 104632, 	{'train/accuracy': 0.8382493257522583, 'train/loss': 0.8604050874710083, 'validation/accuracy': 0.7122600078582764, 'validation/loss': 1.3814303874969482, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.0447278022766113, 'test/num_examples': 10000, 'score': 35226.643216609955, 'total_duration': 36475.528554201126, 'accumulated_submission_time': 35226.643216609955, 'accumulated_eval_time': 1242.8338084220886, 'accumulated_logging_time': 2.702409267425537}
I0128 05:49:54.216466 140004676327168 logging_writer.py:48] [104632] accumulated_eval_time=1242.833808, accumulated_logging_time=2.702409, accumulated_submission_time=35226.643217, global_step=104632, preemption_count=0, score=35226.643217, test/accuracy=0.581000, test/loss=2.044728, test/num_examples=10000, total_duration=36475.528554, train/accuracy=0.838249, train/loss=0.860405, validation/accuracy=0.712260, validation/loss=1.381430, validation/num_examples=50000
I0128 05:50:17.433611 140005288683264 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.710195541381836, loss=2.8963255882263184
I0128 05:50:51.100139 140004676327168 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.4532878398895264, loss=2.8994693756103516
I0128 05:51:24.712913 140005288683264 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.679469108581543, loss=2.8900654315948486
I0128 05:51:58.372030 140004676327168 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.482205629348755, loss=2.878211259841919
I0128 05:52:32.010294 140005288683264 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.9258880615234375, loss=2.9606573581695557
I0128 05:53:05.642745 140004676327168 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.548234462738037, loss=2.8474655151367188
I0128 05:53:39.302677 140005288683264 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.6762242317199707, loss=2.938251256942749
I0128 05:54:12.946311 140004676327168 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.7147018909454346, loss=2.9584977626800537
I0128 05:54:46.565418 140005288683264 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.4234981536865234, loss=2.85603404045105
I0128 05:55:20.219959 140004676327168 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.642310619354248, loss=2.877427101135254
I0128 05:55:53.862005 140005288683264 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.5482208728790283, loss=2.962106943130493
I0128 05:56:27.484189 140004676327168 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.6063270568847656, loss=2.8483705520629883
I0128 05:57:01.145282 140005288683264 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.5380330085754395, loss=2.874298572540283
I0128 05:57:34.769291 140004676327168 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.5581085681915283, loss=2.8353285789489746
I0128 05:58:08.418959 140005288683264 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.720111131668091, loss=2.9195010662078857
I0128 05:58:24.366449 140169137129280 spec.py:321] Evaluating on the training split.
I0128 05:58:30.585177 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 05:58:39.401929 140169137129280 spec.py:349] Evaluating on the test split.
I0128 05:58:41.915040 140169137129280 submission_runner.py:408] Time since start: 37003.27s, 	Step: 106149, 	{'train/accuracy': 0.8146125674247742, 'train/loss': 0.9301750063896179, 'validation/accuracy': 0.7082599997520447, 'validation/loss': 1.390093445777893, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0411343574523926, 'test/num_examples': 10000, 'score': 35736.733736753464, 'total_duration': 37003.26521921158, 'accumulated_submission_time': 35736.733736753464, 'accumulated_eval_time': 1260.3823611736298, 'accumulated_logging_time': 2.751145124435425}
I0128 05:58:41.953068 140005313861376 logging_writer.py:48] [106149] accumulated_eval_time=1260.382361, accumulated_logging_time=2.751145, accumulated_submission_time=35736.733737, global_step=106149, preemption_count=0, score=35736.733737, test/accuracy=0.583500, test/loss=2.041134, test/num_examples=10000, total_duration=37003.265219, train/accuracy=0.814613, train/loss=0.930175, validation/accuracy=0.708260, validation/loss=1.390093, validation/num_examples=50000
I0128 05:58:59.429633 140005322254080 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.7574446201324463, loss=2.8860068321228027
I0128 05:59:32.963027 140005313861376 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.8436267375946045, loss=2.9415016174316406
I0128 06:00:06.516866 140005322254080 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.722806215286255, loss=2.9125421047210693
I0128 06:00:40.125691 140005313861376 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.8196749687194824, loss=2.9134697914123535
I0128 06:01:13.780573 140005322254080 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.816282272338867, loss=2.9402718544006348
I0128 06:01:47.420473 140005313861376 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.7213385105133057, loss=2.8805959224700928
I0128 06:02:21.054508 140005322254080 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.8093204498291016, loss=2.8755991458892822
I0128 06:02:54.765384 140005313861376 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.9434878826141357, loss=2.9684927463531494
I0128 06:03:28.393960 140005322254080 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.5645527839660645, loss=2.862852096557617
I0128 06:04:02.061748 140005313861376 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.577066659927368, loss=2.8217692375183105
I0128 06:04:35.692597 140005322254080 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.744255304336548, loss=2.889833450317383
I0128 06:05:09.301789 140005313861376 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.911431312561035, loss=2.9864795207977295
I0128 06:05:42.868381 140005322254080 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.830900192260742, loss=2.946154832839966
I0128 06:06:16.433081 140005313861376 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.66098952293396, loss=2.919222593307495
I0128 06:06:50.016164 140005322254080 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.9646694660186768, loss=2.9373342990875244
I0128 06:07:11.980974 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:07:18.272677 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:07:27.065197 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:07:29.589913 140169137129280 submission_runner.py:408] Time since start: 37530.94s, 	Step: 107667, 	{'train/accuracy': 0.8170041441917419, 'train/loss': 0.9116354584693909, 'validation/accuracy': 0.7121999859809875, 'validation/loss': 1.3599272966384888, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 2.011181116104126, 'test/num_examples': 10000, 'score': 36246.702476263046, 'total_duration': 37530.93986582756, 'accumulated_submission_time': 36246.702476263046, 'accumulated_eval_time': 1277.9910361766815, 'accumulated_logging_time': 2.79943585395813}
I0128 06:07:29.625600 140004676327168 logging_writer.py:48] [107667] accumulated_eval_time=1277.991036, accumulated_logging_time=2.799436, accumulated_submission_time=36246.702476, global_step=107667, preemption_count=0, score=36246.702476, test/accuracy=0.589600, test/loss=2.011181, test/num_examples=10000, total_duration=37530.939866, train/accuracy=0.817004, train/loss=0.911635, validation/accuracy=0.712200, validation/loss=1.359927, validation/num_examples=50000
I0128 06:07:41.017419 140005288683264 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.6534132957458496, loss=2.95632004737854
I0128 06:08:14.553867 140004676327168 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.837484121322632, loss=3.0022635459899902
I0128 06:08:48.193524 140005288683264 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.816960334777832, loss=2.933568000793457
I0128 06:09:21.925590 140004676327168 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.7967116832733154, loss=2.950657844543457
I0128 06:09:55.591633 140005288683264 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.866523265838623, loss=2.933621644973755
I0128 06:10:29.217913 140004676327168 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.7469482421875, loss=2.8774349689483643
I0128 06:11:02.831365 140005288683264 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.752364158630371, loss=2.850527286529541
I0128 06:11:36.475970 140004676327168 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.6753571033477783, loss=2.9437499046325684
I0128 06:12:10.134243 140005288683264 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.7492334842681885, loss=2.885545015335083
I0128 06:12:43.759423 140004676327168 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.9397683143615723, loss=2.861271381378174
I0128 06:13:17.395614 140005288683264 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6585710048675537, loss=2.878060817718506
I0128 06:13:51.006571 140004676327168 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.59661602973938, loss=2.8845434188842773
I0128 06:14:24.623134 140005288683264 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.8826000690460205, loss=2.97532057762146
I0128 06:14:58.260051 140004676327168 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.5862178802490234, loss=2.8822035789489746
I0128 06:15:31.971642 140005288683264 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.550168991088867, loss=2.8533291816711426
I0128 06:15:59.706277 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:16:06.014020 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:16:14.995719 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:16:18.229555 140169137129280 submission_runner.py:408] Time since start: 38059.58s, 	Step: 109184, 	{'train/accuracy': 0.8053650856018066, 'train/loss': 0.9917887449264526, 'validation/accuracy': 0.7057399749755859, 'validation/loss': 1.4216396808624268, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 2.094308614730835, 'test/num_examples': 10000, 'score': 36756.72316074371, 'total_duration': 38059.57973694801, 'accumulated_submission_time': 36756.72316074371, 'accumulated_eval_time': 1296.5142834186554, 'accumulated_logging_time': 2.8462047576904297}
I0128 06:16:18.261096 140004676327168 logging_writer.py:48] [109184] accumulated_eval_time=1296.514283, accumulated_logging_time=2.846205, accumulated_submission_time=36756.723161, global_step=109184, preemption_count=0, score=36756.723161, test/accuracy=0.575900, test/loss=2.094309, test/num_examples=10000, total_duration=38059.579737, train/accuracy=0.805365, train/loss=0.991789, validation/accuracy=0.705740, validation/loss=1.421640, validation/num_examples=50000
I0128 06:16:23.967429 140005305468672 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.683643102645874, loss=2.8715970516204834
I0128 06:16:57.493551 140004676327168 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.8342201709747314, loss=2.9052233695983887
I0128 06:17:31.094930 140005305468672 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.8436450958251953, loss=2.8535220623016357
I0128 06:18:04.734059 140004676327168 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.5072433948516846, loss=2.8564608097076416
I0128 06:18:38.394815 140005305468672 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.9150643348693848, loss=2.8769726753234863
I0128 06:19:12.001923 140004676327168 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.9925596714019775, loss=2.956899404525757
I0128 06:19:45.597784 140005305468672 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.845923662185669, loss=2.8581953048706055
I0128 06:20:19.220252 140004676327168 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.066873073577881, loss=2.9695866107940674
I0128 06:20:52.901636 140005305468672 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.9123458862304688, loss=2.9909279346466064
I0128 06:21:26.614794 140004676327168 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.9082040786743164, loss=2.8858630657196045
I0128 06:22:00.251509 140005305468672 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.624983787536621, loss=2.8138742446899414
I0128 06:22:33.909870 140004676327168 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.989464044570923, loss=2.9750399589538574
I0128 06:23:07.548463 140005305468672 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.710904598236084, loss=2.850048065185547
I0128 06:23:41.178610 140004676327168 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.7717185020446777, loss=2.826446294784546
I0128 06:24:14.831157 140005305468672 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.7412943840026855, loss=2.9700493812561035
I0128 06:24:48.474634 140004676327168 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.743356704711914, loss=2.8532509803771973
I0128 06:24:48.481749 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:24:54.737970 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:25:03.582880 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:25:06.110963 140169137129280 submission_runner.py:408] Time since start: 38587.46s, 	Step: 110701, 	{'train/accuracy': 0.8196946382522583, 'train/loss': 0.9097425937652588, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.3392237424850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.9844063520431519, 'test/num_examples': 10000, 'score': 37266.88493037224, 'total_duration': 38587.46114182472, 'accumulated_submission_time': 37266.88493037224, 'accumulated_eval_time': 1314.1434371471405, 'accumulated_logging_time': 2.8875298500061035}
I0128 06:25:06.148879 140005297075968 logging_writer.py:48] [110701] accumulated_eval_time=1314.143437, accumulated_logging_time=2.887530, accumulated_submission_time=37266.884930, global_step=110701, preemption_count=0, score=37266.884930, test/accuracy=0.590000, test/loss=1.984406, test/num_examples=10000, total_duration=38587.461142, train/accuracy=0.819695, train/loss=0.909743, validation/accuracy=0.718420, validation/loss=1.339224, validation/num_examples=50000
I0128 06:25:39.683264 140005330646784 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.6250712871551514, loss=2.74647855758667
I0128 06:26:13.253596 140005297075968 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.769157648086548, loss=2.9178102016448975
I0128 06:26:46.893085 140005330646784 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.7162840366363525, loss=2.877692699432373
I0128 06:27:20.515953 140005297075968 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.8351473808288574, loss=2.881054639816284
I0128 06:27:54.205613 140005330646784 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.8301568031311035, loss=2.8257291316986084
I0128 06:28:27.856594 140005297075968 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.824929714202881, loss=2.837362766265869
I0128 06:29:01.500849 140005330646784 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.7367546558380127, loss=2.945619583129883
I0128 06:29:35.131421 140005297075968 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7971935272216797, loss=2.8511977195739746
I0128 06:30:08.794804 140005330646784 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.692950963973999, loss=2.861541509628296
I0128 06:30:42.433836 140005297075968 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.7973928451538086, loss=2.8792788982391357
I0128 06:31:16.057071 140005330646784 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.804837465286255, loss=2.945824146270752
I0128 06:31:49.676144 140005297075968 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.6998255252838135, loss=2.8385767936706543
I0128 06:32:23.268033 140005330646784 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.7690186500549316, loss=2.8669979572296143
I0128 06:32:56.923444 140005297075968 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.7455663681030273, loss=2.9028403759002686
I0128 06:33:30.574014 140005330646784 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.7739908695220947, loss=2.9269986152648926
I0128 06:33:36.435554 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:33:42.660149 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:33:51.649095 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:33:54.490755 140169137129280 submission_runner.py:408] Time since start: 39115.84s, 	Step: 112219, 	{'train/accuracy': 0.8258330225944519, 'train/loss': 0.8885695338249207, 'validation/accuracy': 0.7242000102996826, 'validation/loss': 1.3239599466323853, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.9667091369628906, 'test/num_examples': 10000, 'score': 37777.112483501434, 'total_duration': 39115.840937137604, 'accumulated_submission_time': 37777.112483501434, 'accumulated_eval_time': 1332.1986136436462, 'accumulated_logging_time': 2.9358580112457275}
I0128 06:33:54.529181 140005288683264 logging_writer.py:48] [112219] accumulated_eval_time=1332.198614, accumulated_logging_time=2.935858, accumulated_submission_time=37777.112484, global_step=112219, preemption_count=0, score=37777.112484, test/accuracy=0.593400, test/loss=1.966709, test/num_examples=10000, total_duration=39115.840937, train/accuracy=0.825833, train/loss=0.888570, validation/accuracy=0.724200, validation/loss=1.323960, validation/num_examples=50000
I0128 06:34:22.069037 140005305468672 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.868547201156616, loss=2.8447718620300293
I0128 06:34:55.675464 140005288683264 logging_writer.py:48] [112400] global_step=112400, grad_norm=3.02143931388855, loss=2.946934938430786
I0128 06:35:29.321816 140005305468672 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.7283737659454346, loss=2.8599095344543457
I0128 06:36:02.964215 140005288683264 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.9539952278137207, loss=2.94559907913208
I0128 06:36:36.579028 140005305468672 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.760631799697876, loss=2.95048189163208
I0128 06:37:10.238333 140005288683264 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.7822484970092773, loss=2.8760058879852295
I0128 06:37:43.878696 140005305468672 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.9769015312194824, loss=2.9133777618408203
I0128 06:38:17.512858 140005288683264 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.777994155883789, loss=2.90342378616333
I0128 06:38:51.120439 140005305468672 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.892085313796997, loss=2.881350517272949
I0128 06:39:24.784377 140005288683264 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.870047092437744, loss=2.866008758544922
I0128 06:39:58.429930 140005305468672 logging_writer.py:48] [113300] global_step=113300, grad_norm=3.1079113483428955, loss=2.9785687923431396
I0128 06:40:32.071497 140005288683264 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.7946178913116455, loss=2.8954031467437744
I0128 06:41:05.655969 140005305468672 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.7331738471984863, loss=2.869588613510132
I0128 06:41:39.305524 140005288683264 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8893444538116455, loss=2.8828675746917725
I0128 06:42:12.925811 140005305468672 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.789113998413086, loss=2.899606227874756
I0128 06:42:24.496782 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:42:30.982014 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:42:39.740968 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:42:42.266009 140169137129280 submission_runner.py:408] Time since start: 39643.62s, 	Step: 113736, 	{'train/accuracy': 0.8484932780265808, 'train/loss': 0.8184272646903992, 'validation/accuracy': 0.7218599915504456, 'validation/loss': 1.3505167961120605, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.9928845167160034, 'test/num_examples': 10000, 'score': 38287.02067565918, 'total_duration': 39643.61619019508, 'accumulated_submission_time': 38287.02067565918, 'accumulated_eval_time': 1349.967808008194, 'accumulated_logging_time': 2.9852118492126465}
I0128 06:42:42.306571 140004667934464 logging_writer.py:48] [113736] accumulated_eval_time=1349.967808, accumulated_logging_time=2.985212, accumulated_submission_time=38287.020676, global_step=113736, preemption_count=0, score=38287.020676, test/accuracy=0.596200, test/loss=1.992885, test/num_examples=10000, total_duration=39643.616190, train/accuracy=0.848493, train/loss=0.818427, validation/accuracy=0.721860, validation/loss=1.350517, validation/num_examples=50000
I0128 06:43:04.135832 140005297075968 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.6252317428588867, loss=2.838763475418091
I0128 06:43:37.746102 140004667934464 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.8547329902648926, loss=2.8939738273620605
I0128 06:44:11.405768 140005297075968 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.900681972503662, loss=2.891077995300293
I0128 06:44:45.042704 140004667934464 logging_writer.py:48] [114100] global_step=114100, grad_norm=3.0658440589904785, loss=2.896195411682129
I0128 06:45:18.662036 140005297075968 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.919539213180542, loss=2.852795124053955
I0128 06:45:52.330317 140004667934464 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.900615930557251, loss=2.8615732192993164
I0128 06:46:26.049185 140005297075968 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.8889613151550293, loss=2.8849263191223145
I0128 06:46:59.704644 140004667934464 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.8629891872406006, loss=2.877373218536377
I0128 06:47:33.345475 140005297075968 logging_writer.py:48] [114600] global_step=114600, grad_norm=3.1242175102233887, loss=2.8936729431152344
I0128 06:48:06.929864 140004667934464 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.744044780731201, loss=2.8211238384246826
I0128 06:48:40.533354 140005297075968 logging_writer.py:48] [114800] global_step=114800, grad_norm=3.029905080795288, loss=2.834864616394043
I0128 06:49:14.128579 140004667934464 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.9672324657440186, loss=2.8880414962768555
I0128 06:49:47.734927 140005297075968 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.7315049171447754, loss=2.7718119621276855
I0128 06:50:21.378587 140004667934464 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.932246685028076, loss=2.9246129989624023
I0128 06:50:55.002855 140005297075968 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.841919183731079, loss=2.8479933738708496
I0128 06:51:12.298595 140169137129280 spec.py:321] Evaluating on the training split.
I0128 06:51:18.764457 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 06:51:27.570988 140169137129280 spec.py:349] Evaluating on the test split.
I0128 06:51:30.063221 140169137129280 submission_runner.py:408] Time since start: 40171.41s, 	Step: 115253, 	{'train/accuracy': 0.8364556431770325, 'train/loss': 0.8760803937911987, 'validation/accuracy': 0.7188000082969666, 'validation/loss': 1.3691059350967407, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 2.007413864135742, 'test/num_examples': 10000, 'score': 38796.95326471329, 'total_duration': 40171.41336941719, 'accumulated_submission_time': 38796.95326471329, 'accumulated_eval_time': 1367.7323701381683, 'accumulated_logging_time': 3.036560297012329}
I0128 06:51:30.103211 140004659541760 logging_writer.py:48] [115253] accumulated_eval_time=1367.732370, accumulated_logging_time=3.036560, accumulated_submission_time=38796.953265, global_step=115253, preemption_count=0, score=38796.953265, test/accuracy=0.591400, test/loss=2.007414, test/num_examples=10000, total_duration=40171.413369, train/accuracy=0.836456, train/loss=0.876080, validation/accuracy=0.718800, validation/loss=1.369106, validation/num_examples=50000
I0128 06:51:46.256948 140005288683264 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.9321725368499756, loss=2.8425707817077637
I0128 06:52:19.849682 140004659541760 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.9693119525909424, loss=2.859215259552002
I0128 06:52:53.542311 140005288683264 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.7539455890655518, loss=2.809803009033203
I0128 06:53:27.164572 140004659541760 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.8020057678222656, loss=2.8340542316436768
I0128 06:54:00.818499 140005288683264 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.819114923477173, loss=2.872866153717041
I0128 06:54:34.447696 140004659541760 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.8824679851531982, loss=2.771843671798706
I0128 06:55:08.111026 140005288683264 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.8772010803222656, loss=2.8652000427246094
I0128 06:55:41.738202 140004659541760 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.90714168548584, loss=2.8027424812316895
I0128 06:56:15.380119 140005288683264 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.7936460971832275, loss=2.8290882110595703
I0128 06:56:48.981562 140004659541760 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.8523316383361816, loss=2.8130738735198975
I0128 06:57:22.625704 140005288683264 logging_writer.py:48] [116300] global_step=116300, grad_norm=3.0075347423553467, loss=2.8177404403686523
I0128 06:57:56.271545 140004659541760 logging_writer.py:48] [116400] global_step=116400, grad_norm=3.1522152423858643, loss=2.886409044265747
I0128 06:58:29.912062 140005288683264 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.7420778274536133, loss=2.8288722038269043
I0128 06:59:03.627406 140004659541760 logging_writer.py:48] [116600] global_step=116600, grad_norm=3.0200934410095215, loss=2.8570172786712646
I0128 06:59:37.284472 140005288683264 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.9103763103485107, loss=2.829514980316162
I0128 07:00:00.291019 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:00:06.705815 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:00:15.282706 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:00:17.803299 140169137129280 submission_runner.py:408] Time since start: 40699.15s, 	Step: 116770, 	{'train/accuracy': 0.8357780575752258, 'train/loss': 0.8407694101333618, 'validation/accuracy': 0.7210800051689148, 'validation/loss': 1.322500228881836, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.9723232984542847, 'test/num_examples': 10000, 'score': 39307.07882666588, 'total_duration': 40699.153477191925, 'accumulated_submission_time': 39307.07882666588, 'accumulated_eval_time': 1385.2446112632751, 'accumulated_logging_time': 3.0897364616394043}
I0128 07:00:17.842266 140005288683264 logging_writer.py:48] [116770] accumulated_eval_time=1385.244611, accumulated_logging_time=3.089736, accumulated_submission_time=39307.078827, global_step=116770, preemption_count=0, score=39307.078827, test/accuracy=0.592000, test/loss=1.972323, test/num_examples=10000, total_duration=40699.153477, train/accuracy=0.835778, train/loss=0.840769, validation/accuracy=0.721080, validation/loss=1.322500, validation/num_examples=50000
I0128 07:00:28.272892 140005297075968 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.8556292057037354, loss=2.8381662368774414
I0128 07:01:01.790420 140005288683264 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.8369431495666504, loss=2.8737375736236572
I0128 07:01:35.391492 140005297075968 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9610769748687744, loss=2.831439733505249
I0128 07:02:09.001854 140005288683264 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.8158717155456543, loss=2.8308913707733154
I0128 07:02:42.649376 140005297075968 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.839491605758667, loss=2.859041213989258
I0128 07:03:16.283844 140005288683264 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.3585712909698486, loss=2.851864814758301
I0128 07:03:49.915012 140005297075968 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.9417264461517334, loss=2.884403705596924
I0128 07:04:23.525843 140005288683264 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.79583740234375, loss=2.8104569911956787
I0128 07:04:57.253333 140005297075968 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.8850696086883545, loss=2.9087536334991455
I0128 07:05:30.839439 140005288683264 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.928194999694824, loss=2.818906545639038
I0128 07:06:04.404256 140005297075968 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.943774461746216, loss=2.833217144012451
I0128 07:06:37.945389 140005288683264 logging_writer.py:48] [117900] global_step=117900, grad_norm=3.011389970779419, loss=2.8470983505249023
I0128 07:07:11.511199 140005297075968 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.162158489227295, loss=2.8770930767059326
I0128 07:07:45.140752 140005288683264 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.0497913360595703, loss=2.8655149936676025
I0128 07:08:18.775833 140005297075968 logging_writer.py:48] [118200] global_step=118200, grad_norm=3.018737554550171, loss=2.8763670921325684
I0128 07:08:47.852921 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:08:54.210765 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:09:03.092585 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:09:05.621451 140169137129280 submission_runner.py:408] Time since start: 41226.97s, 	Step: 118288, 	{'train/accuracy': 0.8320910334587097, 'train/loss': 0.842000424861908, 'validation/accuracy': 0.7231999635696411, 'validation/loss': 1.3130041360855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.9613901376724243, 'test/num_examples': 10000, 'score': 39817.029213666916, 'total_duration': 41226.97157096863, 'accumulated_submission_time': 39817.029213666916, 'accumulated_eval_time': 1403.0130491256714, 'accumulated_logging_time': 3.1403777599334717}
I0128 07:09:05.667073 140004676327168 logging_writer.py:48] [118288] accumulated_eval_time=1403.013049, accumulated_logging_time=3.140378, accumulated_submission_time=39817.029214, global_step=118288, preemption_count=0, score=39817.029214, test/accuracy=0.598100, test/loss=1.961390, test/num_examples=10000, total_duration=41226.971571, train/accuracy=0.832091, train/loss=0.842000, validation/accuracy=0.723200, validation/loss=1.313004, validation/num_examples=50000
I0128 07:09:10.043020 140005305468672 logging_writer.py:48] [118300] global_step=118300, grad_norm=3.1686336994171143, loss=2.8243353366851807
I0128 07:09:43.641276 140004676327168 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.990333318710327, loss=2.7948334217071533
I0128 07:10:17.241667 140005305468672 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.0721635818481445, loss=2.8477718830108643
I0128 07:10:50.897268 140004676327168 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.8989410400390625, loss=2.794618606567383
I0128 07:11:24.589362 140005305468672 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7193522453308105, loss=2.738173484802246
I0128 07:11:58.171504 140004676327168 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.8203606605529785, loss=2.821535587310791
I0128 07:12:31.746329 140005305468672 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.833890914916992, loss=2.8392245769500732
I0128 07:13:05.302276 140004676327168 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.824535608291626, loss=2.76774263381958
I0128 07:13:38.853743 140005305468672 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.0643985271453857, loss=2.8649606704711914
I0128 07:14:12.451598 140004676327168 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.8427512645721436, loss=2.849856376647949
I0128 07:14:46.073087 140005305468672 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.8886196613311768, loss=2.8051352500915527
I0128 07:15:19.679560 140004676327168 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.9771740436553955, loss=2.8367557525634766
I0128 07:15:53.314182 140005305468672 logging_writer.py:48] [119500] global_step=119500, grad_norm=3.0543625354766846, loss=2.8262171745300293
I0128 07:16:26.925526 140004676327168 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.931166887283325, loss=2.90791654586792
I0128 07:17:00.549248 140005305468672 logging_writer.py:48] [119700] global_step=119700, grad_norm=3.0368006229400635, loss=2.845383882522583
I0128 07:17:34.262560 140004676327168 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.002156972885132, loss=2.843505620956421
I0128 07:17:35.756231 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:17:42.779528 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:17:51.755818 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:17:54.203390 140169137129280 submission_runner.py:408] Time since start: 41755.55s, 	Step: 119806, 	{'train/accuracy': 0.8359375, 'train/loss': 0.8376394510269165, 'validation/accuracy': 0.7249799966812134, 'validation/loss': 1.3017497062683105, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9448192119598389, 'test/num_examples': 10000, 'score': 40327.05902671814, 'total_duration': 41755.553564310074, 'accumulated_submission_time': 40327.05902671814, 'accumulated_eval_time': 1421.460168838501, 'accumulated_logging_time': 3.1964597702026367}
I0128 07:17:54.243356 140004676327168 logging_writer.py:48] [119806] accumulated_eval_time=1421.460169, accumulated_logging_time=3.196460, accumulated_submission_time=40327.059027, global_step=119806, preemption_count=0, score=40327.059027, test/accuracy=0.602500, test/loss=1.944819, test/num_examples=10000, total_duration=41755.553564, train/accuracy=0.835938, train/loss=0.837639, validation/accuracy=0.724980, validation/loss=1.301750, validation/num_examples=50000
I0128 07:18:26.146716 140005288683264 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.890591621398926, loss=2.805485963821411
I0128 07:18:59.773477 140004676327168 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.0175647735595703, loss=2.848529577255249
I0128 07:19:33.405607 140005288683264 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.9455759525299072, loss=2.83229660987854
I0128 07:20:07.052951 140004676327168 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9795100688934326, loss=2.8331117630004883
I0128 07:20:40.714918 140005288683264 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.9711880683898926, loss=2.7917280197143555
I0128 07:21:14.357425 140004676327168 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.109269857406616, loss=2.8318030834198
I0128 07:21:47.994224 140005288683264 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.1257874965667725, loss=2.8094332218170166
I0128 07:22:21.638622 140004676327168 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.847862720489502, loss=2.8234703540802
I0128 07:22:55.258787 140005288683264 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.937364101409912, loss=2.8542895317077637
I0128 07:23:28.864649 140004676327168 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.180645227432251, loss=2.821820020675659
I0128 07:24:02.591108 140005288683264 logging_writer.py:48] [120900] global_step=120900, grad_norm=3.099682331085205, loss=2.901839256286621
I0128 07:24:36.253033 140004676327168 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.0811400413513184, loss=2.809239625930786
I0128 07:25:09.869724 140005288683264 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.0744528770446777, loss=2.83536434173584
I0128 07:25:43.474987 140004676327168 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.8834073543548584, loss=2.7918765544891357
I0128 07:26:17.034696 140005288683264 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.9945731163024902, loss=2.7802205085754395
I0128 07:26:24.228464 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:26:30.476148 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:26:39.130403 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:26:41.674444 140169137129280 submission_runner.py:408] Time since start: 42283.02s, 	Step: 121323, 	{'train/accuracy': 0.8392458558082581, 'train/loss': 0.7929951548576355, 'validation/accuracy': 0.7260000109672546, 'validation/loss': 1.2777409553527832, 'validation/num_examples': 50000, 'test/accuracy': 0.5984000563621521, 'test/loss': 1.9204200506210327, 'test/num_examples': 10000, 'score': 40836.98566198349, 'total_duration': 42283.02461075783, 'accumulated_submission_time': 40836.98566198349, 'accumulated_eval_time': 1438.906097650528, 'accumulated_logging_time': 3.2463817596435547}
I0128 07:26:41.715390 140004659541760 logging_writer.py:48] [121323] accumulated_eval_time=1438.906098, accumulated_logging_time=3.246382, accumulated_submission_time=40836.985662, global_step=121323, preemption_count=0, score=40836.985662, test/accuracy=0.598400, test/loss=1.920420, test/num_examples=10000, total_duration=42283.024611, train/accuracy=0.839246, train/loss=0.792995, validation/accuracy=0.726000, validation/loss=1.277741, validation/num_examples=50000
I0128 07:27:07.908677 140005305468672 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.05741810798645, loss=2.8321728706359863
I0128 07:27:41.544332 140004659541760 logging_writer.py:48] [121500] global_step=121500, grad_norm=3.2336041927337646, loss=2.8807268142700195
I0128 07:28:15.191685 140005305468672 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.118384599685669, loss=2.7836523056030273
I0128 07:28:48.823240 140004659541760 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.2775633335113525, loss=2.8374104499816895
I0128 07:29:22.488464 140005305468672 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.148571252822876, loss=2.897026538848877
I0128 07:29:56.181585 140004659541760 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.88004994392395, loss=2.8071718215942383
I0128 07:30:29.802478 140005305468672 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.848301410675049, loss=2.781512498855591
I0128 07:31:03.443196 140004659541760 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.102738857269287, loss=2.833616256713867
I0128 07:31:37.101982 140005305468672 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.016552209854126, loss=2.7763259410858154
I0128 07:32:10.718994 140004659541760 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.965092420578003, loss=2.8485617637634277
I0128 07:32:44.369788 140005305468672 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.105159282684326, loss=2.7891769409179688
I0128 07:33:18.019037 140004659541760 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.058359384536743, loss=2.880232095718384
I0128 07:33:51.645682 140005305468672 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.966259002685547, loss=2.796255111694336
I0128 07:34:25.282692 140004659541760 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.0995376110076904, loss=2.8383023738861084
I0128 07:34:58.921001 140005305468672 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.051278829574585, loss=2.7832589149475098
I0128 07:35:11.853967 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:35:18.077654 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:35:26.997983 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:35:29.527263 140169137129280 submission_runner.py:408] Time since start: 42810.88s, 	Step: 122840, 	{'train/accuracy': 0.856465220451355, 'train/loss': 0.7566264867782593, 'validation/accuracy': 0.7263199687004089, 'validation/loss': 1.3030215501785278, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9397914409637451, 'test/num_examples': 10000, 'score': 41347.065252542496, 'total_duration': 42810.877432107925, 'accumulated_submission_time': 41347.065252542496, 'accumulated_eval_time': 1456.5793447494507, 'accumulated_logging_time': 3.2976536750793457}
I0128 07:35:29.566398 140004676327168 logging_writer.py:48] [122840] accumulated_eval_time=1456.579345, accumulated_logging_time=3.297654, accumulated_submission_time=41347.065253, global_step=122840, preemption_count=0, score=41347.065253, test/accuracy=0.602700, test/loss=1.939791, test/num_examples=10000, total_duration=42810.877432, train/accuracy=0.856465, train/loss=0.756626, validation/accuracy=0.726320, validation/loss=1.303022, validation/num_examples=50000
I0128 07:35:50.075360 140005288683264 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.157284736633301, loss=2.812831401824951
I0128 07:36:23.750495 140004676327168 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.047569513320923, loss=2.8813586235046387
I0128 07:36:57.376324 140005288683264 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.0849435329437256, loss=2.7343904972076416
I0128 07:37:31.022132 140004676327168 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.927065134048462, loss=2.7401955127716064
I0128 07:38:04.663541 140005288683264 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.9875802993774414, loss=2.7442049980163574
I0128 07:38:38.278647 140004676327168 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.2158429622650146, loss=2.792992115020752
I0128 07:39:11.894462 140005288683264 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.9274470806121826, loss=2.8294076919555664
I0128 07:39:45.528238 140004676327168 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.9925928115844727, loss=2.81146502494812
I0128 07:40:19.177196 140005288683264 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.2681565284729004, loss=2.8251452445983887
I0128 07:40:52.808372 140004676327168 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.09889554977417, loss=2.816974639892578
I0128 07:41:26.432512 140005288683264 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.029940366744995, loss=2.815354347229004
I0128 07:42:00.053822 140004676327168 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.188124418258667, loss=2.8405721187591553
I0128 07:42:33.747898 140005288683264 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.1703670024871826, loss=2.799445629119873
I0128 07:43:07.410214 140004676327168 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.13381028175354, loss=2.8247573375701904
I0128 07:43:41.070399 140005288683264 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.094041585922241, loss=2.825106620788574
I0128 07:43:59.716057 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:44:06.032609 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:44:14.778448 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:44:17.763380 140169137129280 submission_runner.py:408] Time since start: 43339.11s, 	Step: 124357, 	{'train/accuracy': 0.8516820669174194, 'train/loss': 0.7917265295982361, 'validation/accuracy': 0.7299000024795532, 'validation/loss': 1.3059499263763428, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.9366973638534546, 'test/num_examples': 10000, 'score': 41857.15559768677, 'total_duration': 43339.1133646965, 'accumulated_submission_time': 41857.15559768677, 'accumulated_eval_time': 1474.6264395713806, 'accumulated_logging_time': 3.3471579551696777}
I0128 07:44:17.800713 140004667934464 logging_writer.py:48] [124357] accumulated_eval_time=1474.626440, accumulated_logging_time=3.347158, accumulated_submission_time=41857.155598, global_step=124357, preemption_count=0, score=41857.155598, test/accuracy=0.604000, test/loss=1.936697, test/num_examples=10000, total_duration=43339.113365, train/accuracy=0.851682, train/loss=0.791727, validation/accuracy=0.729900, validation/loss=1.305950, validation/num_examples=50000
I0128 07:44:32.582400 140005305468672 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.1111738681793213, loss=2.806647300720215
I0128 07:45:06.103937 140004667934464 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.050894260406494, loss=2.823957920074463
I0128 07:45:39.723453 140005305468672 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.0192031860351562, loss=2.7755484580993652
I0128 07:46:13.340368 140004667934464 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.1197476387023926, loss=2.8690683841705322
I0128 07:46:46.969352 140005305468672 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0946030616760254, loss=2.837747573852539
I0128 07:47:20.628100 140004667934464 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.0901191234588623, loss=2.7969655990600586
I0128 07:47:54.283874 140005305468672 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.343580722808838, loss=2.739577531814575
I0128 07:48:27.945829 140004667934464 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.0644752979278564, loss=2.8484129905700684
I0128 07:49:01.572714 140005305468672 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.3543546199798584, loss=2.8364222049713135
I0128 07:49:35.230011 140004667934464 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.250279188156128, loss=2.857495069503784
I0128 07:50:08.850023 140005305468672 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.0148630142211914, loss=2.8091318607330322
I0128 07:50:42.464164 140004667934464 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.180107593536377, loss=2.851022720336914
I0128 07:51:16.093383 140005305468672 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.185673952102661, loss=2.770566463470459
I0128 07:51:49.753306 140004667934464 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1692442893981934, loss=2.7917368412017822
I0128 07:52:23.379585 140005305468672 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.166490316390991, loss=2.812870502471924
I0128 07:52:48.084707 140169137129280 spec.py:321] Evaluating on the training split.
I0128 07:52:54.443217 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 07:53:03.290747 140169137129280 spec.py:349] Evaluating on the test split.
I0128 07:53:05.812318 140169137129280 submission_runner.py:408] Time since start: 43867.16s, 	Step: 125875, 	{'train/accuracy': 0.8467992544174194, 'train/loss': 0.8255721926689148, 'validation/accuracy': 0.7300599813461304, 'validation/loss': 1.3159984350204468, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.9633678197860718, 'test/num_examples': 10000, 'score': 42367.3814136982, 'total_duration': 43867.16249752045, 'accumulated_submission_time': 42367.3814136982, 'accumulated_eval_time': 1492.354014635086, 'accumulated_logging_time': 3.394263982772827}
I0128 07:53:05.855158 140004676327168 logging_writer.py:48] [125875] accumulated_eval_time=1492.354015, accumulated_logging_time=3.394264, accumulated_submission_time=42367.381414, global_step=125875, preemption_count=0, score=42367.381414, test/accuracy=0.601600, test/loss=1.963368, test/num_examples=10000, total_duration=43867.162498, train/accuracy=0.846799, train/loss=0.825572, validation/accuracy=0.730060, validation/loss=1.315998, validation/num_examples=50000
I0128 07:53:14.608280 140005288683264 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.1259396076202393, loss=2.7916104793548584
I0128 07:53:48.208321 140004676327168 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.0332159996032715, loss=2.747188091278076
I0128 07:54:21.849972 140005288683264 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.4997916221618652, loss=2.797379493713379
I0128 07:54:55.500458 140004676327168 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.180095672607422, loss=2.8515124320983887
I0128 07:55:29.095731 140005288683264 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.8942177295684814, loss=2.7831645011901855
I0128 07:56:02.716793 140004676327168 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.2092373371124268, loss=2.7608354091644287
I0128 07:56:36.360290 140005288683264 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.1690750122070312, loss=2.771834135055542
I0128 07:57:10.032456 140004676327168 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.149625778198242, loss=2.797245502471924
I0128 07:57:43.656856 140005288683264 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.388608694076538, loss=2.7308969497680664
I0128 07:58:17.259737 140004676327168 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.131380558013916, loss=2.7173855304718018
I0128 07:58:50.830526 140005288683264 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.3372254371643066, loss=2.842655658721924
I0128 07:59:24.433909 140004676327168 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.37813138961792, loss=2.8075613975524902
I0128 07:59:58.088306 140005288683264 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.2051005363464355, loss=2.771195411682129
I0128 08:00:31.727254 140004676327168 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.0522968769073486, loss=2.815298557281494
I0128 08:01:05.459751 140005288683264 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.1234447956085205, loss=2.78804612159729
I0128 08:01:35.898491 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:01:42.147831 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:01:51.073729 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:01:53.595085 140169137129280 submission_runner.py:408] Time since start: 44394.95s, 	Step: 127392, 	{'train/accuracy': 0.8463209271430969, 'train/loss': 0.8251389861106873, 'validation/accuracy': 0.7315599918365479, 'validation/loss': 1.3149410486221313, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.9545058012008667, 'test/num_examples': 10000, 'score': 42877.36577987671, 'total_duration': 44394.94526076317, 'accumulated_submission_time': 42877.36577987671, 'accumulated_eval_time': 1510.0505783557892, 'accumulated_logging_time': 3.447601556777954}
I0128 08:01:53.637035 140005305468672 logging_writer.py:48] [127392] accumulated_eval_time=1510.050578, accumulated_logging_time=3.447602, accumulated_submission_time=42877.365780, global_step=127392, preemption_count=0, score=42877.365780, test/accuracy=0.608000, test/loss=1.954506, test/num_examples=10000, total_duration=44394.945261, train/accuracy=0.846321, train/loss=0.825139, validation/accuracy=0.731560, validation/loss=1.314941, validation/num_examples=50000
I0128 08:01:56.671935 140005313861376 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.1834909915924072, loss=2.7306439876556396
I0128 08:02:30.198153 140005305468672 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.217362403869629, loss=2.7383475303649902
I0128 08:03:03.750249 140005313861376 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.1510562896728516, loss=2.7277181148529053
I0128 08:03:37.358292 140005305468672 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.218470335006714, loss=2.7904014587402344
I0128 08:04:11.007468 140005313861376 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.43026065826416, loss=2.8254547119140625
I0128 08:04:44.658126 140005305468672 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.083479404449463, loss=2.7975707054138184
I0128 08:05:18.286081 140005313861376 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.042397975921631, loss=2.7846622467041016
I0128 08:05:51.908568 140005305468672 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.215167284011841, loss=2.756377696990967
I0128 08:06:25.527940 140005313861376 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.222776412963867, loss=2.7780444622039795
I0128 08:06:59.166309 140005305468672 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.398533821105957, loss=2.775376796722412
I0128 08:07:32.873152 140005313861376 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.143770217895508, loss=2.766822099685669
I0128 08:08:06.547548 140005305468672 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.174973964691162, loss=2.703986406326294
I0128 08:08:40.174970 140005313861376 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.06523060798645, loss=2.7885313034057617
I0128 08:09:13.799972 140005305468672 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.0210959911346436, loss=2.755126714706421
I0128 08:09:47.430204 140005313861376 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.3372161388397217, loss=2.7823543548583984
I0128 08:10:21.091080 140005305468672 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.071693181991577, loss=2.804415225982666
I0128 08:10:23.599312 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:10:29.883563 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:10:38.736559 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:10:41.326726 140169137129280 submission_runner.py:408] Time since start: 44922.68s, 	Step: 128909, 	{'train/accuracy': 0.8551897406578064, 'train/loss': 0.7878064513206482, 'validation/accuracy': 0.7348999977111816, 'validation/loss': 1.2902276515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.9117834568023682, 'test/num_examples': 10000, 'score': 43387.268907785416, 'total_duration': 44922.67690491676, 'accumulated_submission_time': 43387.268907785416, 'accumulated_eval_time': 1527.7779560089111, 'accumulated_logging_time': 3.5003559589385986}
I0128 08:10:41.370791 140004667934464 logging_writer.py:48] [128909] accumulated_eval_time=1527.777956, accumulated_logging_time=3.500356, accumulated_submission_time=43387.268908, global_step=128909, preemption_count=0, score=43387.268908, test/accuracy=0.614800, test/loss=1.911783, test/num_examples=10000, total_duration=44922.676905, train/accuracy=0.855190, train/loss=0.787806, validation/accuracy=0.734900, validation/loss=1.290228, validation/num_examples=50000
I0128 08:11:12.310887 140004676327168 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.2177319526672363, loss=2.7474722862243652
I0128 08:11:45.933960 140004667934464 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.2971184253692627, loss=2.7738780975341797
I0128 08:12:19.582071 140004676327168 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.2395050525665283, loss=2.7287373542785645
I0128 08:12:53.175782 140004667934464 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.243785858154297, loss=2.8307225704193115
I0128 08:13:26.827191 140004676327168 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.432926654815674, loss=2.76602840423584
I0128 08:14:00.468938 140004667934464 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.4180595874786377, loss=2.7496724128723145
I0128 08:14:34.075181 140004676327168 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.120835065841675, loss=2.7175581455230713
I0128 08:15:07.721329 140004667934464 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.172758102416992, loss=2.756786823272705
I0128 08:15:41.322961 140004676327168 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.3222804069519043, loss=2.7992849349975586
I0128 08:16:14.956528 140004667934464 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.327389717102051, loss=2.757333517074585
I0128 08:16:48.613495 140004676327168 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.244253396987915, loss=2.7356317043304443
I0128 08:17:22.262827 140004667934464 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.143950939178467, loss=2.676170587539673
I0128 08:17:55.918328 140004676327168 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.2187857627868652, loss=2.7839794158935547
I0128 08:18:29.566300 140004667934464 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.207180976867676, loss=2.7965657711029053
I0128 08:19:03.202323 140004676327168 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.1831066608428955, loss=2.7271695137023926
I0128 08:19:11.422067 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:19:17.701932 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:19:26.732976 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:19:29.285643 140169137129280 submission_runner.py:408] Time since start: 45450.64s, 	Step: 130426, 	{'train/accuracy': 0.8754384517669678, 'train/loss': 0.675972580909729, 'validation/accuracy': 0.7377600073814392, 'validation/loss': 1.2478464841842651, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.8745336532592773, 'test/num_examples': 10000, 'score': 43897.26140189171, 'total_duration': 45450.63581967354, 'accumulated_submission_time': 43897.26140189171, 'accumulated_eval_time': 1545.6414897441864, 'accumulated_logging_time': 3.5544705390930176}
I0128 08:19:29.328207 140004667934464 logging_writer.py:48] [130426] accumulated_eval_time=1545.641490, accumulated_logging_time=3.554471, accumulated_submission_time=43897.261402, global_step=130426, preemption_count=0, score=43897.261402, test/accuracy=0.613500, test/loss=1.874534, test/num_examples=10000, total_duration=45450.635820, train/accuracy=0.875438, train/loss=0.675973, validation/accuracy=0.737760, validation/loss=1.247846, validation/num_examples=50000
I0128 08:19:54.635603 140005313861376 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.4243197441101074, loss=2.8198699951171875
I0128 08:20:28.218240 140004667934464 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.1352975368499756, loss=2.7524220943450928
I0128 08:21:01.801017 140005313861376 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.3774166107177734, loss=2.7062764167785645
I0128 08:21:35.469526 140004667934464 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.340510129928589, loss=2.7243940830230713
I0128 08:22:09.130089 140005313861376 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.2480039596557617, loss=2.80953311920166
I0128 08:22:42.765496 140004667934464 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.057673454284668, loss=2.7327699661254883
I0128 08:23:16.348702 140005313861376 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.2492406368255615, loss=2.7895448207855225
I0128 08:23:49.968177 140004667934464 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.2748374938964844, loss=2.753007650375366
I0128 08:24:23.601831 140005313861376 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.2183783054351807, loss=2.7610907554626465
I0128 08:24:57.251534 140004667934464 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.5519278049468994, loss=2.7707607746124268
I0128 08:25:30.871710 140005313861376 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.3355391025543213, loss=2.8078720569610596
I0128 08:26:04.557672 140004667934464 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.251455545425415, loss=2.739612579345703
I0128 08:26:38.570787 140005313861376 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.2879714965820312, loss=2.7154314517974854
I0128 08:27:12.193150 140004667934464 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.313009738922119, loss=2.764636993408203
I0128 08:27:45.856879 140005313861376 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.1263720989227295, loss=2.6972193717956543
I0128 08:27:59.462948 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:28:05.779423 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:28:14.761507 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:28:17.815933 140169137129280 submission_runner.py:408] Time since start: 45979.17s, 	Step: 131942, 	{'train/accuracy': 0.8747608065605164, 'train/loss': 0.712469220161438, 'validation/accuracy': 0.7366399765014648, 'validation/loss': 1.283627986907959, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.9361129999160767, 'test/num_examples': 10000, 'score': 44407.33543777466, 'total_duration': 45979.166120290756, 'accumulated_submission_time': 44407.33543777466, 'accumulated_eval_time': 1563.9944469928741, 'accumulated_logging_time': 3.6094768047332764}
I0128 08:28:17.851493 140005305468672 logging_writer.py:48] [131942] accumulated_eval_time=1563.994447, accumulated_logging_time=3.609477, accumulated_submission_time=44407.335438, global_step=131942, preemption_count=0, score=44407.335438, test/accuracy=0.610200, test/loss=1.936113, test/num_examples=10000, total_duration=45979.166120, train/accuracy=0.874761, train/loss=0.712469, validation/accuracy=0.736640, validation/loss=1.283628, validation/num_examples=50000
I0128 08:28:37.622594 140005330646784 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.33542799949646, loss=2.739520311355591
I0128 08:29:11.191629 140005305468672 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.135563850402832, loss=2.6728224754333496
I0128 08:29:44.797006 140005330646784 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2934184074401855, loss=2.7301714420318604
I0128 08:30:18.427512 140005305468672 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.353855609893799, loss=2.694654703140259
I0128 08:30:52.027160 140005330646784 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.3364946842193604, loss=2.846693754196167
I0128 08:31:25.626138 140005305468672 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.227130651473999, loss=2.735624313354492
I0128 08:31:59.305932 140005330646784 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.1987624168395996, loss=2.7311947345733643
I0128 08:32:32.876468 140005305468672 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.3754374980926514, loss=2.718093156814575
I0128 08:33:06.514192 140005330646784 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2605538368225098, loss=2.72216796875
I0128 08:33:40.114316 140005305468672 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.6034438610076904, loss=2.8187012672424316
I0128 08:34:13.752619 140005330646784 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.32682466506958, loss=2.7500417232513428
I0128 08:34:47.397719 140005305468672 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.399614095687866, loss=2.7536025047302246
I0128 08:35:21.023393 140005330646784 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.248289108276367, loss=2.765872001647949
I0128 08:35:54.670068 140005305468672 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.3868041038513184, loss=2.739612340927124
I0128 08:36:28.336809 140005330646784 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.0692033767700195, loss=2.691357135772705
I0128 08:36:48.005902 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:36:54.243168 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:37:03.255148 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:37:05.858246 140169137129280 submission_runner.py:408] Time since start: 46507.21s, 	Step: 133460, 	{'train/accuracy': 0.872468888759613, 'train/loss': 0.7190256714820862, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.2690998315811157, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.9060282707214355, 'test/num_examples': 10000, 'score': 44917.43026852608, 'total_duration': 46507.20842504501, 'accumulated_submission_time': 44917.43026852608, 'accumulated_eval_time': 1581.8467528820038, 'accumulated_logging_time': 3.656475305557251}
I0128 08:37:05.901653 140005288683264 logging_writer.py:48] [133460] accumulated_eval_time=1581.846753, accumulated_logging_time=3.656475, accumulated_submission_time=44917.430269, global_step=133460, preemption_count=0, score=44917.430269, test/accuracy=0.617900, test/loss=1.906028, test/num_examples=10000, total_duration=46507.208425, train/accuracy=0.872469, train/loss=0.719026, validation/accuracy=0.737660, validation/loss=1.269100, validation/num_examples=50000
I0128 08:37:19.704989 140005297075968 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.518787145614624, loss=2.7499349117279053
I0128 08:37:53.337914 140005288683264 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.6468076705932617, loss=2.751293659210205
I0128 08:38:27.020057 140005297075968 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.3184356689453125, loss=2.7421939373016357
I0128 08:39:00.581100 140005288683264 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.404151678085327, loss=2.716038942337036
I0128 08:39:34.184225 140005297075968 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.3582146167755127, loss=2.7603797912597656
I0128 08:40:07.835874 140005288683264 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.494251251220703, loss=2.715287923812866
I0128 08:40:41.479247 140005297075968 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.471694231033325, loss=2.7542686462402344
I0128 08:41:15.111598 140005288683264 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.4007818698883057, loss=2.685260772705078
I0128 08:41:48.754657 140005297075968 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.380882501602173, loss=2.7599847316741943
I0128 08:42:22.409028 140005288683264 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.5753860473632812, loss=2.773660898208618
I0128 08:42:56.049139 140005297075968 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.3672356605529785, loss=2.7374014854431152
I0128 08:43:29.690263 140005288683264 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.399451971054077, loss=2.772906541824341
I0128 08:44:03.333900 140005297075968 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.336277961730957, loss=2.7291061878204346
I0128 08:44:37.005926 140005288683264 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.4802064895629883, loss=2.7293217182159424
I0128 08:45:10.630020 140005297075968 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.431739568710327, loss=2.6973443031311035
I0128 08:45:35.966150 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:45:42.296718 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:45:51.073806 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:45:53.574919 140169137129280 submission_runner.py:408] Time since start: 47034.93s, 	Step: 134977, 	{'train/accuracy': 0.8716716766357422, 'train/loss': 0.7018413543701172, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.2499456405639648, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.884137749671936, 'test/num_examples': 10000, 'score': 45427.43486452103, 'total_duration': 47034.92509889603, 'accumulated_submission_time': 45427.43486452103, 'accumulated_eval_time': 1599.4554841518402, 'accumulated_logging_time': 3.710766077041626}
I0128 08:45:53.617632 140004676327168 logging_writer.py:48] [134977] accumulated_eval_time=1599.455484, accumulated_logging_time=3.710766, accumulated_submission_time=45427.434865, global_step=134977, preemption_count=0, score=45427.434865, test/accuracy=0.616700, test/loss=1.884138, test/num_examples=10000, total_duration=47034.925099, train/accuracy=0.871672, train/loss=0.701841, validation/accuracy=0.739340, validation/loss=1.249946, validation/num_examples=50000
I0128 08:46:01.702076 140005305468672 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.3210625648498535, loss=2.7470297813415527
I0128 08:46:35.242913 140004676327168 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.566020965576172, loss=2.787686824798584
I0128 08:47:08.842952 140005305468672 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.486506700515747, loss=2.7095932960510254
I0128 08:47:42.504757 140004676327168 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.2368412017822266, loss=2.6944689750671387
I0128 08:48:16.138549 140005305468672 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.4481570720672607, loss=2.7255375385284424
I0128 08:48:49.782930 140004676327168 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.297427177429199, loss=2.671123504638672
I0128 08:49:23.457544 140005305468672 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.309663772583008, loss=2.715001344680786
I0128 08:49:57.122540 140004676327168 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.330549716949463, loss=2.7911221981048584
I0128 08:50:30.791119 140005305468672 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.2263126373291016, loss=2.738886833190918
I0128 08:51:04.462445 140004676327168 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.3853626251220703, loss=2.7429943084716797
I0128 08:51:38.034776 140005305468672 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.2709896564483643, loss=2.75089430809021
I0128 08:52:11.676787 140004676327168 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.1031603813171387, loss=2.7033164501190186
I0128 08:52:45.317943 140005305468672 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.3527157306671143, loss=2.7181625366210938
I0128 08:53:18.945938 140004676327168 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.3672895431518555, loss=2.7302098274230957
I0128 08:53:52.591439 140005305468672 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.365541458129883, loss=2.7447049617767334
I0128 08:54:23.609642 140169137129280 spec.py:321] Evaluating on the training split.
I0128 08:54:29.905009 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 08:54:38.700595 140169137129280 spec.py:349] Evaluating on the test split.
I0128 08:54:41.231503 140169137129280 submission_runner.py:408] Time since start: 47562.58s, 	Step: 136494, 	{'train/accuracy': 0.8722297549247742, 'train/loss': 0.7077077031135559, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.2572718858718872, 'validation/num_examples': 50000, 'test/accuracy': 0.6141000390052795, 'test/loss': 1.8976376056671143, 'test/num_examples': 10000, 'score': 45937.36474323273, 'total_duration': 47562.581681489944, 'accumulated_submission_time': 45937.36474323273, 'accumulated_eval_time': 1617.0773251056671, 'accumulated_logging_time': 3.766352891921997}
I0128 08:54:41.277752 140004667934464 logging_writer.py:48] [136494] accumulated_eval_time=1617.077325, accumulated_logging_time=3.766353, accumulated_submission_time=45937.364743, global_step=136494, preemption_count=0, score=45937.364743, test/accuracy=0.614100, test/loss=1.897638, test/num_examples=10000, total_duration=47562.581681, train/accuracy=0.872230, train/loss=0.707708, validation/accuracy=0.739340, validation/loss=1.257272, validation/num_examples=50000
I0128 08:54:43.645478 140004676327168 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.416627883911133, loss=2.7331998348236084
I0128 08:55:17.265292 140004667934464 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.5766632556915283, loss=2.726196765899658
I0128 08:55:50.907724 140004676327168 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.248037815093994, loss=2.72186017036438
I0128 08:56:24.557870 140004667934464 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.390650749206543, loss=2.699233055114746
I0128 08:56:58.207849 140004676327168 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.3963828086853027, loss=2.7424654960632324
I0128 08:57:31.767268 140004667934464 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.5523643493652344, loss=2.6968607902526855
I0128 08:58:05.350165 140004676327168 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.623932123184204, loss=2.7195334434509277
I0128 08:58:38.970463 140004667934464 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.4972217082977295, loss=2.7242846488952637
I0128 08:59:12.575913 140004676327168 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.4684975147247314, loss=2.656816244125366
I0128 08:59:46.197844 140004667934464 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.1998281478881836, loss=2.6479270458221436
I0128 09:00:19.817471 140004676327168 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.6254496574401855, loss=2.7805473804473877
I0128 09:00:53.453883 140004667934464 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.3637847900390625, loss=2.676584243774414
I0128 09:01:27.068442 140004676327168 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.618359327316284, loss=2.698057174682617
I0128 09:02:00.720435 140004667934464 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.4954071044921875, loss=2.6862659454345703
I0128 09:02:34.357457 140004676327168 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.459476947784424, loss=2.6872403621673584
I0128 09:03:08.079637 140004667934464 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.559729814529419, loss=2.6933937072753906
I0128 09:03:11.250903 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:03:17.533760 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:03:26.208907 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:03:28.731230 140169137129280 submission_runner.py:408] Time since start: 48090.08s, 	Step: 138011, 	{'train/accuracy': 0.87015700340271, 'train/loss': 0.7002887725830078, 'validation/accuracy': 0.7409999966621399, 'validation/loss': 1.2513980865478516, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8796846866607666, 'test/num_examples': 10000, 'score': 46447.27852892876, 'total_duration': 48090.08140492439, 'accumulated_submission_time': 46447.27852892876, 'accumulated_eval_time': 1634.5576055049896, 'accumulated_logging_time': 3.823425769805908}
I0128 09:03:28.773755 140004667934464 logging_writer.py:48] [138011] accumulated_eval_time=1634.557606, accumulated_logging_time=3.823426, accumulated_submission_time=46447.278529, global_step=138011, preemption_count=0, score=46447.278529, test/accuracy=0.618600, test/loss=1.879685, test/num_examples=10000, total_duration=48090.081405, train/accuracy=0.870157, train/loss=0.700289, validation/accuracy=0.741000, validation/loss=1.251398, validation/num_examples=50000
I0128 09:03:58.999638 140005305468672 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.3533780574798584, loss=2.7146315574645996
I0128 09:04:32.544433 140004667934464 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.212432622909546, loss=2.6777303218841553
I0128 09:05:06.144086 140005305468672 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.3541252613067627, loss=2.663011312484741
I0128 09:05:39.775490 140004667934464 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.319275140762329, loss=2.67806339263916
I0128 09:06:13.426695 140005305468672 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.604170083999634, loss=2.7658350467681885
I0128 09:06:47.005029 140004667934464 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.6418371200561523, loss=2.719067335128784
I0128 09:07:20.577736 140005305468672 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.335832357406616, loss=2.6933603286743164
I0128 09:07:54.234326 140004667934464 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.6284806728363037, loss=2.7190535068511963
I0128 09:08:27.883949 140005305468672 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.54211688041687, loss=2.73880672454834
I0128 09:09:01.507336 140004667934464 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.413825035095215, loss=2.7357983589172363
I0128 09:09:35.154916 140005305468672 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.513408660888672, loss=2.732221841812134
I0128 09:10:08.798187 140004667934464 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.3006417751312256, loss=2.686655044555664
I0128 09:10:42.447552 140005305468672 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.3797996044158936, loss=2.671041250228882
I0128 09:11:16.088652 140004667934464 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.5654296875, loss=2.7545814514160156
I0128 09:11:49.749226 140005305468672 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.339618444442749, loss=2.687039852142334
I0128 09:11:58.995539 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:12:05.343445 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:12:14.228156 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:12:16.924079 140169137129280 submission_runner.py:408] Time since start: 48618.27s, 	Step: 139529, 	{'train/accuracy': 0.8952487111091614, 'train/loss': 0.6294954419136047, 'validation/accuracy': 0.7395399808883667, 'validation/loss': 1.2603678703308105, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.9022327661514282, 'test/num_examples': 10000, 'score': 46957.439453840256, 'total_duration': 48618.27424407005, 'accumulated_submission_time': 46957.439453840256, 'accumulated_eval_time': 1652.4860928058624, 'accumulated_logging_time': 3.877878189086914}
I0128 09:12:16.969091 140004676327168 logging_writer.py:48] [139529] accumulated_eval_time=1652.486093, accumulated_logging_time=3.877878, accumulated_submission_time=46957.439454, global_step=139529, preemption_count=0, score=46957.439454, test/accuracy=0.612600, test/loss=1.902233, test/num_examples=10000, total_duration=48618.274244, train/accuracy=0.895249, train/loss=0.629495, validation/accuracy=0.739540, validation/loss=1.260368, validation/num_examples=50000
I0128 09:12:41.167450 140005288683264 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.6505649089813232, loss=2.7172577381134033
I0128 09:13:14.771661 140004676327168 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.2513818740844727, loss=2.6575047969818115
I0128 09:13:48.429486 140005288683264 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4759867191314697, loss=2.673130512237549
I0128 09:14:22.062471 140004676327168 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.1354403495788574, loss=2.6547775268554688
I0128 09:14:55.699709 140005288683264 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.8418960571289062, loss=2.7041068077087402
I0128 09:15:29.385963 140004676327168 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.446672201156616, loss=2.6974236965179443
I0128 09:16:03.011487 140005288683264 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.642974376678467, loss=2.7066643238067627
I0128 09:16:36.636147 140004676327168 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.724583625793457, loss=2.6976499557495117
I0128 09:17:10.279929 140005288683264 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.6325669288635254, loss=2.693643808364868
I0128 09:17:43.929788 140004676327168 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.4208409786224365, loss=2.6946868896484375
I0128 09:18:17.595996 140005288683264 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.63932728767395, loss=2.7205567359924316
I0128 09:18:51.217048 140004676327168 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.762558937072754, loss=2.681400775909424
I0128 09:19:24.869069 140005288683264 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.3010411262512207, loss=2.6464297771453857
I0128 09:19:58.528737 140004676327168 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.4438700675964355, loss=2.663069248199463
I0128 09:20:32.154995 140005288683264 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.39632511138916, loss=2.606769561767578
I0128 09:20:47.092247 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:20:53.363093 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:21:02.126709 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:21:04.667298 140169137129280 submission_runner.py:408] Time since start: 49146.02s, 	Step: 141046, 	{'train/accuracy': 0.8868981003761292, 'train/loss': 0.6444560289382935, 'validation/accuracy': 0.7418599724769592, 'validation/loss': 1.247159719467163, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.898269772529602, 'test/num_examples': 10000, 'score': 47467.50153207779, 'total_duration': 49146.01747059822, 'accumulated_submission_time': 47467.50153207779, 'accumulated_eval_time': 1670.0611016750336, 'accumulated_logging_time': 3.9352123737335205}
I0128 09:21:04.716781 140004667934464 logging_writer.py:48] [141046] accumulated_eval_time=1670.061102, accumulated_logging_time=3.935212, accumulated_submission_time=47467.501532, global_step=141046, preemption_count=0, score=47467.501532, test/accuracy=0.617300, test/loss=1.898270, test/num_examples=10000, total_duration=49146.017471, train/accuracy=0.886898, train/loss=0.644456, validation/accuracy=0.741860, validation/loss=1.247160, validation/num_examples=50000
I0128 09:21:23.198134 140004676327168 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.7081692218780518, loss=2.692575454711914
I0128 09:21:56.855222 140004667934464 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.379502058029175, loss=2.593698263168335
I0128 09:22:30.483561 140004676327168 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.4173786640167236, loss=2.770205020904541
I0128 09:23:04.106909 140004667934464 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.8427882194519043, loss=2.7288601398468018
I0128 09:23:37.714877 140004676327168 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.6485979557037354, loss=2.7488677501678467
I0128 09:24:11.372396 140004667934464 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.51896333694458, loss=2.7067339420318604
I0128 09:24:45.023481 140004676327168 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.565302848815918, loss=2.680853843688965
I0128 09:25:18.659379 140004667934464 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.686695098876953, loss=2.7157182693481445
I0128 09:25:52.305121 140004676327168 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.674412250518799, loss=2.6662204265594482
I0128 09:26:25.950060 140004667934464 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.862705707550049, loss=2.694521903991699
I0128 09:26:59.584856 140004676327168 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.5915346145629883, loss=2.714108943939209
I0128 09:27:33.222167 140004667934464 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.5510973930358887, loss=2.73283052444458
I0128 09:28:06.909737 140004676327168 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.474149465560913, loss=2.6548049449920654
I0128 09:28:40.462048 140004667934464 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.621166706085205, loss=2.7129733562469482
I0128 09:29:14.004206 140004676327168 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.6540653705596924, loss=2.607834815979004
I0128 09:29:34.956578 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:29:41.205300 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:29:50.181438 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:29:52.785519 140169137129280 submission_runner.py:408] Time since start: 49674.14s, 	Step: 142564, 	{'train/accuracy': 0.8874959945678711, 'train/loss': 0.652847409248352, 'validation/accuracy': 0.741599977016449, 'validation/loss': 1.2515697479248047, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8916077613830566, 'test/num_examples': 10000, 'score': 47977.68201828003, 'total_duration': 49674.135682582855, 'accumulated_submission_time': 47977.68201828003, 'accumulated_eval_time': 1687.8899908065796, 'accumulated_logging_time': 3.9951000213623047}
I0128 09:29:52.831735 140005313861376 logging_writer.py:48] [142564] accumulated_eval_time=1687.889991, accumulated_logging_time=3.995100, accumulated_submission_time=47977.682018, global_step=142564, preemption_count=0, score=47977.682018, test/accuracy=0.618400, test/loss=1.891608, test/num_examples=10000, total_duration=49674.135683, train/accuracy=0.887496, train/loss=0.652847, validation/accuracy=0.741600, validation/loss=1.251570, validation/num_examples=50000
I0128 09:30:05.253019 140005322254080 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.2235679626464844, loss=2.5888261795043945
I0128 09:30:38.778711 140005313861376 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.37345027923584, loss=2.682684898376465
I0128 09:31:12.341138 140005322254080 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.339616537094116, loss=2.611032009124756
I0128 09:31:45.939663 140005313861376 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.4650235176086426, loss=2.70820951461792
I0128 09:32:19.623364 140005322254080 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.64992356300354, loss=2.6949033737182617
I0128 09:32:53.260708 140005313861376 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.635932683944702, loss=2.6760993003845215
I0128 09:33:26.892048 140005322254080 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.611161231994629, loss=2.686441659927368
I0128 09:34:00.534771 140005313861376 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.5347347259521484, loss=2.6794121265411377
I0128 09:34:34.251975 140005322254080 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.7214033603668213, loss=2.6654224395751953
I0128 09:35:07.899876 140005313861376 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.7824859619140625, loss=2.6605186462402344
I0128 09:35:41.563891 140005322254080 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.694873332977295, loss=2.6557517051696777
I0128 09:36:15.183993 140005313861376 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.4705758094787598, loss=2.7062604427337646
I0128 09:36:48.837105 140005322254080 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.5596556663513184, loss=2.6966567039489746
I0128 09:37:22.471846 140005313861376 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.689147472381592, loss=2.6649093627929688
I0128 09:37:56.132192 140005322254080 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.712853193283081, loss=2.7139832973480225
I0128 09:38:22.836892 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:38:29.082501 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:38:37.874884 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:38:40.412330 140169137129280 submission_runner.py:408] Time since start: 50201.76s, 	Step: 144081, 	{'train/accuracy': 0.8844068646430969, 'train/loss': 0.6725389957427979, 'validation/accuracy': 0.7425000071525574, 'validation/loss': 1.2619949579238892, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8994200229644775, 'test/num_examples': 10000, 'score': 48487.624255895615, 'total_duration': 50201.76250863075, 'accumulated_submission_time': 48487.624255895615, 'accumulated_eval_time': 1705.4653916358948, 'accumulated_logging_time': 4.055452346801758}
I0128 09:38:40.456703 140004676327168 logging_writer.py:48] [144081] accumulated_eval_time=1705.465392, accumulated_logging_time=4.055452, accumulated_submission_time=48487.624256, global_step=144081, preemption_count=0, score=48487.624256, test/accuracy=0.618400, test/loss=1.899420, test/num_examples=10000, total_duration=50201.762509, train/accuracy=0.884407, train/loss=0.672539, validation/accuracy=0.742500, validation/loss=1.261995, validation/num_examples=50000
I0128 09:38:47.181607 140005288683264 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.4019217491149902, loss=2.6549766063690186
I0128 09:39:20.815146 140004676327168 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.5218605995178223, loss=2.687697172164917
I0128 09:39:54.465432 140005288683264 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.792306900024414, loss=2.68222975730896
I0128 09:40:28.197930 140004676327168 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.607937812805176, loss=2.6594901084899902
I0128 09:41:01.853468 140005288683264 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.57780122756958, loss=2.688218355178833
I0128 09:41:35.506010 140004676327168 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.3933706283569336, loss=2.627511501312256
I0128 09:42:09.188032 140005288683264 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.8220465183258057, loss=2.700352430343628
I0128 09:42:42.819057 140004676327168 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.772732734680176, loss=2.763450860977173
I0128 09:43:16.485069 140005288683264 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.6240315437316895, loss=2.6769890785217285
I0128 09:43:50.126476 140004676327168 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.669814348220825, loss=2.6628174781799316
I0128 09:44:23.762379 140005288683264 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6056156158447266, loss=2.691392660140991
I0128 09:44:57.431019 140004676327168 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.6938648223876953, loss=2.676166534423828
I0128 09:45:31.060290 140005288683264 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.43648624420166, loss=2.6137094497680664
I0128 09:46:04.707134 140004676327168 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.6477925777435303, loss=2.6450674533843994
I0128 09:46:38.393720 140005288683264 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.7981913089752197, loss=2.677506923675537
I0128 09:47:10.455438 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:47:16.704209 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:47:25.660504 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:47:28.180420 140169137129280 submission_runner.py:408] Time since start: 50729.53s, 	Step: 145597, 	{'train/accuracy': 0.893973171710968, 'train/loss': 0.6204026341438293, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.2134588956832886, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8494324684143066, 'test/num_examples': 10000, 'score': 48997.56184363365, 'total_duration': 50729.53057074547, 'accumulated_submission_time': 48997.56184363365, 'accumulated_eval_time': 1723.1903104782104, 'accumulated_logging_time': 4.112022161483765}
I0128 09:47:28.236699 140005305468672 logging_writer.py:48] [145597] accumulated_eval_time=1723.190310, accumulated_logging_time=4.112022, accumulated_submission_time=48997.561844, global_step=145597, preemption_count=0, score=48997.561844, test/accuracy=0.624700, test/loss=1.849432, test/num_examples=10000, total_duration=50729.530571, train/accuracy=0.893973, train/loss=0.620403, validation/accuracy=0.750500, validation/loss=1.213459, validation/num_examples=50000
I0128 09:47:29.592771 140005313861376 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.662336587905884, loss=2.5910110473632812
I0128 09:48:03.113420 140005305468672 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.4824142456054688, loss=2.6382832527160645
I0128 09:48:36.714672 140005313861376 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.719000816345215, loss=2.6955108642578125
I0128 09:49:10.335217 140005305468672 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.2706735134124756, loss=2.560803174972534
I0128 09:49:44.003043 140005313861376 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.7875540256500244, loss=2.6851699352264404
I0128 09:50:17.627893 140005305468672 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.8899905681610107, loss=2.749222993850708
I0128 09:50:51.250427 140005313861376 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.5865156650543213, loss=2.634352922439575
I0128 09:51:24.929132 140005305468672 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.655334711074829, loss=2.6621272563934326
I0128 09:51:58.538859 140005313861376 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.8330905437469482, loss=2.663334846496582
I0128 09:52:32.165747 140005305468672 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.7128047943115234, loss=2.6331396102905273
I0128 09:53:05.895494 140005313861376 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.825155258178711, loss=2.665299892425537
I0128 09:53:39.556418 140005305468672 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.6220407485961914, loss=2.6338958740234375
I0128 09:54:13.173891 140005313861376 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.627157211303711, loss=2.658143997192383
I0128 09:54:46.801360 140005305468672 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.88342022895813, loss=2.7171270847320557
I0128 09:55:20.441196 140005313861376 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.6912620067596436, loss=2.706022262573242
I0128 09:55:54.099159 140005305468672 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.83793044090271, loss=2.6778006553649902
I0128 09:55:58.282705 140169137129280 spec.py:321] Evaluating on the training split.
I0128 09:56:04.592802 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 09:56:13.328025 140169137129280 spec.py:349] Evaluating on the test split.
I0128 09:56:15.865433 140169137129280 submission_runner.py:408] Time since start: 51257.22s, 	Step: 147114, 	{'train/accuracy': 0.8909637928009033, 'train/loss': 0.6227953433990479, 'validation/accuracy': 0.7475199699401855, 'validation/loss': 1.2216311693191528, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.849046230316162, 'test/num_examples': 10000, 'score': 49507.542610406876, 'total_duration': 51257.21557235718, 'accumulated_submission_time': 49507.542610406876, 'accumulated_eval_time': 1740.7729632854462, 'accumulated_logging_time': 4.185078859329224}
I0128 09:56:15.925440 140004676327168 logging_writer.py:48] [147114] accumulated_eval_time=1740.772963, accumulated_logging_time=4.185079, accumulated_submission_time=49507.542610, global_step=147114, preemption_count=0, score=49507.542610, test/accuracy=0.626100, test/loss=1.849046, test/num_examples=10000, total_duration=51257.215572, train/accuracy=0.890964, train/loss=0.622795, validation/accuracy=0.747520, validation/loss=1.221631, validation/num_examples=50000
I0128 09:56:45.144065 140005288683264 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.725742816925049, loss=2.596531629562378
I0128 09:57:18.702578 140004676327168 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.8757660388946533, loss=2.656494140625
I0128 09:57:52.341841 140005288683264 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.72239089012146, loss=2.700852870941162
I0128 09:58:25.987548 140004676327168 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.663048028945923, loss=2.660916328430176
I0128 09:58:59.690080 140005288683264 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.609628677368164, loss=2.6231703758239746
I0128 09:59:33.243615 140004676327168 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.64052414894104, loss=2.652294397354126
I0128 10:00:06.825221 140005288683264 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.884255886077881, loss=2.6422297954559326
I0128 10:00:40.493125 140004676327168 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.527235269546509, loss=2.64682674407959
I0128 10:01:14.131742 140005288683264 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.743457078933716, loss=2.6157073974609375
I0128 10:01:47.736961 140004676327168 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.673463821411133, loss=2.6651501655578613
I0128 10:02:21.325909 140005288683264 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.9148128032684326, loss=2.6407015323638916
I0128 10:02:54.961819 140004676327168 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.7101943492889404, loss=2.6445934772491455
I0128 10:03:28.599481 140005288683264 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.7275307178497314, loss=2.667895793914795
I0128 10:04:02.240908 140004676327168 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.697063684463501, loss=2.6064016819000244
I0128 10:04:35.890623 140005288683264 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.8355531692504883, loss=2.7036585807800293
I0128 10:04:46.130882 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:04:52.455936 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:05:01.534341 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:05:04.048007 140169137129280 submission_runner.py:408] Time since start: 51785.40s, 	Step: 148632, 	{'train/accuracy': 0.9108936190605164, 'train/loss': 0.5526172518730164, 'validation/accuracy': 0.7495799660682678, 'validation/loss': 1.2158610820770264, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8357360363006592, 'test/num_examples': 10000, 'score': 50017.68539023399, 'total_duration': 51785.398183107376, 'accumulated_submission_time': 50017.68539023399, 'accumulated_eval_time': 1758.6900448799133, 'accumulated_logging_time': 4.2591400146484375}
I0128 10:05:04.092042 140004667934464 logging_writer.py:48] [148632] accumulated_eval_time=1758.690045, accumulated_logging_time=4.259140, accumulated_submission_time=50017.685390, global_step=148632, preemption_count=0, score=50017.685390, test/accuracy=0.632400, test/loss=1.835736, test/num_examples=10000, total_duration=51785.398183, train/accuracy=0.910894, train/loss=0.552617, validation/accuracy=0.749580, validation/loss=1.215861, validation/num_examples=50000
I0128 10:05:27.344473 140004676327168 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.6188957691192627, loss=2.654212236404419
I0128 10:06:00.966438 140004667934464 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.7145092487335205, loss=2.686257839202881
I0128 10:06:34.634945 140004676327168 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.969886302947998, loss=2.6267952919006348
I0128 10:07:08.289144 140004667934464 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.7912678718566895, loss=2.6212456226348877
I0128 10:07:41.932379 140004676327168 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.6774022579193115, loss=2.578200340270996
I0128 10:08:15.581962 140004667934464 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.5027127265930176, loss=2.5697388648986816
I0128 10:08:49.200033 140004676327168 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.7991833686828613, loss=2.637763023376465
I0128 10:09:22.853639 140004667934464 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.755314588546753, loss=2.6547162532806396
I0128 10:09:56.475002 140004676327168 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.9113309383392334, loss=2.653635263442993
I0128 10:10:30.129533 140004667934464 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7420668601989746, loss=2.657209873199463
I0128 10:11:03.756769 140004676327168 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.6392970085144043, loss=2.6134252548217773
I0128 10:11:37.492870 140004667934464 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.899404287338257, loss=2.685835123062134
I0128 10:12:11.161915 140004676327168 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.7018234729766846, loss=2.6307735443115234
I0128 10:12:44.793959 140004667934464 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.8432440757751465, loss=2.692387104034424
I0128 10:13:18.429287 140004676327168 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.7854626178741455, loss=2.6273608207702637
I0128 10:13:34.053658 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:13:40.366644 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:13:49.124031 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:13:51.676314 140169137129280 submission_runner.py:408] Time since start: 52313.03s, 	Step: 150148, 	{'train/accuracy': 0.9068877100944519, 'train/loss': 0.5834668278694153, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.21971595287323, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.8467143774032593, 'test/num_examples': 10000, 'score': 50527.5884706974, 'total_duration': 52313.0264942646, 'accumulated_submission_time': 50527.5884706974, 'accumulated_eval_time': 1776.3126661777496, 'accumulated_logging_time': 4.313075065612793}
I0128 10:13:51.720775 140004676327168 logging_writer.py:48] [150148] accumulated_eval_time=1776.312666, accumulated_logging_time=4.313075, accumulated_submission_time=50527.588471, global_step=150148, preemption_count=0, score=50527.588471, test/accuracy=0.632800, test/loss=1.846714, test/num_examples=10000, total_duration=52313.026494, train/accuracy=0.906888, train/loss=0.583467, validation/accuracy=0.751040, validation/loss=1.219716, validation/num_examples=50000
I0128 10:14:09.517887 140005297075968 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.7620620727539062, loss=2.6727757453918457
I0128 10:14:43.040766 140004676327168 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.8212780952453613, loss=2.6944198608398438
I0128 10:15:16.595056 140005297075968 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.7326865196228027, loss=2.6257741451263428
I0128 10:15:50.204656 140004676327168 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.635157346725464, loss=2.6257054805755615
I0128 10:16:23.871995 140005297075968 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.808748722076416, loss=2.640324115753174
I0128 10:16:57.496926 140004676327168 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.6391587257385254, loss=2.6247870922088623
I0128 10:17:31.144492 140005297075968 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.713968515396118, loss=2.6524550914764404
I0128 10:18:04.869609 140004676327168 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.047852039337158, loss=2.655116081237793
I0128 10:18:38.454019 140005297075968 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.8068528175354004, loss=2.6080756187438965
I0128 10:19:12.124537 140004676327168 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.6661338806152344, loss=2.5987625122070312
I0128 10:19:45.769085 140005297075968 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.496779680252075, loss=2.5923945903778076
I0128 10:20:19.395648 140004676327168 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.6233174800872803, loss=2.5843536853790283
I0128 10:20:52.999034 140005297075968 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.785188913345337, loss=2.6134190559387207
I0128 10:21:26.617525 140004676327168 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.6252212524414062, loss=2.641742467880249
I0128 10:22:00.263033 140005297075968 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.9445574283599854, loss=2.6827709674835205
I0128 10:22:21.928156 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:22:28.251976 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:22:37.140201 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:22:39.661330 140169137129280 submission_runner.py:408] Time since start: 52841.01s, 	Step: 151666, 	{'train/accuracy': 0.9066286683082581, 'train/loss': 0.5731101632118225, 'validation/accuracy': 0.753279983997345, 'validation/loss': 1.204783320426941, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8276084661483765, 'test/num_examples': 10000, 'score': 51037.737275362015, 'total_duration': 52841.011503219604, 'accumulated_submission_time': 51037.737275362015, 'accumulated_eval_time': 1794.0457956790924, 'accumulated_logging_time': 4.367350816726685}
I0128 10:22:39.707646 140004667934464 logging_writer.py:48] [151666] accumulated_eval_time=1794.045796, accumulated_logging_time=4.367351, accumulated_submission_time=51037.737275, global_step=151666, preemption_count=0, score=51037.737275, test/accuracy=0.630600, test/loss=1.827608, test/num_examples=10000, total_duration=52841.011503, train/accuracy=0.906629, train/loss=0.573110, validation/accuracy=0.753280, validation/loss=1.204783, validation/num_examples=50000
I0128 10:22:51.465016 140005313861376 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.7653849124908447, loss=2.6405975818634033
I0128 10:23:25.000972 140004667934464 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.684715509414673, loss=2.619063138961792
I0128 10:23:58.668234 140005313861376 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.7548248767852783, loss=2.604647636413574
I0128 10:24:32.331673 140004667934464 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.582676649093628, loss=2.586509943008423
I0128 10:25:05.990428 140005313861376 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.74709415435791, loss=2.6191680431365967
I0128 10:25:39.648111 140004667934464 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.759920358657837, loss=2.5937623977661133
I0128 10:26:13.314281 140005313861376 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.658076524734497, loss=2.6119544506073
I0128 10:26:46.950711 140004667934464 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.87222957611084, loss=2.5773544311523438
I0128 10:27:20.597248 140005313861376 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.910244941711426, loss=2.571807861328125
I0128 10:27:54.245147 140004667934464 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.356039047241211, loss=2.6382973194122314
I0128 10:28:27.891067 140005313861376 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.7733843326568604, loss=2.6430439949035645
I0128 10:29:01.537060 140004667934464 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.8502752780914307, loss=2.640645980834961
I0128 10:29:35.186127 140005313861376 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.9432811737060547, loss=2.653877019882202
I0128 10:30:08.871292 140004667934464 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.950267791748047, loss=2.622610330581665
I0128 10:30:42.490481 140005313861376 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.5857462882995605, loss=2.6436572074890137
I0128 10:31:09.914171 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:31:16.204738 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:31:25.024278 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:31:27.558468 140169137129280 submission_runner.py:408] Time since start: 53368.91s, 	Step: 153183, 	{'train/accuracy': 0.9112324714660645, 'train/loss': 0.56153404712677, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.1990495920181274, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.8135104179382324, 'test/num_examples': 10000, 'score': 51547.88285636902, 'total_duration': 53368.90864777565, 'accumulated_submission_time': 51547.88285636902, 'accumulated_eval_time': 1811.6900601387024, 'accumulated_logging_time': 4.4254045486450195}
I0128 10:31:27.605293 140004676327168 logging_writer.py:48] [153183] accumulated_eval_time=1811.690060, accumulated_logging_time=4.425405, accumulated_submission_time=51547.882856, global_step=153183, preemption_count=0, score=51547.882856, test/accuracy=0.633900, test/loss=1.813510, test/num_examples=10000, total_duration=53368.908648, train/accuracy=0.911232, train/loss=0.561534, validation/accuracy=0.755040, validation/loss=1.199050, validation/num_examples=50000
I0128 10:31:33.673704 140005288683264 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.990004539489746, loss=2.60364031791687
I0128 10:32:07.247703 140004676327168 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.956925868988037, loss=2.7259888648986816
I0128 10:32:40.902908 140005288683264 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.8059208393096924, loss=2.6199898719787598
I0128 10:33:14.541056 140004676327168 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.0366106033325195, loss=2.6461827754974365
I0128 10:33:48.195068 140005288683264 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.6999197006225586, loss=2.5683043003082275
I0128 10:34:21.819773 140004676327168 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.680417776107788, loss=2.599400520324707
I0128 10:34:55.462061 140005288683264 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.7121174335479736, loss=2.6207871437072754
I0128 10:35:29.087330 140004676327168 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.756894111633301, loss=2.5668344497680664
I0128 10:36:02.711683 140005288683264 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.921945571899414, loss=2.6023576259613037
I0128 10:36:36.351734 140004676327168 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.757296323776245, loss=2.6400952339172363
I0128 10:37:09.955672 140005288683264 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.979942560195923, loss=2.6914772987365723
I0128 10:37:43.530216 140004676327168 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.935168981552124, loss=2.600614547729492
I0128 10:38:17.099676 140005288683264 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.729449987411499, loss=2.603672504425049
I0128 10:38:50.691977 140004676327168 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.8607630729675293, loss=2.603616237640381
I0128 10:39:24.244626 140005288683264 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.849015235900879, loss=2.6175010204315186
I0128 10:39:57.798479 140004676327168 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.0789313316345215, loss=2.681415319442749
I0128 10:39:57.806236 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:40:04.141150 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:40:13.097913 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:40:15.640589 140169137129280 submission_runner.py:408] Time since start: 53896.99s, 	Step: 154701, 	{'train/accuracy': 0.9130659699440002, 'train/loss': 0.5653855800628662, 'validation/accuracy': 0.7539599537849426, 'validation/loss': 1.209978461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.840086817741394, 'test/num_examples': 10000, 'score': 52058.02461576462, 'total_duration': 53896.99075818062, 'accumulated_submission_time': 52058.02461576462, 'accumulated_eval_time': 1829.5243427753448, 'accumulated_logging_time': 4.483033180236816}
I0128 10:40:15.710593 140004667934464 logging_writer.py:48] [154701] accumulated_eval_time=1829.524343, accumulated_logging_time=4.483033, accumulated_submission_time=52058.024616, global_step=154701, preemption_count=0, score=52058.024616, test/accuracy=0.631300, test/loss=1.840087, test/num_examples=10000, total_duration=53896.990758, train/accuracy=0.913066, train/loss=0.565386, validation/accuracy=0.753960, validation/loss=1.209978, validation/num_examples=50000
I0128 10:40:49.333868 140004676327168 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.4204070568084717, loss=2.5542798042297363
I0128 10:41:22.922597 140004667934464 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.9013054370880127, loss=2.6225366592407227
I0128 10:41:56.561422 140004676327168 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.9907960891723633, loss=2.6109328269958496
I0128 10:42:30.292176 140004667934464 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.13007116317749, loss=2.660456418991089
I0128 10:43:03.867032 140004676327168 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.766011953353882, loss=2.6232850551605225
I0128 10:43:37.427407 140004667934464 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.657207489013672, loss=2.5865392684936523
I0128 10:44:11.014747 140004676327168 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.8404994010925293, loss=2.636671304702759
I0128 10:44:44.574033 140004667934464 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.848931312561035, loss=2.5542542934417725
I0128 10:45:18.133095 140004676327168 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.079721927642822, loss=2.603886365890503
I0128 10:45:51.701855 140004667934464 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.7071540355682373, loss=2.6197149753570557
I0128 10:46:25.348107 140004676327168 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.8032355308532715, loss=2.5979578495025635
I0128 10:46:58.987075 140004667934464 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8871920108795166, loss=2.618746042251587
I0128 10:47:32.621657 140004676327168 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.031959533691406, loss=2.6299796104431152
I0128 10:48:06.256425 140004667934464 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.909613847732544, loss=2.6185989379882812
I0128 10:48:39.969363 140004676327168 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.033185005187988, loss=2.6363422870635986
I0128 10:48:45.831687 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:48:52.144122 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:49:01.097862 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:49:03.707333 140169137129280 submission_runner.py:408] Time since start: 54425.06s, 	Step: 156219, 	{'train/accuracy': 0.9151187539100647, 'train/loss': 0.5467893481254578, 'validation/accuracy': 0.7532999515533447, 'validation/loss': 1.2041923999786377, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8272104263305664, 'test/num_examples': 10000, 'score': 52568.0820877552, 'total_duration': 54425.057513952255, 'accumulated_submission_time': 52568.0820877552, 'accumulated_eval_time': 1847.3999516963959, 'accumulated_logging_time': 4.56777286529541}
I0128 10:49:03.752524 140004676327168 logging_writer.py:48] [156219] accumulated_eval_time=1847.399952, accumulated_logging_time=4.567773, accumulated_submission_time=52568.082088, global_step=156219, preemption_count=0, score=52568.082088, test/accuracy=0.629100, test/loss=1.827210, test/num_examples=10000, total_duration=54425.057514, train/accuracy=0.915119, train/loss=0.546789, validation/accuracy=0.753300, validation/loss=1.204192, validation/num_examples=50000
I0128 10:49:31.265416 140005288683264 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.8358376026153564, loss=2.588791608810425
I0128 10:50:04.784453 140004676327168 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.814612627029419, loss=2.60093092918396
I0128 10:50:38.380393 140005288683264 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.919440746307373, loss=2.530292272567749
I0128 10:51:12.026805 140004676327168 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.824544906616211, loss=2.552255392074585
I0128 10:51:45.687703 140005288683264 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.7507715225219727, loss=2.5520944595336914
I0128 10:52:19.329009 140004676327168 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.812227964401245, loss=2.5874407291412354
I0128 10:52:52.973149 140005288683264 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.04416561126709, loss=2.5910000801086426
I0128 10:53:26.618338 140004676327168 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.3580193519592285, loss=2.6583752632141113
I0128 10:54:00.269607 140005288683264 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.866572618484497, loss=2.568110942840576
I0128 10:54:33.916452 140004676327168 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.1591644287109375, loss=2.6263203620910645
I0128 10:55:07.601843 140005288683264 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.004674434661865, loss=2.607802391052246
I0128 10:55:41.237219 140004676327168 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.779468297958374, loss=2.5615382194519043
I0128 10:56:14.897133 140005288683264 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.573904514312744, loss=2.5296473503112793
I0128 10:56:48.558019 140004676327168 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.9763946533203125, loss=2.5910463333129883
I0128 10:57:22.168755 140005288683264 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.699695348739624, loss=2.53450345993042
I0128 10:57:33.740353 140169137129280 spec.py:321] Evaluating on the training split.
I0128 10:57:40.074438 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 10:57:48.962169 140169137129280 spec.py:349] Evaluating on the test split.
I0128 10:57:51.488282 140169137129280 submission_runner.py:408] Time since start: 54952.84s, 	Step: 157736, 	{'train/accuracy': 0.9242864847183228, 'train/loss': 0.5116089582443237, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.1921504735946655, 'validation/num_examples': 50000, 'test/accuracy': 0.6354000568389893, 'test/loss': 1.8231844902038574, 'test/num_examples': 10000, 'score': 53078.011219501495, 'total_duration': 54952.83845996857, 'accumulated_submission_time': 53078.011219501495, 'accumulated_eval_time': 1865.1478443145752, 'accumulated_logging_time': 4.623232364654541}
I0128 10:57:51.535191 140004667934464 logging_writer.py:48] [157736] accumulated_eval_time=1865.147844, accumulated_logging_time=4.623232, accumulated_submission_time=53078.011220, global_step=157736, preemption_count=0, score=53078.011220, test/accuracy=0.635400, test/loss=1.823184, test/num_examples=10000, total_duration=54952.838460, train/accuracy=0.924286, train/loss=0.511609, validation/accuracy=0.754960, validation/loss=1.192150, validation/num_examples=50000
I0128 10:58:13.399129 140004676327168 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.805333375930786, loss=2.6159749031066895
I0128 10:58:46.986737 140004667934464 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.058254718780518, loss=2.6142868995666504
I0128 10:59:20.587767 140004676327168 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.90498948097229, loss=2.6370580196380615
I0128 10:59:54.254548 140004667934464 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.876237154006958, loss=2.6074628829956055
I0128 11:00:27.900793 140004676327168 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.9030392169952393, loss=2.60648250579834
I0128 11:01:01.611344 140004667934464 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.865785598754883, loss=2.553621768951416
I0128 11:01:35.259390 140004676327168 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.8817927837371826, loss=2.54788875579834
I0128 11:02:08.894305 140004667934464 logging_writer.py:48] [158500] global_step=158500, grad_norm=3.4927821159362793, loss=2.56497859954834
I0128 11:02:42.550851 140004676327168 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.7462589740753174, loss=2.582972288131714
I0128 11:03:16.195023 140004667934464 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.072627544403076, loss=2.589477062225342
I0128 11:03:49.820973 140004676327168 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.25231409072876, loss=2.61047101020813
I0128 11:04:23.478659 140004667934464 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.052690505981445, loss=2.648047924041748
I0128 11:04:57.124696 140004676327168 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.797668933868408, loss=2.581402063369751
I0128 11:05:30.759306 140004667934464 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.094944477081299, loss=2.6158499717712402
I0128 11:06:04.394921 140004676327168 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.6165027618408203, loss=2.584840774536133
I0128 11:06:21.697048 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:06:28.580040 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:06:37.387040 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:06:39.906799 140169137129280 submission_runner.py:408] Time since start: 55481.26s, 	Step: 159253, 	{'train/accuracy': 0.9227519035339355, 'train/loss': 0.5066460371017456, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.1838161945343018, 'validation/num_examples': 50000, 'test/accuracy': 0.6379000544548035, 'test/loss': 1.805983066558838, 'test/num_examples': 10000, 'score': 53588.11385130882, 'total_duration': 55481.25697994232, 'accumulated_submission_time': 53588.11385130882, 'accumulated_eval_time': 1883.357558965683, 'accumulated_logging_time': 4.680333614349365}
I0128 11:06:39.954570 140004676327168 logging_writer.py:48] [159253] accumulated_eval_time=1883.357559, accumulated_logging_time=4.680334, accumulated_submission_time=53588.113851, global_step=159253, preemption_count=0, score=53588.113851, test/accuracy=0.637900, test/loss=1.805983, test/num_examples=10000, total_duration=55481.256980, train/accuracy=0.922752, train/loss=0.506646, validation/accuracy=0.756100, validation/loss=1.183816, validation/num_examples=50000
I0128 11:06:56.046057 140005305468672 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.8678395748138428, loss=2.572037935256958
I0128 11:07:29.647809 140004676327168 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.032039642333984, loss=2.579951047897339
I0128 11:08:03.205260 140005305468672 logging_writer.py:48] [159500] global_step=159500, grad_norm=3.975214719772339, loss=2.587592840194702
I0128 11:08:36.753407 140004676327168 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.7553491592407227, loss=2.552950382232666
I0128 11:09:10.316996 140005305468672 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.845552682876587, loss=2.5614171028137207
I0128 11:09:43.886353 140004676327168 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.7247824668884277, loss=2.5606138706207275
I0128 11:10:17.435321 140005305468672 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.762816905975342, loss=2.553697109222412
I0128 11:10:51.000337 140004676327168 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.214920997619629, loss=2.6384036540985107
I0128 11:11:24.593823 140005305468672 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.899759292602539, loss=2.564126491546631
I0128 11:11:58.220954 140004676327168 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.131312847137451, loss=2.6958658695220947
I0128 11:12:31.862186 140005305468672 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.8751208782196045, loss=2.5817432403564453
I0128 11:13:05.521481 140004676327168 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.100247383117676, loss=2.592017650604248
I0128 11:13:39.210005 140005305468672 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.911602735519409, loss=2.578976631164551
I0128 11:14:12.824875 140004676327168 logging_writer.py:48] [160600] global_step=160600, grad_norm=3.9917571544647217, loss=2.625455856323242
I0128 11:14:46.475268 140005305468672 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.452362537384033, loss=2.634894847869873
I0128 11:15:10.190819 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:15:16.525633 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:15:25.210490 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:15:27.741181 140169137129280 submission_runner.py:408] Time since start: 56009.09s, 	Step: 160772, 	{'train/accuracy': 0.9209582209587097, 'train/loss': 0.5264387726783752, 'validation/accuracy': 0.7565599679946899, 'validation/loss': 1.198883295059204, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8179877996444702, 'test/num_examples': 10000, 'score': 54098.28979110718, 'total_duration': 56009.091359615326, 'accumulated_submission_time': 54098.28979110718, 'accumulated_eval_time': 1900.907883644104, 'accumulated_logging_time': 4.739240646362305}
I0128 11:15:27.788781 140004667934464 logging_writer.py:48] [160772] accumulated_eval_time=1900.907884, accumulated_logging_time=4.739241, accumulated_submission_time=54098.289791, global_step=160772, preemption_count=0, score=54098.289791, test/accuracy=0.631500, test/loss=1.817988, test/num_examples=10000, total_duration=56009.091360, train/accuracy=0.920958, train/loss=0.526439, validation/accuracy=0.756560, validation/loss=1.198883, validation/num_examples=50000
I0128 11:15:37.563783 140004676327168 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.062788009643555, loss=2.59377121925354
I0128 11:16:11.171105 140004667934464 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.8004350662231445, loss=2.580544948577881
I0128 11:16:44.813746 140004676327168 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.8665528297424316, loss=2.521153688430786
I0128 11:17:18.463462 140004667934464 logging_writer.py:48] [161100] global_step=161100, grad_norm=3.7469208240509033, loss=2.5665149688720703
I0128 11:17:52.099307 140004676327168 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.048650741577148, loss=2.6103663444519043
I0128 11:18:25.759004 140004667934464 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.9542694091796875, loss=2.603070020675659
I0128 11:18:59.378937 140004676327168 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.6111137866973877, loss=2.544384717941284
I0128 11:19:32.998301 140004667934464 logging_writer.py:48] [161500] global_step=161500, grad_norm=3.8225789070129395, loss=2.5340614318847656
I0128 11:20:06.730825 140004676327168 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.0035505294799805, loss=2.553776264190674
I0128 11:20:40.375175 140004667934464 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.8067328929901123, loss=2.6139938831329346
I0128 11:21:14.026579 140004676327168 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.9720475673675537, loss=2.573585271835327
I0128 11:21:47.667076 140004667934464 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.075021743774414, loss=2.5635569095611572
I0128 11:22:21.301703 140004676327168 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.079229831695557, loss=2.583627223968506
I0128 11:22:54.912326 140004667934464 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.935894727706909, loss=2.529512882232666
I0128 11:23:28.557626 140004676327168 logging_writer.py:48] [162200] global_step=162200, grad_norm=3.928957939147949, loss=2.593698024749756
I0128 11:23:57.987944 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:24:04.269744 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:24:13.200387 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:24:15.743890 140169137129280 submission_runner.py:408] Time since start: 56537.09s, 	Step: 162289, 	{'train/accuracy': 0.9260801672935486, 'train/loss': 0.5113489627838135, 'validation/accuracy': 0.7582199573516846, 'validation/loss': 1.190921425819397, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.8157532215118408, 'test/num_examples': 10000, 'score': 54608.4274187088, 'total_duration': 56537.093678474426, 'accumulated_submission_time': 54608.4274187088, 'accumulated_eval_time': 1918.6634063720703, 'accumulated_logging_time': 4.798632621765137}
I0128 11:24:15.819246 140004667934464 logging_writer.py:48] [162289] accumulated_eval_time=1918.663406, accumulated_logging_time=4.798633, accumulated_submission_time=54608.427419, global_step=162289, preemption_count=0, score=54608.427419, test/accuracy=0.636100, test/loss=1.815753, test/num_examples=10000, total_duration=56537.093678, train/accuracy=0.926080, train/loss=0.511349, validation/accuracy=0.758220, validation/loss=1.190921, validation/num_examples=50000
I0128 11:24:19.911104 140005305468672 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.8982927799224854, loss=2.5793120861053467
I0128 11:24:53.450403 140004667934464 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.10975980758667, loss=2.5951404571533203
I0128 11:25:26.998816 140005305468672 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.0056610107421875, loss=2.5866174697875977
I0128 11:26:00.658318 140004667934464 logging_writer.py:48] [162600] global_step=162600, grad_norm=3.8286166191101074, loss=2.527399778366089
I0128 11:26:34.287765 140005305468672 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.033769607543945, loss=2.6001734733581543
I0128 11:27:07.942245 140004667934464 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.2617411613464355, loss=2.5650625228881836
I0128 11:27:41.557829 140005305468672 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.02584981918335, loss=2.570734977722168
I0128 11:28:15.155738 140004667934464 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.015934467315674, loss=2.5783185958862305
I0128 11:28:48.803188 140005305468672 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.06347131729126, loss=2.543618679046631
I0128 11:29:22.448292 140004667934464 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.850841760635376, loss=2.530891180038452
I0128 11:29:56.092642 140005305468672 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.199178218841553, loss=2.54312801361084
I0128 11:30:29.713741 140004667934464 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.1971211433410645, loss=2.619443893432617
I0128 11:31:03.368048 140005305468672 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.956958770751953, loss=2.5623507499694824
I0128 11:31:36.982764 140004667934464 logging_writer.py:48] [163600] global_step=163600, grad_norm=3.9315648078918457, loss=2.5165116786956787
I0128 11:32:10.698041 140005305468672 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.9504661560058594, loss=2.564074993133545
I0128 11:32:44.357628 140004667934464 logging_writer.py:48] [163800] global_step=163800, grad_norm=3.850320816040039, loss=2.5491557121276855
I0128 11:32:45.856676 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:32:52.143242 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:33:01.174246 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:33:03.791788 140169137129280 submission_runner.py:408] Time since start: 57065.14s, 	Step: 163806, 	{'train/accuracy': 0.9243263602256775, 'train/loss': 0.5113154053688049, 'validation/accuracy': 0.7582599520683289, 'validation/loss': 1.1876296997070312, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.8151981830596924, 'test/num_examples': 10000, 'score': 55118.39876580238, 'total_duration': 57065.14196181297, 'accumulated_submission_time': 55118.39876580238, 'accumulated_eval_time': 1936.598509311676, 'accumulated_logging_time': 4.890327453613281}
I0128 11:33:03.840646 140005313861376 logging_writer.py:48] [163806] accumulated_eval_time=1936.598509, accumulated_logging_time=4.890327, accumulated_submission_time=55118.398766, global_step=163806, preemption_count=0, score=55118.398766, test/accuracy=0.638300, test/loss=1.815198, test/num_examples=10000, total_duration=57065.141962, train/accuracy=0.924326, train/loss=0.511315, validation/accuracy=0.758260, validation/loss=1.187630, validation/num_examples=50000
I0128 11:33:35.759697 140005322254080 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.882814407348633, loss=2.5231432914733887
I0128 11:34:09.377146 140005313861376 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.0189971923828125, loss=2.583099842071533
I0128 11:34:43.002978 140005322254080 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.12020206451416, loss=2.6099326610565186
I0128 11:35:16.651462 140005313861376 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.320630073547363, loss=2.6027944087982178
I0128 11:35:50.273200 140005322254080 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.7479445934295654, loss=2.519916534423828
I0128 11:36:23.915486 140005313861376 logging_writer.py:48] [164400] global_step=164400, grad_norm=3.9966797828674316, loss=2.591613531112671
I0128 11:36:57.548031 140005322254080 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.950153112411499, loss=2.4927244186401367
I0128 11:37:31.168457 140005313861376 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.136724472045898, loss=2.542349338531494
I0128 11:38:04.807812 140005322254080 logging_writer.py:48] [164700] global_step=164700, grad_norm=3.9128658771514893, loss=2.5991389751434326
I0128 11:38:38.543028 140005313861376 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.143601894378662, loss=2.6242785453796387
I0128 11:39:12.202434 140005322254080 logging_writer.py:48] [164900] global_step=164900, grad_norm=3.9938840866088867, loss=2.5608105659484863
I0128 11:39:45.867566 140005313861376 logging_writer.py:48] [165000] global_step=165000, grad_norm=3.9364469051361084, loss=2.5565450191497803
I0128 11:40:19.512358 140005322254080 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.172860145568848, loss=2.596151113510132
I0128 11:40:53.148972 140005313861376 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.220911979675293, loss=2.501152992248535
I0128 11:41:26.781062 140005322254080 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.088166236877441, loss=2.6062684059143066
I0128 11:41:33.993571 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:41:40.257432 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:41:49.183757 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:41:51.675004 140169137129280 submission_runner.py:408] Time since start: 57593.03s, 	Step: 165323, 	{'train/accuracy': 0.9312419891357422, 'train/loss': 0.4898253381252289, 'validation/accuracy': 0.7599200010299683, 'validation/loss': 1.1844065189361572, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.8147510290145874, 'test/num_examples': 10000, 'score': 55628.49181294441, 'total_duration': 57593.02518582344, 'accumulated_submission_time': 55628.49181294441, 'accumulated_eval_time': 1954.2799079418182, 'accumulated_logging_time': 4.950265169143677}
I0128 11:41:51.722361 140004676327168 logging_writer.py:48] [165323] accumulated_eval_time=1954.279908, accumulated_logging_time=4.950265, accumulated_submission_time=55628.491813, global_step=165323, preemption_count=0, score=55628.491813, test/accuracy=0.638000, test/loss=1.814751, test/num_examples=10000, total_duration=57593.025186, train/accuracy=0.931242, train/loss=0.489825, validation/accuracy=0.759920, validation/loss=1.184407, validation/num_examples=50000
I0128 11:42:17.974770 140005288683264 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.982105255126953, loss=2.562645196914673
I0128 11:42:51.575969 140004676327168 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.133636951446533, loss=2.5577216148376465
I0128 11:43:25.212489 140005288683264 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.1648478507995605, loss=2.532356023788452
I0128 11:43:58.854649 140004676327168 logging_writer.py:48] [165700] global_step=165700, grad_norm=3.930744171142578, loss=2.523679494857788
I0128 11:44:32.576729 140005288683264 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.1601996421813965, loss=2.5493814945220947
I0128 11:45:06.159838 140004676327168 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.195288181304932, loss=2.581106185913086
I0128 11:45:39.828468 140005288683264 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.1149582862854, loss=2.589613437652588
I0128 11:46:13.477803 140004676327168 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.9420464038848877, loss=2.4895756244659424
I0128 11:46:47.124632 140005288683264 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.1675238609313965, loss=2.5728743076324463
I0128 11:47:20.763845 140004676327168 logging_writer.py:48] [166300] global_step=166300, grad_norm=3.7945802211761475, loss=2.512108564376831
I0128 11:47:54.405947 140005288683264 logging_writer.py:48] [166400] global_step=166400, grad_norm=3.928179979324341, loss=2.5465219020843506
I0128 11:48:28.056563 140004676327168 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.070262432098389, loss=2.5501248836517334
I0128 11:49:01.706897 140005288683264 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.9739863872528076, loss=2.5694804191589355
I0128 11:49:35.345375 140004676327168 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.344461441040039, loss=2.5701732635498047
I0128 11:50:09.005144 140005288683264 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.4433393478393555, loss=2.5635805130004883
I0128 11:50:21.937414 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:50:28.229593 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:50:37.101046 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:50:39.603478 140169137129280 submission_runner.py:408] Time since start: 58120.95s, 	Step: 166840, 	{'train/accuracy': 0.9327367544174194, 'train/loss': 0.4756486117839813, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 1.1765327453613281, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.793910026550293, 'test/num_examples': 10000, 'score': 56138.64579749107, 'total_duration': 58120.9536485672, 'accumulated_submission_time': 56138.64579749107, 'accumulated_eval_time': 1971.945927143097, 'accumulated_logging_time': 5.009655952453613}
I0128 11:50:39.652864 140004659541760 logging_writer.py:48] [166840] accumulated_eval_time=1971.945927, accumulated_logging_time=5.009656, accumulated_submission_time=56138.645797, global_step=166840, preemption_count=0, score=56138.645797, test/accuracy=0.638900, test/loss=1.793910, test/num_examples=10000, total_duration=58120.953649, train/accuracy=0.932737, train/loss=0.475649, validation/accuracy=0.760320, validation/loss=1.176533, validation/num_examples=50000
I0128 11:51:00.221961 140004667934464 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.0841593742370605, loss=2.535264253616333
I0128 11:51:33.840366 140004659541760 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.181534767150879, loss=2.5556015968322754
I0128 11:52:07.493897 140004667934464 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.0854973793029785, loss=2.544243812561035
I0128 11:52:41.114166 140004659541760 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.0264787673950195, loss=2.548912525177002
I0128 11:53:14.750142 140004667934464 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.251170635223389, loss=2.6103811264038086
I0128 11:53:48.349418 140004659541760 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.8425891399383545, loss=2.5173566341400146
I0128 11:54:21.971879 140004667934464 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.281256675720215, loss=2.4913666248321533
I0128 11:54:55.602835 140004659541760 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.315849304199219, loss=2.5730040073394775
I0128 11:55:29.224237 140004667934464 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.195032596588135, loss=2.555882453918457
I0128 11:56:02.877363 140004659541760 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.2136335372924805, loss=2.5841686725616455
I0128 11:56:36.531830 140004667934464 logging_writer.py:48] [167900] global_step=167900, grad_norm=3.937075138092041, loss=2.5819849967956543
I0128 11:57:10.234417 140004659541760 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.7877607345581055, loss=2.5841777324676514
I0128 11:57:43.820002 140004667934464 logging_writer.py:48] [168100] global_step=168100, grad_norm=3.9750096797943115, loss=2.522624969482422
I0128 11:58:17.469456 140004659541760 logging_writer.py:48] [168200] global_step=168200, grad_norm=3.8544375896453857, loss=2.5650475025177
I0128 11:58:51.116570 140004667934464 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.011354446411133, loss=2.5608041286468506
I0128 11:59:09.785138 140169137129280 spec.py:321] Evaluating on the training split.
I0128 11:59:16.049000 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 11:59:24.980697 140169137129280 spec.py:349] Evaluating on the test split.
I0128 11:59:27.591269 140169137129280 submission_runner.py:408] Time since start: 58648.94s, 	Step: 168357, 	{'train/accuracy': 0.9333944320678711, 'train/loss': 0.47956064343452454, 'validation/accuracy': 0.7605199813842773, 'validation/loss': 1.178816318511963, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.8000293970108032, 'test/num_examples': 10000, 'score': 56648.71825623512, 'total_duration': 58648.94144535065, 'accumulated_submission_time': 56648.71825623512, 'accumulated_eval_time': 1989.7520174980164, 'accumulated_logging_time': 5.06985878944397}
I0128 11:59:27.641664 140005297075968 logging_writer.py:48] [168357] accumulated_eval_time=1989.752017, accumulated_logging_time=5.069859, accumulated_submission_time=56648.718256, global_step=168357, preemption_count=0, score=56648.718256, test/accuracy=0.642100, test/loss=1.800029, test/num_examples=10000, total_duration=58648.941445, train/accuracy=0.933394, train/loss=0.479561, validation/accuracy=0.760520, validation/loss=1.178816, validation/num_examples=50000
I0128 11:59:42.417842 140005305468672 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.9363622665405273, loss=2.5112500190734863
I0128 12:00:15.952255 140005297075968 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.926016092300415, loss=2.5469090938568115
I0128 12:00:49.565536 140005305468672 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.224117279052734, loss=2.574368953704834
I0128 12:01:23.205922 140005297075968 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.913404703140259, loss=2.4555180072784424
I0128 12:01:56.822391 140005305468672 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.006505966186523, loss=2.523587703704834
I0128 12:02:30.471596 140005297075968 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.021744728088379, loss=2.5889573097229004
I0128 12:03:04.090508 140005305468672 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.979975700378418, loss=2.520315647125244
I0128 12:03:37.787951 140005297075968 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.173433303833008, loss=2.539747476577759
I0128 12:04:11.371237 140005305468672 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.384987831115723, loss=2.5896668434143066
I0128 12:04:45.032204 140005297075968 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.955223798751831, loss=2.5248570442199707
I0128 12:05:18.677897 140005305468672 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.212605953216553, loss=2.5306015014648438
I0128 12:05:52.314289 140005297075968 logging_writer.py:48] [169500] global_step=169500, grad_norm=3.840796947479248, loss=2.498962879180908
I0128 12:06:25.975985 140005305468672 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.116570472717285, loss=2.6021556854248047
I0128 12:06:59.617581 140005297075968 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.186625003814697, loss=2.5278215408325195
I0128 12:07:33.232179 140005305468672 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.5251264572143555, loss=2.6394877433776855
I0128 12:07:57.603970 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:08:03.939121 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:08:12.966197 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:08:15.509856 140169137129280 submission_runner.py:408] Time since start: 59176.86s, 	Step: 169874, 	{'train/accuracy': 0.9356465339660645, 'train/loss': 0.46728038787841797, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 1.1741501092910767, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.791094422340393, 'test/num_examples': 10000, 'score': 57158.62016701698, 'total_duration': 59176.86003422737, 'accumulated_submission_time': 57158.62016701698, 'accumulated_eval_time': 2007.657867193222, 'accumulated_logging_time': 5.131593704223633}
I0128 12:08:15.556509 140004659541760 logging_writer.py:48] [169874] accumulated_eval_time=2007.657867, accumulated_logging_time=5.131594, accumulated_submission_time=57158.620167, global_step=169874, preemption_count=0, score=57158.620167, test/accuracy=0.642900, test/loss=1.791094, test/num_examples=10000, total_duration=59176.860034, train/accuracy=0.935647, train/loss=0.467280, validation/accuracy=0.762040, validation/loss=1.174150, validation/num_examples=50000
I0128 12:08:24.715862 140004667934464 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.681483030319214, loss=2.544114589691162
I0128 12:08:58.318199 140004659541760 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.05866003036499, loss=2.560075044631958
I0128 12:09:31.983964 140004667934464 logging_writer.py:48] [170100] global_step=170100, grad_norm=3.866203546524048, loss=2.532404899597168
I0128 12:10:05.619645 140004659541760 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.088224411010742, loss=2.4854793548583984
I0128 12:10:39.237749 140004667934464 logging_writer.py:48] [170300] global_step=170300, grad_norm=3.977106809616089, loss=2.5679128170013428
I0128 12:11:12.891973 140004659541760 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.112165927886963, loss=2.496164321899414
I0128 12:11:46.513435 140004667934464 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.378686428070068, loss=2.564448118209839
I0128 12:12:20.155107 140004659541760 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.065788269042969, loss=2.579652786254883
I0128 12:12:53.780358 140004667934464 logging_writer.py:48] [170700] global_step=170700, grad_norm=3.9842634201049805, loss=2.522099256515503
I0128 12:13:27.438087 140004659541760 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.342705249786377, loss=2.5698232650756836
I0128 12:14:01.070293 140004667934464 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.190881729125977, loss=2.5028820037841797
I0128 12:14:34.727087 140004659541760 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.613682270050049, loss=2.54797625541687
I0128 12:15:08.366498 140004667934464 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.1248650550842285, loss=2.5435543060302734
I0128 12:15:42.089011 140004659541760 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.9173502922058105, loss=2.536099433898926
I0128 12:16:15.741815 140004667934464 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.066854476928711, loss=2.501584768295288
I0128 12:16:45.824645 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:16:52.064985 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:17:00.679965 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:17:03.241658 140169137129280 submission_runner.py:408] Time since start: 59704.59s, 	Step: 171391, 	{'train/accuracy': 0.9360251426696777, 'train/loss': 0.4715806841850281, 'validation/accuracy': 0.7620599865913391, 'validation/loss': 1.177618145942688, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.793129324913025, 'test/num_examples': 10000, 'score': 57668.82580900192, 'total_duration': 59704.59183359146, 'accumulated_submission_time': 57668.82580900192, 'accumulated_eval_time': 2025.0748419761658, 'accumulated_logging_time': 5.192117214202881}
I0128 12:17:03.292112 140005322254080 logging_writer.py:48] [171391] accumulated_eval_time=2025.074842, accumulated_logging_time=5.192117, accumulated_submission_time=57668.825809, global_step=171391, preemption_count=0, score=57668.825809, test/accuracy=0.644900, test/loss=1.793129, test/num_examples=10000, total_duration=59704.591834, train/accuracy=0.936025, train/loss=0.471581, validation/accuracy=0.762060, validation/loss=1.177618, validation/num_examples=50000
I0128 12:17:06.659530 140005330646784 logging_writer.py:48] [171400] global_step=171400, grad_norm=3.9937853813171387, loss=2.510739326477051
I0128 12:17:40.212860 140005322254080 logging_writer.py:48] [171500] global_step=171500, grad_norm=3.975762128829956, loss=2.48256254196167
I0128 12:18:13.779982 140005330646784 logging_writer.py:48] [171600] global_step=171600, grad_norm=3.844722270965576, loss=2.4736480712890625
I0128 12:18:47.357102 140005322254080 logging_writer.py:48] [171700] global_step=171700, grad_norm=3.9654598236083984, loss=2.54595947265625
I0128 12:19:21.027056 140005330646784 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.218147277832031, loss=2.5364670753479004
I0128 12:19:54.673613 140005322254080 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.118084907531738, loss=2.555119514465332
I0128 12:20:28.308612 140005330646784 logging_writer.py:48] [172000] global_step=172000, grad_norm=3.742504835128784, loss=2.479810953140259
I0128 12:21:01.959209 140005322254080 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.23632287979126, loss=2.5257766246795654
I0128 12:21:35.613049 140005330646784 logging_writer.py:48] [172200] global_step=172200, grad_norm=3.7790093421936035, loss=2.4806113243103027
I0128 12:22:09.341463 140005322254080 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.11366081237793, loss=2.5860297679901123
I0128 12:22:43.005516 140005330646784 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.147831439971924, loss=2.569269895553589
I0128 12:23:16.639237 140005322254080 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.034887790679932, loss=2.5134623050689697
I0128 12:23:50.296148 140005330646784 logging_writer.py:48] [172600] global_step=172600, grad_norm=3.9909627437591553, loss=2.5228469371795654
I0128 12:24:23.926943 140005322254080 logging_writer.py:48] [172700] global_step=172700, grad_norm=3.9569382667541504, loss=2.524397134780884
I0128 12:24:57.531898 140005330646784 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.067848205566406, loss=2.4764387607574463
I0128 12:25:31.198066 140005322254080 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.022931098937988, loss=2.483180522918701
I0128 12:25:33.368157 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:25:39.675135 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:25:48.650729 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:25:51.159816 140169137129280 submission_runner.py:408] Time since start: 60232.51s, 	Step: 172908, 	{'train/accuracy': 0.935546875, 'train/loss': 0.47176775336265564, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.175244688987732, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.8016676902770996, 'test/num_examples': 10000, 'score': 58178.83932805061, 'total_duration': 60232.509991168976, 'accumulated_submission_time': 58178.83932805061, 'accumulated_eval_time': 2042.8664588928223, 'accumulated_logging_time': 5.2558934688568115}
I0128 12:25:51.214763 140004676327168 logging_writer.py:48] [172908] accumulated_eval_time=2042.866459, accumulated_logging_time=5.255893, accumulated_submission_time=58178.839328, global_step=172908, preemption_count=0, score=58178.839328, test/accuracy=0.640500, test/loss=1.801668, test/num_examples=10000, total_duration=60232.509991, train/accuracy=0.935547, train/loss=0.471768, validation/accuracy=0.762580, validation/loss=1.175245, validation/num_examples=50000
I0128 12:26:22.432186 140005288683264 logging_writer.py:48] [173000] global_step=173000, grad_norm=3.935152769088745, loss=2.5554356575012207
I0128 12:26:56.034071 140004676327168 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.043645858764648, loss=2.527275800704956
I0128 12:27:29.693819 140005288683264 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.1325578689575195, loss=2.54679274559021
I0128 12:28:03.337398 140004676327168 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.057729721069336, loss=2.501054525375366
I0128 12:28:37.043645 140005288683264 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.161959171295166, loss=2.593047618865967
I0128 12:29:10.715800 140004676327168 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.164402008056641, loss=2.5210747718811035
I0128 12:29:44.347053 140005288683264 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.286489963531494, loss=2.5431177616119385
I0128 12:30:18.020699 140004676327168 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.109203338623047, loss=2.5497922897338867
I0128 12:30:51.684632 140005288683264 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.26995325088501, loss=2.5424141883850098
I0128 12:31:25.332849 140004676327168 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.221042156219482, loss=2.527277946472168
I0128 12:31:58.967143 140005288683264 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.164742469787598, loss=2.4884400367736816
I0128 12:32:32.625218 140004676327168 logging_writer.py:48] [174100] global_step=174100, grad_norm=3.950319766998291, loss=2.511744976043701
I0128 12:33:06.277586 140005288683264 logging_writer.py:48] [174200] global_step=174200, grad_norm=3.9499237537384033, loss=2.5002036094665527
I0128 12:33:39.943780 140004676327168 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.166147232055664, loss=2.566570520401001
I0128 12:34:13.584511 140005288683264 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.5142998695373535, loss=2.570255756378174
I0128 12:34:21.211036 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:34:27.444172 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:34:36.378544 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:34:38.923916 140169137129280 submission_runner.py:408] Time since start: 60760.27s, 	Step: 174424, 	{'train/accuracy': 0.9393534660339355, 'train/loss': 0.45975035429000854, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.1763155460357666, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.7961137294769287, 'test/num_examples': 10000, 'score': 58688.776404857635, 'total_duration': 60760.27409243584, 'accumulated_submission_time': 58688.776404857635, 'accumulated_eval_time': 2060.579292535782, 'accumulated_logging_time': 5.320931911468506}
I0128 12:34:38.974152 140005313861376 logging_writer.py:48] [174424] accumulated_eval_time=2060.579293, accumulated_logging_time=5.320932, accumulated_submission_time=58688.776405, global_step=174424, preemption_count=0, score=58688.776405, test/accuracy=0.644400, test/loss=1.796114, test/num_examples=10000, total_duration=60760.274092, train/accuracy=0.939353, train/loss=0.459750, validation/accuracy=0.762860, validation/loss=1.176316, validation/num_examples=50000
I0128 12:35:04.911010 140005322254080 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.295247554779053, loss=2.5267865657806396
I0128 12:35:38.553355 140005313861376 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.143806457519531, loss=2.5394418239593506
I0128 12:36:12.182169 140005322254080 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.015923023223877, loss=2.529360055923462
I0128 12:36:45.792265 140005313861376 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.321237564086914, loss=2.5301897525787354
I0128 12:37:19.412360 140005322254080 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.31646728515625, loss=2.5101206302642822
I0128 12:37:53.066554 140005313861376 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.244491100311279, loss=2.5743706226348877
I0128 12:38:26.707136 140005322254080 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.19503116607666, loss=2.550128936767578
I0128 12:39:00.346544 140005313861376 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.244212627410889, loss=2.58109188079834
I0128 12:39:34.017438 140005322254080 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.014283180236816, loss=2.5171775817871094
I0128 12:40:07.640909 140005313861376 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.300994873046875, loss=2.552537679672241
I0128 12:40:41.348039 140005322254080 logging_writer.py:48] [175500] global_step=175500, grad_norm=3.9014294147491455, loss=2.517024040222168
I0128 12:41:14.926386 140005313861376 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.117464065551758, loss=2.5496182441711426
I0128 12:41:48.602863 140005322254080 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.3204569816589355, loss=2.576843738555908
I0128 12:42:22.229815 140005313861376 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.29986047744751, loss=2.5849685668945312
I0128 12:42:55.879161 140005322254080 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.380406856536865, loss=2.5707807540893555
I0128 12:43:09.134462 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:43:15.426549 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:43:24.335315 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:43:26.872963 140169137129280 submission_runner.py:408] Time since start: 61288.22s, 	Step: 175941, 	{'train/accuracy': 0.9392338991165161, 'train/loss': 0.45668110251426697, 'validation/accuracy': 0.7627999782562256, 'validation/loss': 1.1772570610046387, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.7967631816864014, 'test/num_examples': 10000, 'score': 59198.87612986565, 'total_duration': 61288.2229487896, 'accumulated_submission_time': 59198.87612986565, 'accumulated_eval_time': 2078.3175649642944, 'accumulated_logging_time': 5.382373809814453}
I0128 12:43:26.919321 140004676327168 logging_writer.py:48] [175941] accumulated_eval_time=2078.317565, accumulated_logging_time=5.382374, accumulated_submission_time=59198.876130, global_step=175941, preemption_count=0, score=59198.876130, test/accuracy=0.643400, test/loss=1.796763, test/num_examples=10000, total_duration=61288.222949, train/accuracy=0.939234, train/loss=0.456681, validation/accuracy=0.762800, validation/loss=1.177257, validation/num_examples=50000
I0128 12:43:47.101587 140005288683264 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.069498538970947, loss=2.479560613632202
I0128 12:44:20.715346 140004676327168 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.267848014831543, loss=2.5188872814178467
I0128 12:44:54.295504 140005288683264 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.212778091430664, loss=2.5580849647521973
I0128 12:45:27.940683 140004676327168 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.125288963317871, loss=2.5236856937408447
I0128 12:46:01.620204 140005288683264 logging_writer.py:48] [176400] global_step=176400, grad_norm=3.8838255405426025, loss=2.5040082931518555
I0128 12:46:35.258829 140004676327168 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.135250568389893, loss=2.5240042209625244
I0128 12:47:08.952344 140005288683264 logging_writer.py:48] [176600] global_step=176600, grad_norm=3.9865825176239014, loss=2.4778387546539307
I0128 12:47:42.541352 140004676327168 logging_writer.py:48] [176700] global_step=176700, grad_norm=3.8071436882019043, loss=2.5074827671051025
I0128 12:48:16.189121 140005288683264 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.028106689453125, loss=2.540254592895508
I0128 12:48:49.769768 140004676327168 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.188145637512207, loss=2.5785300731658936
I0128 12:49:23.417853 140005288683264 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.140318393707275, loss=2.5467419624328613
I0128 12:49:57.086609 140004676327168 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.305185794830322, loss=2.497842788696289
I0128 12:50:30.745922 140005288683264 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.272976398468018, loss=2.5381720066070557
I0128 12:51:04.417056 140004676327168 logging_writer.py:48] [177300] global_step=177300, grad_norm=3.9236435890197754, loss=2.5278213024139404
I0128 12:51:38.003938 140005288683264 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.307807445526123, loss=2.5408992767333984
I0128 12:51:56.984325 140169137129280 spec.py:321] Evaluating on the training split.
I0128 12:52:03.315165 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 12:52:11.950522 140169137129280 spec.py:349] Evaluating on the test split.
I0128 12:52:14.487900 140169137129280 submission_runner.py:408] Time since start: 61815.84s, 	Step: 177458, 	{'train/accuracy': 0.9397919178009033, 'train/loss': 0.45647042989730835, 'validation/accuracy': 0.7625600099563599, 'validation/loss': 1.1731715202331543, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7910445928573608, 'test/num_examples': 10000, 'score': 59708.882165670395, 'total_duration': 61815.838076114655, 'accumulated_submission_time': 59708.882165670395, 'accumulated_eval_time': 2095.8211035728455, 'accumulated_logging_time': 5.4390709400177}
I0128 12:52:14.539847 140005305468672 logging_writer.py:48] [177458] accumulated_eval_time=2095.821104, accumulated_logging_time=5.439071, accumulated_submission_time=59708.882166, global_step=177458, preemption_count=0, score=59708.882166, test/accuracy=0.645200, test/loss=1.791045, test/num_examples=10000, total_duration=61815.838076, train/accuracy=0.939792, train/loss=0.456470, validation/accuracy=0.762560, validation/loss=1.173172, validation/num_examples=50000
I0128 12:52:28.964070 140005313861376 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.199911594390869, loss=2.526968479156494
I0128 12:53:02.520143 140005305468672 logging_writer.py:48] [177600] global_step=177600, grad_norm=3.948970079421997, loss=2.511887550354004
I0128 12:53:36.252670 140005313861376 logging_writer.py:48] [177700] global_step=177700, grad_norm=3.9718990325927734, loss=2.5309226512908936
I0128 12:54:09.890234 140005305468672 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.225193023681641, loss=2.5561366081237793
I0128 12:54:43.517350 140005313861376 logging_writer.py:48] [177900] global_step=177900, grad_norm=3.949514150619507, loss=2.5449204444885254
I0128 12:55:17.131762 140005305468672 logging_writer.py:48] [178000] global_step=178000, grad_norm=3.992894172668457, loss=2.535257577896118
I0128 12:55:50.768655 140005313861376 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.123057842254639, loss=2.5237865447998047
I0128 12:56:24.384943 140005305468672 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.2093353271484375, loss=2.528700828552246
I0128 12:56:57.972135 140005313861376 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.365554332733154, loss=2.5383172035217285
I0128 12:57:31.642959 140005305468672 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.194994926452637, loss=2.472724437713623
I0128 12:58:05.284474 140005313861376 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.315038681030273, loss=2.551553964614868
I0128 12:58:38.876806 140005305468672 logging_writer.py:48] [178600] global_step=178600, grad_norm=3.7717502117156982, loss=2.4589617252349854
I0128 12:59:12.504676 140005313861376 logging_writer.py:48] [178700] global_step=178700, grad_norm=3.982078790664673, loss=2.5446979999542236
I0128 12:59:46.163647 140005305468672 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.166388988494873, loss=2.5191195011138916
I0128 13:00:19.726082 140005313861376 logging_writer.py:48] [178900] global_step=178900, grad_norm=3.991745710372925, loss=2.515371084213257
I0128 13:00:44.751496 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:00:51.024259 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:00:59.900338 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:01:02.428405 140169137129280 submission_runner.py:408] Time since start: 62343.78s, 	Step: 178976, 	{'train/accuracy': 0.9402303695678711, 'train/loss': 0.4584923982620239, 'validation/accuracy': 0.7633799910545349, 'validation/loss': 1.1745049953460693, 'validation/num_examples': 50000, 'test/accuracy': 0.6419000029563904, 'test/loss': 1.791907548904419, 'test/num_examples': 10000, 'score': 60219.034625291824, 'total_duration': 62343.77858257294, 'accumulated_submission_time': 60219.034625291824, 'accumulated_eval_time': 2113.4979746341705, 'accumulated_logging_time': 5.501019239425659}
I0128 13:01:02.477336 140005297075968 logging_writer.py:48] [178976] accumulated_eval_time=2113.497975, accumulated_logging_time=5.501019, accumulated_submission_time=60219.034625, global_step=178976, preemption_count=0, score=60219.034625, test/accuracy=0.641900, test/loss=1.791908, test/num_examples=10000, total_duration=62343.778583, train/accuracy=0.940230, train/loss=0.458492, validation/accuracy=0.763380, validation/loss=1.174505, validation/num_examples=50000
I0128 13:01:10.912616 140005330646784 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.164246559143066, loss=2.5108816623687744
I0128 13:01:44.431078 140005297075968 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.059120178222656, loss=2.5582096576690674
I0128 13:02:17.995843 140005330646784 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.113299369812012, loss=2.5688700675964355
I0128 13:02:51.655910 140005297075968 logging_writer.py:48] [179300] global_step=179300, grad_norm=3.9449877738952637, loss=2.4892964363098145
I0128 13:03:25.304358 140005330646784 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.376651763916016, loss=2.5323734283447266
I0128 13:03:58.925103 140005297075968 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.079876899719238, loss=2.5014383792877197
I0128 13:04:32.532615 140005330646784 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.902557373046875, loss=2.4814181327819824
I0128 13:05:06.157070 140005297075968 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.033745765686035, loss=2.458317518234253
I0128 13:05:39.854409 140005330646784 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.081382751464844, loss=2.5040135383605957
I0128 13:06:13.481180 140005297075968 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.263307094573975, loss=2.5564756393432617
I0128 13:06:47.152831 140005330646784 logging_writer.py:48] [180000] global_step=180000, grad_norm=3.776564359664917, loss=2.4737207889556885
I0128 13:07:20.785884 140005297075968 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.206949710845947, loss=2.5230305194854736
I0128 13:07:54.439338 140005330646784 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.205899715423584, loss=2.5644912719726562
I0128 13:08:28.068524 140005297075968 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.277076721191406, loss=2.5580320358276367
I0128 13:09:01.745939 140005330646784 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.103120803833008, loss=2.4626076221466064
I0128 13:09:32.519641 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:09:38.839820 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:09:47.765071 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:09:50.271639 140169137129280 submission_runner.py:408] Time since start: 62871.62s, 	Step: 180493, 	{'train/accuracy': 0.9389349222183228, 'train/loss': 0.45853328704833984, 'validation/accuracy': 0.7633999586105347, 'validation/loss': 1.175327181816101, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.7949339151382446, 'test/num_examples': 10000, 'score': 60729.01041841507, 'total_duration': 62871.621817588806, 'accumulated_submission_time': 60729.01041841507, 'accumulated_eval_time': 2131.2499561309814, 'accumulated_logging_time': 5.567572593688965}
I0128 13:09:50.320685 140004667934464 logging_writer.py:48] [180493] accumulated_eval_time=2131.249956, accumulated_logging_time=5.567573, accumulated_submission_time=60729.010418, global_step=180493, preemption_count=0, score=60729.010418, test/accuracy=0.645700, test/loss=1.794934, test/num_examples=10000, total_duration=62871.621818, train/accuracy=0.938935, train/loss=0.458533, validation/accuracy=0.763400, validation/loss=1.175327, validation/num_examples=50000
I0128 13:09:53.021709 140004676327168 logging_writer.py:48] [180500] global_step=180500, grad_norm=3.9938573837280273, loss=2.484208822250366
I0128 13:10:26.587119 140004667934464 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.3943634033203125, loss=2.5619962215423584
I0128 13:11:00.124034 140004676327168 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.077052116394043, loss=2.5027289390563965
I0128 13:11:33.677248 140004667934464 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.434006214141846, loss=2.5288238525390625
I0128 13:12:07.306276 140004676327168 logging_writer.py:48] [180900] global_step=180900, grad_norm=3.925248622894287, loss=2.546570062637329
I0128 13:12:40.878262 140004667934464 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.090457439422607, loss=2.5108437538146973
I0128 13:13:14.484133 140004676327168 logging_writer.py:48] [181100] global_step=181100, grad_norm=3.9822256565093994, loss=2.5089268684387207
I0128 13:13:48.172975 140004667934464 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.044970989227295, loss=2.515199899673462
I0128 13:14:21.807606 140004676327168 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.125551700592041, loss=2.5124664306640625
I0128 13:14:55.441590 140004667934464 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.075016498565674, loss=2.4875028133392334
I0128 13:15:29.060968 140004676327168 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.166804790496826, loss=2.5621511936187744
I0128 13:16:02.705802 140004667934464 logging_writer.py:48] [181600] global_step=181600, grad_norm=3.9668686389923096, loss=2.4602112770080566
I0128 13:16:36.330331 140004676327168 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.413320541381836, loss=2.538848638534546
I0128 13:17:09.960771 140004667934464 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.110457897186279, loss=2.537835121154785
I0128 13:17:43.928620 140004676327168 logging_writer.py:48] [181900] global_step=181900, grad_norm=3.8130886554718018, loss=2.477874755859375
I0128 13:18:17.637686 140004667934464 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.06641149520874, loss=2.5283660888671875
I0128 13:18:20.490792 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:18:26.771193 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:18:35.543847 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:18:38.062293 140169137129280 submission_runner.py:408] Time since start: 63399.41s, 	Step: 182010, 	{'train/accuracy': 0.93949294090271, 'train/loss': 0.45538613200187683, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 1.1701064109802246, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.789433240890503, 'test/num_examples': 10000, 'score': 61239.120810985565, 'total_duration': 63399.412459373474, 'accumulated_submission_time': 61239.120810985565, 'accumulated_eval_time': 2148.8214042186737, 'accumulated_logging_time': 5.627314567565918}
I0128 13:18:38.111247 140005313861376 logging_writer.py:48] [182010] accumulated_eval_time=2148.821404, accumulated_logging_time=5.627315, accumulated_submission_time=61239.120811, global_step=182010, preemption_count=0, score=61239.120811, test/accuracy=0.644900, test/loss=1.789433, test/num_examples=10000, total_duration=63399.412459, train/accuracy=0.939493, train/loss=0.455386, validation/accuracy=0.763640, validation/loss=1.170106, validation/num_examples=50000
I0128 13:19:08.712516 140005322254080 logging_writer.py:48] [182100] global_step=182100, grad_norm=3.951417922973633, loss=2.5404977798461914
I0128 13:19:42.339153 140005313861376 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.238569736480713, loss=2.5450427532196045
I0128 13:20:15.981764 140005322254080 logging_writer.py:48] [182300] global_step=182300, grad_norm=3.9594409465789795, loss=2.542630434036255
I0128 13:20:49.618170 140005313861376 logging_writer.py:48] [182400] global_step=182400, grad_norm=3.9744961261749268, loss=2.5039920806884766
I0128 13:21:23.260580 140005322254080 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.003175735473633, loss=2.4947028160095215
I0128 13:21:56.919197 140005313861376 logging_writer.py:48] [182600] global_step=182600, grad_norm=3.9986860752105713, loss=2.51479434967041
I0128 13:22:30.544754 140005322254080 logging_writer.py:48] [182700] global_step=182700, grad_norm=3.8482158184051514, loss=2.53475284576416
I0128 13:23:04.204695 140005313861376 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.521801948547363, loss=2.560267448425293
I0128 13:23:37.849272 140005322254080 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.14256477355957, loss=2.5400304794311523
I0128 13:24:11.468352 140005313861376 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.104002475738525, loss=2.5309603214263916
I0128 13:24:45.187062 140005322254080 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.0787835121154785, loss=2.4733617305755615
I0128 13:25:18.848685 140005313861376 logging_writer.py:48] [183200] global_step=183200, grad_norm=3.9867680072784424, loss=2.4911789894104004
I0128 13:25:52.470771 140005322254080 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.230040073394775, loss=2.522275924682617
I0128 13:26:26.126996 140005313861376 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.059473037719727, loss=2.5360615253448486
I0128 13:26:59.784967 140005322254080 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.37925910949707, loss=2.4825210571289062
I0128 13:27:08.349471 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:27:14.678185 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:27:23.370157 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:27:25.892127 140169137129280 submission_runner.py:408] Time since start: 63927.24s, 	Step: 183527, 	{'train/accuracy': 0.9393534660339355, 'train/loss': 0.45500364899635315, 'validation/accuracy': 0.7640799880027771, 'validation/loss': 1.1725746393203735, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.7934223413467407, 'test/num_examples': 10000, 'score': 61749.299342632294, 'total_duration': 63927.24230384827, 'accumulated_submission_time': 61749.299342632294, 'accumulated_eval_time': 2166.364020586014, 'accumulated_logging_time': 5.686914920806885}
I0128 13:27:25.943440 140004667934464 logging_writer.py:48] [183527] accumulated_eval_time=2166.364021, accumulated_logging_time=5.686915, accumulated_submission_time=61749.299343, global_step=183527, preemption_count=0, score=61749.299343, test/accuracy=0.644400, test/loss=1.793422, test/num_examples=10000, total_duration=63927.242304, train/accuracy=0.939353, train/loss=0.455004, validation/accuracy=0.764080, validation/loss=1.172575, validation/num_examples=50000
I0128 13:27:50.826661 140004676327168 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.178564548492432, loss=2.5059990882873535
I0128 13:28:24.451119 140004667934464 logging_writer.py:48] [183700] global_step=183700, grad_norm=3.8663711547851562, loss=2.453145742416382
I0128 13:28:58.082265 140004676327168 logging_writer.py:48] [183800] global_step=183800, grad_norm=3.8170857429504395, loss=2.5150468349456787
I0128 13:29:31.752696 140004667934464 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.128906726837158, loss=2.522440195083618
I0128 13:30:05.399160 140004676327168 logging_writer.py:48] [184000] global_step=184000, grad_norm=3.934199094772339, loss=2.4910106658935547
I0128 13:30:39.079697 140004667934464 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.257211685180664, loss=2.5960025787353516
I0128 13:31:12.653787 140004676327168 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.180565357208252, loss=2.5462281703948975
I0128 13:31:46.295716 140004667934464 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.277990341186523, loss=2.6140360832214355
I0128 13:32:19.959718 140004676327168 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.330846786499023, loss=2.519369125366211
I0128 13:32:53.571388 140004667934464 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.101932525634766, loss=2.5103445053100586
I0128 13:33:27.215761 140004676327168 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.021103382110596, loss=2.5069384574890137
I0128 13:34:00.879185 140004667934464 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.359555244445801, loss=2.5163919925689697
I0128 13:34:34.539219 140004676327168 logging_writer.py:48] [184800] global_step=184800, grad_norm=3.937439441680908, loss=2.4592108726501465
I0128 13:35:08.177117 140004667934464 logging_writer.py:48] [184900] global_step=184900, grad_norm=4.146878719329834, loss=2.5216503143310547
I0128 13:35:41.849166 140004676327168 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.454463481903076, loss=2.5314552783966064
I0128 13:35:56.115599 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:36:02.445276 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:36:11.225133 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:36:13.807816 140169137129280 submission_runner.py:408] Time since start: 64455.16s, 	Step: 185044, 	{'train/accuracy': 0.9412667155265808, 'train/loss': 0.4489424228668213, 'validation/accuracy': 0.763979971408844, 'validation/loss': 1.1695361137390137, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.7893344163894653, 'test/num_examples': 10000, 'score': 62259.412586927414, 'total_duration': 64455.15799450874, 'accumulated_submission_time': 62259.412586927414, 'accumulated_eval_time': 2184.0561985969543, 'accumulated_logging_time': 5.7481689453125}
I0128 13:36:13.859952 140004667934464 logging_writer.py:48] [185044] accumulated_eval_time=2184.056199, accumulated_logging_time=5.748169, accumulated_submission_time=62259.412587, global_step=185044, preemption_count=0, score=62259.412587, test/accuracy=0.644900, test/loss=1.789334, test/num_examples=10000, total_duration=64455.157995, train/accuracy=0.941267, train/loss=0.448942, validation/accuracy=0.763980, validation/loss=1.169536, validation/num_examples=50000
I0128 13:36:33.010197 140005305468672 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.280186176300049, loss=2.503730297088623
I0128 13:37:06.653189 140004667934464 logging_writer.py:48] [185200] global_step=185200, grad_norm=3.8952865600585938, loss=2.5013842582702637
I0128 13:37:40.286577 140005305468672 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.15878963470459, loss=2.4926416873931885
I0128 13:38:13.942209 140004667934464 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.32397985458374, loss=2.515004873275757
I0128 13:38:47.595113 140005305468672 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.094099521636963, loss=2.545905113220215
I0128 13:39:21.256909 140004667934464 logging_writer.py:48] [185600] global_step=185600, grad_norm=3.876801013946533, loss=2.5188851356506348
I0128 13:39:54.906250 140005305468672 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.15907096862793, loss=2.5193214416503906
I0128 13:40:28.557324 140004667934464 logging_writer.py:48] [185800] global_step=185800, grad_norm=4.4247636795043945, loss=2.531301498413086
I0128 13:41:02.191968 140005305468672 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.196279048919678, loss=2.54960298538208
I0128 13:41:35.802014 140004667934464 logging_writer.py:48] [186000] global_step=186000, grad_norm=3.994903326034546, loss=2.5168838500976562
I0128 13:42:09.447123 140005305468672 logging_writer.py:48] [186100] global_step=186100, grad_norm=3.972317695617676, loss=2.5456507205963135
I0128 13:42:43.099979 140004667934464 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.036129951477051, loss=2.4859700202941895
I0128 13:43:16.814899 140005305468672 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.240896224975586, loss=2.550556182861328
I0128 13:43:50.493669 140004667934464 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.157219409942627, loss=2.5187511444091797
I0128 13:44:24.114165 140005305468672 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.120488166809082, loss=2.5275826454162598
I0128 13:44:44.103469 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:44:50.365822 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:44:59.083563 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:45:01.613212 140169137129280 submission_runner.py:408] Time since start: 64982.96s, 	Step: 186561, 	{'train/accuracy': 0.9403699040412903, 'train/loss': 0.452570378780365, 'validation/accuracy': 0.7641800045967102, 'validation/loss': 1.1703591346740723, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7891464233398438, 'test/num_examples': 10000, 'score': 62769.59384036064, 'total_duration': 64982.96338915825, 'accumulated_submission_time': 62769.59384036064, 'accumulated_eval_time': 2201.5659034252167, 'accumulated_logging_time': 5.813524484634399}
I0128 13:45:01.666007 140005297075968 logging_writer.py:48] [186561] accumulated_eval_time=2201.565903, accumulated_logging_time=5.813524, accumulated_submission_time=62769.593840, global_step=186561, preemption_count=0, score=62769.593840, test/accuracy=0.645500, test/loss=1.789146, test/num_examples=10000, total_duration=64982.963389, train/accuracy=0.940370, train/loss=0.452570, validation/accuracy=0.764180, validation/loss=1.170359, validation/num_examples=50000
I0128 13:45:15.099879 140005322254080 logging_writer.py:48] [186600] global_step=186600, grad_norm=3.7103185653686523, loss=2.468867301940918
I0128 13:45:36.700593 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:45:42.973998 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:45:51.815600 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:45:54.331848 140169137129280 submission_runner.py:408] Time since start: 65035.68s, 	Step: 186666, 	{'train/accuracy': 0.9392936825752258, 'train/loss': 0.4516395628452301, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 1.167160987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.787272572517395, 'test/num_examples': 10000, 'score': 62804.61339139938, 'total_duration': 65035.68202996254, 'accumulated_submission_time': 62804.61339139938, 'accumulated_eval_time': 2219.1971287727356, 'accumulated_logging_time': 5.877562046051025}
I0128 13:45:54.380211 140004659541760 logging_writer.py:48] [186666] accumulated_eval_time=2219.197129, accumulated_logging_time=5.877562, accumulated_submission_time=62804.613391, global_step=186666, preemption_count=0, score=62804.613391, test/accuracy=0.644100, test/loss=1.787273, test/num_examples=10000, total_duration=65035.682030, train/accuracy=0.939294, train/loss=0.451640, validation/accuracy=0.764700, validation/loss=1.167161, validation/num_examples=50000
I0128 13:45:54.425480 140004667934464 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62804.613391
I0128 13:45:54.765604 140169137129280 checkpoints.py:490] Saving checkpoint at step: 186666
I0128 13:45:55.917428 140169137129280 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2/checkpoint_186666
I0128 13:45:55.937384 140169137129280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_2/checkpoint_186666.
I0128 13:45:56.704802 140169137129280 submission_runner.py:583] Tuning trial 2/5
I0128 13:45:56.705024 140169137129280 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0128 13:45:56.711011 140169137129280 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.00047831633128225803, 'train/loss': 6.910229682922363, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 31.64558982849121, 'total_duration': 48.83828067779541, 'accumulated_submission_time': 31.64558982849121, 'accumulated_eval_time': 17.192532777786255, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1514, {'train/accuracy': 0.08402423560619354, 'train/loss': 5.227553844451904, 'validation/accuracy': 0.0806799978017807, 'validation/loss': 5.2884297370910645, 'validation/num_examples': 50000, 'test/accuracy': 0.055000003427267075, 'test/loss': 5.537538528442383, 'test/num_examples': 10000, 'score': 541.6560969352722, 'total_duration': 576.5695464611053, 'accumulated_submission_time': 541.6560969352722, 'accumulated_eval_time': 34.846673011779785, 'accumulated_logging_time': 0.01880931854248047, 'global_step': 1514, 'preemption_count': 0}), (3026, {'train/accuracy': 0.19959741830825806, 'train/loss': 4.143094062805176, 'validation/accuracy': 0.1790200024843216, 'validation/loss': 4.270390033721924, 'validation/num_examples': 50000, 'test/accuracy': 0.1348000019788742, 'test/loss': 4.705352783203125, 'test/num_examples': 10000, 'score': 1051.752347946167, 'total_duration': 1104.6250972747803, 'accumulated_submission_time': 1051.752347946167, 'accumulated_eval_time': 52.729986906051636, 'accumulated_logging_time': 0.04701733589172363, 'global_step': 3026, 'preemption_count': 0}), (4538, {'train/accuracy': 0.3047672212123871, 'train/loss': 3.462144136428833, 'validation/accuracy': 0.2767399847507477, 'validation/loss': 3.6103830337524414, 'validation/num_examples': 50000, 'test/accuracy': 0.20430001616477966, 'test/loss': 4.173789024353027, 'test/num_examples': 10000, 'score': 1561.9691922664642, 'total_duration': 1633.4822070598602, 'accumulated_submission_time': 1561.9691922664642, 'accumulated_eval_time': 71.29328966140747, 'accumulated_logging_time': 0.07619142532348633, 'global_step': 4538, 'preemption_count': 0}), (6051, {'train/accuracy': 0.38402822613716125, 'train/loss': 2.9529519081115723, 'validation/accuracy': 0.3597399890422821, 'validation/loss': 3.0945801734924316, 'validation/num_examples': 50000, 'test/accuracy': 0.2770000100135803, 'test/loss': 3.678683042526245, 'test/num_examples': 10000, 'score': 2072.2266323566437, 'total_duration': 2161.6236131191254, 'accumulated_submission_time': 2072.2266323566437, 'accumulated_eval_time': 89.09815335273743, 'accumulated_logging_time': 0.10786223411560059, 'global_step': 6051, 'preemption_count': 0}), (7563, {'train/accuracy': 0.4412667453289032, 'train/loss': 2.6990714073181152, 'validation/accuracy': 0.40026000142097473, 'validation/loss': 2.899838447570801, 'validation/num_examples': 50000, 'test/accuracy': 0.30390000343322754, 'test/loss': 3.5461227893829346, 'test/num_examples': 10000, 'score': 2582.2532529830933, 'total_duration': 2689.541732311249, 'accumulated_submission_time': 2582.2532529830933, 'accumulated_eval_time': 106.90979647636414, 'accumulated_logging_time': 0.140455961227417, 'global_step': 7563, 'preemption_count': 0}), (9077, {'train/accuracy': 0.5027104616165161, 'train/loss': 2.3729019165039062, 'validation/accuracy': 0.45291998982429504, 'validation/loss': 2.6144769191741943, 'validation/num_examples': 50000, 'test/accuracy': 0.3500000238418579, 'test/loss': 3.250772476196289, 'test/num_examples': 10000, 'score': 3092.440190553665, 'total_duration': 3217.438376188278, 'accumulated_submission_time': 3092.440190553665, 'accumulated_eval_time': 124.54310297966003, 'accumulated_logging_time': 0.16915082931518555, 'global_step': 9077, 'preemption_count': 0}), (10591, {'train/accuracy': 0.5700932741165161, 'train/loss': 2.06361722946167, 'validation/accuracy': 0.5237799882888794, 'validation/loss': 2.2730724811553955, 'validation/num_examples': 50000, 'test/accuracy': 0.4058000147342682, 'test/loss': 2.908501148223877, 'test/num_examples': 10000, 'score': 3602.4188084602356, 'total_duration': 3745.6586899757385, 'accumulated_submission_time': 3602.4188084602356, 'accumulated_eval_time': 142.70739150047302, 'accumulated_logging_time': 0.19852828979492188, 'global_step': 10591, 'preemption_count': 0}), (12105, {'train/accuracy': 0.5975366830825806, 'train/loss': 1.9064337015151978, 'validation/accuracy': 0.5470799803733826, 'validation/loss': 2.1341938972473145, 'validation/num_examples': 50000, 'test/accuracy': 0.42160001397132874, 'test/loss': 2.831441640853882, 'test/num_examples': 10000, 'score': 4112.355051517487, 'total_duration': 4273.3349623680115, 'accumulated_submission_time': 4112.355051517487, 'accumulated_eval_time': 160.36835479736328, 'accumulated_logging_time': 0.2296433448791504, 'global_step': 12105, 'preemption_count': 0}), (13620, {'train/accuracy': 0.615632951259613, 'train/loss': 1.8004908561706543, 'validation/accuracy': 0.5709399580955505, 'validation/loss': 2.0190188884735107, 'validation/num_examples': 50000, 'test/accuracy': 0.44930002093315125, 'test/loss': 2.669410228729248, 'test/num_examples': 10000, 'score': 4622.293032407761, 'total_duration': 4800.818227529526, 'accumulated_submission_time': 4622.293032407761, 'accumulated_eval_time': 177.8351345062256, 'accumulated_logging_time': 0.2601957321166992, 'global_step': 13620, 'preemption_count': 0}), (15135, {'train/accuracy': 0.6227080821990967, 'train/loss': 1.854886531829834, 'validation/accuracy': 0.5773000121116638, 'validation/loss': 2.06612229347229, 'validation/num_examples': 50000, 'test/accuracy': 0.44360002875328064, 'test/loss': 2.7721447944641113, 'test/num_examples': 10000, 'score': 5132.363230466843, 'total_duration': 5328.756629705429, 'accumulated_submission_time': 5132.363230466843, 'accumulated_eval_time': 195.62596201896667, 'accumulated_logging_time': 0.2902054786682129, 'global_step': 15135, 'preemption_count': 0}), (16650, {'train/accuracy': 0.6803053021430969, 'train/loss': 1.539376974105835, 'validation/accuracy': 0.6041600108146667, 'validation/loss': 1.8682082891464233, 'validation/num_examples': 50000, 'test/accuracy': 0.48020002245903015, 'test/loss': 2.5343410968780518, 'test/num_examples': 10000, 'score': 5642.407160997391, 'total_duration': 5856.522654294968, 'accumulated_submission_time': 5642.407160997391, 'accumulated_eval_time': 213.26491689682007, 'accumulated_logging_time': 0.3256824016571045, 'global_step': 16650, 'preemption_count': 0}), (18165, {'train/accuracy': 0.6703802347183228, 'train/loss': 1.531219720840454, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.8504778146743774, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.5441064834594727, 'test/num_examples': 10000, 'score': 6152.345567941666, 'total_duration': 6384.078463315964, 'accumulated_submission_time': 6152.345567941666, 'accumulated_eval_time': 230.80410766601562, 'accumulated_logging_time': 0.3559126853942871, 'global_step': 18165, 'preemption_count': 0}), (19681, {'train/accuracy': 0.6851084232330322, 'train/loss': 1.5148324966430664, 'validation/accuracy': 0.6193000078201294, 'validation/loss': 1.7937859296798706, 'validation/num_examples': 50000, 'test/accuracy': 0.4904000163078308, 'test/loss': 2.440124988555908, 'test/num_examples': 10000, 'score': 6662.31097984314, 'total_duration': 6911.979278564453, 'accumulated_submission_time': 6662.31097984314, 'accumulated_eval_time': 248.65882778167725, 'accumulated_logging_time': 0.3883523941040039, 'global_step': 19681, 'preemption_count': 0}), (21197, {'train/accuracy': 0.6892338991165161, 'train/loss': 1.42905855178833, 'validation/accuracy': 0.627020001411438, 'validation/loss': 1.7129013538360596, 'validation/num_examples': 50000, 'test/accuracy': 0.49880000948905945, 'test/loss': 2.369382619857788, 'test/num_examples': 10000, 'score': 7172.2588493824005, 'total_duration': 7440.037328243256, 'accumulated_submission_time': 7172.2588493824005, 'accumulated_eval_time': 266.68954062461853, 'accumulated_logging_time': 0.4194791316986084, 'global_step': 21197, 'preemption_count': 0}), (22713, {'train/accuracy': 0.684012234210968, 'train/loss': 1.5042533874511719, 'validation/accuracy': 0.6255599856376648, 'validation/loss': 1.766654372215271, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.4623095989227295, 'test/num_examples': 10000, 'score': 7682.2843725681305, 'total_duration': 7967.854699373245, 'accumulated_submission_time': 7682.2843725681305, 'accumulated_eval_time': 284.39879989624023, 'accumulated_logging_time': 0.4545462131500244, 'global_step': 22713, 'preemption_count': 0}), (24229, {'train/accuracy': 0.6905492544174194, 'train/loss': 1.4637855291366577, 'validation/accuracy': 0.6354599595069885, 'validation/loss': 1.7087171077728271, 'validation/num_examples': 50000, 'test/accuracy': 0.5049999952316284, 'test/loss': 2.389516592025757, 'test/num_examples': 10000, 'score': 8192.348526477814, 'total_duration': 8495.57875084877, 'accumulated_submission_time': 8192.348526477814, 'accumulated_eval_time': 301.9742715358734, 'accumulated_logging_time': 0.4906270503997803, 'global_step': 24229, 'preemption_count': 0}), (25745, {'train/accuracy': 0.7331194281578064, 'train/loss': 1.299314022064209, 'validation/accuracy': 0.6287800073623657, 'validation/loss': 1.7513171434402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5076000094413757, 'test/loss': 2.4090499877929688, 'test/num_examples': 10000, 'score': 8702.524030208588, 'total_duration': 9023.23773431778, 'accumulated_submission_time': 8702.524030208588, 'accumulated_eval_time': 319.3741111755371, 'accumulated_logging_time': 0.5262205600738525, 'global_step': 25745, 'preemption_count': 0}), (27261, {'train/accuracy': 0.7091438174247742, 'train/loss': 1.3814681768417358, 'validation/accuracy': 0.6342599987983704, 'validation/loss': 1.7121782302856445, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.3702125549316406, 'test/num_examples': 10000, 'score': 9212.554991006851, 'total_duration': 9551.120630264282, 'accumulated_submission_time': 9212.554991006851, 'accumulated_eval_time': 337.14146423339844, 'accumulated_logging_time': 0.562938928604126, 'global_step': 27261, 'preemption_count': 0}), (28777, {'train/accuracy': 0.7155014276504517, 'train/loss': 1.364587664604187, 'validation/accuracy': 0.6462999582290649, 'validation/loss': 1.6733760833740234, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.340773820877075, 'test/num_examples': 10000, 'score': 9722.621646165848, 'total_duration': 10078.848618268967, 'accumulated_submission_time': 9722.621646165848, 'accumulated_eval_time': 354.72114634513855, 'accumulated_logging_time': 0.5966382026672363, 'global_step': 28777, 'preemption_count': 0}), (30293, {'train/accuracy': 0.7161391973495483, 'train/loss': 1.3571972846984863, 'validation/accuracy': 0.6464399695396423, 'validation/loss': 1.6608171463012695, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.338981866836548, 'test/num_examples': 10000, 'score': 10232.737368822098, 'total_duration': 10606.741770744324, 'accumulated_submission_time': 10232.737368822098, 'accumulated_eval_time': 372.41778016090393, 'accumulated_logging_time': 0.6293659210205078, 'global_step': 30293, 'preemption_count': 0}), (31809, {'train/accuracy': 0.70511794090271, 'train/loss': 1.4040005207061768, 'validation/accuracy': 0.64028000831604, 'validation/loss': 1.6982835531234741, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.3254666328430176, 'test/num_examples': 10000, 'score': 10742.950670957565, 'total_duration': 11134.55994296074, 'accumulated_submission_time': 10742.950670957565, 'accumulated_eval_time': 389.93855023384094, 'accumulated_logging_time': 0.665184497833252, 'global_step': 31809, 'preemption_count': 0}), (33325, {'train/accuracy': 0.7089444994926453, 'train/loss': 1.3775570392608643, 'validation/accuracy': 0.6474399566650391, 'validation/loss': 1.6512587070465088, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.3262481689453125, 'test/num_examples': 10000, 'score': 11253.040585517883, 'total_duration': 11662.289780378342, 'accumulated_submission_time': 11253.040585517883, 'accumulated_eval_time': 407.4942150115967, 'accumulated_logging_time': 0.7008495330810547, 'global_step': 33325, 'preemption_count': 0}), (34841, {'train/accuracy': 0.7420878410339355, 'train/loss': 1.2352484464645386, 'validation/accuracy': 0.6431399583816528, 'validation/loss': 1.672220230102539, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.30928897857666, 'test/num_examples': 10000, 'score': 11763.106231689453, 'total_duration': 12190.231878995895, 'accumulated_submission_time': 11763.106231689453, 'accumulated_eval_time': 425.2882845401764, 'accumulated_logging_time': 0.7346818447113037, 'global_step': 34841, 'preemption_count': 0}), (36358, {'train/accuracy': 0.7210817933082581, 'train/loss': 1.3398000001907349, 'validation/accuracy': 0.6416599750518799, 'validation/loss': 1.693834900856018, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.3678078651428223, 'test/num_examples': 10000, 'score': 12273.201929330826, 'total_duration': 12718.294162273407, 'accumulated_submission_time': 12273.201929330826, 'accumulated_eval_time': 443.1710669994354, 'accumulated_logging_time': 0.7699494361877441, 'global_step': 36358, 'preemption_count': 0}), (37874, {'train/accuracy': 0.7284956574440002, 'train/loss': 1.312329649925232, 'validation/accuracy': 0.6582199931144714, 'validation/loss': 1.6255125999450684, 'validation/num_examples': 50000, 'test/accuracy': 0.5271000266075134, 'test/loss': 2.265986442565918, 'test/num_examples': 10000, 'score': 12783.18024611473, 'total_duration': 13245.977816104889, 'accumulated_submission_time': 12783.18024611473, 'accumulated_eval_time': 460.7906458377838, 'accumulated_logging_time': 0.8074545860290527, 'global_step': 37874, 'preemption_count': 0}), (39391, {'train/accuracy': 0.7284956574440002, 'train/loss': 1.298724889755249, 'validation/accuracy': 0.6593199968338013, 'validation/loss': 1.6137521266937256, 'validation/num_examples': 50000, 'test/accuracy': 0.5283000469207764, 'test/loss': 2.272613525390625, 'test/num_examples': 10000, 'score': 13293.384312152863, 'total_duration': 13773.91310286522, 'accumulated_submission_time': 13293.384312152863, 'accumulated_eval_time': 478.4318549633026, 'accumulated_logging_time': 0.8491504192352295, 'global_step': 39391, 'preemption_count': 0}), (40907, {'train/accuracy': 0.7203842401504517, 'train/loss': 1.3398540019989014, 'validation/accuracy': 0.6532599925994873, 'validation/loss': 1.6414886713027954, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.2787270545959473, 'test/num_examples': 10000, 'score': 13803.354566812515, 'total_duration': 14301.691583871841, 'accumulated_submission_time': 13803.354566812515, 'accumulated_eval_time': 496.1568441390991, 'accumulated_logging_time': 0.8838469982147217, 'global_step': 40907, 'preemption_count': 0}), (42423, {'train/accuracy': 0.7283163070678711, 'train/loss': 1.3241567611694336, 'validation/accuracy': 0.6582599878311157, 'validation/loss': 1.6258599758148193, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.2608909606933594, 'test/num_examples': 10000, 'score': 14313.371778011322, 'total_duration': 14830.411584377289, 'accumulated_submission_time': 14313.371778011322, 'accumulated_eval_time': 514.7730877399445, 'accumulated_logging_time': 0.921715259552002, 'global_step': 42423, 'preemption_count': 0}), (43940, {'train/accuracy': 0.7344746589660645, 'train/loss': 1.301184892654419, 'validation/accuracy': 0.6468999981880188, 'validation/loss': 1.689677119255066, 'validation/num_examples': 50000, 'test/accuracy': 0.5181000232696533, 'test/loss': 2.357792377471924, 'test/num_examples': 10000, 'score': 14823.624761104584, 'total_duration': 15358.438279390335, 'accumulated_submission_time': 14823.624761104584, 'accumulated_eval_time': 532.4613721370697, 'accumulated_logging_time': 0.9581685066223145, 'global_step': 43940, 'preemption_count': 0}), (45458, {'train/accuracy': 0.7339963316917419, 'train/loss': 1.248931646347046, 'validation/accuracy': 0.6551799774169922, 'validation/loss': 1.5993354320526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2523021697998047, 'test/num_examples': 10000, 'score': 15333.792750835419, 'total_duration': 15886.385159492493, 'accumulated_submission_time': 15333.792750835419, 'accumulated_eval_time': 550.1557083129883, 'accumulated_logging_time': 0.9941191673278809, 'global_step': 45458, 'preemption_count': 0}), (46975, {'train/accuracy': 0.7340162396430969, 'train/loss': 1.2588564157485962, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.5765480995178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.250394344329834, 'test/num_examples': 10000, 'score': 15843.919181346893, 'total_duration': 16414.092190027237, 'accumulated_submission_time': 15843.919181346893, 'accumulated_eval_time': 567.648931980133, 'accumulated_logging_time': 1.0327842235565186, 'global_step': 46975, 'preemption_count': 0}), (48491, {'train/accuracy': 0.7350525856018066, 'train/loss': 1.2592726945877075, 'validation/accuracy': 0.665619969367981, 'validation/loss': 1.5760258436203003, 'validation/num_examples': 50000, 'test/accuracy': 0.5386000275611877, 'test/loss': 2.2234253883361816, 'test/num_examples': 10000, 'score': 16353.850271701813, 'total_duration': 16941.92165875435, 'accumulated_submission_time': 16353.850271701813, 'accumulated_eval_time': 585.4615631103516, 'accumulated_logging_time': 1.0696442127227783, 'global_step': 48491, 'preemption_count': 0}), (50008, {'train/accuracy': 0.7184112071990967, 'train/loss': 1.2996251583099365, 'validation/accuracy': 0.6577799916267395, 'validation/loss': 1.5904572010040283, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.27528715133667, 'test/num_examples': 10000, 'score': 16863.88741350174, 'total_duration': 17470.022649526596, 'accumulated_submission_time': 16863.88741350174, 'accumulated_eval_time': 603.4375021457672, 'accumulated_logging_time': 1.108870029449463, 'global_step': 50008, 'preemption_count': 0}), (51525, {'train/accuracy': 0.7494618892669678, 'train/loss': 1.2071516513824463, 'validation/accuracy': 0.6755399703979492, 'validation/loss': 1.532850742340088, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.1815757751464844, 'test/num_examples': 10000, 'score': 17374.02089715004, 'total_duration': 17997.889188289642, 'accumulated_submission_time': 17374.02089715004, 'accumulated_eval_time': 621.0799465179443, 'accumulated_logging_time': 1.1506271362304688, 'global_step': 51525, 'preemption_count': 0}), (53042, {'train/accuracy': 0.7538065910339355, 'train/loss': 1.2099741697311401, 'validation/accuracy': 0.6650999784469604, 'validation/loss': 1.5931397676467896, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.2342357635498047, 'test/num_examples': 10000, 'score': 17884.068472623825, 'total_duration': 18525.674897909164, 'accumulated_submission_time': 17884.068472623825, 'accumulated_eval_time': 638.7280640602112, 'accumulated_logging_time': 1.1915946006774902, 'global_step': 53042, 'preemption_count': 0}), (54560, {'train/accuracy': 0.7381616830825806, 'train/loss': 1.2561745643615723, 'validation/accuracy': 0.659559965133667, 'validation/loss': 1.598878264427185, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.2853755950927734, 'test/num_examples': 10000, 'score': 18394.205976486206, 'total_duration': 19053.840754270554, 'accumulated_submission_time': 18394.205976486206, 'accumulated_eval_time': 656.6700406074524, 'accumulated_logging_time': 1.2295169830322266, 'global_step': 54560, 'preemption_count': 0}), (56077, {'train/accuracy': 0.7382214665412903, 'train/loss': 1.2343106269836426, 'validation/accuracy': 0.6646400094032288, 'validation/loss': 1.5662380456924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.216094732284546, 'test/num_examples': 10000, 'score': 18904.258904457092, 'total_duration': 19581.609267950058, 'accumulated_submission_time': 18904.258904457092, 'accumulated_eval_time': 674.2945744991302, 'accumulated_logging_time': 1.2716209888458252, 'global_step': 56077, 'preemption_count': 0}), (57594, {'train/accuracy': 0.7438416481018066, 'train/loss': 1.227565050125122, 'validation/accuracy': 0.6708399653434753, 'validation/loss': 1.5553630590438843, 'validation/num_examples': 50000, 'test/accuracy': 0.5416000485420227, 'test/loss': 2.204932928085327, 'test/num_examples': 10000, 'score': 19414.452545642853, 'total_duration': 20109.543710708618, 'accumulated_submission_time': 19414.452545642853, 'accumulated_eval_time': 691.946305513382, 'accumulated_logging_time': 1.3120112419128418, 'global_step': 57594, 'preemption_count': 0}), (59112, {'train/accuracy': 0.7326809763908386, 'train/loss': 1.2575324773788452, 'validation/accuracy': 0.6616599559783936, 'validation/loss': 1.569987416267395, 'validation/num_examples': 50000, 'test/accuracy': 0.52920001745224, 'test/loss': 2.269146203994751, 'test/num_examples': 10000, 'score': 19924.543339967728, 'total_duration': 20637.42899608612, 'accumulated_submission_time': 19924.543339967728, 'accumulated_eval_time': 709.6492412090302, 'accumulated_logging_time': 1.3549623489379883, 'global_step': 59112, 'preemption_count': 0}), (60630, {'train/accuracy': 0.7663623690605164, 'train/loss': 1.119907021522522, 'validation/accuracy': 0.6607800126075745, 'validation/loss': 1.5795252323150635, 'validation/num_examples': 50000, 'test/accuracy': 0.5357000231742859, 'test/loss': 2.234541177749634, 'test/num_examples': 10000, 'score': 20434.725960969925, 'total_duration': 21165.11572289467, 'accumulated_submission_time': 20434.725960969925, 'accumulated_eval_time': 727.0632431507111, 'accumulated_logging_time': 1.3961291313171387, 'global_step': 60630, 'preemption_count': 0}), (62148, {'train/accuracy': 0.7565170526504517, 'train/loss': 1.1901875734329224, 'validation/accuracy': 0.6680799722671509, 'validation/loss': 1.5773533582687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5370000004768372, 'test/loss': 2.229222059249878, 'test/num_examples': 10000, 'score': 20944.98304605484, 'total_duration': 21693.09570145607, 'accumulated_submission_time': 20944.98304605484, 'accumulated_eval_time': 744.6991124153137, 'accumulated_logging_time': 1.4345617294311523, 'global_step': 62148, 'preemption_count': 0}), (63665, {'train/accuracy': 0.7577726244926453, 'train/loss': 1.1620841026306152, 'validation/accuracy': 0.6750199794769287, 'validation/loss': 1.5228710174560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.200866222381592, 'test/num_examples': 10000, 'score': 21455.00278377533, 'total_duration': 22220.902873277664, 'accumulated_submission_time': 21455.00278377533, 'accumulated_eval_time': 762.3969528675079, 'accumulated_logging_time': 1.4752840995788574, 'global_step': 63665, 'preemption_count': 0}), (65182, {'train/accuracy': 0.7588488459587097, 'train/loss': 1.1486854553222656, 'validation/accuracy': 0.6837799549102783, 'validation/loss': 1.4862542152404785, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.1577084064483643, 'test/num_examples': 10000, 'score': 21964.96987080574, 'total_duration': 22748.91353034973, 'accumulated_submission_time': 21964.96987080574, 'accumulated_eval_time': 780.350729227066, 'accumulated_logging_time': 1.5167927742004395, 'global_step': 65182, 'preemption_count': 0}), (66699, {'train/accuracy': 0.7502591013908386, 'train/loss': 1.2132987976074219, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.5442116260528564, 'validation/num_examples': 50000, 'test/accuracy': 0.5546000003814697, 'test/loss': 2.189296245574951, 'test/num_examples': 10000, 'score': 22475.05553460121, 'total_duration': 23276.70662856102, 'accumulated_submission_time': 22475.05553460121, 'accumulated_eval_time': 797.9648485183716, 'accumulated_logging_time': 1.5609960556030273, 'global_step': 66699, 'preemption_count': 0}), (68216, {'train/accuracy': 0.7473692297935486, 'train/loss': 1.19962477684021, 'validation/accuracy': 0.6743800044059753, 'validation/loss': 1.5287421941757202, 'validation/num_examples': 50000, 'test/accuracy': 0.5452000498771667, 'test/loss': 2.196040153503418, 'test/num_examples': 10000, 'score': 22985.083032131195, 'total_duration': 23804.51207590103, 'accumulated_submission_time': 22985.083032131195, 'accumulated_eval_time': 815.652755022049, 'accumulated_logging_time': 1.6021060943603516, 'global_step': 68216, 'preemption_count': 0}), (69734, {'train/accuracy': 0.7683553695678711, 'train/loss': 1.1409224271774292, 'validation/accuracy': 0.6607999801635742, 'validation/loss': 1.5949475765228271, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.243528127670288, 'test/num_examples': 10000, 'score': 23495.33017706871, 'total_duration': 24332.515582323074, 'accumulated_submission_time': 23495.33017706871, 'accumulated_eval_time': 833.3205726146698, 'accumulated_logging_time': 1.642003059387207, 'global_step': 69734, 'preemption_count': 0}), (71251, {'train/accuracy': 0.7674585580825806, 'train/loss': 1.128574013710022, 'validation/accuracy': 0.6775799989700317, 'validation/loss': 1.5199021100997925, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.167146682739258, 'test/num_examples': 10000, 'score': 24005.278024673462, 'total_duration': 24860.202106952667, 'accumulated_submission_time': 24005.278024673462, 'accumulated_eval_time': 850.9681005477905, 'accumulated_logging_time': 1.6843080520629883, 'global_step': 71251, 'preemption_count': 0}), (72768, {'train/accuracy': 0.7587292790412903, 'train/loss': 1.159325361251831, 'validation/accuracy': 0.6759200096130371, 'validation/loss': 1.519149661064148, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.178276538848877, 'test/num_examples': 10000, 'score': 24515.25668501854, 'total_duration': 25388.056608200073, 'accumulated_submission_time': 24515.25668501854, 'accumulated_eval_time': 868.7539627552032, 'accumulated_logging_time': 1.7253928184509277, 'global_step': 72768, 'preemption_count': 0}), (74285, {'train/accuracy': 0.7687141299247742, 'train/loss': 1.109268307685852, 'validation/accuracy': 0.6859599947929382, 'validation/loss': 1.4653456211090088, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.108211040496826, 'test/num_examples': 10000, 'score': 25025.17741537094, 'total_duration': 25915.98461484909, 'accumulated_submission_time': 25025.17741537094, 'accumulated_eval_time': 886.672360420227, 'accumulated_logging_time': 1.7654447555541992, 'global_step': 74285, 'preemption_count': 0}), (75803, {'train/accuracy': 0.7549425959587097, 'train/loss': 1.1974339485168457, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5412228107452393, 'validation/num_examples': 50000, 'test/accuracy': 0.5576000213623047, 'test/loss': 2.1966655254364014, 'test/num_examples': 10000, 'score': 25535.3845744133, 'total_duration': 26444.38476872444, 'accumulated_submission_time': 25535.3845744133, 'accumulated_eval_time': 904.7744419574738, 'accumulated_logging_time': 1.8068821430206299, 'global_step': 75803, 'preemption_count': 0}), (77320, {'train/accuracy': 0.7669403553009033, 'train/loss': 1.1282862424850464, 'validation/accuracy': 0.6887999773025513, 'validation/loss': 1.4690423011779785, 'validation/num_examples': 50000, 'test/accuracy': 0.5628000497817993, 'test/loss': 2.1257145404815674, 'test/num_examples': 10000, 'score': 26045.367203950882, 'total_duration': 26972.264142274857, 'accumulated_submission_time': 26045.367203950882, 'accumulated_eval_time': 922.5809574127197, 'accumulated_logging_time': 1.8478496074676514, 'global_step': 77320, 'preemption_count': 0}), (78837, {'train/accuracy': 0.7891621589660645, 'train/loss': 1.0352303981781006, 'validation/accuracy': 0.684719979763031, 'validation/loss': 1.4883527755737305, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.133809804916382, 'test/num_examples': 10000, 'score': 26555.412185668945, 'total_duration': 27500.41028022766, 'accumulated_submission_time': 26555.412185668945, 'accumulated_eval_time': 940.5901341438293, 'accumulated_logging_time': 1.890838623046875, 'global_step': 78837, 'preemption_count': 0}), (80354, {'train/accuracy': 0.7771245241165161, 'train/loss': 1.0599044561386108, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.464746356010437, 'validation/num_examples': 50000, 'test/accuracy': 0.5583000183105469, 'test/loss': 2.1267716884613037, 'test/num_examples': 10000, 'score': 27065.312031030655, 'total_duration': 28028.457494974136, 'accumulated_submission_time': 27065.312031030655, 'accumulated_eval_time': 958.6321983337402, 'accumulated_logging_time': 1.9472503662109375, 'global_step': 80354, 'preemption_count': 0}), (81870, {'train/accuracy': 0.7718032598495483, 'train/loss': 1.0625839233398438, 'validation/accuracy': 0.6845200061798096, 'validation/loss': 1.4451771974563599, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.112879991531372, 'test/num_examples': 10000, 'score': 27575.380538225174, 'total_duration': 28557.02544283867, 'accumulated_submission_time': 27575.380538225174, 'accumulated_eval_time': 977.0377764701843, 'accumulated_logging_time': 1.9921598434448242, 'global_step': 81870, 'preemption_count': 0}), (83388, {'train/accuracy': 0.7626355290412903, 'train/loss': 1.1467963457107544, 'validation/accuracy': 0.6807599663734436, 'validation/loss': 1.5086842775344849, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 2.1497886180877686, 'test/num_examples': 10000, 'score': 28085.509213924408, 'total_duration': 29084.97811937332, 'accumulated_submission_time': 28085.509213924408, 'accumulated_eval_time': 994.7666764259338, 'accumulated_logging_time': 2.0380008220672607, 'global_step': 83388, 'preemption_count': 0}), (84905, {'train/accuracy': 0.7693120241165161, 'train/loss': 1.1221765279769897, 'validation/accuracy': 0.6900999546051025, 'validation/loss': 1.4688431024551392, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.1390833854675293, 'test/num_examples': 10000, 'score': 28595.469446659088, 'total_duration': 29612.574474573135, 'accumulated_submission_time': 28595.469446659088, 'accumulated_eval_time': 1012.3052713871002, 'accumulated_logging_time': 2.0864999294281006, 'global_step': 84905, 'preemption_count': 0}), (86423, {'train/accuracy': 0.78125, 'train/loss': 1.0466645956039429, 'validation/accuracy': 0.6961999535560608, 'validation/loss': 1.407152771949768, 'validation/num_examples': 50000, 'test/accuracy': 0.5702000260353088, 'test/loss': 2.054215669631958, 'test/num_examples': 10000, 'score': 29105.538024902344, 'total_duration': 30140.310687065125, 'accumulated_submission_time': 29105.538024902344, 'accumulated_eval_time': 1029.8799359798431, 'accumulated_logging_time': 2.1302878856658936, 'global_step': 86423, 'preemption_count': 0}), (87940, {'train/accuracy': 0.7981704473495483, 'train/loss': 1.0359044075012207, 'validation/accuracy': 0.6924200057983398, 'validation/loss': 1.4844554662704468, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.1412951946258545, 'test/num_examples': 10000, 'score': 29615.679981470108, 'total_duration': 30668.1747674942, 'accumulated_submission_time': 29615.679981470108, 'accumulated_eval_time': 1047.508416891098, 'accumulated_logging_time': 2.1748242378234863, 'global_step': 87940, 'preemption_count': 0}), (89457, {'train/accuracy': 0.7861925959587097, 'train/loss': 1.0689496994018555, 'validation/accuracy': 0.692359983921051, 'validation/loss': 1.4794917106628418, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.101494789123535, 'test/num_examples': 10000, 'score': 30125.72606277466, 'total_duration': 31195.812687158585, 'accumulated_submission_time': 30125.72606277466, 'accumulated_eval_time': 1065.0068988800049, 'accumulated_logging_time': 2.219353675842285, 'global_step': 89457, 'preemption_count': 0}), (90974, {'train/accuracy': 0.783621609210968, 'train/loss': 1.0466140508651733, 'validation/accuracy': 0.6930199861526489, 'validation/loss': 1.4299108982086182, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 2.0710158348083496, 'test/num_examples': 10000, 'score': 30635.79270672798, 'total_duration': 31723.562306404114, 'accumulated_submission_time': 30635.79270672798, 'accumulated_eval_time': 1082.5966680049896, 'accumulated_logging_time': 2.2636475563049316, 'global_step': 90974, 'preemption_count': 0}), (92491, {'train/accuracy': 0.7875877022743225, 'train/loss': 1.0378481149673462, 'validation/accuracy': 0.6992799639701843, 'validation/loss': 1.4238126277923584, 'validation/num_examples': 50000, 'test/accuracy': 0.5730000138282776, 'test/loss': 2.0835328102111816, 'test/num_examples': 10000, 'score': 31145.915197372437, 'total_duration': 32251.58094215393, 'accumulated_submission_time': 31145.915197372437, 'accumulated_eval_time': 1100.3993997573853, 'accumulated_logging_time': 2.308422565460205, 'global_step': 92491, 'preemption_count': 0}), (94008, {'train/accuracy': 0.7824856638908386, 'train/loss': 1.0473655462265015, 'validation/accuracy': 0.6983399987220764, 'validation/loss': 1.4176557064056396, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 2.070194721221924, 'test/num_examples': 10000, 'score': 31656.0486536026, 'total_duration': 32779.9430668354, 'accumulated_submission_time': 31656.0486536026, 'accumulated_eval_time': 1118.535723209381, 'accumulated_logging_time': 2.3523941040039062, 'global_step': 94008, 'preemption_count': 0}), (95526, {'train/accuracy': 0.8123804330825806, 'train/loss': 0.9565143585205078, 'validation/accuracy': 0.6977799534797668, 'validation/loss': 1.4305815696716309, 'validation/num_examples': 50000, 'test/accuracy': 0.5761000514030457, 'test/loss': 2.0659029483795166, 'test/num_examples': 10000, 'score': 32166.133952617645, 'total_duration': 33308.10327959061, 'accumulated_submission_time': 32166.133952617645, 'accumulated_eval_time': 1136.5134971141815, 'accumulated_logging_time': 2.4002532958984375, 'global_step': 95526, 'preemption_count': 0}), (97045, {'train/accuracy': 0.7994260191917419, 'train/loss': 0.9897308945655823, 'validation/accuracy': 0.6978600025177002, 'validation/loss': 1.441440463066101, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 2.0669610500335693, 'test/num_examples': 10000, 'score': 32676.36093187332, 'total_duration': 33836.25459456444, 'accumulated_submission_time': 32676.36093187332, 'accumulated_eval_time': 1154.3426752090454, 'accumulated_logging_time': 2.4465677738189697, 'global_step': 97045, 'preemption_count': 0}), (98562, {'train/accuracy': 0.8053650856018066, 'train/loss': 0.9407090544700623, 'validation/accuracy': 0.7060999870300293, 'validation/loss': 1.369234323501587, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.9954557418823242, 'test/num_examples': 10000, 'score': 33186.46888136864, 'total_duration': 34364.05491042137, 'accumulated_submission_time': 33186.46888136864, 'accumulated_eval_time': 1171.936819076538, 'accumulated_logging_time': 2.4959654808044434, 'global_step': 98562, 'preemption_count': 0}), (100080, {'train/accuracy': 0.7974131107330322, 'train/loss': 1.000417709350586, 'validation/accuracy': 0.7032999992370605, 'validation/loss': 1.4029895067214966, 'validation/num_examples': 50000, 'test/accuracy': 0.5737000107765198, 'test/loss': 2.07080340385437, 'test/num_examples': 10000, 'score': 33696.61943101883, 'total_duration': 34891.91277551651, 'accumulated_submission_time': 33696.61943101883, 'accumulated_eval_time': 1189.5462565422058, 'accumulated_logging_time': 2.544489622116089, 'global_step': 100080, 'preemption_count': 0}), (101597, {'train/accuracy': 0.7987882494926453, 'train/loss': 0.9974828362464905, 'validation/accuracy': 0.7042399644851685, 'validation/loss': 1.3996078968048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5823000073432922, 'test/loss': 2.036078691482544, 'test/num_examples': 10000, 'score': 34206.52481389046, 'total_duration': 35419.85327458382, 'accumulated_submission_time': 34206.52481389046, 'accumulated_eval_time': 1207.4842991828918, 'accumulated_logging_time': 2.5927844047546387, 'global_step': 101597, 'preemption_count': 0}), (103115, {'train/accuracy': 0.7995654940605164, 'train/loss': 0.9808319807052612, 'validation/accuracy': 0.7089200019836426, 'validation/loss': 1.3838368654251099, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0236704349517822, 'test/num_examples': 10000, 'score': 34716.570257902145, 'total_duration': 35947.67550730705, 'accumulated_submission_time': 34716.570257902145, 'accumulated_eval_time': 1225.1640086174011, 'accumulated_logging_time': 2.6405186653137207, 'global_step': 103115, 'preemption_count': 0}), (104632, {'train/accuracy': 0.8382493257522583, 'train/loss': 0.8604050874710083, 'validation/accuracy': 0.7122600078582764, 'validation/loss': 1.3814303874969482, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.0447278022766113, 'test/num_examples': 10000, 'score': 35226.643216609955, 'total_duration': 36475.528554201126, 'accumulated_submission_time': 35226.643216609955, 'accumulated_eval_time': 1242.8338084220886, 'accumulated_logging_time': 2.702409267425537, 'global_step': 104632, 'preemption_count': 0}), (106149, {'train/accuracy': 0.8146125674247742, 'train/loss': 0.9301750063896179, 'validation/accuracy': 0.7082599997520447, 'validation/loss': 1.390093445777893, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.0411343574523926, 'test/num_examples': 10000, 'score': 35736.733736753464, 'total_duration': 37003.26521921158, 'accumulated_submission_time': 35736.733736753464, 'accumulated_eval_time': 1260.3823611736298, 'accumulated_logging_time': 2.751145124435425, 'global_step': 106149, 'preemption_count': 0}), (107667, {'train/accuracy': 0.8170041441917419, 'train/loss': 0.9116354584693909, 'validation/accuracy': 0.7121999859809875, 'validation/loss': 1.3599272966384888, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 2.011181116104126, 'test/num_examples': 10000, 'score': 36246.702476263046, 'total_duration': 37530.93986582756, 'accumulated_submission_time': 36246.702476263046, 'accumulated_eval_time': 1277.9910361766815, 'accumulated_logging_time': 2.79943585395813, 'global_step': 107667, 'preemption_count': 0}), (109184, {'train/accuracy': 0.8053650856018066, 'train/loss': 0.9917887449264526, 'validation/accuracy': 0.7057399749755859, 'validation/loss': 1.4216396808624268, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 2.094308614730835, 'test/num_examples': 10000, 'score': 36756.72316074371, 'total_duration': 38059.57973694801, 'accumulated_submission_time': 36756.72316074371, 'accumulated_eval_time': 1296.5142834186554, 'accumulated_logging_time': 2.8462047576904297, 'global_step': 109184, 'preemption_count': 0}), (110701, {'train/accuracy': 0.8196946382522583, 'train/loss': 0.9097425937652588, 'validation/accuracy': 0.7184199690818787, 'validation/loss': 1.3392237424850464, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.9844063520431519, 'test/num_examples': 10000, 'score': 37266.88493037224, 'total_duration': 38587.46114182472, 'accumulated_submission_time': 37266.88493037224, 'accumulated_eval_time': 1314.1434371471405, 'accumulated_logging_time': 2.8875298500061035, 'global_step': 110701, 'preemption_count': 0}), (112219, {'train/accuracy': 0.8258330225944519, 'train/loss': 0.8885695338249207, 'validation/accuracy': 0.7242000102996826, 'validation/loss': 1.3239599466323853, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.9667091369628906, 'test/num_examples': 10000, 'score': 37777.112483501434, 'total_duration': 39115.840937137604, 'accumulated_submission_time': 37777.112483501434, 'accumulated_eval_time': 1332.1986136436462, 'accumulated_logging_time': 2.9358580112457275, 'global_step': 112219, 'preemption_count': 0}), (113736, {'train/accuracy': 0.8484932780265808, 'train/loss': 0.8184272646903992, 'validation/accuracy': 0.7218599915504456, 'validation/loss': 1.3505167961120605, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.9928845167160034, 'test/num_examples': 10000, 'score': 38287.02067565918, 'total_duration': 39643.61619019508, 'accumulated_submission_time': 38287.02067565918, 'accumulated_eval_time': 1349.967808008194, 'accumulated_logging_time': 2.9852118492126465, 'global_step': 113736, 'preemption_count': 0}), (115253, {'train/accuracy': 0.8364556431770325, 'train/loss': 0.8760803937911987, 'validation/accuracy': 0.7188000082969666, 'validation/loss': 1.3691059350967407, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 2.007413864135742, 'test/num_examples': 10000, 'score': 38796.95326471329, 'total_duration': 40171.41336941719, 'accumulated_submission_time': 38796.95326471329, 'accumulated_eval_time': 1367.7323701381683, 'accumulated_logging_time': 3.036560297012329, 'global_step': 115253, 'preemption_count': 0}), (116770, {'train/accuracy': 0.8357780575752258, 'train/loss': 0.8407694101333618, 'validation/accuracy': 0.7210800051689148, 'validation/loss': 1.322500228881836, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.9723232984542847, 'test/num_examples': 10000, 'score': 39307.07882666588, 'total_duration': 40699.153477191925, 'accumulated_submission_time': 39307.07882666588, 'accumulated_eval_time': 1385.2446112632751, 'accumulated_logging_time': 3.0897364616394043, 'global_step': 116770, 'preemption_count': 0}), (118288, {'train/accuracy': 0.8320910334587097, 'train/loss': 0.842000424861908, 'validation/accuracy': 0.7231999635696411, 'validation/loss': 1.3130041360855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.9613901376724243, 'test/num_examples': 10000, 'score': 39817.029213666916, 'total_duration': 41226.97157096863, 'accumulated_submission_time': 39817.029213666916, 'accumulated_eval_time': 1403.0130491256714, 'accumulated_logging_time': 3.1403777599334717, 'global_step': 118288, 'preemption_count': 0}), (119806, {'train/accuracy': 0.8359375, 'train/loss': 0.8376394510269165, 'validation/accuracy': 0.7249799966812134, 'validation/loss': 1.3017497062683105, 'validation/num_examples': 50000, 'test/accuracy': 0.6025000214576721, 'test/loss': 1.9448192119598389, 'test/num_examples': 10000, 'score': 40327.05902671814, 'total_duration': 41755.553564310074, 'accumulated_submission_time': 40327.05902671814, 'accumulated_eval_time': 1421.460168838501, 'accumulated_logging_time': 3.1964597702026367, 'global_step': 119806, 'preemption_count': 0}), (121323, {'train/accuracy': 0.8392458558082581, 'train/loss': 0.7929951548576355, 'validation/accuracy': 0.7260000109672546, 'validation/loss': 1.2777409553527832, 'validation/num_examples': 50000, 'test/accuracy': 0.5984000563621521, 'test/loss': 1.9204200506210327, 'test/num_examples': 10000, 'score': 40836.98566198349, 'total_duration': 42283.02461075783, 'accumulated_submission_time': 40836.98566198349, 'accumulated_eval_time': 1438.906097650528, 'accumulated_logging_time': 3.2463817596435547, 'global_step': 121323, 'preemption_count': 0}), (122840, {'train/accuracy': 0.856465220451355, 'train/loss': 0.7566264867782593, 'validation/accuracy': 0.7263199687004089, 'validation/loss': 1.3030215501785278, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9397914409637451, 'test/num_examples': 10000, 'score': 41347.065252542496, 'total_duration': 42810.877432107925, 'accumulated_submission_time': 41347.065252542496, 'accumulated_eval_time': 1456.5793447494507, 'accumulated_logging_time': 3.2976536750793457, 'global_step': 122840, 'preemption_count': 0}), (124357, {'train/accuracy': 0.8516820669174194, 'train/loss': 0.7917265295982361, 'validation/accuracy': 0.7299000024795532, 'validation/loss': 1.3059499263763428, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.9366973638534546, 'test/num_examples': 10000, 'score': 41857.15559768677, 'total_duration': 43339.1133646965, 'accumulated_submission_time': 41857.15559768677, 'accumulated_eval_time': 1474.6264395713806, 'accumulated_logging_time': 3.3471579551696777, 'global_step': 124357, 'preemption_count': 0}), (125875, {'train/accuracy': 0.8467992544174194, 'train/loss': 0.8255721926689148, 'validation/accuracy': 0.7300599813461304, 'validation/loss': 1.3159984350204468, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.9633678197860718, 'test/num_examples': 10000, 'score': 42367.3814136982, 'total_duration': 43867.16249752045, 'accumulated_submission_time': 42367.3814136982, 'accumulated_eval_time': 1492.354014635086, 'accumulated_logging_time': 3.394263982772827, 'global_step': 125875, 'preemption_count': 0}), (127392, {'train/accuracy': 0.8463209271430969, 'train/loss': 0.8251389861106873, 'validation/accuracy': 0.7315599918365479, 'validation/loss': 1.3149410486221313, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.9545058012008667, 'test/num_examples': 10000, 'score': 42877.36577987671, 'total_duration': 44394.94526076317, 'accumulated_submission_time': 42877.36577987671, 'accumulated_eval_time': 1510.0505783557892, 'accumulated_logging_time': 3.447601556777954, 'global_step': 127392, 'preemption_count': 0}), (128909, {'train/accuracy': 0.8551897406578064, 'train/loss': 0.7878064513206482, 'validation/accuracy': 0.7348999977111816, 'validation/loss': 1.2902276515960693, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.9117834568023682, 'test/num_examples': 10000, 'score': 43387.268907785416, 'total_duration': 44922.67690491676, 'accumulated_submission_time': 43387.268907785416, 'accumulated_eval_time': 1527.7779560089111, 'accumulated_logging_time': 3.5003559589385986, 'global_step': 128909, 'preemption_count': 0}), (130426, {'train/accuracy': 0.8754384517669678, 'train/loss': 0.675972580909729, 'validation/accuracy': 0.7377600073814392, 'validation/loss': 1.2478464841842651, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.8745336532592773, 'test/num_examples': 10000, 'score': 43897.26140189171, 'total_duration': 45450.63581967354, 'accumulated_submission_time': 43897.26140189171, 'accumulated_eval_time': 1545.6414897441864, 'accumulated_logging_time': 3.5544705390930176, 'global_step': 130426, 'preemption_count': 0}), (131942, {'train/accuracy': 0.8747608065605164, 'train/loss': 0.712469220161438, 'validation/accuracy': 0.7366399765014648, 'validation/loss': 1.283627986907959, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.9361129999160767, 'test/num_examples': 10000, 'score': 44407.33543777466, 'total_duration': 45979.166120290756, 'accumulated_submission_time': 44407.33543777466, 'accumulated_eval_time': 1563.9944469928741, 'accumulated_logging_time': 3.6094768047332764, 'global_step': 131942, 'preemption_count': 0}), (133460, {'train/accuracy': 0.872468888759613, 'train/loss': 0.7190256714820862, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.2690998315811157, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.9060282707214355, 'test/num_examples': 10000, 'score': 44917.43026852608, 'total_duration': 46507.20842504501, 'accumulated_submission_time': 44917.43026852608, 'accumulated_eval_time': 1581.8467528820038, 'accumulated_logging_time': 3.656475305557251, 'global_step': 133460, 'preemption_count': 0}), (134977, {'train/accuracy': 0.8716716766357422, 'train/loss': 0.7018413543701172, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.2499456405639648, 'validation/num_examples': 50000, 'test/accuracy': 0.6167000532150269, 'test/loss': 1.884137749671936, 'test/num_examples': 10000, 'score': 45427.43486452103, 'total_duration': 47034.92509889603, 'accumulated_submission_time': 45427.43486452103, 'accumulated_eval_time': 1599.4554841518402, 'accumulated_logging_time': 3.710766077041626, 'global_step': 134977, 'preemption_count': 0}), (136494, {'train/accuracy': 0.8722297549247742, 'train/loss': 0.7077077031135559, 'validation/accuracy': 0.7393400073051453, 'validation/loss': 1.2572718858718872, 'validation/num_examples': 50000, 'test/accuracy': 0.6141000390052795, 'test/loss': 1.8976376056671143, 'test/num_examples': 10000, 'score': 45937.36474323273, 'total_duration': 47562.581681489944, 'accumulated_submission_time': 45937.36474323273, 'accumulated_eval_time': 1617.0773251056671, 'accumulated_logging_time': 3.766352891921997, 'global_step': 136494, 'preemption_count': 0}), (138011, {'train/accuracy': 0.87015700340271, 'train/loss': 0.7002887725830078, 'validation/accuracy': 0.7409999966621399, 'validation/loss': 1.2513980865478516, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.8796846866607666, 'test/num_examples': 10000, 'score': 46447.27852892876, 'total_duration': 48090.08140492439, 'accumulated_submission_time': 46447.27852892876, 'accumulated_eval_time': 1634.5576055049896, 'accumulated_logging_time': 3.823425769805908, 'global_step': 138011, 'preemption_count': 0}), (139529, {'train/accuracy': 0.8952487111091614, 'train/loss': 0.6294954419136047, 'validation/accuracy': 0.7395399808883667, 'validation/loss': 1.2603678703308105, 'validation/num_examples': 50000, 'test/accuracy': 0.6126000285148621, 'test/loss': 1.9022327661514282, 'test/num_examples': 10000, 'score': 46957.439453840256, 'total_duration': 48618.27424407005, 'accumulated_submission_time': 46957.439453840256, 'accumulated_eval_time': 1652.4860928058624, 'accumulated_logging_time': 3.877878189086914, 'global_step': 139529, 'preemption_count': 0}), (141046, {'train/accuracy': 0.8868981003761292, 'train/loss': 0.6444560289382935, 'validation/accuracy': 0.7418599724769592, 'validation/loss': 1.247159719467163, 'validation/num_examples': 50000, 'test/accuracy': 0.6173000335693359, 'test/loss': 1.898269772529602, 'test/num_examples': 10000, 'score': 47467.50153207779, 'total_duration': 49146.01747059822, 'accumulated_submission_time': 47467.50153207779, 'accumulated_eval_time': 1670.0611016750336, 'accumulated_logging_time': 3.9352123737335205, 'global_step': 141046, 'preemption_count': 0}), (142564, {'train/accuracy': 0.8874959945678711, 'train/loss': 0.652847409248352, 'validation/accuracy': 0.741599977016449, 'validation/loss': 1.2515697479248047, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8916077613830566, 'test/num_examples': 10000, 'score': 47977.68201828003, 'total_duration': 49674.135682582855, 'accumulated_submission_time': 47977.68201828003, 'accumulated_eval_time': 1687.8899908065796, 'accumulated_logging_time': 3.9951000213623047, 'global_step': 142564, 'preemption_count': 0}), (144081, {'train/accuracy': 0.8844068646430969, 'train/loss': 0.6725389957427979, 'validation/accuracy': 0.7425000071525574, 'validation/loss': 1.2619949579238892, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8994200229644775, 'test/num_examples': 10000, 'score': 48487.624255895615, 'total_duration': 50201.76250863075, 'accumulated_submission_time': 48487.624255895615, 'accumulated_eval_time': 1705.4653916358948, 'accumulated_logging_time': 4.055452346801758, 'global_step': 144081, 'preemption_count': 0}), (145597, {'train/accuracy': 0.893973171710968, 'train/loss': 0.6204026341438293, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.2134588956832886, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8494324684143066, 'test/num_examples': 10000, 'score': 48997.56184363365, 'total_duration': 50729.53057074547, 'accumulated_submission_time': 48997.56184363365, 'accumulated_eval_time': 1723.1903104782104, 'accumulated_logging_time': 4.112022161483765, 'global_step': 145597, 'preemption_count': 0}), (147114, {'train/accuracy': 0.8909637928009033, 'train/loss': 0.6227953433990479, 'validation/accuracy': 0.7475199699401855, 'validation/loss': 1.2216311693191528, 'validation/num_examples': 50000, 'test/accuracy': 0.6261000037193298, 'test/loss': 1.849046230316162, 'test/num_examples': 10000, 'score': 49507.542610406876, 'total_duration': 51257.21557235718, 'accumulated_submission_time': 49507.542610406876, 'accumulated_eval_time': 1740.7729632854462, 'accumulated_logging_time': 4.185078859329224, 'global_step': 147114, 'preemption_count': 0}), (148632, {'train/accuracy': 0.9108936190605164, 'train/loss': 0.5526172518730164, 'validation/accuracy': 0.7495799660682678, 'validation/loss': 1.2158610820770264, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.8357360363006592, 'test/num_examples': 10000, 'score': 50017.68539023399, 'total_duration': 51785.398183107376, 'accumulated_submission_time': 50017.68539023399, 'accumulated_eval_time': 1758.6900448799133, 'accumulated_logging_time': 4.2591400146484375, 'global_step': 148632, 'preemption_count': 0}), (150148, {'train/accuracy': 0.9068877100944519, 'train/loss': 0.5834668278694153, 'validation/accuracy': 0.751039981842041, 'validation/loss': 1.21971595287323, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.8467143774032593, 'test/num_examples': 10000, 'score': 50527.5884706974, 'total_duration': 52313.0264942646, 'accumulated_submission_time': 50527.5884706974, 'accumulated_eval_time': 1776.3126661777496, 'accumulated_logging_time': 4.313075065612793, 'global_step': 150148, 'preemption_count': 0}), (151666, {'train/accuracy': 0.9066286683082581, 'train/loss': 0.5731101632118225, 'validation/accuracy': 0.753279983997345, 'validation/loss': 1.204783320426941, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8276084661483765, 'test/num_examples': 10000, 'score': 51037.737275362015, 'total_duration': 52841.011503219604, 'accumulated_submission_time': 51037.737275362015, 'accumulated_eval_time': 1794.0457956790924, 'accumulated_logging_time': 4.367350816726685, 'global_step': 151666, 'preemption_count': 0}), (153183, {'train/accuracy': 0.9112324714660645, 'train/loss': 0.56153404712677, 'validation/accuracy': 0.7550399899482727, 'validation/loss': 1.1990495920181274, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.8135104179382324, 'test/num_examples': 10000, 'score': 51547.88285636902, 'total_duration': 53368.90864777565, 'accumulated_submission_time': 51547.88285636902, 'accumulated_eval_time': 1811.6900601387024, 'accumulated_logging_time': 4.4254045486450195, 'global_step': 153183, 'preemption_count': 0}), (154701, {'train/accuracy': 0.9130659699440002, 'train/loss': 0.5653855800628662, 'validation/accuracy': 0.7539599537849426, 'validation/loss': 1.209978461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.840086817741394, 'test/num_examples': 10000, 'score': 52058.02461576462, 'total_duration': 53896.99075818062, 'accumulated_submission_time': 52058.02461576462, 'accumulated_eval_time': 1829.5243427753448, 'accumulated_logging_time': 4.483033180236816, 'global_step': 154701, 'preemption_count': 0}), (156219, {'train/accuracy': 0.9151187539100647, 'train/loss': 0.5467893481254578, 'validation/accuracy': 0.7532999515533447, 'validation/loss': 1.2041923999786377, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.8272104263305664, 'test/num_examples': 10000, 'score': 52568.0820877552, 'total_duration': 54425.057513952255, 'accumulated_submission_time': 52568.0820877552, 'accumulated_eval_time': 1847.3999516963959, 'accumulated_logging_time': 4.56777286529541, 'global_step': 156219, 'preemption_count': 0}), (157736, {'train/accuracy': 0.9242864847183228, 'train/loss': 0.5116089582443237, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.1921504735946655, 'validation/num_examples': 50000, 'test/accuracy': 0.6354000568389893, 'test/loss': 1.8231844902038574, 'test/num_examples': 10000, 'score': 53078.011219501495, 'total_duration': 54952.83845996857, 'accumulated_submission_time': 53078.011219501495, 'accumulated_eval_time': 1865.1478443145752, 'accumulated_logging_time': 4.623232364654541, 'global_step': 157736, 'preemption_count': 0}), (159253, {'train/accuracy': 0.9227519035339355, 'train/loss': 0.5066460371017456, 'validation/accuracy': 0.7560999989509583, 'validation/loss': 1.1838161945343018, 'validation/num_examples': 50000, 'test/accuracy': 0.6379000544548035, 'test/loss': 1.805983066558838, 'test/num_examples': 10000, 'score': 53588.11385130882, 'total_duration': 55481.25697994232, 'accumulated_submission_time': 53588.11385130882, 'accumulated_eval_time': 1883.357558965683, 'accumulated_logging_time': 4.680333614349365, 'global_step': 159253, 'preemption_count': 0}), (160772, {'train/accuracy': 0.9209582209587097, 'train/loss': 0.5264387726783752, 'validation/accuracy': 0.7565599679946899, 'validation/loss': 1.198883295059204, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.8179877996444702, 'test/num_examples': 10000, 'score': 54098.28979110718, 'total_duration': 56009.091359615326, 'accumulated_submission_time': 54098.28979110718, 'accumulated_eval_time': 1900.907883644104, 'accumulated_logging_time': 4.739240646362305, 'global_step': 160772, 'preemption_count': 0}), (162289, {'train/accuracy': 0.9260801672935486, 'train/loss': 0.5113489627838135, 'validation/accuracy': 0.7582199573516846, 'validation/loss': 1.190921425819397, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.8157532215118408, 'test/num_examples': 10000, 'score': 54608.4274187088, 'total_duration': 56537.093678474426, 'accumulated_submission_time': 54608.4274187088, 'accumulated_eval_time': 1918.6634063720703, 'accumulated_logging_time': 4.798632621765137, 'global_step': 162289, 'preemption_count': 0}), (163806, {'train/accuracy': 0.9243263602256775, 'train/loss': 0.5113154053688049, 'validation/accuracy': 0.7582599520683289, 'validation/loss': 1.1876296997070312, 'validation/num_examples': 50000, 'test/accuracy': 0.6383000016212463, 'test/loss': 1.8151981830596924, 'test/num_examples': 10000, 'score': 55118.39876580238, 'total_duration': 57065.14196181297, 'accumulated_submission_time': 55118.39876580238, 'accumulated_eval_time': 1936.598509311676, 'accumulated_logging_time': 4.890327453613281, 'global_step': 163806, 'preemption_count': 0}), (165323, {'train/accuracy': 0.9312419891357422, 'train/loss': 0.4898253381252289, 'validation/accuracy': 0.7599200010299683, 'validation/loss': 1.1844065189361572, 'validation/num_examples': 50000, 'test/accuracy': 0.6380000114440918, 'test/loss': 1.8147510290145874, 'test/num_examples': 10000, 'score': 55628.49181294441, 'total_duration': 57593.02518582344, 'accumulated_submission_time': 55628.49181294441, 'accumulated_eval_time': 1954.2799079418182, 'accumulated_logging_time': 4.950265169143677, 'global_step': 165323, 'preemption_count': 0}), (166840, {'train/accuracy': 0.9327367544174194, 'train/loss': 0.4756486117839813, 'validation/accuracy': 0.7603200078010559, 'validation/loss': 1.1765327453613281, 'validation/num_examples': 50000, 'test/accuracy': 0.6389000415802002, 'test/loss': 1.793910026550293, 'test/num_examples': 10000, 'score': 56138.64579749107, 'total_duration': 58120.9536485672, 'accumulated_submission_time': 56138.64579749107, 'accumulated_eval_time': 1971.945927143097, 'accumulated_logging_time': 5.009655952453613, 'global_step': 166840, 'preemption_count': 0}), (168357, {'train/accuracy': 0.9333944320678711, 'train/loss': 0.47956064343452454, 'validation/accuracy': 0.7605199813842773, 'validation/loss': 1.178816318511963, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.8000293970108032, 'test/num_examples': 10000, 'score': 56648.71825623512, 'total_duration': 58648.94144535065, 'accumulated_submission_time': 56648.71825623512, 'accumulated_eval_time': 1989.7520174980164, 'accumulated_logging_time': 5.06985878944397, 'global_step': 168357, 'preemption_count': 0}), (169874, {'train/accuracy': 0.9356465339660645, 'train/loss': 0.46728038787841797, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 1.1741501092910767, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.791094422340393, 'test/num_examples': 10000, 'score': 57158.62016701698, 'total_duration': 59176.86003422737, 'accumulated_submission_time': 57158.62016701698, 'accumulated_eval_time': 2007.657867193222, 'accumulated_logging_time': 5.131593704223633, 'global_step': 169874, 'preemption_count': 0}), (171391, {'train/accuracy': 0.9360251426696777, 'train/loss': 0.4715806841850281, 'validation/accuracy': 0.7620599865913391, 'validation/loss': 1.177618145942688, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.793129324913025, 'test/num_examples': 10000, 'score': 57668.82580900192, 'total_duration': 59704.59183359146, 'accumulated_submission_time': 57668.82580900192, 'accumulated_eval_time': 2025.0748419761658, 'accumulated_logging_time': 5.192117214202881, 'global_step': 171391, 'preemption_count': 0}), (172908, {'train/accuracy': 0.935546875, 'train/loss': 0.47176775336265564, 'validation/accuracy': 0.7625799775123596, 'validation/loss': 1.175244688987732, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.8016676902770996, 'test/num_examples': 10000, 'score': 58178.83932805061, 'total_duration': 60232.509991168976, 'accumulated_submission_time': 58178.83932805061, 'accumulated_eval_time': 2042.8664588928223, 'accumulated_logging_time': 5.2558934688568115, 'global_step': 172908, 'preemption_count': 0}), (174424, {'train/accuracy': 0.9393534660339355, 'train/loss': 0.45975035429000854, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.1763155460357666, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.7961137294769287, 'test/num_examples': 10000, 'score': 58688.776404857635, 'total_duration': 60760.27409243584, 'accumulated_submission_time': 58688.776404857635, 'accumulated_eval_time': 2060.579292535782, 'accumulated_logging_time': 5.320931911468506, 'global_step': 174424, 'preemption_count': 0}), (175941, {'train/accuracy': 0.9392338991165161, 'train/loss': 0.45668110251426697, 'validation/accuracy': 0.7627999782562256, 'validation/loss': 1.1772570610046387, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.7967631816864014, 'test/num_examples': 10000, 'score': 59198.87612986565, 'total_duration': 61288.2229487896, 'accumulated_submission_time': 59198.87612986565, 'accumulated_eval_time': 2078.3175649642944, 'accumulated_logging_time': 5.382373809814453, 'global_step': 175941, 'preemption_count': 0}), (177458, {'train/accuracy': 0.9397919178009033, 'train/loss': 0.45647042989730835, 'validation/accuracy': 0.7625600099563599, 'validation/loss': 1.1731715202331543, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7910445928573608, 'test/num_examples': 10000, 'score': 59708.882165670395, 'total_duration': 61815.838076114655, 'accumulated_submission_time': 59708.882165670395, 'accumulated_eval_time': 2095.8211035728455, 'accumulated_logging_time': 5.4390709400177, 'global_step': 177458, 'preemption_count': 0}), (178976, {'train/accuracy': 0.9402303695678711, 'train/loss': 0.4584923982620239, 'validation/accuracy': 0.7633799910545349, 'validation/loss': 1.1745049953460693, 'validation/num_examples': 50000, 'test/accuracy': 0.6419000029563904, 'test/loss': 1.791907548904419, 'test/num_examples': 10000, 'score': 60219.034625291824, 'total_duration': 62343.77858257294, 'accumulated_submission_time': 60219.034625291824, 'accumulated_eval_time': 2113.4979746341705, 'accumulated_logging_time': 5.501019239425659, 'global_step': 178976, 'preemption_count': 0}), (180493, {'train/accuracy': 0.9389349222183228, 'train/loss': 0.45853328704833984, 'validation/accuracy': 0.7633999586105347, 'validation/loss': 1.175327181816101, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.7949339151382446, 'test/num_examples': 10000, 'score': 60729.01041841507, 'total_duration': 62871.621817588806, 'accumulated_submission_time': 60729.01041841507, 'accumulated_eval_time': 2131.2499561309814, 'accumulated_logging_time': 5.567572593688965, 'global_step': 180493, 'preemption_count': 0}), (182010, {'train/accuracy': 0.93949294090271, 'train/loss': 0.45538613200187683, 'validation/accuracy': 0.7636399865150452, 'validation/loss': 1.1701064109802246, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.789433240890503, 'test/num_examples': 10000, 'score': 61239.120810985565, 'total_duration': 63399.412459373474, 'accumulated_submission_time': 61239.120810985565, 'accumulated_eval_time': 2148.8214042186737, 'accumulated_logging_time': 5.627314567565918, 'global_step': 182010, 'preemption_count': 0}), (183527, {'train/accuracy': 0.9393534660339355, 'train/loss': 0.45500364899635315, 'validation/accuracy': 0.7640799880027771, 'validation/loss': 1.1725746393203735, 'validation/num_examples': 50000, 'test/accuracy': 0.6444000601768494, 'test/loss': 1.7934223413467407, 'test/num_examples': 10000, 'score': 61749.299342632294, 'total_duration': 63927.24230384827, 'accumulated_submission_time': 61749.299342632294, 'accumulated_eval_time': 2166.364020586014, 'accumulated_logging_time': 5.686914920806885, 'global_step': 183527, 'preemption_count': 0}), (185044, {'train/accuracy': 0.9412667155265808, 'train/loss': 0.4489424228668213, 'validation/accuracy': 0.763979971408844, 'validation/loss': 1.1695361137390137, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.7893344163894653, 'test/num_examples': 10000, 'score': 62259.412586927414, 'total_duration': 64455.15799450874, 'accumulated_submission_time': 62259.412586927414, 'accumulated_eval_time': 2184.0561985969543, 'accumulated_logging_time': 5.7481689453125, 'global_step': 185044, 'preemption_count': 0}), (186561, {'train/accuracy': 0.9403699040412903, 'train/loss': 0.452570378780365, 'validation/accuracy': 0.7641800045967102, 'validation/loss': 1.1703591346740723, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7891464233398438, 'test/num_examples': 10000, 'score': 62769.59384036064, 'total_duration': 64982.96338915825, 'accumulated_submission_time': 62769.59384036064, 'accumulated_eval_time': 2201.5659034252167, 'accumulated_logging_time': 5.813524484634399, 'global_step': 186561, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9392936825752258, 'train/loss': 0.4516395628452301, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 1.167160987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.787272572517395, 'test/num_examples': 10000, 'score': 62804.61339139938, 'total_duration': 65035.68202996254, 'accumulated_submission_time': 62804.61339139938, 'accumulated_eval_time': 2219.1971287727356, 'accumulated_logging_time': 5.877562046051025, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0128 13:45:56.711346 140169137129280 submission_runner.py:586] Timing: 62804.61339139938
I0128 13:45:56.711413 140169137129280 submission_runner.py:588] Total number of evals: 125
I0128 13:45:56.711456 140169137129280 submission_runner.py:589] ====================
I0128 13:45:56.711500 140169137129280 submission_runner.py:542] Using RNG seed 3614520272
I0128 13:45:56.712940 140169137129280 submission_runner.py:551] --- Tuning run 3/5 ---
I0128 13:45:56.713056 140169137129280 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3.
I0128 13:45:56.719162 140169137129280 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3/hparams.json.
I0128 13:45:56.719971 140169137129280 submission_runner.py:206] Initializing dataset.
I0128 13:45:56.732603 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0128 13:45:56.746509 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0128 13:45:56.927158 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0128 13:45:57.221686 140169137129280 submission_runner.py:213] Initializing model.
I0128 13:46:02.787501 140169137129280 submission_runner.py:255] Initializing optimizer.
I0128 13:46:03.433285 140169137129280 submission_runner.py:262] Initializing metrics bundle.
I0128 13:46:03.433458 140169137129280 submission_runner.py:280] Initializing checkpoint and logger.
I0128 13:46:03.447155 140169137129280 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0128 13:46:03.447274 140169137129280 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0128 13:46:15.921307 140169137129280 logger_utils.py:220] Unable to record git information. Continuing without it.
I0128 13:46:28.146052 140169137129280 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3/flags_0.json.
I0128 13:46:28.150848 140169137129280 submission_runner.py:314] Starting training loop.
I0128 13:46:58.680296 140004667934464 logging_writer.py:48] [0] global_step=0, grad_norm=0.6634446382522583, loss=6.919498443603516
I0128 13:46:58.693348 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:47:05.103964 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:47:13.698688 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:47:16.350821 140169137129280 submission_runner.py:408] Time since start: 48.20s, 	Step: 1, 	{'train/accuracy': 0.0006576849264092743, 'train/loss': 6.909994125366211, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 30.54241108894348, 'total_duration': 48.19990372657776, 'accumulated_submission_time': 30.54241108894348, 'accumulated_eval_time': 17.657403707504272, 'accumulated_logging_time': 0}
I0128 13:47:16.362701 140004676327168 logging_writer.py:48] [1] accumulated_eval_time=17.657404, accumulated_logging_time=0, accumulated_submission_time=30.542411, global_step=1, preemption_count=0, score=30.542411, test/accuracy=0.000600, test/loss=6.910250, test/num_examples=10000, total_duration=48.199904, train/accuracy=0.000658, train/loss=6.909994, validation/accuracy=0.000960, validation/loss=6.910244, validation/num_examples=50000
I0128 13:47:50.040187 140005305468672 logging_writer.py:48] [100] global_step=100, grad_norm=0.6422910690307617, loss=6.901210308074951
I0128 13:48:23.722346 140004676327168 logging_writer.py:48] [200] global_step=200, grad_norm=0.6539818048477173, loss=6.85966682434082
I0128 13:48:57.403212 140005305468672 logging_writer.py:48] [300] global_step=300, grad_norm=0.711705207824707, loss=6.770520210266113
I0128 13:49:31.127759 140004676327168 logging_writer.py:48] [400] global_step=400, grad_norm=0.741453230381012, loss=6.650941848754883
I0128 13:50:04.862523 140005305468672 logging_writer.py:48] [500] global_step=500, grad_norm=0.7978390455245972, loss=6.528564929962158
I0128 13:50:38.577552 140004676327168 logging_writer.py:48] [600] global_step=600, grad_norm=0.8509767651557922, loss=6.475006103515625
I0128 13:51:12.324080 140005305468672 logging_writer.py:48] [700] global_step=700, grad_norm=0.9111471772193909, loss=6.316205024719238
I0128 13:51:46.096492 140004676327168 logging_writer.py:48] [800] global_step=800, grad_norm=0.9988937377929688, loss=6.125086307525635
I0128 13:52:19.846282 140005305468672 logging_writer.py:48] [900] global_step=900, grad_norm=1.4699045419692993, loss=6.037616729736328
I0128 13:52:53.665002 140004676327168 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.9983644485473633, loss=6.005972862243652
I0128 13:53:27.428759 140005305468672 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.5328880548477173, loss=5.875402927398682
I0128 13:54:01.182073 140004676327168 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.2010064125061035, loss=5.830358028411865
I0128 13:54:34.921004 140005305468672 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.973191738128662, loss=5.657201766967773
I0128 13:55:08.637068 140004676327168 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.9692575931549072, loss=5.569962501525879
I0128 13:55:42.374859 140005305468672 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.2432355880737305, loss=5.5180511474609375
I0128 13:55:46.583370 140169137129280 spec.py:321] Evaluating on the training split.
I0128 13:55:53.021395 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 13:56:01.765617 140169137129280 spec.py:349] Evaluating on the test split.
I0128 13:56:04.512844 140169137129280 submission_runner.py:408] Time since start: 576.36s, 	Step: 1514, 	{'train/accuracy': 0.07302295416593552, 'train/loss': 5.302392959594727, 'validation/accuracy': 0.06689999997615814, 'validation/loss': 5.365999221801758, 'validation/num_examples': 50000, 'test/accuracy': 0.048600003123283386, 'test/loss': 5.617886543273926, 'test/num_examples': 10000, 'score': 540.7001566886902, 'total_duration': 576.3619163036346, 'accumulated_submission_time': 540.7001566886902, 'accumulated_eval_time': 35.58681917190552, 'accumulated_logging_time': 0.025716066360473633}
I0128 13:56:04.531660 140004676327168 logging_writer.py:48] [1514] accumulated_eval_time=35.586819, accumulated_logging_time=0.025716, accumulated_submission_time=540.700157, global_step=1514, preemption_count=0, score=540.700157, test/accuracy=0.048600, test/loss=5.617887, test/num_examples=10000, total_duration=576.361916, train/accuracy=0.073023, train/loss=5.302393, validation/accuracy=0.066900, validation/loss=5.365999, validation/num_examples=50000
I0128 13:56:33.866250 140005288683264 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.8372788429260254, loss=5.445355415344238
I0128 13:57:07.539806 140004676327168 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.909632921218872, loss=5.373727798461914
I0128 13:57:41.270398 140005288683264 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.952787160873413, loss=5.2500786781311035
I0128 13:58:15.008660 140004676327168 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.524838447570801, loss=5.198054790496826
I0128 13:58:48.734082 140005288683264 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.239443778991699, loss=5.062805652618408
I0128 13:59:22.567066 140004676327168 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.420957088470459, loss=5.026223182678223
I0128 13:59:56.299445 140005288683264 logging_writer.py:48] [2200] global_step=2200, grad_norm=6.896294116973877, loss=4.978001594543457
I0128 14:00:30.029567 140004676327168 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.733689785003662, loss=5.004350185394287
I0128 14:01:03.739784 140005288683264 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.384199619293213, loss=4.836941242218018
I0128 14:01:37.489518 140004676327168 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.5742082595825195, loss=4.837573051452637
I0128 14:02:11.240983 140005288683264 logging_writer.py:48] [2600] global_step=2600, grad_norm=7.422987461090088, loss=4.779816627502441
I0128 14:02:44.994135 140004676327168 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.349161624908447, loss=4.77105188369751
I0128 14:03:18.725550 140005288683264 logging_writer.py:48] [2800] global_step=2800, grad_norm=5.280250549316406, loss=4.703373908996582
I0128 14:03:52.472405 140004676327168 logging_writer.py:48] [2900] global_step=2900, grad_norm=5.701993942260742, loss=4.669005393981934
I0128 14:04:26.240198 140005288683264 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.478309154510498, loss=4.457527160644531
I0128 14:04:34.832233 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:04:41.200594 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:04:50.229206 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:04:52.870301 140169137129280 submission_runner.py:408] Time since start: 1104.72s, 	Step: 3027, 	{'train/accuracy': 0.1718152016401291, 'train/loss': 4.267473220825195, 'validation/accuracy': 0.15605999529361725, 'validation/loss': 4.383291721343994, 'validation/num_examples': 50000, 'test/accuracy': 0.1161000058054924, 'test/loss': 4.863234043121338, 'test/num_examples': 10000, 'score': 1050.9415996074677, 'total_duration': 1104.719381570816, 'accumulated_submission_time': 1050.9415996074677, 'accumulated_eval_time': 53.62484097480774, 'accumulated_logging_time': 0.055043935775756836}
I0128 14:04:52.888172 140004676327168 logging_writer.py:48] [3027] accumulated_eval_time=53.624841, accumulated_logging_time=0.055044, accumulated_submission_time=1050.941600, global_step=3027, preemption_count=0, score=1050.941600, test/accuracy=0.116100, test/loss=4.863234, test/num_examples=10000, total_duration=1104.719382, train/accuracy=0.171815, train/loss=4.267473, validation/accuracy=0.156060, validation/loss=4.383292, validation/num_examples=50000
I0128 14:05:17.851705 140005322254080 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.747740268707275, loss=4.513782978057861
I0128 14:05:51.656072 140004676327168 logging_writer.py:48] [3200] global_step=3200, grad_norm=9.580976486206055, loss=4.415099143981934
I0128 14:06:25.357799 140005322254080 logging_writer.py:48] [3300] global_step=3300, grad_norm=5.8487548828125, loss=4.471198081970215
I0128 14:06:59.094186 140004676327168 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.986414909362793, loss=4.433009624481201
I0128 14:07:32.785004 140005322254080 logging_writer.py:48] [3500] global_step=3500, grad_norm=7.357248783111572, loss=4.259864807128906
I0128 14:08:06.523360 140004676327168 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.785696983337402, loss=4.229288101196289
I0128 14:08:40.258382 140005322254080 logging_writer.py:48] [3700] global_step=3700, grad_norm=7.331911563873291, loss=4.227832317352295
I0128 14:09:13.993896 140004676327168 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.852302551269531, loss=4.061699867248535
I0128 14:09:47.696348 140005322254080 logging_writer.py:48] [3900] global_step=3900, grad_norm=8.050774574279785, loss=4.0989155769348145
I0128 14:10:21.416498 140004676327168 logging_writer.py:48] [4000] global_step=4000, grad_norm=5.089019298553467, loss=4.106430530548096
I0128 14:10:55.159833 140005322254080 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.747235298156738, loss=4.087098121643066
I0128 14:11:28.897624 140004676327168 logging_writer.py:48] [4200] global_step=4200, grad_norm=9.397979736328125, loss=3.994731903076172
I0128 14:12:02.704908 140005322254080 logging_writer.py:48] [4300] global_step=4300, grad_norm=7.235872268676758, loss=4.0012664794921875
I0128 14:12:36.471072 140004676327168 logging_writer.py:48] [4400] global_step=4400, grad_norm=5.433344841003418, loss=3.942111015319824
I0128 14:13:10.234682 140005322254080 logging_writer.py:48] [4500] global_step=4500, grad_norm=10.48851203918457, loss=3.8341753482818604
I0128 14:13:22.887752 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:13:29.300694 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:13:37.976576 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:13:40.579274 140169137129280 submission_runner.py:408] Time since start: 1632.43s, 	Step: 4539, 	{'train/accuracy': 0.27080675959587097, 'train/loss': 3.4995758533477783, 'validation/accuracy': 0.24903999269008636, 'validation/loss': 3.6413185596466064, 'validation/num_examples': 50000, 'test/accuracy': 0.1802000105381012, 'test/loss': 4.247800827026367, 'test/num_examples': 10000, 'score': 1560.8828208446503, 'total_duration': 1632.4283664226532, 'accumulated_submission_time': 1560.8828208446503, 'accumulated_eval_time': 71.31632661819458, 'accumulated_logging_time': 0.08308601379394531}
I0128 14:13:40.599284 140005297075968 logging_writer.py:48] [4539] accumulated_eval_time=71.316327, accumulated_logging_time=0.083086, accumulated_submission_time=1560.882821, global_step=4539, preemption_count=0, score=1560.882821, test/accuracy=0.180200, test/loss=4.247801, test/num_examples=10000, total_duration=1632.428366, train/accuracy=0.270807, train/loss=3.499576, validation/accuracy=0.249040, validation/loss=3.641319, validation/num_examples=50000
I0128 14:14:01.514992 140005305468672 logging_writer.py:48] [4600] global_step=4600, grad_norm=6.155745506286621, loss=3.764735221862793
I0128 14:14:35.203124 140005297075968 logging_writer.py:48] [4700] global_step=4700, grad_norm=6.271332740783691, loss=3.7976653575897217
I0128 14:15:08.954670 140005305468672 logging_writer.py:48] [4800] global_step=4800, grad_norm=10.177705764770508, loss=3.6958346366882324
I0128 14:15:42.714268 140005297075968 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.725825786590576, loss=3.725985050201416
I0128 14:16:16.497148 140005305468672 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.0425519943237305, loss=3.625767230987549
I0128 14:16:50.264827 140005297075968 logging_writer.py:48] [5100] global_step=5100, grad_norm=13.520610809326172, loss=3.559575319290161
I0128 14:17:24.038980 140005305468672 logging_writer.py:48] [5200] global_step=5200, grad_norm=8.716532707214355, loss=3.551633834838867
I0128 14:17:57.767754 140005297075968 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.98645544052124, loss=3.661684513092041
I0128 14:18:31.622479 140005305468672 logging_writer.py:48] [5400] global_step=5400, grad_norm=10.541708946228027, loss=3.549471855163574
I0128 14:19:05.383588 140005297075968 logging_writer.py:48] [5500] global_step=5500, grad_norm=11.524239540100098, loss=3.469174861907959
I0128 14:19:39.116259 140005305468672 logging_writer.py:48] [5600] global_step=5600, grad_norm=11.421966552734375, loss=3.527453899383545
I0128 14:20:12.878792 140005297075968 logging_writer.py:48] [5700] global_step=5700, grad_norm=8.050196647644043, loss=3.404697895050049
I0128 14:20:46.622398 140005305468672 logging_writer.py:48] [5800] global_step=5800, grad_norm=7.451212406158447, loss=3.467557668685913
I0128 14:21:20.395654 140005297075968 logging_writer.py:48] [5900] global_step=5900, grad_norm=6.5606184005737305, loss=3.3519763946533203
I0128 14:21:54.151436 140005305468672 logging_writer.py:48] [6000] global_step=6000, grad_norm=9.858002662658691, loss=3.279759168624878
I0128 14:22:10.837663 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:22:17.251196 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:22:26.289445 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:22:29.003984 140169137129280 submission_runner.py:408] Time since start: 2160.85s, 	Step: 6051, 	{'train/accuracy': 0.36945950984954834, 'train/loss': 2.88207745552063, 'validation/accuracy': 0.34147998690605164, 'validation/loss': 3.04559588432312, 'validation/num_examples': 50000, 'test/accuracy': 0.2535000145435333, 'test/loss': 3.7423603534698486, 'test/num_examples': 10000, 'score': 2071.0617468357086, 'total_duration': 2160.8530712127686, 'accumulated_submission_time': 2071.0617468357086, 'accumulated_eval_time': 89.48260450363159, 'accumulated_logging_time': 0.1137394905090332}
I0128 14:22:29.024658 140004676327168 logging_writer.py:48] [6051] accumulated_eval_time=89.482605, accumulated_logging_time=0.113739, accumulated_submission_time=2071.061747, global_step=6051, preemption_count=0, score=2071.061747, test/accuracy=0.253500, test/loss=3.742360, test/num_examples=10000, total_duration=2160.853071, train/accuracy=0.369460, train/loss=2.882077, validation/accuracy=0.341480, validation/loss=3.045596, validation/num_examples=50000
I0128 14:22:45.889283 140005288683264 logging_writer.py:48] [6100] global_step=6100, grad_norm=5.776933193206787, loss=3.272493839263916
I0128 14:23:19.593124 140004676327168 logging_writer.py:48] [6200] global_step=6200, grad_norm=8.773853302001953, loss=3.196320056915283
I0128 14:23:53.324221 140005288683264 logging_writer.py:48] [6300] global_step=6300, grad_norm=6.957119941711426, loss=3.297680139541626
I0128 14:24:27.047818 140004676327168 logging_writer.py:48] [6400] global_step=6400, grad_norm=7.780928611755371, loss=3.255967617034912
I0128 14:25:00.881943 140005288683264 logging_writer.py:48] [6500] global_step=6500, grad_norm=8.688368797302246, loss=3.346930742263794
I0128 14:25:34.630375 140004676327168 logging_writer.py:48] [6600] global_step=6600, grad_norm=10.737655639648438, loss=3.119685411453247
I0128 14:26:08.380711 140005288683264 logging_writer.py:48] [6700] global_step=6700, grad_norm=7.820777893066406, loss=3.1874520778656006
I0128 14:26:42.110960 140004676327168 logging_writer.py:48] [6800] global_step=6800, grad_norm=7.289512634277344, loss=3.0931568145751953
I0128 14:27:15.852451 140005288683264 logging_writer.py:48] [6900] global_step=6900, grad_norm=6.474232196807861, loss=3.0870261192321777
I0128 14:27:49.599500 140004676327168 logging_writer.py:48] [7000] global_step=7000, grad_norm=4.554122447967529, loss=3.0956356525421143
I0128 14:28:23.355878 140005288683264 logging_writer.py:48] [7100] global_step=7100, grad_norm=6.517308712005615, loss=3.118743658065796
I0128 14:28:57.097914 140004676327168 logging_writer.py:48] [7200] global_step=7200, grad_norm=10.09157943725586, loss=3.11061954498291
I0128 14:29:30.836777 140005288683264 logging_writer.py:48] [7300] global_step=7300, grad_norm=6.729843616485596, loss=3.050259590148926
I0128 14:30:04.567395 140004676327168 logging_writer.py:48] [7400] global_step=7400, grad_norm=15.514636039733887, loss=3.1092000007629395
I0128 14:30:38.303985 140005288683264 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.094234466552734, loss=2.9992151260375977
I0128 14:30:59.130574 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:31:06.259103 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:31:15.267319 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:31:18.604824 140169137129280 submission_runner.py:408] Time since start: 2690.45s, 	Step: 7563, 	{'train/accuracy': 0.45023515820503235, 'train/loss': 2.4000375270843506, 'validation/accuracy': 0.39282000064849854, 'validation/loss': 2.7543821334838867, 'validation/num_examples': 50000, 'test/accuracy': 0.2979000210762024, 'test/loss': 3.4580790996551514, 'test/num_examples': 10000, 'score': 2581.1082031726837, 'total_duration': 2690.453903913498, 'accumulated_submission_time': 2581.1082031726837, 'accumulated_eval_time': 108.95680356025696, 'accumulated_logging_time': 0.144883394241333}
I0128 14:31:18.621145 140004676327168 logging_writer.py:48] [7563] accumulated_eval_time=108.956804, accumulated_logging_time=0.144883, accumulated_submission_time=2581.108203, global_step=7563, preemption_count=0, score=2581.108203, test/accuracy=0.297900, test/loss=3.458079, test/num_examples=10000, total_duration=2690.453904, train/accuracy=0.450235, train/loss=2.400038, validation/accuracy=0.392820, validation/loss=2.754382, validation/num_examples=50000
I0128 14:31:31.427792 140005297075968 logging_writer.py:48] [7600] global_step=7600, grad_norm=7.5616455078125, loss=2.998676300048828
I0128 14:32:05.102818 140004676327168 logging_writer.py:48] [7700] global_step=7700, grad_norm=10.442184448242188, loss=2.9352073669433594
I0128 14:32:38.769127 140005297075968 logging_writer.py:48] [7800] global_step=7800, grad_norm=7.954776763916016, loss=2.875356674194336
I0128 14:33:12.514938 140004676327168 logging_writer.py:48] [7900] global_step=7900, grad_norm=10.13707447052002, loss=2.9644980430603027
I0128 14:33:46.201031 140005297075968 logging_writer.py:48] [8000] global_step=8000, grad_norm=9.970020294189453, loss=2.8845787048339844
I0128 14:34:19.936727 140004676327168 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.155618190765381, loss=2.8630998134613037
I0128 14:34:53.684378 140005297075968 logging_writer.py:48] [8200] global_step=8200, grad_norm=8.087204933166504, loss=2.866370677947998
I0128 14:35:27.433372 140004676327168 logging_writer.py:48] [8300] global_step=8300, grad_norm=7.020240783691406, loss=2.827996253967285
I0128 14:36:01.167206 140005297075968 logging_writer.py:48] [8400] global_step=8400, grad_norm=6.486457824707031, loss=2.861809730529785
I0128 14:36:34.916309 140004676327168 logging_writer.py:48] [8500] global_step=8500, grad_norm=7.682197093963623, loss=2.7598397731781006
I0128 14:37:08.622847 140005297075968 logging_writer.py:48] [8600] global_step=8600, grad_norm=5.047331809997559, loss=2.7779698371887207
I0128 14:37:42.450680 140004676327168 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.5162434577941895, loss=2.755826473236084
I0128 14:38:16.143730 140005297075968 logging_writer.py:48] [8800] global_step=8800, grad_norm=8.412603378295898, loss=2.791823148727417
I0128 14:38:49.915208 140004676327168 logging_writer.py:48] [8900] global_step=8900, grad_norm=8.325637817382812, loss=2.7684390544891357
I0128 14:39:23.680460 140005297075968 logging_writer.py:48] [9000] global_step=9000, grad_norm=8.60130786895752, loss=2.80595064163208
I0128 14:39:48.756142 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:39:55.123844 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:40:03.849110 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:40:06.439609 140169137129280 submission_runner.py:408] Time since start: 3218.29s, 	Step: 9076, 	{'train/accuracy': 0.49607381224632263, 'train/loss': 2.1706013679504395, 'validation/accuracy': 0.4490000009536743, 'validation/loss': 2.427058219909668, 'validation/num_examples': 50000, 'test/accuracy': 0.3509000241756439, 'test/loss': 3.098984479904175, 'test/num_examples': 10000, 'score': 3091.184275150299, 'total_duration': 3218.2887029647827, 'accumulated_submission_time': 3091.184275150299, 'accumulated_eval_time': 126.64023494720459, 'accumulated_logging_time': 0.17103147506713867}
I0128 14:40:06.458951 140005288683264 logging_writer.py:48] [9076] accumulated_eval_time=126.640235, accumulated_logging_time=0.171031, accumulated_submission_time=3091.184275, global_step=9076, preemption_count=0, score=3091.184275, test/accuracy=0.350900, test/loss=3.098984, test/num_examples=10000, total_duration=3218.288703, train/accuracy=0.496074, train/loss=2.170601, validation/accuracy=0.449000, validation/loss=2.427058, validation/num_examples=50000
I0128 14:40:14.889133 140005313861376 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.809879302978516, loss=2.5526583194732666
I0128 14:40:48.572275 140005288683264 logging_writer.py:48] [9200] global_step=9200, grad_norm=6.6411848068237305, loss=2.654928207397461
I0128 14:41:22.243308 140005313861376 logging_writer.py:48] [9300] global_step=9300, grad_norm=4.949934005737305, loss=2.7134385108947754
I0128 14:41:55.965153 140005288683264 logging_writer.py:48] [9400] global_step=9400, grad_norm=7.284761905670166, loss=2.6836371421813965
I0128 14:42:29.706709 140005313861376 logging_writer.py:48] [9500] global_step=9500, grad_norm=11.637571334838867, loss=2.5902769565582275
I0128 14:43:03.405717 140005288683264 logging_writer.py:48] [9600] global_step=9600, grad_norm=8.590895652770996, loss=2.6536223888397217
I0128 14:43:37.205398 140005313861376 logging_writer.py:48] [9700] global_step=9700, grad_norm=8.545928001403809, loss=2.7496073246002197
I0128 14:44:10.925859 140005288683264 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.996652603149414, loss=2.5508906841278076
I0128 14:44:44.638485 140005313861376 logging_writer.py:48] [9900] global_step=9900, grad_norm=7.555672645568848, loss=2.5586278438568115
I0128 14:45:18.330662 140005288683264 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.160584926605225, loss=2.602139711380005
I0128 14:45:52.012696 140005313861376 logging_writer.py:48] [10100] global_step=10100, grad_norm=7.808109283447266, loss=2.541517496109009
I0128 14:46:25.725757 140005288683264 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.951345920562744, loss=2.6774916648864746
I0128 14:46:59.377462 140005313861376 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.465359687805176, loss=2.590332508087158
I0128 14:47:33.061214 140005288683264 logging_writer.py:48] [10400] global_step=10400, grad_norm=6.22365140914917, loss=2.5962114334106445
I0128 14:48:06.763003 140005313861376 logging_writer.py:48] [10500] global_step=10500, grad_norm=5.767752170562744, loss=2.523442268371582
I0128 14:48:36.597224 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:48:43.090515 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:48:51.771455 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:48:54.407718 140169137129280 submission_runner.py:408] Time since start: 3746.26s, 	Step: 10590, 	{'train/accuracy': 0.5216438174247742, 'train/loss': 2.0399041175842285, 'validation/accuracy': 0.48475998640060425, 'validation/loss': 2.257338762283325, 'validation/num_examples': 50000, 'test/accuracy': 0.36410000920295715, 'test/loss': 3.029717445373535, 'test/num_examples': 10000, 'score': 3601.2611100673676, 'total_duration': 3746.2567975521088, 'accumulated_submission_time': 3601.2611100673676, 'accumulated_eval_time': 144.45068073272705, 'accumulated_logging_time': 0.2024247646331787}
I0128 14:48:54.427928 140004676327168 logging_writer.py:48] [10590] accumulated_eval_time=144.450681, accumulated_logging_time=0.202425, accumulated_submission_time=3601.261110, global_step=10590, preemption_count=0, score=3601.261110, test/accuracy=0.364100, test/loss=3.029717, test/num_examples=10000, total_duration=3746.256798, train/accuracy=0.521644, train/loss=2.039904, validation/accuracy=0.484760, validation/loss=2.257339, validation/num_examples=50000
I0128 14:48:58.159584 140005297075968 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.827800273895264, loss=2.4769368171691895
I0128 14:49:31.824095 140004676327168 logging_writer.py:48] [10700] global_step=10700, grad_norm=5.477447509765625, loss=2.48957896232605
I0128 14:50:05.618929 140005297075968 logging_writer.py:48] [10800] global_step=10800, grad_norm=8.024877548217773, loss=2.6084494590759277
I0128 14:50:39.341276 140004676327168 logging_writer.py:48] [10900] global_step=10900, grad_norm=6.431628227233887, loss=2.476384401321411
I0128 14:51:13.047078 140005297075968 logging_writer.py:48] [11000] global_step=11000, grad_norm=6.255470275878906, loss=2.3890795707702637
I0128 14:51:46.711400 140004676327168 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.819300651550293, loss=2.4473342895507812
I0128 14:52:20.406520 140005297075968 logging_writer.py:48] [11200] global_step=11200, grad_norm=5.384557723999023, loss=2.376695156097412
I0128 14:52:54.082639 140004676327168 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.047831058502197, loss=2.4434173107147217
I0128 14:53:27.795978 140005297075968 logging_writer.py:48] [11400] global_step=11400, grad_norm=6.5494184494018555, loss=2.522484302520752
I0128 14:54:01.467391 140004676327168 logging_writer.py:48] [11500] global_step=11500, grad_norm=6.257083415985107, loss=2.4551444053649902
I0128 14:54:35.187278 140005297075968 logging_writer.py:48] [11600] global_step=11600, grad_norm=5.274440288543701, loss=2.6303720474243164
I0128 14:55:08.872174 140004676327168 logging_writer.py:48] [11700] global_step=11700, grad_norm=8.78160572052002, loss=2.4292922019958496
I0128 14:55:42.568585 140005297075968 logging_writer.py:48] [11800] global_step=11800, grad_norm=6.867091655731201, loss=2.3914239406585693
I0128 14:56:16.337428 140004676327168 logging_writer.py:48] [11900] global_step=11900, grad_norm=7.5662431716918945, loss=2.501992702484131
I0128 14:56:50.014125 140005297075968 logging_writer.py:48] [12000] global_step=12000, grad_norm=8.023101806640625, loss=2.3258461952209473
I0128 14:57:23.698992 140004676327168 logging_writer.py:48] [12100] global_step=12100, grad_norm=6.104882717132568, loss=2.2361650466918945
I0128 14:57:24.521684 140169137129280 spec.py:321] Evaluating on the training split.
I0128 14:57:30.908514 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 14:57:39.925291 140169137129280 spec.py:349] Evaluating on the test split.
I0128 14:57:42.587604 140169137129280 submission_runner.py:408] Time since start: 4274.44s, 	Step: 12104, 	{'train/accuracy': 0.5573580861091614, 'train/loss': 1.8680070638656616, 'validation/accuracy': 0.5182600021362305, 'validation/loss': 2.074094533920288, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.803204298019409, 'test/num_examples': 10000, 'score': 4111.291719198227, 'total_duration': 4274.436697483063, 'accumulated_submission_time': 4111.291719198227, 'accumulated_eval_time': 162.51656198501587, 'accumulated_logging_time': 0.23605084419250488}
I0128 14:57:42.607481 140005313861376 logging_writer.py:48] [12104] accumulated_eval_time=162.516562, accumulated_logging_time=0.236051, accumulated_submission_time=4111.291719, global_step=12104, preemption_count=0, score=4111.291719, test/accuracy=0.397500, test/loss=2.803204, test/num_examples=10000, total_duration=4274.436697, train/accuracy=0.557358, train/loss=1.868007, validation/accuracy=0.518260, validation/loss=2.074095, validation/num_examples=50000
I0128 14:58:15.247364 140005322254080 logging_writer.py:48] [12200] global_step=12200, grad_norm=6.491443634033203, loss=2.279696464538574
I0128 14:58:48.911742 140005313861376 logging_writer.py:48] [12300] global_step=12300, grad_norm=5.356059551239014, loss=2.2023212909698486
I0128 14:59:22.598487 140005322254080 logging_writer.py:48] [12400] global_step=12400, grad_norm=6.557902812957764, loss=2.430210828781128
I0128 14:59:56.256601 140005313861376 logging_writer.py:48] [12500] global_step=12500, grad_norm=7.2835917472839355, loss=2.3726069927215576
I0128 15:00:29.953464 140005322254080 logging_writer.py:48] [12600] global_step=12600, grad_norm=8.238485336303711, loss=2.4129104614257812
I0128 15:01:03.664961 140005313861376 logging_writer.py:48] [12700] global_step=12700, grad_norm=7.635243892669678, loss=2.301105499267578
I0128 15:01:37.322625 140005322254080 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.295454502105713, loss=2.3300766944885254
I0128 15:02:11.013252 140005313861376 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.203222751617432, loss=2.3905739784240723
I0128 15:02:44.781229 140005322254080 logging_writer.py:48] [13000] global_step=13000, grad_norm=6.6995463371276855, loss=2.458749771118164
I0128 15:03:18.452308 140005313861376 logging_writer.py:48] [13100] global_step=13100, grad_norm=6.536040782928467, loss=2.3600735664367676
I0128 15:03:52.066293 140005322254080 logging_writer.py:48] [13200] global_step=13200, grad_norm=8.265872955322266, loss=2.2045257091522217
I0128 15:04:25.702010 140005313861376 logging_writer.py:48] [13300] global_step=13300, grad_norm=5.598678112030029, loss=2.2842671871185303
I0128 15:04:59.315638 140005322254080 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.160574913024902, loss=2.344984531402588
I0128 15:05:32.967774 140005313861376 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.501843452453613, loss=2.184844493865967
I0128 15:06:06.599097 140005322254080 logging_writer.py:48] [13600] global_step=13600, grad_norm=7.499996185302734, loss=2.229332208633423
I0128 15:06:12.822594 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:06:19.163158 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:06:27.919594 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:06:30.564299 140169137129280 submission_runner.py:408] Time since start: 4802.41s, 	Step: 13620, 	{'train/accuracy': 0.5631775856018066, 'train/loss': 1.82098388671875, 'validation/accuracy': 0.5280199646949768, 'validation/loss': 2.0292091369628906, 'validation/num_examples': 50000, 'test/accuracy': 0.4131000339984894, 'test/loss': 2.741434097290039, 'test/num_examples': 10000, 'score': 4621.447257757187, 'total_duration': 4802.413382053375, 'accumulated_submission_time': 4621.447257757187, 'accumulated_eval_time': 180.25822043418884, 'accumulated_logging_time': 0.2662384510040283}
I0128 15:06:30.589002 140004676327168 logging_writer.py:48] [13620] accumulated_eval_time=180.258220, accumulated_logging_time=0.266238, accumulated_submission_time=4621.447258, global_step=13620, preemption_count=0, score=4621.447258, test/accuracy=0.413100, test/loss=2.741434, test/num_examples=10000, total_duration=4802.413382, train/accuracy=0.563178, train/loss=1.820984, validation/accuracy=0.528020, validation/loss=2.029209, validation/num_examples=50000
I0128 15:06:57.806651 140005288683264 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.94883918762207, loss=2.308493137359619
I0128 15:07:31.474901 140004676327168 logging_writer.py:48] [13800] global_step=13800, grad_norm=6.541677474975586, loss=2.2051119804382324
I0128 15:08:05.155621 140005288683264 logging_writer.py:48] [13900] global_step=13900, grad_norm=6.332095146179199, loss=2.3709588050842285
I0128 15:08:38.854045 140004676327168 logging_writer.py:48] [14000] global_step=14000, grad_norm=11.166296005249023, loss=2.43625807762146
I0128 15:09:12.593850 140005288683264 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.488043785095215, loss=2.4349923133850098
I0128 15:09:46.268618 140004676327168 logging_writer.py:48] [14200] global_step=14200, grad_norm=5.113098621368408, loss=2.4407577514648438
I0128 15:10:19.933619 140005288683264 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.1502790451049805, loss=2.2474021911621094
I0128 15:10:53.561522 140004676327168 logging_writer.py:48] [14400] global_step=14400, grad_norm=9.367591857910156, loss=2.1426708698272705
I0128 15:11:27.172998 140005288683264 logging_writer.py:48] [14500] global_step=14500, grad_norm=7.099473476409912, loss=2.1940977573394775
I0128 15:12:00.857422 140004676327168 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.598674774169922, loss=2.2992491722106934
I0128 15:12:34.515830 140005288683264 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.761939764022827, loss=2.4475462436676025
I0128 15:13:08.167131 140004676327168 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.634397983551025, loss=2.1402833461761475
I0128 15:13:41.869541 140005288683264 logging_writer.py:48] [14900] global_step=14900, grad_norm=4.470439910888672, loss=2.187807083129883
I0128 15:14:15.534862 140004676327168 logging_writer.py:48] [15000] global_step=15000, grad_norm=9.725892066955566, loss=2.224092721939087
I0128 15:14:49.220345 140005288683264 logging_writer.py:48] [15100] global_step=15100, grad_norm=6.6274800300598145, loss=2.364400863647461
I0128 15:15:00.823202 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:15:07.281548 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:15:16.078309 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:15:18.721340 140169137129280 submission_runner.py:408] Time since start: 5330.57s, 	Step: 15136, 	{'train/accuracy': 0.5808952450752258, 'train/loss': 1.7555890083312988, 'validation/accuracy': 0.5410000085830688, 'validation/loss': 1.951683759689331, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.7061638832092285, 'test/num_examples': 10000, 'score': 5131.6218910217285, 'total_duration': 5330.570430994034, 'accumulated_submission_time': 5131.6218910217285, 'accumulated_eval_time': 198.1563205718994, 'accumulated_logging_time': 0.3011949062347412}
I0128 15:15:18.744120 140004676327168 logging_writer.py:48] [15136] accumulated_eval_time=198.156321, accumulated_logging_time=0.301195, accumulated_submission_time=5131.621891, global_step=15136, preemption_count=0, score=5131.621891, test/accuracy=0.417000, test/loss=2.706164, test/num_examples=10000, total_duration=5330.570431, train/accuracy=0.580895, train/loss=1.755589, validation/accuracy=0.541000, validation/loss=1.951684, validation/num_examples=50000
I0128 15:15:40.693384 140005297075968 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.8442301750183105, loss=2.253782033920288
I0128 15:16:14.267516 140004676327168 logging_writer.py:48] [15300] global_step=15300, grad_norm=6.965296268463135, loss=2.222714424133301
I0128 15:16:47.875953 140005297075968 logging_writer.py:48] [15400] global_step=15400, grad_norm=5.135825157165527, loss=2.2078640460968018
I0128 15:17:21.561453 140004676327168 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.717747211456299, loss=2.2760396003723145
I0128 15:17:55.198526 140005297075968 logging_writer.py:48] [15600] global_step=15600, grad_norm=6.666286945343018, loss=2.2901339530944824
I0128 15:18:28.809777 140004676327168 logging_writer.py:48] [15700] global_step=15700, grad_norm=11.554184913635254, loss=2.155198812484741
I0128 15:19:02.457477 140005297075968 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.567015647888184, loss=2.203812837600708
I0128 15:19:36.117510 140004676327168 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.645805835723877, loss=2.2182507514953613
I0128 15:20:09.825326 140005297075968 logging_writer.py:48] [16000] global_step=16000, grad_norm=4.72775411605835, loss=2.158796787261963
I0128 15:20:43.506506 140004676327168 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.866090774536133, loss=2.20033597946167
I0128 15:21:17.211750 140005297075968 logging_writer.py:48] [16200] global_step=16200, grad_norm=7.313698768615723, loss=2.3414742946624756
I0128 15:21:50.975966 140004676327168 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.190433502197266, loss=2.2850472927093506
I0128 15:22:24.662980 140005297075968 logging_writer.py:48] [16400] global_step=16400, grad_norm=8.992191314697266, loss=2.239973783493042
I0128 15:22:58.291577 140004676327168 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.234594821929932, loss=2.2473278045654297
I0128 15:23:31.932644 140005297075968 logging_writer.py:48] [16600] global_step=16600, grad_norm=6.279004096984863, loss=2.10841965675354
I0128 15:23:48.930815 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:23:55.322909 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:24:04.398752 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:24:07.054295 140169137129280 submission_runner.py:408] Time since start: 5858.90s, 	Step: 16652, 	{'train/accuracy': 0.6128427982330322, 'train/loss': 1.5677685737609863, 'validation/accuracy': 0.5491600036621094, 'validation/loss': 1.9117090702056885, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.689218759536743, 'test/num_examples': 10000, 'score': 5641.749358415604, 'total_duration': 5858.903384923935, 'accumulated_submission_time': 5641.749358415604, 'accumulated_eval_time': 216.2797749042511, 'accumulated_logging_time': 0.3344736099243164}
I0128 15:24:07.077137 140005313861376 logging_writer.py:48] [16652] accumulated_eval_time=216.279775, accumulated_logging_time=0.334474, accumulated_submission_time=5641.749358, global_step=16652, preemption_count=0, score=5641.749358, test/accuracy=0.424300, test/loss=2.689219, test/num_examples=10000, total_duration=5858.903385, train/accuracy=0.612843, train/loss=1.567769, validation/accuracy=0.549160, validation/loss=1.911709, validation/num_examples=50000
I0128 15:24:23.559617 140005322254080 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.695758819580078, loss=2.2098286151885986
I0128 15:24:57.189862 140005313861376 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.847532272338867, loss=2.236475706100464
I0128 15:25:30.833248 140005322254080 logging_writer.py:48] [16900] global_step=16900, grad_norm=6.502742767333984, loss=2.063401460647583
I0128 15:26:04.461399 140005313861376 logging_writer.py:48] [17000] global_step=17000, grad_norm=6.0611701011657715, loss=2.315964937210083
I0128 15:26:38.092766 140005322254080 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.813855171203613, loss=2.1701812744140625
I0128 15:27:11.778995 140005313861376 logging_writer.py:48] [17200] global_step=17200, grad_norm=5.344235420227051, loss=2.241708993911743
I0128 15:27:45.427244 140005322254080 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.836658954620361, loss=2.117840051651001
I0128 15:28:19.147420 140005313861376 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.6460752487182617, loss=2.189638614654541
I0128 15:28:52.830363 140005322254080 logging_writer.py:48] [17500] global_step=17500, grad_norm=5.143913269042969, loss=2.127399444580078
I0128 15:29:26.476198 140005313861376 logging_writer.py:48] [17600] global_step=17600, grad_norm=5.091393947601318, loss=2.2152230739593506
I0128 15:30:00.107420 140005322254080 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.9385910034179688, loss=2.191516876220703
I0128 15:30:33.752164 140005313861376 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.845161437988281, loss=2.2670047283172607
I0128 15:31:07.399836 140005322254080 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.876679420471191, loss=2.1525564193725586
I0128 15:31:41.063135 140005313861376 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.780320644378662, loss=2.1798148155212402
I0128 15:32:14.731725 140005322254080 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.237253189086914, loss=2.0963709354400635
I0128 15:32:37.095634 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:32:43.437410 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:32:52.351354 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:32:55.011535 140169137129280 submission_runner.py:408] Time since start: 6386.86s, 	Step: 18168, 	{'train/accuracy': 0.6028379797935486, 'train/loss': 1.6164047718048096, 'validation/accuracy': 0.5559599995613098, 'validation/loss': 1.8733510971069336, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.6176419258117676, 'test/num_examples': 10000, 'score': 6151.70788192749, 'total_duration': 6386.860626220703, 'accumulated_submission_time': 6151.70788192749, 'accumulated_eval_time': 234.1956398487091, 'accumulated_logging_time': 0.3683803081512451}
I0128 15:32:55.036427 140004676327168 logging_writer.py:48] [18168] accumulated_eval_time=234.195640, accumulated_logging_time=0.368380, accumulated_submission_time=6151.707882, global_step=18168, preemption_count=0, score=6151.707882, test/accuracy=0.431900, test/loss=2.617642, test/num_examples=10000, total_duration=6386.860626, train/accuracy=0.602838, train/loss=1.616405, validation/accuracy=0.555960, validation/loss=1.873351, validation/num_examples=50000
I0128 15:33:06.160785 140005288683264 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.510768413543701, loss=2.2742114067077637
I0128 15:33:39.747174 140004676327168 logging_writer.py:48] [18300] global_step=18300, grad_norm=4.397949695587158, loss=2.0895092487335205
I0128 15:34:13.398857 140005288683264 logging_writer.py:48] [18400] global_step=18400, grad_norm=5.1579813957214355, loss=2.024322986602783
I0128 15:34:47.145016 140004676327168 logging_writer.py:48] [18500] global_step=18500, grad_norm=7.562295913696289, loss=2.2061171531677246
I0128 15:35:20.804147 140005288683264 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.510740280151367, loss=2.2059569358825684
I0128 15:35:54.432615 140004676327168 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.860305070877075, loss=2.117060661315918
I0128 15:36:28.084207 140005288683264 logging_writer.py:48] [18800] global_step=18800, grad_norm=3.672919273376465, loss=2.2128682136535645
I0128 15:37:01.749961 140004676327168 logging_writer.py:48] [18900] global_step=18900, grad_norm=4.927088260650635, loss=2.191220283508301
I0128 15:37:35.402647 140005288683264 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.951210975646973, loss=2.0523152351379395
I0128 15:38:09.061892 140004676327168 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.8101308345794678, loss=2.062746524810791
I0128 15:38:42.693120 140005288683264 logging_writer.py:48] [19200] global_step=19200, grad_norm=5.763810634613037, loss=2.152113437652588
I0128 15:39:16.330223 140004676327168 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.588648319244385, loss=2.215723752975464
I0128 15:39:50.004119 140005288683264 logging_writer.py:48] [19400] global_step=19400, grad_norm=8.235357284545898, loss=2.1298210620880127
I0128 15:40:23.640688 140004676327168 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.148458480834961, loss=2.089850902557373
I0128 15:40:57.420351 140005288683264 logging_writer.py:48] [19600] global_step=19600, grad_norm=4.365752220153809, loss=2.064579963684082
I0128 15:41:25.165638 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:41:31.534960 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:41:40.568430 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:41:43.226557 140169137129280 submission_runner.py:408] Time since start: 6915.08s, 	Step: 19684, 	{'train/accuracy': 0.6129025816917419, 'train/loss': 1.5699564218521118, 'validation/accuracy': 0.5642799735069275, 'validation/loss': 1.820169448852539, 'validation/num_examples': 50000, 'test/accuracy': 0.4443000257015228, 'test/loss': 2.5521981716156006, 'test/num_examples': 10000, 'score': 6661.777126550674, 'total_duration': 6915.075645208359, 'accumulated_submission_time': 6661.777126550674, 'accumulated_eval_time': 252.25651955604553, 'accumulated_logging_time': 0.4041624069213867}
I0128 15:41:43.248351 140005313861376 logging_writer.py:48] [19684] accumulated_eval_time=252.256520, accumulated_logging_time=0.404162, accumulated_submission_time=6661.777127, global_step=19684, preemption_count=0, score=6661.777127, test/accuracy=0.444300, test/loss=2.552198, test/num_examples=10000, total_duration=6915.075645, train/accuracy=0.612903, train/loss=1.569956, validation/accuracy=0.564280, validation/loss=1.820169, validation/num_examples=50000
I0128 15:41:48.996459 140005322254080 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.7923989295959473, loss=2.1901650428771973
I0128 15:42:22.624888 140005313861376 logging_writer.py:48] [19800] global_step=19800, grad_norm=5.053295135498047, loss=2.186114549636841
I0128 15:42:56.276926 140005322254080 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.755705833435059, loss=2.022756576538086
I0128 15:43:29.899938 140005313861376 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.6086082458496094, loss=2.180018424987793
I0128 15:44:03.546213 140005322254080 logging_writer.py:48] [20100] global_step=20100, grad_norm=5.0941267013549805, loss=2.1558632850646973
I0128 15:44:37.196426 140005313861376 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.74210786819458, loss=2.324009418487549
I0128 15:45:10.846552 140005322254080 logging_writer.py:48] [20300] global_step=20300, grad_norm=4.160251617431641, loss=2.066232681274414
I0128 15:45:44.468696 140005313861376 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.410473346710205, loss=2.0764541625976562
I0128 15:46:18.111213 140005322254080 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.8112199306488037, loss=2.1573643684387207
I0128 15:46:51.768093 140005313861376 logging_writer.py:48] [20600] global_step=20600, grad_norm=4.153928279876709, loss=2.071711540222168
I0128 15:47:25.543374 140005322254080 logging_writer.py:48] [20700] global_step=20700, grad_norm=5.75493860244751, loss=2.1268715858459473
I0128 15:47:59.170298 140005313861376 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.631182909011841, loss=2.254180431365967
I0128 15:48:32.791554 140005322254080 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.676182746887207, loss=2.0858404636383057
I0128 15:49:06.468476 140005313861376 logging_writer.py:48] [21000] global_step=21000, grad_norm=4.504949569702148, loss=2.2393131256103516
I0128 15:49:40.108977 140005322254080 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.8000378608703613, loss=2.2226388454437256
I0128 15:50:13.744485 140005313861376 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.997809410095215, loss=2.0237135887145996
I0128 15:50:13.751927 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:50:20.121193 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:50:29.095642 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:50:31.743561 140169137129280 submission_runner.py:408] Time since start: 7443.59s, 	Step: 21201, 	{'train/accuracy': 0.6129623651504517, 'train/loss': 1.5876972675323486, 'validation/accuracy': 0.5711599588394165, 'validation/loss': 1.8076725006103516, 'validation/num_examples': 50000, 'test/accuracy': 0.45000001788139343, 'test/loss': 2.5406386852264404, 'test/num_examples': 10000, 'score': 7172.218660593033, 'total_duration': 7443.592651605606, 'accumulated_submission_time': 7172.218660593033, 'accumulated_eval_time': 270.24809288978577, 'accumulated_logging_time': 0.4388735294342041}
I0128 15:50:31.766757 140004676327168 logging_writer.py:48] [21201] accumulated_eval_time=270.248093, accumulated_logging_time=0.438874, accumulated_submission_time=7172.218661, global_step=21201, preemption_count=0, score=7172.218661, test/accuracy=0.450000, test/loss=2.540639, test/num_examples=10000, total_duration=7443.592652, train/accuracy=0.612962, train/loss=1.587697, validation/accuracy=0.571160, validation/loss=1.807673, validation/num_examples=50000
I0128 15:51:05.353858 140005288683264 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.705883502960205, loss=2.035881996154785
I0128 15:51:38.988834 140004676327168 logging_writer.py:48] [21400] global_step=21400, grad_norm=4.691328048706055, loss=1.9929354190826416
I0128 15:52:12.623893 140005288683264 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.382129669189453, loss=2.1207807064056396
I0128 15:52:46.262320 140004676327168 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.038480281829834, loss=2.083747625350952
I0128 15:53:20.005934 140005288683264 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.125584602355957, loss=2.118048667907715
I0128 15:53:53.665094 140004676327168 logging_writer.py:48] [21800] global_step=21800, grad_norm=4.000892639160156, loss=2.040595293045044
I0128 15:54:27.303555 140005288683264 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.6148784160614014, loss=1.975533127784729
I0128 15:55:00.927003 140004676327168 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.065455675125122, loss=2.012974262237549
I0128 15:55:34.570700 140005288683264 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.5281853675842285, loss=2.0321261882781982
I0128 15:56:08.223430 140004676327168 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.9754016399383545, loss=2.1124207973480225
I0128 15:56:41.894478 140005288683264 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.9131686687469482, loss=2.0914416313171387
I0128 15:57:15.560581 140004676327168 logging_writer.py:48] [22400] global_step=22400, grad_norm=5.605188846588135, loss=2.064253091812134
I0128 15:57:49.195988 140005288683264 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.293344497680664, loss=2.0353753566741943
I0128 15:58:22.844014 140004676327168 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.040430068969727, loss=2.004041910171509
I0128 15:58:56.442402 140005288683264 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.8393731117248535, loss=2.078117847442627
I0128 15:59:01.975212 140169137129280 spec.py:321] Evaluating on the training split.
I0128 15:59:08.301257 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 15:59:17.046282 140169137129280 spec.py:349] Evaluating on the test split.
I0128 15:59:19.680962 140169137129280 submission_runner.py:408] Time since start: 7971.53s, 	Step: 22718, 	{'train/accuracy': 0.6120854616165161, 'train/loss': 1.5852470397949219, 'validation/accuracy': 0.5758599638938904, 'validation/loss': 1.7821930646896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4487000107765198, 'test/loss': 2.5585756301879883, 'test/num_examples': 10000, 'score': 7682.367743968964, 'total_duration': 7971.530035734177, 'accumulated_submission_time': 7682.367743968964, 'accumulated_eval_time': 287.95379066467285, 'accumulated_logging_time': 0.4722630977630615}
I0128 15:59:19.704668 140004659541760 logging_writer.py:48] [22718] accumulated_eval_time=287.953791, accumulated_logging_time=0.472263, accumulated_submission_time=7682.367744, global_step=22718, preemption_count=0, score=7682.367744, test/accuracy=0.448700, test/loss=2.558576, test/num_examples=10000, total_duration=7971.530036, train/accuracy=0.612085, train/loss=1.585247, validation/accuracy=0.575860, validation/loss=1.782193, validation/num_examples=50000
I0128 15:59:47.660389 140004667934464 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.992339849472046, loss=1.9485993385314941
I0128 16:00:21.234859 140004659541760 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.659945487976074, loss=2.0269815921783447
I0128 16:00:54.877313 140004667934464 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.9990222454071045, loss=2.0850448608398438
I0128 16:01:28.540602 140004659541760 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.107015609741211, loss=2.309882164001465
I0128 16:02:02.163635 140004667934464 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.088899850845337, loss=2.1620264053344727
I0128 16:02:35.789692 140004659541760 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.215701580047607, loss=2.000941753387451
I0128 16:03:09.448771 140004667934464 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.838768243789673, loss=2.025775671005249
I0128 16:03:43.092848 140004659541760 logging_writer.py:48] [23500] global_step=23500, grad_norm=5.359292030334473, loss=2.061579704284668
I0128 16:04:16.715767 140004667934464 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.4379284381866455, loss=2.161447286605835
I0128 16:04:50.359306 140004659541760 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.229389190673828, loss=2.015604257583618
I0128 16:05:23.995227 140004667934464 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.095798015594482, loss=2.0359392166137695
I0128 16:05:57.705778 140004659541760 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.8174970149993896, loss=2.084437131881714
I0128 16:06:31.276483 140004667934464 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.5543599128723145, loss=1.9312325716018677
I0128 16:07:04.879744 140004659541760 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.793010950088501, loss=2.1147189140319824
I0128 16:07:38.553607 140004667934464 logging_writer.py:48] [24200] global_step=24200, grad_norm=4.367864608764648, loss=2.151160478591919
I0128 16:07:49.797993 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:07:56.159276 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:08:05.036292 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:08:07.694026 140169137129280 submission_runner.py:408] Time since start: 8499.54s, 	Step: 24235, 	{'train/accuracy': 0.6190210580825806, 'train/loss': 1.554908275604248, 'validation/accuracy': 0.5786600112915039, 'validation/loss': 1.7810800075531006, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.5390639305114746, 'test/num_examples': 10000, 'score': 8192.398866891861, 'total_duration': 8499.543118476868, 'accumulated_submission_time': 8192.398866891861, 'accumulated_eval_time': 305.84978795051575, 'accumulated_logging_time': 0.5091888904571533}
I0128 16:08:07.716442 140005297075968 logging_writer.py:48] [24235] accumulated_eval_time=305.849788, accumulated_logging_time=0.509189, accumulated_submission_time=8192.398867, global_step=24235, preemption_count=0, score=8192.398867, test/accuracy=0.448100, test/loss=2.539064, test/num_examples=10000, total_duration=8499.543118, train/accuracy=0.619021, train/loss=1.554908, validation/accuracy=0.578660, validation/loss=1.781080, validation/num_examples=50000
I0128 16:08:29.846482 140005305468672 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.5360958576202393, loss=2.087878704071045
I0128 16:09:03.363564 140005297075968 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.3488214015960693, loss=2.060176134109497
I0128 16:09:36.933817 140005305468672 logging_writer.py:48] [24500] global_step=24500, grad_norm=4.1422576904296875, loss=2.0771400928497314
I0128 16:10:10.549869 140005297075968 logging_writer.py:48] [24600] global_step=24600, grad_norm=4.092480659484863, loss=1.9644317626953125
I0128 16:10:44.175379 140005305468672 logging_writer.py:48] [24700] global_step=24700, grad_norm=4.946130275726318, loss=2.1278586387634277
I0128 16:11:17.802654 140005297075968 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.7118122577667236, loss=2.0049500465393066
I0128 16:11:51.447773 140005305468672 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.409623146057129, loss=2.0212552547454834
I0128 16:12:25.107827 140005297075968 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.3907079696655273, loss=2.1133008003234863
I0128 16:12:58.681083 140005305468672 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.1206417083740234, loss=1.9023358821868896
I0128 16:13:32.274149 140005297075968 logging_writer.py:48] [25200] global_step=25200, grad_norm=4.440089225769043, loss=2.013566017150879
I0128 16:14:05.931728 140005305468672 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.638136386871338, loss=2.0264692306518555
I0128 16:14:39.558722 140005297075968 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.606074810028076, loss=2.022782325744629
I0128 16:15:13.197287 140005305468672 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.367236852645874, loss=1.991316556930542
I0128 16:15:46.835849 140005297075968 logging_writer.py:48] [25600] global_step=25600, grad_norm=4.119929790496826, loss=2.0528268814086914
I0128 16:16:20.503141 140005305468672 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.5150909423828125, loss=1.9430774450302124
I0128 16:16:37.812173 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:16:44.188503 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:16:53.157024 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:16:55.789009 140169137129280 submission_runner.py:408] Time since start: 9027.64s, 	Step: 25753, 	{'train/accuracy': 0.644551157951355, 'train/loss': 1.4304097890853882, 'validation/accuracy': 0.5829799771308899, 'validation/loss': 1.7570055723190308, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.4996163845062256, 'test/num_examples': 10000, 'score': 8702.434856891632, 'total_duration': 9027.637979269028, 'accumulated_submission_time': 8702.434856891632, 'accumulated_eval_time': 323.8264684677124, 'accumulated_logging_time': 0.5421411991119385}
I0128 16:16:55.811462 140005288683264 logging_writer.py:48] [25753] accumulated_eval_time=323.826468, accumulated_logging_time=0.542141, accumulated_submission_time=8702.434857, global_step=25753, preemption_count=0, score=8702.434857, test/accuracy=0.461500, test/loss=2.499616, test/num_examples=10000, total_duration=9027.637979, train/accuracy=0.644551, train/loss=1.430410, validation/accuracy=0.582980, validation/loss=1.757006, validation/num_examples=50000
I0128 16:17:11.910828 140005297075968 logging_writer.py:48] [25800] global_step=25800, grad_norm=4.126340389251709, loss=2.0784993171691895
I0128 16:17:45.458840 140005288683264 logging_writer.py:48] [25900] global_step=25900, grad_norm=4.207411766052246, loss=2.072603702545166
I0128 16:18:19.043595 140005297075968 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.325205087661743, loss=2.000751495361328
I0128 16:18:52.712727 140005288683264 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.611286163330078, loss=2.063342571258545
I0128 16:19:26.331292 140005297075968 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.0988070964813232, loss=1.9696438312530518
I0128 16:19:59.950568 140005288683264 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.561307668685913, loss=2.0636394023895264
I0128 16:20:33.579327 140005297075968 logging_writer.py:48] [26400] global_step=26400, grad_norm=5.919121265411377, loss=2.0393052101135254
I0128 16:21:07.189047 140005288683264 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.1391966342926025, loss=2.0727486610412598
I0128 16:21:40.817429 140005297075968 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.011007308959961, loss=2.065410852432251
I0128 16:22:14.443476 140005288683264 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.0532350540161133, loss=2.09118914604187
I0128 16:22:48.070558 140005297075968 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.667217969894409, loss=1.9899177551269531
I0128 16:23:21.724247 140005288683264 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.2588300704956055, loss=1.9485095739364624
I0128 16:23:55.346663 140005297075968 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.8500218391418457, loss=1.8387341499328613
I0128 16:24:28.983496 140005288683264 logging_writer.py:48] [27100] global_step=27100, grad_norm=4.12484073638916, loss=2.213242292404175
I0128 16:25:02.658568 140005297075968 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.8367276191711426, loss=1.91413152217865
I0128 16:25:25.996056 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:25:32.317037 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:25:41.126936 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:25:43.722269 140169137129280 submission_runner.py:408] Time since start: 9555.57s, 	Step: 27271, 	{'train/accuracy': 0.62890625, 'train/loss': 1.5057846307754517, 'validation/accuracy': 0.5762799978256226, 'validation/loss': 1.7802006006240845, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.5254218578338623, 'test/num_examples': 10000, 'score': 9212.556456327438, 'total_duration': 9555.571353673935, 'accumulated_submission_time': 9212.556456327438, 'accumulated_eval_time': 341.5526399612427, 'accumulated_logging_time': 0.5781388282775879}
I0128 16:25:43.746648 140005288683264 logging_writer.py:48] [27271] accumulated_eval_time=341.552640, accumulated_logging_time=0.578139, accumulated_submission_time=9212.556456, global_step=27271, preemption_count=0, score=9212.556456, test/accuracy=0.450800, test/loss=2.525422, test/num_examples=10000, total_duration=9555.571354, train/accuracy=0.628906, train/loss=1.505785, validation/accuracy=0.576280, validation/loss=1.780201, validation/num_examples=50000
I0128 16:25:53.837783 140005297075968 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.3889474868774414, loss=1.887865662574768
I0128 16:26:27.468484 140005288683264 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.4630508422851562, loss=2.0119168758392334
I0128 16:27:01.115928 140005297075968 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.4896445274353027, loss=2.0268936157226562
I0128 16:27:34.756007 140005288683264 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.8829026222229004, loss=1.9935461282730103
I0128 16:28:08.373725 140005297075968 logging_writer.py:48] [27700] global_step=27700, grad_norm=4.898137092590332, loss=1.949580430984497
I0128 16:28:41.999495 140005288683264 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.8549466133117676, loss=2.018010377883911
I0128 16:29:15.624098 140005297075968 logging_writer.py:48] [27900] global_step=27900, grad_norm=4.293144226074219, loss=1.9946653842926025
I0128 16:29:49.271911 140005288683264 logging_writer.py:48] [28000] global_step=28000, grad_norm=4.11704683303833, loss=2.0121264457702637
I0128 16:30:22.903805 140005297075968 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.348186731338501, loss=2.080693483352661
I0128 16:30:56.541266 140005288683264 logging_writer.py:48] [28200] global_step=28200, grad_norm=4.638963222503662, loss=1.937849521636963
I0128 16:31:30.187157 140005297075968 logging_writer.py:48] [28300] global_step=28300, grad_norm=4.090592384338379, loss=1.9586460590362549
I0128 16:32:03.811400 140005288683264 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.626793146133423, loss=2.063278913497925
I0128 16:32:37.457164 140005297075968 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.414491653442383, loss=1.9378825426101685
I0128 16:33:11.100383 140005288683264 logging_writer.py:48] [28600] global_step=28600, grad_norm=4.679998397827148, loss=1.912161946296692
I0128 16:33:44.714799 140005297075968 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.519589900970459, loss=2.0318472385406494
I0128 16:34:13.784466 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:34:20.137697 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:34:29.142581 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:34:31.679507 140169137129280 submission_runner.py:408] Time since start: 10083.53s, 	Step: 28788, 	{'train/accuracy': 0.6389309763908386, 'train/loss': 1.4611449241638184, 'validation/accuracy': 0.5892400145530701, 'validation/loss': 1.712038516998291, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4030990600585938, 'test/num_examples': 10000, 'score': 9722.534570932388, 'total_duration': 10083.528599262238, 'accumulated_submission_time': 9722.534570932388, 'accumulated_eval_time': 359.44764280319214, 'accumulated_logging_time': 0.6132323741912842}
I0128 16:34:31.702483 140004676327168 logging_writer.py:48] [28788] accumulated_eval_time=359.447643, accumulated_logging_time=0.613232, accumulated_submission_time=9722.534571, global_step=28788, preemption_count=0, score=9722.534571, test/accuracy=0.465900, test/loss=2.403099, test/num_examples=10000, total_duration=10083.528599, train/accuracy=0.638931, train/loss=1.461145, validation/accuracy=0.589240, validation/loss=1.712039, validation/num_examples=50000
I0128 16:34:36.065658 140005288683264 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.2831339836120605, loss=2.0098416805267334
I0128 16:35:09.635902 140004676327168 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.866210460662842, loss=2.0553061962127686
I0128 16:35:43.238824 140005288683264 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.2802889347076416, loss=1.9435040950775146
I0128 16:36:16.900179 140004676327168 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.548287868499756, loss=1.9297484159469604
I0128 16:36:50.514762 140005288683264 logging_writer.py:48] [29200] global_step=29200, grad_norm=4.132591724395752, loss=1.8916010856628418
I0128 16:37:24.258738 140004676327168 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.9040756225585938, loss=1.8745564222335815
I0128 16:37:57.822140 140005288683264 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.8730990886688232, loss=1.9406731128692627
I0128 16:38:31.387665 140004676327168 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.6775667667388916, loss=2.036592721939087
I0128 16:39:04.993314 140005288683264 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.7553017139434814, loss=1.9519633054733276
I0128 16:39:38.654436 140004676327168 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.71419095993042, loss=1.992828130722046
I0128 16:40:12.283250 140005288683264 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.301692247390747, loss=1.933449625968933
I0128 16:40:45.935980 140004676327168 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.495455026626587, loss=2.036630868911743
I0128 16:41:19.545605 140005288683264 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.4833247661590576, loss=2.073817014694214
I0128 16:41:53.164740 140004676327168 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.435675859451294, loss=1.8906290531158447
I0128 16:42:26.815056 140005288683264 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.621591091156006, loss=1.9664006233215332
I0128 16:43:00.409283 140004676327168 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.1992530822753906, loss=2.0993235111236572
I0128 16:43:01.908673 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:43:08.309268 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:43:16.972439 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:43:19.659817 140169137129280 submission_runner.py:408] Time since start: 10611.51s, 	Step: 30306, 	{'train/accuracy': 0.6354631781578064, 'train/loss': 1.4758671522140503, 'validation/accuracy': 0.5902199745178223, 'validation/loss': 1.7170330286026, 'validation/num_examples': 50000, 'test/accuracy': 0.4637000262737274, 'test/loss': 2.4629597663879395, 'test/num_examples': 10000, 'score': 10232.679866552353, 'total_duration': 10611.508907079697, 'accumulated_submission_time': 10232.679866552353, 'accumulated_eval_time': 377.1987464427948, 'accumulated_logging_time': 0.6481332778930664}
I0128 16:43:19.685033 140005313861376 logging_writer.py:48] [30306] accumulated_eval_time=377.198746, accumulated_logging_time=0.648133, accumulated_submission_time=10232.679867, global_step=30306, preemption_count=0, score=10232.679867, test/accuracy=0.463700, test/loss=2.462960, test/num_examples=10000, total_duration=10611.508907, train/accuracy=0.635463, train/loss=1.475867, validation/accuracy=0.590220, validation/loss=1.717033, validation/num_examples=50000
I0128 16:43:51.693856 140005322254080 logging_writer.py:48] [30400] global_step=30400, grad_norm=3.609605073928833, loss=1.9465433359146118
I0128 16:44:25.301549 140005313861376 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.464526414871216, loss=1.994993805885315
I0128 16:44:58.922076 140005322254080 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.4334652423858643, loss=1.944492220878601
I0128 16:45:32.561659 140005313861376 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.883298397064209, loss=2.056887626647949
I0128 16:46:06.184165 140005322254080 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.0231916904449463, loss=1.9543439149856567
I0128 16:46:39.815551 140005313861376 logging_writer.py:48] [30900] global_step=30900, grad_norm=4.357374668121338, loss=2.0176503658294678
I0128 16:47:13.408329 140005322254080 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.6850967407226562, loss=1.9369605779647827
I0128 16:47:46.948262 140005313861376 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.10571026802063, loss=2.040402412414551
I0128 16:48:20.522361 140005322254080 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.483873128890991, loss=1.8741317987442017
I0128 16:48:54.111677 140005313861376 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.3644607067108154, loss=1.8829349279403687
I0128 16:49:27.665057 140005322254080 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.153661012649536, loss=2.0240347385406494
I0128 16:50:01.277613 140005313861376 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.6775760650634766, loss=1.881028652191162
I0128 16:50:34.939668 140005322254080 logging_writer.py:48] [31600] global_step=31600, grad_norm=4.250645160675049, loss=1.8625459671020508
I0128 16:51:08.569799 140005313861376 logging_writer.py:48] [31700] global_step=31700, grad_norm=4.483733654022217, loss=1.9808589220046997
I0128 16:51:42.192178 140005322254080 logging_writer.py:48] [31800] global_step=31800, grad_norm=4.282506942749023, loss=2.0934112071990967
I0128 16:51:49.747418 140169137129280 spec.py:321] Evaluating on the training split.
I0128 16:51:56.066763 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 16:52:04.951456 140169137129280 spec.py:349] Evaluating on the test split.
I0128 16:52:07.625054 140169137129280 submission_runner.py:408] Time since start: 11139.47s, 	Step: 31824, 	{'train/accuracy': 0.6270925998687744, 'train/loss': 1.5033053159713745, 'validation/accuracy': 0.5899400115013123, 'validation/loss': 1.7156438827514648, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.449855327606201, 'test/num_examples': 10000, 'score': 10742.683179616928, 'total_duration': 11139.474148750305, 'accumulated_submission_time': 10742.683179616928, 'accumulated_eval_time': 395.0763454437256, 'accumulated_logging_time': 0.6836209297180176}
I0128 16:52:07.648366 140004676327168 logging_writer.py:48] [31824] accumulated_eval_time=395.076345, accumulated_logging_time=0.683621, accumulated_submission_time=10742.683180, global_step=31824, preemption_count=0, score=10742.683180, test/accuracy=0.464900, test/loss=2.449855, test/num_examples=10000, total_duration=11139.474149, train/accuracy=0.627093, train/loss=1.503305, validation/accuracy=0.589940, validation/loss=1.715644, validation/num_examples=50000
I0128 16:52:33.522532 140005288683264 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.3526737689971924, loss=2.08229660987854
I0128 16:53:07.036527 140004676327168 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.594700574874878, loss=2.1138269901275635
I0128 16:53:40.592405 140005288683264 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.5424089431762695, loss=1.8804495334625244
I0128 16:54:14.147292 140004676327168 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.722867965698242, loss=1.9347875118255615
I0128 16:54:47.765130 140005288683264 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.944770574569702, loss=1.8216177225112915
I0128 16:55:21.402579 140004676327168 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.741809844970703, loss=1.833251714706421
I0128 16:55:55.027047 140005288683264 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.4440362453460693, loss=2.0442683696746826
I0128 16:56:28.733257 140004676327168 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.3381083011627197, loss=1.9618568420410156
I0128 16:57:02.334375 140005288683264 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.5767767429351807, loss=1.9509729146957397
I0128 16:57:35.939428 140004676327168 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.206789970397949, loss=1.9852286577224731
I0128 16:58:09.584019 140005288683264 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.1579666137695312, loss=1.8729654550552368
I0128 16:58:43.221024 140004676327168 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.5713021755218506, loss=1.9013705253601074
I0128 16:59:16.836420 140005288683264 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.554429292678833, loss=2.002931594848633
I0128 16:59:50.490416 140004676327168 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.5561888217926025, loss=1.964426040649414
I0128 17:00:24.106510 140005288683264 logging_writer.py:48] [33300] global_step=33300, grad_norm=4.775094985961914, loss=2.017460346221924
I0128 17:00:37.715778 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:00:44.080555 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:00:53.064238 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:00:55.737607 140169137129280 submission_runner.py:408] Time since start: 11667.59s, 	Step: 33342, 	{'train/accuracy': 0.6690449714660645, 'train/loss': 1.3072504997253418, 'validation/accuracy': 0.5861600041389465, 'validation/loss': 1.723919153213501, 'validation/num_examples': 50000, 'test/accuracy': 0.46240001916885376, 'test/loss': 2.4317657947540283, 'test/num_examples': 10000, 'score': 11252.691735982895, 'total_duration': 11667.586690425873, 'accumulated_submission_time': 11252.691735982895, 'accumulated_eval_time': 413.09812903404236, 'accumulated_logging_time': 0.716942310333252}
I0128 17:00:55.762926 140004676327168 logging_writer.py:48] [33342] accumulated_eval_time=413.098129, accumulated_logging_time=0.716942, accumulated_submission_time=11252.691736, global_step=33342, preemption_count=0, score=11252.691736, test/accuracy=0.462400, test/loss=2.431766, test/num_examples=10000, total_duration=11667.586690, train/accuracy=0.669045, train/loss=1.307250, validation/accuracy=0.586160, validation/loss=1.723919, validation/num_examples=50000
I0128 17:01:15.549253 140005313861376 logging_writer.py:48] [33400] global_step=33400, grad_norm=3.513267993927002, loss=1.885837197303772
I0128 17:01:49.102028 140004676327168 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.8413381576538086, loss=1.8884625434875488
I0128 17:02:22.649169 140005313861376 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.307277202606201, loss=1.9364378452301025
I0128 17:02:56.314210 140004676327168 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.0019850730895996, loss=2.042121410369873
I0128 17:03:29.944480 140005313861376 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.8746187686920166, loss=1.898952841758728
I0128 17:04:03.566955 140004676327168 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.482356548309326, loss=1.9697599411010742
I0128 17:04:37.191943 140005313861376 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.706768035888672, loss=2.0467782020568848
I0128 17:05:10.795718 140004676327168 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.5004489421844482, loss=1.978549599647522
I0128 17:05:44.370556 140005313861376 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.9056854248046875, loss=2.018432140350342
I0128 17:06:18.009260 140004676327168 logging_writer.py:48] [34300] global_step=34300, grad_norm=4.215612411499023, loss=2.1203999519348145
I0128 17:06:51.650809 140005313861376 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.688194751739502, loss=2.0479724407196045
I0128 17:07:25.302311 140004676327168 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.363703966140747, loss=1.9805307388305664
I0128 17:07:58.922681 140005313861376 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.8810110092163086, loss=2.0107083320617676
I0128 17:08:32.518216 140004676327168 logging_writer.py:48] [34700] global_step=34700, grad_norm=4.147691249847412, loss=1.9112319946289062
I0128 17:09:06.156554 140005313861376 logging_writer.py:48] [34800] global_step=34800, grad_norm=4.272866249084473, loss=1.9797497987747192
I0128 17:09:25.816451 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:09:32.175348 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:09:41.152064 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:09:43.817491 140169137129280 submission_runner.py:408] Time since start: 12195.67s, 	Step: 34860, 	{'train/accuracy': 0.6469627022743225, 'train/loss': 1.3967673778533936, 'validation/accuracy': 0.5877000093460083, 'validation/loss': 1.7086846828460693, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.405595541000366, 'test/num_examples': 10000, 'score': 11762.682977676392, 'total_duration': 12195.666572332382, 'accumulated_submission_time': 11762.682977676392, 'accumulated_eval_time': 431.09912037849426, 'accumulated_logging_time': 0.7553849220275879}
I0128 17:09:43.847154 140004676327168 logging_writer.py:48] [34860] accumulated_eval_time=431.099120, accumulated_logging_time=0.755385, accumulated_submission_time=11762.682978, global_step=34860, preemption_count=0, score=11762.682978, test/accuracy=0.472600, test/loss=2.405596, test/num_examples=10000, total_duration=12195.666572, train/accuracy=0.646963, train/loss=1.396767, validation/accuracy=0.587700, validation/loss=1.708685, validation/num_examples=50000
I0128 17:09:57.605541 140005288683264 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.5469698905944824, loss=2.0250511169433594
I0128 17:10:31.553328 140004676327168 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.43589186668396, loss=2.0274975299835205
I0128 17:11:05.109740 140005288683264 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.6843647956848145, loss=1.9314589500427246
I0128 17:11:38.702134 140004676327168 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.8846707344055176, loss=1.9117636680603027
I0128 17:12:12.342351 140005288683264 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.369575023651123, loss=1.908534049987793
I0128 17:12:45.954015 140004676327168 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.3830251693725586, loss=1.9773375988006592
I0128 17:13:19.573266 140005288683264 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.882319927215576, loss=1.9318660497665405
I0128 17:13:53.204833 140004676327168 logging_writer.py:48] [35600] global_step=35600, grad_norm=4.015610694885254, loss=1.9191659688949585
I0128 17:14:26.840011 140005288683264 logging_writer.py:48] [35700] global_step=35700, grad_norm=4.696578025817871, loss=1.9322874546051025
I0128 17:15:00.459056 140004676327168 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.548717737197876, loss=2.0048155784606934
I0128 17:15:34.134097 140005288683264 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.6910367012023926, loss=1.947965383529663
I0128 17:16:07.754867 140004676327168 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.4150896072387695, loss=1.9267196655273438
I0128 17:16:41.374095 140005288683264 logging_writer.py:48] [36100] global_step=36100, grad_norm=4.070026397705078, loss=1.8844692707061768
I0128 17:17:15.005634 140004676327168 logging_writer.py:48] [36200] global_step=36200, grad_norm=4.1177263259887695, loss=1.933927059173584
I0128 17:17:48.637261 140005288683264 logging_writer.py:48] [36300] global_step=36300, grad_norm=4.451887607574463, loss=2.0053772926330566
I0128 17:18:13.989553 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:18:20.377577 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:18:29.394549 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:18:32.027617 140169137129280 submission_runner.py:408] Time since start: 12723.88s, 	Step: 36377, 	{'train/accuracy': 0.6559311151504517, 'train/loss': 1.3725244998931885, 'validation/accuracy': 0.6061399579048157, 'validation/loss': 1.6266961097717285, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3845810890197754, 'test/num_examples': 10000, 'score': 12272.765510082245, 'total_duration': 12723.876703977585, 'accumulated_submission_time': 12272.765510082245, 'accumulated_eval_time': 449.1371431350708, 'accumulated_logging_time': 0.7958519458770752}
I0128 17:18:32.056025 140005313861376 logging_writer.py:48] [36377] accumulated_eval_time=449.137143, accumulated_logging_time=0.795852, accumulated_submission_time=12272.765510, global_step=36377, preemption_count=0, score=12272.765510, test/accuracy=0.479300, test/loss=2.384581, test/num_examples=10000, total_duration=12723.876704, train/accuracy=0.655931, train/loss=1.372524, validation/accuracy=0.606140, validation/loss=1.626696, validation/num_examples=50000
I0128 17:18:40.119549 140005322254080 logging_writer.py:48] [36400] global_step=36400, grad_norm=4.201687335968018, loss=1.986917495727539
I0128 17:19:13.623565 140005313861376 logging_writer.py:48] [36500] global_step=36500, grad_norm=4.40627908706665, loss=2.082718849182129
I0128 17:19:47.184242 140005322254080 logging_writer.py:48] [36600] global_step=36600, grad_norm=4.149851322174072, loss=1.7717148065567017
I0128 17:20:20.717677 140005313861376 logging_writer.py:48] [36700] global_step=36700, grad_norm=4.610655784606934, loss=1.9593307971954346
I0128 17:20:54.246797 140005322254080 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.348992109298706, loss=1.8819786310195923
I0128 17:21:27.821958 140005313861376 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.4401304721832275, loss=1.9909381866455078
I0128 17:22:01.433609 140005322254080 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.498075246810913, loss=1.9270983934402466
I0128 17:22:35.000366 140005313861376 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.2833847999572754, loss=1.919940710067749
I0128 17:23:08.620954 140005322254080 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.4888134002685547, loss=1.950829029083252
I0128 17:23:42.240597 140005313861376 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.662698745727539, loss=1.9456958770751953
I0128 17:24:15.853394 140005322254080 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.517087697982788, loss=1.921089768409729
I0128 17:24:49.480749 140005313861376 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.101963520050049, loss=1.9928128719329834
I0128 17:25:23.102800 140005322254080 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.4023358821868896, loss=1.9297000169754028
I0128 17:25:56.751566 140005313861376 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.2735602855682373, loss=1.9835728406906128
I0128 17:26:30.374974 140005322254080 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.276824712753296, loss=1.8505374193191528
I0128 17:27:02.152386 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:27:08.477561 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:27:17.369223 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:27:19.976499 140169137129280 submission_runner.py:408] Time since start: 13251.83s, 	Step: 37896, 	{'train/accuracy': 0.6361208558082581, 'train/loss': 1.4662803411483765, 'validation/accuracy': 0.5847199559211731, 'validation/loss': 1.7373977899551392, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.486398220062256, 'test/num_examples': 10000, 'score': 12782.802032232285, 'total_duration': 13251.825589179993, 'accumulated_submission_time': 12782.802032232285, 'accumulated_eval_time': 466.961225271225, 'accumulated_logging_time': 0.8347640037536621}
I0128 17:27:20.003177 140005288683264 logging_writer.py:48] [37896] accumulated_eval_time=466.961225, accumulated_logging_time=0.834764, accumulated_submission_time=12782.802032, global_step=37896, preemption_count=0, score=12782.802032, test/accuracy=0.463800, test/loss=2.486398, test/num_examples=10000, total_duration=13251.825589, train/accuracy=0.636121, train/loss=1.466280, validation/accuracy=0.584720, validation/loss=1.737398, validation/num_examples=50000
I0128 17:27:21.692251 140005297075968 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.862830400466919, loss=2.006361246109009
I0128 17:27:55.309602 140005288683264 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.9135828018188477, loss=1.9180943965911865
I0128 17:28:28.897979 140005297075968 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.3857269287109375, loss=1.8736047744750977
I0128 17:29:02.507133 140005288683264 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.057767152786255, loss=1.9711945056915283
I0128 17:29:36.144934 140005297075968 logging_writer.py:48] [38300] global_step=38300, grad_norm=4.091917514801025, loss=1.9950121641159058
I0128 17:30:09.772979 140005288683264 logging_writer.py:48] [38400] global_step=38400, grad_norm=4.332728862762451, loss=1.8920867443084717
I0128 17:30:43.399281 140005297075968 logging_writer.py:48] [38500] global_step=38500, grad_norm=4.25191068649292, loss=1.9261664152145386
I0128 17:31:17.025609 140005288683264 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.369264841079712, loss=1.9337884187698364
I0128 17:31:50.633867 140005297075968 logging_writer.py:48] [38700] global_step=38700, grad_norm=4.164651393890381, loss=1.9114433526992798
I0128 17:32:24.269810 140005288683264 logging_writer.py:48] [38800] global_step=38800, grad_norm=4.5056376457214355, loss=1.924208402633667
I0128 17:32:57.904418 140005297075968 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.2275149822235107, loss=1.9277150630950928
I0128 17:33:31.532974 140005288683264 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.4172635078430176, loss=1.7845542430877686
I0128 17:34:05.232185 140005297075968 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.5733397006988525, loss=2.0713963508605957
I0128 17:34:38.855154 140005288683264 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.4093875885009766, loss=1.954361915588379
I0128 17:35:12.419930 140005297075968 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.911287546157837, loss=1.9521512985229492
I0128 17:35:46.033053 140005288683264 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.6329143047332764, loss=1.8834503889083862
I0128 17:35:50.233278 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:35:56.610422 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:36:05.391242 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:36:08.036834 140169137129280 submission_runner.py:408] Time since start: 13779.89s, 	Step: 39414, 	{'train/accuracy': 0.6498525142669678, 'train/loss': 1.4036788940429688, 'validation/accuracy': 0.6021599769592285, 'validation/loss': 1.6521224975585938, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.3708035945892334, 'test/num_examples': 10000, 'score': 13292.97028517723, 'total_duration': 13779.885905742645, 'accumulated_submission_time': 13292.97028517723, 'accumulated_eval_time': 484.7647216320038, 'accumulated_logging_time': 0.873910665512085}
I0128 17:36:08.062931 140005313861376 logging_writer.py:48] [39414] accumulated_eval_time=484.764722, accumulated_logging_time=0.873911, accumulated_submission_time=13292.970285, global_step=39414, preemption_count=0, score=13292.970285, test/accuracy=0.481300, test/loss=2.370804, test/num_examples=10000, total_duration=13779.885906, train/accuracy=0.649853, train/loss=1.403679, validation/accuracy=0.602160, validation/loss=1.652122, validation/num_examples=50000
I0128 17:36:37.265808 140005322254080 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.972874164581299, loss=1.9211299419403076
I0128 17:37:10.895172 140005313861376 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.4474687576293945, loss=1.9171946048736572
I0128 17:37:44.504358 140005322254080 logging_writer.py:48] [39700] global_step=39700, grad_norm=4.61217737197876, loss=1.9277534484863281
I0128 17:38:18.144217 140005313861376 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.3631954193115234, loss=2.0339674949645996
I0128 17:38:51.787286 140005322254080 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.058188438415527, loss=1.9310801029205322
I0128 17:39:25.425406 140005313861376 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.673987627029419, loss=2.0347554683685303
I0128 17:39:59.038516 140005322254080 logging_writer.py:48] [40100] global_step=40100, grad_norm=4.237020015716553, loss=1.8298407793045044
I0128 17:40:32.750587 140005313861376 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.6109089851379395, loss=1.9440650939941406
I0128 17:41:06.282993 140005322254080 logging_writer.py:48] [40300] global_step=40300, grad_norm=4.208341121673584, loss=1.8703691959381104
I0128 17:41:39.841190 140005313861376 logging_writer.py:48] [40400] global_step=40400, grad_norm=4.100914478302002, loss=1.9846751689910889
I0128 17:42:13.431349 140005322254080 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.4423558712005615, loss=1.8300936222076416
I0128 17:42:46.958766 140005313861376 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.204532146453857, loss=1.9450676441192627
I0128 17:43:20.501887 140005322254080 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.9757556915283203, loss=2.0790042877197266
I0128 17:43:54.074747 140005313861376 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.5579018592834473, loss=1.9589892625808716
I0128 17:44:27.685754 140005322254080 logging_writer.py:48] [40900] global_step=40900, grad_norm=4.263714790344238, loss=1.9649569988250732
I0128 17:44:38.262392 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:44:44.781136 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:44:53.631522 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:44:56.305240 140169137129280 submission_runner.py:408] Time since start: 14308.15s, 	Step: 40933, 	{'train/accuracy': 0.6477798223495483, 'train/loss': 1.4108294248580933, 'validation/accuracy': 0.6057999730110168, 'validation/loss': 1.6259907484054565, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.3724279403686523, 'test/num_examples': 10000, 'score': 13803.110150814056, 'total_duration': 14308.154326200485, 'accumulated_submission_time': 13803.110150814056, 'accumulated_eval_time': 502.8075284957886, 'accumulated_logging_time': 0.9099681377410889}
I0128 17:44:56.330096 140004659541760 logging_writer.py:48] [40933] accumulated_eval_time=502.807528, accumulated_logging_time=0.909968, accumulated_submission_time=13803.110151, global_step=40933, preemption_count=0, score=13803.110151, test/accuracy=0.478300, test/loss=2.372428, test/num_examples=10000, total_duration=14308.154326, train/accuracy=0.647780, train/loss=1.410829, validation/accuracy=0.605800, validation/loss=1.625991, validation/num_examples=50000
I0128 17:45:19.156013 140004667934464 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.472358465194702, loss=2.0458967685699463
I0128 17:45:52.643854 140004659541760 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.2928764820098877, loss=1.9382871389389038
I0128 17:46:26.243824 140004667934464 logging_writer.py:48] [41200] global_step=41200, grad_norm=4.214198112487793, loss=1.997654676437378
I0128 17:46:59.896313 140004659541760 logging_writer.py:48] [41300] global_step=41300, grad_norm=4.072331428527832, loss=1.972569465637207
I0128 17:47:33.448405 140004667934464 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.7390615940093994, loss=1.8549995422363281
I0128 17:48:07.069368 140004659541760 logging_writer.py:48] [41500] global_step=41500, grad_norm=4.413445949554443, loss=2.012417793273926
I0128 17:48:40.698191 140004667934464 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.533628225326538, loss=1.9070991277694702
I0128 17:49:14.327313 140004659541760 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.6356446743011475, loss=1.9360743761062622
I0128 17:49:47.944443 140004667934464 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.605295419692993, loss=1.8574219942092896
I0128 17:50:21.513411 140004659541760 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.7944273948669434, loss=1.9530751705169678
I0128 17:50:55.054063 140004667934464 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.731100559234619, loss=1.8985199928283691
I0128 17:51:28.593068 140004659541760 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.4056859016418457, loss=1.7542468309402466
I0128 17:52:02.199112 140004667934464 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.00994873046875, loss=1.9066978693008423
I0128 17:52:35.728274 140004659541760 logging_writer.py:48] [42300] global_step=42300, grad_norm=3.4645872116088867, loss=1.8564070463180542
I0128 17:53:09.352995 140004667934464 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.8402318954467773, loss=1.9485523700714111
I0128 17:53:26.610077 140169137129280 spec.py:321] Evaluating on the training split.
I0128 17:53:32.980203 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 17:53:42.006820 140169137129280 spec.py:349] Evaluating on the test split.
I0128 17:53:44.642920 140169137129280 submission_runner.py:408] Time since start: 14836.49s, 	Step: 42453, 	{'train/accuracy': 0.6938576102256775, 'train/loss': 1.2015491724014282, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.6163575649261475, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.3288164138793945, 'test/num_examples': 10000, 'score': 14313.330046653748, 'total_duration': 14836.492009878159, 'accumulated_submission_time': 14313.330046653748, 'accumulated_eval_time': 520.8403308391571, 'accumulated_logging_time': 0.9460337162017822}
I0128 17:53:44.672619 140004676327168 logging_writer.py:48] [42453] accumulated_eval_time=520.840331, accumulated_logging_time=0.946034, accumulated_submission_time=14313.330047, global_step=42453, preemption_count=0, score=14313.330047, test/accuracy=0.484400, test/loss=2.328816, test/num_examples=10000, total_duration=14836.492010, train/accuracy=0.693858, train/loss=1.201549, validation/accuracy=0.608280, validation/loss=1.616358, validation/num_examples=50000
I0128 17:54:00.757816 140005313861376 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.5546395778656006, loss=1.8469548225402832
I0128 17:54:34.276483 140004676327168 logging_writer.py:48] [42600] global_step=42600, grad_norm=4.099137306213379, loss=1.88496994972229
I0128 17:55:07.791389 140005313861376 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.2374637126922607, loss=1.878638505935669
I0128 17:55:41.342579 140004676327168 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.3215861320495605, loss=1.853097677230835
I0128 17:56:14.896644 140005313861376 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.150152206420898, loss=1.920570969581604
I0128 17:56:48.470576 140004676327168 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.2062323093414307, loss=1.8996535539627075
I0128 17:57:22.104848 140005313861376 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.3408801555633545, loss=1.9121997356414795
I0128 17:57:55.730031 140004676327168 logging_writer.py:48] [43200] global_step=43200, grad_norm=5.092698097229004, loss=1.93369722366333
I0128 17:58:29.363946 140005313861376 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.1381897926330566, loss=1.8919333219528198
I0128 17:59:02.979012 140004676327168 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.4382309913635254, loss=1.779639720916748
I0128 17:59:36.627303 140005313861376 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.2917447090148926, loss=1.8746153116226196
I0128 18:00:10.209240 140004676327168 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.8653082847595215, loss=1.9877803325653076
I0128 18:00:43.816708 140005313861376 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.7701313495635986, loss=1.8308111429214478
I0128 18:01:17.404609 140004676327168 logging_writer.py:48] [43800] global_step=43800, grad_norm=4.099325656890869, loss=1.947812795639038
I0128 18:01:51.025994 140005313861376 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.2965803146362305, loss=1.9469317197799683
I0128 18:02:14.668566 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:02:20.997098 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:02:30.112407 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:02:32.733517 140169137129280 submission_runner.py:408] Time since start: 15364.58s, 	Step: 43972, 	{'train/accuracy': 0.6569873690605164, 'train/loss': 1.3638534545898438, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6688741445541382, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.3888916969299316, 'test/num_examples': 10000, 'score': 14823.266336917877, 'total_duration': 15364.582607030869, 'accumulated_submission_time': 14823.266336917877, 'accumulated_eval_time': 538.9052393436432, 'accumulated_logging_time': 0.9865646362304688}
I0128 18:02:32.759726 140004659541760 logging_writer.py:48] [43972] accumulated_eval_time=538.905239, accumulated_logging_time=0.986565, accumulated_submission_time=14823.266337, global_step=43972, preemption_count=0, score=14823.266337, test/accuracy=0.477900, test/loss=2.388892, test/num_examples=10000, total_duration=15364.582607, train/accuracy=0.656987, train/loss=1.363853, validation/accuracy=0.599040, validation/loss=1.668874, validation/num_examples=50000
I0128 18:02:42.489737 140004667934464 logging_writer.py:48] [44000] global_step=44000, grad_norm=4.018897533416748, loss=1.9939056634902954
I0128 18:03:15.992796 140004659541760 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.3995609283447266, loss=1.9106953144073486
I0128 18:03:49.491019 140004667934464 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.4473330974578857, loss=1.9926526546478271
I0128 18:04:23.075756 140004659541760 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.725882053375244, loss=1.9289398193359375
I0128 18:04:56.640906 140004667934464 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.509995460510254, loss=1.9752955436706543
I0128 18:05:30.195873 140004659541760 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.938173532485962, loss=2.0978481769561768
I0128 18:06:03.858908 140004667934464 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.85237717628479, loss=1.9193332195281982
I0128 18:06:37.455016 140004659541760 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.259082794189453, loss=1.9436542987823486
I0128 18:07:11.079242 140004667934464 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.4670257568359375, loss=1.9467586278915405
I0128 18:07:44.693841 140004659541760 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.7359344959259033, loss=1.8762612342834473
I0128 18:08:18.317993 140004667934464 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.092653751373291, loss=1.8491711616516113
I0128 18:08:51.925138 140004659541760 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.622823715209961, loss=1.9555323123931885
I0128 18:09:25.472439 140004667934464 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.9406208992004395, loss=1.7715308666229248
I0128 18:09:59.087156 140004659541760 logging_writer.py:48] [45300] global_step=45300, grad_norm=4.102813720703125, loss=1.9810545444488525
I0128 18:10:32.694601 140004667934464 logging_writer.py:48] [45400] global_step=45400, grad_norm=4.151444435119629, loss=1.8017116785049438
I0128 18:11:02.767661 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:11:09.130395 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:11:17.827259 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:11:20.473376 140169137129280 submission_runner.py:408] Time since start: 15892.32s, 	Step: 45491, 	{'train/accuracy': 0.6528021097183228, 'train/loss': 1.3831080198287964, 'validation/accuracy': 0.6025800108909607, 'validation/loss': 1.6469758749008179, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.3950185775756836, 'test/num_examples': 10000, 'score': 15333.213256835938, 'total_duration': 15892.32246518135, 'accumulated_submission_time': 15333.213256835938, 'accumulated_eval_time': 556.6109170913696, 'accumulated_logging_time': 1.0245742797851562}
I0128 18:11:20.501665 140004676327168 logging_writer.py:48] [45491] accumulated_eval_time=556.610917, accumulated_logging_time=1.024574, accumulated_submission_time=15333.213257, global_step=45491, preemption_count=0, score=15333.213257, test/accuracy=0.477900, test/loss=2.395019, test/num_examples=10000, total_duration=15892.322465, train/accuracy=0.652802, train/loss=1.383108, validation/accuracy=0.602580, validation/loss=1.646976, validation/num_examples=50000
I0128 18:11:23.868021 140005305468672 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.291066884994507, loss=1.7514173984527588
I0128 18:11:57.370380 140004676327168 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.68969464302063, loss=1.928432583808899
I0128 18:12:31.017684 140005305468672 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.11037015914917, loss=1.8927720785140991
I0128 18:13:04.625053 140004676327168 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.7177083492279053, loss=1.8517731428146362
I0128 18:13:38.272194 140005305468672 logging_writer.py:48] [45900] global_step=45900, grad_norm=4.11630916595459, loss=1.9358755350112915
I0128 18:14:11.890299 140004676327168 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.21294903755188, loss=1.9451929330825806
I0128 18:14:45.519464 140005305468672 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.802868127822876, loss=1.920325756072998
I0128 18:15:19.131512 140004676327168 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.361826181411743, loss=2.0438179969787598
I0128 18:15:52.739470 140005305468672 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.7512784004211426, loss=1.7058637142181396
I0128 18:16:26.363727 140004676327168 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.21480655670166, loss=1.8215388059616089
I0128 18:16:59.992071 140005305468672 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.720095157623291, loss=1.8744107484817505
I0128 18:17:33.620556 140004676327168 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.265319585800171, loss=1.9614391326904297
I0128 18:18:07.252251 140005305468672 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.565194606781006, loss=1.8327277898788452
I0128 18:18:40.873574 140004676327168 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.5573089122772217, loss=1.9229156970977783
I0128 18:19:14.475806 140005305468672 logging_writer.py:48] [46900] global_step=46900, grad_norm=4.1383843421936035, loss=1.9074556827545166
I0128 18:19:48.083990 140004676327168 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2955808639526367, loss=2.0012667179107666
I0128 18:19:50.593919 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:19:57.591416 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:20:06.746593 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:20:09.393691 140169137129280 submission_runner.py:408] Time since start: 16421.24s, 	Step: 47009, 	{'train/accuracy': 0.6518455147743225, 'train/loss': 1.3824917078018188, 'validation/accuracy': 0.6039599776268005, 'validation/loss': 1.638573408126831, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.371513843536377, 'test/num_examples': 10000, 'score': 15843.245793819427, 'total_duration': 16421.24270606041, 'accumulated_submission_time': 15843.245793819427, 'accumulated_eval_time': 575.4105768203735, 'accumulated_logging_time': 1.063220739364624}
I0128 18:20:09.419881 140005288683264 logging_writer.py:48] [47009] accumulated_eval_time=575.410577, accumulated_logging_time=1.063221, accumulated_submission_time=15843.245794, global_step=47009, preemption_count=0, score=15843.245794, test/accuracy=0.474600, test/loss=2.371514, test/num_examples=10000, total_duration=16421.242706, train/accuracy=0.651846, train/loss=1.382492, validation/accuracy=0.603960, validation/loss=1.638573, validation/num_examples=50000
I0128 18:20:40.297146 140005297075968 logging_writer.py:48] [47100] global_step=47100, grad_norm=5.523353576660156, loss=1.8488047122955322
I0128 18:21:13.809053 140005288683264 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.8022358417510986, loss=1.9884473085403442
I0128 18:21:47.383117 140005297075968 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.3751296997070312, loss=1.9318366050720215
I0128 18:22:20.962780 140005288683264 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.3428914546966553, loss=1.9195955991744995
I0128 18:22:54.597081 140005297075968 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.3681769371032715, loss=1.9721431732177734
I0128 18:23:28.221519 140005288683264 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.381962299346924, loss=1.8365788459777832
I0128 18:24:01.835210 140005297075968 logging_writer.py:48] [47700] global_step=47700, grad_norm=4.068680763244629, loss=1.8325833082199097
I0128 18:24:35.540221 140005288683264 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.612035036087036, loss=1.965987205505371
I0128 18:25:09.092707 140005297075968 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.801839828491211, loss=1.84463369846344
I0128 18:25:42.692621 140005288683264 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.7921292781829834, loss=1.8518650531768799
I0128 18:26:16.221927 140005297075968 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.676828145980835, loss=1.7743821144104004
I0128 18:26:49.749318 140005288683264 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.3586268424987793, loss=1.8822381496429443
I0128 18:27:23.325056 140005297075968 logging_writer.py:48] [48300] global_step=48300, grad_norm=4.161513805389404, loss=2.0066285133361816
I0128 18:27:56.924351 140005288683264 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.325883150100708, loss=2.0833983421325684
I0128 18:28:30.544404 140005297075968 logging_writer.py:48] [48500] global_step=48500, grad_norm=4.967387676239014, loss=1.8034766912460327
I0128 18:28:39.435922 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:28:45.774541 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:28:54.475670 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:28:57.203898 140169137129280 submission_runner.py:408] Time since start: 16949.05s, 	Step: 48528, 	{'train/accuracy': 0.6499919891357422, 'train/loss': 1.4085599184036255, 'validation/accuracy': 0.602620005607605, 'validation/loss': 1.6432632207870483, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.336787700653076, 'test/num_examples': 10000, 'score': 16353.201929330826, 'total_duration': 16949.052980184555, 'accumulated_submission_time': 16353.201929330826, 'accumulated_eval_time': 593.1785054206848, 'accumulated_logging_time': 1.1000022888183594}
I0128 18:28:57.232663 140004667934464 logging_writer.py:48] [48528] accumulated_eval_time=593.178505, accumulated_logging_time=1.100002, accumulated_submission_time=16353.201929, global_step=48528, preemption_count=0, score=16353.201929, test/accuracy=0.483700, test/loss=2.336788, test/num_examples=10000, total_duration=16949.052980, train/accuracy=0.649992, train/loss=1.408560, validation/accuracy=0.602620, validation/loss=1.643263, validation/num_examples=50000
I0128 18:29:21.733335 140004676327168 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.607591152191162, loss=1.7998906373977661
I0128 18:29:55.276371 140004667934464 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.616582155227661, loss=1.8156507015228271
I0128 18:30:28.812108 140004676327168 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.3130269050598145, loss=1.8282825946807861
I0128 18:31:02.441494 140004667934464 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.4431111812591553, loss=1.9645155668258667
I0128 18:31:36.075464 140004676327168 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.5380680561065674, loss=1.8493977785110474
I0128 18:32:09.703330 140004667934464 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.8227248191833496, loss=1.8120895624160767
I0128 18:32:43.334460 140004676327168 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.2462000846862793, loss=1.780112624168396
I0128 18:33:16.958137 140004667934464 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.3727293014526367, loss=1.9389922618865967
I0128 18:33:50.592080 140004676327168 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.564643144607544, loss=1.9617711305618286
I0128 18:34:24.210750 140004667934464 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.7900969982147217, loss=1.7960666418075562
I0128 18:34:57.853049 140004676327168 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.613823890686035, loss=1.9536786079406738
I0128 18:35:31.474251 140004667934464 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.5802125930786133, loss=1.923728585243225
I0128 18:36:05.087138 140004676327168 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.399829149246216, loss=1.930198073387146
I0128 18:36:38.718869 140004667934464 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.5021610260009766, loss=1.908607006072998
I0128 18:37:12.389044 140004676327168 logging_writer.py:48] [50000] global_step=50000, grad_norm=4.224803447723389, loss=1.8333399295806885
I0128 18:37:27.274237 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:37:33.672669 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:37:42.649367 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:37:45.275272 140169137129280 submission_runner.py:408] Time since start: 17477.12s, 	Step: 50046, 	{'train/accuracy': 0.6486168503761292, 'train/loss': 1.4161732196807861, 'validation/accuracy': 0.6020399928092957, 'validation/loss': 1.6556391716003418, 'validation/num_examples': 50000, 'test/accuracy': 0.4767000079154968, 'test/loss': 2.3756377696990967, 'test/num_examples': 10000, 'score': 16863.183659791946, 'total_duration': 17477.124361991882, 'accumulated_submission_time': 16863.183659791946, 'accumulated_eval_time': 611.1795015335083, 'accumulated_logging_time': 1.1396148204803467}
I0128 18:37:45.305943 140004676327168 logging_writer.py:48] [50046] accumulated_eval_time=611.179502, accumulated_logging_time=1.139615, accumulated_submission_time=16863.183660, global_step=50046, preemption_count=0, score=16863.183660, test/accuracy=0.476700, test/loss=2.375638, test/num_examples=10000, total_duration=17477.124362, train/accuracy=0.648617, train/loss=1.416173, validation/accuracy=0.602040, validation/loss=1.655639, validation/num_examples=50000
I0128 18:38:03.712241 140005297075968 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.1099445819854736, loss=1.8306246995925903
I0128 18:38:37.211393 140004676327168 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.7906417846679688, loss=2.0164897441864014
I0128 18:39:10.738167 140005297075968 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.4270379543304443, loss=1.9524176120758057
I0128 18:39:44.279340 140004676327168 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.484914779663086, loss=1.780502438545227
I0128 18:40:17.848947 140005297075968 logging_writer.py:48] [50500] global_step=50500, grad_norm=4.218922138214111, loss=2.0297586917877197
I0128 18:40:51.470572 140004676327168 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.6176364421844482, loss=1.88649320602417
I0128 18:41:25.052714 140005297075968 logging_writer.py:48] [50700] global_step=50700, grad_norm=4.707329750061035, loss=1.8683902025222778
I0128 18:41:58.605179 140004676327168 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.6256260871887207, loss=1.9485976696014404
I0128 18:42:32.198792 140005297075968 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.8516533374786377, loss=1.9060438871383667
I0128 18:43:05.832112 140004676327168 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.4018356800079346, loss=1.954336166381836
I0128 18:43:39.521304 140005297075968 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.997767448425293, loss=1.9313727617263794
I0128 18:44:13.121542 140004676327168 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.639849901199341, loss=1.877248764038086
I0128 18:44:46.714821 140005297075968 logging_writer.py:48] [51300] global_step=51300, grad_norm=4.044308185577393, loss=2.0330772399902344
I0128 18:45:20.356276 140004676327168 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.4312832355499268, loss=1.8963074684143066
I0128 18:45:53.964938 140005297075968 logging_writer.py:48] [51500] global_step=51500, grad_norm=4.531954765319824, loss=1.8539392948150635
I0128 18:46:15.301135 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:46:21.778714 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:46:30.573083 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:46:33.217509 140169137129280 submission_runner.py:408] Time since start: 18005.07s, 	Step: 51565, 	{'train/accuracy': 0.6805644035339355, 'train/loss': 1.269840955734253, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.6356245279312134, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.3638558387756348, 'test/num_examples': 10000, 'score': 17373.11920595169, 'total_duration': 18005.066600322723, 'accumulated_submission_time': 17373.11920595169, 'accumulated_eval_time': 629.0958392620087, 'accumulated_logging_time': 1.1804418563842773}
I0128 18:46:33.246927 140004667934464 logging_writer.py:48] [51565] accumulated_eval_time=629.095839, accumulated_logging_time=1.180442, accumulated_submission_time=17373.119206, global_step=51565, preemption_count=0, score=17373.119206, test/accuracy=0.491500, test/loss=2.363856, test/num_examples=10000, total_duration=18005.066600, train/accuracy=0.680564, train/loss=1.269841, validation/accuracy=0.607800, validation/loss=1.635625, validation/num_examples=50000
I0128 18:46:45.325252 140005288683264 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.984419345855713, loss=1.8048046827316284
I0128 18:47:18.817761 140004667934464 logging_writer.py:48] [51700] global_step=51700, grad_norm=4.628566265106201, loss=1.9418365955352783
I0128 18:47:52.399970 140005288683264 logging_writer.py:48] [51800] global_step=51800, grad_norm=4.471686840057373, loss=1.9325761795043945
I0128 18:48:25.930641 140004667934464 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.983232021331787, loss=1.9801592826843262
I0128 18:48:59.466448 140005288683264 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.560234785079956, loss=1.8893985748291016
I0128 18:49:33.006760 140004667934464 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.4653828144073486, loss=1.740186333656311
I0128 18:50:06.652991 140005288683264 logging_writer.py:48] [52200] global_step=52200, grad_norm=4.007312774658203, loss=1.7765687704086304
I0128 18:50:40.263031 140004667934464 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.4825868606567383, loss=1.9069271087646484
I0128 18:51:13.896539 140005288683264 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.3549082279205322, loss=1.8705812692642212
I0128 18:51:47.510550 140004667934464 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.8144595623016357, loss=1.8310027122497559
I0128 18:52:21.139265 140005288683264 logging_writer.py:48] [52600] global_step=52600, grad_norm=4.192542552947998, loss=1.7622319459915161
I0128 18:52:54.764896 140004667934464 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.7494752407073975, loss=1.921903133392334
I0128 18:53:28.382722 140005288683264 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.734116315841675, loss=1.8149079084396362
I0128 18:54:01.932606 140004667934464 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.648615837097168, loss=1.9085692167282104
I0128 18:54:35.538044 140005288683264 logging_writer.py:48] [53000] global_step=53000, grad_norm=4.011844158172607, loss=1.8441743850708008
I0128 18:55:03.229613 140169137129280 spec.py:321] Evaluating on the training split.
I0128 18:55:09.565714 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 18:55:18.232639 140169137129280 spec.py:349] Evaluating on the test split.
I0128 18:55:20.891461 140169137129280 submission_runner.py:408] Time since start: 18532.74s, 	Step: 53084, 	{'train/accuracy': 0.6640226244926453, 'train/loss': 1.340054988861084, 'validation/accuracy': 0.6099599599838257, 'validation/loss': 1.6146225929260254, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.3278653621673584, 'test/num_examples': 10000, 'score': 17883.04052567482, 'total_duration': 18532.7405500412, 'accumulated_submission_time': 17883.04052567482, 'accumulated_eval_time': 646.7576491832733, 'accumulated_logging_time': 1.2222459316253662}
I0128 18:55:20.921161 140005297075968 logging_writer.py:48] [53084] accumulated_eval_time=646.757649, accumulated_logging_time=1.222246, accumulated_submission_time=17883.040526, global_step=53084, preemption_count=0, score=17883.040526, test/accuracy=0.488100, test/loss=2.327865, test/num_examples=10000, total_duration=18532.740550, train/accuracy=0.664023, train/loss=1.340055, validation/accuracy=0.609960, validation/loss=1.614623, validation/num_examples=50000
I0128 18:55:26.619984 140005322254080 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.6957545280456543, loss=2.004436731338501
I0128 18:56:00.187416 140005297075968 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.5378258228302, loss=1.8206639289855957
I0128 18:56:33.826911 140005322254080 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.483506679534912, loss=1.8940905332565308
I0128 18:57:07.441051 140005297075968 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.8820619583129883, loss=1.7778549194335938
I0128 18:57:41.047985 140005322254080 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.197617530822754, loss=1.8348404169082642
I0128 18:58:14.676114 140005297075968 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.3737518787384033, loss=1.8602255582809448
I0128 18:58:48.308251 140005322254080 logging_writer.py:48] [53700] global_step=53700, grad_norm=4.33094596862793, loss=1.882838487625122
I0128 18:59:21.896641 140005297075968 logging_writer.py:48] [53800] global_step=53800, grad_norm=4.509856224060059, loss=1.9694396257400513
I0128 18:59:55.491888 140005322254080 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.3464314937591553, loss=1.8462836742401123
I0128 19:00:29.090636 140005297075968 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.6043388843536377, loss=1.8390204906463623
I0128 19:01:02.619232 140005322254080 logging_writer.py:48] [54100] global_step=54100, grad_norm=4.215197563171387, loss=1.9466360807418823
I0128 19:01:36.177037 140005297075968 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.3525807857513428, loss=1.8694548606872559
I0128 19:02:09.745784 140005322254080 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.9827659130096436, loss=1.9967198371887207
I0128 19:02:43.422465 140005297075968 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.3735008239746094, loss=1.8585493564605713
I0128 19:03:17.042688 140005322254080 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.609412431716919, loss=1.8154047727584839
I0128 19:03:50.680236 140005297075968 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.7505722045898438, loss=1.7860703468322754
I0128 19:03:51.165195 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:03:57.541241 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:04:06.550280 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:04:09.193404 140169137129280 submission_runner.py:408] Time since start: 19061.04s, 	Step: 54603, 	{'train/accuracy': 0.6684470772743225, 'train/loss': 1.310733675956726, 'validation/accuracy': 0.6151599884033203, 'validation/loss': 1.5864496231079102, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.2576968669891357, 'test/num_examples': 10000, 'score': 18393.22454881668, 'total_duration': 19061.042494535446, 'accumulated_submission_time': 18393.22454881668, 'accumulated_eval_time': 664.7858171463013, 'accumulated_logging_time': 1.2626171112060547}
I0128 19:04:09.226633 140004624946944 logging_writer.py:48] [54603] accumulated_eval_time=664.785817, accumulated_logging_time=1.262617, accumulated_submission_time=18393.224549, global_step=54603, preemption_count=0, score=18393.224549, test/accuracy=0.496500, test/loss=2.257697, test/num_examples=10000, total_duration=19061.042495, train/accuracy=0.668447, train/loss=1.310734, validation/accuracy=0.615160, validation/loss=1.586450, validation/num_examples=50000
I0128 19:04:42.046093 140005288683264 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.6162972450256348, loss=1.7763960361480713
I0128 19:05:15.643260 140004624946944 logging_writer.py:48] [54800] global_step=54800, grad_norm=4.554473876953125, loss=2.013101577758789
I0128 19:05:49.276974 140005288683264 logging_writer.py:48] [54900] global_step=54900, grad_norm=3.9810330867767334, loss=1.9008673429489136
I0128 19:06:22.901199 140004624946944 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.5867247581481934, loss=1.8797392845153809
I0128 19:06:56.507543 140005288683264 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.7782764434814453, loss=1.9076000452041626
I0128 19:07:30.068793 140004624946944 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.8896470069885254, loss=1.9137122631072998
I0128 19:08:03.571191 140005288683264 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.5216102600097656, loss=1.7577226161956787
I0128 19:08:37.118600 140004624946944 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.6425399780273438, loss=1.801853895187378
I0128 19:09:10.759811 140005288683264 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.786407232284546, loss=1.773561716079712
I0128 19:09:44.382115 140004624946944 logging_writer.py:48] [55600] global_step=55600, grad_norm=4.087181568145752, loss=1.7388015985488892
I0128 19:10:18.006394 140005288683264 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.652513265609741, loss=1.8279249668121338
I0128 19:10:51.595293 140004624946944 logging_writer.py:48] [55800] global_step=55800, grad_norm=4.746773719787598, loss=1.8813973665237427
I0128 19:11:25.213455 140005288683264 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.6091623306274414, loss=1.928891658782959
I0128 19:11:58.787448 140004624946944 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.5159554481506348, loss=1.8575206995010376
I0128 19:12:32.416099 140005288683264 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.863588333129883, loss=1.7336955070495605
I0128 19:12:39.289195 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:12:45.645349 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:12:54.403275 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:12:57.031028 140169137129280 submission_runner.py:408] Time since start: 19588.88s, 	Step: 56122, 	{'train/accuracy': 0.6581233739852905, 'train/loss': 1.3662872314453125, 'validation/accuracy': 0.6144399642944336, 'validation/loss': 1.60483980178833, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.338015079498291, 'test/num_examples': 10000, 'score': 18903.22807765007, 'total_duration': 19588.880088090897, 'accumulated_submission_time': 18903.22807765007, 'accumulated_eval_time': 682.5275778770447, 'accumulated_logging_time': 1.305849313735962}
I0128 19:12:57.063437 140005313861376 logging_writer.py:48] [56122] accumulated_eval_time=682.527578, accumulated_logging_time=1.305849, accumulated_submission_time=18903.228078, global_step=56122, preemption_count=0, score=18903.228078, test/accuracy=0.486100, test/loss=2.338015, test/num_examples=10000, total_duration=19588.880088, train/accuracy=0.658123, train/loss=1.366287, validation/accuracy=0.614440, validation/loss=1.604840, validation/num_examples=50000
I0128 19:13:23.496365 140005322254080 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.8762667179107666, loss=1.9097797870635986
I0128 19:13:57.028362 140005313861376 logging_writer.py:48] [56300] global_step=56300, grad_norm=4.059711456298828, loss=1.7511156797409058
I0128 19:14:30.612419 140005322254080 logging_writer.py:48] [56400] global_step=56400, grad_norm=4.36505126953125, loss=1.9226787090301514
I0128 19:15:04.180791 140005313861376 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.5836503505706787, loss=1.8805363178253174
I0128 19:15:37.855739 140005322254080 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.7255349159240723, loss=2.0083847045898438
I0128 19:16:11.456641 140005313861376 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.7141568660736084, loss=1.9341530799865723
I0128 19:16:45.079438 140005322254080 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.8783271312713623, loss=1.9745643138885498
I0128 19:17:18.689808 140005313861376 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.0695979595184326, loss=1.8087011575698853
I0128 19:17:52.317052 140005322254080 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.8241209983825684, loss=1.9337210655212402
I0128 19:18:25.930092 140005313861376 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.527148962020874, loss=1.7956950664520264
I0128 19:18:59.561995 140005322254080 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.6037981510162354, loss=1.8787331581115723
I0128 19:19:33.179073 140005313861376 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.5135927200317383, loss=1.947016954421997
I0128 19:20:06.806173 140005322254080 logging_writer.py:48] [57400] global_step=57400, grad_norm=4.087947368621826, loss=1.87013840675354
I0128 19:20:40.406263 140005313861376 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.8849284648895264, loss=1.8754422664642334
I0128 19:21:14.040214 140005322254080 logging_writer.py:48] [57600] global_step=57600, grad_norm=4.07574462890625, loss=1.8597419261932373
I0128 19:21:27.040444 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:21:33.414932 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:21:42.429083 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:21:45.042109 140169137129280 submission_runner.py:408] Time since start: 20116.89s, 	Step: 57640, 	{'train/accuracy': 0.6615911722183228, 'train/loss': 1.3483868837356567, 'validation/accuracy': 0.615559995174408, 'validation/loss': 1.5824005603790283, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2923803329467773, 'test/num_examples': 10000, 'score': 19413.14436841011, 'total_duration': 20116.891170024872, 'accumulated_submission_time': 19413.14436841011, 'accumulated_eval_time': 700.529173374176, 'accumulated_logging_time': 1.3492977619171143}
I0128 19:21:45.072878 140004624946944 logging_writer.py:48] [57640] accumulated_eval_time=700.529173, accumulated_logging_time=1.349298, accumulated_submission_time=19413.144368, global_step=57640, preemption_count=0, score=19413.144368, test/accuracy=0.495800, test/loss=2.292380, test/num_examples=10000, total_duration=20116.891170, train/accuracy=0.661591, train/loss=1.348387, validation/accuracy=0.615560, validation/loss=1.582401, validation/num_examples=50000
I0128 19:22:05.520271 140005288683264 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.7986066341400146, loss=1.8207416534423828
I0128 19:22:39.003455 140004624946944 logging_writer.py:48] [57800] global_step=57800, grad_norm=4.409815788269043, loss=1.7875688076019287
I0128 19:23:12.593155 140005288683264 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.3991382122039795, loss=1.8543299436569214
I0128 19:23:46.210833 140004624946944 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.5731866359710693, loss=1.8637053966522217
I0128 19:24:19.761808 140005288683264 logging_writer.py:48] [58100] global_step=58100, grad_norm=4.191021919250488, loss=1.6666924953460693
I0128 19:24:53.267218 140004624946944 logging_writer.py:48] [58200] global_step=58200, grad_norm=4.190449237823486, loss=1.9051358699798584
I0128 19:25:26.805106 140005288683264 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.350644111633301, loss=1.6857578754425049
I0128 19:26:00.384641 140004624946944 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.665815591812134, loss=1.7563663721084595
I0128 19:26:33.919423 140005288683264 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.5434038639068604, loss=1.8140777349472046
I0128 19:27:07.421170 140004624946944 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.327451229095459, loss=1.8192216157913208
I0128 19:27:41.063926 140005288683264 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.3911619186401367, loss=1.8661034107208252
I0128 19:28:14.690682 140004624946944 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.679330587387085, loss=1.8693668842315674
I0128 19:28:48.258826 140005288683264 logging_writer.py:48] [58900] global_step=58900, grad_norm=4.380002498626709, loss=1.7394180297851562
I0128 19:29:21.772845 140004624946944 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.8825180530548096, loss=1.9365661144256592
I0128 19:29:55.304160 140005288683264 logging_writer.py:48] [59100] global_step=59100, grad_norm=4.276572227478027, loss=1.8743536472320557
I0128 19:30:15.259635 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:30:21.634534 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:30:30.562037 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:30:33.172676 140169137129280 submission_runner.py:408] Time since start: 20645.02s, 	Step: 59161, 	{'train/accuracy': 0.6617307066917419, 'train/loss': 1.3514286279678345, 'validation/accuracy': 0.6044600009918213, 'validation/loss': 1.642971396446228, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.3952414989471436, 'test/num_examples': 10000, 'score': 19923.271131277084, 'total_duration': 20645.021767616272, 'accumulated_submission_time': 19923.271131277084, 'accumulated_eval_time': 718.4421739578247, 'accumulated_logging_time': 1.3909647464752197}
I0128 19:30:33.203876 140004624946944 logging_writer.py:48] [59161] accumulated_eval_time=718.442174, accumulated_logging_time=1.390965, accumulated_submission_time=19923.271131, global_step=59161, preemption_count=0, score=19923.271131, test/accuracy=0.476200, test/loss=2.395241, test/num_examples=10000, total_duration=20645.021768, train/accuracy=0.661731, train/loss=1.351429, validation/accuracy=0.604460, validation/loss=1.642971, validation/num_examples=50000
I0128 19:30:46.622420 140005313861376 logging_writer.py:48] [59200] global_step=59200, grad_norm=4.081921100616455, loss=1.8159700632095337
I0128 19:31:20.081796 140004624946944 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.80132794380188, loss=1.7606428861618042
I0128 19:31:53.639775 140005313861376 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.978757619857788, loss=1.7882529497146606
I0128 19:32:27.252571 140004624946944 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.8848845958709717, loss=1.734348177909851
I0128 19:33:00.827231 140005313861376 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.8757681846618652, loss=1.9132839441299438
I0128 19:33:34.328416 140004624946944 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.536522150039673, loss=1.8556058406829834
I0128 19:34:07.954047 140005313861376 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.9180047512054443, loss=1.811477541923523
I0128 19:34:41.480092 140004624946944 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.5243144035339355, loss=1.8752235174179077
I0128 19:35:14.995492 140005313861376 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.887172222137451, loss=1.9325064420700073
I0128 19:35:48.574815 140004624946944 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.7778220176696777, loss=1.9030952453613281
I0128 19:36:22.131615 140005313861376 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.543342351913452, loss=1.874622106552124
I0128 19:36:55.640936 140004624946944 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.4099173545837402, loss=1.7791416645050049
I0128 19:37:29.205961 140005313861376 logging_writer.py:48] [60400] global_step=60400, grad_norm=4.0392303466796875, loss=1.7848272323608398
I0128 19:38:02.754023 140004624946944 logging_writer.py:48] [60500] global_step=60500, grad_norm=4.33972692489624, loss=1.8092033863067627
I0128 19:38:36.354732 140005313861376 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.4181978702545166, loss=1.702527403831482
I0128 19:39:03.400622 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:39:09.755292 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:39:18.377559 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:39:20.984268 140169137129280 submission_runner.py:408] Time since start: 21172.83s, 	Step: 60682, 	{'train/accuracy': 0.6777144074440002, 'train/loss': 1.2699861526489258, 'validation/accuracy': 0.6165800094604492, 'validation/loss': 1.5854922533035278, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.322281837463379, 'test/num_examples': 10000, 'score': 20433.406439065933, 'total_duration': 21172.833348035812, 'accumulated_submission_time': 20433.406439065933, 'accumulated_eval_time': 736.025773525238, 'accumulated_logging_time': 1.4345412254333496}
I0128 19:39:21.016941 140005305468672 logging_writer.py:48] [60682] accumulated_eval_time=736.025774, accumulated_logging_time=1.434541, accumulated_submission_time=20433.406439, global_step=60682, preemption_count=0, score=20433.406439, test/accuracy=0.489300, test/loss=2.322282, test/num_examples=10000, total_duration=21172.833348, train/accuracy=0.677714, train/loss=1.269986, validation/accuracy=0.616580, validation/loss=1.585492, validation/num_examples=50000
I0128 19:39:27.393562 140005322254080 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.667879819869995, loss=1.8128010034561157
I0128 19:40:00.874891 140005305468672 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.3640596866607666, loss=1.977734088897705
I0128 19:40:34.498824 140005322254080 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.7694849967956543, loss=1.8115508556365967
I0128 19:41:08.019723 140005305468672 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.863551616668701, loss=1.8219105005264282
I0128 19:41:41.578577 140005322254080 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.6494529247283936, loss=1.7720140218734741
I0128 19:42:15.118600 140005305468672 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.414233684539795, loss=1.800851821899414
I0128 19:42:48.705137 140005322254080 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.514061689376831, loss=1.7338950634002686
I0128 19:43:22.324053 140005305468672 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.2191035747528076, loss=1.853074073791504
I0128 19:43:55.937534 140005322254080 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.7330284118652344, loss=1.9734869003295898
I0128 19:44:29.569715 140005305468672 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.555037498474121, loss=1.7487725019454956
I0128 19:45:03.173297 140005322254080 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.6044247150421143, loss=1.8300899267196655
I0128 19:45:36.800840 140005305468672 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.5806057453155518, loss=1.8941582441329956
I0128 19:46:10.413454 140005322254080 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.6275033950805664, loss=1.903533697128296
I0128 19:46:44.058827 140005305468672 logging_writer.py:48] [62000] global_step=62000, grad_norm=4.218451499938965, loss=1.9400187730789185
I0128 19:47:17.648851 140005322254080 logging_writer.py:48] [62100] global_step=62100, grad_norm=4.326460838317871, loss=1.7983444929122925
I0128 19:47:51.268000 140005305468672 logging_writer.py:48] [62200] global_step=62200, grad_norm=4.373732566833496, loss=1.975868821144104
I0128 19:47:51.275557 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:47:57.607441 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:48:06.432357 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:48:09.104035 140169137129280 submission_runner.py:408] Time since start: 21700.95s, 	Step: 62201, 	{'train/accuracy': 0.6781728267669678, 'train/loss': 1.2760369777679443, 'validation/accuracy': 0.6189999580383301, 'validation/loss': 1.5650577545166016, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.293773651123047, 'test/num_examples': 10000, 'score': 20943.60581111908, 'total_duration': 21700.95312690735, 'accumulated_submission_time': 20943.60581111908, 'accumulated_eval_time': 753.8541922569275, 'accumulated_logging_time': 1.4773738384246826}
I0128 19:48:09.138654 140004624946944 logging_writer.py:48] [62201] accumulated_eval_time=753.854192, accumulated_logging_time=1.477374, accumulated_submission_time=20943.605811, global_step=62201, preemption_count=0, score=20943.605811, test/accuracy=0.489400, test/loss=2.293774, test/num_examples=10000, total_duration=21700.953127, train/accuracy=0.678173, train/loss=1.276037, validation/accuracy=0.619000, validation/loss=1.565058, validation/num_examples=50000
I0128 19:48:42.640224 140005288683264 logging_writer.py:48] [62300] global_step=62300, grad_norm=4.208107948303223, loss=1.786696434020996
I0128 19:49:16.242260 140004624946944 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.8096325397491455, loss=1.8206672668457031
I0128 19:49:49.805171 140005288683264 logging_writer.py:48] [62500] global_step=62500, grad_norm=4.709936618804932, loss=1.8343760967254639
I0128 19:50:23.334801 140004624946944 logging_writer.py:48] [62600] global_step=62600, grad_norm=4.132095813751221, loss=1.8326209783554077
I0128 19:50:56.869568 140005288683264 logging_writer.py:48] [62700] global_step=62700, grad_norm=4.407948017120361, loss=1.990721344947815
I0128 19:51:30.418243 140004624946944 logging_writer.py:48] [62800] global_step=62800, grad_norm=3.7217602729797363, loss=1.8128455877304077
I0128 19:52:04.017417 140005288683264 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.4758055210113525, loss=1.7551079988479614
I0128 19:52:37.639012 140004624946944 logging_writer.py:48] [63000] global_step=63000, grad_norm=4.274366855621338, loss=1.7351526021957397
I0128 19:53:11.300203 140005288683264 logging_writer.py:48] [63100] global_step=63100, grad_norm=4.540916442871094, loss=1.9275178909301758
I0128 19:53:44.898648 140004624946944 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.78313946723938, loss=1.7703514099121094
I0128 19:54:18.488228 140005288683264 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.7120988368988037, loss=1.8260689973831177
I0128 19:54:52.089917 140004624946944 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.589015007019043, loss=1.8473536968231201
I0128 19:55:25.708071 140005288683264 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.3097991943359375, loss=1.7673726081848145
I0128 19:55:59.327398 140004624946944 logging_writer.py:48] [63600] global_step=63600, grad_norm=4.011464595794678, loss=1.8795686960220337
I0128 19:56:32.869911 140005288683264 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.538135528564453, loss=1.7243572473526
I0128 19:56:39.386602 140169137129280 spec.py:321] Evaluating on the training split.
I0128 19:56:45.787882 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 19:56:54.390704 140169137129280 spec.py:349] Evaluating on the test split.
I0128 19:56:57.026856 140169137129280 submission_runner.py:408] Time since start: 22228.88s, 	Step: 63721, 	{'train/accuracy': 0.6701809763908386, 'train/loss': 1.300941824913025, 'validation/accuracy': 0.6164000034332275, 'validation/loss': 1.5907514095306396, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.286545991897583, 'test/num_examples': 10000, 'score': 21453.792240858078, 'total_duration': 22228.8758354187, 'accumulated_submission_time': 21453.792240858078, 'accumulated_eval_time': 771.4942946434021, 'accumulated_logging_time': 1.524343490600586}
I0128 19:56:57.061449 140005322254080 logging_writer.py:48] [63721] accumulated_eval_time=771.494295, accumulated_logging_time=1.524343, accumulated_submission_time=21453.792241, global_step=63721, preemption_count=0, score=21453.792241, test/accuracy=0.495700, test/loss=2.286546, test/num_examples=10000, total_duration=22228.875835, train/accuracy=0.670181, train/loss=1.300942, validation/accuracy=0.616400, validation/loss=1.590751, validation/num_examples=50000
I0128 19:57:23.883787 140005330646784 logging_writer.py:48] [63800] global_step=63800, grad_norm=4.162623405456543, loss=1.7596888542175293
I0128 19:57:57.389193 140005322254080 logging_writer.py:48] [63900] global_step=63900, grad_norm=4.5356831550598145, loss=1.7607479095458984
I0128 19:58:30.946926 140005330646784 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.772207736968994, loss=1.8974969387054443
I0128 19:59:04.472674 140005322254080 logging_writer.py:48] [64100] global_step=64100, grad_norm=4.294247627258301, loss=1.7654870748519897
I0128 19:59:38.121206 140005330646784 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.8205575942993164, loss=1.842651128768921
I0128 20:00:11.689628 140005322254080 logging_writer.py:48] [64300] global_step=64300, grad_norm=4.175743579864502, loss=1.8774358034133911
I0128 20:00:45.326944 140005330646784 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.73583984375, loss=1.8110271692276
I0128 20:01:18.957531 140005322254080 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.914098024368286, loss=1.8605732917785645
I0128 20:01:52.560744 140005330646784 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.659977436065674, loss=1.7103991508483887
I0128 20:02:26.124886 140005322254080 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.438504457473755, loss=1.8203940391540527
I0128 20:02:59.686264 140005330646784 logging_writer.py:48] [64800] global_step=64800, grad_norm=4.029056072235107, loss=1.855447769165039
I0128 20:03:33.188795 140005322254080 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.4239861965179443, loss=1.9117308855056763
I0128 20:04:06.755511 140005330646784 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.9246554374694824, loss=1.7253310680389404
I0128 20:04:40.378958 140005322254080 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.8168673515319824, loss=1.8200139999389648
I0128 20:05:13.967056 140005330646784 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.664611577987671, loss=1.8417987823486328
I0128 20:05:27.268963 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:05:33.633270 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:05:42.501218 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:05:45.147670 140169137129280 submission_runner.py:408] Time since start: 22757.00s, 	Step: 65241, 	{'train/accuracy': 0.6570671200752258, 'train/loss': 1.3706705570220947, 'validation/accuracy': 0.6152200102806091, 'validation/loss': 1.615429162979126, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.4254047870635986, 'test/num_examples': 10000, 'score': 21963.940816640854, 'total_duration': 22756.996761083603, 'accumulated_submission_time': 21963.940816640854, 'accumulated_eval_time': 789.372960805893, 'accumulated_logging_time': 1.5687105655670166}
I0128 20:05:45.179976 140004624946944 logging_writer.py:48] [65241] accumulated_eval_time=789.372961, accumulated_logging_time=1.568711, accumulated_submission_time=21963.940817, global_step=65241, preemption_count=0, score=21963.940817, test/accuracy=0.481100, test/loss=2.425405, test/num_examples=10000, total_duration=22756.996761, train/accuracy=0.657067, train/loss=1.370671, validation/accuracy=0.615220, validation/loss=1.615429, validation/num_examples=50000
I0128 20:06:05.312653 140005288683264 logging_writer.py:48] [65300] global_step=65300, grad_norm=4.02916145324707, loss=1.7911887168884277
I0128 20:06:38.801422 140004624946944 logging_writer.py:48] [65400] global_step=65400, grad_norm=4.1052165031433105, loss=1.8275892734527588
I0128 20:07:12.392725 140005288683264 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.8654050827026367, loss=1.8835945129394531
I0128 20:07:46.008245 140004624946944 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.5871434211730957, loss=1.82771635055542
I0128 20:08:19.623985 140005288683264 logging_writer.py:48] [65700] global_step=65700, grad_norm=4.04476261138916, loss=1.7087790966033936
I0128 20:08:53.238591 140004624946944 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.8824336528778076, loss=1.867356538772583
I0128 20:09:26.861382 140005288683264 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.9914262294769287, loss=1.8443292379379272
I0128 20:10:00.442363 140004624946944 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.2586305141448975, loss=1.7934702634811401
I0128 20:10:33.979829 140005288683264 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.899590492248535, loss=1.8023126125335693
I0128 20:11:07.504254 140004624946944 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.997079372406006, loss=1.777616024017334
I0128 20:11:41.114009 140005288683264 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.7520856857299805, loss=1.835242509841919
I0128 20:12:14.671313 140004624946944 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.4588470458984375, loss=1.8605889081954956
I0128 20:12:48.191209 140005288683264 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.552471160888672, loss=1.8090896606445312
I0128 20:13:21.779055 140004624946944 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.3721845149993896, loss=1.8047348260879517
I0128 20:13:55.388101 140005288683264 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.376047372817993, loss=1.8768752813339233
I0128 20:14:15.365789 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:14:21.716415 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:14:30.758763 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:14:33.387697 140169137129280 submission_runner.py:408] Time since start: 23285.24s, 	Step: 66761, 	{'train/accuracy': 0.6267139315605164, 'train/loss': 1.5229088068008423, 'validation/accuracy': 0.5832399725914001, 'validation/loss': 1.7533199787139893, 'validation/num_examples': 50000, 'test/accuracy': 0.45820000767707825, 'test/loss': 2.52813458442688, 'test/num_examples': 10000, 'score': 22474.066437244415, 'total_duration': 23285.236786603928, 'accumulated_submission_time': 22474.066437244415, 'accumulated_eval_time': 807.3948268890381, 'accumulated_logging_time': 1.6115176677703857}
I0128 20:14:33.420177 140005322254080 logging_writer.py:48] [66761] accumulated_eval_time=807.394827, accumulated_logging_time=1.611518, accumulated_submission_time=22474.066437, global_step=66761, preemption_count=0, score=22474.066437, test/accuracy=0.458200, test/loss=2.528135, test/num_examples=10000, total_duration=23285.236787, train/accuracy=0.626714, train/loss=1.522909, validation/accuracy=0.583240, validation/loss=1.753320, validation/num_examples=50000
I0128 20:14:46.819843 140005330646784 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.840040683746338, loss=1.8212647438049316
I0128 20:15:20.302323 140005322254080 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.693018674850464, loss=1.8069266080856323
I0128 20:15:53.872284 140005330646784 logging_writer.py:48] [67000] global_step=67000, grad_norm=4.1869049072265625, loss=1.8628222942352295
I0128 20:16:27.463726 140005322254080 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.5330703258514404, loss=1.8212095499038696
I0128 20:17:01.016348 140005330646784 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.8160359859466553, loss=1.8716050386428833
I0128 20:17:34.529239 140005322254080 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.834674596786499, loss=1.8462233543395996
I0128 20:18:08.182624 140005330646784 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.47204327583313, loss=1.8232468366622925
I0128 20:18:41.777584 140005322254080 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.7220864295959473, loss=1.7701280117034912
I0128 20:19:15.329476 140005330646784 logging_writer.py:48] [67600] global_step=67600, grad_norm=4.347903728485107, loss=1.780104637145996
I0128 20:19:48.845726 140005322254080 logging_writer.py:48] [67700] global_step=67700, grad_norm=4.2031168937683105, loss=1.8106169700622559
I0128 20:20:22.391265 140005330646784 logging_writer.py:48] [67800] global_step=67800, grad_norm=4.29314661026001, loss=1.8095781803131104
I0128 20:20:55.953845 140005322254080 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.97619366645813, loss=1.68861722946167
I0128 20:21:29.526784 140005330646784 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.239785671234131, loss=1.9498809576034546
I0128 20:22:03.136134 140005322254080 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.391908884048462, loss=1.8042744398117065
I0128 20:22:36.759302 140005330646784 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.84378981590271, loss=1.8033576011657715
I0128 20:23:03.470105 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:23:09.858905 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:23:18.485426 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:23:21.168948 140169137129280 submission_runner.py:408] Time since start: 23813.02s, 	Step: 68281, 	{'train/accuracy': 0.7253667116165161, 'train/loss': 1.0721691846847534, 'validation/accuracy': 0.6342999935150146, 'validation/loss': 1.5084197521209717, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.1850202083587646, 'test/num_examples': 10000, 'score': 22984.056749343872, 'total_duration': 23813.018038749695, 'accumulated_submission_time': 22984.056749343872, 'accumulated_eval_time': 825.0936350822449, 'accumulated_logging_time': 1.6541671752929688}
I0128 20:23:21.202216 140004616554240 logging_writer.py:48] [68281] accumulated_eval_time=825.093635, accumulated_logging_time=1.654167, accumulated_submission_time=22984.056749, global_step=68281, preemption_count=0, score=22984.056749, test/accuracy=0.512500, test/loss=2.185020, test/num_examples=10000, total_duration=23813.018039, train/accuracy=0.725367, train/loss=1.072169, validation/accuracy=0.634300, validation/loss=1.508420, validation/num_examples=50000
I0128 20:23:27.910310 140004624946944 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.448288917541504, loss=1.823630928993225
I0128 20:24:01.415043 140004616554240 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.9923086166381836, loss=1.8150802850723267
I0128 20:24:35.061877 140004624946944 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.9901065826416016, loss=1.898092269897461
I0128 20:25:08.647264 140004616554240 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.6645920276641846, loss=1.7692915201187134
I0128 20:25:42.274466 140004624946944 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.772930383682251, loss=1.772223711013794
I0128 20:26:15.893962 140004616554240 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.366739273071289, loss=1.7737947702407837
I0128 20:26:49.496093 140004624946944 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.68951678276062, loss=1.880583643913269
I0128 20:27:23.126128 140004616554240 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.823308229446411, loss=1.766702651977539
I0128 20:27:56.720446 140004624946944 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.9796700477600098, loss=1.8510394096374512
I0128 20:28:30.352318 140004616554240 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.9433131217956543, loss=1.8326151371002197
I0128 20:29:03.962929 140004624946944 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.6696858406066895, loss=1.7830626964569092
I0128 20:29:37.585835 140004616554240 logging_writer.py:48] [69400] global_step=69400, grad_norm=4.354435920715332, loss=1.8054804801940918
I0128 20:30:11.198615 140004624946944 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.911914825439453, loss=1.7221777439117432
I0128 20:30:44.843091 140004616554240 logging_writer.py:48] [69600] global_step=69600, grad_norm=4.154935836791992, loss=1.663083553314209
I0128 20:31:18.439608 140004624946944 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.3913681507110596, loss=1.7141729593276978
I0128 20:31:51.182725 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:31:57.587517 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:32:06.317642 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:32:08.982112 140169137129280 submission_runner.py:408] Time since start: 24340.83s, 	Step: 69799, 	{'train/accuracy': 0.6788305044174194, 'train/loss': 1.2620773315429688, 'validation/accuracy': 0.6158999800682068, 'validation/loss': 1.5898820161819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.3095078468322754, 'test/num_examples': 10000, 'score': 23493.977559566498, 'total_duration': 24340.83118534088, 'accumulated_submission_time': 23493.977559566498, 'accumulated_eval_time': 842.8929722309113, 'accumulated_logging_time': 1.6980137825012207}
I0128 20:32:09.014757 140004624946944 logging_writer.py:48] [69799] accumulated_eval_time=842.892972, accumulated_logging_time=1.698014, accumulated_submission_time=23493.977560, global_step=69799, preemption_count=0, score=23493.977560, test/accuracy=0.492700, test/loss=2.309508, test/num_examples=10000, total_duration=24340.831185, train/accuracy=0.678831, train/loss=1.262077, validation/accuracy=0.615900, validation/loss=1.589882, validation/num_examples=50000
I0128 20:32:09.709767 140005313861376 logging_writer.py:48] [69800] global_step=69800, grad_norm=4.028425693511963, loss=1.767157793045044
I0128 20:32:43.227540 140004624946944 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.7416884899139404, loss=1.7698957920074463
I0128 20:33:16.737433 140005313861376 logging_writer.py:48] [70000] global_step=70000, grad_norm=4.055188179016113, loss=1.7853038311004639
I0128 20:33:50.294241 140004624946944 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.839171886444092, loss=1.898196816444397
I0128 20:34:23.861933 140005313861376 logging_writer.py:48] [70200] global_step=70200, grad_norm=4.383786678314209, loss=1.808673620223999
I0128 20:34:57.376373 140004624946944 logging_writer.py:48] [70300] global_step=70300, grad_norm=4.04799222946167, loss=1.6855639219284058
I0128 20:35:30.909692 140005313861376 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.918313980102539, loss=1.7258529663085938
I0128 20:36:04.460025 140004624946944 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.9257068634033203, loss=1.840175747871399
I0128 20:36:38.044745 140005313861376 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.6937081813812256, loss=1.8402353525161743
I0128 20:37:11.710406 140004624946944 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.959904432296753, loss=1.8711204528808594
I0128 20:37:45.254314 140005313861376 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.617365837097168, loss=1.8283740282058716
I0128 20:38:18.866649 140004624946944 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.5476295948028564, loss=1.7796258926391602
I0128 20:38:52.473128 140005313861376 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.9328038692474365, loss=1.7336840629577637
I0128 20:39:26.096266 140004624946944 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.6567201614379883, loss=1.7771776914596558
I0128 20:39:59.709946 140005313861376 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.58208966255188, loss=1.8984562158584595
I0128 20:40:33.327437 140004624946944 logging_writer.py:48] [71300] global_step=71300, grad_norm=4.018441677093506, loss=1.775852084159851
I0128 20:40:39.187982 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:40:45.492479 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:40:54.434592 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:40:57.092599 140169137129280 submission_runner.py:408] Time since start: 24868.94s, 	Step: 71319, 	{'train/accuracy': 0.6802256107330322, 'train/loss': 1.253443956375122, 'validation/accuracy': 0.6263799667358398, 'validation/loss': 1.537427306175232, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2284297943115234, 'test/num_examples': 10000, 'score': 24004.088791370392, 'total_duration': 24868.941687583923, 'accumulated_submission_time': 24004.088791370392, 'accumulated_eval_time': 860.7975602149963, 'accumulated_logging_time': 1.7430338859558105}
I0128 20:40:57.124577 140004616554240 logging_writer.py:48] [71319] accumulated_eval_time=860.797560, accumulated_logging_time=1.743034, accumulated_submission_time=24004.088791, global_step=71319, preemption_count=0, score=24004.088791, test/accuracy=0.506000, test/loss=2.228430, test/num_examples=10000, total_duration=24868.941688, train/accuracy=0.680226, train/loss=1.253444, validation/accuracy=0.626380, validation/loss=1.537427, validation/num_examples=50000
I0128 20:41:24.581949 140004624946944 logging_writer.py:48] [71400] global_step=71400, grad_norm=4.553826332092285, loss=1.771175503730774
I0128 20:41:58.113392 140004616554240 logging_writer.py:48] [71500] global_step=71500, grad_norm=4.301372528076172, loss=1.7229547500610352
I0128 20:42:31.723085 140004624946944 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.590421676635742, loss=1.775718092918396
I0128 20:43:05.349801 140004616554240 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.7805893421173096, loss=1.8925285339355469
I0128 20:43:38.959371 140004624946944 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.4085590839385986, loss=1.8090559244155884
I0128 20:44:12.545383 140004616554240 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.571613073348999, loss=1.8095892667770386
I0128 20:44:46.152398 140004624946944 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.035026550292969, loss=1.721380352973938
I0128 20:45:19.769441 140004616554240 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.037158966064453, loss=1.814695119857788
I0128 20:45:53.354763 140004624946944 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.8258674144744873, loss=1.8440254926681519
I0128 20:46:26.991188 140004616554240 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.8871073722839355, loss=1.8231821060180664
I0128 20:47:00.623804 140004624946944 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.7645373344421387, loss=1.7851831912994385
I0128 20:47:34.231735 140004616554240 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.9583866596221924, loss=1.7059698104858398
I0128 20:48:07.790994 140004624946944 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.8277852535247803, loss=1.8631218671798706
I0128 20:48:41.391923 140004616554240 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.996530055999756, loss=1.916804313659668
I0128 20:49:15.015409 140004624946944 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.914637327194214, loss=1.6812013387680054
I0128 20:49:27.264274 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:49:34.024142 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:49:42.687687 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:49:45.345874 140169137129280 submission_runner.py:408] Time since start: 25397.19s, 	Step: 72838, 	{'train/accuracy': 0.6843112111091614, 'train/loss': 1.2410756349563599, 'validation/accuracy': 0.6298199892044067, 'validation/loss': 1.5166643857955933, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.216010332107544, 'test/num_examples': 10000, 'score': 24514.16850876808, 'total_duration': 25397.194969654083, 'accumulated_submission_time': 24514.16850876808, 'accumulated_eval_time': 878.8791308403015, 'accumulated_logging_time': 1.7854652404785156}
I0128 20:49:45.376479 140004616554240 logging_writer.py:48] [72838] accumulated_eval_time=878.879131, accumulated_logging_time=1.785465, accumulated_submission_time=24514.168509, global_step=72838, preemption_count=0, score=24514.168509, test/accuracy=0.505300, test/loss=2.216010, test/num_examples=10000, total_duration=25397.194970, train/accuracy=0.684311, train/loss=1.241076, validation/accuracy=0.629820, validation/loss=1.516664, validation/num_examples=50000
I0128 20:50:06.491804 140005305468672 logging_writer.py:48] [72900] global_step=72900, grad_norm=4.272200584411621, loss=1.850952386856079
I0128 20:50:39.965971 140004616554240 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.7265570163726807, loss=1.7026888132095337
I0128 20:51:13.546924 140005305468672 logging_writer.py:48] [73100] global_step=73100, grad_norm=4.304243564605713, loss=1.7144347429275513
I0128 20:51:47.133382 140004616554240 logging_writer.py:48] [73200] global_step=73200, grad_norm=4.058790683746338, loss=1.7497055530548096
I0128 20:52:20.740259 140005305468672 logging_writer.py:48] [73300] global_step=73300, grad_norm=4.2713541984558105, loss=1.7370219230651855
I0128 20:52:54.269001 140004616554240 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.7070891857147217, loss=1.6267675161361694
I0128 20:53:27.785004 140005305468672 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.881080150604248, loss=1.6726014614105225
I0128 20:54:01.366456 140004616554240 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.762084484100342, loss=1.6844860315322876
I0128 20:54:34.915786 140005305468672 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.6692564487457275, loss=1.87847900390625
I0128 20:55:08.434824 140004616554240 logging_writer.py:48] [73800] global_step=73800, grad_norm=4.164793491363525, loss=1.7446519136428833
I0128 20:55:41.965584 140005305468672 logging_writer.py:48] [73900] global_step=73900, grad_norm=4.532962322235107, loss=1.7635928392410278
I0128 20:56:15.594754 140004616554240 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.78364634513855, loss=1.6697280406951904
I0128 20:56:49.146660 140005305468672 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.7331385612487793, loss=1.6843106746673584
I0128 20:57:22.743226 140004616554240 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.879664182662964, loss=1.7769607305526733
I0128 20:57:56.347809 140005305468672 logging_writer.py:48] [74300] global_step=74300, grad_norm=4.264253616333008, loss=1.8101552724838257
I0128 20:58:15.622510 140169137129280 spec.py:321] Evaluating on the training split.
I0128 20:58:22.101654 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 20:58:31.017728 140169137129280 spec.py:349] Evaluating on the test split.
I0128 20:58:33.650062 140169137129280 submission_runner.py:408] Time since start: 25925.50s, 	Step: 74359, 	{'train/accuracy': 0.6735291481018066, 'train/loss': 1.2930454015731812, 'validation/accuracy': 0.6256600022315979, 'validation/loss': 1.5487301349639893, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2394707202911377, 'test/num_examples': 10000, 'score': 25024.354562044144, 'total_duration': 25925.49914598465, 'accumulated_submission_time': 25024.354562044144, 'accumulated_eval_time': 896.9066410064697, 'accumulated_logging_time': 1.8266286849975586}
I0128 20:58:33.682242 140004624946944 logging_writer.py:48] [74359] accumulated_eval_time=896.906641, accumulated_logging_time=1.826629, accumulated_submission_time=25024.354562, global_step=74359, preemption_count=0, score=25024.354562, test/accuracy=0.504500, test/loss=2.239471, test/num_examples=10000, total_duration=25925.499146, train/accuracy=0.673529, train/loss=1.293045, validation/accuracy=0.625660, validation/loss=1.548730, validation/num_examples=50000
I0128 20:58:47.757671 140005297075968 logging_writer.py:48] [74400] global_step=74400, grad_norm=4.232972621917725, loss=1.6764552593231201
I0128 20:59:21.237334 140004624946944 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.800715446472168, loss=1.7835637331008911
I0128 20:59:54.770593 140005297075968 logging_writer.py:48] [74600] global_step=74600, grad_norm=4.4309916496276855, loss=1.8397202491760254
I0128 21:00:28.387506 140004624946944 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.936645984649658, loss=1.7119536399841309
I0128 21:01:02.003616 140005297075968 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.9582011699676514, loss=1.7578487396240234
I0128 21:01:35.603478 140004624946944 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.8632898330688477, loss=1.7742903232574463
I0128 21:02:09.228872 140005297075968 logging_writer.py:48] [75000] global_step=75000, grad_norm=5.69516134262085, loss=1.7458339929580688
I0128 21:02:42.854593 140004624946944 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.8347926139831543, loss=1.7622413635253906
I0128 21:03:16.435795 140005297075968 logging_writer.py:48] [75200] global_step=75200, grad_norm=4.14105749130249, loss=1.6489524841308594
I0128 21:03:50.017616 140004624946944 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.742676258087158, loss=1.7831190824508667
I0128 21:04:23.638889 140005297075968 logging_writer.py:48] [75400] global_step=75400, grad_norm=4.391291618347168, loss=1.6440340280532837
I0128 21:04:57.263496 140004624946944 logging_writer.py:48] [75500] global_step=75500, grad_norm=4.13602876663208, loss=1.7574726343154907
I0128 21:05:30.875303 140005297075968 logging_writer.py:48] [75600] global_step=75600, grad_norm=4.12388277053833, loss=1.7321339845657349
I0128 21:06:04.490760 140004624946944 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.5110363960266113, loss=1.819624423980713
I0128 21:06:38.112204 140005297075968 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.4995369911193848, loss=1.7158589363098145
I0128 21:07:03.774311 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:07:10.133399 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:07:18.823285 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:07:21.427871 140169137129280 submission_runner.py:408] Time since start: 26453.28s, 	Step: 75878, 	{'train/accuracy': 0.6875996589660645, 'train/loss': 1.2251676321029663, 'validation/accuracy': 0.6364799737930298, 'validation/loss': 1.4780266284942627, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2012691497802734, 'test/num_examples': 10000, 'score': 25534.386019468307, 'total_duration': 26453.276960134506, 'accumulated_submission_time': 25534.386019468307, 'accumulated_eval_time': 914.560183763504, 'accumulated_logging_time': 1.8696751594543457}
I0128 21:07:21.461883 140004616554240 logging_writer.py:48] [75878] accumulated_eval_time=914.560184, accumulated_logging_time=1.869675, accumulated_submission_time=25534.386019, global_step=75878, preemption_count=0, score=25534.386019, test/accuracy=0.509400, test/loss=2.201269, test/num_examples=10000, total_duration=26453.276960, train/accuracy=0.687600, train/loss=1.225168, validation/accuracy=0.636480, validation/loss=1.478027, validation/num_examples=50000
I0128 21:07:29.196130 140004624946944 logging_writer.py:48] [75900] global_step=75900, grad_norm=4.118293762207031, loss=1.7481392621994019
I0128 21:08:02.692431 140004616554240 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.9242401123046875, loss=1.6754093170166016
I0128 21:08:36.198593 140004624946944 logging_writer.py:48] [76100] global_step=76100, grad_norm=4.4584784507751465, loss=1.845238447189331
I0128 21:09:09.786664 140004616554240 logging_writer.py:48] [76200] global_step=76200, grad_norm=4.475318431854248, loss=1.809070348739624
I0128 21:09:43.389551 140004624946944 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.769129753112793, loss=1.7613158226013184
I0128 21:10:16.985013 140004616554240 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.5729031562805176, loss=1.685808777809143
I0128 21:10:50.600089 140004624946944 logging_writer.py:48] [76500] global_step=76500, grad_norm=4.2062249183654785, loss=1.8996304273605347
I0128 21:11:24.185282 140004616554240 logging_writer.py:48] [76600] global_step=76600, grad_norm=4.337207794189453, loss=1.7427204847335815
I0128 21:11:57.804359 140004624946944 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.852013349533081, loss=1.773423671722412
I0128 21:12:31.391884 140004616554240 logging_writer.py:48] [76800] global_step=76800, grad_norm=4.46865177154541, loss=1.6759718656539917
I0128 21:13:05.020565 140004624946944 logging_writer.py:48] [76900] global_step=76900, grad_norm=4.115755558013916, loss=1.8289577960968018
I0128 21:13:38.605342 140004616554240 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.8520724773406982, loss=1.7461507320404053
I0128 21:14:12.126975 140004624946944 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.8110318183898926, loss=1.6923391819000244
I0128 21:14:45.643975 140004616554240 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.8579084873199463, loss=1.7580220699310303
I0128 21:15:19.244795 140004624946944 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.423970699310303, loss=1.793288230895996
I0128 21:15:51.564324 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:15:58.134306 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:16:06.764301 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:16:09.400748 140169137129280 submission_runner.py:408] Time since start: 26981.25s, 	Step: 77398, 	{'train/accuracy': 0.7134685516357422, 'train/loss': 1.1057634353637695, 'validation/accuracy': 0.6320599913597107, 'validation/loss': 1.5262938737869263, 'validation/num_examples': 50000, 'test/accuracy': 0.49640002846717834, 'test/loss': 2.239720582962036, 'test/num_examples': 10000, 'score': 26044.426952838898, 'total_duration': 26981.249837636948, 'accumulated_submission_time': 26044.426952838898, 'accumulated_eval_time': 932.3965713977814, 'accumulated_logging_time': 1.9160151481628418}
I0128 21:16:09.438619 140004616554240 logging_writer.py:48] [77398] accumulated_eval_time=932.396571, accumulated_logging_time=1.916015, accumulated_submission_time=26044.426953, global_step=77398, preemption_count=0, score=26044.426953, test/accuracy=0.496400, test/loss=2.239721, test/num_examples=10000, total_duration=26981.249838, train/accuracy=0.713469, train/loss=1.105763, validation/accuracy=0.632060, validation/loss=1.526294, validation/num_examples=50000
I0128 21:16:10.475604 140005297075968 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.611304998397827, loss=1.7700533866882324
I0128 21:16:43.987560 140004616554240 logging_writer.py:48] [77500] global_step=77500, grad_norm=4.168604373931885, loss=1.824594497680664
I0128 21:17:17.510286 140005297075968 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.888441562652588, loss=1.7012972831726074
I0128 21:17:51.111070 140004616554240 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.1073994636535645, loss=1.8160815238952637
I0128 21:18:24.636503 140005297075968 logging_writer.py:48] [77800] global_step=77800, grad_norm=4.1591596603393555, loss=1.7437509298324585
I0128 21:18:58.145696 140004616554240 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.679701805114746, loss=1.795777678489685
I0128 21:19:31.718965 140005297075968 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.7737648487091064, loss=1.73163640499115
I0128 21:20:05.298568 140004616554240 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.8964507579803467, loss=1.730468988418579
I0128 21:20:38.830425 140005297075968 logging_writer.py:48] [78200] global_step=78200, grad_norm=4.052955150604248, loss=1.6134191751480103
I0128 21:21:12.447141 140004616554240 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.873891592025757, loss=1.7539546489715576
I0128 21:21:46.044067 140005297075968 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.7816879749298096, loss=1.7092702388763428
I0128 21:22:19.662359 140004616554240 logging_writer.py:48] [78500] global_step=78500, grad_norm=5.436419486999512, loss=1.7508658170700073
I0128 21:22:53.281468 140005297075968 logging_writer.py:48] [78600] global_step=78600, grad_norm=4.252938270568848, loss=1.6944949626922607
I0128 21:23:26.889020 140004616554240 logging_writer.py:48] [78700] global_step=78700, grad_norm=4.286308765411377, loss=1.8396140336990356
I0128 21:24:00.516188 140005297075968 logging_writer.py:48] [78800] global_step=78800, grad_norm=4.357080936431885, loss=1.7782739400863647
I0128 21:24:34.123166 140004616554240 logging_writer.py:48] [78900] global_step=78900, grad_norm=4.150944232940674, loss=1.7509684562683105
I0128 21:24:39.655508 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:24:45.999850 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:24:54.793612 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:24:57.487427 140169137129280 submission_runner.py:408] Time since start: 27509.34s, 	Step: 78918, 	{'train/accuracy': 0.7004145383834839, 'train/loss': 1.1687120199203491, 'validation/accuracy': 0.6345599889755249, 'validation/loss': 1.4978222846984863, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2021243572235107, 'test/num_examples': 10000, 'score': 26554.584349632263, 'total_duration': 27509.33651995659, 'accumulated_submission_time': 26554.584349632263, 'accumulated_eval_time': 950.2284531593323, 'accumulated_logging_time': 1.9640913009643555}
I0128 21:24:57.518234 140004624946944 logging_writer.py:48] [78918] accumulated_eval_time=950.228453, accumulated_logging_time=1.964091, accumulated_submission_time=26554.584350, global_step=78918, preemption_count=0, score=26554.584350, test/accuracy=0.511900, test/loss=2.202124, test/num_examples=10000, total_duration=27509.336520, train/accuracy=0.700415, train/loss=1.168712, validation/accuracy=0.634560, validation/loss=1.497822, validation/num_examples=50000
I0128 21:25:25.347160 140005288683264 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.036158561706543, loss=1.7885898351669312
I0128 21:25:58.905097 140004624946944 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.7668068408966064, loss=1.7437355518341064
I0128 21:26:32.389926 140005288683264 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.833592891693115, loss=1.8435401916503906
I0128 21:27:05.947409 140004624946944 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.0524396896362305, loss=1.7441819906234741
I0128 21:27:39.607749 140005288683264 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.227995872497559, loss=1.842555046081543
I0128 21:28:13.164487 140004624946944 logging_writer.py:48] [79500] global_step=79500, grad_norm=4.095617771148682, loss=1.8204456567764282
I0128 21:28:46.761705 140005288683264 logging_writer.py:48] [79600] global_step=79600, grad_norm=4.170329570770264, loss=1.732023000717163
I0128 21:29:20.313282 140004624946944 logging_writer.py:48] [79700] global_step=79700, grad_norm=4.222421169281006, loss=1.5934803485870361
I0128 21:29:53.813570 140005288683264 logging_writer.py:48] [79800] global_step=79800, grad_norm=4.367605686187744, loss=1.7747433185577393
I0128 21:30:27.396853 140004624946944 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.838914632797241, loss=1.6339305639266968
I0128 21:31:00.984222 140005288683264 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.916529655456543, loss=1.7619236707687378
I0128 21:31:34.557531 140004624946944 logging_writer.py:48] [80100] global_step=80100, grad_norm=4.297904014587402, loss=1.7842588424682617
I0128 21:32:08.185338 140005288683264 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.733049154281616, loss=1.7453696727752686
I0128 21:32:41.755414 140004624946944 logging_writer.py:48] [80300] global_step=80300, grad_norm=4.282861232757568, loss=1.7186675071716309
I0128 21:33:15.291765 140005288683264 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.495194435119629, loss=1.7654857635498047
I0128 21:33:27.504235 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:33:33.854204 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:33:42.829796 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:33:45.449655 140169137129280 submission_runner.py:408] Time since start: 28037.30s, 	Step: 80438, 	{'train/accuracy': 0.6895527839660645, 'train/loss': 1.2138910293579102, 'validation/accuracy': 0.6326000094413757, 'validation/loss': 1.5095300674438477, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.279034376144409, 'test/num_examples': 10000, 'score': 27064.510639190674, 'total_duration': 28037.29874444008, 'accumulated_submission_time': 27064.510639190674, 'accumulated_eval_time': 968.1738333702087, 'accumulated_logging_time': 2.0051536560058594}
I0128 21:33:45.488686 140005313861376 logging_writer.py:48] [80438] accumulated_eval_time=968.173833, accumulated_logging_time=2.005154, accumulated_submission_time=27064.510639, global_step=80438, preemption_count=0, score=27064.510639, test/accuracy=0.499900, test/loss=2.279034, test/num_examples=10000, total_duration=28037.298744, train/accuracy=0.689553, train/loss=1.213891, validation/accuracy=0.632600, validation/loss=1.509530, validation/num_examples=50000
I0128 21:34:06.711906 140005322254080 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.21630334854126, loss=1.7829080820083618
I0128 21:34:40.308878 140005313861376 logging_writer.py:48] [80600] global_step=80600, grad_norm=4.154932498931885, loss=1.7138898372650146
I0128 21:35:13.907521 140005322254080 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.936898946762085, loss=1.7195371389389038
I0128 21:35:47.519325 140005313861376 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.482317924499512, loss=1.8663415908813477
I0128 21:36:21.128281 140005322254080 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.087675094604492, loss=1.7333197593688965
I0128 21:36:54.742536 140005313861376 logging_writer.py:48] [81000] global_step=81000, grad_norm=4.162827968597412, loss=1.6332533359527588
I0128 21:37:28.284078 140005322254080 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.6648385524749756, loss=1.769749402999878
I0128 21:38:01.778431 140005313861376 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.810112476348877, loss=1.6690828800201416
I0128 21:38:35.362513 140005322254080 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.889695405960083, loss=1.7743895053863525
I0128 21:39:08.958184 140005313861376 logging_writer.py:48] [81400] global_step=81400, grad_norm=4.127283096313477, loss=1.6607296466827393
I0128 21:39:42.501480 140005322254080 logging_writer.py:48] [81500] global_step=81500, grad_norm=4.437368869781494, loss=1.6824853420257568
I0128 21:40:16.178322 140005313861376 logging_writer.py:48] [81600] global_step=81600, grad_norm=4.687648296356201, loss=1.6775784492492676
I0128 21:40:49.755592 140005322254080 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.164914131164551, loss=1.640866756439209
I0128 21:41:23.341958 140005313861376 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.910218238830566, loss=1.7971786260604858
I0128 21:41:56.853309 140005322254080 logging_writer.py:48] [81900] global_step=81900, grad_norm=4.354825019836426, loss=1.6967774629592896
I0128 21:42:15.771710 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:42:22.094365 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:42:31.225296 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:42:33.950633 140169137129280 submission_runner.py:408] Time since start: 28565.80s, 	Step: 81958, 	{'train/accuracy': 0.6863241195678711, 'train/loss': 1.2330068349838257, 'validation/accuracy': 0.6323399543762207, 'validation/loss': 1.5172523260116577, 'validation/num_examples': 50000, 'test/accuracy': 0.5067999958992004, 'test/loss': 2.2435383796691895, 'test/num_examples': 10000, 'score': 27574.732538461685, 'total_duration': 28565.799509763718, 'accumulated_submission_time': 27574.732538461685, 'accumulated_eval_time': 986.3525202274323, 'accumulated_logging_time': 2.056394100189209}
I0128 21:42:33.984692 140004624946944 logging_writer.py:48] [81958] accumulated_eval_time=986.352520, accumulated_logging_time=2.056394, accumulated_submission_time=27574.732538, global_step=81958, preemption_count=0, score=27574.732538, test/accuracy=0.506800, test/loss=2.243538, test/num_examples=10000, total_duration=28565.799510, train/accuracy=0.686324, train/loss=1.233007, validation/accuracy=0.632340, validation/loss=1.517252, validation/num_examples=50000
I0128 21:42:48.435604 140005288683264 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.301285743713379, loss=1.718981385231018
I0128 21:43:21.912235 140004624946944 logging_writer.py:48] [82100] global_step=82100, grad_norm=5.4647321701049805, loss=1.6856706142425537
I0128 21:43:55.454376 140005288683264 logging_writer.py:48] [82200] global_step=82200, grad_norm=4.257582187652588, loss=1.696584701538086
I0128 21:44:29.006744 140004624946944 logging_writer.py:48] [82300] global_step=82300, grad_norm=4.074800968170166, loss=1.758351445198059
I0128 21:45:02.627081 140005288683264 logging_writer.py:48] [82400] global_step=82400, grad_norm=4.227123737335205, loss=1.8760600090026855
I0128 21:45:36.252067 140004624946944 logging_writer.py:48] [82500] global_step=82500, grad_norm=4.003625392913818, loss=1.6953768730163574
I0128 21:46:09.858846 140005288683264 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.7929630279541016, loss=1.898150086402893
I0128 21:46:43.486859 140004624946944 logging_writer.py:48] [82700] global_step=82700, grad_norm=4.285513401031494, loss=1.7687897682189941
I0128 21:47:17.080850 140005288683264 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.4015278816223145, loss=1.7189139127731323
I0128 21:47:50.694900 140004624946944 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.296030521392822, loss=1.6716983318328857
I0128 21:48:24.241229 140005288683264 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.616647243499756, loss=1.6783982515335083
I0128 21:48:57.740434 140004624946944 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.723717212677002, loss=1.6230905055999756
I0128 21:49:31.298901 140005288683264 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.796602725982666, loss=1.7600756883621216
I0128 21:50:04.912044 140004624946944 logging_writer.py:48] [83300] global_step=83300, grad_norm=4.18296480178833, loss=1.6812878847122192
I0128 21:50:38.531623 140005288683264 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.863439083099365, loss=1.7088700532913208
I0128 21:51:04.230825 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:51:10.712706 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 21:51:19.645267 140169137129280 spec.py:349] Evaluating on the test split.
I0128 21:51:22.270387 140169137129280 submission_runner.py:408] Time since start: 29094.12s, 	Step: 83478, 	{'train/accuracy': 0.6814213991165161, 'train/loss': 1.2530291080474854, 'validation/accuracy': 0.6282199621200562, 'validation/loss': 1.5206059217453003, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.229276418685913, 'test/num_examples': 10000, 'score': 28084.9185359478, 'total_duration': 29094.119473934174, 'accumulated_submission_time': 28084.9185359478, 'accumulated_eval_time': 1004.3920419216156, 'accumulated_logging_time': 2.1011264324188232}
I0128 21:51:22.302612 140004616554240 logging_writer.py:48] [83478] accumulated_eval_time=1004.392042, accumulated_logging_time=2.101126, accumulated_submission_time=28084.918536, global_step=83478, preemption_count=0, score=28084.918536, test/accuracy=0.505600, test/loss=2.229276, test/num_examples=10000, total_duration=29094.119474, train/accuracy=0.681421, train/loss=1.253029, validation/accuracy=0.628220, validation/loss=1.520606, validation/num_examples=50000
I0128 21:51:30.013360 140005313861376 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.401878356933594, loss=1.7701983451843262
I0128 21:52:03.519179 140004616554240 logging_writer.py:48] [83600] global_step=83600, grad_norm=4.308335304260254, loss=1.7436189651489258
I0128 21:52:37.069263 140005313861376 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.9713428020477295, loss=1.675010085105896
I0128 21:53:10.715126 140004616554240 logging_writer.py:48] [83800] global_step=83800, grad_norm=4.1758952140808105, loss=1.7894536256790161
I0128 21:53:44.273373 140005313861376 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.67587423324585, loss=1.7952816486358643
I0128 21:54:17.794058 140004616554240 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.9957563877105713, loss=1.6772853136062622
I0128 21:54:51.336633 140005313861376 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.288950443267822, loss=1.653344988822937
I0128 21:55:24.840964 140004616554240 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.9991366863250732, loss=1.7364850044250488
I0128 21:55:58.377357 140005313861376 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.234063625335693, loss=1.7011127471923828
I0128 21:56:31.933725 140004616554240 logging_writer.py:48] [84400] global_step=84400, grad_norm=5.039123058319092, loss=1.7269963026046753
I0128 21:57:05.529306 140005313861376 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.9423789978027344, loss=1.708672046661377
I0128 21:57:39.145449 140004616554240 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.414906978607178, loss=1.7891286611557007
I0128 21:58:12.758149 140005313861376 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.050232887268066, loss=1.7193318605422974
I0128 21:58:46.380550 140004616554240 logging_writer.py:48] [84800] global_step=84800, grad_norm=4.124745845794678, loss=1.834607720375061
I0128 21:59:20.010549 140005313861376 logging_writer.py:48] [84900] global_step=84900, grad_norm=4.9267144203186035, loss=1.743393898010254
I0128 21:59:52.417607 140169137129280 spec.py:321] Evaluating on the training split.
I0128 21:59:58.797518 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:00:07.955182 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:00:10.584517 140169137129280 submission_runner.py:408] Time since start: 29622.43s, 	Step: 84998, 	{'train/accuracy': 0.7023875713348389, 'train/loss': 1.1722116470336914, 'validation/accuracy': 0.6421599984169006, 'validation/loss': 1.463862419128418, 'validation/num_examples': 50000, 'test/accuracy': 0.5214000344276428, 'test/loss': 2.1729736328125, 'test/num_examples': 10000, 'score': 28594.972878456116, 'total_duration': 29622.433605909348, 'accumulated_submission_time': 28594.972878456116, 'accumulated_eval_time': 1022.558931350708, 'accumulated_logging_time': 2.1451663970947266}
I0128 22:00:10.619993 140004624946944 logging_writer.py:48] [84998] accumulated_eval_time=1022.558931, accumulated_logging_time=2.145166, accumulated_submission_time=28594.972878, global_step=84998, preemption_count=0, score=28594.972878, test/accuracy=0.521400, test/loss=2.172974, test/num_examples=10000, total_duration=29622.433606, train/accuracy=0.702388, train/loss=1.172212, validation/accuracy=0.642160, validation/loss=1.463862, validation/num_examples=50000
I0128 22:00:11.636496 140005288683264 logging_writer.py:48] [85000] global_step=85000, grad_norm=4.619613170623779, loss=1.7440882921218872
I0128 22:00:45.124370 140004624946944 logging_writer.py:48] [85100] global_step=85100, grad_norm=4.554712772369385, loss=1.6924957036972046
I0128 22:01:19.043698 140005288683264 logging_writer.py:48] [85200] global_step=85200, grad_norm=4.385805606842041, loss=1.6861517429351807
I0128 22:01:52.558093 140004624946944 logging_writer.py:48] [85300] global_step=85300, grad_norm=4.3732171058654785, loss=1.6299798488616943
I0128 22:02:26.121054 140005288683264 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.358310222625732, loss=1.6828995943069458
I0128 22:02:59.650654 140004624946944 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.862308979034424, loss=1.7679564952850342
I0128 22:03:33.224035 140005288683264 logging_writer.py:48] [85600] global_step=85600, grad_norm=4.310859203338623, loss=1.631857991218567
I0128 22:04:06.738984 140004624946944 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.159386157989502, loss=1.7340773344039917
I0128 22:04:40.270678 140005288683264 logging_writer.py:48] [85800] global_step=85800, grad_norm=4.429870128631592, loss=1.6744601726531982
I0128 22:05:13.821277 140004624946944 logging_writer.py:48] [85900] global_step=85900, grad_norm=5.253299236297607, loss=1.6681768894195557
I0128 22:05:47.450974 140005288683264 logging_writer.py:48] [86000] global_step=86000, grad_norm=4.609199523925781, loss=1.6804989576339722
I0128 22:06:20.998766 140004624946944 logging_writer.py:48] [86100] global_step=86100, grad_norm=4.726867198944092, loss=1.6440004110336304
I0128 22:06:54.508796 140005288683264 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.972757339477539, loss=1.6521174907684326
I0128 22:07:28.048032 140004624946944 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.814153671264648, loss=1.7247662544250488
I0128 22:08:01.653542 140005288683264 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.404886722564697, loss=1.729828953742981
I0128 22:08:35.255863 140004624946944 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.140141487121582, loss=1.812525987625122
I0128 22:08:40.786571 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:08:47.161815 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:08:55.860676 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:08:58.502513 140169137129280 submission_runner.py:408] Time since start: 30150.35s, 	Step: 86518, 	{'train/accuracy': 0.7252471446990967, 'train/loss': 1.0614542961120605, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.429309368133545, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.1262402534484863, 'test/num_examples': 10000, 'score': 29105.079597473145, 'total_duration': 30150.35160303116, 'accumulated_submission_time': 29105.079597473145, 'accumulated_eval_time': 1040.2748339176178, 'accumulated_logging_time': 2.191434383392334}
I0128 22:08:58.540137 140004616554240 logging_writer.py:48] [86518] accumulated_eval_time=1040.274834, accumulated_logging_time=2.191434, accumulated_submission_time=29105.079597, global_step=86518, preemption_count=0, score=29105.079597, test/accuracy=0.519500, test/loss=2.126240, test/num_examples=10000, total_duration=30150.351603, train/accuracy=0.725247, train/loss=1.061454, validation/accuracy=0.648480, validation/loss=1.429309, validation/num_examples=50000
I0128 22:09:26.325436 140004624946944 logging_writer.py:48] [86600] global_step=86600, grad_norm=4.261296272277832, loss=1.7011799812316895
I0128 22:09:59.890648 140004616554240 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.414045333862305, loss=1.6698591709136963
I0128 22:10:33.484908 140004624946944 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.577652454376221, loss=1.807104229927063
I0128 22:11:07.104119 140004616554240 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.988288164138794, loss=1.6696374416351318
I0128 22:11:40.772596 140004624946944 logging_writer.py:48] [87000] global_step=87000, grad_norm=5.138389587402344, loss=1.7502529621124268
I0128 22:12:14.339647 140004616554240 logging_writer.py:48] [87100] global_step=87100, grad_norm=4.182621479034424, loss=1.6413990259170532
I0128 22:12:47.940931 140004624946944 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.966127872467041, loss=1.6505064964294434
I0128 22:13:21.542560 140004616554240 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.9009034633636475, loss=1.7686742544174194
I0128 22:13:55.166374 140004624946944 logging_writer.py:48] [87400] global_step=87400, grad_norm=4.429651737213135, loss=1.7024588584899902
I0128 22:14:28.789669 140004616554240 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.279642581939697, loss=1.6973861455917358
I0128 22:15:02.395714 140004624946944 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.185544013977051, loss=1.7434546947479248
I0128 22:15:36.020434 140004616554240 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.518136024475098, loss=1.63349449634552
I0128 22:16:09.632612 140004624946944 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.837216854095459, loss=1.6492996215820312
I0128 22:16:43.257247 140004616554240 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.309947967529297, loss=1.716330647468567
I0128 22:17:16.870181 140004624946944 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.024068832397461, loss=1.6744657754898071
I0128 22:17:28.778798 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:17:35.148661 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:17:44.001489 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:17:46.606247 140169137129280 submission_runner.py:408] Time since start: 30678.46s, 	Step: 88037, 	{'train/accuracy': 0.7028061151504517, 'train/loss': 1.1444787979125977, 'validation/accuracy': 0.6430799961090088, 'validation/loss': 1.4514235258102417, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1785125732421875, 'test/num_examples': 10000, 'score': 29615.256961107254, 'total_duration': 30678.455330610275, 'accumulated_submission_time': 29615.256961107254, 'accumulated_eval_time': 1058.1022355556488, 'accumulated_logging_time': 2.2411510944366455}
I0128 22:17:46.641281 140004624946944 logging_writer.py:48] [88037] accumulated_eval_time=1058.102236, accumulated_logging_time=2.241151, accumulated_submission_time=29615.256961, global_step=88037, preemption_count=0, score=29615.256961, test/accuracy=0.518500, test/loss=2.178513, test/num_examples=10000, total_duration=30678.455331, train/accuracy=0.702806, train/loss=1.144479, validation/accuracy=0.643080, validation/loss=1.451424, validation/num_examples=50000
I0128 22:18:08.167627 140005305468672 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.8756911754608154, loss=1.5914148092269897
I0128 22:18:41.698577 140004624946944 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.094457149505615, loss=1.7026289701461792
I0128 22:19:15.314406 140005305468672 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.7400922775268555, loss=1.6784803867340088
I0128 22:19:48.881591 140004624946944 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.169649124145508, loss=1.6226725578308105
I0128 22:20:22.397807 140005305468672 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.676279067993164, loss=1.6552059650421143
I0128 22:20:55.931628 140004624946944 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.8232314586639404, loss=1.6251909732818604
I0128 22:21:29.534285 140005305468672 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.8670854568481445, loss=1.71194326877594
I0128 22:22:03.068047 140004624946944 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.082619667053223, loss=1.5527442693710327
I0128 22:22:36.564216 140005305468672 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.9968602657318115, loss=1.6213113069534302
I0128 22:23:10.147803 140004624946944 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.652013778686523, loss=1.6820297241210938
I0128 22:23:43.762795 140005305468672 logging_writer.py:48] [89100] global_step=89100, grad_norm=4.141622543334961, loss=1.6595039367675781
I0128 22:24:17.437290 140004624946944 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.9166040420532227, loss=1.6871147155761719
I0128 22:24:50.964114 140005305468672 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.9937846660614014, loss=1.6210949420928955
I0128 22:25:24.570693 140004624946944 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.9992613792419434, loss=1.604596495628357
I0128 22:25:58.181277 140005305468672 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.821098566055298, loss=1.4878489971160889
I0128 22:26:16.806459 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:26:23.165319 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:26:32.105378 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:26:34.834329 140169137129280 submission_runner.py:408] Time since start: 31206.68s, 	Step: 89557, 	{'train/accuracy': 0.7121930718421936, 'train/loss': 1.1185708045959473, 'validation/accuracy': 0.6528599858283997, 'validation/loss': 1.4161499738693237, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.140740156173706, 'test/num_examples': 10000, 'score': 30125.36184811592, 'total_duration': 31206.683420419693, 'accumulated_submission_time': 30125.36184811592, 'accumulated_eval_time': 1076.1300678253174, 'accumulated_logging_time': 2.2871439456939697}
I0128 22:26:34.877977 140005297075968 logging_writer.py:48] [89557] accumulated_eval_time=1076.130068, accumulated_logging_time=2.287144, accumulated_submission_time=30125.361848, global_step=89557, preemption_count=0, score=30125.361848, test/accuracy=0.526500, test/loss=2.140740, test/num_examples=10000, total_duration=31206.683420, train/accuracy=0.712193, train/loss=1.118571, validation/accuracy=0.652860, validation/loss=1.416150, validation/num_examples=50000
I0128 22:26:49.600687 140005322254080 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.9869210720062256, loss=1.7052116394042969
I0128 22:27:23.131898 140005297075968 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.954068183898926, loss=1.8162062168121338
I0128 22:27:56.636212 140005322254080 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.23705530166626, loss=1.6958563327789307
I0128 22:28:30.181647 140005297075968 logging_writer.py:48] [89900] global_step=89900, grad_norm=4.350932598114014, loss=1.6761139631271362
I0128 22:29:03.777864 140005322254080 logging_writer.py:48] [90000] global_step=90000, grad_norm=4.227187633514404, loss=1.6208112239837646
I0128 22:29:37.374709 140005297075968 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.795468807220459, loss=1.714448094367981
I0128 22:30:10.985568 140005322254080 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.872208595275879, loss=1.659752368927002
I0128 22:30:44.653523 140005297075968 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.28892707824707, loss=1.7086148262023926
I0128 22:31:18.200463 140005322254080 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.2714314460754395, loss=1.740835189819336
I0128 22:31:51.755232 140005297075968 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.094778537750244, loss=1.715787410736084
I0128 22:32:25.291965 140005322254080 logging_writer.py:48] [90600] global_step=90600, grad_norm=5.0681633949279785, loss=1.6256697177886963
I0128 22:32:58.794502 140005297075968 logging_writer.py:48] [90700] global_step=90700, grad_norm=4.354099273681641, loss=1.551332950592041
I0128 22:33:32.363474 140005322254080 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.650166034698486, loss=1.6730525493621826
I0128 22:34:05.928324 140005297075968 logging_writer.py:48] [90900] global_step=90900, grad_norm=4.110905647277832, loss=1.5573017597198486
I0128 22:34:39.449166 140005322254080 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.046448707580566, loss=1.6791954040527344
I0128 22:35:05.051635 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:35:11.462429 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:35:20.246520 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:35:22.880512 140169137129280 submission_runner.py:408] Time since start: 31734.73s, 	Step: 91078, 	{'train/accuracy': 0.7091438174247742, 'train/loss': 1.1337120532989502, 'validation/accuracy': 0.6495800018310547, 'validation/loss': 1.4375072717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.142979145050049, 'test/num_examples': 10000, 'score': 30635.47507429123, 'total_duration': 31734.729600191116, 'accumulated_submission_time': 30635.47507429123, 'accumulated_eval_time': 1093.9589052200317, 'accumulated_logging_time': 2.341646194458008}
I0128 22:35:22.917714 140004608161536 logging_writer.py:48] [91078] accumulated_eval_time=1093.958905, accumulated_logging_time=2.341646, accumulated_submission_time=30635.475074, global_step=91078, preemption_count=0, score=30635.475074, test/accuracy=0.521000, test/loss=2.142979, test/num_examples=10000, total_duration=31734.729600, train/accuracy=0.709144, train/loss=1.133712, validation/accuracy=0.649580, validation/loss=1.437507, validation/num_examples=50000
I0128 22:35:30.620401 140005288683264 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.310342311859131, loss=1.723414421081543
I0128 22:36:04.088079 140004608161536 logging_writer.py:48] [91200] global_step=91200, grad_norm=5.099742889404297, loss=1.7452154159545898
I0128 22:36:37.599416 140005288683264 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.550382614135742, loss=1.7055983543395996
I0128 22:37:11.224481 140004608161536 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.9219138622283936, loss=1.5672650337219238
I0128 22:37:44.827600 140005288683264 logging_writer.py:48] [91500] global_step=91500, grad_norm=4.107525825500488, loss=1.7364755868911743
I0128 22:38:18.446677 140004608161536 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.8692572116851807, loss=1.7455596923828125
I0128 22:38:52.029379 140005288683264 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.596436500549316, loss=1.6549334526062012
I0128 22:39:25.535183 140004608161536 logging_writer.py:48] [91800] global_step=91800, grad_norm=5.081369876861572, loss=1.6797139644622803
I0128 22:39:59.078353 140005288683264 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.766800880432129, loss=1.6644991636276245
I0128 22:40:32.644998 140004608161536 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.28086519241333, loss=1.6953685283660889
I0128 22:41:06.149358 140005288683264 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.391095161437988, loss=1.7276015281677246
I0128 22:41:39.676974 140004608161536 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.287134170532227, loss=1.7154548168182373
I0128 22:42:13.277689 140005288683264 logging_writer.py:48] [92300] global_step=92300, grad_norm=4.825530529022217, loss=1.6205410957336426
I0128 22:42:46.887479 140004608161536 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.265098571777344, loss=1.6668598651885986
I0128 22:43:20.542779 140005288683264 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.350601673126221, loss=1.6246789693832397
I0128 22:43:52.902019 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:43:59.352218 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:44:07.993057 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:44:10.649608 140169137129280 submission_runner.py:408] Time since start: 32262.50s, 	Step: 92598, 	{'train/accuracy': 0.7068518400192261, 'train/loss': 1.1458536386489868, 'validation/accuracy': 0.6547399759292603, 'validation/loss': 1.4125068187713623, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1197195053100586, 'test/num_examples': 10000, 'score': 31145.398854970932, 'total_duration': 32262.498690605164, 'accumulated_submission_time': 31145.398854970932, 'accumulated_eval_time': 1111.7064609527588, 'accumulated_logging_time': 2.390122890472412}
I0128 22:44:10.686151 140005322254080 logging_writer.py:48] [92598] accumulated_eval_time=1111.706461, accumulated_logging_time=2.390123, accumulated_submission_time=31145.398855, global_step=92598, preemption_count=0, score=31145.398855, test/accuracy=0.523800, test/loss=2.119720, test/num_examples=10000, total_duration=32262.498691, train/accuracy=0.706852, train/loss=1.145854, validation/accuracy=0.654740, validation/loss=1.412507, validation/num_examples=50000
I0128 22:44:11.706844 140005330646784 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.111240386962891, loss=1.6001554727554321
I0128 22:44:45.216722 140005322254080 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.07821798324585, loss=1.6840487718582153
I0128 22:45:18.717919 140005330646784 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.2501959800720215, loss=1.6256126165390015
I0128 22:45:52.323668 140005322254080 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.567244052886963, loss=1.5834496021270752
I0128 22:46:25.871184 140005330646784 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.344425201416016, loss=1.6560368537902832
I0128 22:46:59.393997 140005322254080 logging_writer.py:48] [93100] global_step=93100, grad_norm=3.900111198425293, loss=1.670185923576355
I0128 22:47:32.923592 140005330646784 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.255130290985107, loss=1.564029574394226
I0128 22:48:06.530286 140005322254080 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.322963714599609, loss=1.702518343925476
I0128 22:48:40.092836 140005330646784 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.860142946243286, loss=1.5392216444015503
I0128 22:49:13.597316 140005322254080 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.421257019042969, loss=1.6437876224517822
I0128 22:49:47.202825 140005330646784 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.903605937957764, loss=1.5962759256362915
I0128 22:50:20.730200 140005322254080 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.9515774250030518, loss=1.7101842164993286
I0128 22:50:54.240006 140005330646784 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.614161014556885, loss=1.6166309118270874
I0128 22:51:27.814936 140005322254080 logging_writer.py:48] [93900] global_step=93900, grad_norm=4.57975435256958, loss=1.554263710975647
I0128 22:52:01.356014 140005330646784 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.3634562492370605, loss=1.6440579891204834
I0128 22:52:34.875859 140005322254080 logging_writer.py:48] [94100] global_step=94100, grad_norm=4.646974086761475, loss=1.6753193140029907
I0128 22:52:40.722756 140169137129280 spec.py:321] Evaluating on the training split.
I0128 22:52:47.078432 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 22:52:55.915574 140169137129280 spec.py:349] Evaluating on the test split.
I0128 22:52:58.526876 140169137129280 submission_runner.py:408] Time since start: 32790.38s, 	Step: 94119, 	{'train/accuracy': 0.7403738498687744, 'train/loss': 1.0028444528579712, 'validation/accuracy': 0.6489399671554565, 'validation/loss': 1.430841326713562, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1304824352264404, 'test/num_examples': 10000, 'score': 31655.375497341156, 'total_duration': 32790.37596988678, 'accumulated_submission_time': 31655.375497341156, 'accumulated_eval_time': 1129.5105464458466, 'accumulated_logging_time': 2.4372799396514893}
I0128 22:52:58.560885 140004624946944 logging_writer.py:48] [94119] accumulated_eval_time=1129.510546, accumulated_logging_time=2.437280, accumulated_submission_time=31655.375497, global_step=94119, preemption_count=0, score=31655.375497, test/accuracy=0.518800, test/loss=2.130482, test/num_examples=10000, total_duration=32790.375970, train/accuracy=0.740374, train/loss=1.002844, validation/accuracy=0.648940, validation/loss=1.430841, validation/num_examples=50000
I0128 22:53:26.005091 140005288683264 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.00508975982666, loss=1.6263965368270874
I0128 22:53:59.543826 140004624946944 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.375169277191162, loss=1.6847091913223267
I0128 22:54:33.151387 140005288683264 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.287655353546143, loss=1.6371725797653198
I0128 22:55:06.771105 140004624946944 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.314537525177002, loss=1.6312260627746582
I0128 22:55:40.372426 140005288683264 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.997316360473633, loss=1.6548758745193481
I0128 22:56:13.993156 140004624946944 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.192212104797363, loss=1.5514731407165527
I0128 22:56:47.600677 140005288683264 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.326305389404297, loss=1.5634222030639648
I0128 22:57:21.204055 140004624946944 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.292208194732666, loss=1.7149934768676758
I0128 22:57:54.763638 140005288683264 logging_writer.py:48] [95000] global_step=95000, grad_norm=5.64267635345459, loss=1.6948949098587036
I0128 22:58:28.267485 140004624946944 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.1524977684021, loss=1.6576299667358398
I0128 22:59:01.829986 140005288683264 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.512720584869385, loss=1.7351727485656738
I0128 22:59:35.437416 140004624946944 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.00315523147583, loss=1.5667178630828857
I0128 23:00:08.995025 140005288683264 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.526830673217773, loss=1.7104655504226685
I0128 23:00:42.494019 140004624946944 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.346570014953613, loss=1.6920639276504517
I0128 23:01:16.057862 140005288683264 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.756908893585205, loss=1.6774249076843262
I0128 23:01:28.626660 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:01:34.941662 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:01:43.753844 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:01:46.369428 140169137129280 submission_runner.py:408] Time since start: 33318.22s, 	Step: 95639, 	{'train/accuracy': 0.7263033986091614, 'train/loss': 1.055260419845581, 'validation/accuracy': 0.6544399857521057, 'validation/loss': 1.4071980714797974, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.102889060974121, 'test/num_examples': 10000, 'score': 32165.38191127777, 'total_duration': 33318.218418598175, 'accumulated_submission_time': 32165.38191127777, 'accumulated_eval_time': 1147.2531747817993, 'accumulated_logging_time': 2.4813926219940186}
I0128 23:01:46.405483 140004608161536 logging_writer.py:48] [95639] accumulated_eval_time=1147.253175, accumulated_logging_time=2.481393, accumulated_submission_time=32165.381911, global_step=95639, preemption_count=0, score=32165.381911, test/accuracy=0.531100, test/loss=2.102889, test/num_examples=10000, total_duration=33318.218419, train/accuracy=0.726303, train/loss=1.055260, validation/accuracy=0.654440, validation/loss=1.407198, validation/num_examples=50000
I0128 23:02:07.232906 140004616554240 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.439733505249023, loss=1.646180510520935
I0128 23:02:40.700010 140004608161536 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.9768707752227783, loss=1.5537738800048828
I0128 23:03:14.247463 140004616554240 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.990384578704834, loss=1.67128324508667
I0128 23:03:47.842532 140004608161536 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.340566158294678, loss=1.6010618209838867
I0128 23:04:21.432390 140004616554240 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.916109800338745, loss=1.692777156829834
I0128 23:04:54.946011 140004608161536 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.444534778594971, loss=1.6934900283813477
I0128 23:05:28.471134 140004616554240 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.446117401123047, loss=1.7008213996887207
I0128 23:06:02.069654 140004608161536 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.133439540863037, loss=1.4957597255706787
I0128 23:06:35.666556 140004616554240 logging_writer.py:48] [96500] global_step=96500, grad_norm=5.058735370635986, loss=1.700074553489685
I0128 23:07:09.292890 140004608161536 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.9840638637542725, loss=1.67268705368042
I0128 23:07:42.891169 140004616554240 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.490846633911133, loss=1.5846673250198364
I0128 23:08:16.518430 140004608161536 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.432530403137207, loss=1.5864945650100708
I0128 23:08:50.148142 140004616554240 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.509270191192627, loss=1.6537591218948364
I0128 23:09:23.707908 140004608161536 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.401057243347168, loss=1.6987813711166382
I0128 23:09:57.212755 140004616554240 logging_writer.py:48] [97100] global_step=97100, grad_norm=4.3528242111206055, loss=1.6518199443817139
I0128 23:10:16.473681 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:10:22.958507 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:10:31.770817 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:10:34.444110 140169137129280 submission_runner.py:408] Time since start: 33846.29s, 	Step: 97159, 	{'train/accuracy': 0.7153021097183228, 'train/loss': 1.094274640083313, 'validation/accuracy': 0.647059977054596, 'validation/loss': 1.4412496089935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.1374266147613525, 'test/num_examples': 10000, 'score': 32675.390652418137, 'total_duration': 33846.29317569733, 'accumulated_submission_time': 32675.390652418137, 'accumulated_eval_time': 1165.2235417366028, 'accumulated_logging_time': 2.5274858474731445}
I0128 23:10:34.486681 140004616554240 logging_writer.py:48] [97159] accumulated_eval_time=1165.223542, accumulated_logging_time=2.527486, accumulated_submission_time=32675.390652, global_step=97159, preemption_count=0, score=32675.390652, test/accuracy=0.524700, test/loss=2.137427, test/num_examples=10000, total_duration=33846.293176, train/accuracy=0.715302, train/loss=1.094275, validation/accuracy=0.647060, validation/loss=1.441250, validation/num_examples=50000
I0128 23:10:48.565560 140005313861376 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.523419380187988, loss=1.6821132898330688
I0128 23:11:22.036548 140004616554240 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.352210521697998, loss=1.6649712324142456
I0128 23:11:55.578838 140005313861376 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.17582368850708, loss=1.5503311157226562
I0128 23:12:29.144622 140004616554240 logging_writer.py:48] [97500] global_step=97500, grad_norm=4.292673587799072, loss=1.5154062509536743
I0128 23:13:02.640108 140005313861376 logging_writer.py:48] [97600] global_step=97600, grad_norm=5.349942684173584, loss=1.6780427694320679
I0128 23:13:36.173781 140004616554240 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.957024574279785, loss=1.650054693222046
I0128 23:14:09.746386 140005313861376 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.037559986114502, loss=1.58982515335083
I0128 23:14:43.441419 140004616554240 logging_writer.py:48] [97900] global_step=97900, grad_norm=5.092685699462891, loss=1.5871272087097168
I0128 23:15:16.969765 140005313861376 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.153376579284668, loss=1.5547711849212646
I0128 23:15:50.578224 140004616554240 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.7018637657165527, loss=1.5554888248443604
I0128 23:16:24.195809 140005313861376 logging_writer.py:48] [98200] global_step=98200, grad_norm=5.012795925140381, loss=1.6305112838745117
I0128 23:16:57.822297 140004616554240 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.990362167358398, loss=1.6974220275878906
I0128 23:17:31.429328 140005313861376 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.225019454956055, loss=1.5493149757385254
I0128 23:18:04.967681 140004616554240 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.582785129547119, loss=1.606270670890808
I0128 23:18:38.466423 140005313861376 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.718408107757568, loss=1.5475364923477173
I0128 23:19:04.465626 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:19:10.851900 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:19:19.743654 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:19:22.391064 140169137129280 submission_runner.py:408] Time since start: 34374.24s, 	Step: 98679, 	{'train/accuracy': 0.7119140625, 'train/loss': 1.1197112798690796, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.4109952449798584, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1488847732543945, 'test/num_examples': 10000, 'score': 33185.30989527702, 'total_duration': 34374.24014735222, 'accumulated_submission_time': 33185.30989527702, 'accumulated_eval_time': 1183.1489372253418, 'accumulated_logging_time': 2.5802924633026123}
I0128 23:19:22.425791 140005288683264 logging_writer.py:48] [98679] accumulated_eval_time=1183.148937, accumulated_logging_time=2.580292, accumulated_submission_time=33185.309895, global_step=98679, preemption_count=0, score=33185.309895, test/accuracy=0.522900, test/loss=2.148885, test/num_examples=10000, total_duration=34374.240147, train/accuracy=0.711914, train/loss=1.119711, validation/accuracy=0.654080, validation/loss=1.410995, validation/num_examples=50000
I0128 23:19:29.823009 140005297075968 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.661497592926025, loss=1.501192569732666
I0128 23:20:03.306591 140005288683264 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.3531999588012695, loss=1.5781641006469727
I0128 23:20:36.874017 140005297075968 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.965194225311279, loss=1.6574878692626953
I0128 23:21:10.498019 140005288683264 logging_writer.py:48] [99000] global_step=99000, grad_norm=5.853819847106934, loss=1.5906786918640137
I0128 23:21:44.088572 140005297075968 logging_writer.py:48] [99100] global_step=99100, grad_norm=5.047792434692383, loss=1.6550043821334839
I0128 23:22:17.635214 140005288683264 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.1845316886901855, loss=1.6370611190795898
I0128 23:22:51.144796 140005297075968 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.379029750823975, loss=1.531083345413208
I0128 23:23:24.707848 140005288683264 logging_writer.py:48] [99400] global_step=99400, grad_norm=4.82071590423584, loss=1.5996266603469849
I0128 23:23:58.315388 140005297075968 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.516000747680664, loss=1.7668225765228271
I0128 23:24:31.936629 140005288683264 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.644123077392578, loss=1.6810635328292847
I0128 23:25:05.554281 140005297075968 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.597859859466553, loss=1.746164321899414
I0128 23:25:39.168832 140005288683264 logging_writer.py:48] [99800] global_step=99800, grad_norm=4.316554546356201, loss=1.4582345485687256
I0128 23:26:12.780798 140005297075968 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.79390287399292, loss=1.5743199586868286
I0128 23:26:46.346390 140005288683264 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.5934247970581055, loss=1.5505120754241943
I0128 23:27:19.930981 140005297075968 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.400203227996826, loss=1.6123080253601074
I0128 23:27:52.694540 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:27:59.104296 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:28:07.813750 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:28:10.459808 140169137129280 submission_runner.py:408] Time since start: 34902.31s, 	Step: 100199, 	{'train/accuracy': 0.7194873690605164, 'train/loss': 1.0825355052947998, 'validation/accuracy': 0.6628199815750122, 'validation/loss': 1.372917652130127, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.116262435913086, 'test/num_examples': 10000, 'score': 33695.51848888397, 'total_duration': 34902.30881524086, 'accumulated_submission_time': 33695.51848888397, 'accumulated_eval_time': 1200.9140849113464, 'accumulated_logging_time': 2.625534772872925}
I0128 23:28:10.498746 140005313861376 logging_writer.py:48] [100199] accumulated_eval_time=1200.914085, accumulated_logging_time=2.625535, accumulated_submission_time=33695.518489, global_step=100199, preemption_count=0, score=33695.518489, test/accuracy=0.534400, test/loss=2.116262, test/num_examples=10000, total_duration=34902.308815, train/accuracy=0.719487, train/loss=1.082536, validation/accuracy=0.662820, validation/loss=1.372918, validation/num_examples=50000
I0128 23:28:11.190490 140005322254080 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.861932754516602, loss=1.5947630405426025
I0128 23:28:44.744161 140005313861376 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.268246173858643, loss=1.6292097568511963
I0128 23:29:18.226255 140005322254080 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.184764385223389, loss=1.6814624071121216
I0128 23:29:51.816710 140005313861376 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.283505916595459, loss=1.6455793380737305
I0128 23:30:25.430415 140005322254080 logging_writer.py:48] [100600] global_step=100600, grad_norm=5.328192234039307, loss=1.5236719846725464
I0128 23:30:59.044598 140005313861376 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.106110095977783, loss=1.5600578784942627
I0128 23:31:32.655876 140005322254080 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.250882625579834, loss=1.4830238819122314
I0128 23:32:06.272219 140005313861376 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.439103126525879, loss=1.6708781719207764
I0128 23:32:39.887238 140005322254080 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.631909370422363, loss=1.5851445198059082
I0128 23:33:13.500503 140005313861376 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.233585357666016, loss=1.5158421993255615
I0128 23:33:47.156807 140005322254080 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.159152984619141, loss=1.4434670209884644
I0128 23:34:20.707012 140005313861376 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.282889366149902, loss=1.5458416938781738
I0128 23:34:54.288097 140005322254080 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.615173816680908, loss=1.5222982168197632
I0128 23:35:27.890536 140005313861376 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.707587718963623, loss=1.6048232316970825
I0128 23:36:01.459655 140005322254080 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.350997447967529, loss=1.5698341131210327
I0128 23:36:35.073042 140005313861376 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.676502704620361, loss=1.6174466609954834
I0128 23:36:40.600894 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:36:47.122817 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:36:55.915266 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:36:58.581630 140169137129280 submission_runner.py:408] Time since start: 35430.43s, 	Step: 101718, 	{'train/accuracy': 0.7230349183082581, 'train/loss': 1.0770888328552246, 'validation/accuracy': 0.6646599769592285, 'validation/loss': 1.3627052307128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.0898184776306152, 'test/num_examples': 10000, 'score': 34205.558450460434, 'total_duration': 35430.43071985245, 'accumulated_submission_time': 34205.558450460434, 'accumulated_eval_time': 1218.8947851657867, 'accumulated_logging_time': 2.6776158809661865}
I0128 23:36:58.619360 140005288683264 logging_writer.py:48] [101718] accumulated_eval_time=1218.894785, accumulated_logging_time=2.677616, accumulated_submission_time=34205.558450, global_step=101718, preemption_count=0, score=34205.558450, test/accuracy=0.534800, test/loss=2.089818, test/num_examples=10000, total_duration=35430.430720, train/accuracy=0.723035, train/loss=1.077089, validation/accuracy=0.664660, validation/loss=1.362705, validation/num_examples=50000
I0128 23:37:26.461443 140005297075968 logging_writer.py:48] [101800] global_step=101800, grad_norm=5.364622116088867, loss=1.6080422401428223
I0128 23:37:59.966316 140005288683264 logging_writer.py:48] [101900] global_step=101900, grad_norm=4.877804756164551, loss=1.6911877393722534
I0128 23:38:33.487890 140005297075968 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.995019435882568, loss=1.562110185623169
I0128 23:39:07.055454 140005288683264 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.369626045227051, loss=1.6141295433044434
I0128 23:39:40.577784 140005297075968 logging_writer.py:48] [102200] global_step=102200, grad_norm=5.067763805389404, loss=1.5773110389709473
I0128 23:40:14.206440 140005288683264 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.980502605438232, loss=1.675338625907898
I0128 23:40:47.808938 140005297075968 logging_writer.py:48] [102400] global_step=102400, grad_norm=5.055211067199707, loss=1.500798225402832
I0128 23:41:21.386868 140005288683264 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.63965368270874, loss=1.661039113998413
I0128 23:41:54.872917 140005297075968 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.524176120758057, loss=1.5463677644729614
I0128 23:42:28.449336 140005288683264 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.416911602020264, loss=1.5247215032577515
I0128 23:43:02.049998 140005297075968 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.658609867095947, loss=1.661468267440796
I0128 23:43:35.640524 140005288683264 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.705929279327393, loss=1.5527377128601074
I0128 23:44:09.262179 140005297075968 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.501761436462402, loss=1.7214454412460327
I0128 23:44:42.834293 140005288683264 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.324928283691406, loss=1.6071616411209106
I0128 23:45:16.367091 140005297075968 logging_writer.py:48] [103200] global_step=103200, grad_norm=5.279138088226318, loss=1.6091573238372803
I0128 23:45:28.900001 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:45:35.383941 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:45:44.251374 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:45:46.896145 140169137129280 submission_runner.py:408] Time since start: 35958.75s, 	Step: 103239, 	{'train/accuracy': 0.7520726919174194, 'train/loss': 0.9424518346786499, 'validation/accuracy': 0.662559986114502, 'validation/loss': 1.3745607137680054, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.0718114376068115, 'test/num_examples': 10000, 'score': 34715.77932167053, 'total_duration': 35958.745235681534, 'accumulated_submission_time': 34715.77932167053, 'accumulated_eval_time': 1236.8908894062042, 'accumulated_logging_time': 2.725346565246582}
I0128 23:45:46.933992 140005313861376 logging_writer.py:48] [103239] accumulated_eval_time=1236.890889, accumulated_logging_time=2.725347, accumulated_submission_time=34715.779322, global_step=103239, preemption_count=0, score=34715.779322, test/accuracy=0.537600, test/loss=2.071811, test/num_examples=10000, total_duration=35958.745236, train/accuracy=0.752073, train/loss=0.942452, validation/accuracy=0.662560, validation/loss=1.374561, validation/num_examples=50000
I0128 23:46:07.677755 140005322254080 logging_writer.py:48] [103300] global_step=103300, grad_norm=5.598372459411621, loss=1.534237027168274
I0128 23:46:41.237711 140005313861376 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.523629665374756, loss=1.5762851238250732
I0128 23:47:14.753324 140005322254080 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.5507588386535645, loss=1.4879237413406372
I0128 23:47:48.279081 140005313861376 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.724788188934326, loss=1.5104541778564453
I0128 23:48:21.831949 140005322254080 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.395076274871826, loss=1.4929059743881226
I0128 23:48:55.426783 140005313861376 logging_writer.py:48] [103800] global_step=103800, grad_norm=5.038675785064697, loss=1.5593888759613037
I0128 23:49:29.017161 140005322254080 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.672616481781006, loss=1.5537052154541016
I0128 23:50:02.637336 140005313861376 logging_writer.py:48] [104000] global_step=104000, grad_norm=5.045098304748535, loss=1.6289873123168945
I0128 23:50:36.260232 140005322254080 logging_writer.py:48] [104100] global_step=104100, grad_norm=5.4546074867248535, loss=1.587496280670166
I0128 23:51:09.855276 140005313861376 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.668295383453369, loss=1.4743528366088867
I0128 23:51:43.385390 140005322254080 logging_writer.py:48] [104300] global_step=104300, grad_norm=5.097286224365234, loss=1.668076992034912
I0128 23:52:16.894488 140005313861376 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.640681743621826, loss=1.5852221250534058
I0128 23:52:50.462624 140005322254080 logging_writer.py:48] [104500] global_step=104500, grad_norm=5.165117263793945, loss=1.5124660730361938
I0128 23:53:23.984419 140005313861376 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.6305060386657715, loss=1.4935740232467651
I0128 23:53:57.533101 140005322254080 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.254241943359375, loss=1.5307791233062744
I0128 23:54:17.153221 140169137129280 spec.py:321] Evaluating on the training split.
I0128 23:54:23.426886 140169137129280 spec.py:333] Evaluating on the validation split.
I0128 23:54:32.150760 140169137129280 spec.py:349] Evaluating on the test split.
I0128 23:54:34.790629 140169137129280 submission_runner.py:408] Time since start: 36486.64s, 	Step: 104760, 	{'train/accuracy': 0.7437419891357422, 'train/loss': 0.9715170860290527, 'validation/accuracy': 0.6689199805259705, 'validation/loss': 1.338512659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.064157247543335, 'test/num_examples': 10000, 'score': 35225.935331106186, 'total_duration': 36486.63972115517, 'accumulated_submission_time': 35225.935331106186, 'accumulated_eval_time': 1254.5282595157623, 'accumulated_logging_time': 2.777066469192505}
I0128 23:54:34.833276 140005297075968 logging_writer.py:48] [104760] accumulated_eval_time=1254.528260, accumulated_logging_time=2.777066, accumulated_submission_time=35225.935331, global_step=104760, preemption_count=0, score=35225.935331, test/accuracy=0.541700, test/loss=2.064157, test/num_examples=10000, total_duration=36486.639721, train/accuracy=0.743742, train/loss=0.971517, validation/accuracy=0.668920, validation/loss=1.338513, validation/num_examples=50000
I0128 23:54:48.576030 140005305468672 logging_writer.py:48] [104800] global_step=104800, grad_norm=5.158232688903809, loss=1.5269112586975098
I0128 23:55:22.023797 140005297075968 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.5168232917785645, loss=1.551489233970642
I0128 23:55:55.537830 140005305468672 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.0455427169799805, loss=1.5358370542526245
I0128 23:56:29.026104 140005297075968 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.537738800048828, loss=1.6423760652542114
I0128 23:57:02.618513 140005305468672 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.539975643157959, loss=1.501429796218872
I0128 23:57:36.210795 140005297075968 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.6168212890625, loss=1.5729732513427734
I0128 23:58:09.757395 140005305468672 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.78840446472168, loss=1.64475417137146
I0128 23:58:43.236151 140005297075968 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.881723403930664, loss=1.5285147428512573
I0128 23:59:16.842998 140005305468672 logging_writer.py:48] [105600] global_step=105600, grad_norm=5.446627616882324, loss=1.5158030986785889
I0128 23:59:50.377899 140005297075968 logging_writer.py:48] [105700] global_step=105700, grad_norm=5.728429794311523, loss=1.6332118511199951
I0129 00:00:23.936854 140005305468672 logging_writer.py:48] [105800] global_step=105800, grad_norm=5.11407470703125, loss=1.5188723802566528
I0129 00:00:57.519529 140005297075968 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.709097862243652, loss=1.54714035987854
I0129 00:01:31.129944 140005305468672 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.732259273529053, loss=1.4820802211761475
I0129 00:02:04.750545 140005297075968 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.682618141174316, loss=1.6166510581970215
I0129 00:02:38.347412 140005305468672 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.292586326599121, loss=1.508310079574585
I0129 00:03:05.058583 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:03:11.414584 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:03:20.036273 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:03:22.669481 140169137129280 submission_runner.py:408] Time since start: 37014.52s, 	Step: 106281, 	{'train/accuracy': 0.7352120280265808, 'train/loss': 1.0203943252563477, 'validation/accuracy': 0.6659799814224243, 'validation/loss': 1.3560991287231445, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.052748441696167, 'test/num_examples': 10000, 'score': 35736.099576711655, 'total_duration': 37014.51857161522, 'accumulated_submission_time': 35736.099576711655, 'accumulated_eval_time': 1272.1391229629517, 'accumulated_logging_time': 2.8311715126037598}
I0129 00:03:22.706330 140004616554240 logging_writer.py:48] [106281] accumulated_eval_time=1272.139123, accumulated_logging_time=2.831172, accumulated_submission_time=35736.099577, global_step=106281, preemption_count=0, score=35736.099577, test/accuracy=0.543000, test/loss=2.052748, test/num_examples=10000, total_duration=37014.518572, train/accuracy=0.735212, train/loss=1.020394, validation/accuracy=0.665980, validation/loss=1.356099, validation/num_examples=50000
I0129 00:03:29.416293 140004624946944 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.932643890380859, loss=1.5886112451553345
I0129 00:04:02.872760 140004616554240 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.525142192840576, loss=1.581254243850708
I0129 00:04:36.413856 140004624946944 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.897129535675049, loss=1.6347450017929077
I0129 00:05:09.916535 140004616554240 logging_writer.py:48] [106600] global_step=106600, grad_norm=5.116841793060303, loss=1.625776767730713
I0129 00:05:43.541723 140004624946944 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.970264911651611, loss=1.444396734237671
I0129 00:06:17.078259 140004616554240 logging_writer.py:48] [106800] global_step=106800, grad_norm=5.257535457611084, loss=1.5131675004959106
I0129 00:06:50.675166 140004624946944 logging_writer.py:48] [106900] global_step=106900, grad_norm=5.185982704162598, loss=1.606867790222168
I0129 00:07:24.198212 140004616554240 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.555708885192871, loss=1.445515751838684
I0129 00:07:57.709309 140004624946944 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.563985347747803, loss=1.4894397258758545
I0129 00:08:31.290709 140004616554240 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.4067511558532715, loss=1.508746862411499
I0129 00:09:04.876367 140004624946944 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.331491947174072, loss=1.5301387310028076
I0129 00:09:38.494240 140004616554240 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.622615337371826, loss=1.5927358865737915
I0129 00:10:12.104612 140004624946944 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.684986114501953, loss=1.5448085069656372
I0129 00:10:45.641162 140004616554240 logging_writer.py:48] [107600] global_step=107600, grad_norm=5.795269012451172, loss=1.6963390111923218
I0129 00:11:19.143432 140004624946944 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.6398468017578125, loss=1.5695469379425049
I0129 00:11:52.762462 140004616554240 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.682378768920898, loss=1.6261050701141357
I0129 00:11:52.770019 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:11:59.185001 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:12:08.117838 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:12:10.792975 140169137129280 submission_runner.py:408] Time since start: 37542.64s, 	Step: 107801, 	{'train/accuracy': 0.7411311864852905, 'train/loss': 0.9817953109741211, 'validation/accuracy': 0.675279974937439, 'validation/loss': 1.3169835805892944, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 1.9979313611984253, 'test/num_examples': 10000, 'score': 36246.10291814804, 'total_duration': 37542.64195537567, 'accumulated_submission_time': 36246.10291814804, 'accumulated_eval_time': 1290.1619033813477, 'accumulated_logging_time': 2.8791487216949463}
I0129 00:12:10.831596 140005297075968 logging_writer.py:48] [107801] accumulated_eval_time=1290.161903, accumulated_logging_time=2.879149, accumulated_submission_time=36246.102918, global_step=107801, preemption_count=0, score=36246.102918, test/accuracy=0.553900, test/loss=1.997931, test/num_examples=10000, total_duration=37542.641955, train/accuracy=0.741131, train/loss=0.981795, validation/accuracy=0.675280, validation/loss=1.316984, validation/num_examples=50000
I0129 00:12:44.357653 140005305468672 logging_writer.py:48] [107900] global_step=107900, grad_norm=5.131617069244385, loss=1.5412441492080688
I0129 00:13:17.913341 140005297075968 logging_writer.py:48] [108000] global_step=108000, grad_norm=5.112434387207031, loss=1.6146049499511719
I0129 00:13:51.460120 140005305468672 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.604800224304199, loss=1.5804343223571777
I0129 00:14:24.946360 140005297075968 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.49550724029541, loss=1.5222604274749756
I0129 00:14:58.530131 140005305468672 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.212691783905029, loss=1.436275839805603
I0129 00:15:32.119253 140005297075968 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.574397563934326, loss=1.5836639404296875
I0129 00:16:05.726119 140005305468672 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.948395252227783, loss=1.5315062999725342
I0129 00:16:39.346229 140005297075968 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.608865737915039, loss=1.4917060136795044
I0129 00:17:12.961930 140005305468672 logging_writer.py:48] [108700] global_step=108700, grad_norm=5.482742786407471, loss=1.5625659227371216
I0129 00:17:46.644200 140005297075968 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.568563461303711, loss=1.5450667142868042
I0129 00:18:20.206330 140005305468672 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.988223075866699, loss=1.6791132688522339
I0129 00:18:53.802614 140005297075968 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.129179000854492, loss=1.4958739280700684
I0129 00:19:27.340130 140005305468672 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.766851902008057, loss=1.49656343460083
I0129 00:20:00.845802 140005297075968 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.150736331939697, loss=1.4501161575317383
I0129 00:20:34.429433 140005305468672 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.72360372543335, loss=1.5314973592758179
I0129 00:20:40.959173 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:20:47.336459 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:20:56.247861 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:20:58.896050 140169137129280 submission_runner.py:408] Time since start: 38070.75s, 	Step: 109321, 	{'train/accuracy': 0.7330197691917419, 'train/loss': 1.0413142442703247, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.3357013463974, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.053678274154663, 'test/num_examples': 10000, 'score': 36756.17011857033, 'total_duration': 38070.74506354332, 'accumulated_submission_time': 36756.17011857033, 'accumulated_eval_time': 1308.0986626148224, 'accumulated_logging_time': 2.928966760635376}
I0129 00:20:58.934535 140004624946944 logging_writer.py:48] [109321] accumulated_eval_time=1308.098663, accumulated_logging_time=2.928967, accumulated_submission_time=36756.170119, global_step=109321, preemption_count=0, score=36756.170119, test/accuracy=0.540100, test/loss=2.053678, test/num_examples=10000, total_duration=38070.745064, train/accuracy=0.733020, train/loss=1.041314, validation/accuracy=0.670960, validation/loss=1.335701, validation/num_examples=50000
I0129 00:21:25.778886 140005288683264 logging_writer.py:48] [109400] global_step=109400, grad_norm=5.5769429206848145, loss=1.4594995975494385
I0129 00:21:59.238339 140004624946944 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.98371696472168, loss=1.5119786262512207
I0129 00:22:32.774034 140005288683264 logging_writer.py:48] [109600] global_step=109600, grad_norm=5.615820407867432, loss=1.5316740274429321
I0129 00:23:06.302278 140004624946944 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.849380016326904, loss=1.5960782766342163
I0129 00:23:39.808706 140005288683264 logging_writer.py:48] [109800] global_step=109800, grad_norm=5.409438133239746, loss=1.5672111511230469
I0129 00:24:13.433378 140004624946944 logging_writer.py:48] [109900] global_step=109900, grad_norm=5.450789451599121, loss=1.6888096332550049
I0129 00:24:46.946325 140005288683264 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.650570869445801, loss=1.636880874633789
I0129 00:25:20.529965 140004624946944 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.6891889572143555, loss=1.4588531255722046
I0129 00:25:54.135694 140005288683264 logging_writer.py:48] [110200] global_step=110200, grad_norm=5.588014125823975, loss=1.4350039958953857
I0129 00:26:27.654973 140004624946944 logging_writer.py:48] [110300] global_step=110300, grad_norm=5.100002288818359, loss=1.608718752861023
I0129 00:27:01.160330 140005288683264 logging_writer.py:48] [110400] global_step=110400, grad_norm=5.0295820236206055, loss=1.462664246559143
I0129 00:27:34.728329 140004624946944 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.44852876663208, loss=1.4202402830123901
I0129 00:28:08.299664 140005288683264 logging_writer.py:48] [110600] global_step=110600, grad_norm=7.290775775909424, loss=1.6398298740386963
I0129 00:28:41.901423 140004624946944 logging_writer.py:48] [110700] global_step=110700, grad_norm=5.074270725250244, loss=1.4428679943084717
I0129 00:29:15.490430 140005288683264 logging_writer.py:48] [110800] global_step=110800, grad_norm=5.313859462738037, loss=1.4072668552398682
I0129 00:29:29.079286 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:29:35.459324 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:29:44.127188 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:29:46.776758 140169137129280 submission_runner.py:408] Time since start: 38598.63s, 	Step: 110842, 	{'train/accuracy': 0.7404536008834839, 'train/loss': 0.9842280745506287, 'validation/accuracy': 0.675819993019104, 'validation/loss': 1.313747763633728, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.0273501873016357, 'test/num_examples': 10000, 'score': 37266.255848646164, 'total_duration': 38598.62584900856, 'accumulated_submission_time': 37266.255848646164, 'accumulated_eval_time': 1325.7961132526398, 'accumulated_logging_time': 2.9773285388946533}
I0129 00:29:46.816179 140005313861376 logging_writer.py:48] [110842] accumulated_eval_time=1325.796113, accumulated_logging_time=2.977329, accumulated_submission_time=37266.255849, global_step=110842, preemption_count=0, score=37266.255849, test/accuracy=0.550300, test/loss=2.027350, test/num_examples=10000, total_duration=38598.625849, train/accuracy=0.740454, train/loss=0.984228, validation/accuracy=0.675820, validation/loss=1.313748, validation/num_examples=50000
I0129 00:30:06.565732 140005322254080 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.835592269897461, loss=1.588756799697876
I0129 00:30:40.161008 140005313861376 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.919152736663818, loss=1.476597785949707
I0129 00:31:13.756032 140005322254080 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.58809757232666, loss=1.5042405128479004
I0129 00:31:47.307012 140005313861376 logging_writer.py:48] [111200] global_step=111200, grad_norm=5.261220455169678, loss=1.4339624643325806
I0129 00:32:20.788479 140005322254080 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.8504438400268555, loss=1.4802629947662354
I0129 00:32:54.377819 140005313861376 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.979278564453125, loss=1.5688161849975586
I0129 00:33:27.975726 140005322254080 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.82238245010376, loss=1.506511926651001
I0129 00:34:01.481598 140005313861376 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.987326145172119, loss=1.4760205745697021
I0129 00:34:34.984482 140005322254080 logging_writer.py:48] [111700] global_step=111700, grad_norm=5.001220226287842, loss=1.5674166679382324
I0129 00:35:08.553526 140005313861376 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.738729953765869, loss=1.555882453918457
I0129 00:35:42.137676 140005322254080 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.728987693786621, loss=1.4271941184997559
I0129 00:36:15.623863 140005313861376 logging_writer.py:48] [112000] global_step=112000, grad_norm=5.3300580978393555, loss=1.508057951927185
I0129 00:36:49.222521 140005322254080 logging_writer.py:48] [112100] global_step=112100, grad_norm=5.209841728210449, loss=1.6053423881530762
I0129 00:37:22.749386 140005313861376 logging_writer.py:48] [112200] global_step=112200, grad_norm=4.9426774978637695, loss=1.5937883853912354
I0129 00:37:56.286717 140005322254080 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.4185686111450195, loss=1.4550683498382568
I0129 00:38:16.865540 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:38:23.248185 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:38:31.802646 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:38:34.563261 140169137129280 submission_runner.py:408] Time since start: 39126.41s, 	Step: 112363, 	{'train/accuracy': 0.7700294852256775, 'train/loss': 0.8577762246131897, 'validation/accuracy': 0.6807999610900879, 'validation/loss': 1.2912861108779907, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.9689432382583618, 'test/num_examples': 10000, 'score': 37776.244634628296, 'total_duration': 39126.41235136986, 'accumulated_submission_time': 37776.244634628296, 'accumulated_eval_time': 1343.4937970638275, 'accumulated_logging_time': 3.02756404876709}
I0129 00:38:34.599847 140005288683264 logging_writer.py:48] [112363] accumulated_eval_time=1343.493797, accumulated_logging_time=3.027564, accumulated_submission_time=37776.244635, global_step=112363, preemption_count=0, score=37776.244635, test/accuracy=0.554400, test/loss=1.968943, test/num_examples=10000, total_duration=39126.412351, train/accuracy=0.770029, train/loss=0.857776, validation/accuracy=0.680800, validation/loss=1.291286, validation/num_examples=50000
I0129 00:38:47.348236 140005297075968 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.919942855834961, loss=1.5388805866241455
I0129 00:39:20.846136 140005288683264 logging_writer.py:48] [112500] global_step=112500, grad_norm=5.70454216003418, loss=1.4378858804702759
I0129 00:39:54.416033 140005297075968 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.853222370147705, loss=1.5555424690246582
I0129 00:40:27.951393 140005288683264 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.963198661804199, loss=1.583118200302124
I0129 00:41:01.458849 140005297075968 logging_writer.py:48] [112800] global_step=112800, grad_norm=5.677767276763916, loss=1.503930926322937
I0129 00:41:35.011006 140005288683264 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.167291164398193, loss=1.5868172645568848
I0129 00:42:08.566477 140005297075968 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.922625541687012, loss=1.5656101703643799
I0129 00:42:42.125733 140005288683264 logging_writer.py:48] [113100] global_step=113100, grad_norm=5.760692596435547, loss=1.5175429582595825
I0129 00:43:15.766093 140005297075968 logging_writer.py:48] [113200] global_step=113200, grad_norm=5.08951997756958, loss=1.430392861366272
I0129 00:43:49.373314 140005288683264 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.813936710357666, loss=1.6374167203903198
I0129 00:44:22.990735 140005297075968 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.928391933441162, loss=1.545799732208252
I0129 00:44:56.584020 140005288683264 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.992895126342773, loss=1.5120799541473389
I0129 00:45:30.184473 140005297075968 logging_writer.py:48] [113600] global_step=113600, grad_norm=5.020013809204102, loss=1.5136871337890625
I0129 00:46:03.772949 140005288683264 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.975930690765381, loss=1.5328726768493652
I0129 00:46:37.397445 140005297075968 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.771270275115967, loss=1.4654908180236816
I0129 00:47:04.764712 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:47:11.104570 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:47:20.088180 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:47:22.725624 140169137129280 submission_runner.py:408] Time since start: 39654.57s, 	Step: 113883, 	{'train/accuracy': 0.7518534660339355, 'train/loss': 0.9311055541038513, 'validation/accuracy': 0.6769199967384338, 'validation/loss': 1.313949704170227, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.041203260421753, 'test/num_examples': 10000, 'score': 38286.34936618805, 'total_duration': 39654.57463693619, 'accumulated_submission_time': 38286.34936618805, 'accumulated_eval_time': 1361.454597234726, 'accumulated_logging_time': 3.0750892162323}
I0129 00:47:22.763304 140005313861376 logging_writer.py:48] [113883] accumulated_eval_time=1361.454597, accumulated_logging_time=3.075089, accumulated_submission_time=38286.349366, global_step=113883, preemption_count=0, score=38286.349366, test/accuracy=0.542900, test/loss=2.041203, test/num_examples=10000, total_duration=39654.574637, train/accuracy=0.751853, train/loss=0.931106, validation/accuracy=0.676920, validation/loss=1.313950, validation/num_examples=50000
I0129 00:47:28.805075 140005322254080 logging_writer.py:48] [113900] global_step=113900, grad_norm=5.496257781982422, loss=1.5492085218429565
I0129 00:48:02.310624 140005313861376 logging_writer.py:48] [114000] global_step=114000, grad_norm=5.04232931137085, loss=1.5076435804367065
I0129 00:48:35.833363 140005322254080 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.91864538192749, loss=1.501725435256958
I0129 00:49:09.406479 140005313861376 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.743636131286621, loss=1.4218194484710693
I0129 00:49:43.067744 140005322254080 logging_writer.py:48] [114300] global_step=114300, grad_norm=5.61888313293457, loss=1.457062005996704
I0129 00:50:16.605010 140005313861376 logging_writer.py:48] [114400] global_step=114400, grad_norm=6.1070876121521, loss=1.5126347541809082
I0129 00:50:50.127864 140005322254080 logging_writer.py:48] [114500] global_step=114500, grad_norm=5.362961769104004, loss=1.5101118087768555
I0129 00:51:23.680429 140005313861376 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.727664470672607, loss=1.5050042867660522
I0129 00:51:57.197731 140005322254080 logging_writer.py:48] [114700] global_step=114700, grad_norm=5.188598155975342, loss=1.4497771263122559
I0129 00:52:30.766872 140005313861376 logging_writer.py:48] [114800] global_step=114800, grad_norm=5.026621341705322, loss=1.4767749309539795
I0129 00:53:04.341500 140005322254080 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.945123195648193, loss=1.4761265516281128
I0129 00:53:37.836880 140005313861376 logging_writer.py:48] [115000] global_step=115000, grad_norm=4.803268909454346, loss=1.355304479598999
I0129 00:54:11.413387 140005322254080 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.800388813018799, loss=1.5965991020202637
I0129 00:54:44.996310 140005313861376 logging_writer.py:48] [115200] global_step=115200, grad_norm=5.105859756469727, loss=1.4709588289260864
I0129 00:55:18.525913 140005322254080 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.726278305053711, loss=1.4212788343429565
I0129 00:55:52.147594 140005313861376 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.227574825286865, loss=1.4571882486343384
I0129 00:55:52.969231 140169137129280 spec.py:321] Evaluating on the training split.
I0129 00:55:59.337430 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 00:56:08.172343 140169137129280 spec.py:349] Evaluating on the test split.
I0129 00:56:10.829566 140169137129280 submission_runner.py:408] Time since start: 40182.68s, 	Step: 115404, 	{'train/accuracy': 0.7578722834587097, 'train/loss': 0.9229457974433899, 'validation/accuracy': 0.6810199618339539, 'validation/loss': 1.289506435394287, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 1.9755940437316895, 'test/num_examples': 10000, 'score': 38796.49595832825, 'total_duration': 40182.678636074066, 'accumulated_submission_time': 38796.49595832825, 'accumulated_eval_time': 1379.3148682117462, 'accumulated_logging_time': 3.1231749057769775}
I0129 00:56:10.872024 140004608161536 logging_writer.py:48] [115404] accumulated_eval_time=1379.314868, accumulated_logging_time=3.123175, accumulated_submission_time=38796.495958, global_step=115404, preemption_count=0, score=38796.495958, test/accuracy=0.549400, test/loss=1.975594, test/num_examples=10000, total_duration=40182.678636, train/accuracy=0.757872, train/loss=0.922946, validation/accuracy=0.681020, validation/loss=1.289506, validation/num_examples=50000
I0129 00:56:43.350708 140004616554240 logging_writer.py:48] [115500] global_step=115500, grad_norm=5.128386497497559, loss=1.4527837038040161
I0129 00:57:16.871370 140004608161536 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.7546000480651855, loss=1.4817514419555664
I0129 00:57:50.421932 140004616554240 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.826595783233643, loss=1.4729806184768677
I0129 00:58:23.923954 140004608161536 logging_writer.py:48] [115800] global_step=115800, grad_norm=5.548934459686279, loss=1.4320963621139526
I0129 00:58:57.497520 140004616554240 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.995011806488037, loss=1.5084627866744995
I0129 00:59:31.081630 140004608161536 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.833353519439697, loss=1.3909859657287598
I0129 01:00:04.615440 140004616554240 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.15667724609375, loss=1.3895480632781982
I0129 01:00:38.127118 140004608161536 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.296992301940918, loss=1.4082024097442627
I0129 01:01:11.721241 140004616554240 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.2031354904174805, loss=1.3962242603302002
I0129 01:01:45.236718 140004608161536 logging_writer.py:48] [116400] global_step=116400, grad_norm=5.175429344177246, loss=1.4974037408828735
I0129 01:02:18.800206 140004616554240 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.5170063972473145, loss=1.4684345722198486
I0129 01:02:52.320842 140004608161536 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.93154239654541, loss=1.4871113300323486
I0129 01:03:25.923731 140004616554240 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.99173641204834, loss=1.4374301433563232
I0129 01:03:59.521605 140004608161536 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.816081523895264, loss=1.407236933708191
I0129 01:04:33.131122 140004616554240 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.831203937530518, loss=1.479767918586731
I0129 01:04:41.010469 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:04:47.375629 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:04:56.246857 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:04:58.901179 140169137129280 submission_runner.py:408] Time since start: 40710.75s, 	Step: 116925, 	{'train/accuracy': 0.7563177347183228, 'train/loss': 0.9218325614929199, 'validation/accuracy': 0.6868000030517578, 'validation/loss': 1.2699737548828125, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.9535342454910278, 'test/num_examples': 10000, 'score': 39306.57546567917, 'total_duration': 40710.75027012825, 'accumulated_submission_time': 39306.57546567917, 'accumulated_eval_time': 1397.2055568695068, 'accumulated_logging_time': 3.1755475997924805}
I0129 01:04:58.941917 140005305468672 logging_writer.py:48] [116925] accumulated_eval_time=1397.205557, accumulated_logging_time=3.175548, accumulated_submission_time=39306.575466, global_step=116925, preemption_count=0, score=39306.575466, test/accuracy=0.563800, test/loss=1.953534, test/num_examples=10000, total_duration=40710.750270, train/accuracy=0.756318, train/loss=0.921833, validation/accuracy=0.686800, validation/loss=1.269974, validation/num_examples=50000
I0129 01:05:24.376717 140005313861376 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.427404880523682, loss=1.419160008430481
I0129 01:05:57.896505 140005305468672 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.738563060760498, loss=1.4423027038574219
I0129 01:06:31.387956 140005313861376 logging_writer.py:48] [117200] global_step=117200, grad_norm=5.204974174499512, loss=1.491033673286438
I0129 01:07:04.920253 140005305468672 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.2146077156066895, loss=1.4967057704925537
I0129 01:07:38.435071 140005313861376 logging_writer.py:48] [117400] global_step=117400, grad_norm=5.288578510284424, loss=1.4650084972381592
I0129 01:08:11.928684 140005305468672 logging_writer.py:48] [117500] global_step=117500, grad_norm=5.044952869415283, loss=1.4019800424575806
I0129 01:08:45.538391 140005313861376 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.174506187438965, loss=1.5355126857757568
I0129 01:09:19.080276 140005305468672 logging_writer.py:48] [117700] global_step=117700, grad_norm=5.086630344390869, loss=1.4539026021957397
I0129 01:09:52.581983 140005313861376 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.973480701446533, loss=1.4344029426574707
I0129 01:10:26.151799 140005305468672 logging_writer.py:48] [117900] global_step=117900, grad_norm=5.8921074867248535, loss=1.4507838487625122
I0129 01:10:59.646806 140005313861376 logging_writer.py:48] [118000] global_step=118000, grad_norm=6.414428234100342, loss=1.4673664569854736
I0129 01:11:33.191111 140005305468672 logging_writer.py:48] [118100] global_step=118100, grad_norm=5.5367021560668945, loss=1.4523963928222656
I0129 01:12:06.774882 140005313861376 logging_writer.py:48] [118200] global_step=118200, grad_norm=6.004834175109863, loss=1.512526273727417
I0129 01:12:40.375965 140005305468672 logging_writer.py:48] [118300] global_step=118300, grad_norm=5.155396461486816, loss=1.4149985313415527
I0129 01:13:13.961325 140005313861376 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.275629043579102, loss=1.313019871711731
I0129 01:13:29.231325 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:13:35.717328 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:13:44.365889 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:13:47.021197 140169137129280 submission_runner.py:408] Time since start: 41238.87s, 	Step: 118447, 	{'train/accuracy': 0.7603236436843872, 'train/loss': 0.9137925505638123, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2629296779632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 1.9791010618209839, 'test/num_examples': 10000, 'score': 39816.80427622795, 'total_duration': 41238.8702480793, 'accumulated_submission_time': 39816.80427622795, 'accumulated_eval_time': 1414.9953515529633, 'accumulated_logging_time': 3.2276132106781006}
I0129 01:13:47.079207 140004616554240 logging_writer.py:48] [118447] accumulated_eval_time=1414.995352, accumulated_logging_time=3.227613, accumulated_submission_time=39816.804276, global_step=118447, preemption_count=0, score=39816.804276, test/accuracy=0.555300, test/loss=1.979101, test/num_examples=10000, total_duration=41238.870248, train/accuracy=0.760324, train/loss=0.913793, validation/accuracy=0.689940, validation/loss=1.262930, validation/num_examples=50000
I0129 01:14:05.202495 140004624946944 logging_writer.py:48] [118500] global_step=118500, grad_norm=5.437137126922607, loss=1.4569613933563232
I0129 01:14:38.779233 140004616554240 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.012659549713135, loss=1.3993430137634277
I0129 01:15:12.297144 140004624946944 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.5069684982299805, loss=1.2337620258331299
I0129 01:15:45.840623 140004616554240 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.803382873535156, loss=1.3880420923233032
I0129 01:16:19.352601 140004624946944 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.841434955596924, loss=1.4287316799163818
I0129 01:16:52.902255 140004616554240 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.161686897277832, loss=1.3666664361953735
I0129 01:17:26.448695 140004624946944 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.8919997215271, loss=1.4674363136291504
I0129 01:17:59.951079 140004616554240 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.291096210479736, loss=1.495890736579895
I0129 01:18:33.498707 140004624946944 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.1459455490112305, loss=1.4415960311889648
I0129 01:19:07.088543 140004616554240 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.749594688415527, loss=1.423327922821045
I0129 01:19:40.701372 140004624946944 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.772387981414795, loss=1.404064416885376
I0129 01:20:14.310751 140004616554240 logging_writer.py:48] [119600] global_step=119600, grad_norm=5.499346733093262, loss=1.5670533180236816
I0129 01:20:47.933769 140004624946944 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.086151599884033, loss=1.435869574546814
I0129 01:21:21.544988 140004616554240 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.560108661651611, loss=1.4739494323730469
I0129 01:21:55.152149 140004624946944 logging_writer.py:48] [119900] global_step=119900, grad_norm=5.302062511444092, loss=1.3668160438537598
I0129 01:22:17.140071 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:22:23.520061 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:22:32.431618 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:22:35.073984 140169137129280 submission_runner.py:408] Time since start: 41766.92s, 	Step: 119967, 	{'train/accuracy': 0.7940250039100647, 'train/loss': 0.7722108364105225, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.2503635883331299, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 1.9592362642288208, 'test/num_examples': 10000, 'score': 40326.800953388214, 'total_duration': 41766.923069000244, 'accumulated_submission_time': 40326.800953388214, 'accumulated_eval_time': 1432.9292194843292, 'accumulated_logging_time': 3.300215721130371}
I0129 01:22:35.115102 140004624946944 logging_writer.py:48] [119967] accumulated_eval_time=1432.929219, accumulated_logging_time=3.300216, accumulated_submission_time=40326.800953, global_step=119967, preemption_count=0, score=40326.800953, test/accuracy=0.556500, test/loss=1.959236, test/num_examples=10000, total_duration=41766.923069, train/accuracy=0.794025, train/loss=0.772211, validation/accuracy=0.690420, validation/loss=1.250364, validation/num_examples=50000
I0129 01:22:46.515697 140005288683264 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.223930358886719, loss=1.4835172891616821
I0129 01:23:19.962764 140004624946944 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.06939697265625, loss=1.4047596454620361
I0129 01:23:53.547078 140005288683264 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.362111568450928, loss=1.454927682876587
I0129 01:24:27.075019 140004624946944 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.210516452789307, loss=1.4191306829452515
I0129 01:25:00.578471 140005288683264 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.2624406814575195, loss=1.448667049407959
I0129 01:25:34.144352 140004624946944 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.468481540679932, loss=1.3884280920028687
I0129 01:26:07.733712 140005288683264 logging_writer.py:48] [120600] global_step=120600, grad_norm=6.505847930908203, loss=1.464975118637085
I0129 01:26:41.345831 140004624946944 logging_writer.py:48] [120700] global_step=120700, grad_norm=6.5763936042785645, loss=1.492532730102539
I0129 01:27:15.006763 140005288683264 logging_writer.py:48] [120800] global_step=120800, grad_norm=5.97987699508667, loss=1.4520446062088013
I0129 01:27:48.578234 140004624946944 logging_writer.py:48] [120900] global_step=120900, grad_norm=6.144285202026367, loss=1.584829568862915
I0129 01:28:22.140988 140005288683264 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.304215908050537, loss=1.4076117277145386
I0129 01:28:55.733059 140004624946944 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.26345157623291, loss=1.3641057014465332
I0129 01:29:29.342522 140005288683264 logging_writer.py:48] [121200] global_step=121200, grad_norm=5.16686487197876, loss=1.4350030422210693
I0129 01:30:02.960967 140004624946944 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.521556377410889, loss=1.3999990224838257
I0129 01:30:36.569537 140005288683264 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.734982013702393, loss=1.5027718544006348
I0129 01:31:05.292536 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:31:11.638406 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:31:20.446718 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:31:23.057730 140169137129280 submission_runner.py:408] Time since start: 42294.91s, 	Step: 121487, 	{'train/accuracy': 0.7747727632522583, 'train/loss': 0.8482697010040283, 'validation/accuracy': 0.6867799758911133, 'validation/loss': 1.2761807441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 1.9938021898269653, 'test/num_examples': 10000, 'score': 40836.91822504997, 'total_duration': 42294.90674686432, 'accumulated_submission_time': 40836.91822504997, 'accumulated_eval_time': 1450.694310426712, 'accumulated_logging_time': 3.352198362350464}
I0129 01:31:23.098209 140005297075968 logging_writer.py:48] [121487] accumulated_eval_time=1450.694310, accumulated_logging_time=3.352198, accumulated_submission_time=40836.918225, global_step=121487, preemption_count=0, score=40836.918225, test/accuracy=0.552200, test/loss=1.993802, test/num_examples=10000, total_duration=42294.906747, train/accuracy=0.774773, train/loss=0.848270, validation/accuracy=0.686780, validation/loss=1.276181, validation/num_examples=50000
I0129 01:31:27.810434 140005322254080 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.273028373718262, loss=1.4893908500671387
I0129 01:32:01.288723 140005297075968 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.851115703582764, loss=1.4314912557601929
I0129 01:32:34.821190 140005322254080 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.9152631759643555, loss=1.3516708612442017
I0129 01:33:08.358004 140005297075968 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.298791885375977, loss=1.5243773460388184
I0129 01:33:41.988060 140005322254080 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.21637487411499, loss=1.4205642938613892
I0129 01:34:15.538645 140005297075968 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.082583904266357, loss=1.3682047128677368
I0129 01:34:49.025370 140005322254080 logging_writer.py:48] [122100] global_step=122100, grad_norm=5.555080413818359, loss=1.388179063796997
I0129 01:35:22.578477 140005297075968 logging_writer.py:48] [122200] global_step=122200, grad_norm=5.370328426361084, loss=1.4061275720596313
I0129 01:35:56.186816 140005322254080 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.656774044036865, loss=1.5108650922775269
I0129 01:36:29.726024 140005297075968 logging_writer.py:48] [122400] global_step=122400, grad_norm=5.5944132804870605, loss=1.3215888738632202
I0129 01:37:03.227962 140005322254080 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.2301483154296875, loss=1.4333256483078003
I0129 01:37:36.789945 140005297075968 logging_writer.py:48] [122600] global_step=122600, grad_norm=5.75585412979126, loss=1.4520448446273804
I0129 01:38:10.387245 140005322254080 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.361142158508301, loss=1.507836937904358
I0129 01:38:44.002127 140005297075968 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.796778202056885, loss=1.319103479385376
I0129 01:39:17.611720 140005322254080 logging_writer.py:48] [122900] global_step=122900, grad_norm=5.316391944885254, loss=1.4290790557861328
I0129 01:39:51.266869 140005297075968 logging_writer.py:48] [123000] global_step=123000, grad_norm=6.50110387802124, loss=1.5135494470596313
I0129 01:39:53.088542 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:39:59.475035 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:40:08.410680 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:40:11.075154 140169137129280 submission_runner.py:408] Time since start: 42822.92s, 	Step: 123007, 	{'train/accuracy': 0.762137234210968, 'train/loss': 0.8999484181404114, 'validation/accuracy': 0.6813399791717529, 'validation/loss': 1.2865992784500122, 'validation/num_examples': 50000, 'test/accuracy': 0.5526000261306763, 'test/loss': 2.0052297115325928, 'test/num_examples': 10000, 'score': 41346.84619688988, 'total_duration': 42822.92417263985, 'accumulated_submission_time': 41346.84619688988, 'accumulated_eval_time': 1468.680810213089, 'accumulated_logging_time': 3.4059770107269287}
I0129 01:40:11.115581 140005288683264 logging_writer.py:48] [123007] accumulated_eval_time=1468.680810, accumulated_logging_time=3.405977, accumulated_submission_time=41346.846197, global_step=123007, preemption_count=0, score=41346.846197, test/accuracy=0.552600, test/loss=2.005230, test/num_examples=10000, total_duration=42822.924173, train/accuracy=0.762137, train/loss=0.899948, validation/accuracy=0.681340, validation/loss=1.286599, validation/num_examples=50000
I0129 01:40:42.580504 140005305468672 logging_writer.py:48] [123100] global_step=123100, grad_norm=5.27910041809082, loss=1.274303674697876
I0129 01:41:16.044372 140005288683264 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.813440799713135, loss=1.3292534351348877
I0129 01:41:49.597852 140005305468672 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.75889253616333, loss=1.3451330661773682
I0129 01:42:23.225005 140005288683264 logging_writer.py:48] [123400] global_step=123400, grad_norm=5.389254570007324, loss=1.3358943462371826
I0129 01:42:56.824418 140005305468672 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.588473796844482, loss=1.4320063591003418
I0129 01:43:30.364121 140005288683264 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.107692718505859, loss=1.3638604879379272
I0129 01:44:03.846064 140005305468672 logging_writer.py:48] [123700] global_step=123700, grad_norm=5.190406322479248, loss=1.4268232583999634
I0129 01:44:37.437307 140005288683264 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.765261173248291, loss=1.4541910886764526
I0129 01:45:11.023510 140005305468672 logging_writer.py:48] [123900] global_step=123900, grad_norm=6.074338436126709, loss=1.4001712799072266
I0129 01:45:44.572810 140005288683264 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.7199249267578125, loss=1.4272123575210571
I0129 01:46:18.191851 140005305468672 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.4971489906311035, loss=1.4060959815979004
I0129 01:46:51.780018 140005288683264 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.806370735168457, loss=1.4319145679473877
I0129 01:47:25.387460 140005305468672 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.272954940795898, loss=1.40771484375
I0129 01:47:59.015262 140005288683264 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.801407337188721, loss=1.4510362148284912
I0129 01:48:32.625888 140005305468672 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.175940036773682, loss=1.4191747903823853
I0129 01:48:41.187534 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:48:48.191600 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:48:56.736879 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:48:59.373214 140169137129280 submission_runner.py:408] Time since start: 43351.22s, 	Step: 124527, 	{'train/accuracy': 0.7736367583274841, 'train/loss': 0.8483324646949768, 'validation/accuracy': 0.692579984664917, 'validation/loss': 1.2330009937286377, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.933935284614563, 'test/num_examples': 10000, 'score': 41856.85910201073, 'total_duration': 43351.22230386734, 'accumulated_submission_time': 41856.85910201073, 'accumulated_eval_time': 1486.8664498329163, 'accumulated_logging_time': 3.4564247131347656}
I0129 01:48:59.413755 140005297075968 logging_writer.py:48] [124527] accumulated_eval_time=1486.866450, accumulated_logging_time=3.456425, accumulated_submission_time=41856.859102, global_step=124527, preemption_count=0, score=41856.859102, test/accuracy=0.565200, test/loss=1.933935, test/num_examples=10000, total_duration=43351.222304, train/accuracy=0.773637, train/loss=0.848332, validation/accuracy=0.692580, validation/loss=1.233001, validation/num_examples=50000
I0129 01:49:24.206879 140005322254080 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.299615383148193, loss=1.3401107788085938
I0129 01:49:57.772106 140005297075968 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.409809589385986, loss=1.461246371269226
I0129 01:50:31.385289 140005322254080 logging_writer.py:48] [124800] global_step=124800, grad_norm=6.193625450134277, loss=1.4541858434677124
I0129 01:51:04.943053 140005297075968 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.32761812210083, loss=1.3586279153823853
I0129 01:51:38.421916 140005322254080 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.887598514556885, loss=1.3344718217849731
I0129 01:52:12.004308 140005297075968 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.8935322761535645, loss=1.4504501819610596
I0129 01:52:45.683796 140005322254080 logging_writer.py:48] [125200] global_step=125200, grad_norm=6.2502641677856445, loss=1.4433608055114746
I0129 01:53:19.260132 140005297075968 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.826879501342773, loss=1.441953420639038
I0129 01:53:52.747355 140005322254080 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.875450611114502, loss=1.3669376373291016
I0129 01:54:26.317779 140005297075968 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.604123115539551, loss=1.4983807802200317
I0129 01:54:59.921178 140005322254080 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.31462287902832, loss=1.3682564496994019
I0129 01:55:33.468487 140005297075968 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.108278274536133, loss=1.4054826498031616
I0129 01:56:06.970308 140005322254080 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.530797958374023, loss=1.3820741176605225
I0129 01:56:40.574002 140005297075968 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.4546332359313965, loss=1.3845347166061401
I0129 01:57:14.180783 140005322254080 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.434289455413818, loss=1.3607330322265625
I0129 01:57:29.447396 140169137129280 spec.py:321] Evaluating on the training split.
I0129 01:57:35.810122 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 01:57:44.712409 140169137129280 spec.py:349] Evaluating on the test split.
I0129 01:57:47.404856 140169137129280 submission_runner.py:408] Time since start: 43879.25s, 	Step: 126047, 	{'train/accuracy': 0.7742147445678711, 'train/loss': 0.844782292842865, 'validation/accuracy': 0.6972999572753906, 'validation/loss': 1.2068699598312378, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.8856645822525024, 'test/num_examples': 10000, 'score': 42366.83264923096, 'total_duration': 43879.25383400917, 'accumulated_submission_time': 42366.83264923096, 'accumulated_eval_time': 1504.823757648468, 'accumulated_logging_time': 3.5072178840637207}
I0129 01:57:47.448865 140004616554240 logging_writer.py:48] [126047] accumulated_eval_time=1504.823758, accumulated_logging_time=3.507218, accumulated_submission_time=42366.832649, global_step=126047, preemption_count=0, score=42366.832649, test/accuracy=0.570900, test/loss=1.885665, test/num_examples=10000, total_duration=43879.253834, train/accuracy=0.774215, train/loss=0.844782, validation/accuracy=0.697300, validation/loss=1.206870, validation/num_examples=50000
I0129 01:58:05.583686 140004624946944 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.538941383361816, loss=1.3549448251724243
I0129 01:58:39.098595 140004616554240 logging_writer.py:48] [126200] global_step=126200, grad_norm=6.040884494781494, loss=1.4917670488357544
I0129 01:59:12.730813 140004624946944 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.556074619293213, loss=1.342865228652954
I0129 01:59:46.310730 140004616554240 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.387577056884766, loss=1.32096266746521
I0129 02:00:19.806900 140004624946944 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.505249977111816, loss=1.3647655248641968
I0129 02:00:53.354207 140004616554240 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.154072284698486, loss=1.3370131254196167
I0129 02:01:26.899592 140004624946944 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.433688640594482, loss=1.286241888999939
I0129 02:02:00.434664 140004616554240 logging_writer.py:48] [126800] global_step=126800, grad_norm=6.340969562530518, loss=1.2796454429626465
I0129 02:02:33.923952 140004624946944 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.709616184234619, loss=1.4610860347747803
I0129 02:03:07.505438 140004616554240 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.507609844207764, loss=1.4049482345581055
I0129 02:03:41.115014 140004624946944 logging_writer.py:48] [127100] global_step=127100, grad_norm=5.316425800323486, loss=1.2736984491348267
I0129 02:04:14.726791 140004616554240 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.514249801635742, loss=1.4187190532684326
I0129 02:04:48.336613 140004624946944 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.785701751708984, loss=1.3878543376922607
I0129 02:05:21.985808 140004616554240 logging_writer.py:48] [127400] global_step=127400, grad_norm=6.551330089569092, loss=1.3016425371170044
I0129 02:05:55.577409 140004624946944 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.60810661315918, loss=1.2686344385147095
I0129 02:06:17.559875 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:06:23.887846 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:06:32.665853 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:06:35.297874 140169137129280 submission_runner.py:408] Time since start: 44407.15s, 	Step: 127567, 	{'train/accuracy': 0.7678172588348389, 'train/loss': 0.8690696358680725, 'validation/accuracy': 0.6976400017738342, 'validation/loss': 1.232379674911499, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.928336262702942, 'test/num_examples': 10000, 'score': 42876.883296728134, 'total_duration': 44407.14696741104, 'accumulated_submission_time': 42876.883296728134, 'accumulated_eval_time': 1522.5617182254791, 'accumulated_logging_time': 3.56189227104187}
I0129 02:06:35.338879 140005305468672 logging_writer.py:48] [127567] accumulated_eval_time=1522.561718, accumulated_logging_time=3.561892, accumulated_submission_time=42876.883297, global_step=127567, preemption_count=0, score=42876.883297, test/accuracy=0.574600, test/loss=1.928336, test/num_examples=10000, total_duration=44407.146967, train/accuracy=0.767817, train/loss=0.869070, validation/accuracy=0.697640, validation/loss=1.232380, validation/num_examples=50000
I0129 02:06:46.762632 140005313861376 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.384360313415527, loss=1.2715802192687988
I0129 02:07:20.206940 140005305468672 logging_writer.py:48] [127700] global_step=127700, grad_norm=6.07523250579834, loss=1.3943326473236084
I0129 02:07:53.772621 140005313861376 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.835961818695068, loss=1.434923768043518
I0129 02:08:27.388682 140005305468672 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.51366662979126, loss=1.334206223487854
I0129 02:09:01.004045 140005313861376 logging_writer.py:48] [128000] global_step=128000, grad_norm=5.533692836761475, loss=1.3333699703216553
I0129 02:09:34.613104 140005305468672 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.346227169036865, loss=1.3408633470535278
I0129 02:10:08.185927 140005313861376 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.804791450500488, loss=1.3988536596298218
I0129 02:10:41.660584 140005305468672 logging_writer.py:48] [128300] global_step=128300, grad_norm=6.186733245849609, loss=1.3106210231781006
I0129 02:11:15.232840 140005313861376 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.881640911102295, loss=1.3364185094833374
I0129 02:11:48.821647 140005305468672 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.458250522613525, loss=1.2154548168182373
I0129 02:12:22.400048 140005313861376 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.603404521942139, loss=1.3589274883270264
I0129 02:12:56.018887 140005305468672 logging_writer.py:48] [128700] global_step=128700, grad_norm=6.539541721343994, loss=1.3231016397476196
I0129 02:13:29.570856 140005313861376 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.828304290771484, loss=1.357992172241211
I0129 02:14:03.075276 140005305468672 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.766834259033203, loss=1.4121918678283691
I0129 02:14:36.635291 140005313861376 logging_writer.py:48] [129000] global_step=129000, grad_norm=5.964951515197754, loss=1.2834358215332031
I0129 02:15:05.621540 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:15:11.990336 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:15:20.807177 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:15:23.433477 140169137129280 submission_runner.py:408] Time since start: 44935.28s, 	Step: 129088, 	{'train/accuracy': 0.8156289458274841, 'train/loss': 0.6751767992973328, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.1848829984664917, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.8793182373046875, 'test/num_examples': 10000, 'score': 43387.103593587875, 'total_duration': 44935.282566308975, 'accumulated_submission_time': 43387.103593587875, 'accumulated_eval_time': 1540.3736391067505, 'accumulated_logging_time': 3.616084575653076}
I0129 02:15:23.473477 140004624946944 logging_writer.py:48] [129088] accumulated_eval_time=1540.373639, accumulated_logging_time=3.616085, accumulated_submission_time=43387.103594, global_step=129088, preemption_count=0, score=43387.103594, test/accuracy=0.578800, test/loss=1.879318, test/num_examples=10000, total_duration=44935.282566, train/accuracy=0.815629, train/loss=0.675177, validation/accuracy=0.704680, validation/loss=1.184883, validation/num_examples=50000
I0129 02:15:27.838528 140005288683264 logging_writer.py:48] [129100] global_step=129100, grad_norm=6.045297145843506, loss=1.309323787689209
I0129 02:16:01.336767 140004624946944 logging_writer.py:48] [129200] global_step=129200, grad_norm=6.0125603675842285, loss=1.343418836593628
I0129 02:16:34.858388 140005288683264 logging_writer.py:48] [129300] global_step=129300, grad_norm=6.085216522216797, loss=1.419548511505127
I0129 02:17:08.457274 140004624946944 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.7218546867370605, loss=1.257913589477539
I0129 02:17:42.133221 140005288683264 logging_writer.py:48] [129500] global_step=129500, grad_norm=6.273675441741943, loss=1.3334062099456787
I0129 02:18:15.701562 140004624946944 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.35795783996582, loss=1.2749567031860352
I0129 02:18:49.292636 140005288683264 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.268927097320557, loss=1.284731388092041
I0129 02:19:22.891948 140004624946944 logging_writer.py:48] [129800] global_step=129800, grad_norm=5.429336071014404, loss=1.31606125831604
I0129 02:19:56.481031 140005288683264 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.509457588195801, loss=1.2819494009017944
I0129 02:20:30.004730 140004624946944 logging_writer.py:48] [130000] global_step=130000, grad_norm=6.0805792808532715, loss=1.2850512266159058
I0129 02:21:03.518066 140005288683264 logging_writer.py:48] [130100] global_step=130100, grad_norm=4.814967632293701, loss=1.1608566045761108
I0129 02:21:37.094589 140004624946944 logging_writer.py:48] [130200] global_step=130200, grad_norm=5.559526443481445, loss=1.345433235168457
I0129 02:22:10.662467 140005288683264 logging_writer.py:48] [130300] global_step=130300, grad_norm=6.173153877258301, loss=1.3563789129257202
I0129 02:22:44.264662 140004624946944 logging_writer.py:48] [130400] global_step=130400, grad_norm=6.236894130706787, loss=1.2969155311584473
I0129 02:23:17.840258 140005288683264 logging_writer.py:48] [130500] global_step=130500, grad_norm=6.199548721313477, loss=1.3455785512924194
I0129 02:23:51.449857 140004624946944 logging_writer.py:48] [130600] global_step=130600, grad_norm=5.476442337036133, loss=1.244761347770691
I0129 02:23:53.623612 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:23:59.960312 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:24:08.639943 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:24:11.268454 140169137129280 submission_runner.py:408] Time since start: 45463.12s, 	Step: 130608, 	{'train/accuracy': 0.7964963316917419, 'train/loss': 0.7662532925605774, 'validation/accuracy': 0.7048999667167664, 'validation/loss': 1.1896097660064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.8776094913482666, 'test/num_examples': 10000, 'score': 43897.194222450256, 'total_duration': 45463.11754608154, 'accumulated_submission_time': 43897.194222450256, 'accumulated_eval_time': 1558.0184445381165, 'accumulated_logging_time': 3.6664631366729736}
I0129 02:24:11.311618 140004616554240 logging_writer.py:48] [130608] accumulated_eval_time=1558.018445, accumulated_logging_time=3.666463, accumulated_submission_time=43897.194222, global_step=130608, preemption_count=0, score=43897.194222, test/accuracy=0.580400, test/loss=1.877609, test/num_examples=10000, total_duration=45463.117546, train/accuracy=0.796496, train/loss=0.766253, validation/accuracy=0.704900, validation/loss=1.189610, validation/num_examples=50000
I0129 02:24:42.466490 140005313861376 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.916699409484863, loss=1.285461187362671
I0129 02:25:16.052476 140004616554240 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.824185371398926, loss=1.249650478363037
I0129 02:25:49.586485 140005313861376 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.318326950073242, loss=1.4009425640106201
I0129 02:26:23.078036 140004616554240 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.338357448577881, loss=1.2861409187316895
I0129 02:26:56.679812 140005313861376 logging_writer.py:48] [131100] global_step=131100, grad_norm=6.376713275909424, loss=1.368476152420044
I0129 02:27:30.279464 140004616554240 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.673643112182617, loss=1.2955867052078247
I0129 02:28:03.857040 140005313861376 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.554930686950684, loss=1.306157112121582
I0129 02:28:37.463722 140004616554240 logging_writer.py:48] [131400] global_step=131400, grad_norm=6.1060357093811035, loss=1.3599610328674316
I0129 02:29:11.002255 140005313861376 logging_writer.py:48] [131500] global_step=131500, grad_norm=6.072304725646973, loss=1.426495909690857
I0129 02:29:44.508979 140004616554240 logging_writer.py:48] [131600] global_step=131600, grad_norm=6.233939170837402, loss=1.303499460220337
I0129 02:30:18.133966 140005313861376 logging_writer.py:48] [131700] global_step=131700, grad_norm=6.304596424102783, loss=1.2708872556686401
I0129 02:30:51.625493 140004616554240 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.694401741027832, loss=1.2927008867263794
I0129 02:31:25.169866 140005313861376 logging_writer.py:48] [131900] global_step=131900, grad_norm=6.3299336433410645, loss=1.2556339502334595
I0129 02:31:58.743185 140004616554240 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.600588798522949, loss=1.3139216899871826
I0129 02:32:32.255636 140005313861376 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.981488227844238, loss=1.1908713579177856
I0129 02:32:41.444380 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:32:47.777709 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:32:56.515101 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:32:59.165544 140169137129280 submission_runner.py:408] Time since start: 45991.01s, 	Step: 132129, 	{'train/accuracy': 0.7948620915412903, 'train/loss': 0.7571321725845337, 'validation/accuracy': 0.7081599831581116, 'validation/loss': 1.174660325050354, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.8872145414352417, 'test/num_examples': 10000, 'score': 44407.264327287674, 'total_duration': 45991.0145611763, 'accumulated_submission_time': 44407.264327287674, 'accumulated_eval_time': 1575.7394952774048, 'accumulated_logging_time': 3.723146915435791}
I0129 02:32:59.208221 140004624946944 logging_writer.py:48] [132129] accumulated_eval_time=1575.739495, accumulated_logging_time=3.723147, accumulated_submission_time=44407.264327, global_step=132129, preemption_count=0, score=44407.264327, test/accuracy=0.579800, test/loss=1.887215, test/num_examples=10000, total_duration=45991.014561, train/accuracy=0.794862, train/loss=0.757132, validation/accuracy=0.708160, validation/loss=1.174660, validation/num_examples=50000
I0129 02:33:23.310514 140005288683264 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.9007792472839355, loss=1.28495454788208
I0129 02:33:56.795101 140004624946944 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.4918670654296875, loss=1.2443046569824219
I0129 02:34:30.340353 140005288683264 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.99235725402832, loss=1.417259931564331
I0129 02:35:03.832054 140004624946944 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.993753910064697, loss=1.257568359375
I0129 02:35:37.380266 140005288683264 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.9461212158203125, loss=1.289320707321167
I0129 02:36:10.878871 140004624946944 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.0917463302612305, loss=1.227012038230896
I0129 02:36:44.484702 140005288683264 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.668605327606201, loss=1.214335560798645
I0129 02:37:18.024435 140004624946944 logging_writer.py:48] [132900] global_step=132900, grad_norm=6.521313190460205, loss=1.4402917623519897
I0129 02:37:51.602026 140005288683264 logging_writer.py:48] [133000] global_step=133000, grad_norm=5.82708215713501, loss=1.2948498725891113
I0129 02:38:25.126497 140004624946944 logging_writer.py:48] [133100] global_step=133100, grad_norm=6.611301422119141, loss=1.274878740310669
I0129 02:38:58.644487 140005288683264 logging_writer.py:48] [133200] global_step=133200, grad_norm=6.133615016937256, loss=1.3347289562225342
I0129 02:39:32.194350 140004624946944 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.8718976974487305, loss=1.282568335533142
I0129 02:40:05.745890 140005288683264 logging_writer.py:48] [133400] global_step=133400, grad_norm=6.046157360076904, loss=1.2061450481414795
I0129 02:40:39.260791 140004624946944 logging_writer.py:48] [133500] global_step=133500, grad_norm=6.370552062988281, loss=1.301849603652954
I0129 02:41:12.791384 140005288683264 logging_writer.py:48] [133600] global_step=133600, grad_norm=7.05234956741333, loss=1.3224327564239502
I0129 02:41:29.392246 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:41:35.761228 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:41:44.728431 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:41:47.511131 140169137129280 submission_runner.py:408] Time since start: 46519.36s, 	Step: 133651, 	{'train/accuracy': 0.7874082922935486, 'train/loss': 0.7908145189285278, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.193451166152954, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.8885688781738281, 'test/num_examples': 10000, 'score': 44917.387357234955, 'total_duration': 46519.360219955444, 'accumulated_submission_time': 44917.387357234955, 'accumulated_eval_time': 1593.8583455085754, 'accumulated_logging_time': 3.7774014472961426}
I0129 02:41:47.557794 140005313861376 logging_writer.py:48] [133651] accumulated_eval_time=1593.858346, accumulated_logging_time=3.777401, accumulated_submission_time=44917.387357, global_step=133651, preemption_count=0, score=44917.387357, test/accuracy=0.577300, test/loss=1.888569, test/num_examples=10000, total_duration=46519.360220, train/accuracy=0.787408, train/loss=0.790815, validation/accuracy=0.702040, validation/loss=1.193451, validation/num_examples=50000
I0129 02:42:04.302510 140005322254080 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.815238952636719, loss=1.2613627910614014
I0129 02:42:37.766887 140005313861376 logging_writer.py:48] [133800] global_step=133800, grad_norm=6.326844215393066, loss=1.2709970474243164
I0129 02:43:11.360535 140005322254080 logging_writer.py:48] [133900] global_step=133900, grad_norm=6.691175937652588, loss=1.34788179397583
I0129 02:43:44.921189 140005313861376 logging_writer.py:48] [134000] global_step=134000, grad_norm=6.085672855377197, loss=1.242548942565918
I0129 02:44:18.438407 140005322254080 logging_writer.py:48] [134100] global_step=134100, grad_norm=6.144067287445068, loss=1.2725242376327515
I0129 02:44:51.968979 140005313861376 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.8094868659973145, loss=1.2086150646209717
I0129 02:45:25.532094 140005322254080 logging_writer.py:48] [134300] global_step=134300, grad_norm=6.166375160217285, loss=1.3394815921783447
I0129 02:45:59.050400 140005313861376 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.761460304260254, loss=1.3040155172348022
I0129 02:46:32.591047 140005322254080 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.987253665924072, loss=1.2582632303237915
I0129 02:47:06.102001 140005313861376 logging_writer.py:48] [134600] global_step=134600, grad_norm=7.508874416351318, loss=1.3425095081329346
I0129 02:47:39.651423 140005322254080 logging_writer.py:48] [134700] global_step=134700, grad_norm=6.021214485168457, loss=1.2488456964492798
I0129 02:48:13.182893 140005313861376 logging_writer.py:48] [134800] global_step=134800, grad_norm=6.098080158233643, loss=1.1787298917770386
I0129 02:48:46.745373 140005322254080 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.971096038818359, loss=1.2165677547454834
I0129 02:49:20.395900 140005313861376 logging_writer.py:48] [135000] global_step=135000, grad_norm=6.1166510581970215, loss=1.3156986236572266
I0129 02:49:54.002469 140005322254080 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.234155654907227, loss=1.3114174604415894
I0129 02:50:17.630001 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:50:23.999867 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:50:32.877436 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:50:35.563070 140169137129280 submission_runner.py:408] Time since start: 47047.41s, 	Step: 135172, 	{'train/accuracy': 0.8009805083274841, 'train/loss': 0.7329120635986328, 'validation/accuracy': 0.7127000093460083, 'validation/loss': 1.1449772119522095, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.8212326765060425, 'test/num_examples': 10000, 'score': 45427.400640010834, 'total_duration': 47047.412162303925, 'accumulated_submission_time': 45427.400640010834, 'accumulated_eval_time': 1611.7913794517517, 'accumulated_logging_time': 3.833778142929077}
I0129 02:50:35.604594 140004616554240 logging_writer.py:48] [135172] accumulated_eval_time=1611.791379, accumulated_logging_time=3.833778, accumulated_submission_time=45427.400640, global_step=135172, preemption_count=0, score=45427.400640, test/accuracy=0.589500, test/loss=1.821233, test/num_examples=10000, total_duration=47047.412162, train/accuracy=0.800981, train/loss=0.732912, validation/accuracy=0.712700, validation/loss=1.144977, validation/num_examples=50000
I0129 02:50:45.324464 140004624946944 logging_writer.py:48] [135200] global_step=135200, grad_norm=6.30568790435791, loss=1.1954374313354492
I0129 02:51:19.211013 140004616554240 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.43958854675293, loss=1.2337629795074463
I0129 02:51:52.738929 140004624946944 logging_writer.py:48] [135400] global_step=135400, grad_norm=6.265706539154053, loss=1.2885750532150269
I0129 02:52:26.264734 140004616554240 logging_writer.py:48] [135500] global_step=135500, grad_norm=6.327630996704102, loss=1.1873329877853394
I0129 02:52:59.767108 140004624946944 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.905763626098633, loss=1.2436301708221436
I0129 02:53:33.297499 140004616554240 logging_writer.py:48] [135700] global_step=135700, grad_norm=6.663685321807861, loss=1.3453720808029175
I0129 02:54:06.904261 140004624946944 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.6606059074401855, loss=1.258678674697876
I0129 02:54:40.487488 140004616554240 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.380170822143555, loss=1.279412031173706
I0129 02:55:13.995547 140004624946944 logging_writer.py:48] [136000] global_step=136000, grad_norm=6.262007713317871, loss=1.3224513530731201
I0129 02:55:47.658576 140004616554240 logging_writer.py:48] [136100] global_step=136100, grad_norm=6.886324882507324, loss=1.2800281047821045
I0129 02:56:21.286400 140004624946944 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.152735233306885, loss=1.2233198881149292
I0129 02:56:54.864383 140004616554240 logging_writer.py:48] [136300] global_step=136300, grad_norm=8.078597068786621, loss=1.2837023735046387
I0129 02:57:28.352231 140004624946944 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.071475505828857, loss=1.2835326194763184
I0129 02:58:01.901262 140004616554240 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.848283767700195, loss=1.316455364227295
I0129 02:58:35.508184 140004624946944 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.345319747924805, loss=1.2025483846664429
I0129 02:59:05.867521 140169137129280 spec.py:321] Evaluating on the training split.
I0129 02:59:12.225370 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 02:59:21.010132 140169137129280 spec.py:349] Evaluating on the test split.
I0129 02:59:23.643796 140169137129280 submission_runner.py:408] Time since start: 47575.49s, 	Step: 136692, 	{'train/accuracy': 0.7917529940605164, 'train/loss': 0.7613667845726013, 'validation/accuracy': 0.7073599696159363, 'validation/loss': 1.1776471138000488, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8573424816131592, 'test/num_examples': 10000, 'score': 45937.60295057297, 'total_duration': 47575.49288535118, 'accumulated_submission_time': 45937.60295057297, 'accumulated_eval_time': 1629.5676186084747, 'accumulated_logging_time': 3.88659930229187}
I0129 02:59:23.687693 140005305468672 logging_writer.py:48] [136692] accumulated_eval_time=1629.567619, accumulated_logging_time=3.886599, accumulated_submission_time=45937.602951, global_step=136692, preemption_count=0, score=45937.602951, test/accuracy=0.581900, test/loss=1.857342, test/num_examples=10000, total_duration=47575.492885, train/accuracy=0.791753, train/loss=0.761367, validation/accuracy=0.707360, validation/loss=1.177647, validation/num_examples=50000
I0129 02:59:26.716348 140005313861376 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.555152893066406, loss=1.2990459203720093
I0129 03:00:00.215397 140005305468672 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.8480424880981445, loss=1.2290139198303223
I0129 03:00:33.757231 140005313861376 logging_writer.py:48] [136900] global_step=136900, grad_norm=6.0293073654174805, loss=1.279500961303711
I0129 03:01:07.336703 140005305468672 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.963282108306885, loss=1.183791995048523
I0129 03:01:40.860845 140005313861376 logging_writer.py:48] [137100] global_step=137100, grad_norm=7.216782569885254, loss=1.256889820098877
I0129 03:02:14.536340 140005305468672 logging_writer.py:48] [137200] global_step=137200, grad_norm=6.438864707946777, loss=1.2061841487884521
I0129 03:02:48.120767 140005313861376 logging_writer.py:48] [137300] global_step=137300, grad_norm=6.108696460723877, loss=1.1665987968444824
I0129 03:03:21.604437 140005305468672 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.201233863830566, loss=1.1726948022842407
I0129 03:03:55.152866 140005313861376 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.56956672668457, loss=1.3443387746810913
I0129 03:04:28.689874 140005305468672 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.609947681427002, loss=1.1751046180725098
I0129 03:05:02.267912 140005313861376 logging_writer.py:48] [137700] global_step=137700, grad_norm=6.598819732666016, loss=1.2276530265808105
I0129 03:05:35.804371 140005305468672 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.197577953338623, loss=1.2069839239120483
I0129 03:06:09.320984 140005313861376 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.807819843292236, loss=1.1247309446334839
I0129 03:06:42.909287 140005305468672 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.900431156158447, loss=1.1604697704315186
I0129 03:07:16.418740 140005313861376 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.019742012023926, loss=1.2662560939788818
I0129 03:07:49.908718 140005305468672 logging_writer.py:48] [138200] global_step=138200, grad_norm=6.028392791748047, loss=1.2136276960372925
I0129 03:07:53.755515 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:08:00.156531 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:08:09.010719 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:08:11.640363 140169137129280 submission_runner.py:408] Time since start: 48103.49s, 	Step: 138213, 	{'train/accuracy': 0.8251953125, 'train/loss': 0.6313984990119934, 'validation/accuracy': 0.7180399894714355, 'validation/loss': 1.1386464834213257, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.820005178451538, 'test/num_examples': 10000, 'score': 46447.6070895195, 'total_duration': 48103.48945856094, 'accumulated_submission_time': 46447.6070895195, 'accumulated_eval_time': 1647.4524323940277, 'accumulated_logging_time': 3.945059061050415}
I0129 03:08:11.687771 140005297075968 logging_writer.py:48] [138213] accumulated_eval_time=1647.452432, accumulated_logging_time=3.945059, accumulated_submission_time=46447.607090, global_step=138213, preemption_count=0, score=46447.607090, test/accuracy=0.589900, test/loss=1.820005, test/num_examples=10000, total_duration=48103.489459, train/accuracy=0.825195, train/loss=0.631398, validation/accuracy=0.718040, validation/loss=1.138646, validation/num_examples=50000
I0129 03:08:41.152439 140005330646784 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.155107498168945, loss=1.178557276725769
I0129 03:09:14.636045 140005297075968 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.103050708770752, loss=1.1547867059707642
I0129 03:09:48.114735 140005330646784 logging_writer.py:48] [138500] global_step=138500, grad_norm=6.55195951461792, loss=1.278194546699524
I0129 03:10:21.625502 140005297075968 logging_writer.py:48] [138600] global_step=138600, grad_norm=6.896091938018799, loss=1.2648248672485352
I0129 03:10:55.123974 140005330646784 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.410537242889404, loss=1.2424129247665405
I0129 03:11:28.656370 140005297075968 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.764194488525391, loss=1.2403455972671509
I0129 03:12:02.218648 140005330646784 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.319711685180664, loss=1.2475409507751465
I0129 03:12:35.746228 140005297075968 logging_writer.py:48] [139000] global_step=139000, grad_norm=6.5927886962890625, loss=1.2854048013687134
I0129 03:13:09.259675 140005330646784 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.498002052307129, loss=1.2604525089263916
I0129 03:13:42.820770 140005297075968 logging_writer.py:48] [139200] global_step=139200, grad_norm=6.492250442504883, loss=1.2508543729782104
I0129 03:14:16.328632 140005330646784 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.121241569519043, loss=1.1098917722702026
I0129 03:14:49.926025 140005297075968 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.350478172302246, loss=1.2771261930465698
I0129 03:15:23.441126 140005330646784 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.08648157119751, loss=1.145681619644165
I0129 03:15:57.020272 140005297075968 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.990572452545166, loss=1.2286121845245361
I0129 03:16:30.612681 140005330646784 logging_writer.py:48] [139700] global_step=139700, grad_norm=6.083025932312012, loss=1.1674729585647583
I0129 03:16:41.859297 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:16:48.247899 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:16:56.866569 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:16:59.515833 140169137129280 submission_runner.py:408] Time since start: 48631.36s, 	Step: 139735, 	{'train/accuracy': 0.8220463991165161, 'train/loss': 0.6473369002342224, 'validation/accuracy': 0.7188199758529663, 'validation/loss': 1.1304951906204224, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8218023777008057, 'test/num_examples': 10000, 'score': 46957.71826648712, 'total_duration': 48631.36492419243, 'accumulated_submission_time': 46957.71826648712, 'accumulated_eval_time': 1665.1089329719543, 'accumulated_logging_time': 4.003594875335693}
I0129 03:16:59.559374 140004616554240 logging_writer.py:48] [139735] accumulated_eval_time=1665.108933, accumulated_logging_time=4.003595, accumulated_submission_time=46957.718266, global_step=139735, preemption_count=0, score=46957.718266, test/accuracy=0.591000, test/loss=1.821802, test/num_examples=10000, total_duration=48631.364924, train/accuracy=0.822046, train/loss=0.647337, validation/accuracy=0.718820, validation/loss=1.130495, validation/num_examples=50000
I0129 03:17:21.695318 140004624946944 logging_writer.py:48] [139800] global_step=139800, grad_norm=6.689617156982422, loss=1.1985533237457275
I0129 03:17:55.277034 140004616554240 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.519540309906006, loss=1.2046693563461304
I0129 03:18:28.798129 140004624946944 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.192081928253174, loss=1.2114156484603882
I0129 03:19:02.304253 140004616554240 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.912592887878418, loss=1.2117173671722412
I0129 03:19:35.896213 140004624946944 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.841890811920166, loss=1.1733245849609375
I0129 03:20:09.494710 140004616554240 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.818425178527832, loss=1.1897221803665161
I0129 03:20:43.091145 140004624946944 logging_writer.py:48] [140400] global_step=140400, grad_norm=7.504771709442139, loss=1.2145156860351562
I0129 03:21:16.757896 140004616554240 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.765964508056641, loss=1.1861560344696045
I0129 03:21:50.274605 140004624946944 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.23497200012207, loss=1.2160670757293701
I0129 03:22:23.822585 140004616554240 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.912153720855713, loss=1.1780791282653809
I0129 03:22:57.432678 140004624946944 logging_writer.py:48] [140800] global_step=140800, grad_norm=7.582512378692627, loss=1.1230229139328003
I0129 03:23:31.037460 140004616554240 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.609918117523193, loss=1.1446938514709473
I0129 03:24:04.618874 140004624946944 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.735605239868164, loss=1.0610803365707397
I0129 03:24:38.238920 140004616554240 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.733685493469238, loss=1.209306001663208
I0129 03:25:11.860541 140004624946944 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.07119607925415, loss=1.0855156183242798
I0129 03:25:29.821951 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:25:36.173038 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:25:45.084657 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:25:47.734313 140169137129280 submission_runner.py:408] Time since start: 49159.58s, 	Step: 141255, 	{'train/accuracy': 0.8222058415412903, 'train/loss': 0.6530354619026184, 'validation/accuracy': 0.7206199765205383, 'validation/loss': 1.1245300769805908, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.8244554996490479, 'test/num_examples': 10000, 'score': 47467.91790008545, 'total_duration': 49159.58340501785, 'accumulated_submission_time': 47467.91790008545, 'accumulated_eval_time': 1683.0212621688843, 'accumulated_logging_time': 4.060739040374756}
I0129 03:25:47.778484 140005297075968 logging_writer.py:48] [141255] accumulated_eval_time=1683.021262, accumulated_logging_time=4.060739, accumulated_submission_time=47467.917900, global_step=141255, preemption_count=0, score=47467.917900, test/accuracy=0.597100, test/loss=1.824455, test/num_examples=10000, total_duration=49159.583405, train/accuracy=0.822206, train/loss=0.653035, validation/accuracy=0.720620, validation/loss=1.124530, validation/num_examples=50000
I0129 03:26:03.225059 140005313861376 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.8246917724609375, loss=1.336968183517456
I0129 03:26:36.679022 140005297075968 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.470275402069092, loss=1.299060344696045
I0129 03:27:10.355443 140005313861376 logging_writer.py:48] [141500] global_step=141500, grad_norm=6.5497026443481445, loss=1.2750457525253296
I0129 03:27:43.886197 140005297075968 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.643553733825684, loss=1.2263988256454468
I0129 03:28:17.506455 140005313861376 logging_writer.py:48] [141700] global_step=141700, grad_norm=7.049524307250977, loss=1.1682546138763428
I0129 03:28:51.118772 140005297075968 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.890600681304932, loss=1.2360742092132568
I0129 03:29:24.712930 140005313861376 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.769963264465332, loss=1.176446795463562
I0129 03:29:58.332495 140005297075968 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.976651668548584, loss=1.1689214706420898
I0129 03:30:31.923129 140005313861376 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.705402374267578, loss=1.231255292892456
I0129 03:31:05.548262 140005297075968 logging_writer.py:48] [142200] global_step=142200, grad_norm=6.19960880279541, loss=1.1896687746047974
I0129 03:31:39.124092 140005313861376 logging_writer.py:48] [142300] global_step=142300, grad_norm=6.03258752822876, loss=1.087844967842102
I0129 03:32:12.656441 140005297075968 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.8514533042907715, loss=1.2069255113601685
I0129 03:32:46.178088 140005313861376 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.2737932205200195, loss=1.045672059059143
I0129 03:33:19.745777 140005297075968 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.144164562225342, loss=1.0794923305511475
I0129 03:33:53.386632 140005313861376 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.370782375335693, loss=1.2122770547866821
I0129 03:34:18.030913 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:34:24.449192 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:34:33.335771 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:34:35.957633 140169137129280 submission_runner.py:408] Time since start: 49687.81s, 	Step: 142775, 	{'train/accuracy': 0.8192163705825806, 'train/loss': 0.6568843126296997, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.1111185550689697, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.8181012868881226, 'test/num_examples': 10000, 'score': 47978.107671022415, 'total_duration': 49687.80672287941, 'accumulated_submission_time': 47978.107671022415, 'accumulated_eval_time': 1700.9479422569275, 'accumulated_logging_time': 4.1180760860443115}
I0129 03:34:36.002498 140005288683264 logging_writer.py:48] [142775] accumulated_eval_time=1700.947942, accumulated_logging_time=4.118076, accumulated_submission_time=47978.107671, global_step=142775, preemption_count=0, score=47978.107671, test/accuracy=0.591300, test/loss=1.818101, test/num_examples=10000, total_duration=49687.806723, train/accuracy=0.819216, train/loss=0.656884, validation/accuracy=0.722820, validation/loss=1.111119, validation/num_examples=50000
I0129 03:34:44.729971 140005297075968 logging_writer.py:48] [142800] global_step=142800, grad_norm=6.236380577087402, loss=1.0692341327667236
I0129 03:35:18.253299 140005288683264 logging_writer.py:48] [142900] global_step=142900, grad_norm=7.4583330154418945, loss=1.2565968036651611
I0129 03:35:51.755262 140005297075968 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.505244731903076, loss=1.181086540222168
I0129 03:36:25.349621 140005288683264 logging_writer.py:48] [143100] global_step=143100, grad_norm=6.760854721069336, loss=1.1387648582458496
I0129 03:36:58.950028 140005297075968 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.91365909576416, loss=1.1961767673492432
I0129 03:37:32.510276 140005288683264 logging_writer.py:48] [143300] global_step=143300, grad_norm=7.037065505981445, loss=1.193739891052246
I0129 03:38:06.000885 140005297075968 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.19155740737915, loss=1.1634424924850464
I0129 03:38:39.548918 140005288683264 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.413744926452637, loss=1.1113090515136719
I0129 03:39:13.120623 140005297075968 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.331230163574219, loss=1.1065411567687988
I0129 03:39:46.739688 140005288683264 logging_writer.py:48] [143700] global_step=143700, grad_norm=7.061487674713135, loss=1.2137255668640137
I0129 03:40:20.302681 140005297075968 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.6194610595703125, loss=1.1454441547393799
I0129 03:40:53.851153 140005288683264 logging_writer.py:48] [143900] global_step=143900, grad_norm=6.205642223358154, loss=1.1482160091400146
I0129 03:41:27.439461 140005297075968 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.621596336364746, loss=1.186429738998413
I0129 03:42:00.962862 140005288683264 logging_writer.py:48] [144100] global_step=144100, grad_norm=7.066425800323486, loss=1.1238735914230347
I0129 03:42:34.484383 140005297075968 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.951076507568359, loss=1.1213657855987549
I0129 03:43:06.222515 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:43:12.528997 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:43:21.384367 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:43:24.027572 140169137129280 submission_runner.py:408] Time since start: 50215.88s, 	Step: 144296, 	{'train/accuracy': 0.8274872303009033, 'train/loss': 0.6358198523521423, 'validation/accuracy': 0.7251600027084351, 'validation/loss': 1.0935237407684326, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.785703420639038, 'test/num_examples': 10000, 'score': 48488.267067193985, 'total_duration': 50215.87666511536, 'accumulated_submission_time': 48488.267067193985, 'accumulated_eval_time': 1718.752968788147, 'accumulated_logging_time': 4.174353361129761}
I0129 03:43:24.075154 140004624946944 logging_writer.py:48] [144296] accumulated_eval_time=1718.752969, accumulated_logging_time=4.174353, accumulated_submission_time=48488.267067, global_step=144296, preemption_count=0, score=48488.267067, test/accuracy=0.596100, test/loss=1.785703, test/num_examples=10000, total_duration=50215.876665, train/accuracy=0.827487, train/loss=0.635820, validation/accuracy=0.725160, validation/loss=1.093524, validation/num_examples=50000
I0129 03:43:25.770191 140005288683264 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.609459400177002, loss=1.1704272031784058
I0129 03:43:59.264939 140004624946944 logging_writer.py:48] [144400] global_step=144400, grad_norm=7.48278284072876, loss=1.1657886505126953
I0129 03:44:32.768410 140005288683264 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.497854232788086, loss=1.1428747177124023
I0129 03:45:06.313232 140004624946944 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.419370651245117, loss=1.080558180809021
I0129 03:45:39.912200 140005288683264 logging_writer.py:48] [144700] global_step=144700, grad_norm=7.370270252227783, loss=1.2479095458984375
I0129 03:46:13.578980 140004624946944 logging_writer.py:48] [144800] global_step=144800, grad_norm=7.691379547119141, loss=1.3014169931411743
I0129 03:46:47.143982 140005288683264 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.44524621963501, loss=1.1697381734848022
I0129 03:47:20.686274 140004624946944 logging_writer.py:48] [145000] global_step=145000, grad_norm=7.094226360321045, loss=1.1743861436843872
I0129 03:47:54.227688 140005288683264 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.743523597717285, loss=1.1363738775253296
I0129 03:48:27.747990 140004624946944 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.990379333496094, loss=1.137534499168396
I0129 03:49:01.296615 140005288683264 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.441129207611084, loss=1.0457983016967773
I0129 03:49:34.831168 140004624946944 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.917733669281006, loss=1.0777201652526855
I0129 03:50:08.374159 140005288683264 logging_writer.py:48] [145500] global_step=145500, grad_norm=7.65036153793335, loss=1.193753957748413
I0129 03:50:41.932152 140004624946944 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.561567783355713, loss=1.0466655492782593
I0129 03:51:15.415965 140005288683264 logging_writer.py:48] [145700] global_step=145700, grad_norm=7.61127233505249, loss=1.1344634294509888
I0129 03:51:49.002708 140004624946944 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.640248775482178, loss=1.2149595022201538
I0129 03:51:54.194678 140169137129280 spec.py:321] Evaluating on the training split.
I0129 03:52:00.514648 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 03:52:09.584253 140169137129280 spec.py:349] Evaluating on the test split.
I0129 03:52:12.282325 140169137129280 submission_runner.py:408] Time since start: 50744.13s, 	Step: 145817, 	{'train/accuracy': 0.8562061190605164, 'train/loss': 0.5220614075660706, 'validation/accuracy': 0.7297799587249756, 'validation/loss': 1.0798618793487549, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.7527598142623901, 'test/num_examples': 10000, 'score': 48998.324323415756, 'total_duration': 50744.13141441345, 'accumulated_submission_time': 48998.324323415756, 'accumulated_eval_time': 1736.8405735492706, 'accumulated_logging_time': 4.234951734542847}
I0129 03:52:12.327342 140005297075968 logging_writer.py:48] [145817] accumulated_eval_time=1736.840574, accumulated_logging_time=4.234952, accumulated_submission_time=48998.324323, global_step=145817, preemption_count=0, score=48998.324323, test/accuracy=0.605300, test/loss=1.752760, test/num_examples=10000, total_duration=50744.131414, train/accuracy=0.856206, train/loss=0.522061, validation/accuracy=0.729780, validation/loss=1.079862, validation/num_examples=50000
I0129 03:52:40.568969 140005305468672 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.759798526763916, loss=0.9967864751815796
I0129 03:53:14.113417 140005297075968 logging_writer.py:48] [146000] global_step=146000, grad_norm=7.462428092956543, loss=1.142940640449524
I0129 03:53:47.702348 140005305468672 logging_writer.py:48] [146100] global_step=146100, grad_norm=7.27805233001709, loss=1.191185474395752
I0129 03:54:21.222920 140005297075968 logging_writer.py:48] [146200] global_step=146200, grad_norm=6.648167610168457, loss=1.1256848573684692
I0129 03:54:54.731741 140005305468672 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.838798999786377, loss=1.1271988153457642
I0129 03:55:28.326432 140005297075968 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.5501627922058105, loss=1.1268911361694336
I0129 03:56:01.928683 140005305468672 logging_writer.py:48] [146500] global_step=146500, grad_norm=7.209006309509277, loss=1.0915417671203613
I0129 03:56:35.557734 140005297075968 logging_writer.py:48] [146600] global_step=146600, grad_norm=7.206860065460205, loss=1.163872241973877
I0129 03:57:09.173315 140005305468672 logging_writer.py:48] [146700] global_step=146700, grad_norm=7.37335205078125, loss=1.1367567777633667
I0129 03:57:42.789314 140005297075968 logging_writer.py:48] [146800] global_step=146800, grad_norm=7.514644145965576, loss=1.1256849765777588
I0129 03:58:16.388088 140005305468672 logging_writer.py:48] [146900] global_step=146900, grad_norm=7.205301284790039, loss=1.1894538402557373
I0129 03:58:50.009766 140005297075968 logging_writer.py:48] [147000] global_step=147000, grad_norm=7.016845703125, loss=1.1472336053848267
I0129 03:59:23.593330 140005305468672 logging_writer.py:48] [147100] global_step=147100, grad_norm=7.907372951507568, loss=1.153844952583313
I0129 03:59:57.113958 140005297075968 logging_writer.py:48] [147200] global_step=147200, grad_norm=7.174968242645264, loss=1.0347827672958374
I0129 04:00:30.610254 140005305468672 logging_writer.py:48] [147300] global_step=147300, grad_norm=7.557976722717285, loss=1.1213715076446533
I0129 04:00:42.522738 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:00:48.873068 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:00:57.667386 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:01:00.300425 140169137129280 submission_runner.py:408] Time since start: 51272.15s, 	Step: 147337, 	{'train/accuracy': 0.8483338356018066, 'train/loss': 0.5386354327201843, 'validation/accuracy': 0.7303999662399292, 'validation/loss': 1.0842643976211548, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.7619547843933105, 'test/num_examples': 10000, 'score': 49508.459768772125, 'total_duration': 51272.14950942993, 'accumulated_submission_time': 49508.459768772125, 'accumulated_eval_time': 1754.6182186603546, 'accumulated_logging_time': 4.2909040451049805}
I0129 04:01:00.345788 140004616554240 logging_writer.py:48] [147337] accumulated_eval_time=1754.618219, accumulated_logging_time=4.290904, accumulated_submission_time=49508.459769, global_step=147337, preemption_count=0, score=49508.459769, test/accuracy=0.604200, test/loss=1.761955, test/num_examples=10000, total_duration=51272.149509, train/accuracy=0.848334, train/loss=0.538635, validation/accuracy=0.730400, validation/loss=1.084264, validation/num_examples=50000
I0129 04:01:21.786396 140004624946944 logging_writer.py:48] [147400] global_step=147400, grad_norm=7.511075496673584, loss=1.1951431035995483
I0129 04:01:55.298342 140004616554240 logging_writer.py:48] [147500] global_step=147500, grad_norm=7.051072597503662, loss=1.126571536064148
I0129 04:02:28.819568 140004624946944 logging_writer.py:48] [147600] global_step=147600, grad_norm=7.052845478057861, loss=1.0551095008850098
I0129 04:03:02.351922 140004616554240 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.5654296875, loss=1.0890611410140991
I0129 04:03:35.954159 140004624946944 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.980404853820801, loss=1.0676498413085938
I0129 04:04:09.571126 140004616554240 logging_writer.py:48] [147900] global_step=147900, grad_norm=7.219648361206055, loss=1.1226941347122192
I0129 04:04:43.183351 140004624946944 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.805046558380127, loss=1.0849920511245728
I0129 04:05:16.831810 140004616554240 logging_writer.py:48] [148100] global_step=148100, grad_norm=6.534606456756592, loss=1.0797420740127563
I0129 04:05:50.394020 140004624946944 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.709962368011475, loss=1.0596129894256592
I0129 04:06:24.015292 140004616554240 logging_writer.py:48] [148300] global_step=148300, grad_norm=7.333181858062744, loss=1.0805498361587524
I0129 04:06:57.634170 140004624946944 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.97672700881958, loss=1.1271576881408691
I0129 04:07:31.202975 140004616554240 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.761244297027588, loss=1.0623985528945923
I0129 04:08:04.756105 140004624946944 logging_writer.py:48] [148600] global_step=148600, grad_norm=8.029650688171387, loss=1.1689963340759277
I0129 04:08:38.269528 140004616554240 logging_writer.py:48] [148700] global_step=148700, grad_norm=7.365466117858887, loss=1.1538469791412354
I0129 04:09:11.840842 140004624946944 logging_writer.py:48] [148800] global_step=148800, grad_norm=8.074934959411621, loss=1.1928980350494385
I0129 04:09:30.459549 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:09:36.834139 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:09:45.394083 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:09:48.034075 140169137129280 submission_runner.py:408] Time since start: 51799.88s, 	Step: 148857, 	{'train/accuracy': 0.8486328125, 'train/loss': 0.5442114472389221, 'validation/accuracy': 0.7340599894523621, 'validation/loss': 1.065670132637024, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.7632725238800049, 'test/num_examples': 10000, 'score': 50018.51382923126, 'total_duration': 51799.88316822052, 'accumulated_submission_time': 50018.51382923126, 'accumulated_eval_time': 1772.1927328109741, 'accumulated_logging_time': 4.347080707550049}
I0129 04:09:48.079743 140004616554240 logging_writer.py:48] [148857] accumulated_eval_time=1772.192733, accumulated_logging_time=4.347081, accumulated_submission_time=50018.513829, global_step=148857, preemption_count=0, score=50018.513829, test/accuracy=0.606500, test/loss=1.763273, test/num_examples=10000, total_duration=51799.883168, train/accuracy=0.848633, train/loss=0.544211, validation/accuracy=0.734060, validation/loss=1.065670, validation/num_examples=50000
I0129 04:10:02.856952 140005297075968 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.081242084503174, loss=1.0533732175827026
I0129 04:10:36.353792 140004616554240 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.909092426300049, loss=1.0684995651245117
I0129 04:11:09.908385 140005297075968 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.506568908691406, loss=0.9856514930725098
I0129 04:11:43.584393 140004616554240 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.770712375640869, loss=1.0591309070587158
I0129 04:12:17.124164 140005297075968 logging_writer.py:48] [149300] global_step=149300, grad_norm=7.257599353790283, loss=1.0888605117797852
I0129 04:12:50.709105 140004616554240 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.650641441345215, loss=1.1098244190216064
I0129 04:13:24.334254 140005297075968 logging_writer.py:48] [149500] global_step=149500, grad_norm=7.324781894683838, loss=1.1034425497055054
I0129 04:13:57.923999 140004616554240 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.01937198638916, loss=1.1698113679885864
I0129 04:14:31.432696 140005297075968 logging_writer.py:48] [149700] global_step=149700, grad_norm=6.900874614715576, loss=1.092310905456543
I0129 04:15:04.967296 140004616554240 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.952593803405762, loss=1.2039412260055542
I0129 04:15:38.595518 140005297075968 logging_writer.py:48] [149900] global_step=149900, grad_norm=7.038806438446045, loss=1.0941060781478882
I0129 04:16:12.179898 140004616554240 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.140382289886475, loss=1.1741764545440674
I0129 04:16:45.751260 140005297075968 logging_writer.py:48] [150100] global_step=150100, grad_norm=6.893857955932617, loss=1.0327003002166748
I0129 04:17:19.313734 140004616554240 logging_writer.py:48] [150200] global_step=150200, grad_norm=7.1766791343688965, loss=1.1085904836654663
I0129 04:17:52.942513 140005297075968 logging_writer.py:48] [150300] global_step=150300, grad_norm=8.553600311279297, loss=1.1228961944580078
I0129 04:18:18.276767 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:18:24.638902 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:18:33.380666 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:18:36.030533 140169137129280 submission_runner.py:408] Time since start: 52327.88s, 	Step: 150377, 	{'train/accuracy': 0.8487922549247742, 'train/loss': 0.5424256324768066, 'validation/accuracy': 0.7318800091743469, 'validation/loss': 1.0758512020111084, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.749652624130249, 'test/num_examples': 10000, 'score': 50528.65114068985, 'total_duration': 52327.87962150574, 'accumulated_submission_time': 50528.65114068985, 'accumulated_eval_time': 1789.946456670761, 'accumulated_logging_time': 4.4032227993011475}
I0129 04:18:36.075190 140004608161536 logging_writer.py:48] [150377] accumulated_eval_time=1789.946457, accumulated_logging_time=4.403223, accumulated_submission_time=50528.651141, global_step=150377, preemption_count=0, score=50528.651141, test/accuracy=0.609300, test/loss=1.749653, test/num_examples=10000, total_duration=52327.879622, train/accuracy=0.848792, train/loss=0.542426, validation/accuracy=0.731880, validation/loss=1.075851, validation/num_examples=50000
I0129 04:18:44.130359 140004616554240 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.665934085845947, loss=1.0611636638641357
I0129 04:19:17.586771 140004608161536 logging_writer.py:48] [150500] global_step=150500, grad_norm=7.234554290771484, loss=1.016961693763733
I0129 04:19:51.101662 140004616554240 logging_writer.py:48] [150600] global_step=150600, grad_norm=8.098671913146973, loss=1.1290874481201172
I0129 04:20:24.583199 140004608161536 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.971371173858643, loss=1.0684516429901123
I0129 04:20:58.135902 140004616554240 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.621969699859619, loss=1.0569218397140503
I0129 04:21:31.744893 140004608161536 logging_writer.py:48] [150900] global_step=150900, grad_norm=7.420395374298096, loss=1.0819834470748901
I0129 04:22:05.278209 140004616554240 logging_writer.py:48] [151000] global_step=151000, grad_norm=7.615633010864258, loss=1.0130093097686768
I0129 04:22:38.868789 140004608161536 logging_writer.py:48] [151100] global_step=151100, grad_norm=7.224924564361572, loss=1.0141315460205078
I0129 04:23:12.442605 140004616554240 logging_writer.py:48] [151200] global_step=151200, grad_norm=7.202139854431152, loss=1.0384533405303955
I0129 04:23:45.945777 140004608161536 logging_writer.py:48] [151300] global_step=151300, grad_norm=7.636784553527832, loss=1.0241690874099731
I0129 04:24:19.566064 140004616554240 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.950531482696533, loss=1.0167783498764038
I0129 04:24:53.062641 140004608161536 logging_writer.py:48] [151500] global_step=151500, grad_norm=7.745316505432129, loss=1.1307154893875122
I0129 04:25:26.598691 140004616554240 logging_writer.py:48] [151600] global_step=151600, grad_norm=7.05170202255249, loss=1.1503486633300781
I0129 04:26:00.186972 140004608161536 logging_writer.py:48] [151700] global_step=151700, grad_norm=7.665074348449707, loss=1.0737580060958862
I0129 04:26:33.748016 140004616554240 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.461275577545166, loss=1.0152024030685425
I0129 04:27:06.160551 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:27:12.506146 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:27:21.330581 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:27:23.954773 140169137129280 submission_runner.py:408] Time since start: 52855.80s, 	Step: 151898, 	{'train/accuracy': 0.8529974222183228, 'train/loss': 0.52959805727005, 'validation/accuracy': 0.7369999885559082, 'validation/loss': 1.0513129234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.7403059005737305, 'test/num_examples': 10000, 'score': 51038.67733478546, 'total_duration': 52855.8038623333, 'accumulated_submission_time': 51038.67733478546, 'accumulated_eval_time': 1807.7406420707703, 'accumulated_logging_time': 4.458520889282227}
I0129 04:27:24.001056 140005297075968 logging_writer.py:48] [151898] accumulated_eval_time=1807.740642, accumulated_logging_time=4.458521, accumulated_submission_time=51038.677335, global_step=151898, preemption_count=0, score=51038.677335, test/accuracy=0.614200, test/loss=1.740306, test/num_examples=10000, total_duration=52855.803862, train/accuracy=0.852997, train/loss=0.529598, validation/accuracy=0.737000, validation/loss=1.051313, validation/num_examples=50000
I0129 04:27:25.042431 140005305468672 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.9163360595703125, loss=0.9972250461578369
I0129 04:27:58.548959 140005297075968 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.947622299194336, loss=1.005370020866394
I0129 04:28:32.088646 140005305468672 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.629450798034668, loss=1.0296159982681274
I0129 04:29:05.594860 140005297075968 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.558769702911377, loss=1.0119677782058716
I0129 04:29:39.121882 140005305468672 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.753772735595703, loss=1.0172779560089111
I0129 04:30:12.671772 140005297075968 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.3486127853393555, loss=0.9639098048210144
I0129 04:30:46.298282 140005305468672 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.053038597106934, loss=0.9622002840042114
I0129 04:31:19.917204 140005297075968 logging_writer.py:48] [152600] global_step=152600, grad_norm=7.0433220863342285, loss=1.067124366760254
I0129 04:31:53.500593 140005305468672 logging_writer.py:48] [152700] global_step=152700, grad_norm=6.98106575012207, loss=1.0145145654678345
I0129 04:32:27.033866 140005297075968 logging_writer.py:48] [152800] global_step=152800, grad_norm=7.5770416259765625, loss=1.0699234008789062
I0129 04:33:00.545867 140005305468672 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.099140644073486, loss=1.0568325519561768
I0129 04:33:34.134722 140005297075968 logging_writer.py:48] [153000] global_step=153000, grad_norm=7.44893741607666, loss=1.0188908576965332
I0129 04:34:07.730658 140005305468672 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.289495468139648, loss=1.0663018226623535
I0129 04:34:41.346961 140005297075968 logging_writer.py:48] [153200] global_step=153200, grad_norm=7.687516212463379, loss=1.0045127868652344
I0129 04:35:14.957328 140005305468672 logging_writer.py:48] [153300] global_step=153300, grad_norm=7.57017707824707, loss=1.180687665939331
I0129 04:35:48.587511 140005297075968 logging_writer.py:48] [153400] global_step=153400, grad_norm=8.14163875579834, loss=1.042283296585083
I0129 04:35:54.119543 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:36:00.472215 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:36:09.273062 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:36:11.944312 140169137129280 submission_runner.py:408] Time since start: 53383.79s, 	Step: 153418, 	{'train/accuracy': 0.8550103306770325, 'train/loss': 0.5078051090240479, 'validation/accuracy': 0.7404800057411194, 'validation/loss': 1.0449867248535156, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.75309419631958, 'test/num_examples': 10000, 'score': 51548.73529744148, 'total_duration': 53383.79339146614, 'accumulated_submission_time': 51548.73529744148, 'accumulated_eval_time': 1825.565360069275, 'accumulated_logging_time': 4.516251564025879}
I0129 04:36:11.991849 140004624946944 logging_writer.py:48] [153418] accumulated_eval_time=1825.565360, accumulated_logging_time=4.516252, accumulated_submission_time=51548.735297, global_step=153418, preemption_count=0, score=51548.735297, test/accuracy=0.609000, test/loss=1.753094, test/num_examples=10000, total_duration=53383.793391, train/accuracy=0.855010, train/loss=0.507805, validation/accuracy=0.740480, validation/loss=1.044987, validation/num_examples=50000
I0129 04:36:39.875912 140005288683264 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.12092924118042, loss=1.0514020919799805
I0129 04:37:13.405375 140004624946944 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.011548042297363, loss=0.9629218578338623
I0129 04:37:46.959072 140005288683264 logging_writer.py:48] [153700] global_step=153700, grad_norm=7.974982261657715, loss=1.0213770866394043
I0129 04:38:20.448630 140004624946944 logging_writer.py:48] [153800] global_step=153800, grad_norm=7.253876686096191, loss=1.042557716369629
I0129 04:38:54.020382 140005288683264 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.303552627563477, loss=0.9715157151222229
I0129 04:39:27.617648 140004624946944 logging_writer.py:48] [154000] global_step=154000, grad_norm=7.760364055633545, loss=1.00991952419281
I0129 04:40:01.153624 140005288683264 logging_writer.py:48] [154100] global_step=154100, grad_norm=7.45872163772583, loss=1.0856513977050781
I0129 04:40:34.656519 140004624946944 logging_writer.py:48] [154200] global_step=154200, grad_norm=8.02063274383545, loss=1.1070544719696045
I0129 04:41:08.227555 140005288683264 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.404534816741943, loss=0.998714804649353
I0129 04:41:41.842306 140004624946944 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.4139404296875, loss=1.0281950235366821
I0129 04:42:15.471332 140005288683264 logging_writer.py:48] [154500] global_step=154500, grad_norm=6.976119518280029, loss=1.0177398920059204
I0129 04:42:49.084215 140004624946944 logging_writer.py:48] [154600] global_step=154600, grad_norm=7.1222758293151855, loss=1.0117405652999878
I0129 04:43:22.714960 140005288683264 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.787439346313477, loss=1.110429048538208
I0129 04:43:56.302602 140004624946944 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.784028053283691, loss=0.9576313495635986
I0129 04:44:29.902981 140005288683264 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.181573390960693, loss=0.9837691187858582
I0129 04:44:42.148275 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:44:48.512091 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:44:57.139775 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:44:59.766351 140169137129280 submission_runner.py:408] Time since start: 53911.62s, 	Step: 154938, 	{'train/accuracy': 0.8839883208274841, 'train/loss': 0.40737801790237427, 'validation/accuracy': 0.7401799559593201, 'validation/loss': 1.042005181312561, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.7246414422988892, 'test/num_examples': 10000, 'score': 52058.83040165901, 'total_duration': 53911.61543941498, 'accumulated_submission_time': 52058.83040165901, 'accumulated_eval_time': 1843.1833944320679, 'accumulated_logging_time': 4.576179504394531}
I0129 04:44:59.816163 140005313861376 logging_writer.py:48] [154938] accumulated_eval_time=1843.183394, accumulated_logging_time=4.576180, accumulated_submission_time=52058.830402, global_step=154938, preemption_count=0, score=52058.830402, test/accuracy=0.617400, test/loss=1.724641, test/num_examples=10000, total_duration=53911.615439, train/accuracy=0.883988, train/loss=0.407378, validation/accuracy=0.740180, validation/loss=1.042005, validation/num_examples=50000
I0129 04:45:20.924406 140005322254080 logging_writer.py:48] [155000] global_step=155000, grad_norm=7.799230575561523, loss=1.0680745840072632
I0129 04:45:54.371627 140005313861376 logging_writer.py:48] [155100] global_step=155100, grad_norm=8.665992736816406, loss=1.1090278625488281
I0129 04:46:27.895035 140005322254080 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.853652477264404, loss=0.975721538066864
I0129 04:47:01.391158 140005313861376 logging_writer.py:48] [155300] global_step=155300, grad_norm=7.458855152130127, loss=0.9582730531692505
I0129 04:47:34.937768 140005322254080 logging_writer.py:48] [155400] global_step=155400, grad_norm=8.173073768615723, loss=0.9844105243682861
I0129 04:48:08.543246 140005313861376 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.908864498138428, loss=0.927739679813385
I0129 04:48:42.087199 140005322254080 logging_writer.py:48] [155600] global_step=155600, grad_norm=7.790014743804932, loss=0.969991147518158
I0129 04:49:15.712728 140005313861376 logging_writer.py:48] [155700] global_step=155700, grad_norm=7.122811317443848, loss=1.0097051858901978
I0129 04:49:49.267058 140005322254080 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.996429920196533, loss=0.9648853540420532
I0129 04:50:22.779618 140005313861376 logging_writer.py:48] [155900] global_step=155900, grad_norm=8.798489570617676, loss=0.9982407093048096
I0129 04:50:56.304488 140005322254080 logging_writer.py:48] [156000] global_step=156000, grad_norm=7.258814811706543, loss=0.9835826754570007
I0129 04:51:29.848479 140005313861376 logging_writer.py:48] [156100] global_step=156100, grad_norm=7.580929279327393, loss=1.0418405532836914
I0129 04:52:03.391229 140005322254080 logging_writer.py:48] [156200] global_step=156200, grad_norm=8.969956398010254, loss=1.03987455368042
I0129 04:52:36.904081 140005313861376 logging_writer.py:48] [156300] global_step=156300, grad_norm=8.103520393371582, loss=0.9562722444534302
I0129 04:53:10.446509 140005322254080 logging_writer.py:48] [156400] global_step=156400, grad_norm=7.1756205558776855, loss=0.9708073139190674
I0129 04:53:30.037725 140169137129280 spec.py:321] Evaluating on the training split.
I0129 04:53:36.449117 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 04:53:45.047859 140169137129280 spec.py:349] Evaluating on the test split.
I0129 04:53:47.679136 140169137129280 submission_runner.py:408] Time since start: 54439.53s, 	Step: 156460, 	{'train/accuracy': 0.8816167116165161, 'train/loss': 0.4281372129917145, 'validation/accuracy': 0.7430399656295776, 'validation/loss': 1.0246366262435913, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.7108631134033203, 'test/num_examples': 10000, 'score': 52568.992547512054, 'total_duration': 54439.52820849419, 'accumulated_submission_time': 52568.992547512054, 'accumulated_eval_time': 1860.8247520923615, 'accumulated_logging_time': 4.636116027832031}
I0129 04:53:47.723114 140004624946944 logging_writer.py:48] [156460] accumulated_eval_time=1860.824752, accumulated_logging_time=4.636116, accumulated_submission_time=52568.992548, global_step=156460, preemption_count=0, score=52568.992548, test/accuracy=0.619100, test/loss=1.710863, test/num_examples=10000, total_duration=54439.528208, train/accuracy=0.881617, train/loss=0.428137, validation/accuracy=0.743040, validation/loss=1.024637, validation/num_examples=50000
I0129 04:54:01.487740 140005288683264 logging_writer.py:48] [156500] global_step=156500, grad_norm=8.048233032226562, loss=0.936128556728363
I0129 04:54:34.962649 140004624946944 logging_writer.py:48] [156600] global_step=156600, grad_norm=7.7292351722717285, loss=0.9317548871040344
I0129 04:55:08.552705 140005288683264 logging_writer.py:48] [156700] global_step=156700, grad_norm=7.671839714050293, loss=0.9176017045974731
I0129 04:55:42.207807 140004624946944 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.74038553237915, loss=0.9984416365623474
I0129 04:56:15.773955 140005288683264 logging_writer.py:48] [156900] global_step=156900, grad_norm=8.071154594421387, loss=0.9859293699264526
I0129 04:56:49.371623 140004624946944 logging_writer.py:48] [157000] global_step=157000, grad_norm=8.049398422241211, loss=1.0697433948516846
I0129 04:57:22.991595 140005288683264 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.846314907073975, loss=0.917184591293335
I0129 04:57:56.541492 140004624946944 logging_writer.py:48] [157200] global_step=157200, grad_norm=7.77987003326416, loss=0.9725199341773987
I0129 04:58:30.038677 140005288683264 logging_writer.py:48] [157300] global_step=157300, grad_norm=8.847233772277832, loss=1.0442308187484741
I0129 04:59:03.581268 140004624946944 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.27468204498291, loss=0.9422301650047302
I0129 04:59:37.181343 140005288683264 logging_writer.py:48] [157500] global_step=157500, grad_norm=8.411144256591797, loss=0.867590606212616
I0129 05:00:10.793337 140004624946944 logging_writer.py:48] [157600] global_step=157600, grad_norm=8.336881637573242, loss=0.9858216047286987
I0129 05:00:44.413352 140005288683264 logging_writer.py:48] [157700] global_step=157700, grad_norm=8.338647842407227, loss=0.8981284499168396
I0129 05:01:17.967402 140004624946944 logging_writer.py:48] [157800] global_step=157800, grad_norm=8.329649925231934, loss=0.9937744140625
I0129 05:01:51.588154 140005288683264 logging_writer.py:48] [157900] global_step=157900, grad_norm=8.330663681030273, loss=0.9870262145996094
I0129 05:02:17.896887 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:02:24.272049 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:02:32.955266 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:02:35.617268 140169137129280 submission_runner.py:408] Time since start: 54967.47s, 	Step: 157980, 	{'train/accuracy': 0.8804408311843872, 'train/loss': 0.4245630204677582, 'validation/accuracy': 0.7468999624252319, 'validation/loss': 1.0233200788497925, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.7170692682266235, 'test/num_examples': 10000, 'score': 53079.10682106018, 'total_duration': 54967.46635508537, 'accumulated_submission_time': 53079.10682106018, 'accumulated_eval_time': 1878.545128583908, 'accumulated_logging_time': 4.690703630447388}
I0129 05:02:35.665086 140004616554240 logging_writer.py:48] [157980] accumulated_eval_time=1878.545129, accumulated_logging_time=4.690704, accumulated_submission_time=53079.106821, global_step=157980, preemption_count=0, score=53079.106821, test/accuracy=0.618300, test/loss=1.717069, test/num_examples=10000, total_duration=54967.466355, train/accuracy=0.880441, train/loss=0.424563, validation/accuracy=0.746900, validation/loss=1.023320, validation/num_examples=50000
I0129 05:02:42.701552 140004624946944 logging_writer.py:48] [158000] global_step=158000, grad_norm=7.73433780670166, loss=1.0217424631118774
I0129 05:03:16.167526 140004616554240 logging_writer.py:48] [158100] global_step=158100, grad_norm=8.21497917175293, loss=0.9685370922088623
I0129 05:03:49.637930 140004624946944 logging_writer.py:48] [158200] global_step=158200, grad_norm=8.071317672729492, loss=0.9902700185775757
I0129 05:04:23.177220 140004616554240 logging_writer.py:48] [158300] global_step=158300, grad_norm=8.228374481201172, loss=0.9030028581619263
I0129 05:04:56.796469 140004624946944 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.784783840179443, loss=0.9177983999252319
I0129 05:05:30.385796 140004616554240 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.4613518714904785, loss=0.9125708937644958
I0129 05:06:03.903168 140004624946944 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.822880744934082, loss=0.9197035431861877
I0129 05:06:37.432317 140004616554240 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.7184977531433105, loss=0.9697308540344238
I0129 05:07:11.028631 140004624946944 logging_writer.py:48] [158800] global_step=158800, grad_norm=8.99389934539795, loss=1.0052378177642822
I0129 05:07:44.627611 140004616554240 logging_writer.py:48] [158900] global_step=158900, grad_norm=8.050898551940918, loss=1.0546457767486572
I0129 05:08:18.276470 140004624946944 logging_writer.py:48] [159000] global_step=159000, grad_norm=7.650730133056641, loss=0.9631582498550415
I0129 05:08:51.850763 140004616554240 logging_writer.py:48] [159100] global_step=159100, grad_norm=8.013670921325684, loss=1.0125833749771118
I0129 05:09:25.461887 140004624946944 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.2948222160339355, loss=0.9281474351882935
I0129 05:09:59.023757 140004616554240 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.8761396408081055, loss=0.9612565040588379
I0129 05:10:32.548382 140004624946944 logging_writer.py:48] [159400] global_step=159400, grad_norm=8.436351776123047, loss=0.9849794507026672
I0129 05:11:06.067311 140004616554240 logging_writer.py:48] [159500] global_step=159500, grad_norm=8.594856262207031, loss=0.9251507520675659
I0129 05:11:06.075553 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:11:12.388950 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:11:21.129147 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:11:23.794874 140169137129280 submission_runner.py:408] Time since start: 55495.64s, 	Step: 159501, 	{'train/accuracy': 0.8826530575752258, 'train/loss': 0.41485026478767395, 'validation/accuracy': 0.7451199889183044, 'validation/loss': 1.0272034406661987, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.7209504842758179, 'test/num_examples': 10000, 'score': 53589.45731592178, 'total_duration': 55495.64396739006, 'accumulated_submission_time': 53589.45731592178, 'accumulated_eval_time': 1896.2643899917603, 'accumulated_logging_time': 4.749074697494507}
I0129 05:11:23.848232 140005313861376 logging_writer.py:48] [159501] accumulated_eval_time=1896.264390, accumulated_logging_time=4.749075, accumulated_submission_time=53589.457316, global_step=159501, preemption_count=0, score=53589.457316, test/accuracy=0.622300, test/loss=1.720950, test/num_examples=10000, total_duration=55495.643967, train/accuracy=0.882653, train/loss=0.414850, validation/accuracy=0.745120, validation/loss=1.027203, validation/num_examples=50000
I0129 05:11:57.342795 140005322254080 logging_writer.py:48] [159600] global_step=159600, grad_norm=7.655699729919434, loss=0.9041875600814819
I0129 05:12:30.863956 140005313861376 logging_writer.py:48] [159700] global_step=159700, grad_norm=8.046014785766602, loss=0.9229463338851929
I0129 05:13:04.431914 140005322254080 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.361179351806641, loss=0.9460289478302002
I0129 05:13:37.920771 140005313861376 logging_writer.py:48] [159900] global_step=159900, grad_norm=8.399784088134766, loss=0.9132293462753296
I0129 05:14:11.503370 140005322254080 logging_writer.py:48] [160000] global_step=160000, grad_norm=8.229828834533691, loss=1.0370539426803589
I0129 05:14:45.140094 140005313861376 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.778812885284424, loss=0.9279624819755554
I0129 05:15:18.727551 140005322254080 logging_writer.py:48] [160200] global_step=160200, grad_norm=8.783557891845703, loss=1.12139892578125
I0129 05:15:52.282892 140005313861376 logging_writer.py:48] [160300] global_step=160300, grad_norm=8.252551078796387, loss=0.9743258953094482
I0129 05:16:25.767519 140005322254080 logging_writer.py:48] [160400] global_step=160400, grad_norm=8.594329833984375, loss=0.957503080368042
I0129 05:16:59.363167 140005313861376 logging_writer.py:48] [160500] global_step=160500, grad_norm=8.1846342086792, loss=0.9035046696662903
I0129 05:17:32.957003 140005322254080 logging_writer.py:48] [160600] global_step=160600, grad_norm=9.124528884887695, loss=0.9859742522239685
I0129 05:18:06.494514 140005313861376 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.359481811523438, loss=0.9975139498710632
I0129 05:18:39.998724 140005322254080 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.691263198852539, loss=0.9463871121406555
I0129 05:19:13.569603 140005313861376 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.896626949310303, loss=0.918059229850769
I0129 05:19:47.157287 140005322254080 logging_writer.py:48] [161000] global_step=161000, grad_norm=7.365194797515869, loss=0.8169583678245544
I0129 05:19:54.014083 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:20:00.367254 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:20:09.179700 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:20:11.829972 140169137129280 submission_runner.py:408] Time since start: 56023.68s, 	Step: 161022, 	{'train/accuracy': 0.8878945708274841, 'train/loss': 0.39847174286842346, 'validation/accuracy': 0.7486799955368042, 'validation/loss': 1.0120301246643066, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.695212960243225, 'test/num_examples': 10000, 'score': 54099.56357502937, 'total_duration': 56023.67906188965, 'accumulated_submission_time': 54099.56357502937, 'accumulated_eval_time': 1914.0802400112152, 'accumulated_logging_time': 4.812868118286133}
I0129 05:20:11.874781 140004624946944 logging_writer.py:48] [161022] accumulated_eval_time=1914.080240, accumulated_logging_time=4.812868, accumulated_submission_time=54099.563575, global_step=161022, preemption_count=0, score=54099.563575, test/accuracy=0.624700, test/loss=1.695213, test/num_examples=10000, total_duration=56023.679062, train/accuracy=0.887895, train/loss=0.398472, validation/accuracy=0.748680, validation/loss=1.012030, validation/num_examples=50000
I0129 05:20:38.361376 140005288683264 logging_writer.py:48] [161100] global_step=161100, grad_norm=7.9944891929626465, loss=0.9079721570014954
I0129 05:21:11.984584 140004624946944 logging_writer.py:48] [161200] global_step=161200, grad_norm=8.503497123718262, loss=0.9788375496864319
I0129 05:21:45.603157 140005288683264 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.98238468170166, loss=0.9643076658248901
I0129 05:22:19.204569 140004624946944 logging_writer.py:48] [161400] global_step=161400, grad_norm=8.64042854309082, loss=0.8852930068969727
I0129 05:22:52.831445 140005288683264 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.674361228942871, loss=0.9137064814567566
I0129 05:23:26.443978 140004624946944 logging_writer.py:48] [161600] global_step=161600, grad_norm=8.194473266601562, loss=0.8768035769462585
I0129 05:24:00.057677 140005288683264 logging_writer.py:48] [161700] global_step=161700, grad_norm=7.847772598266602, loss=0.9687901139259338
I0129 05:24:33.599805 140004624946944 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.547700881958008, loss=0.9311414957046509
I0129 05:25:07.193150 140005288683264 logging_writer.py:48] [161900] global_step=161900, grad_norm=8.021890640258789, loss=0.9266467094421387
I0129 05:25:40.795145 140004624946944 logging_writer.py:48] [162000] global_step=162000, grad_norm=8.34665298461914, loss=0.9296392202377319
I0129 05:26:14.427754 140005288683264 logging_writer.py:48] [162100] global_step=162100, grad_norm=8.219186782836914, loss=0.8892532587051392
I0129 05:26:48.037838 140004624946944 logging_writer.py:48] [162200] global_step=162200, grad_norm=8.550978660583496, loss=0.9287102222442627
I0129 05:27:21.678344 140005288683264 logging_writer.py:48] [162300] global_step=162300, grad_norm=8.158390998840332, loss=0.9015324711799622
I0129 05:27:55.217984 140004624946944 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.9325175285339355, loss=0.9160242080688477
I0129 05:28:28.776926 140005288683264 logging_writer.py:48] [162500] global_step=162500, grad_norm=7.764218807220459, loss=0.9281361103057861
I0129 05:28:42.023430 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:28:48.411691 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:28:57.443522 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:29:00.084661 140169137129280 submission_runner.py:408] Time since start: 56551.93s, 	Step: 162541, 	{'train/accuracy': 0.8908840417861938, 'train/loss': 0.3822648227214813, 'validation/accuracy': 0.751259982585907, 'validation/loss': 1.008744239807129, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.6943116188049316, 'test/num_examples': 10000, 'score': 54609.65087771416, 'total_duration': 56551.9337553978, 'accumulated_submission_time': 54609.65087771416, 'accumulated_eval_time': 1932.1414377689362, 'accumulated_logging_time': 4.868780136108398}
I0129 05:29:00.130257 140005313861376 logging_writer.py:48] [162541] accumulated_eval_time=1932.141438, accumulated_logging_time=4.868780, accumulated_submission_time=54609.650878, global_step=162541, preemption_count=0, score=54609.650878, test/accuracy=0.628500, test/loss=1.694312, test/num_examples=10000, total_duration=56551.933755, train/accuracy=0.890884, train/loss=0.382265, validation/accuracy=0.751260, validation/loss=1.008744, validation/num_examples=50000
I0129 05:29:20.219457 140005322254080 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.825351238250732, loss=0.846130907535553
I0129 05:29:53.749244 140005313861376 logging_writer.py:48] [162700] global_step=162700, grad_norm=8.742094039916992, loss=0.9328469038009644
I0129 05:30:27.294767 140005322254080 logging_writer.py:48] [162800] global_step=162800, grad_norm=8.63641357421875, loss=0.8934392929077148
I0129 05:31:00.800159 140005313861376 logging_writer.py:48] [162900] global_step=162900, grad_norm=9.732200622558594, loss=0.9519252777099609
I0129 05:31:34.357923 140005322254080 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.620716571807861, loss=0.8975877165794373
I0129 05:32:07.955339 140005313861376 logging_writer.py:48] [163100] global_step=163100, grad_norm=8.834877967834473, loss=0.8575602173805237
I0129 05:32:41.482183 140005322254080 logging_writer.py:48] [163200] global_step=163200, grad_norm=8.548215866088867, loss=0.8793196678161621
I0129 05:33:14.996972 140005313861376 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.165019989013672, loss=0.8619945049285889
I0129 05:33:48.603078 140005322254080 logging_writer.py:48] [163400] global_step=163400, grad_norm=8.524277687072754, loss=0.9333339929580688
I0129 05:34:22.110931 140005313861376 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.978542804718018, loss=0.8868106603622437
I0129 05:34:55.659939 140005322254080 logging_writer.py:48] [163600] global_step=163600, grad_norm=8.112935066223145, loss=0.8805227279663086
I0129 05:35:29.221817 140005313861376 logging_writer.py:48] [163700] global_step=163700, grad_norm=8.656164169311523, loss=0.8858790993690491
I0129 05:36:02.734543 140005322254080 logging_writer.py:48] [163800] global_step=163800, grad_norm=9.166006088256836, loss=0.8871316313743591
I0129 05:36:36.257210 140005313861376 logging_writer.py:48] [163900] global_step=163900, grad_norm=8.160123825073242, loss=0.8353116512298584
I0129 05:37:09.835514 140005322254080 logging_writer.py:48] [164000] global_step=164000, grad_norm=8.700835227966309, loss=0.9448555111885071
I0129 05:37:30.127837 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:37:36.504568 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:37:45.248576 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:37:47.902159 140169137129280 submission_runner.py:408] Time since start: 57079.75s, 	Step: 164062, 	{'train/accuracy': 0.9062101244926453, 'train/loss': 0.3281794488430023, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 1.0044119358062744, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.7114315032958984, 'test/num_examples': 10000, 'score': 55119.58855962753, 'total_duration': 57079.75123023987, 'accumulated_submission_time': 55119.58855962753, 'accumulated_eval_time': 1949.9157021045685, 'accumulated_logging_time': 4.9245476722717285}
I0129 05:37:47.950101 140005288683264 logging_writer.py:48] [164062] accumulated_eval_time=1949.915702, accumulated_logging_time=4.924548, accumulated_submission_time=55119.588560, global_step=164062, preemption_count=0, score=55119.588560, test/accuracy=0.623700, test/loss=1.711432, test/num_examples=10000, total_duration=57079.751230, train/accuracy=0.906210, train/loss=0.328179, validation/accuracy=0.752320, validation/loss=1.004412, validation/num_examples=50000
I0129 05:38:01.037653 140005297075968 logging_writer.py:48] [164100] global_step=164100, grad_norm=8.547369956970215, loss=0.9477749466896057
I0129 05:38:34.488920 140005288683264 logging_writer.py:48] [164200] global_step=164200, grad_norm=9.913176536560059, loss=0.9581100344657898
I0129 05:39:08.083756 140005297075968 logging_writer.py:48] [164300] global_step=164300, grad_norm=8.641831398010254, loss=0.8267869353294373
I0129 05:39:41.694459 140005288683264 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.185765266418457, loss=0.9129514694213867
I0129 05:40:15.334068 140005297075968 logging_writer.py:48] [164500] global_step=164500, grad_norm=7.576968193054199, loss=0.7697171568870544
I0129 05:40:48.931283 140005288683264 logging_writer.py:48] [164600] global_step=164600, grad_norm=8.086475372314453, loss=0.8698359727859497
I0129 05:41:22.518627 140005297075968 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.832254886627197, loss=0.923742413520813
I0129 05:41:56.029061 140005288683264 logging_writer.py:48] [164800] global_step=164800, grad_norm=8.834486961364746, loss=0.9734858870506287
I0129 05:42:29.553230 140005297075968 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.78657341003418, loss=0.8649234771728516
I0129 05:43:03.138560 140005288683264 logging_writer.py:48] [165000] global_step=165000, grad_norm=8.437173843383789, loss=0.8882428407669067
I0129 05:43:36.681807 140005297075968 logging_writer.py:48] [165100] global_step=165100, grad_norm=8.138433456420898, loss=0.8848796486854553
I0129 05:44:10.190045 140005288683264 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.9772820472717285, loss=0.8001899123191833
I0129 05:44:43.720213 140005297075968 logging_writer.py:48] [165300] global_step=165300, grad_norm=8.497664451599121, loss=0.9245448112487793
I0129 05:45:17.297089 140005288683264 logging_writer.py:48] [165400] global_step=165400, grad_norm=7.961734771728516, loss=0.8867401480674744
I0129 05:45:50.861807 140005297075968 logging_writer.py:48] [165500] global_step=165500, grad_norm=8.205657005310059, loss=0.8564153909683228
I0129 05:46:17.918321 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:46:24.381972 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:46:32.801709 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:46:35.458173 140169137129280 submission_runner.py:408] Time since start: 57607.31s, 	Step: 165582, 	{'train/accuracy': 0.9041573405265808, 'train/loss': 0.3352792263031006, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 0.9923732280731201, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.6897133588790894, 'test/num_examples': 10000, 'score': 55629.49757552147, 'total_duration': 57607.30726504326, 'accumulated_submission_time': 55629.49757552147, 'accumulated_eval_time': 1967.4555170536041, 'accumulated_logging_time': 4.982961177825928}
I0129 05:46:35.506536 140004624946944 logging_writer.py:48] [165582] accumulated_eval_time=1967.455517, accumulated_logging_time=4.982961, accumulated_submission_time=55629.497576, global_step=165582, preemption_count=0, score=55629.497576, test/accuracy=0.630000, test/loss=1.689713, test/num_examples=10000, total_duration=57607.307265, train/accuracy=0.904157, train/loss=0.335279, validation/accuracy=0.754660, validation/loss=0.992373, validation/num_examples=50000
I0129 05:46:41.887937 140005288683264 logging_writer.py:48] [165600] global_step=165600, grad_norm=8.791739463806152, loss=0.8174290657043457
I0129 05:47:15.363585 140004624946944 logging_writer.py:48] [165700] global_step=165700, grad_norm=8.185330390930176, loss=0.8442632555961609
I0129 05:47:48.877104 140005288683264 logging_writer.py:48] [165800] global_step=165800, grad_norm=8.330914497375488, loss=0.8610584139823914
I0129 05:48:22.435927 140004624946944 logging_writer.py:48] [165900] global_step=165900, grad_norm=7.885663986206055, loss=0.8375587463378906
I0129 05:48:56.041342 140005288683264 logging_writer.py:48] [166000] global_step=166000, grad_norm=9.092741012573242, loss=0.9163075685501099
I0129 05:49:29.567111 140004624946944 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.497652053833008, loss=0.7723374366760254
I0129 05:50:03.063002 140005288683264 logging_writer.py:48] [166200] global_step=166200, grad_norm=8.521185874938965, loss=0.8721799850463867
I0129 05:50:36.658510 140004624946944 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.986567497253418, loss=0.8151600360870361
I0129 05:51:10.262647 140005288683264 logging_writer.py:48] [166400] global_step=166400, grad_norm=8.366286277770996, loss=0.8414900302886963
I0129 05:51:43.878040 140004624946944 logging_writer.py:48] [166500] global_step=166500, grad_norm=9.51042366027832, loss=0.8649003505706787
I0129 05:52:17.551600 140005288683264 logging_writer.py:48] [166600] global_step=166600, grad_norm=8.429399490356445, loss=0.855025589466095
I0129 05:52:51.128916 140004624946944 logging_writer.py:48] [166700] global_step=166700, grad_norm=9.274033546447754, loss=0.8927023410797119
I0129 05:53:24.698025 140005288683264 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.286130905151367, loss=0.8489354848861694
I0129 05:53:58.297870 140004624946944 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.897375106811523, loss=0.8157456517219543
I0129 05:54:31.898495 140005288683264 logging_writer.py:48] [167000] global_step=167000, grad_norm=8.762929916381836, loss=0.8772056102752686
I0129 05:55:05.446339 140004624946944 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.57007884979248, loss=0.8315494656562805
I0129 05:55:05.610806 140169137129280 spec.py:321] Evaluating on the training split.
I0129 05:55:11.995721 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 05:55:20.638903 140169137129280 spec.py:349] Evaluating on the test split.
I0129 05:55:23.274132 140169137129280 submission_runner.py:408] Time since start: 58135.12s, 	Step: 167102, 	{'train/accuracy': 0.9091398119926453, 'train/loss': 0.32033541798591614, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 0.9870730042457581, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.6737549304962158, 'test/num_examples': 10000, 'score': 56139.54078269005, 'total_duration': 58135.12322330475, 'accumulated_submission_time': 56139.54078269005, 'accumulated_eval_time': 1985.1188147068024, 'accumulated_logging_time': 5.042438745498657}
I0129 05:55:23.319419 140005297075968 logging_writer.py:48] [167102] accumulated_eval_time=1985.118815, accumulated_logging_time=5.042439, accumulated_submission_time=56139.540783, global_step=167102, preemption_count=0, score=56139.540783, test/accuracy=0.633900, test/loss=1.673755, test/num_examples=10000, total_duration=58135.123223, train/accuracy=0.909140, train/loss=0.320335, validation/accuracy=0.757400, validation/loss=0.987073, validation/num_examples=50000
I0129 05:55:56.478312 140005305468672 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.65914535522461, loss=0.8271901607513428
I0129 05:56:29.957446 140005297075968 logging_writer.py:48] [167300] global_step=167300, grad_norm=9.859705924987793, loss=0.9698549509048462
I0129 05:57:03.496549 140005305468672 logging_writer.py:48] [167400] global_step=167400, grad_norm=7.918385982513428, loss=0.7882518172264099
I0129 05:57:37.003185 140005297075968 logging_writer.py:48] [167500] global_step=167500, grad_norm=10.32078742980957, loss=0.7633458375930786
I0129 05:58:10.530934 140005305468672 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.922447204589844, loss=0.8491783738136292
I0129 05:58:44.165457 140005297075968 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.220793724060059, loss=0.862159788608551
I0129 05:59:17.715995 140005305468672 logging_writer.py:48] [167800] global_step=167800, grad_norm=8.312394142150879, loss=0.8843663930892944
I0129 05:59:51.290424 140005297075968 logging_writer.py:48] [167900] global_step=167900, grad_norm=8.893004417419434, loss=0.9247902631759644
I0129 06:00:24.845279 140005305468672 logging_writer.py:48] [168000] global_step=168000, grad_norm=9.104517936706543, loss=0.8983045220375061
I0129 06:00:58.355416 140005297075968 logging_writer.py:48] [168100] global_step=168100, grad_norm=7.932446002960205, loss=0.7607260942459106
I0129 06:01:31.927944 140005305468672 logging_writer.py:48] [168200] global_step=168200, grad_norm=8.602666854858398, loss=0.8449824452400208
I0129 06:02:05.465408 140005297075968 logging_writer.py:48] [168300] global_step=168300, grad_norm=9.004729270935059, loss=0.8643909096717834
I0129 06:02:39.002508 140005305468672 logging_writer.py:48] [168400] global_step=168400, grad_norm=8.83823299407959, loss=0.819853663444519
I0129 06:03:12.550401 140005297075968 logging_writer.py:48] [168500] global_step=168500, grad_norm=8.783120155334473, loss=0.8217757940292358
I0129 06:03:46.095684 140005305468672 logging_writer.py:48] [168600] global_step=168600, grad_norm=7.950223445892334, loss=0.814685583114624
I0129 06:03:53.292063 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:03:59.634211 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:04:08.549136 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:04:11.172873 140169137129280 submission_runner.py:408] Time since start: 58663.02s, 	Step: 168623, 	{'train/accuracy': 0.9107740521430969, 'train/loss': 0.31885087490081787, 'validation/accuracy': 0.7586399912834167, 'validation/loss': 0.9830759763717651, 'validation/num_examples': 50000, 'test/accuracy': 0.6332000494003296, 'test/loss': 1.6756737232208252, 'test/num_examples': 10000, 'score': 56649.45196771622, 'total_duration': 58663.021944761276, 'accumulated_submission_time': 56649.45196771622, 'accumulated_eval_time': 2002.999568939209, 'accumulated_logging_time': 5.100426197052002}
I0129 06:04:11.222034 140004624946944 logging_writer.py:48] [168623] accumulated_eval_time=2002.999569, accumulated_logging_time=5.100426, accumulated_submission_time=56649.451968, global_step=168623, preemption_count=0, score=56649.451968, test/accuracy=0.633200, test/loss=1.675674, test/num_examples=10000, total_duration=58663.021945, train/accuracy=0.910774, train/loss=0.318851, validation/accuracy=0.758640, validation/loss=0.983076, validation/num_examples=50000
I0129 06:04:37.353474 140005288683264 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.395842552185059, loss=0.7048866748809814
I0129 06:05:10.955625 140004624946944 logging_writer.py:48] [168800] global_step=168800, grad_norm=9.361465454101562, loss=0.7933666706085205
I0129 06:05:44.537177 140005288683264 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.031519889831543, loss=0.8713518381118774
I0129 06:06:18.057903 140004624946944 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.4895806312561035, loss=0.759768009185791
I0129 06:06:51.586876 140005288683264 logging_writer.py:48] [169100] global_step=169100, grad_norm=9.175067901611328, loss=0.8134020566940308
I0129 06:07:25.136516 140004624946944 logging_writer.py:48] [169200] global_step=169200, grad_norm=8.684114456176758, loss=0.8455219268798828
I0129 06:07:58.695530 140005288683264 logging_writer.py:48] [169300] global_step=169300, grad_norm=9.103325843811035, loss=0.8014587163925171
I0129 06:08:32.207923 140004624946944 logging_writer.py:48] [169400] global_step=169400, grad_norm=7.942005157470703, loss=0.7811726331710815
I0129 06:09:05.743936 140005288683264 logging_writer.py:48] [169500] global_step=169500, grad_norm=8.72268295288086, loss=0.734741747379303
I0129 06:09:39.290711 140004624946944 logging_writer.py:48] [169600] global_step=169600, grad_norm=8.693498611450195, loss=0.8793410062789917
I0129 06:10:12.852891 140005288683264 logging_writer.py:48] [169700] global_step=169700, grad_norm=8.55392837524414, loss=0.8000196218490601
I0129 06:10:46.367359 140004624946944 logging_writer.py:48] [169800] global_step=169800, grad_norm=10.942818641662598, loss=0.9098120927810669
I0129 06:11:20.036639 140005288683264 logging_writer.py:48] [169900] global_step=169900, grad_norm=8.45120620727539, loss=0.8100231885910034
I0129 06:11:53.649187 140004624946944 logging_writer.py:48] [170000] global_step=170000, grad_norm=9.034893989562988, loss=0.8409342765808105
I0129 06:12:27.259174 140005288683264 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.998645305633545, loss=0.7846888899803162
I0129 06:12:41.194233 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:12:47.576234 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:12:56.455214 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:12:59.114292 140169137129280 submission_runner.py:408] Time since start: 59190.96s, 	Step: 170143, 	{'train/accuracy': 0.9118303656578064, 'train/loss': 0.30593132972717285, 'validation/accuracy': 0.7596799731254578, 'validation/loss': 0.9778745174407959, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.6767271757125854, 'test/num_examples': 10000, 'score': 57159.36017847061, 'total_duration': 59190.963366508484, 'accumulated_submission_time': 57159.36017847061, 'accumulated_eval_time': 2020.919572353363, 'accumulated_logging_time': 5.164439678192139}
I0129 06:12:59.165371 140004608161536 logging_writer.py:48] [170143] accumulated_eval_time=2020.919572, accumulated_logging_time=5.164440, accumulated_submission_time=57159.360178, global_step=170143, preemption_count=0, score=57159.360178, test/accuracy=0.636800, test/loss=1.676727, test/num_examples=10000, total_duration=59190.963367, train/accuracy=0.911830, train/loss=0.305931, validation/accuracy=0.759680, validation/loss=0.977875, validation/num_examples=50000
I0129 06:13:18.658915 140004616554240 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.687378883361816, loss=0.725631833076477
I0129 06:13:52.171320 140004608161536 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.912919044494629, loss=0.8533821105957031
I0129 06:14:25.703683 140004616554240 logging_writer.py:48] [170400] global_step=170400, grad_norm=8.809126853942871, loss=0.7167394161224365
I0129 06:14:59.309467 140004608161536 logging_writer.py:48] [170500] global_step=170500, grad_norm=9.13996696472168, loss=0.8374655246734619
I0129 06:15:32.919682 140004616554240 logging_writer.py:48] [170600] global_step=170600, grad_norm=8.734698295593262, loss=0.8714269399642944
I0129 06:16:06.496818 140004608161536 logging_writer.py:48] [170700] global_step=170700, grad_norm=8.928726196289062, loss=0.7946120500564575
I0129 06:16:40.015100 140004616554240 logging_writer.py:48] [170800] global_step=170800, grad_norm=9.275500297546387, loss=0.8358102440834045
I0129 06:17:13.552283 140004608161536 logging_writer.py:48] [170900] global_step=170900, grad_norm=9.293777465820312, loss=0.7897883653640747
I0129 06:17:47.213706 140004616554240 logging_writer.py:48] [171000] global_step=171000, grad_norm=8.53193187713623, loss=0.8281314373016357
I0129 06:18:20.802480 140004608161536 logging_writer.py:48] [171100] global_step=171100, grad_norm=9.755215644836426, loss=0.8255007266998291
I0129 06:18:54.350872 140004616554240 logging_writer.py:48] [171200] global_step=171200, grad_norm=8.135156631469727, loss=0.7964191436767578
I0129 06:19:27.970026 140004608161536 logging_writer.py:48] [171300] global_step=171300, grad_norm=7.981045722961426, loss=0.7442339658737183
I0129 06:20:01.591286 140004616554240 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.205370903015137, loss=0.7353299260139465
I0129 06:20:35.146450 140004608161536 logging_writer.py:48] [171500] global_step=171500, grad_norm=9.218239784240723, loss=0.7249124050140381
I0129 06:21:08.629696 140004616554240 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.514689445495605, loss=0.7017095685005188
I0129 06:21:29.243669 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:21:35.609650 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:21:44.509648 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:21:47.167113 140169137129280 submission_runner.py:408] Time since start: 59719.02s, 	Step: 171663, 	{'train/accuracy': 0.9235889315605164, 'train/loss': 0.2766060531139374, 'validation/accuracy': 0.7594000101089478, 'validation/loss': 0.9787272214889526, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.6702979803085327, 'test/num_examples': 10000, 'score': 57669.37860417366, 'total_duration': 59719.016202926636, 'accumulated_submission_time': 57669.37860417366, 'accumulated_eval_time': 2038.8429753780365, 'accumulated_logging_time': 5.226194143295288}
I0129 06:21:47.217151 140005305468672 logging_writer.py:48] [171663] accumulated_eval_time=2038.842975, accumulated_logging_time=5.226194, accumulated_submission_time=57669.378604, global_step=171663, preemption_count=0, score=57669.378604, test/accuracy=0.633800, test/loss=1.670298, test/num_examples=10000, total_duration=59719.016203, train/accuracy=0.923589, train/loss=0.276606, validation/accuracy=0.759400, validation/loss=0.978727, validation/num_examples=50000
I0129 06:21:59.978472 140005313861376 logging_writer.py:48] [171700] global_step=171700, grad_norm=9.447249412536621, loss=0.840936541557312
I0129 06:22:33.523039 140005305468672 logging_writer.py:48] [171800] global_step=171800, grad_norm=8.40866756439209, loss=0.7898030281066895
I0129 06:23:07.010983 140005313861376 logging_writer.py:48] [171900] global_step=171900, grad_norm=8.639223098754883, loss=0.7947561740875244
I0129 06:23:40.585535 140005305468672 logging_writer.py:48] [172000] global_step=172000, grad_norm=8.846258163452148, loss=0.7224464416503906
I0129 06:24:14.341789 140005313861376 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.797195434570312, loss=0.7560741901397705
I0129 06:24:47.945662 140005305468672 logging_writer.py:48] [172200] global_step=172200, grad_norm=9.074319839477539, loss=0.7879968285560608
I0129 06:25:21.580182 140005313861376 logging_writer.py:48] [172300] global_step=172300, grad_norm=9.225237846374512, loss=0.8362933993339539
I0129 06:25:55.177915 140005305468672 logging_writer.py:48] [172400] global_step=172400, grad_norm=10.167884826660156, loss=0.8537554740905762
I0129 06:26:28.706835 140005313861376 logging_writer.py:48] [172500] global_step=172500, grad_norm=8.626703262329102, loss=0.7426796555519104
I0129 06:27:02.230755 140005305468672 logging_writer.py:48] [172600] global_step=172600, grad_norm=9.313258171081543, loss=0.8027135729789734
I0129 06:27:35.842668 140005313861376 logging_writer.py:48] [172700] global_step=172700, grad_norm=8.881813049316406, loss=0.7810778617858887
I0129 06:28:09.416868 140005305468672 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.82880687713623, loss=0.717241644859314
I0129 06:28:42.935888 140005313861376 logging_writer.py:48] [172900] global_step=172900, grad_norm=8.238751411437988, loss=0.7197253704071045
I0129 06:29:16.458141 140005305468672 logging_writer.py:48] [173000] global_step=173000, grad_norm=8.84615421295166, loss=0.799701988697052
I0129 06:29:50.051140 140005313861376 logging_writer.py:48] [173100] global_step=173100, grad_norm=8.244522094726562, loss=0.7444606423377991
I0129 06:30:17.501554 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:30:23.916007 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:30:32.588959 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:30:35.240308 140169137129280 submission_runner.py:408] Time since start: 60247.09s, 	Step: 173183, 	{'train/accuracy': 0.9255819320678711, 'train/loss': 0.2669574022293091, 'validation/accuracy': 0.7631999850273132, 'validation/loss': 0.9670901298522949, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6677082777023315, 'test/num_examples': 10000, 'score': 58179.60224986076, 'total_duration': 60247.089400053024, 'accumulated_submission_time': 58179.60224986076, 'accumulated_eval_time': 2056.5816910266876, 'accumulated_logging_time': 5.2877233028411865}
I0129 06:30:35.288113 140004616554240 logging_writer.py:48] [173183] accumulated_eval_time=2056.581691, accumulated_logging_time=5.287723, accumulated_submission_time=58179.602250, global_step=173183, preemption_count=0, score=58179.602250, test/accuracy=0.637400, test/loss=1.667708, test/num_examples=10000, total_duration=60247.089400, train/accuracy=0.925582, train/loss=0.266957, validation/accuracy=0.763200, validation/loss=0.967090, validation/num_examples=50000
I0129 06:30:41.332941 140004624946944 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.696186065673828, loss=0.7935598492622375
I0129 06:31:14.861446 140004616554240 logging_writer.py:48] [173300] global_step=173300, grad_norm=8.785423278808594, loss=0.7412109375
I0129 06:31:48.355365 140004624946944 logging_writer.py:48] [173400] global_step=173400, grad_norm=9.081695556640625, loss=0.8537806868553162
I0129 06:32:21.904853 140004616554240 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.825797080993652, loss=0.7911338210105896
I0129 06:32:55.494150 140004624946944 logging_writer.py:48] [173600] global_step=173600, grad_norm=9.222673416137695, loss=0.7924821376800537
I0129 06:33:29.111399 140004616554240 logging_writer.py:48] [173700] global_step=173700, grad_norm=10.021769523620605, loss=0.7944099307060242
I0129 06:34:02.650487 140004624946944 logging_writer.py:48] [173800] global_step=173800, grad_norm=9.354471206665039, loss=0.7878872752189636
I0129 06:34:36.135506 140004616554240 logging_writer.py:48] [173900] global_step=173900, grad_norm=8.462374687194824, loss=0.7870607376098633
I0129 06:35:09.741926 140004624946944 logging_writer.py:48] [174000] global_step=174000, grad_norm=8.616909980773926, loss=0.7159212231636047
I0129 06:35:43.355210 140004616554240 logging_writer.py:48] [174100] global_step=174100, grad_norm=8.543243408203125, loss=0.7199428081512451
I0129 06:36:16.988901 140004624946944 logging_writer.py:48] [174200] global_step=174200, grad_norm=8.17412281036377, loss=0.7681360244750977
I0129 06:36:50.593456 140004616554240 logging_writer.py:48] [174300] global_step=174300, grad_norm=8.678372383117676, loss=0.8238468766212463
I0129 06:37:24.196819 140004624946944 logging_writer.py:48] [174400] global_step=174400, grad_norm=9.496027946472168, loss=0.8510507345199585
I0129 06:37:57.803245 140004616554240 logging_writer.py:48] [174500] global_step=174500, grad_norm=9.04154109954834, loss=0.7741023302078247
I0129 06:38:31.346880 140004624946944 logging_writer.py:48] [174600] global_step=174600, grad_norm=8.637101173400879, loss=0.7723734974861145
I0129 06:39:04.963072 140004616554240 logging_writer.py:48] [174700] global_step=174700, grad_norm=9.729905128479004, loss=0.8096250891685486
I0129 06:39:05.461371 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:39:11.892147 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:39:20.490149 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:39:23.185960 140169137129280 submission_runner.py:408] Time since start: 60775.04s, 	Step: 174703, 	{'train/accuracy': 0.927754282951355, 'train/loss': 0.2576439678668976, 'validation/accuracy': 0.7628799676895142, 'validation/loss': 0.9702308177947998, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.670340895652771, 'test/num_examples': 10000, 'score': 58689.71545791626, 'total_duration': 60775.03505182266, 'accumulated_submission_time': 58689.71545791626, 'accumulated_eval_time': 2074.306261062622, 'accumulated_logging_time': 5.346015691757202}
I0129 06:39:23.238492 140004616554240 logging_writer.py:48] [174703] accumulated_eval_time=2074.306261, accumulated_logging_time=5.346016, accumulated_submission_time=58689.715458, global_step=174703, preemption_count=0, score=58689.715458, test/accuracy=0.636700, test/loss=1.670341, test/num_examples=10000, total_duration=60775.035052, train/accuracy=0.927754, train/loss=0.257644, validation/accuracy=0.762880, validation/loss=0.970231, validation/num_examples=50000
I0129 06:39:56.030052 140005313861376 logging_writer.py:48] [174800] global_step=174800, grad_norm=8.701894760131836, loss=0.7727465629577637
I0129 06:40:29.592347 140004616554240 logging_writer.py:48] [174900] global_step=174900, grad_norm=8.368252754211426, loss=0.7432793378829956
I0129 06:41:03.202618 140005313861376 logging_writer.py:48] [175000] global_step=175000, grad_norm=9.393048286437988, loss=0.8579416275024414
I0129 06:41:36.792381 140004616554240 logging_writer.py:48] [175100] global_step=175100, grad_norm=9.16464900970459, loss=0.8062653541564941
I0129 06:42:10.385965 140005313861376 logging_writer.py:48] [175200] global_step=175200, grad_norm=9.114424705505371, loss=0.8322073817253113
I0129 06:42:43.984252 140004616554240 logging_writer.py:48] [175300] global_step=175300, grad_norm=9.054181098937988, loss=0.7459120750427246
I0129 06:43:17.625261 140005313861376 logging_writer.py:48] [175400] global_step=175400, grad_norm=9.5587797164917, loss=0.826263964176178
I0129 06:43:51.192689 140004616554240 logging_writer.py:48] [175500] global_step=175500, grad_norm=9.125749588012695, loss=0.720611572265625
I0129 06:44:24.801381 140005313861376 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.759723663330078, loss=0.7838258743286133
I0129 06:44:58.398139 140004616554240 logging_writer.py:48] [175700] global_step=175700, grad_norm=9.8975830078125, loss=0.8690847158432007
I0129 06:45:32.010293 140005313861376 logging_writer.py:48] [175800] global_step=175800, grad_norm=9.876089096069336, loss=0.8467777371406555
I0129 06:46:05.615772 140004616554240 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.441186904907227, loss=0.8568131327629089
I0129 06:46:39.251268 140005313861376 logging_writer.py:48] [176000] global_step=176000, grad_norm=9.489293098449707, loss=0.7357916235923767
I0129 06:47:12.852098 140004616554240 logging_writer.py:48] [176100] global_step=176100, grad_norm=9.142775535583496, loss=0.7585413455963135
I0129 06:47:46.474708 140005313861376 logging_writer.py:48] [176200] global_step=176200, grad_norm=9.607492446899414, loss=0.7589636445045471
I0129 06:47:53.352161 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:47:59.708762 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:48:08.587977 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:48:11.205802 140169137129280 submission_runner.py:408] Time since start: 61303.05s, 	Step: 176222, 	{'train/accuracy': 0.9254623651504517, 'train/loss': 0.26531344652175903, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9638875722885132, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6606649160385132, 'test/num_examples': 10000, 'score': 59199.76896715164, 'total_duration': 61303.05489516258, 'accumulated_submission_time': 59199.76896715164, 'accumulated_eval_time': 2092.1598665714264, 'accumulated_logging_time': 5.409138917922974}
I0129 06:48:11.255980 140004616554240 logging_writer.py:48] [176222] accumulated_eval_time=2092.159867, accumulated_logging_time=5.409139, accumulated_submission_time=59199.768967, global_step=176222, preemption_count=0, score=59199.768967, test/accuracy=0.640700, test/loss=1.660665, test/num_examples=10000, total_duration=61303.054895, train/accuracy=0.925462, train/loss=0.265313, validation/accuracy=0.763320, validation/loss=0.963888, validation/num_examples=50000
I0129 06:48:37.720805 140004624946944 logging_writer.py:48] [176300] global_step=176300, grad_norm=8.548892974853516, loss=0.6986891627311707
I0129 06:49:11.338738 140004616554240 logging_writer.py:48] [176400] global_step=176400, grad_norm=8.671703338623047, loss=0.6970080733299255
I0129 06:49:44.920111 140004624946944 logging_writer.py:48] [176500] global_step=176500, grad_norm=8.435235977172852, loss=0.7379859089851379
I0129 06:50:18.527301 140004616554240 logging_writer.py:48] [176600] global_step=176600, grad_norm=8.563950538635254, loss=0.7364764213562012
I0129 06:50:52.152033 140004624946944 logging_writer.py:48] [176700] global_step=176700, grad_norm=8.930580139160156, loss=0.7554041743278503
I0129 06:51:25.777285 140004616554240 logging_writer.py:48] [176800] global_step=176800, grad_norm=9.259003639221191, loss=0.7809149026870728
I0129 06:51:59.379480 140004624946944 logging_writer.py:48] [176900] global_step=176900, grad_norm=9.788002967834473, loss=0.8102045655250549
I0129 06:52:32.991109 140004616554240 logging_writer.py:48] [177000] global_step=177000, grad_norm=9.380464553833008, loss=0.7763378024101257
I0129 06:53:06.602945 140004624946944 logging_writer.py:48] [177100] global_step=177100, grad_norm=8.851946830749512, loss=0.7460374236106873
I0129 06:53:40.223775 140004616554240 logging_writer.py:48] [177200] global_step=177200, grad_norm=10.365875244140625, loss=0.8116210699081421
I0129 06:54:13.781792 140004624946944 logging_writer.py:48] [177300] global_step=177300, grad_norm=8.328009605407715, loss=0.7279837131500244
I0129 06:54:47.292911 140004616554240 logging_writer.py:48] [177400] global_step=177400, grad_norm=8.989866256713867, loss=0.796558141708374
I0129 06:55:20.843736 140004624946944 logging_writer.py:48] [177500] global_step=177500, grad_norm=8.659847259521484, loss=0.7398681044578552
I0129 06:55:54.524771 140004616554240 logging_writer.py:48] [177600] global_step=177600, grad_norm=8.750589370727539, loss=0.7276673316955566
I0129 06:56:28.078957 140004624946944 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.581536293029785, loss=0.7894913554191589
I0129 06:56:41.307172 140169137129280 spec.py:321] Evaluating on the training split.
I0129 06:56:47.691666 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 06:56:56.604159 140169137129280 spec.py:349] Evaluating on the test split.
I0129 06:56:59.218256 140169137129280 submission_runner.py:408] Time since start: 61831.07s, 	Step: 177741, 	{'train/accuracy': 0.9296875, 'train/loss': 0.254580557346344, 'validation/accuracy': 0.7645799517631531, 'validation/loss': 0.9602543711662292, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.6592662334442139, 'test/num_examples': 10000, 'score': 59709.756935596466, 'total_duration': 61831.0673494339, 'accumulated_submission_time': 59709.756935596466, 'accumulated_eval_time': 2110.070912361145, 'accumulated_logging_time': 5.473035573959351}
I0129 06:56:59.268148 140005305468672 logging_writer.py:48] [177741] accumulated_eval_time=2110.070912, accumulated_logging_time=5.473036, accumulated_submission_time=59709.756936, global_step=177741, preemption_count=0, score=59709.756936, test/accuracy=0.640600, test/loss=1.659266, test/num_examples=10000, total_duration=61831.067349, train/accuracy=0.929688, train/loss=0.254581, validation/accuracy=0.764580, validation/loss=0.960254, validation/num_examples=50000
I0129 06:57:19.360927 140005313861376 logging_writer.py:48] [177800] global_step=177800, grad_norm=8.316522598266602, loss=0.7884754538536072
I0129 06:57:52.907945 140005305468672 logging_writer.py:48] [177900] global_step=177900, grad_norm=8.019721031188965, loss=0.7433346509933472
I0129 06:58:26.448549 140005313861376 logging_writer.py:48] [178000] global_step=178000, grad_norm=9.33544921875, loss=0.7796295881271362
I0129 06:58:59.961271 140005305468672 logging_writer.py:48] [178100] global_step=178100, grad_norm=9.271668434143066, loss=0.7515223622322083
I0129 06:59:33.536229 140005313861376 logging_writer.py:48] [178200] global_step=178200, grad_norm=9.401338577270508, loss=0.768062174320221
I0129 07:00:07.153321 140005305468672 logging_writer.py:48] [178300] global_step=178300, grad_norm=9.49462604522705, loss=0.8134859800338745
I0129 07:00:40.731862 140005313861376 logging_writer.py:48] [178400] global_step=178400, grad_norm=8.427085876464844, loss=0.6933454871177673
I0129 07:01:14.339784 140005305468672 logging_writer.py:48] [178500] global_step=178500, grad_norm=9.206676483154297, loss=0.8013980388641357
I0129 07:01:47.940957 140005313861376 logging_writer.py:48] [178600] global_step=178600, grad_norm=9.029535293579102, loss=0.6965014934539795
I0129 07:02:21.580228 140005305468672 logging_writer.py:48] [178700] global_step=178700, grad_norm=8.853108406066895, loss=0.7925492525100708
I0129 07:02:55.122546 140005313861376 logging_writer.py:48] [178800] global_step=178800, grad_norm=8.495119094848633, loss=0.7316276431083679
I0129 07:03:28.635947 140005305468672 logging_writer.py:48] [178900] global_step=178900, grad_norm=9.082917213439941, loss=0.7711045145988464
I0129 07:04:02.186285 140005313861376 logging_writer.py:48] [179000] global_step=179000, grad_norm=9.157054901123047, loss=0.7450098991394043
I0129 07:04:35.736908 140005305468672 logging_writer.py:48] [179100] global_step=179100, grad_norm=8.780265808105469, loss=0.7833694219589233
I0129 07:05:09.346914 140005313861376 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.683585166931152, loss=0.8124346137046814
I0129 07:05:29.334536 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:05:35.658459 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:05:44.332122 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:05:46.959936 140169137129280 submission_runner.py:408] Time since start: 62358.81s, 	Step: 179261, 	{'train/accuracy': 0.9309031963348389, 'train/loss': 0.25196245312690735, 'validation/accuracy': 0.7645399570465088, 'validation/loss': 0.9593108296394348, 'validation/num_examples': 50000, 'test/accuracy': 0.6409000158309937, 'test/loss': 1.654666543006897, 'test/num_examples': 10000, 'score': 60219.763149023056, 'total_duration': 62358.809020757675, 'accumulated_submission_time': 60219.763149023056, 'accumulated_eval_time': 2127.696268796921, 'accumulated_logging_time': 5.533898830413818}
I0129 07:05:47.009320 140004616554240 logging_writer.py:48] [179261] accumulated_eval_time=2127.696269, accumulated_logging_time=5.533899, accumulated_submission_time=60219.763149, global_step=179261, preemption_count=0, score=60219.763149, test/accuracy=0.640900, test/loss=1.654667, test/num_examples=10000, total_duration=62358.809021, train/accuracy=0.930903, train/loss=0.251962, validation/accuracy=0.764540, validation/loss=0.959311, validation/num_examples=50000
I0129 07:06:00.425139 140004624946944 logging_writer.py:48] [179300] global_step=179300, grad_norm=8.742836952209473, loss=0.6762099266052246
I0129 07:06:33.896396 140004616554240 logging_writer.py:48] [179400] global_step=179400, grad_norm=8.824966430664062, loss=0.8032103180885315
I0129 07:07:07.404520 140004624946944 logging_writer.py:48] [179500] global_step=179500, grad_norm=8.9050931930542, loss=0.709642767906189
I0129 07:07:41.012654 140004616554240 logging_writer.py:48] [179600] global_step=179600, grad_norm=7.964822769165039, loss=0.725098729133606
I0129 07:08:14.688774 140004624946944 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.929660797119141, loss=0.6617674827575684
I0129 07:08:48.260474 140004616554240 logging_writer.py:48] [179800] global_step=179800, grad_norm=8.744876861572266, loss=0.7199680805206299
I0129 07:09:21.849721 140004624946944 logging_writer.py:48] [179900] global_step=179900, grad_norm=9.309358596801758, loss=0.7719854116439819
I0129 07:09:55.471137 140004616554240 logging_writer.py:48] [180000] global_step=180000, grad_norm=8.44677448272705, loss=0.6824193596839905
I0129 07:10:29.083961 140004624946944 logging_writer.py:48] [180100] global_step=180100, grad_norm=9.12037467956543, loss=0.7174379825592041
I0129 07:11:02.655441 140004616554240 logging_writer.py:48] [180200] global_step=180200, grad_norm=8.974774360656738, loss=0.8407398462295532
I0129 07:11:36.241968 140004624946944 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.736291885375977, loss=0.748851478099823
I0129 07:12:09.824365 140004616554240 logging_writer.py:48] [180400] global_step=180400, grad_norm=8.361871719360352, loss=0.6739903092384338
I0129 07:12:43.345071 140004624946944 logging_writer.py:48] [180500] global_step=180500, grad_norm=9.87550163269043, loss=0.7190050482749939
I0129 07:13:16.877190 140004616554240 logging_writer.py:48] [180600] global_step=180600, grad_norm=9.23398208618164, loss=0.7876170873641968
I0129 07:13:50.442909 140004624946944 logging_writer.py:48] [180700] global_step=180700, grad_norm=8.790682792663574, loss=0.7225659489631653
I0129 07:14:17.144060 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:14:23.462097 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:14:32.536739 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:14:35.173351 140169137129280 submission_runner.py:408] Time since start: 62887.02s, 	Step: 180781, 	{'train/accuracy': 0.9323779940605164, 'train/loss': 0.244142547249794, 'validation/accuracy': 0.76419997215271, 'validation/loss': 0.958231508731842, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.6557667255401611, 'test/num_examples': 10000, 'score': 60729.83857727051, 'total_duration': 62887.02244234085, 'accumulated_submission_time': 60729.83857727051, 'accumulated_eval_time': 2145.7255239486694, 'accumulated_logging_time': 5.593425512313843}
I0129 07:14:35.226626 140004616554240 logging_writer.py:48] [180781] accumulated_eval_time=2145.725524, accumulated_logging_time=5.593426, accumulated_submission_time=60729.838577, global_step=180781, preemption_count=0, score=60729.838577, test/accuracy=0.639100, test/loss=1.655767, test/num_examples=10000, total_duration=62887.022442, train/accuracy=0.932378, train/loss=0.244143, validation/accuracy=0.764200, validation/loss=0.958232, validation/num_examples=50000
I0129 07:14:41.957509 140004624946944 logging_writer.py:48] [180800] global_step=180800, grad_norm=8.626132011413574, loss=0.7432688474655151
I0129 07:15:15.450441 140004616554240 logging_writer.py:48] [180900] global_step=180900, grad_norm=8.73879337310791, loss=0.7999240756034851
I0129 07:15:49.023890 140004624946944 logging_writer.py:48] [181000] global_step=181000, grad_norm=9.076204299926758, loss=0.7487690448760986
I0129 07:16:22.581423 140004616554240 logging_writer.py:48] [181100] global_step=181100, grad_norm=8.893119812011719, loss=0.7279143929481506
I0129 07:16:56.096187 140004624946944 logging_writer.py:48] [181200] global_step=181200, grad_norm=8.485153198242188, loss=0.7947410941123962
I0129 07:17:29.642650 140004616554240 logging_writer.py:48] [181300] global_step=181300, grad_norm=8.616778373718262, loss=0.7412279844284058
I0129 07:18:03.188861 140004624946944 logging_writer.py:48] [181400] global_step=181400, grad_norm=9.573493957519531, loss=0.6923493146896362
I0129 07:18:36.735446 140004616554240 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.984036445617676, loss=0.7524888515472412
I0129 07:19:10.325504 140004624946944 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.493473052978516, loss=0.673833429813385
I0129 07:19:43.955507 140004616554240 logging_writer.py:48] [181700] global_step=181700, grad_norm=8.374314308166504, loss=0.7690210342407227
I0129 07:20:17.582877 140004624946944 logging_writer.py:48] [181800] global_step=181800, grad_norm=9.720829963684082, loss=0.8005692362785339
I0129 07:20:51.264672 140004616554240 logging_writer.py:48] [181900] global_step=181900, grad_norm=8.262236595153809, loss=0.6570074558258057
I0129 07:21:24.802116 140004624946944 logging_writer.py:48] [182000] global_step=182000, grad_norm=9.836583137512207, loss=0.7800435423851013
I0129 07:21:58.397334 140004616554240 logging_writer.py:48] [182100] global_step=182100, grad_norm=8.6382417678833, loss=0.7348646521568298
I0129 07:22:32.020081 140004624946944 logging_writer.py:48] [182200] global_step=182200, grad_norm=9.099422454833984, loss=0.7223269939422607
I0129 07:23:05.599850 140004616554240 logging_writer.py:48] [182300] global_step=182300, grad_norm=9.421600341796875, loss=0.7457603812217712
I0129 07:23:05.608390 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:23:11.914819 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:23:20.852270 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:23:23.493540 140169137129280 submission_runner.py:408] Time since start: 63415.34s, 	Step: 182301, 	{'train/accuracy': 0.9343510866165161, 'train/loss': 0.239544078707695, 'validation/accuracy': 0.7648999691009521, 'validation/loss': 0.9564946889877319, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.6545675992965698, 'test/num_examples': 10000, 'score': 61240.160959005356, 'total_duration': 63415.34263134003, 'accumulated_submission_time': 61240.160959005356, 'accumulated_eval_time': 2163.610629081726, 'accumulated_logging_time': 5.656862735748291}
I0129 07:23:23.544029 140005288683264 logging_writer.py:48] [182301] accumulated_eval_time=2163.610629, accumulated_logging_time=5.656863, accumulated_submission_time=61240.160959, global_step=182301, preemption_count=0, score=61240.160959, test/accuracy=0.641200, test/loss=1.654568, test/num_examples=10000, total_duration=63415.342631, train/accuracy=0.934351, train/loss=0.239544, validation/accuracy=0.764900, validation/loss=0.956495, validation/num_examples=50000
I0129 07:23:57.022592 140005297075968 logging_writer.py:48] [182400] global_step=182400, grad_norm=8.934989929199219, loss=0.7097216844558716
I0129 07:24:30.609634 140005288683264 logging_writer.py:48] [182500] global_step=182500, grad_norm=8.360045433044434, loss=0.716737687587738
I0129 07:25:04.228463 140005297075968 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.593300819396973, loss=0.7388789057731628
I0129 07:25:37.858763 140005288683264 logging_writer.py:48] [182700] global_step=182700, grad_norm=9.055047035217285, loss=0.7744606733322144
I0129 07:26:11.465135 140005297075968 logging_writer.py:48] [182800] global_step=182800, grad_norm=9.540671348571777, loss=0.7689548134803772
I0129 07:26:45.062542 140005288683264 logging_writer.py:48] [182900] global_step=182900, grad_norm=8.674393653869629, loss=0.7233034372329712
I0129 07:27:18.730001 140005297075968 logging_writer.py:48] [183000] global_step=183000, grad_norm=9.187566757202148, loss=0.7106183171272278
I0129 07:27:52.324797 140005288683264 logging_writer.py:48] [183100] global_step=183100, grad_norm=8.480135917663574, loss=0.658392071723938
I0129 07:28:25.946642 140005297075968 logging_writer.py:48] [183200] global_step=183200, grad_norm=8.319396018981934, loss=0.700744092464447
I0129 07:28:59.584952 140005288683264 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.925546646118164, loss=0.7303244471549988
I0129 07:29:33.196507 140005297075968 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.471182823181152, loss=0.7499076128005981
I0129 07:30:06.803181 140005288683264 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.814888000488281, loss=0.6794901490211487
I0129 07:30:40.420091 140005297075968 logging_writer.py:48] [183600] global_step=183600, grad_norm=8.836851119995117, loss=0.7250329256057739
I0129 07:31:13.969946 140005288683264 logging_writer.py:48] [183700] global_step=183700, grad_norm=8.332611083984375, loss=0.6266285181045532
I0129 07:31:47.494274 140005297075968 logging_writer.py:48] [183800] global_step=183800, grad_norm=9.02706241607666, loss=0.7479598522186279
I0129 07:31:53.674561 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:31:59.988019 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:32:08.737197 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:32:11.388858 140169137129280 submission_runner.py:408] Time since start: 63943.24s, 	Step: 183820, 	{'train/accuracy': 0.9310028553009033, 'train/loss': 0.24653080105781555, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 0.9564112424850464, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6530959606170654, 'test/num_examples': 10000, 'score': 61750.23178052902, 'total_duration': 63943.237928152084, 'accumulated_submission_time': 61750.23178052902, 'accumulated_eval_time': 2181.3248648643494, 'accumulated_logging_time': 5.717596530914307}
I0129 07:32:11.440180 140005305468672 logging_writer.py:48] [183820] accumulated_eval_time=2181.324865, accumulated_logging_time=5.717597, accumulated_submission_time=61750.231781, global_step=183820, preemption_count=0, score=61750.231781, test/accuracy=0.640700, test/loss=1.653096, test/num_examples=10000, total_duration=63943.237928, train/accuracy=0.931003, train/loss=0.246531, validation/accuracy=0.765520, validation/loss=0.956411, validation/num_examples=50000
I0129 07:32:38.612637 140005313861376 logging_writer.py:48] [183900] global_step=183900, grad_norm=9.186302185058594, loss=0.7284935712814331
I0129 07:33:12.176098 140005305468672 logging_writer.py:48] [184000] global_step=184000, grad_norm=9.616059303283691, loss=0.7094568014144897
I0129 07:33:45.822384 140005313861376 logging_writer.py:48] [184100] global_step=184100, grad_norm=9.287849426269531, loss=0.8884812593460083
I0129 07:34:19.414704 140005305468672 logging_writer.py:48] [184200] global_step=184200, grad_norm=8.954774856567383, loss=0.7618219256401062
I0129 07:34:53.023342 140005313861376 logging_writer.py:48] [184300] global_step=184300, grad_norm=9.287288665771484, loss=0.8501951694488525
I0129 07:35:26.586064 140005305468672 logging_writer.py:48] [184400] global_step=184400, grad_norm=9.169750213623047, loss=0.7272122502326965
I0129 07:36:00.103679 140005313861376 logging_writer.py:48] [184500] global_step=184500, grad_norm=8.735806465148926, loss=0.6831281781196594
I0129 07:36:33.632717 140005305468672 logging_writer.py:48] [184600] global_step=184600, grad_norm=8.591206550598145, loss=0.7241469025611877
I0129 07:37:07.189587 140005313861376 logging_writer.py:48] [184700] global_step=184700, grad_norm=9.269390106201172, loss=0.763091504573822
I0129 07:37:40.744682 140005305468672 logging_writer.py:48] [184800] global_step=184800, grad_norm=7.935357093811035, loss=0.6890902519226074
I0129 07:38:14.356937 140005313861376 logging_writer.py:48] [184900] global_step=184900, grad_norm=9.123902320861816, loss=0.7543067336082458
I0129 07:38:47.934750 140005305468672 logging_writer.py:48] [185000] global_step=185000, grad_norm=8.061104774475098, loss=0.7401448488235474
I0129 07:39:21.473240 140005313861376 logging_writer.py:48] [185100] global_step=185100, grad_norm=9.32691764831543, loss=0.7182081937789917
I0129 07:39:55.101158 140005305468672 logging_writer.py:48] [185200] global_step=185200, grad_norm=8.731881141662598, loss=0.7339276075363159
I0129 07:40:28.635266 140005313861376 logging_writer.py:48] [185300] global_step=185300, grad_norm=9.138711929321289, loss=0.6787048578262329
I0129 07:40:41.519965 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:40:47.852247 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:40:56.607139 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:40:59.394600 140169137129280 submission_runner.py:408] Time since start: 64471.24s, 	Step: 185340, 	{'train/accuracy': 0.9342713356018066, 'train/loss': 0.2396325170993805, 'validation/accuracy': 0.765779972076416, 'validation/loss': 0.95612633228302, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.6519469022750854, 'test/num_examples': 10000, 'score': 62260.25123000145, 'total_duration': 64471.24368929863, 'accumulated_submission_time': 62260.25123000145, 'accumulated_eval_time': 2199.1994581222534, 'accumulated_logging_time': 5.779508590698242}
I0129 07:40:59.444444 140004624946944 logging_writer.py:48] [185340] accumulated_eval_time=2199.199458, accumulated_logging_time=5.779509, accumulated_submission_time=62260.251230, global_step=185340, preemption_count=0, score=62260.251230, test/accuracy=0.641000, test/loss=1.651947, test/num_examples=10000, total_duration=64471.243689, train/accuracy=0.934271, train/loss=0.239633, validation/accuracy=0.765780, validation/loss=0.956126, validation/num_examples=50000
I0129 07:41:19.897345 140005288683264 logging_writer.py:48] [185400] global_step=185400, grad_norm=9.283563613891602, loss=0.7494339346885681
I0129 07:41:53.833930 140004624946944 logging_writer.py:48] [185500] global_step=185500, grad_norm=9.59854507446289, loss=0.7751514911651611
I0129 07:42:27.335248 140005288683264 logging_writer.py:48] [185600] global_step=185600, grad_norm=8.521329879760742, loss=0.7107654213905334
I0129 07:43:00.870455 140004624946944 logging_writer.py:48] [185700] global_step=185700, grad_norm=9.540914535522461, loss=0.7686684131622314
I0129 07:43:34.462122 140005288683264 logging_writer.py:48] [185800] global_step=185800, grad_norm=8.923593521118164, loss=0.7616459131240845
I0129 07:44:08.060405 140004624946944 logging_writer.py:48] [185900] global_step=185900, grad_norm=9.422893524169922, loss=0.7554031014442444
I0129 07:44:41.680533 140005288683264 logging_writer.py:48] [186000] global_step=186000, grad_norm=9.03400707244873, loss=0.7361592054367065
I0129 07:45:15.275583 140004624946944 logging_writer.py:48] [186100] global_step=186100, grad_norm=9.69664478302002, loss=0.8029423356056213
I0129 07:45:48.888418 140005288683264 logging_writer.py:48] [186200] global_step=186200, grad_norm=8.7876615524292, loss=0.6918251514434814
I0129 07:46:22.527290 140004624946944 logging_writer.py:48] [186300] global_step=186300, grad_norm=9.312724113464355, loss=0.763479471206665
I0129 07:46:56.113636 140005288683264 logging_writer.py:48] [186400] global_step=186400, grad_norm=9.166252136230469, loss=0.749967634677887
I0129 07:47:29.690921 140004624946944 logging_writer.py:48] [186500] global_step=186500, grad_norm=9.064055442810059, loss=0.780198872089386
I0129 07:48:03.309185 140005288683264 logging_writer.py:48] [186600] global_step=186600, grad_norm=9.190293312072754, loss=0.7013060450553894
I0129 07:48:24.959099 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:48:31.266669 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:48:39.905626 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:48:42.541971 140169137129280 submission_runner.py:408] Time since start: 64934.39s, 	Step: 186666, 	{'train/accuracy': 0.9329161047935486, 'train/loss': 0.24513345956802368, 'validation/accuracy': 0.7657399773597717, 'validation/loss': 0.9568769335746765, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.6542025804519653, 'test/num_examples': 10000, 'score': 62705.71089506149, 'total_duration': 64934.39106178284, 'accumulated_submission_time': 62705.71089506149, 'accumulated_eval_time': 2216.782294511795, 'accumulated_logging_time': 5.841466188430786}
I0129 07:48:42.594200 140004616554240 logging_writer.py:48] [186666] accumulated_eval_time=2216.782295, accumulated_logging_time=5.841466, accumulated_submission_time=62705.710895, global_step=186666, preemption_count=0, score=62705.710895, test/accuracy=0.640500, test/loss=1.654203, test/num_examples=10000, total_duration=64934.391062, train/accuracy=0.932916, train/loss=0.245133, validation/accuracy=0.765740, validation/loss=0.956877, validation/num_examples=50000
I0129 07:48:42.640611 140004624946944 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62705.710895
I0129 07:48:43.002389 140169137129280 checkpoints.py:490] Saving checkpoint at step: 186666
I0129 07:48:44.172156 140169137129280 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3/checkpoint_186666
I0129 07:48:44.198677 140169137129280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_3/checkpoint_186666.
I0129 07:48:44.951324 140169137129280 submission_runner.py:583] Tuning trial 3/5
I0129 07:48:44.951563 140169137129280 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0129 07:48:44.958649 140169137129280 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006576849264092743, 'train/loss': 6.909994125366211, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 30.54241108894348, 'total_duration': 48.19990372657776, 'accumulated_submission_time': 30.54241108894348, 'accumulated_eval_time': 17.657403707504272, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1514, {'train/accuracy': 0.07302295416593552, 'train/loss': 5.302392959594727, 'validation/accuracy': 0.06689999997615814, 'validation/loss': 5.365999221801758, 'validation/num_examples': 50000, 'test/accuracy': 0.048600003123283386, 'test/loss': 5.617886543273926, 'test/num_examples': 10000, 'score': 540.7001566886902, 'total_duration': 576.3619163036346, 'accumulated_submission_time': 540.7001566886902, 'accumulated_eval_time': 35.58681917190552, 'accumulated_logging_time': 0.025716066360473633, 'global_step': 1514, 'preemption_count': 0}), (3027, {'train/accuracy': 0.1718152016401291, 'train/loss': 4.267473220825195, 'validation/accuracy': 0.15605999529361725, 'validation/loss': 4.383291721343994, 'validation/num_examples': 50000, 'test/accuracy': 0.1161000058054924, 'test/loss': 4.863234043121338, 'test/num_examples': 10000, 'score': 1050.9415996074677, 'total_duration': 1104.719381570816, 'accumulated_submission_time': 1050.9415996074677, 'accumulated_eval_time': 53.62484097480774, 'accumulated_logging_time': 0.055043935775756836, 'global_step': 3027, 'preemption_count': 0}), (4539, {'train/accuracy': 0.27080675959587097, 'train/loss': 3.4995758533477783, 'validation/accuracy': 0.24903999269008636, 'validation/loss': 3.6413185596466064, 'validation/num_examples': 50000, 'test/accuracy': 0.1802000105381012, 'test/loss': 4.247800827026367, 'test/num_examples': 10000, 'score': 1560.8828208446503, 'total_duration': 1632.4283664226532, 'accumulated_submission_time': 1560.8828208446503, 'accumulated_eval_time': 71.31632661819458, 'accumulated_logging_time': 0.08308601379394531, 'global_step': 4539, 'preemption_count': 0}), (6051, {'train/accuracy': 0.36945950984954834, 'train/loss': 2.88207745552063, 'validation/accuracy': 0.34147998690605164, 'validation/loss': 3.04559588432312, 'validation/num_examples': 50000, 'test/accuracy': 0.2535000145435333, 'test/loss': 3.7423603534698486, 'test/num_examples': 10000, 'score': 2071.0617468357086, 'total_duration': 2160.8530712127686, 'accumulated_submission_time': 2071.0617468357086, 'accumulated_eval_time': 89.48260450363159, 'accumulated_logging_time': 0.1137394905090332, 'global_step': 6051, 'preemption_count': 0}), (7563, {'train/accuracy': 0.45023515820503235, 'train/loss': 2.4000375270843506, 'validation/accuracy': 0.39282000064849854, 'validation/loss': 2.7543821334838867, 'validation/num_examples': 50000, 'test/accuracy': 0.2979000210762024, 'test/loss': 3.4580790996551514, 'test/num_examples': 10000, 'score': 2581.1082031726837, 'total_duration': 2690.453903913498, 'accumulated_submission_time': 2581.1082031726837, 'accumulated_eval_time': 108.95680356025696, 'accumulated_logging_time': 0.144883394241333, 'global_step': 7563, 'preemption_count': 0}), (9076, {'train/accuracy': 0.49607381224632263, 'train/loss': 2.1706013679504395, 'validation/accuracy': 0.4490000009536743, 'validation/loss': 2.427058219909668, 'validation/num_examples': 50000, 'test/accuracy': 0.3509000241756439, 'test/loss': 3.098984479904175, 'test/num_examples': 10000, 'score': 3091.184275150299, 'total_duration': 3218.2887029647827, 'accumulated_submission_time': 3091.184275150299, 'accumulated_eval_time': 126.64023494720459, 'accumulated_logging_time': 0.17103147506713867, 'global_step': 9076, 'preemption_count': 0}), (10590, {'train/accuracy': 0.5216438174247742, 'train/loss': 2.0399041175842285, 'validation/accuracy': 0.48475998640060425, 'validation/loss': 2.257338762283325, 'validation/num_examples': 50000, 'test/accuracy': 0.36410000920295715, 'test/loss': 3.029717445373535, 'test/num_examples': 10000, 'score': 3601.2611100673676, 'total_duration': 3746.2567975521088, 'accumulated_submission_time': 3601.2611100673676, 'accumulated_eval_time': 144.45068073272705, 'accumulated_logging_time': 0.2024247646331787, 'global_step': 10590, 'preemption_count': 0}), (12104, {'train/accuracy': 0.5573580861091614, 'train/loss': 1.8680070638656616, 'validation/accuracy': 0.5182600021362305, 'validation/loss': 2.074094533920288, 'validation/num_examples': 50000, 'test/accuracy': 0.39750000834465027, 'test/loss': 2.803204298019409, 'test/num_examples': 10000, 'score': 4111.291719198227, 'total_duration': 4274.436697483063, 'accumulated_submission_time': 4111.291719198227, 'accumulated_eval_time': 162.51656198501587, 'accumulated_logging_time': 0.23605084419250488, 'global_step': 12104, 'preemption_count': 0}), (13620, {'train/accuracy': 0.5631775856018066, 'train/loss': 1.82098388671875, 'validation/accuracy': 0.5280199646949768, 'validation/loss': 2.0292091369628906, 'validation/num_examples': 50000, 'test/accuracy': 0.4131000339984894, 'test/loss': 2.741434097290039, 'test/num_examples': 10000, 'score': 4621.447257757187, 'total_duration': 4802.413382053375, 'accumulated_submission_time': 4621.447257757187, 'accumulated_eval_time': 180.25822043418884, 'accumulated_logging_time': 0.2662384510040283, 'global_step': 13620, 'preemption_count': 0}), (15136, {'train/accuracy': 0.5808952450752258, 'train/loss': 1.7555890083312988, 'validation/accuracy': 0.5410000085830688, 'validation/loss': 1.951683759689331, 'validation/num_examples': 50000, 'test/accuracy': 0.41700002551078796, 'test/loss': 2.7061638832092285, 'test/num_examples': 10000, 'score': 5131.6218910217285, 'total_duration': 5330.570430994034, 'accumulated_submission_time': 5131.6218910217285, 'accumulated_eval_time': 198.1563205718994, 'accumulated_logging_time': 0.3011949062347412, 'global_step': 15136, 'preemption_count': 0}), (16652, {'train/accuracy': 0.6128427982330322, 'train/loss': 1.5677685737609863, 'validation/accuracy': 0.5491600036621094, 'validation/loss': 1.9117090702056885, 'validation/num_examples': 50000, 'test/accuracy': 0.42430001497268677, 'test/loss': 2.689218759536743, 'test/num_examples': 10000, 'score': 5641.749358415604, 'total_duration': 5858.903384923935, 'accumulated_submission_time': 5641.749358415604, 'accumulated_eval_time': 216.2797749042511, 'accumulated_logging_time': 0.3344736099243164, 'global_step': 16652, 'preemption_count': 0}), (18168, {'train/accuracy': 0.6028379797935486, 'train/loss': 1.6164047718048096, 'validation/accuracy': 0.5559599995613098, 'validation/loss': 1.8733510971069336, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.6176419258117676, 'test/num_examples': 10000, 'score': 6151.70788192749, 'total_duration': 6386.860626220703, 'accumulated_submission_time': 6151.70788192749, 'accumulated_eval_time': 234.1956398487091, 'accumulated_logging_time': 0.3683803081512451, 'global_step': 18168, 'preemption_count': 0}), (19684, {'train/accuracy': 0.6129025816917419, 'train/loss': 1.5699564218521118, 'validation/accuracy': 0.5642799735069275, 'validation/loss': 1.820169448852539, 'validation/num_examples': 50000, 'test/accuracy': 0.4443000257015228, 'test/loss': 2.5521981716156006, 'test/num_examples': 10000, 'score': 6661.777126550674, 'total_duration': 6915.075645208359, 'accumulated_submission_time': 6661.777126550674, 'accumulated_eval_time': 252.25651955604553, 'accumulated_logging_time': 0.4041624069213867, 'global_step': 19684, 'preemption_count': 0}), (21201, {'train/accuracy': 0.6129623651504517, 'train/loss': 1.5876972675323486, 'validation/accuracy': 0.5711599588394165, 'validation/loss': 1.8076725006103516, 'validation/num_examples': 50000, 'test/accuracy': 0.45000001788139343, 'test/loss': 2.5406386852264404, 'test/num_examples': 10000, 'score': 7172.218660593033, 'total_duration': 7443.592651605606, 'accumulated_submission_time': 7172.218660593033, 'accumulated_eval_time': 270.24809288978577, 'accumulated_logging_time': 0.4388735294342041, 'global_step': 21201, 'preemption_count': 0}), (22718, {'train/accuracy': 0.6120854616165161, 'train/loss': 1.5852470397949219, 'validation/accuracy': 0.5758599638938904, 'validation/loss': 1.7821930646896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4487000107765198, 'test/loss': 2.5585756301879883, 'test/num_examples': 10000, 'score': 7682.367743968964, 'total_duration': 7971.530035734177, 'accumulated_submission_time': 7682.367743968964, 'accumulated_eval_time': 287.95379066467285, 'accumulated_logging_time': 0.4722630977630615, 'global_step': 22718, 'preemption_count': 0}), (24235, {'train/accuracy': 0.6190210580825806, 'train/loss': 1.554908275604248, 'validation/accuracy': 0.5786600112915039, 'validation/loss': 1.7810800075531006, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.5390639305114746, 'test/num_examples': 10000, 'score': 8192.398866891861, 'total_duration': 8499.543118476868, 'accumulated_submission_time': 8192.398866891861, 'accumulated_eval_time': 305.84978795051575, 'accumulated_logging_time': 0.5091888904571533, 'global_step': 24235, 'preemption_count': 0}), (25753, {'train/accuracy': 0.644551157951355, 'train/loss': 1.4304097890853882, 'validation/accuracy': 0.5829799771308899, 'validation/loss': 1.7570055723190308, 'validation/num_examples': 50000, 'test/accuracy': 0.46150001883506775, 'test/loss': 2.4996163845062256, 'test/num_examples': 10000, 'score': 8702.434856891632, 'total_duration': 9027.637979269028, 'accumulated_submission_time': 8702.434856891632, 'accumulated_eval_time': 323.8264684677124, 'accumulated_logging_time': 0.5421411991119385, 'global_step': 25753, 'preemption_count': 0}), (27271, {'train/accuracy': 0.62890625, 'train/loss': 1.5057846307754517, 'validation/accuracy': 0.5762799978256226, 'validation/loss': 1.7802006006240845, 'validation/num_examples': 50000, 'test/accuracy': 0.4508000314235687, 'test/loss': 2.5254218578338623, 'test/num_examples': 10000, 'score': 9212.556456327438, 'total_duration': 9555.571353673935, 'accumulated_submission_time': 9212.556456327438, 'accumulated_eval_time': 341.5526399612427, 'accumulated_logging_time': 0.5781388282775879, 'global_step': 27271, 'preemption_count': 0}), (28788, {'train/accuracy': 0.6389309763908386, 'train/loss': 1.4611449241638184, 'validation/accuracy': 0.5892400145530701, 'validation/loss': 1.712038516998291, 'validation/num_examples': 50000, 'test/accuracy': 0.4659000337123871, 'test/loss': 2.4030990600585938, 'test/num_examples': 10000, 'score': 9722.534570932388, 'total_duration': 10083.528599262238, 'accumulated_submission_time': 9722.534570932388, 'accumulated_eval_time': 359.44764280319214, 'accumulated_logging_time': 0.6132323741912842, 'global_step': 28788, 'preemption_count': 0}), (30306, {'train/accuracy': 0.6354631781578064, 'train/loss': 1.4758671522140503, 'validation/accuracy': 0.5902199745178223, 'validation/loss': 1.7170330286026, 'validation/num_examples': 50000, 'test/accuracy': 0.4637000262737274, 'test/loss': 2.4629597663879395, 'test/num_examples': 10000, 'score': 10232.679866552353, 'total_duration': 10611.508907079697, 'accumulated_submission_time': 10232.679866552353, 'accumulated_eval_time': 377.1987464427948, 'accumulated_logging_time': 0.6481332778930664, 'global_step': 30306, 'preemption_count': 0}), (31824, {'train/accuracy': 0.6270925998687744, 'train/loss': 1.5033053159713745, 'validation/accuracy': 0.5899400115013123, 'validation/loss': 1.7156438827514648, 'validation/num_examples': 50000, 'test/accuracy': 0.46490001678466797, 'test/loss': 2.449855327606201, 'test/num_examples': 10000, 'score': 10742.683179616928, 'total_duration': 11139.474148750305, 'accumulated_submission_time': 10742.683179616928, 'accumulated_eval_time': 395.0763454437256, 'accumulated_logging_time': 0.6836209297180176, 'global_step': 31824, 'preemption_count': 0}), (33342, {'train/accuracy': 0.6690449714660645, 'train/loss': 1.3072504997253418, 'validation/accuracy': 0.5861600041389465, 'validation/loss': 1.723919153213501, 'validation/num_examples': 50000, 'test/accuracy': 0.46240001916885376, 'test/loss': 2.4317657947540283, 'test/num_examples': 10000, 'score': 11252.691735982895, 'total_duration': 11667.586690425873, 'accumulated_submission_time': 11252.691735982895, 'accumulated_eval_time': 413.09812903404236, 'accumulated_logging_time': 0.716942310333252, 'global_step': 33342, 'preemption_count': 0}), (34860, {'train/accuracy': 0.6469627022743225, 'train/loss': 1.3967673778533936, 'validation/accuracy': 0.5877000093460083, 'validation/loss': 1.7086846828460693, 'validation/num_examples': 50000, 'test/accuracy': 0.4726000130176544, 'test/loss': 2.405595541000366, 'test/num_examples': 10000, 'score': 11762.682977676392, 'total_duration': 12195.666572332382, 'accumulated_submission_time': 11762.682977676392, 'accumulated_eval_time': 431.09912037849426, 'accumulated_logging_time': 0.7553849220275879, 'global_step': 34860, 'preemption_count': 0}), (36377, {'train/accuracy': 0.6559311151504517, 'train/loss': 1.3725244998931885, 'validation/accuracy': 0.6061399579048157, 'validation/loss': 1.6266961097717285, 'validation/num_examples': 50000, 'test/accuracy': 0.47930002212524414, 'test/loss': 2.3845810890197754, 'test/num_examples': 10000, 'score': 12272.765510082245, 'total_duration': 12723.876703977585, 'accumulated_submission_time': 12272.765510082245, 'accumulated_eval_time': 449.1371431350708, 'accumulated_logging_time': 0.7958519458770752, 'global_step': 36377, 'preemption_count': 0}), (37896, {'train/accuracy': 0.6361208558082581, 'train/loss': 1.4662803411483765, 'validation/accuracy': 0.5847199559211731, 'validation/loss': 1.7373977899551392, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.486398220062256, 'test/num_examples': 10000, 'score': 12782.802032232285, 'total_duration': 13251.825589179993, 'accumulated_submission_time': 12782.802032232285, 'accumulated_eval_time': 466.961225271225, 'accumulated_logging_time': 0.8347640037536621, 'global_step': 37896, 'preemption_count': 0}), (39414, {'train/accuracy': 0.6498525142669678, 'train/loss': 1.4036788940429688, 'validation/accuracy': 0.6021599769592285, 'validation/loss': 1.6521224975585938, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.3708035945892334, 'test/num_examples': 10000, 'score': 13292.97028517723, 'total_duration': 13779.885905742645, 'accumulated_submission_time': 13292.97028517723, 'accumulated_eval_time': 484.7647216320038, 'accumulated_logging_time': 0.873910665512085, 'global_step': 39414, 'preemption_count': 0}), (40933, {'train/accuracy': 0.6477798223495483, 'train/loss': 1.4108294248580933, 'validation/accuracy': 0.6057999730110168, 'validation/loss': 1.6259907484054565, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.3724279403686523, 'test/num_examples': 10000, 'score': 13803.110150814056, 'total_duration': 14308.154326200485, 'accumulated_submission_time': 13803.110150814056, 'accumulated_eval_time': 502.8075284957886, 'accumulated_logging_time': 0.9099681377410889, 'global_step': 40933, 'preemption_count': 0}), (42453, {'train/accuracy': 0.6938576102256775, 'train/loss': 1.2015491724014282, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.6163575649261475, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.3288164138793945, 'test/num_examples': 10000, 'score': 14313.330046653748, 'total_duration': 14836.492009878159, 'accumulated_submission_time': 14313.330046653748, 'accumulated_eval_time': 520.8403308391571, 'accumulated_logging_time': 0.9460337162017822, 'global_step': 42453, 'preemption_count': 0}), (43972, {'train/accuracy': 0.6569873690605164, 'train/loss': 1.3638534545898438, 'validation/accuracy': 0.5990399718284607, 'validation/loss': 1.6688741445541382, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.3888916969299316, 'test/num_examples': 10000, 'score': 14823.266336917877, 'total_duration': 15364.582607030869, 'accumulated_submission_time': 14823.266336917877, 'accumulated_eval_time': 538.9052393436432, 'accumulated_logging_time': 0.9865646362304688, 'global_step': 43972, 'preemption_count': 0}), (45491, {'train/accuracy': 0.6528021097183228, 'train/loss': 1.3831080198287964, 'validation/accuracy': 0.6025800108909607, 'validation/loss': 1.6469758749008179, 'validation/num_examples': 50000, 'test/accuracy': 0.47790002822875977, 'test/loss': 2.3950185775756836, 'test/num_examples': 10000, 'score': 15333.213256835938, 'total_duration': 15892.32246518135, 'accumulated_submission_time': 15333.213256835938, 'accumulated_eval_time': 556.6109170913696, 'accumulated_logging_time': 1.0245742797851562, 'global_step': 45491, 'preemption_count': 0}), (47009, {'train/accuracy': 0.6518455147743225, 'train/loss': 1.3824917078018188, 'validation/accuracy': 0.6039599776268005, 'validation/loss': 1.638573408126831, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.371513843536377, 'test/num_examples': 10000, 'score': 15843.245793819427, 'total_duration': 16421.24270606041, 'accumulated_submission_time': 15843.245793819427, 'accumulated_eval_time': 575.4105768203735, 'accumulated_logging_time': 1.063220739364624, 'global_step': 47009, 'preemption_count': 0}), (48528, {'train/accuracy': 0.6499919891357422, 'train/loss': 1.4085599184036255, 'validation/accuracy': 0.602620005607605, 'validation/loss': 1.6432632207870483, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.336787700653076, 'test/num_examples': 10000, 'score': 16353.201929330826, 'total_duration': 16949.052980184555, 'accumulated_submission_time': 16353.201929330826, 'accumulated_eval_time': 593.1785054206848, 'accumulated_logging_time': 1.1000022888183594, 'global_step': 48528, 'preemption_count': 0}), (50046, {'train/accuracy': 0.6486168503761292, 'train/loss': 1.4161732196807861, 'validation/accuracy': 0.6020399928092957, 'validation/loss': 1.6556391716003418, 'validation/num_examples': 50000, 'test/accuracy': 0.4767000079154968, 'test/loss': 2.3756377696990967, 'test/num_examples': 10000, 'score': 16863.183659791946, 'total_duration': 17477.124361991882, 'accumulated_submission_time': 16863.183659791946, 'accumulated_eval_time': 611.1795015335083, 'accumulated_logging_time': 1.1396148204803467, 'global_step': 50046, 'preemption_count': 0}), (51565, {'train/accuracy': 0.6805644035339355, 'train/loss': 1.269840955734253, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.6356245279312134, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.3638558387756348, 'test/num_examples': 10000, 'score': 17373.11920595169, 'total_duration': 18005.066600322723, 'accumulated_submission_time': 17373.11920595169, 'accumulated_eval_time': 629.0958392620087, 'accumulated_logging_time': 1.1804418563842773, 'global_step': 51565, 'preemption_count': 0}), (53084, {'train/accuracy': 0.6640226244926453, 'train/loss': 1.340054988861084, 'validation/accuracy': 0.6099599599838257, 'validation/loss': 1.6146225929260254, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.3278653621673584, 'test/num_examples': 10000, 'score': 17883.04052567482, 'total_duration': 18532.7405500412, 'accumulated_submission_time': 17883.04052567482, 'accumulated_eval_time': 646.7576491832733, 'accumulated_logging_time': 1.2222459316253662, 'global_step': 53084, 'preemption_count': 0}), (54603, {'train/accuracy': 0.6684470772743225, 'train/loss': 1.310733675956726, 'validation/accuracy': 0.6151599884033203, 'validation/loss': 1.5864496231079102, 'validation/num_examples': 50000, 'test/accuracy': 0.49650001525878906, 'test/loss': 2.2576968669891357, 'test/num_examples': 10000, 'score': 18393.22454881668, 'total_duration': 19061.042494535446, 'accumulated_submission_time': 18393.22454881668, 'accumulated_eval_time': 664.7858171463013, 'accumulated_logging_time': 1.2626171112060547, 'global_step': 54603, 'preemption_count': 0}), (56122, {'train/accuracy': 0.6581233739852905, 'train/loss': 1.3662872314453125, 'validation/accuracy': 0.6144399642944336, 'validation/loss': 1.60483980178833, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.338015079498291, 'test/num_examples': 10000, 'score': 18903.22807765007, 'total_duration': 19588.880088090897, 'accumulated_submission_time': 18903.22807765007, 'accumulated_eval_time': 682.5275778770447, 'accumulated_logging_time': 1.305849313735962, 'global_step': 56122, 'preemption_count': 0}), (57640, {'train/accuracy': 0.6615911722183228, 'train/loss': 1.3483868837356567, 'validation/accuracy': 0.615559995174408, 'validation/loss': 1.5824005603790283, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.2923803329467773, 'test/num_examples': 10000, 'score': 19413.14436841011, 'total_duration': 20116.891170024872, 'accumulated_submission_time': 19413.14436841011, 'accumulated_eval_time': 700.529173374176, 'accumulated_logging_time': 1.3492977619171143, 'global_step': 57640, 'preemption_count': 0}), (59161, {'train/accuracy': 0.6617307066917419, 'train/loss': 1.3514286279678345, 'validation/accuracy': 0.6044600009918213, 'validation/loss': 1.642971396446228, 'validation/num_examples': 50000, 'test/accuracy': 0.47620001435279846, 'test/loss': 2.3952414989471436, 'test/num_examples': 10000, 'score': 19923.271131277084, 'total_duration': 20645.021767616272, 'accumulated_submission_time': 19923.271131277084, 'accumulated_eval_time': 718.4421739578247, 'accumulated_logging_time': 1.3909647464752197, 'global_step': 59161, 'preemption_count': 0}), (60682, {'train/accuracy': 0.6777144074440002, 'train/loss': 1.2699861526489258, 'validation/accuracy': 0.6165800094604492, 'validation/loss': 1.5854922533035278, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.322281837463379, 'test/num_examples': 10000, 'score': 20433.406439065933, 'total_duration': 21172.833348035812, 'accumulated_submission_time': 20433.406439065933, 'accumulated_eval_time': 736.025773525238, 'accumulated_logging_time': 1.4345412254333496, 'global_step': 60682, 'preemption_count': 0}), (62201, {'train/accuracy': 0.6781728267669678, 'train/loss': 1.2760369777679443, 'validation/accuracy': 0.6189999580383301, 'validation/loss': 1.5650577545166016, 'validation/num_examples': 50000, 'test/accuracy': 0.4894000291824341, 'test/loss': 2.293773651123047, 'test/num_examples': 10000, 'score': 20943.60581111908, 'total_duration': 21700.95312690735, 'accumulated_submission_time': 20943.60581111908, 'accumulated_eval_time': 753.8541922569275, 'accumulated_logging_time': 1.4773738384246826, 'global_step': 62201, 'preemption_count': 0}), (63721, {'train/accuracy': 0.6701809763908386, 'train/loss': 1.300941824913025, 'validation/accuracy': 0.6164000034332275, 'validation/loss': 1.5907514095306396, 'validation/num_examples': 50000, 'test/accuracy': 0.49570003151893616, 'test/loss': 2.286545991897583, 'test/num_examples': 10000, 'score': 21453.792240858078, 'total_duration': 22228.8758354187, 'accumulated_submission_time': 21453.792240858078, 'accumulated_eval_time': 771.4942946434021, 'accumulated_logging_time': 1.524343490600586, 'global_step': 63721, 'preemption_count': 0}), (65241, {'train/accuracy': 0.6570671200752258, 'train/loss': 1.3706705570220947, 'validation/accuracy': 0.6152200102806091, 'validation/loss': 1.615429162979126, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.4254047870635986, 'test/num_examples': 10000, 'score': 21963.940816640854, 'total_duration': 22756.996761083603, 'accumulated_submission_time': 21963.940816640854, 'accumulated_eval_time': 789.372960805893, 'accumulated_logging_time': 1.5687105655670166, 'global_step': 65241, 'preemption_count': 0}), (66761, {'train/accuracy': 0.6267139315605164, 'train/loss': 1.5229088068008423, 'validation/accuracy': 0.5832399725914001, 'validation/loss': 1.7533199787139893, 'validation/num_examples': 50000, 'test/accuracy': 0.45820000767707825, 'test/loss': 2.52813458442688, 'test/num_examples': 10000, 'score': 22474.066437244415, 'total_duration': 23285.236786603928, 'accumulated_submission_time': 22474.066437244415, 'accumulated_eval_time': 807.3948268890381, 'accumulated_logging_time': 1.6115176677703857, 'global_step': 66761, 'preemption_count': 0}), (68281, {'train/accuracy': 0.7253667116165161, 'train/loss': 1.0721691846847534, 'validation/accuracy': 0.6342999935150146, 'validation/loss': 1.5084197521209717, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.1850202083587646, 'test/num_examples': 10000, 'score': 22984.056749343872, 'total_duration': 23813.018038749695, 'accumulated_submission_time': 22984.056749343872, 'accumulated_eval_time': 825.0936350822449, 'accumulated_logging_time': 1.6541671752929688, 'global_step': 68281, 'preemption_count': 0}), (69799, {'train/accuracy': 0.6788305044174194, 'train/loss': 1.2620773315429688, 'validation/accuracy': 0.6158999800682068, 'validation/loss': 1.5898820161819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.3095078468322754, 'test/num_examples': 10000, 'score': 23493.977559566498, 'total_duration': 24340.83118534088, 'accumulated_submission_time': 23493.977559566498, 'accumulated_eval_time': 842.8929722309113, 'accumulated_logging_time': 1.6980137825012207, 'global_step': 69799, 'preemption_count': 0}), (71319, {'train/accuracy': 0.6802256107330322, 'train/loss': 1.253443956375122, 'validation/accuracy': 0.6263799667358398, 'validation/loss': 1.537427306175232, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2284297943115234, 'test/num_examples': 10000, 'score': 24004.088791370392, 'total_duration': 24868.941687583923, 'accumulated_submission_time': 24004.088791370392, 'accumulated_eval_time': 860.7975602149963, 'accumulated_logging_time': 1.7430338859558105, 'global_step': 71319, 'preemption_count': 0}), (72838, {'train/accuracy': 0.6843112111091614, 'train/loss': 1.2410756349563599, 'validation/accuracy': 0.6298199892044067, 'validation/loss': 1.5166643857955933, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.216010332107544, 'test/num_examples': 10000, 'score': 24514.16850876808, 'total_duration': 25397.194969654083, 'accumulated_submission_time': 24514.16850876808, 'accumulated_eval_time': 878.8791308403015, 'accumulated_logging_time': 1.7854652404785156, 'global_step': 72838, 'preemption_count': 0}), (74359, {'train/accuracy': 0.6735291481018066, 'train/loss': 1.2930454015731812, 'validation/accuracy': 0.6256600022315979, 'validation/loss': 1.5487301349639893, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.2394707202911377, 'test/num_examples': 10000, 'score': 25024.354562044144, 'total_duration': 25925.49914598465, 'accumulated_submission_time': 25024.354562044144, 'accumulated_eval_time': 896.9066410064697, 'accumulated_logging_time': 1.8266286849975586, 'global_step': 74359, 'preemption_count': 0}), (75878, {'train/accuracy': 0.6875996589660645, 'train/loss': 1.2251676321029663, 'validation/accuracy': 0.6364799737930298, 'validation/loss': 1.4780266284942627, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2012691497802734, 'test/num_examples': 10000, 'score': 25534.386019468307, 'total_duration': 26453.276960134506, 'accumulated_submission_time': 25534.386019468307, 'accumulated_eval_time': 914.560183763504, 'accumulated_logging_time': 1.8696751594543457, 'global_step': 75878, 'preemption_count': 0}), (77398, {'train/accuracy': 0.7134685516357422, 'train/loss': 1.1057634353637695, 'validation/accuracy': 0.6320599913597107, 'validation/loss': 1.5262938737869263, 'validation/num_examples': 50000, 'test/accuracy': 0.49640002846717834, 'test/loss': 2.239720582962036, 'test/num_examples': 10000, 'score': 26044.426952838898, 'total_duration': 26981.249837636948, 'accumulated_submission_time': 26044.426952838898, 'accumulated_eval_time': 932.3965713977814, 'accumulated_logging_time': 1.9160151481628418, 'global_step': 77398, 'preemption_count': 0}), (78918, {'train/accuracy': 0.7004145383834839, 'train/loss': 1.1687120199203491, 'validation/accuracy': 0.6345599889755249, 'validation/loss': 1.4978222846984863, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2021243572235107, 'test/num_examples': 10000, 'score': 26554.584349632263, 'total_duration': 27509.33651995659, 'accumulated_submission_time': 26554.584349632263, 'accumulated_eval_time': 950.2284531593323, 'accumulated_logging_time': 1.9640913009643555, 'global_step': 78918, 'preemption_count': 0}), (80438, {'train/accuracy': 0.6895527839660645, 'train/loss': 1.2138910293579102, 'validation/accuracy': 0.6326000094413757, 'validation/loss': 1.5095300674438477, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.279034376144409, 'test/num_examples': 10000, 'score': 27064.510639190674, 'total_duration': 28037.29874444008, 'accumulated_submission_time': 27064.510639190674, 'accumulated_eval_time': 968.1738333702087, 'accumulated_logging_time': 2.0051536560058594, 'global_step': 80438, 'preemption_count': 0}), (81958, {'train/accuracy': 0.6863241195678711, 'train/loss': 1.2330068349838257, 'validation/accuracy': 0.6323399543762207, 'validation/loss': 1.5172523260116577, 'validation/num_examples': 50000, 'test/accuracy': 0.5067999958992004, 'test/loss': 2.2435383796691895, 'test/num_examples': 10000, 'score': 27574.732538461685, 'total_duration': 28565.799509763718, 'accumulated_submission_time': 27574.732538461685, 'accumulated_eval_time': 986.3525202274323, 'accumulated_logging_time': 2.056394100189209, 'global_step': 81958, 'preemption_count': 0}), (83478, {'train/accuracy': 0.6814213991165161, 'train/loss': 1.2530291080474854, 'validation/accuracy': 0.6282199621200562, 'validation/loss': 1.5206059217453003, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.229276418685913, 'test/num_examples': 10000, 'score': 28084.9185359478, 'total_duration': 29094.119473934174, 'accumulated_submission_time': 28084.9185359478, 'accumulated_eval_time': 1004.3920419216156, 'accumulated_logging_time': 2.1011264324188232, 'global_step': 83478, 'preemption_count': 0}), (84998, {'train/accuracy': 0.7023875713348389, 'train/loss': 1.1722116470336914, 'validation/accuracy': 0.6421599984169006, 'validation/loss': 1.463862419128418, 'validation/num_examples': 50000, 'test/accuracy': 0.5214000344276428, 'test/loss': 2.1729736328125, 'test/num_examples': 10000, 'score': 28594.972878456116, 'total_duration': 29622.433605909348, 'accumulated_submission_time': 28594.972878456116, 'accumulated_eval_time': 1022.558931350708, 'accumulated_logging_time': 2.1451663970947266, 'global_step': 84998, 'preemption_count': 0}), (86518, {'train/accuracy': 0.7252471446990967, 'train/loss': 1.0614542961120605, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.429309368133545, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.1262402534484863, 'test/num_examples': 10000, 'score': 29105.079597473145, 'total_duration': 30150.35160303116, 'accumulated_submission_time': 29105.079597473145, 'accumulated_eval_time': 1040.2748339176178, 'accumulated_logging_time': 2.191434383392334, 'global_step': 86518, 'preemption_count': 0}), (88037, {'train/accuracy': 0.7028061151504517, 'train/loss': 1.1444787979125977, 'validation/accuracy': 0.6430799961090088, 'validation/loss': 1.4514235258102417, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.1785125732421875, 'test/num_examples': 10000, 'score': 29615.256961107254, 'total_duration': 30678.455330610275, 'accumulated_submission_time': 29615.256961107254, 'accumulated_eval_time': 1058.1022355556488, 'accumulated_logging_time': 2.2411510944366455, 'global_step': 88037, 'preemption_count': 0}), (89557, {'train/accuracy': 0.7121930718421936, 'train/loss': 1.1185708045959473, 'validation/accuracy': 0.6528599858283997, 'validation/loss': 1.4161499738693237, 'validation/num_examples': 50000, 'test/accuracy': 0.5265000462532043, 'test/loss': 2.140740156173706, 'test/num_examples': 10000, 'score': 30125.36184811592, 'total_duration': 31206.683420419693, 'accumulated_submission_time': 30125.36184811592, 'accumulated_eval_time': 1076.1300678253174, 'accumulated_logging_time': 2.2871439456939697, 'global_step': 89557, 'preemption_count': 0}), (91078, {'train/accuracy': 0.7091438174247742, 'train/loss': 1.1337120532989502, 'validation/accuracy': 0.6495800018310547, 'validation/loss': 1.4375072717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.142979145050049, 'test/num_examples': 10000, 'score': 30635.47507429123, 'total_duration': 31734.729600191116, 'accumulated_submission_time': 30635.47507429123, 'accumulated_eval_time': 1093.9589052200317, 'accumulated_logging_time': 2.341646194458008, 'global_step': 91078, 'preemption_count': 0}), (92598, {'train/accuracy': 0.7068518400192261, 'train/loss': 1.1458536386489868, 'validation/accuracy': 0.6547399759292603, 'validation/loss': 1.4125068187713623, 'validation/num_examples': 50000, 'test/accuracy': 0.5238000154495239, 'test/loss': 2.1197195053100586, 'test/num_examples': 10000, 'score': 31145.398854970932, 'total_duration': 32262.498690605164, 'accumulated_submission_time': 31145.398854970932, 'accumulated_eval_time': 1111.7064609527588, 'accumulated_logging_time': 2.390122890472412, 'global_step': 92598, 'preemption_count': 0}), (94119, {'train/accuracy': 0.7403738498687744, 'train/loss': 1.0028444528579712, 'validation/accuracy': 0.6489399671554565, 'validation/loss': 1.430841326713562, 'validation/num_examples': 50000, 'test/accuracy': 0.5188000202178955, 'test/loss': 2.1304824352264404, 'test/num_examples': 10000, 'score': 31655.375497341156, 'total_duration': 32790.37596988678, 'accumulated_submission_time': 31655.375497341156, 'accumulated_eval_time': 1129.5105464458466, 'accumulated_logging_time': 2.4372799396514893, 'global_step': 94119, 'preemption_count': 0}), (95639, {'train/accuracy': 0.7263033986091614, 'train/loss': 1.055260419845581, 'validation/accuracy': 0.6544399857521057, 'validation/loss': 1.4071980714797974, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.102889060974121, 'test/num_examples': 10000, 'score': 32165.38191127777, 'total_duration': 33318.218418598175, 'accumulated_submission_time': 32165.38191127777, 'accumulated_eval_time': 1147.2531747817993, 'accumulated_logging_time': 2.4813926219940186, 'global_step': 95639, 'preemption_count': 0}), (97159, {'train/accuracy': 0.7153021097183228, 'train/loss': 1.094274640083313, 'validation/accuracy': 0.647059977054596, 'validation/loss': 1.4412496089935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.1374266147613525, 'test/num_examples': 10000, 'score': 32675.390652418137, 'total_duration': 33846.29317569733, 'accumulated_submission_time': 32675.390652418137, 'accumulated_eval_time': 1165.2235417366028, 'accumulated_logging_time': 2.5274858474731445, 'global_step': 97159, 'preemption_count': 0}), (98679, {'train/accuracy': 0.7119140625, 'train/loss': 1.1197112798690796, 'validation/accuracy': 0.6540799736976624, 'validation/loss': 1.4109952449798584, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1488847732543945, 'test/num_examples': 10000, 'score': 33185.30989527702, 'total_duration': 34374.24014735222, 'accumulated_submission_time': 33185.30989527702, 'accumulated_eval_time': 1183.1489372253418, 'accumulated_logging_time': 2.5802924633026123, 'global_step': 98679, 'preemption_count': 0}), (100199, {'train/accuracy': 0.7194873690605164, 'train/loss': 1.0825355052947998, 'validation/accuracy': 0.6628199815750122, 'validation/loss': 1.372917652130127, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.116262435913086, 'test/num_examples': 10000, 'score': 33695.51848888397, 'total_duration': 34902.30881524086, 'accumulated_submission_time': 33695.51848888397, 'accumulated_eval_time': 1200.9140849113464, 'accumulated_logging_time': 2.625534772872925, 'global_step': 100199, 'preemption_count': 0}), (101718, {'train/accuracy': 0.7230349183082581, 'train/loss': 1.0770888328552246, 'validation/accuracy': 0.6646599769592285, 'validation/loss': 1.3627052307128906, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.0898184776306152, 'test/num_examples': 10000, 'score': 34205.558450460434, 'total_duration': 35430.43071985245, 'accumulated_submission_time': 34205.558450460434, 'accumulated_eval_time': 1218.8947851657867, 'accumulated_logging_time': 2.6776158809661865, 'global_step': 101718, 'preemption_count': 0}), (103239, {'train/accuracy': 0.7520726919174194, 'train/loss': 0.9424518346786499, 'validation/accuracy': 0.662559986114502, 'validation/loss': 1.3745607137680054, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.0718114376068115, 'test/num_examples': 10000, 'score': 34715.77932167053, 'total_duration': 35958.745235681534, 'accumulated_submission_time': 34715.77932167053, 'accumulated_eval_time': 1236.8908894062042, 'accumulated_logging_time': 2.725346565246582, 'global_step': 103239, 'preemption_count': 0}), (104760, {'train/accuracy': 0.7437419891357422, 'train/loss': 0.9715170860290527, 'validation/accuracy': 0.6689199805259705, 'validation/loss': 1.338512659072876, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.064157247543335, 'test/num_examples': 10000, 'score': 35225.935331106186, 'total_duration': 36486.63972115517, 'accumulated_submission_time': 35225.935331106186, 'accumulated_eval_time': 1254.5282595157623, 'accumulated_logging_time': 2.777066469192505, 'global_step': 104760, 'preemption_count': 0}), (106281, {'train/accuracy': 0.7352120280265808, 'train/loss': 1.0203943252563477, 'validation/accuracy': 0.6659799814224243, 'validation/loss': 1.3560991287231445, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.052748441696167, 'test/num_examples': 10000, 'score': 35736.099576711655, 'total_duration': 37014.51857161522, 'accumulated_submission_time': 35736.099576711655, 'accumulated_eval_time': 1272.1391229629517, 'accumulated_logging_time': 2.8311715126037598, 'global_step': 106281, 'preemption_count': 0}), (107801, {'train/accuracy': 0.7411311864852905, 'train/loss': 0.9817953109741211, 'validation/accuracy': 0.675279974937439, 'validation/loss': 1.3169835805892944, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 1.9979313611984253, 'test/num_examples': 10000, 'score': 36246.10291814804, 'total_duration': 37542.64195537567, 'accumulated_submission_time': 36246.10291814804, 'accumulated_eval_time': 1290.1619033813477, 'accumulated_logging_time': 2.8791487216949463, 'global_step': 107801, 'preemption_count': 0}), (109321, {'train/accuracy': 0.7330197691917419, 'train/loss': 1.0413142442703247, 'validation/accuracy': 0.670960009098053, 'validation/loss': 1.3357013463974, 'validation/num_examples': 50000, 'test/accuracy': 0.5401000380516052, 'test/loss': 2.053678274154663, 'test/num_examples': 10000, 'score': 36756.17011857033, 'total_duration': 38070.74506354332, 'accumulated_submission_time': 36756.17011857033, 'accumulated_eval_time': 1308.0986626148224, 'accumulated_logging_time': 2.928966760635376, 'global_step': 109321, 'preemption_count': 0}), (110842, {'train/accuracy': 0.7404536008834839, 'train/loss': 0.9842280745506287, 'validation/accuracy': 0.675819993019104, 'validation/loss': 1.313747763633728, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.0273501873016357, 'test/num_examples': 10000, 'score': 37266.255848646164, 'total_duration': 38598.62584900856, 'accumulated_submission_time': 37266.255848646164, 'accumulated_eval_time': 1325.7961132526398, 'accumulated_logging_time': 2.9773285388946533, 'global_step': 110842, 'preemption_count': 0}), (112363, {'train/accuracy': 0.7700294852256775, 'train/loss': 0.8577762246131897, 'validation/accuracy': 0.6807999610900879, 'validation/loss': 1.2912861108779907, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.9689432382583618, 'test/num_examples': 10000, 'score': 37776.244634628296, 'total_duration': 39126.41235136986, 'accumulated_submission_time': 37776.244634628296, 'accumulated_eval_time': 1343.4937970638275, 'accumulated_logging_time': 3.02756404876709, 'global_step': 112363, 'preemption_count': 0}), (113883, {'train/accuracy': 0.7518534660339355, 'train/loss': 0.9311055541038513, 'validation/accuracy': 0.6769199967384338, 'validation/loss': 1.313949704170227, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.041203260421753, 'test/num_examples': 10000, 'score': 38286.34936618805, 'total_duration': 39654.57463693619, 'accumulated_submission_time': 38286.34936618805, 'accumulated_eval_time': 1361.454597234726, 'accumulated_logging_time': 3.0750892162323, 'global_step': 113883, 'preemption_count': 0}), (115404, {'train/accuracy': 0.7578722834587097, 'train/loss': 0.9229457974433899, 'validation/accuracy': 0.6810199618339539, 'validation/loss': 1.289506435394287, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 1.9755940437316895, 'test/num_examples': 10000, 'score': 38796.49595832825, 'total_duration': 40182.678636074066, 'accumulated_submission_time': 38796.49595832825, 'accumulated_eval_time': 1379.3148682117462, 'accumulated_logging_time': 3.1231749057769775, 'global_step': 115404, 'preemption_count': 0}), (116925, {'train/accuracy': 0.7563177347183228, 'train/loss': 0.9218325614929199, 'validation/accuracy': 0.6868000030517578, 'validation/loss': 1.2699737548828125, 'validation/num_examples': 50000, 'test/accuracy': 0.563800036907196, 'test/loss': 1.9535342454910278, 'test/num_examples': 10000, 'score': 39306.57546567917, 'total_duration': 40710.75027012825, 'accumulated_submission_time': 39306.57546567917, 'accumulated_eval_time': 1397.2055568695068, 'accumulated_logging_time': 3.1755475997924805, 'global_step': 116925, 'preemption_count': 0}), (118447, {'train/accuracy': 0.7603236436843872, 'train/loss': 0.9137925505638123, 'validation/accuracy': 0.6899399757385254, 'validation/loss': 1.2629296779632568, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 1.9791010618209839, 'test/num_examples': 10000, 'score': 39816.80427622795, 'total_duration': 41238.8702480793, 'accumulated_submission_time': 39816.80427622795, 'accumulated_eval_time': 1414.9953515529633, 'accumulated_logging_time': 3.2276132106781006, 'global_step': 118447, 'preemption_count': 0}), (119967, {'train/accuracy': 0.7940250039100647, 'train/loss': 0.7722108364105225, 'validation/accuracy': 0.6904199719429016, 'validation/loss': 1.2503635883331299, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 1.9592362642288208, 'test/num_examples': 10000, 'score': 40326.800953388214, 'total_duration': 41766.923069000244, 'accumulated_submission_time': 40326.800953388214, 'accumulated_eval_time': 1432.9292194843292, 'accumulated_logging_time': 3.300215721130371, 'global_step': 119967, 'preemption_count': 0}), (121487, {'train/accuracy': 0.7747727632522583, 'train/loss': 0.8482697010040283, 'validation/accuracy': 0.6867799758911133, 'validation/loss': 1.2761807441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 1.9938021898269653, 'test/num_examples': 10000, 'score': 40836.91822504997, 'total_duration': 42294.90674686432, 'accumulated_submission_time': 40836.91822504997, 'accumulated_eval_time': 1450.694310426712, 'accumulated_logging_time': 3.352198362350464, 'global_step': 121487, 'preemption_count': 0}), (123007, {'train/accuracy': 0.762137234210968, 'train/loss': 0.8999484181404114, 'validation/accuracy': 0.6813399791717529, 'validation/loss': 1.2865992784500122, 'validation/num_examples': 50000, 'test/accuracy': 0.5526000261306763, 'test/loss': 2.0052297115325928, 'test/num_examples': 10000, 'score': 41346.84619688988, 'total_duration': 42822.92417263985, 'accumulated_submission_time': 41346.84619688988, 'accumulated_eval_time': 1468.680810213089, 'accumulated_logging_time': 3.4059770107269287, 'global_step': 123007, 'preemption_count': 0}), (124527, {'train/accuracy': 0.7736367583274841, 'train/loss': 0.8483324646949768, 'validation/accuracy': 0.692579984664917, 'validation/loss': 1.2330009937286377, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.933935284614563, 'test/num_examples': 10000, 'score': 41856.85910201073, 'total_duration': 43351.22230386734, 'accumulated_submission_time': 41856.85910201073, 'accumulated_eval_time': 1486.8664498329163, 'accumulated_logging_time': 3.4564247131347656, 'global_step': 124527, 'preemption_count': 0}), (126047, {'train/accuracy': 0.7742147445678711, 'train/loss': 0.844782292842865, 'validation/accuracy': 0.6972999572753906, 'validation/loss': 1.2068699598312378, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 1.8856645822525024, 'test/num_examples': 10000, 'score': 42366.83264923096, 'total_duration': 43879.25383400917, 'accumulated_submission_time': 42366.83264923096, 'accumulated_eval_time': 1504.823757648468, 'accumulated_logging_time': 3.5072178840637207, 'global_step': 126047, 'preemption_count': 0}), (127567, {'train/accuracy': 0.7678172588348389, 'train/loss': 0.8690696358680725, 'validation/accuracy': 0.6976400017738342, 'validation/loss': 1.232379674911499, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.928336262702942, 'test/num_examples': 10000, 'score': 42876.883296728134, 'total_duration': 44407.14696741104, 'accumulated_submission_time': 42876.883296728134, 'accumulated_eval_time': 1522.5617182254791, 'accumulated_logging_time': 3.56189227104187, 'global_step': 127567, 'preemption_count': 0}), (129088, {'train/accuracy': 0.8156289458274841, 'train/loss': 0.6751767992973328, 'validation/accuracy': 0.7046799659729004, 'validation/loss': 1.1848829984664917, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.8793182373046875, 'test/num_examples': 10000, 'score': 43387.103593587875, 'total_duration': 44935.282566308975, 'accumulated_submission_time': 43387.103593587875, 'accumulated_eval_time': 1540.3736391067505, 'accumulated_logging_time': 3.616084575653076, 'global_step': 129088, 'preemption_count': 0}), (130608, {'train/accuracy': 0.7964963316917419, 'train/loss': 0.7662532925605774, 'validation/accuracy': 0.7048999667167664, 'validation/loss': 1.1896097660064697, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.8776094913482666, 'test/num_examples': 10000, 'score': 43897.194222450256, 'total_duration': 45463.11754608154, 'accumulated_submission_time': 43897.194222450256, 'accumulated_eval_time': 1558.0184445381165, 'accumulated_logging_time': 3.6664631366729736, 'global_step': 130608, 'preemption_count': 0}), (132129, {'train/accuracy': 0.7948620915412903, 'train/loss': 0.7571321725845337, 'validation/accuracy': 0.7081599831581116, 'validation/loss': 1.174660325050354, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.8872145414352417, 'test/num_examples': 10000, 'score': 44407.264327287674, 'total_duration': 45991.0145611763, 'accumulated_submission_time': 44407.264327287674, 'accumulated_eval_time': 1575.7394952774048, 'accumulated_logging_time': 3.723146915435791, 'global_step': 132129, 'preemption_count': 0}), (133651, {'train/accuracy': 0.7874082922935486, 'train/loss': 0.7908145189285278, 'validation/accuracy': 0.7020399570465088, 'validation/loss': 1.193451166152954, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.8885688781738281, 'test/num_examples': 10000, 'score': 44917.387357234955, 'total_duration': 46519.360219955444, 'accumulated_submission_time': 44917.387357234955, 'accumulated_eval_time': 1593.8583455085754, 'accumulated_logging_time': 3.7774014472961426, 'global_step': 133651, 'preemption_count': 0}), (135172, {'train/accuracy': 0.8009805083274841, 'train/loss': 0.7329120635986328, 'validation/accuracy': 0.7127000093460083, 'validation/loss': 1.1449772119522095, 'validation/num_examples': 50000, 'test/accuracy': 0.5895000100135803, 'test/loss': 1.8212326765060425, 'test/num_examples': 10000, 'score': 45427.400640010834, 'total_duration': 47047.412162303925, 'accumulated_submission_time': 45427.400640010834, 'accumulated_eval_time': 1611.7913794517517, 'accumulated_logging_time': 3.833778142929077, 'global_step': 135172, 'preemption_count': 0}), (136692, {'train/accuracy': 0.7917529940605164, 'train/loss': 0.7613667845726013, 'validation/accuracy': 0.7073599696159363, 'validation/loss': 1.1776471138000488, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8573424816131592, 'test/num_examples': 10000, 'score': 45937.60295057297, 'total_duration': 47575.49288535118, 'accumulated_submission_time': 45937.60295057297, 'accumulated_eval_time': 1629.5676186084747, 'accumulated_logging_time': 3.88659930229187, 'global_step': 136692, 'preemption_count': 0}), (138213, {'train/accuracy': 0.8251953125, 'train/loss': 0.6313984990119934, 'validation/accuracy': 0.7180399894714355, 'validation/loss': 1.1386464834213257, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.820005178451538, 'test/num_examples': 10000, 'score': 46447.6070895195, 'total_duration': 48103.48945856094, 'accumulated_submission_time': 46447.6070895195, 'accumulated_eval_time': 1647.4524323940277, 'accumulated_logging_time': 3.945059061050415, 'global_step': 138213, 'preemption_count': 0}), (139735, {'train/accuracy': 0.8220463991165161, 'train/loss': 0.6473369002342224, 'validation/accuracy': 0.7188199758529663, 'validation/loss': 1.1304951906204224, 'validation/num_examples': 50000, 'test/accuracy': 0.5910000205039978, 'test/loss': 1.8218023777008057, 'test/num_examples': 10000, 'score': 46957.71826648712, 'total_duration': 48631.36492419243, 'accumulated_submission_time': 46957.71826648712, 'accumulated_eval_time': 1665.1089329719543, 'accumulated_logging_time': 4.003594875335693, 'global_step': 139735, 'preemption_count': 0}), (141255, {'train/accuracy': 0.8222058415412903, 'train/loss': 0.6530354619026184, 'validation/accuracy': 0.7206199765205383, 'validation/loss': 1.1245300769805908, 'validation/num_examples': 50000, 'test/accuracy': 0.597100019454956, 'test/loss': 1.8244554996490479, 'test/num_examples': 10000, 'score': 47467.91790008545, 'total_duration': 49159.58340501785, 'accumulated_submission_time': 47467.91790008545, 'accumulated_eval_time': 1683.0212621688843, 'accumulated_logging_time': 4.060739040374756, 'global_step': 141255, 'preemption_count': 0}), (142775, {'train/accuracy': 0.8192163705825806, 'train/loss': 0.6568843126296997, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.1111185550689697, 'validation/num_examples': 50000, 'test/accuracy': 0.5913000106811523, 'test/loss': 1.8181012868881226, 'test/num_examples': 10000, 'score': 47978.107671022415, 'total_duration': 49687.80672287941, 'accumulated_submission_time': 47978.107671022415, 'accumulated_eval_time': 1700.9479422569275, 'accumulated_logging_time': 4.1180760860443115, 'global_step': 142775, 'preemption_count': 0}), (144296, {'train/accuracy': 0.8274872303009033, 'train/loss': 0.6358198523521423, 'validation/accuracy': 0.7251600027084351, 'validation/loss': 1.0935237407684326, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.785703420639038, 'test/num_examples': 10000, 'score': 48488.267067193985, 'total_duration': 50215.87666511536, 'accumulated_submission_time': 48488.267067193985, 'accumulated_eval_time': 1718.752968788147, 'accumulated_logging_time': 4.174353361129761, 'global_step': 144296, 'preemption_count': 0}), (145817, {'train/accuracy': 0.8562061190605164, 'train/loss': 0.5220614075660706, 'validation/accuracy': 0.7297799587249756, 'validation/loss': 1.0798618793487549, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.7527598142623901, 'test/num_examples': 10000, 'score': 48998.324323415756, 'total_duration': 50744.13141441345, 'accumulated_submission_time': 48998.324323415756, 'accumulated_eval_time': 1736.8405735492706, 'accumulated_logging_time': 4.234951734542847, 'global_step': 145817, 'preemption_count': 0}), (147337, {'train/accuracy': 0.8483338356018066, 'train/loss': 0.5386354327201843, 'validation/accuracy': 0.7303999662399292, 'validation/loss': 1.0842643976211548, 'validation/num_examples': 50000, 'test/accuracy': 0.604200005531311, 'test/loss': 1.7619547843933105, 'test/num_examples': 10000, 'score': 49508.459768772125, 'total_duration': 51272.14950942993, 'accumulated_submission_time': 49508.459768772125, 'accumulated_eval_time': 1754.6182186603546, 'accumulated_logging_time': 4.2909040451049805, 'global_step': 147337, 'preemption_count': 0}), (148857, {'train/accuracy': 0.8486328125, 'train/loss': 0.5442114472389221, 'validation/accuracy': 0.7340599894523621, 'validation/loss': 1.065670132637024, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.7632725238800049, 'test/num_examples': 10000, 'score': 50018.51382923126, 'total_duration': 51799.88316822052, 'accumulated_submission_time': 50018.51382923126, 'accumulated_eval_time': 1772.1927328109741, 'accumulated_logging_time': 4.347080707550049, 'global_step': 148857, 'preemption_count': 0}), (150377, {'train/accuracy': 0.8487922549247742, 'train/loss': 0.5424256324768066, 'validation/accuracy': 0.7318800091743469, 'validation/loss': 1.0758512020111084, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.749652624130249, 'test/num_examples': 10000, 'score': 50528.65114068985, 'total_duration': 52327.87962150574, 'accumulated_submission_time': 50528.65114068985, 'accumulated_eval_time': 1789.946456670761, 'accumulated_logging_time': 4.4032227993011475, 'global_step': 150377, 'preemption_count': 0}), (151898, {'train/accuracy': 0.8529974222183228, 'train/loss': 0.52959805727005, 'validation/accuracy': 0.7369999885559082, 'validation/loss': 1.0513129234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.6142000555992126, 'test/loss': 1.7403059005737305, 'test/num_examples': 10000, 'score': 51038.67733478546, 'total_duration': 52855.8038623333, 'accumulated_submission_time': 51038.67733478546, 'accumulated_eval_time': 1807.7406420707703, 'accumulated_logging_time': 4.458520889282227, 'global_step': 151898, 'preemption_count': 0}), (153418, {'train/accuracy': 0.8550103306770325, 'train/loss': 0.5078051090240479, 'validation/accuracy': 0.7404800057411194, 'validation/loss': 1.0449867248535156, 'validation/num_examples': 50000, 'test/accuracy': 0.609000027179718, 'test/loss': 1.75309419631958, 'test/num_examples': 10000, 'score': 51548.73529744148, 'total_duration': 53383.79339146614, 'accumulated_submission_time': 51548.73529744148, 'accumulated_eval_time': 1825.565360069275, 'accumulated_logging_time': 4.516251564025879, 'global_step': 153418, 'preemption_count': 0}), (154938, {'train/accuracy': 0.8839883208274841, 'train/loss': 0.40737801790237427, 'validation/accuracy': 0.7401799559593201, 'validation/loss': 1.042005181312561, 'validation/num_examples': 50000, 'test/accuracy': 0.617400050163269, 'test/loss': 1.7246414422988892, 'test/num_examples': 10000, 'score': 52058.83040165901, 'total_duration': 53911.61543941498, 'accumulated_submission_time': 52058.83040165901, 'accumulated_eval_time': 1843.1833944320679, 'accumulated_logging_time': 4.576179504394531, 'global_step': 154938, 'preemption_count': 0}), (156460, {'train/accuracy': 0.8816167116165161, 'train/loss': 0.4281372129917145, 'validation/accuracy': 0.7430399656295776, 'validation/loss': 1.0246366262435913, 'validation/num_examples': 50000, 'test/accuracy': 0.619100034236908, 'test/loss': 1.7108631134033203, 'test/num_examples': 10000, 'score': 52568.992547512054, 'total_duration': 54439.52820849419, 'accumulated_submission_time': 52568.992547512054, 'accumulated_eval_time': 1860.8247520923615, 'accumulated_logging_time': 4.636116027832031, 'global_step': 156460, 'preemption_count': 0}), (157980, {'train/accuracy': 0.8804408311843872, 'train/loss': 0.4245630204677582, 'validation/accuracy': 0.7468999624252319, 'validation/loss': 1.0233200788497925, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.7170692682266235, 'test/num_examples': 10000, 'score': 53079.10682106018, 'total_duration': 54967.46635508537, 'accumulated_submission_time': 53079.10682106018, 'accumulated_eval_time': 1878.545128583908, 'accumulated_logging_time': 4.690703630447388, 'global_step': 157980, 'preemption_count': 0}), (159501, {'train/accuracy': 0.8826530575752258, 'train/loss': 0.41485026478767395, 'validation/accuracy': 0.7451199889183044, 'validation/loss': 1.0272034406661987, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.7209504842758179, 'test/num_examples': 10000, 'score': 53589.45731592178, 'total_duration': 55495.64396739006, 'accumulated_submission_time': 53589.45731592178, 'accumulated_eval_time': 1896.2643899917603, 'accumulated_logging_time': 4.749074697494507, 'global_step': 159501, 'preemption_count': 0}), (161022, {'train/accuracy': 0.8878945708274841, 'train/loss': 0.39847174286842346, 'validation/accuracy': 0.7486799955368042, 'validation/loss': 1.0120301246643066, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.695212960243225, 'test/num_examples': 10000, 'score': 54099.56357502937, 'total_duration': 56023.67906188965, 'accumulated_submission_time': 54099.56357502937, 'accumulated_eval_time': 1914.0802400112152, 'accumulated_logging_time': 4.812868118286133, 'global_step': 161022, 'preemption_count': 0}), (162541, {'train/accuracy': 0.8908840417861938, 'train/loss': 0.3822648227214813, 'validation/accuracy': 0.751259982585907, 'validation/loss': 1.008744239807129, 'validation/num_examples': 50000, 'test/accuracy': 0.6285000443458557, 'test/loss': 1.6943116188049316, 'test/num_examples': 10000, 'score': 54609.65087771416, 'total_duration': 56551.9337553978, 'accumulated_submission_time': 54609.65087771416, 'accumulated_eval_time': 1932.1414377689362, 'accumulated_logging_time': 4.868780136108398, 'global_step': 162541, 'preemption_count': 0}), (164062, {'train/accuracy': 0.9062101244926453, 'train/loss': 0.3281794488430023, 'validation/accuracy': 0.7523199915885925, 'validation/loss': 1.0044119358062744, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.7114315032958984, 'test/num_examples': 10000, 'score': 55119.58855962753, 'total_duration': 57079.75123023987, 'accumulated_submission_time': 55119.58855962753, 'accumulated_eval_time': 1949.9157021045685, 'accumulated_logging_time': 4.9245476722717285, 'global_step': 164062, 'preemption_count': 0}), (165582, {'train/accuracy': 0.9041573405265808, 'train/loss': 0.3352792263031006, 'validation/accuracy': 0.7546600103378296, 'validation/loss': 0.9923732280731201, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.6897133588790894, 'test/num_examples': 10000, 'score': 55629.49757552147, 'total_duration': 57607.30726504326, 'accumulated_submission_time': 55629.49757552147, 'accumulated_eval_time': 1967.4555170536041, 'accumulated_logging_time': 4.982961177825928, 'global_step': 165582, 'preemption_count': 0}), (167102, {'train/accuracy': 0.9091398119926453, 'train/loss': 0.32033541798591614, 'validation/accuracy': 0.7573999762535095, 'validation/loss': 0.9870730042457581, 'validation/num_examples': 50000, 'test/accuracy': 0.6339000463485718, 'test/loss': 1.6737549304962158, 'test/num_examples': 10000, 'score': 56139.54078269005, 'total_duration': 58135.12322330475, 'accumulated_submission_time': 56139.54078269005, 'accumulated_eval_time': 1985.1188147068024, 'accumulated_logging_time': 5.042438745498657, 'global_step': 167102, 'preemption_count': 0}), (168623, {'train/accuracy': 0.9107740521430969, 'train/loss': 0.31885087490081787, 'validation/accuracy': 0.7586399912834167, 'validation/loss': 0.9830759763717651, 'validation/num_examples': 50000, 'test/accuracy': 0.6332000494003296, 'test/loss': 1.6756737232208252, 'test/num_examples': 10000, 'score': 56649.45196771622, 'total_duration': 58663.021944761276, 'accumulated_submission_time': 56649.45196771622, 'accumulated_eval_time': 2002.999568939209, 'accumulated_logging_time': 5.100426197052002, 'global_step': 168623, 'preemption_count': 0}), (170143, {'train/accuracy': 0.9118303656578064, 'train/loss': 0.30593132972717285, 'validation/accuracy': 0.7596799731254578, 'validation/loss': 0.9778745174407959, 'validation/num_examples': 50000, 'test/accuracy': 0.6368000507354736, 'test/loss': 1.6767271757125854, 'test/num_examples': 10000, 'score': 57159.36017847061, 'total_duration': 59190.963366508484, 'accumulated_submission_time': 57159.36017847061, 'accumulated_eval_time': 2020.919572353363, 'accumulated_logging_time': 5.164439678192139, 'global_step': 170143, 'preemption_count': 0}), (171663, {'train/accuracy': 0.9235889315605164, 'train/loss': 0.2766060531139374, 'validation/accuracy': 0.7594000101089478, 'validation/loss': 0.9787272214889526, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.6702979803085327, 'test/num_examples': 10000, 'score': 57669.37860417366, 'total_duration': 59719.016202926636, 'accumulated_submission_time': 57669.37860417366, 'accumulated_eval_time': 2038.8429753780365, 'accumulated_logging_time': 5.226194143295288, 'global_step': 171663, 'preemption_count': 0}), (173183, {'train/accuracy': 0.9255819320678711, 'train/loss': 0.2669574022293091, 'validation/accuracy': 0.7631999850273132, 'validation/loss': 0.9670901298522949, 'validation/num_examples': 50000, 'test/accuracy': 0.6374000310897827, 'test/loss': 1.6677082777023315, 'test/num_examples': 10000, 'score': 58179.60224986076, 'total_duration': 60247.089400053024, 'accumulated_submission_time': 58179.60224986076, 'accumulated_eval_time': 2056.5816910266876, 'accumulated_logging_time': 5.2877233028411865, 'global_step': 173183, 'preemption_count': 0}), (174703, {'train/accuracy': 0.927754282951355, 'train/loss': 0.2576439678668976, 'validation/accuracy': 0.7628799676895142, 'validation/loss': 0.9702308177947998, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.670340895652771, 'test/num_examples': 10000, 'score': 58689.71545791626, 'total_duration': 60775.03505182266, 'accumulated_submission_time': 58689.71545791626, 'accumulated_eval_time': 2074.306261062622, 'accumulated_logging_time': 5.346015691757202, 'global_step': 174703, 'preemption_count': 0}), (176222, {'train/accuracy': 0.9254623651504517, 'train/loss': 0.26531344652175903, 'validation/accuracy': 0.7633199691772461, 'validation/loss': 0.9638875722885132, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6606649160385132, 'test/num_examples': 10000, 'score': 59199.76896715164, 'total_duration': 61303.05489516258, 'accumulated_submission_time': 59199.76896715164, 'accumulated_eval_time': 2092.1598665714264, 'accumulated_logging_time': 5.409138917922974, 'global_step': 176222, 'preemption_count': 0}), (177741, {'train/accuracy': 0.9296875, 'train/loss': 0.254580557346344, 'validation/accuracy': 0.7645799517631531, 'validation/loss': 0.9602543711662292, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.6592662334442139, 'test/num_examples': 10000, 'score': 59709.756935596466, 'total_duration': 61831.0673494339, 'accumulated_submission_time': 59709.756935596466, 'accumulated_eval_time': 2110.070912361145, 'accumulated_logging_time': 5.473035573959351, 'global_step': 177741, 'preemption_count': 0}), (179261, {'train/accuracy': 0.9309031963348389, 'train/loss': 0.25196245312690735, 'validation/accuracy': 0.7645399570465088, 'validation/loss': 0.9593108296394348, 'validation/num_examples': 50000, 'test/accuracy': 0.6409000158309937, 'test/loss': 1.654666543006897, 'test/num_examples': 10000, 'score': 60219.763149023056, 'total_duration': 62358.809020757675, 'accumulated_submission_time': 60219.763149023056, 'accumulated_eval_time': 2127.696268796921, 'accumulated_logging_time': 5.533898830413818, 'global_step': 179261, 'preemption_count': 0}), (180781, {'train/accuracy': 0.9323779940605164, 'train/loss': 0.244142547249794, 'validation/accuracy': 0.76419997215271, 'validation/loss': 0.958231508731842, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.6557667255401611, 'test/num_examples': 10000, 'score': 60729.83857727051, 'total_duration': 62887.02244234085, 'accumulated_submission_time': 60729.83857727051, 'accumulated_eval_time': 2145.7255239486694, 'accumulated_logging_time': 5.593425512313843, 'global_step': 180781, 'preemption_count': 0}), (182301, {'train/accuracy': 0.9343510866165161, 'train/loss': 0.239544078707695, 'validation/accuracy': 0.7648999691009521, 'validation/loss': 0.9564946889877319, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.6545675992965698, 'test/num_examples': 10000, 'score': 61240.160959005356, 'total_duration': 63415.34263134003, 'accumulated_submission_time': 61240.160959005356, 'accumulated_eval_time': 2163.610629081726, 'accumulated_logging_time': 5.656862735748291, 'global_step': 182301, 'preemption_count': 0}), (183820, {'train/accuracy': 0.9310028553009033, 'train/loss': 0.24653080105781555, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 0.9564112424850464, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.6530959606170654, 'test/num_examples': 10000, 'score': 61750.23178052902, 'total_duration': 63943.237928152084, 'accumulated_submission_time': 61750.23178052902, 'accumulated_eval_time': 2181.3248648643494, 'accumulated_logging_time': 5.717596530914307, 'global_step': 183820, 'preemption_count': 0}), (185340, {'train/accuracy': 0.9342713356018066, 'train/loss': 0.2396325170993805, 'validation/accuracy': 0.765779972076416, 'validation/loss': 0.95612633228302, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.6519469022750854, 'test/num_examples': 10000, 'score': 62260.25123000145, 'total_duration': 64471.24368929863, 'accumulated_submission_time': 62260.25123000145, 'accumulated_eval_time': 2199.1994581222534, 'accumulated_logging_time': 5.779508590698242, 'global_step': 185340, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9329161047935486, 'train/loss': 0.24513345956802368, 'validation/accuracy': 0.7657399773597717, 'validation/loss': 0.9568769335746765, 'validation/num_examples': 50000, 'test/accuracy': 0.640500009059906, 'test/loss': 1.6542025804519653, 'test/num_examples': 10000, 'score': 62705.71089506149, 'total_duration': 64934.39106178284, 'accumulated_submission_time': 62705.71089506149, 'accumulated_eval_time': 2216.782294511795, 'accumulated_logging_time': 5.841466188430786, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0129 07:48:44.959206 140169137129280 submission_runner.py:586] Timing: 62705.71089506149
I0129 07:48:44.959284 140169137129280 submission_runner.py:588] Total number of evals: 124
I0129 07:48:44.959327 140169137129280 submission_runner.py:589] ====================
I0129 07:48:44.959372 140169137129280 submission_runner.py:542] Using RNG seed 3614520272
I0129 07:48:44.960791 140169137129280 submission_runner.py:551] --- Tuning run 4/5 ---
I0129 07:48:44.960928 140169137129280 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4.
I0129 07:48:44.961812 140169137129280 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4/hparams.json.
I0129 07:48:44.962652 140169137129280 submission_runner.py:206] Initializing dataset.
I0129 07:48:44.971485 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 07:48:44.981460 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 07:48:45.162229 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 07:48:45.397122 140169137129280 submission_runner.py:213] Initializing model.
I0129 07:48:50.984061 140169137129280 submission_runner.py:255] Initializing optimizer.
I0129 07:48:51.371596 140169137129280 submission_runner.py:262] Initializing metrics bundle.
I0129 07:48:51.371766 140169137129280 submission_runner.py:280] Initializing checkpoint and logger.
I0129 07:48:51.386727 140169137129280 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0129 07:48:51.386851 140169137129280 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 07:49:04.266758 140169137129280 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 07:49:16.947968 140169137129280 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4/flags_0.json.
I0129 07:49:16.952469 140169137129280 submission_runner.py:314] Starting training loop.
I0129 07:49:50.054382 140005288683264 logging_writer.py:48] [0] global_step=0, grad_norm=0.6634446382522583, loss=6.919498443603516
I0129 07:49:50.068089 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:49:56.426849 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:50:05.372728 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:50:08.065265 140169137129280 submission_runner.py:408] Time since start: 51.11s, 	Step: 1, 	{'train/accuracy': 0.0009167729294858873, 'train/loss': 6.91040563583374, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 33.11548709869385, 'total_duration': 51.11273193359375, 'accumulated_submission_time': 33.11548709869385, 'accumulated_eval_time': 17.99710965156555, 'accumulated_logging_time': 0}
I0129 07:50:08.075309 140004624946944 logging_writer.py:48] [1] accumulated_eval_time=17.997110, accumulated_logging_time=0, accumulated_submission_time=33.115487, global_step=1, preemption_count=0, score=33.115487, test/accuracy=0.000600, test/loss=6.910250, test/num_examples=10000, total_duration=51.112732, train/accuracy=0.000917, train/loss=6.910406, validation/accuracy=0.000960, validation/loss=6.910244, validation/num_examples=50000
I0129 07:50:41.650254 140005288683264 logging_writer.py:48] [100] global_step=100, grad_norm=0.7601842284202576, loss=6.626143455505371
I0129 07:51:15.343128 140004624946944 logging_writer.py:48] [200] global_step=200, grad_norm=0.9262269139289856, loss=6.222400665283203
I0129 07:51:49.067545 140005288683264 logging_writer.py:48] [300] global_step=300, grad_norm=2.515761137008667, loss=5.930577754974365
I0129 07:52:22.753875 140004624946944 logging_writer.py:48] [400] global_step=400, grad_norm=2.543865203857422, loss=5.675940990447998
I0129 07:52:56.491913 140005288683264 logging_writer.py:48] [500] global_step=500, grad_norm=4.243794918060303, loss=5.494344711303711
I0129 07:53:30.200036 140004624946944 logging_writer.py:48] [600] global_step=600, grad_norm=3.8333775997161865, loss=5.335437774658203
I0129 07:54:03.951198 140005288683264 logging_writer.py:48] [700] global_step=700, grad_norm=5.4438862800598145, loss=5.17056941986084
I0129 07:54:37.682290 140004624946944 logging_writer.py:48] [800] global_step=800, grad_norm=5.251134395599365, loss=4.930952548980713
I0129 07:55:11.412449 140005288683264 logging_writer.py:48] [900] global_step=900, grad_norm=7.067142486572266, loss=4.7756500244140625
I0129 07:55:45.117654 140004624946944 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.9867405891418457, loss=4.641944408416748
I0129 07:56:18.869996 140005288683264 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.3310396671295166, loss=4.60203218460083
I0129 07:56:52.569751 140004624946944 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.784541368484497, loss=4.497668266296387
I0129 07:57:26.388061 140005288683264 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.4910621643066406, loss=4.267276763916016
I0129 07:58:00.105042 140004624946944 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.886552333831787, loss=4.146941184997559
I0129 07:58:33.826782 140005288683264 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.456490993499756, loss=4.071310043334961
I0129 07:58:38.358335 140169137129280 spec.py:321] Evaluating on the training split.
I0129 07:58:44.837260 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 07:58:53.468490 140169137129280 spec.py:349] Evaluating on the test split.
I0129 07:58:56.127941 140169137129280 submission_runner.py:408] Time since start: 579.18s, 	Step: 1515, 	{'train/accuracy': 0.19355866312980652, 'train/loss': 4.136880397796631, 'validation/accuracy': 0.18143999576568604, 'validation/loss': 4.266019344329834, 'validation/num_examples': 50000, 'test/accuracy': 0.13379999995231628, 'test/loss': 4.854593276977539, 'test/num_examples': 10000, 'score': 543.3378028869629, 'total_duration': 579.1754071712494, 'accumulated_submission_time': 543.3378028869629, 'accumulated_eval_time': 35.76667332649231, 'accumulated_logging_time': 0.02084040641784668}
I0129 07:58:56.148203 140005305468672 logging_writer.py:48] [1515] accumulated_eval_time=35.766673, accumulated_logging_time=0.020840, accumulated_submission_time=543.337803, global_step=1515, preemption_count=0, score=543.337803, test/accuracy=0.133800, test/loss=4.854593, test/num_examples=10000, total_duration=579.175407, train/accuracy=0.193559, train/loss=4.136880, validation/accuracy=0.181440, validation/loss=4.266019, validation/num_examples=50000
I0129 07:59:25.098227 140005313861376 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.022937059402466, loss=4.00020694732666
I0129 07:59:58.785969 140005305468672 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.936525583267212, loss=3.8594541549682617
I0129 08:00:32.474384 140005313861376 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.299774646759033, loss=3.725142002105713
I0129 08:01:06.184992 140005305468672 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.9644289016723633, loss=3.7295937538146973
I0129 08:01:39.854479 140005313861376 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.4946858882904053, loss=3.6215384006500244
I0129 08:02:13.566527 140005305468672 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.5081024169921875, loss=3.545865297317505
I0129 08:02:47.249647 140005313861376 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.4457285404205322, loss=3.4643962383270264
I0129 08:03:20.973872 140005305468672 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.9292927980422974, loss=3.5072219371795654
I0129 08:03:54.769842 140005313861376 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.2380188703536987, loss=3.2667489051818848
I0129 08:04:28.425698 140005305468672 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.0495877265930176, loss=3.3173437118530273
I0129 08:05:02.099212 140005313861376 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.4405841827392578, loss=3.2978897094726562
I0129 08:05:35.791690 140005305468672 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0199247598648071, loss=3.256570339202881
I0129 08:06:09.476715 140005313861376 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2615165710449219, loss=3.226943254470825
I0129 08:06:43.165744 140005305468672 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.278647541999817, loss=3.2183918952941895
I0129 08:07:16.820673 140005313861376 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.332593321800232, loss=3.000706195831299
I0129 08:07:26.403586 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:07:32.809539 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:07:41.817703 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:07:44.466779 140169137129280 submission_runner.py:408] Time since start: 1107.51s, 	Step: 3030, 	{'train/accuracy': 0.3678053319454193, 'train/loss': 2.9227986335754395, 'validation/accuracy': 0.34421998262405396, 'validation/loss': 3.063002109527588, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.755113124847412, 'test/num_examples': 10000, 'score': 1053.532371044159, 'total_duration': 1107.5142476558685, 'accumulated_submission_time': 1053.532371044159, 'accumulated_eval_time': 53.829830169677734, 'accumulated_logging_time': 0.05278611183166504}
I0129 08:07:44.486172 140004616554240 logging_writer.py:48] [3030] accumulated_eval_time=53.829830, accumulated_logging_time=0.052786, accumulated_submission_time=1053.532371, global_step=3030, preemption_count=0, score=1053.532371, test/accuracy=0.260600, test/loss=3.755113, test/num_examples=10000, total_duration=1107.514248, train/accuracy=0.367805, train/loss=2.922799, validation/accuracy=0.344220, validation/loss=3.063002, validation/num_examples=50000
I0129 08:08:08.307937 140004624946944 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.1955153942108154, loss=3.1590890884399414
I0129 08:08:41.881185 140004616554240 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0741244554519653, loss=3.0293993949890137
I0129 08:09:15.562936 140004624946944 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.1120003461837769, loss=3.0249502658843994
I0129 08:09:49.163629 140004616554240 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.262344479560852, loss=3.0797665119171143
I0129 08:10:22.941318 140004624946944 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2239339351654053, loss=2.940744638442993
I0129 08:10:56.585961 140004616554240 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.039517879486084, loss=2.9740052223205566
I0129 08:11:30.256640 140004624946944 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.011734962463379, loss=2.91970157623291
I0129 08:12:03.931693 140004616554240 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.869372546672821, loss=2.840312957763672
I0129 08:12:37.587027 140004624946944 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.6027438640594482, loss=2.877777576446533
I0129 08:13:11.227823 140004616554240 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8607460260391235, loss=2.848989486694336
I0129 08:13:44.849724 140004624946944 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9684569239616394, loss=2.9492111206054688
I0129 08:14:18.491881 140004616554240 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8857932090759277, loss=2.839797019958496
I0129 08:14:52.163728 140004624946944 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.1907745599746704, loss=2.878087282180786
I0129 08:15:25.795377 140004616554240 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.154491662979126, loss=2.8070263862609863
I0129 08:15:59.426534 140004624946944 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.1185855865478516, loss=2.773141860961914
I0129 08:16:14.795179 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:16:21.155037 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:16:30.066338 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:16:32.683583 140169137129280 submission_runner.py:408] Time since start: 1635.73s, 	Step: 4547, 	{'train/accuracy': 0.409877210855484, 'train/loss': 2.6684727668762207, 'validation/accuracy': 0.38561999797821045, 'validation/loss': 2.8410749435424805, 'validation/num_examples': 50000, 'test/accuracy': 0.28300002217292786, 'test/loss': 3.582597494125366, 'test/num_examples': 10000, 'score': 1563.7822008132935, 'total_duration': 1635.7310452461243, 'accumulated_submission_time': 1563.7822008132935, 'accumulated_eval_time': 71.71818590164185, 'accumulated_logging_time': 0.08233404159545898}
I0129 08:16:32.702239 140004608161536 logging_writer.py:48] [4547] accumulated_eval_time=71.718186, accumulated_logging_time=0.082334, accumulated_submission_time=1563.782201, global_step=4547, preemption_count=0, score=1563.782201, test/accuracy=0.283000, test/loss=3.582597, test/num_examples=10000, total_duration=1635.731045, train/accuracy=0.409877, train/loss=2.668473, validation/accuracy=0.385620, validation/loss=2.841075, validation/num_examples=50000
I0129 08:16:50.822001 140005305468672 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8466758131980896, loss=2.677762985229492
I0129 08:17:24.393465 140004608161536 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9145205616950989, loss=2.7493555545806885
I0129 08:17:58.021658 140005305468672 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.093626856803894, loss=2.7047433853149414
I0129 08:18:31.683494 140004608161536 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7782008647918701, loss=2.7208335399627686
I0129 08:19:05.325290 140005305468672 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9872543811798096, loss=2.6415915489196777
I0129 08:19:38.964165 140004608161536 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7615673542022705, loss=2.6717381477355957
I0129 08:20:12.644857 140005305468672 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8703762888908386, loss=2.6688177585601807
I0129 08:20:46.274912 140004608161536 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8850041031837463, loss=2.7540204524993896
I0129 08:21:19.904955 140005305468672 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.007379174232483, loss=2.6130738258361816
I0129 08:21:53.560279 140004608161536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9192385077476501, loss=2.6016321182250977
I0129 08:22:27.236039 140005305468672 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8302752375602722, loss=2.7230064868927
I0129 08:23:00.979420 140004608161536 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8174405097961426, loss=2.5562989711761475
I0129 08:23:34.641720 140005305468672 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.9018757343292236, loss=2.5921945571899414
I0129 08:24:08.263489 140004608161536 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.0968888998031616, loss=2.602710008621216
I0129 08:24:41.905751 140005305468672 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9170406460762024, loss=2.5620501041412354
I0129 08:25:02.896485 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:25:09.716482 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:25:18.255852 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:25:20.967061 140169137129280 submission_runner.py:408] Time since start: 2164.01s, 	Step: 6064, 	{'train/accuracy': 0.3488520383834839, 'train/loss': 3.0972437858581543, 'validation/accuracy': 0.3138200044631958, 'validation/loss': 3.3472726345062256, 'validation/num_examples': 50000, 'test/accuracy': 0.22520001232624054, 'test/loss': 4.131947994232178, 'test/num_examples': 10000, 'score': 2073.916249513626, 'total_duration': 2164.014529466629, 'accumulated_submission_time': 2073.916249513626, 'accumulated_eval_time': 89.78872632980347, 'accumulated_logging_time': 0.11142849922180176}
I0129 08:25:20.990204 140004608161536 logging_writer.py:48] [6064] accumulated_eval_time=89.788726, accumulated_logging_time=0.111428, accumulated_submission_time=2073.916250, global_step=6064, preemption_count=0, score=2073.916250, test/accuracy=0.225200, test/loss=4.131948, test/num_examples=10000, total_duration=2164.014529, train/accuracy=0.348852, train/loss=3.097244, validation/accuracy=0.313820, validation/loss=3.347273, validation/num_examples=50000
I0129 08:25:33.449303 140005297075968 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.990972638130188, loss=2.550076484680176
I0129 08:26:07.048053 140004608161536 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8984816670417786, loss=2.5509214401245117
I0129 08:26:40.718164 140005297075968 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9199511408805847, loss=2.70290207862854
I0129 08:27:14.357032 140004608161536 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9750556945800781, loss=2.668781280517578
I0129 08:27:48.000214 140005297075968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8653616905212402, loss=2.6542446613311768
I0129 08:28:21.677486 140004608161536 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.2892969846725464, loss=2.5477213859558105
I0129 08:28:55.302963 140005297075968 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8539758324623108, loss=2.603492498397827
I0129 08:29:28.982292 140004608161536 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8492144346237183, loss=2.554044246673584
I0129 08:30:02.596391 140005297075968 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.0441005229949951, loss=2.5467898845672607
I0129 08:30:36.256753 140004608161536 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.084385633468628, loss=2.5871291160583496
I0129 08:31:09.874850 140005297075968 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9519340991973877, loss=2.657959461212158
I0129 08:31:43.506912 140004608161536 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.0796114206314087, loss=2.6783738136291504
I0129 08:32:17.130685 140005297075968 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9044521450996399, loss=2.493497371673584
I0129 08:32:50.769658 140004608161536 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0662235021591187, loss=2.6407721042633057
I0129 08:33:24.419551 140005297075968 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9204555153846741, loss=2.5999269485473633
I0129 08:33:51.126779 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:33:57.574755 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:34:06.463194 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:34:09.124101 140169137129280 submission_runner.py:408] Time since start: 2692.17s, 	Step: 7581, 	{'train/accuracy': 0.29169324040412903, 'train/loss': 3.627654552459717, 'validation/accuracy': 0.27017998695373535, 'validation/loss': 3.8371315002441406, 'validation/num_examples': 50000, 'test/accuracy': 0.20400001108646393, 'test/loss': 4.534960746765137, 'test/num_examples': 10000, 'score': 2583.992983341217, 'total_duration': 2692.171564102173, 'accumulated_submission_time': 2583.992983341217, 'accumulated_eval_time': 107.7860050201416, 'accumulated_logging_time': 0.14529061317443848}
I0129 08:34:09.142790 140005322254080 logging_writer.py:48] [7581] accumulated_eval_time=107.786005, accumulated_logging_time=0.145291, accumulated_submission_time=2583.992983, global_step=7581, preemption_count=0, score=2583.992983, test/accuracy=0.204000, test/loss=4.534961, test/num_examples=10000, total_duration=2692.171564, train/accuracy=0.291693, train/loss=3.627655, validation/accuracy=0.270180, validation/loss=3.837132, validation/num_examples=50000
I0129 08:34:15.853763 140005330646784 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9240143895149231, loss=2.5167012214660645
I0129 08:34:49.417515 140005322254080 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0792022943496704, loss=2.4800994396209717
I0129 08:35:23.052711 140005330646784 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0351455211639404, loss=2.4826412200927734
I0129 08:35:56.735887 140005322254080 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0205649137496948, loss=2.574239730834961
I0129 08:36:30.357629 140005330646784 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9645947217941284, loss=2.4255712032318115
I0129 08:37:03.984059 140005322254080 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9998008608818054, loss=2.4956040382385254
I0129 08:37:37.594769 140005330646784 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.9857035279273987, loss=2.6179862022399902
I0129 08:38:11.214262 140005322254080 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8582724332809448, loss=2.4591965675354004
I0129 08:38:44.837310 140005330646784 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.9582238793373108, loss=2.5014116764068604
I0129 08:39:18.470151 140005322254080 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0021100044250488, loss=2.4128775596618652
I0129 08:39:52.077971 140005330646784 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9306887984275818, loss=2.4481639862060547
I0129 08:40:25.688050 140005322254080 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0589284896850586, loss=2.4502296447753906
I0129 08:40:59.300386 140005330646784 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.003817081451416, loss=2.531017303466797
I0129 08:41:32.886647 140005322254080 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8912920355796814, loss=2.5433764457702637
I0129 08:42:06.585852 140005330646784 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0177574157714844, loss=2.6182875633239746
I0129 08:42:39.305302 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:42:45.705183 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:42:54.685596 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:42:57.505694 140169137129280 submission_runner.py:408] Time since start: 3220.55s, 	Step: 9099, 	{'train/accuracy': 0.0675223171710968, 'train/loss': 6.560290813446045, 'validation/accuracy': 0.060920000076293945, 'validation/loss': 6.683966159820557, 'validation/num_examples': 50000, 'test/accuracy': 0.047300003468990326, 'test/loss': 7.12650203704834, 'test/num_examples': 10000, 'score': 3094.0958971977234, 'total_duration': 3220.553163051605, 'accumulated_submission_time': 3094.0958971977234, 'accumulated_eval_time': 125.98636031150818, 'accumulated_logging_time': 0.17412614822387695}
I0129 08:42:57.524826 140004616554240 logging_writer.py:48] [9099] accumulated_eval_time=125.986360, accumulated_logging_time=0.174126, accumulated_submission_time=3094.095897, global_step=9099, preemption_count=0, score=3094.095897, test/accuracy=0.047300, test/loss=7.126502, test/num_examples=10000, total_duration=3220.553163, train/accuracy=0.067522, train/loss=6.560291, validation/accuracy=0.060920, validation/loss=6.683966, validation/num_examples=50000
I0129 08:42:58.225302 140004624946944 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9788123965263367, loss=2.370246171951294
I0129 08:43:31.748030 140004616554240 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.0035299062728882, loss=2.423063039779663
I0129 08:44:05.285636 140004624946944 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.0218054056167603, loss=2.5087616443634033
I0129 08:44:38.893374 140004616554240 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8988092541694641, loss=2.414271116256714
I0129 08:45:12.534186 140004624946944 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.0353796482086182, loss=2.3546857833862305
I0129 08:45:46.141997 140004616554240 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8959862589836121, loss=2.5448102951049805
I0129 08:46:19.757207 140004624946944 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0023733377456665, loss=2.543987274169922
I0129 08:46:53.329577 140004616554240 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9171605706214905, loss=2.3853282928466797
I0129 08:47:26.919908 140004624946944 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9806048274040222, loss=2.467494010925293
I0129 08:48:00.521641 140004616554240 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.0175625085830688, loss=2.4730799198150635
I0129 08:48:34.217730 140004624946944 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.0363152027130127, loss=2.4494411945343018
I0129 08:49:07.758350 140004616554240 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0432147979736328, loss=2.536832809448242
I0129 08:49:41.354644 140004624946944 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.02553129196167, loss=2.4675168991088867
I0129 08:50:14.973490 140004616554240 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9066513776779175, loss=2.496128559112549
I0129 08:50:48.593849 140004624946944 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0664347410202026, loss=2.3855535984039307
I0129 08:51:22.216048 140004616554240 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9946156144142151, loss=2.31742000579834
I0129 08:51:27.743202 140169137129280 spec.py:321] Evaluating on the training split.
I0129 08:51:34.160108 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 08:51:43.087741 140169137129280 spec.py:349] Evaluating on the test split.
I0129 08:51:45.748135 140169137129280 submission_runner.py:408] Time since start: 3748.80s, 	Step: 10618, 	{'train/accuracy': 0.36471620202064514, 'train/loss': 3.049172878265381, 'validation/accuracy': 0.34261998534202576, 'validation/loss': 3.222270965576172, 'validation/num_examples': 50000, 'test/accuracy': 0.25460001826286316, 'test/loss': 3.9828693866729736, 'test/num_examples': 10000, 'score': 3604.2537105083466, 'total_duration': 3748.795606613159, 'accumulated_submission_time': 3604.2537105083466, 'accumulated_eval_time': 143.99125719070435, 'accumulated_logging_time': 0.2043311595916748}
I0129 08:51:45.769855 140005313861376 logging_writer.py:48] [10618] accumulated_eval_time=143.991257, accumulated_logging_time=0.204331, accumulated_submission_time=3604.253711, global_step=10618, preemption_count=0, score=3604.253711, test/accuracy=0.254600, test/loss=3.982869, test/num_examples=10000, total_duration=3748.795607, train/accuracy=0.364716, train/loss=3.049173, validation/accuracy=0.342620, validation/loss=3.222271, validation/num_examples=50000
I0129 08:52:13.593748 140005322254080 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9898415803909302, loss=2.434457778930664
I0129 08:52:47.135813 140005313861376 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.9979726076126099, loss=2.490751266479492
I0129 08:53:20.661462 140005322254080 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9657495021820068, loss=2.421332836151123
I0129 08:53:54.200838 140005313861376 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.9667620658874512, loss=2.396303176879883
I0129 08:54:27.782376 140005322254080 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9983502626419067, loss=2.4295084476470947
I0129 08:55:01.429438 140005313861376 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0862163305282593, loss=2.300814151763916
I0129 08:55:34.996976 140005322254080 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.003143310546875, loss=2.428118944168091
I0129 08:56:08.608787 140005313861376 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.04290771484375, loss=2.554455280303955
I0129 08:56:42.198527 140005322254080 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.8226736187934875, loss=2.4063615798950195
I0129 08:57:15.746901 140005313861376 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9884269833564758, loss=2.6187286376953125
I0129 08:57:49.361674 140005322254080 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0395187139511108, loss=2.487823963165283
I0129 08:58:22.940162 140005313861376 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.3039129972457886, loss=2.413114547729492
I0129 08:58:56.558329 140005322254080 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0461701154708862, loss=2.4630343914031982
I0129 08:59:30.173179 140005313861376 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.2473969459533691, loss=2.3489127159118652
I0129 09:00:03.779122 140005322254080 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1095893383026123, loss=2.305757522583008
I0129 09:00:16.011351 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:00:22.381305 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:00:31.293238 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:00:34.055572 140169137129280 submission_runner.py:408] Time since start: 4277.10s, 	Step: 12138, 	{'train/accuracy': 0.26538583636283875, 'train/loss': 3.7979533672332764, 'validation/accuracy': 0.24609999358654022, 'validation/loss': 3.9674124717712402, 'validation/num_examples': 50000, 'test/accuracy': 0.18620000779628754, 'test/loss': 4.577045440673828, 'test/num_examples': 10000, 'score': 4114.4356808662415, 'total_duration': 4277.1030423641205, 'accumulated_submission_time': 4114.4356808662415, 'accumulated_eval_time': 162.03544116020203, 'accumulated_logging_time': 0.23630690574645996}
I0129 09:00:34.078189 140004624946944 logging_writer.py:48] [12138] accumulated_eval_time=162.035441, accumulated_logging_time=0.236307, accumulated_submission_time=4114.435681, global_step=12138, preemption_count=0, score=4114.435681, test/accuracy=0.186200, test/loss=4.577045, test/num_examples=10000, total_duration=4277.103042, train/accuracy=0.265386, train/loss=3.797953, validation/accuracy=0.246100, validation/loss=3.967412, validation/num_examples=50000
I0129 09:00:55.183153 140005288683264 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.0468977689743042, loss=2.2898988723754883
I0129 09:01:28.799129 140004624946944 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.9283813238143921, loss=2.279521942138672
I0129 09:02:02.365706 140005288683264 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1670591831207275, loss=2.4539668560028076
I0129 09:02:35.945066 140004624946944 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.079188346862793, loss=2.4349474906921387
I0129 09:03:09.468050 140005288683264 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9272255897521973, loss=2.4044189453125
I0129 09:03:42.991561 140004624946944 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.9619364142417908, loss=2.3169922828674316
I0129 09:04:16.542870 140005288683264 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9255492687225342, loss=2.3754630088806152
I0129 09:04:50.094242 140004624946944 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9894490838050842, loss=2.4157819747924805
I0129 09:05:23.610408 140005288683264 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.15047025680542, loss=2.541534423828125
I0129 09:05:57.130938 140004624946944 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.893595278263092, loss=2.444044589996338
I0129 09:06:30.718962 140005288683264 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1767542362213135, loss=2.332811117172241
I0129 09:07:04.319268 140004624946944 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.1555076837539673, loss=2.3788974285125732
I0129 09:07:37.937116 140005288683264 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0581012964248657, loss=2.506338357925415
I0129 09:08:11.566625 140004624946944 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0058327913284302, loss=2.289956569671631
I0129 09:08:45.163066 140005288683264 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.133776068687439, loss=2.3563685417175293
I0129 09:09:04.116845 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:09:11.185440 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:09:19.750689 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:09:22.465802 140169137129280 submission_runner.py:408] Time since start: 4805.51s, 	Step: 13658, 	{'train/accuracy': 0.14144212007522583, 'train/loss': 5.066132068634033, 'validation/accuracy': 0.1360199898481369, 'validation/loss': 5.142930030822754, 'validation/num_examples': 50000, 'test/accuracy': 0.0982000082731247, 'test/loss': 5.658753395080566, 'test/num_examples': 10000, 'score': 4624.410368680954, 'total_duration': 4805.513270378113, 'accumulated_submission_time': 4624.410368680954, 'accumulated_eval_time': 180.38438057899475, 'accumulated_logging_time': 0.27373385429382324}
I0129 09:09:22.489192 140005330646784 logging_writer.py:48] [13658] accumulated_eval_time=180.384381, accumulated_logging_time=0.273734, accumulated_submission_time=4624.410369, global_step=13658, preemption_count=0, score=4624.410369, test/accuracy=0.098200, test/loss=5.658753, test/num_examples=10000, total_duration=4805.513270, train/accuracy=0.141442, train/loss=5.066132, validation/accuracy=0.136020, validation/loss=5.142930, validation/num_examples=50000
I0129 09:09:36.906861 140005817149184 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9029854536056519, loss=2.393137216567993
I0129 09:10:10.374734 140005330646784 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.992911159992218, loss=2.293992519378662
I0129 09:10:43.923171 140005817149184 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.060904622077942, loss=2.4299066066741943
I0129 09:11:17.542640 140005330646784 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0374940633773804, loss=2.478468418121338
I0129 09:11:51.162549 140005817149184 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0865827798843384, loss=2.5594253540039062
I0129 09:12:24.763977 140005330646784 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.1077790260314941, loss=2.5929930210113525
I0129 09:12:58.366779 140005817149184 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0796184539794922, loss=2.439176559448242
I0129 09:13:31.966417 140005330646784 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0256162881851196, loss=2.266693115234375
I0129 09:14:05.512828 140005817149184 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.009236454963684, loss=2.305130958557129
I0129 09:14:39.158534 140005330646784 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.1551260948181152, loss=2.4057154655456543
I0129 09:15:12.738830 140005817149184 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0422899723052979, loss=2.5717594623565674
I0129 09:15:46.345180 140005330646784 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0033037662506104, loss=2.275878429412842
I0129 09:16:19.899640 140005817149184 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9458771347999573, loss=2.3434665203094482
I0129 09:16:53.411779 140005330646784 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9928218722343445, loss=2.3551275730133057
I0129 09:17:26.943286 140005817149184 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9934148788452148, loss=2.525909185409546
I0129 09:17:52.612267 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:17:59.011043 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:18:07.885965 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:18:10.478862 140169137129280 submission_runner.py:408] Time since start: 5333.53s, 	Step: 15178, 	{'train/accuracy': 0.29649633169174194, 'train/loss': 3.4820408821105957, 'validation/accuracy': 0.26763999462127686, 'validation/loss': 3.7009527683258057, 'validation/num_examples': 50000, 'test/accuracy': 0.20770001411437988, 'test/loss': 4.33635950088501, 'test/num_examples': 10000, 'score': 5134.47070813179, 'total_duration': 5333.5263023376465, 'accumulated_submission_time': 5134.47070813179, 'accumulated_eval_time': 198.250910282135, 'accumulated_logging_time': 0.31057238578796387}
I0129 09:18:10.502707 140005288683264 logging_writer.py:48] [15178] accumulated_eval_time=198.250910, accumulated_logging_time=0.310572, accumulated_submission_time=5134.470708, global_step=15178, preemption_count=0, score=5134.470708, test/accuracy=0.207700, test/loss=4.336360, test/num_examples=10000, total_duration=5333.526302, train/accuracy=0.296496, train/loss=3.482041, validation/accuracy=0.267640, validation/loss=3.700953, validation/num_examples=50000
I0129 09:18:18.201409 140005297075968 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9151434898376465, loss=2.36739182472229
I0129 09:18:51.691109 140005288683264 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9435786008834839, loss=2.355079412460327
I0129 09:19:25.202881 140005297075968 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.0909335613250732, loss=2.314530611038208
I0129 09:19:58.786674 140005288683264 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.1181068420410156, loss=2.4003400802612305
I0129 09:20:32.327328 140005297075968 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.046839714050293, loss=2.465602397918701
I0129 09:21:05.974182 140005288683264 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0450278520584106, loss=2.3267173767089844
I0129 09:21:39.521785 140005297075968 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0581581592559814, loss=2.375967025756836
I0129 09:22:13.015615 140005288683264 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0237239599227905, loss=2.406883478164673
I0129 09:22:46.576611 140005297075968 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.026821255683899, loss=2.335330009460449
I0129 09:23:20.195442 140005288683264 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0437036752700806, loss=2.3657073974609375
I0129 09:23:53.759693 140005297075968 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9775851368904114, loss=2.4749937057495117
I0129 09:24:27.235579 140005288683264 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9636857509613037, loss=2.3644652366638184
I0129 09:25:00.775041 140005297075968 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.05950129032135, loss=2.391317129135132
I0129 09:25:34.378117 140005288683264 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0039573907852173, loss=2.351858615875244
I0129 09:26:07.943769 140005297075968 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1195749044418335, loss=2.2492549419403076
I0129 09:26:40.582890 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:26:46.952501 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:26:55.819832 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:26:58.527013 140169137129280 submission_runner.py:408] Time since start: 5861.57s, 	Step: 16699, 	{'train/accuracy': 0.2660634517669678, 'train/loss': 3.838587760925293, 'validation/accuracy': 0.2495799958705902, 'validation/loss': 3.9884297847747803, 'validation/num_examples': 50000, 'test/accuracy': 0.17820000648498535, 'test/loss': 4.823949813842773, 'test/num_examples': 10000, 'score': 5644.488646507263, 'total_duration': 5861.574481487274, 'accumulated_submission_time': 5644.488646507263, 'accumulated_eval_time': 216.19500756263733, 'accumulated_logging_time': 0.34738874435424805}
I0129 09:26:58.548717 140005305468672 logging_writer.py:48] [16699] accumulated_eval_time=216.195008, accumulated_logging_time=0.347389, accumulated_submission_time=5644.488647, global_step=16699, preemption_count=0, score=5644.488647, test/accuracy=0.178200, test/loss=4.823950, test/num_examples=10000, total_duration=5861.574481, train/accuracy=0.266063, train/loss=3.838588, validation/accuracy=0.249580, validation/loss=3.988430, validation/num_examples=50000
I0129 09:26:59.247491 140005313861376 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9950107336044312, loss=2.3210201263427734
I0129 09:27:33.060181 140005305468672 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.0217928886413574, loss=2.4231014251708984
I0129 09:28:06.550985 140005313861376 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.1386339664459229, loss=2.289728879928589
I0129 09:28:40.067832 140005305468672 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.072831630706787, loss=2.461111545562744
I0129 09:29:13.612137 140005313861376 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.0210694074630737, loss=2.367827892303467
I0129 09:29:47.110758 140005305468672 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0926761627197266, loss=2.4504027366638184
I0129 09:30:20.629396 140005313861376 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0364234447479248, loss=2.3963959217071533
I0129 09:30:54.130511 140005305468672 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.1890034675598145, loss=2.3960061073303223
I0129 09:31:27.696021 140005313861376 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.1225336790084839, loss=2.372711658477783
I0129 09:32:01.278404 140005305468672 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0295110940933228, loss=2.426316976547241
I0129 09:32:34.874564 140005313861376 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.079933762550354, loss=2.435023784637451
I0129 09:33:08.370676 140005305468672 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.174013376235962, loss=2.4480903148651123
I0129 09:33:42.029820 140005313861376 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0510417222976685, loss=2.3511950969696045
I0129 09:34:15.534229 140005305468672 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0882048606872559, loss=2.363110065460205
I0129 09:34:49.040390 140005313861376 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0503184795379639, loss=2.311427116394043
I0129 09:35:22.537145 140005305468672 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.035146951675415, loss=2.4585373401641846
I0129 09:35:28.713254 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:35:35.065109 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:35:43.836602 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:35:46.513365 140169137129280 submission_runner.py:408] Time since start: 6389.56s, 	Step: 18220, 	{'train/accuracy': 0.18536750972270966, 'train/loss': 4.6938252449035645, 'validation/accuracy': 0.17095999419689178, 'validation/loss': 4.821983337402344, 'validation/num_examples': 50000, 'test/accuracy': 0.12280000746250153, 'test/loss': 5.50702428817749, 'test/num_examples': 10000, 'score': 6154.5932059288025, 'total_duration': 6389.560833454132, 'accumulated_submission_time': 6154.5932059288025, 'accumulated_eval_time': 233.99508047103882, 'accumulated_logging_time': 0.37943339347839355}
I0129 09:35:46.534411 140005288683264 logging_writer.py:48] [18220] accumulated_eval_time=233.995080, accumulated_logging_time=0.379433, accumulated_submission_time=6154.593206, global_step=18220, preemption_count=0, score=6154.593206, test/accuracy=0.122800, test/loss=5.507024, test/num_examples=10000, total_duration=6389.560833, train/accuracy=0.185368, train/loss=4.693825, validation/accuracy=0.170960, validation/loss=4.821983, validation/num_examples=50000
I0129 09:36:13.630330 140005297075968 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.030437707901001, loss=2.277688503265381
I0129 09:36:47.080435 140005288683264 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.009714126586914, loss=2.22159481048584
I0129 09:37:20.573508 140005297075968 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.159821629524231, loss=2.350250720977783
I0129 09:37:54.092686 140005288683264 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0370404720306396, loss=2.3916149139404297
I0129 09:38:27.632471 140005297075968 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9388799667358398, loss=2.290046453475952
I0129 09:39:01.143106 140005288683264 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0727622509002686, loss=2.4502174854278564
I0129 09:39:34.656663 140005297075968 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.990582287311554, loss=2.2942054271698
I0129 09:40:08.249528 140005288683264 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9976332783699036, loss=2.2533037662506104
I0129 09:40:41.747287 140005297075968 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9739562273025513, loss=2.1980578899383545
I0129 09:41:15.252440 140005288683264 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0411076545715332, loss=2.409487724304199
I0129 09:41:48.767122 140005297075968 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.009050965309143, loss=2.429605484008789
I0129 09:42:22.290766 140005288683264 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.1306283473968506, loss=2.3091657161712646
I0129 09:42:55.776453 140005297075968 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.023357629776001, loss=2.351163387298584
I0129 09:43:29.344877 140005288683264 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9825147986412048, loss=2.286850690841675
I0129 09:44:02.891340 140005297075968 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0833041667938232, loss=2.430819034576416
I0129 09:44:16.776751 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:44:23.205149 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:44:32.168795 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:44:34.825082 140169137129280 submission_runner.py:408] Time since start: 6917.87s, 	Step: 19743, 	{'train/accuracy': 0.3005022406578064, 'train/loss': 3.5624327659606934, 'validation/accuracy': 0.2856999933719635, 'validation/loss': 3.7141499519348145, 'validation/num_examples': 50000, 'test/accuracy': 0.20440000295639038, 'test/loss': 4.545632839202881, 'test/num_examples': 10000, 'score': 6664.773620843887, 'total_duration': 6917.872552871704, 'accumulated_submission_time': 6664.773620843887, 'accumulated_eval_time': 252.04337310791016, 'accumulated_logging_time': 0.4124011993408203}
I0129 09:44:34.846813 140004616554240 logging_writer.py:48] [19743] accumulated_eval_time=252.043373, accumulated_logging_time=0.412401, accumulated_submission_time=6664.773621, global_step=19743, preemption_count=0, score=6664.773621, test/accuracy=0.204400, test/loss=4.545633, test/num_examples=10000, total_duration=6917.872553, train/accuracy=0.300502, train/loss=3.562433, validation/accuracy=0.285700, validation/loss=3.714150, validation/num_examples=50000
I0129 09:44:54.262165 140004624946944 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.1079909801483154, loss=2.401494264602661
I0129 09:45:27.709019 140004616554240 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.1659046411514282, loss=2.3234658241271973
I0129 09:46:01.243204 140004624946944 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1518535614013672, loss=2.455475330352783
I0129 09:46:34.844821 140004616554240 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9609180092811584, loss=2.3627071380615234
I0129 09:47:08.435277 140004624946944 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1623003482818604, loss=2.5504980087280273
I0129 09:47:41.917478 140004616554240 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0406559705734253, loss=2.2613677978515625
I0129 09:48:15.450395 140004624946944 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.2262015342712402, loss=2.3041281700134277
I0129 09:48:48.956585 140004616554240 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.1196746826171875, loss=2.3678884506225586
I0129 09:49:22.461598 140004624946944 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.259010672569275, loss=2.289140224456787
I0129 09:49:55.965986 140004616554240 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.061215877532959, loss=2.3343582153320312
I0129 09:50:29.464525 140004624946944 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.024689793586731, loss=2.4588518142700195
I0129 09:51:03.033991 140004616554240 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1662750244140625, loss=2.309964179992676
I0129 09:51:36.606184 140004624946944 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0206137895584106, loss=2.466165542602539
I0129 09:52:10.097017 140004616554240 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.129134178161621, loss=2.395007848739624
I0129 09:52:43.677502 140004624946944 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.0296601057052612, loss=2.3090403079986572
I0129 09:53:04.937332 140169137129280 spec.py:321] Evaluating on the training split.
I0129 09:53:11.392177 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 09:53:20.028137 140169137129280 spec.py:349] Evaluating on the test split.
I0129 09:53:22.718172 140169137129280 submission_runner.py:408] Time since start: 7445.77s, 	Step: 21265, 	{'train/accuracy': 0.24615353345870972, 'train/loss': 4.195694923400879, 'validation/accuracy': 0.2303600013256073, 'validation/loss': 4.367095470428467, 'validation/num_examples': 50000, 'test/accuracy': 0.16830000281333923, 'test/loss': 5.043724060058594, 'test/num_examples': 10000, 'score': 7174.803897380829, 'total_duration': 7445.765621185303, 'accumulated_submission_time': 7174.803897380829, 'accumulated_eval_time': 269.8241550922394, 'accumulated_logging_time': 0.4445338249206543}
I0129 09:53:22.742245 140004616554240 logging_writer.py:48] [21265] accumulated_eval_time=269.824155, accumulated_logging_time=0.444534, accumulated_submission_time=7174.803897, global_step=21265, preemption_count=0, score=7174.803897, test/accuracy=0.168300, test/loss=5.043724, test/num_examples=10000, total_duration=7445.765621, train/accuracy=0.246154, train/loss=4.195695, validation/accuracy=0.230360, validation/loss=4.367095, validation/num_examples=50000
I0129 09:53:34.807185 140005313861376 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0643478631973267, loss=2.3543224334716797
I0129 09:54:08.232989 140004616554240 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.0436707735061646, loss=2.302990674972534
I0129 09:54:41.668245 140005313861376 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0130879878997803, loss=2.328730821609497
I0129 09:55:15.179589 140004616554240 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.067521333694458, loss=2.2896976470947266
I0129 09:55:48.685562 140005313861376 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0668340921401978, loss=2.3720545768737793
I0129 09:56:22.187738 140004616554240 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0634219646453857, loss=2.2639594078063965
I0129 09:56:55.688954 140005313861376 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1230838298797607, loss=2.2738606929779053
I0129 09:57:29.167477 140004616554240 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0097293853759766, loss=2.21657657623291
I0129 09:58:02.685485 140005313861376 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.2052544355392456, loss=2.293109178543091
I0129 09:58:36.149260 140004616554240 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9936671257019043, loss=2.348809003829956
I0129 09:59:09.757202 140005313861376 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.2445483207702637, loss=2.3623785972595215
I0129 09:59:43.269767 140004616554240 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3270903825759888, loss=2.3710265159606934
I0129 10:00:16.735379 140005313861376 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.177975058555603, loss=2.336479425430298
I0129 10:00:50.272425 140004616554240 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0446302890777588, loss=2.2157740592956543
I0129 10:01:23.726053 140005313861376 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.300088882446289, loss=2.3448503017425537
I0129 10:01:53.034291 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:01:59.483834 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:02:08.122825 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:02:10.814026 140169137129280 submission_runner.py:408] Time since start: 7973.86s, 	Step: 22789, 	{'train/accuracy': 0.006437340285629034, 'train/loss': 10.417054176330566, 'validation/accuracy': 0.00571999978274107, 'validation/loss': 10.540876388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.00430000014603138, 'test/loss': 10.739754676818848, 'test/num_examples': 10000, 'score': 7685.032256126404, 'total_duration': 7973.8614938259125, 'accumulated_submission_time': 7685.032256126404, 'accumulated_eval_time': 287.60385155677795, 'accumulated_logging_time': 0.48259806632995605}
I0129 10:02:10.835653 140004624946944 logging_writer.py:48] [22789] accumulated_eval_time=287.603852, accumulated_logging_time=0.482598, accumulated_submission_time=7685.032256, global_step=22789, preemption_count=0, score=7685.032256, test/accuracy=0.004300, test/loss=10.739755, test/num_examples=10000, total_duration=7973.861494, train/accuracy=0.006437, train/loss=10.417054, validation/accuracy=0.005720, validation/loss=10.540876, validation/num_examples=50000
I0129 10:02:14.858736 140005288683264 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.015466570854187, loss=2.213329315185547
I0129 10:02:48.266687 140004624946944 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.032256841659546, loss=2.2765140533447266
I0129 10:03:21.706285 140005288683264 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0582783222198486, loss=2.3315298557281494
I0129 10:03:55.185976 140004624946944 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1347464323043823, loss=2.5382511615753174
I0129 10:04:28.665492 140005288683264 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.1215287446975708, loss=2.406315565109253
I0129 10:05:02.118842 140004624946944 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.2671442031860352, loss=2.36747145652771
I0129 10:05:35.706554 140005288683264 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.108713150024414, loss=2.2562003135681152
I0129 10:06:09.205663 140004624946944 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0481113195419312, loss=2.2521514892578125
I0129 10:06:42.691938 140005288683264 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.049220085144043, loss=2.387502431869507
I0129 10:07:16.166548 140004624946944 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.0244181156158447, loss=2.2854204177856445
I0129 10:07:49.661720 140005288683264 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.0921751260757446, loss=2.343804121017456
I0129 10:08:23.136972 140004624946944 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1054775714874268, loss=2.3286421298980713
I0129 10:08:56.629470 140005288683264 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0540897846221924, loss=2.204847812652588
I0129 10:09:30.126545 140004624946944 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0497231483459473, loss=2.421874523162842
I0129 10:10:03.622728 140005288683264 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1178499460220337, loss=2.4326913356781006
I0129 10:10:37.090307 140004624946944 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.1564359664916992, loss=2.4260177612304688
I0129 10:10:40.921216 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:10:47.300259 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:10:56.150302 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:10:58.831979 140169137129280 submission_runner.py:408] Time since start: 8501.88s, 	Step: 24313, 	{'train/accuracy': 0.33952486515045166, 'train/loss': 3.138223648071289, 'validation/accuracy': 0.3097600042819977, 'validation/loss': 3.3382935523986816, 'validation/num_examples': 50000, 'test/accuracy': 0.22700001299381256, 'test/loss': 4.0918073654174805, 'test/num_examples': 10000, 'score': 8195.057115793228, 'total_duration': 8501.879445791245, 'accumulated_submission_time': 8195.057115793228, 'accumulated_eval_time': 305.51457262039185, 'accumulated_logging_time': 0.5152654647827148}
I0129 10:10:58.854312 140004616554240 logging_writer.py:48] [24313] accumulated_eval_time=305.514573, accumulated_logging_time=0.515265, accumulated_submission_time=8195.057116, global_step=24313, preemption_count=0, score=8195.057116, test/accuracy=0.227000, test/loss=4.091807, test/num_examples=10000, total_duration=8501.879446, train/accuracy=0.339525, train/loss=3.138224, validation/accuracy=0.309760, validation/loss=3.338294, validation/num_examples=50000
I0129 10:11:28.304135 140004624946944 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0662614107131958, loss=2.3427793979644775
I0129 10:12:01.866026 140004616554240 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.1370885372161865, loss=2.3978753089904785
I0129 10:12:35.322391 140004624946944 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0936506986618042, loss=2.313605308532715
I0129 10:13:08.790164 140004616554240 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0727412700653076, loss=2.369757652282715
I0129 10:13:42.266953 140004624946944 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.097934365272522, loss=2.267815589904785
I0129 10:14:15.730652 140004616554240 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0652878284454346, loss=2.2896528244018555
I0129 10:14:49.247133 140004624946944 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0059093236923218, loss=2.437188148498535
I0129 10:15:22.720902 140004616554240 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.1137977838516235, loss=2.2395808696746826
I0129 10:15:56.232077 140004624946944 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0333657264709473, loss=2.293755292892456
I0129 10:16:29.710128 140004616554240 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.0950562953948975, loss=2.302069664001465
I0129 10:17:03.226639 140004624946944 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1325078010559082, loss=2.3543949127197266
I0129 10:17:36.698838 140004616554240 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1764434576034546, loss=2.230238199234009
I0129 10:18:10.335725 140004624946944 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0793076753616333, loss=2.320260524749756
I0129 10:18:43.800592 140004616554240 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1287528276443481, loss=2.202822208404541
I0129 10:19:17.262682 140004624946944 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1985690593719482, loss=2.418002128601074
I0129 10:19:29.133248 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:19:35.550079 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:19:44.135546 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:19:46.806668 140169137129280 submission_runner.py:408] Time since start: 9029.85s, 	Step: 25837, 	{'train/accuracy': 0.30123963952064514, 'train/loss': 3.451519012451172, 'validation/accuracy': 0.2791000008583069, 'validation/loss': 3.6457788944244385, 'validation/num_examples': 50000, 'test/accuracy': 0.2086000144481659, 'test/loss': 4.288328170776367, 'test/num_examples': 10000, 'score': 8705.275005102158, 'total_duration': 9029.854134559631, 'accumulated_submission_time': 8705.275005102158, 'accumulated_eval_time': 323.1879549026489, 'accumulated_logging_time': 0.5490307807922363}
I0129 10:19:46.830917 140004624946944 logging_writer.py:48] [25837] accumulated_eval_time=323.187955, accumulated_logging_time=0.549031, accumulated_submission_time=8705.275005, global_step=25837, preemption_count=0, score=8705.275005, test/accuracy=0.208600, test/loss=4.288328, test/num_examples=10000, total_duration=9029.854135, train/accuracy=0.301240, train/loss=3.451519, validation/accuracy=0.279100, validation/loss=3.645779, validation/num_examples=50000
I0129 10:20:08.252581 140005305468672 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1198209524154663, loss=2.381171941757202
I0129 10:20:41.672366 140004624946944 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.110141396522522, loss=2.304090976715088
I0129 10:21:15.123085 140005305468672 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.1708377599716187, loss=2.3895180225372314
I0129 10:21:48.572979 140004624946944 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0113308429718018, loss=2.2932980060577393
I0129 10:22:22.067391 140005305468672 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.202589988708496, loss=2.4054067134857178
I0129 10:22:55.538962 140004624946944 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.057336449623108, loss=2.364298105239868
I0129 10:23:29.013658 140005305468672 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1166634559631348, loss=2.3532094955444336
I0129 10:24:02.495378 140004624946944 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.202517032623291, loss=2.4006028175354004
I0129 10:24:36.053680 140005305468672 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.061383843421936, loss=2.3704426288604736
I0129 10:25:09.528508 140004624946944 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.0458793640136719, loss=2.2691287994384766
I0129 10:25:43.003756 140005305468672 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1986438035964966, loss=2.2871482372283936
I0129 10:26:16.458654 140004624946944 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1234164237976074, loss=2.1620638370513916
I0129 10:26:49.928961 140005305468672 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1215238571166992, loss=2.5386557579040527
I0129 10:27:23.384502 140004624946944 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1076992750167847, loss=2.280428409576416
I0129 10:27:56.866730 140005305468672 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1326375007629395, loss=2.2688655853271484
I0129 10:28:17.099922 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:28:23.484751 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:28:32.363543 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:28:34.974362 140169137129280 submission_runner.py:408] Time since start: 9558.02s, 	Step: 27362, 	{'train/accuracy': 0.07280372828245163, 'train/loss': 8.18244743347168, 'validation/accuracy': 0.06663999706506729, 'validation/loss': 8.396821975708008, 'validation/num_examples': 50000, 'test/accuracy': 0.04800000041723251, 'test/loss': 9.050444602966309, 'test/num_examples': 10000, 'score': 9215.481248617172, 'total_duration': 9558.021826505661, 'accumulated_submission_time': 9215.481248617172, 'accumulated_eval_time': 341.062353849411, 'accumulated_logging_time': 0.5862481594085693}
I0129 10:28:34.997015 140005288683264 logging_writer.py:48] [27362] accumulated_eval_time=341.062354, accumulated_logging_time=0.586248, accumulated_submission_time=9215.481249, global_step=27362, preemption_count=0, score=9215.481249, test/accuracy=0.048000, test/loss=9.050445, test/num_examples=10000, total_duration=9558.021827, train/accuracy=0.072804, train/loss=8.182447, validation/accuracy=0.066640, validation/loss=8.396822, validation/num_examples=50000
I0129 10:28:48.054297 140005297075968 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1821109056472778, loss=2.3750061988830566
I0129 10:29:21.462815 140005288683264 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.108526349067688, loss=2.3483481407165527
I0129 10:29:54.914376 140005297075968 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1646671295166016, loss=2.2982091903686523
I0129 10:30:28.367473 140005288683264 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.1539794206619263, loss=2.342918634414673
I0129 10:31:01.888192 140005297075968 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.2013529539108276, loss=2.3542275428771973
I0129 10:31:35.377883 140005288683264 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.1603037118911743, loss=2.27888560295105
I0129 10:32:08.833146 140005297075968 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1387039422988892, loss=2.274658679962158
I0129 10:32:42.277180 140005288683264 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.0584601163864136, loss=2.5000486373901367
I0129 10:33:15.741492 140005297075968 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0924028158187866, loss=2.275331497192383
I0129 10:33:49.207696 140005288683264 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.2532625198364258, loss=2.2803382873535156
I0129 10:34:22.694464 140005297075968 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.1110773086547852, loss=2.3444900512695312
I0129 10:34:56.146541 140005288683264 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.100104570388794, loss=2.2275404930114746
I0129 10:35:29.625653 140005297075968 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.1814204454421997, loss=2.2461490631103516
I0129 10:36:03.108632 140005288683264 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.2177026271820068, loss=2.337684392929077
I0129 10:36:36.560390 140005297075968 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.086540699005127, loss=2.3722169399261475
I0129 10:37:05.171691 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:37:11.618961 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:37:20.532104 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:37:23.675575 140169137129280 submission_runner.py:408] Time since start: 10086.72s, 	Step: 28887, 	{'train/accuracy': 0.2673788070678711, 'train/loss': 4.047645092010498, 'validation/accuracy': 0.24875999987125397, 'validation/loss': 4.2008957862854, 'validation/num_examples': 50000, 'test/accuracy': 0.17830000817775726, 'test/loss': 5.061367988586426, 'test/num_examples': 10000, 'score': 9725.596413373947, 'total_duration': 10086.723033189774, 'accumulated_submission_time': 9725.596413373947, 'accumulated_eval_time': 359.5661907196045, 'accumulated_logging_time': 0.6190822124481201}
I0129 10:37:23.698949 140004624946944 logging_writer.py:48] [28887] accumulated_eval_time=359.566191, accumulated_logging_time=0.619082, accumulated_submission_time=9725.596413, global_step=28887, preemption_count=0, score=9725.596413, test/accuracy=0.178300, test/loss=5.061368, test/num_examples=10000, total_duration=10086.723033, train/accuracy=0.267379, train/loss=4.047645, validation/accuracy=0.248760, validation/loss=4.200896, validation/num_examples=50000
I0129 10:37:28.408860 140005305468672 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.1567643880844116, loss=2.3788061141967773
I0129 10:38:01.816578 140004624946944 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.00169038772583, loss=2.23431134223938
I0129 10:38:35.277688 140005305468672 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.0311996936798096, loss=2.25835919380188
I0129 10:39:08.737643 140004624946944 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1906269788742065, loss=2.250730514526367
I0129 10:39:42.183424 140005305468672 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.1521588563919067, loss=2.1584548950195312
I0129 10:40:15.643136 140004624946944 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1249585151672363, loss=2.2739522457122803
I0129 10:40:49.149969 140005305468672 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.080335021018982, loss=2.289340019226074
I0129 10:41:22.612717 140004624946944 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2158862352371216, loss=2.2994868755340576
I0129 10:41:56.077974 140005305468672 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.2482388019561768, loss=2.3008477687835693
I0129 10:42:29.545123 140004624946944 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.026477336883545, loss=2.2999935150146484
I0129 10:43:02.988388 140005305468672 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1684361696243286, loss=2.3876147270202637
I0129 10:43:36.428219 140004624946944 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1378287076950073, loss=2.3807952404022217
I0129 10:44:10.014349 140005305468672 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0826213359832764, loss=2.3134078979492188
I0129 10:44:43.479657 140004624946944 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.3081185817718506, loss=2.3155717849731445
I0129 10:45:16.941007 140005305468672 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0851644277572632, loss=2.460369110107422
I0129 10:45:50.402827 140004624946944 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1115800142288208, loss=2.3137593269348145
I0129 10:45:53.898668 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:46:00.326819 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:46:08.839747 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:46:11.564183 140169137129280 submission_runner.py:408] Time since start: 10614.61s, 	Step: 30412, 	{'train/accuracy': 0.17727598547935486, 'train/loss': 5.089120864868164, 'validation/accuracy': 0.16142000257968903, 'validation/loss': 5.261129379272461, 'validation/num_examples': 50000, 'test/accuracy': 0.12190000712871552, 'test/loss': 5.848219871520996, 'test/num_examples': 10000, 'score': 10235.736080169678, 'total_duration': 10614.611628293991, 'accumulated_submission_time': 10235.736080169678, 'accumulated_eval_time': 377.23164319992065, 'accumulated_logging_time': 0.6528291702270508}
I0129 10:46:11.590098 140004616554240 logging_writer.py:48] [30412] accumulated_eval_time=377.231643, accumulated_logging_time=0.652829, accumulated_submission_time=10235.736080, global_step=30412, preemption_count=0, score=10235.736080, test/accuracy=0.121900, test/loss=5.848220, test/num_examples=10000, total_duration=10614.611628, train/accuracy=0.177276, train/loss=5.089121, validation/accuracy=0.161420, validation/loss=5.261129, validation/num_examples=50000
I0129 10:46:41.343895 140005297075968 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.222429633140564, loss=2.3603146076202393
I0129 10:47:14.764860 140004616554240 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0644476413726807, loss=2.2633578777313232
I0129 10:47:48.224408 140005297075968 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2560555934906006, loss=2.380910873413086
I0129 10:48:21.666293 140004616554240 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0788235664367676, loss=2.3213624954223633
I0129 10:48:55.133534 140005297075968 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0863053798675537, loss=2.308194160461426
I0129 10:49:28.627486 140004616554240 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1509016752243042, loss=2.2822468280792236
I0129 10:50:02.089613 140005297075968 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1870801448822021, loss=2.3924739360809326
I0129 10:50:35.660436 140004616554240 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.0865899324417114, loss=2.208491563796997
I0129 10:51:09.124104 140005297075968 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.1203089952468872, loss=2.204256296157837
I0129 10:51:42.611085 140004616554240 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1257826089859009, loss=2.3287863731384277
I0129 10:52:16.073057 140005297075968 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.2454713582992554, loss=2.2424943447113037
I0129 10:52:49.523287 140004616554240 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.396357536315918, loss=2.2339468002319336
I0129 10:53:22.965561 140005297075968 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2175291776657104, loss=2.289865016937256
I0129 10:53:56.433772 140004616554240 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.234253168106079, loss=2.4846670627593994
I0129 10:54:29.900966 140005297075968 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.1331452131271362, loss=2.4198808670043945
I0129 10:54:41.771106 140169137129280 spec.py:321] Evaluating on the training split.
I0129 10:54:48.138379 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 10:54:56.739583 140169137129280 spec.py:349] Evaluating on the test split.
I0129 10:54:59.430854 140169137129280 submission_runner.py:408] Time since start: 11142.48s, 	Step: 31937, 	{'train/accuracy': 0.18742027878761292, 'train/loss': 4.548458099365234, 'validation/accuracy': 0.17667999863624573, 'validation/loss': 4.695439338684082, 'validation/num_examples': 50000, 'test/accuracy': 0.1275000125169754, 'test/loss': 5.495242595672607, 'test/num_examples': 10000, 'score': 10745.85445523262, 'total_duration': 11142.47832274437, 'accumulated_submission_time': 10745.85445523262, 'accumulated_eval_time': 394.89136147499084, 'accumulated_logging_time': 0.6915583610534668}
I0129 10:54:59.459761 140004608161536 logging_writer.py:48] [31937] accumulated_eval_time=394.891361, accumulated_logging_time=0.691558, accumulated_submission_time=10745.854455, global_step=31937, preemption_count=0, score=10745.854455, test/accuracy=0.127500, test/loss=5.495243, test/num_examples=10000, total_duration=11142.478323, train/accuracy=0.187420, train/loss=4.548458, validation/accuracy=0.176680, validation/loss=4.695439, validation/num_examples=50000
I0129 10:55:20.849481 140004616554240 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.1531894207000732, loss=2.435892105102539
I0129 10:55:54.270486 140004608161536 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.1210249662399292, loss=2.214782953262329
I0129 10:56:27.693488 140004616554240 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.127648115158081, loss=2.2957935333251953
I0129 10:57:01.209187 140004608161536 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.1648658514022827, loss=2.1746675968170166
I0129 10:57:34.665419 140004616554240 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.142217755317688, loss=2.2349627017974854
I0129 10:58:08.112254 140004608161536 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0873439311981201, loss=2.2979793548583984
I0129 10:58:41.566171 140004616554240 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.3865245580673218, loss=2.3400728702545166
I0129 10:59:15.017272 140004608161536 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1567343473434448, loss=2.2617533206939697
I0129 10:59:48.446951 140004616554240 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.1455885171890259, loss=2.3226120471954346
I0129 11:00:21.913317 140004608161536 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.2777091264724731, loss=2.2634100914001465
I0129 11:00:55.370217 140004616554240 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.2234901189804077, loss=2.2728424072265625
I0129 11:01:28.817349 140004608161536 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.2218483686447144, loss=2.313364028930664
I0129 11:02:02.263119 140004616554240 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.1246867179870605, loss=2.3448524475097656
I0129 11:02:35.724475 140004608161536 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.235513687133789, loss=2.3194940090179443
I0129 11:03:09.270520 140004616554240 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1241992712020874, loss=2.2385525703430176
I0129 11:03:29.505032 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:03:35.878960 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:03:44.486284 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:03:47.154210 140169137129280 submission_runner.py:408] Time since start: 11670.20s, 	Step: 33462, 	{'train/accuracy': 0.17797352373600006, 'train/loss': 5.004164218902588, 'validation/accuracy': 0.17418000102043152, 'validation/loss': 5.03352689743042, 'validation/num_examples': 50000, 'test/accuracy': 0.11430000513792038, 'test/loss': 6.097242832183838, 'test/num_examples': 10000, 'score': 11255.838641881943, 'total_duration': 11670.20167684555, 'accumulated_submission_time': 11255.838641881943, 'accumulated_eval_time': 412.54050064086914, 'accumulated_logging_time': 0.7317273616790771}
I0129 11:03:47.179998 140005313861376 logging_writer.py:48] [33462] accumulated_eval_time=412.540501, accumulated_logging_time=0.731727, accumulated_submission_time=11255.838642, global_step=33462, preemption_count=0, score=11255.838642, test/accuracy=0.114300, test/loss=6.097243, test/num_examples=10000, total_duration=11670.201677, train/accuracy=0.177974, train/loss=5.004164, validation/accuracy=0.174180, validation/loss=5.033527, validation/num_examples=50000
I0129 11:04:00.257131 140005322254080 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.1584714651107788, loss=2.1877429485321045
I0129 11:04:33.692006 140005313861376 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.0531058311462402, loss=2.261843681335449
I0129 11:05:07.126238 140005322254080 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.061682939529419, loss=2.379051446914673
I0129 11:05:40.576412 140005313861376 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.059207797050476, loss=2.233642816543579
I0129 11:06:14.040509 140005322254080 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1915439367294312, loss=2.3909826278686523
I0129 11:06:47.496986 140005313861376 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1163182258605957, loss=2.4297332763671875
I0129 11:07:20.942082 140005322254080 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.159023642539978, loss=2.3028149604797363
I0129 11:07:54.400003 140005313861376 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.1895301342010498, loss=2.3473148345947266
I0129 11:08:27.853811 140005322254080 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.2832733392715454, loss=2.40933895111084
I0129 11:09:01.317414 140005313861376 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.2175312042236328, loss=2.3556127548217773
I0129 11:09:34.843865 140005322254080 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1845670938491821, loss=2.3876655101776123
I0129 11:10:08.306501 140005313861376 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.1231135129928589, loss=2.296455144882202
I0129 11:10:41.740212 140005322254080 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.3302572965621948, loss=2.2860004901885986
I0129 11:11:15.202446 140005313861376 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.2661631107330322, loss=2.368800640106201
I0129 11:11:48.644642 140005322254080 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.1967500448226929, loss=2.382138967514038
I0129 11:12:17.249810 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:12:23.678175 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:12:32.775161 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:12:35.433064 140169137129280 submission_runner.py:408] Time since start: 12198.48s, 	Step: 34987, 	{'train/accuracy': 0.18648357689380646, 'train/loss': 4.9204301834106445, 'validation/accuracy': 0.179639995098114, 'validation/loss': 5.009549140930176, 'validation/num_examples': 50000, 'test/accuracy': 0.1218000054359436, 'test/loss': 5.849211692810059, 'test/num_examples': 10000, 'score': 11765.846685171127, 'total_duration': 12198.48053264618, 'accumulated_submission_time': 11765.846685171127, 'accumulated_eval_time': 430.7237157821655, 'accumulated_logging_time': 0.7693831920623779}
I0129 11:12:35.459469 140004624946944 logging_writer.py:48] [34987] accumulated_eval_time=430.723716, accumulated_logging_time=0.769383, accumulated_submission_time=11765.846685, global_step=34987, preemption_count=0, score=11765.846685, test/accuracy=0.121800, test/loss=5.849212, test/num_examples=10000, total_duration=12198.480533, train/accuracy=0.186484, train/loss=4.920430, validation/accuracy=0.179640, validation/loss=5.009549, validation/num_examples=50000
I0129 11:12:40.158505 140005288683264 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.110542893409729, loss=2.375977039337158
I0129 11:13:13.556752 140004624946944 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1078377962112427, loss=2.305771827697754
I0129 11:13:46.967555 140005288683264 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.129090428352356, loss=2.2155418395996094
I0129 11:14:20.422530 140004624946944 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.1668332815170288, loss=2.280029058456421
I0129 11:14:53.861696 140005288683264 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0650036334991455, loss=2.3031275272369385
I0129 11:15:27.309155 140004624946944 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1086877584457397, loss=2.3076889514923096
I0129 11:16:00.838809 140005288683264 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.124274492263794, loss=2.266031503677368
I0129 11:16:34.297365 140004624946944 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.133553385734558, loss=2.2450110912323
I0129 11:17:07.742960 140005288683264 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1500418186187744, loss=2.333765745162964
I0129 11:17:41.173054 140004624946944 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.1358946561813354, loss=2.291703462600708
I0129 11:18:14.604739 140005288683264 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.2335484027862549, loss=2.3062312602996826
I0129 11:18:48.058918 140004624946944 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.1584025621414185, loss=2.206627130508423
I0129 11:19:21.502787 140005288683264 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.0893722772598267, loss=2.285438060760498
I0129 11:19:54.939205 140004624946944 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.3098596334457397, loss=2.3613436222076416
I0129 11:20:28.382692 140005288683264 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1695845127105713, loss=2.3726353645324707
I0129 11:21:01.815385 140004624946944 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.3491158485412598, loss=2.4083452224731445
I0129 11:21:05.659269 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:21:12.075880 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:21:20.875724 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:21:23.589499 140169137129280 submission_runner.py:408] Time since start: 12726.64s, 	Step: 36513, 	{'train/accuracy': 0.30430883169174194, 'train/loss': 3.4593262672424316, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.5893800258636475, 'validation/num_examples': 50000, 'test/accuracy': 0.22230000793933868, 'test/loss': 4.251679420471191, 'test/num_examples': 10000, 'score': 12275.982605934143, 'total_duration': 12726.636957645416, 'accumulated_submission_time': 12275.982605934143, 'accumulated_eval_time': 448.6538984775543, 'accumulated_logging_time': 0.8097381591796875}
I0129 11:21:23.616277 140004624946944 logging_writer.py:48] [36513] accumulated_eval_time=448.653898, accumulated_logging_time=0.809738, accumulated_submission_time=12275.982606, global_step=36513, preemption_count=0, score=12275.982606, test/accuracy=0.222300, test/loss=4.251679, test/num_examples=10000, total_duration=12726.636958, train/accuracy=0.304309, train/loss=3.459326, validation/accuracy=0.290720, validation/loss=3.589380, validation/num_examples=50000
I0129 11:21:53.000464 140005322254080 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.1197410821914673, loss=2.0658445358276367
I0129 11:22:26.503899 140004624946944 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.4632588624954224, loss=2.3190340995788574
I0129 11:22:59.933960 140005322254080 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1590934991836548, loss=2.2238383293151855
I0129 11:23:33.363993 140004624946944 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.2132395505905151, loss=2.323767900466919
I0129 11:24:06.840940 140005322254080 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.087437391281128, loss=2.2269935607910156
I0129 11:24:40.277103 140004624946944 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.2057690620422363, loss=2.21916127204895
I0129 11:25:13.712155 140005322254080 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.3767688274383545, loss=2.3225653171539307
I0129 11:25:47.168580 140004624946944 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.054324746131897, loss=2.2767324447631836
I0129 11:26:20.627587 140005322254080 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.2044786214828491, loss=2.234187602996826
I0129 11:26:54.073822 140004624946944 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.2046663761138916, loss=2.3266963958740234
I0129 11:27:27.503115 140005322254080 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0936102867126465, loss=2.2440285682678223
I0129 11:28:00.943715 140004624946944 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2708516120910645, loss=2.377511501312256
I0129 11:28:34.498480 140005322254080 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.2988464832305908, loss=2.226614475250244
I0129 11:29:07.936162 140004624946944 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.2467743158340454, loss=2.4196107387542725
I0129 11:29:41.369285 140005322254080 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1766177415847778, loss=2.2319512367248535
I0129 11:29:53.889431 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:30:00.257007 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:30:09.236867 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:30:11.900728 140169137129280 submission_runner.py:408] Time since start: 13254.95s, 	Step: 38039, 	{'train/accuracy': 0.2504782974720001, 'train/loss': 3.9678797721862793, 'validation/accuracy': 0.23197999596595764, 'validation/loss': 4.1183319091796875, 'validation/num_examples': 50000, 'test/accuracy': 0.1794000118970871, 'test/loss': 4.738935470581055, 'test/num_examples': 10000, 'score': 12786.193783521652, 'total_duration': 13254.94819188118, 'accumulated_submission_time': 12786.193783521652, 'accumulated_eval_time': 466.66515278816223, 'accumulated_logging_time': 0.8490216732025146}
I0129 11:30:11.927203 140004608161536 logging_writer.py:48] [38039] accumulated_eval_time=466.665153, accumulated_logging_time=0.849022, accumulated_submission_time=12786.193784, global_step=38039, preemption_count=0, score=12786.193784, test/accuracy=0.179400, test/loss=4.738935, test/num_examples=10000, total_duration=13254.948192, train/accuracy=0.250478, train/loss=3.967880, validation/accuracy=0.231980, validation/loss=4.118332, validation/num_examples=50000
I0129 11:30:32.649243 140004616554240 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.2926063537597656, loss=2.223332405090332
I0129 11:31:06.039271 140004608161536 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.219592809677124, loss=2.295008897781372
I0129 11:31:39.486187 140004616554240 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.2845927476882935, loss=2.377040386199951
I0129 11:32:12.918894 140004608161536 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.1113629341125488, loss=2.1817493438720703
I0129 11:32:46.348666 140004616554240 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.2846920490264893, loss=2.3215339183807373
I0129 11:33:19.800263 140004608161536 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.2060712575912476, loss=2.297175407409668
I0129 11:33:53.246728 140004616554240 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.2115223407745361, loss=2.2150139808654785
I0129 11:34:26.704735 140004608161536 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.105306625366211, loss=2.2382559776306152
I0129 11:35:00.364409 140004616554240 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2776713371276855, loss=2.338531494140625
I0129 11:35:33.791778 140004608161536 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1426591873168945, loss=2.159123420715332
I0129 11:36:07.218012 140004616554240 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.160602331161499, loss=2.4849071502685547
I0129 11:36:40.661258 140004608161536 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1728864908218384, loss=2.3421804904937744
I0129 11:37:14.105969 140004616554240 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1517735719680786, loss=2.2561843395233154
I0129 11:37:47.544348 140004608161536 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.3374149799346924, loss=2.2394092082977295
I0129 11:38:20.984627 140004616554240 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.2901508808135986, loss=2.312013626098633
I0129 11:38:42.026434 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:38:48.524761 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:38:57.181164 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:38:59.847564 140169137129280 submission_runner.py:408] Time since start: 13782.90s, 	Step: 39563, 	{'train/accuracy': 0.09614157676696777, 'train/loss': 5.963015556335449, 'validation/accuracy': 0.08895999938249588, 'validation/loss': 6.041711807250977, 'validation/num_examples': 50000, 'test/accuracy': 0.06270000338554382, 'test/loss': 6.510247230529785, 'test/num_examples': 10000, 'score': 13296.231772899628, 'total_duration': 13782.895034313202, 'accumulated_submission_time': 13296.231772899628, 'accumulated_eval_time': 484.48624563217163, 'accumulated_logging_time': 0.8864550590515137}
I0129 11:38:59.874383 140005313861376 logging_writer.py:48] [39563] accumulated_eval_time=484.486246, accumulated_logging_time=0.886455, accumulated_submission_time=13296.231773, global_step=39563, preemption_count=0, score=13296.231773, test/accuracy=0.062700, test/loss=6.510247, test/num_examples=10000, total_duration=13782.895034, train/accuracy=0.096142, train/loss=5.963016, validation/accuracy=0.088960, validation/loss=6.041712, validation/num_examples=50000
I0129 11:39:12.571603 140005322254080 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1690034866333008, loss=2.3008508682250977
I0129 11:39:45.955509 140005313861376 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.2465633153915405, loss=2.3047072887420654
I0129 11:40:19.390762 140005322254080 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.308806300163269, loss=2.4285969734191895
I0129 11:40:52.818830 140005313861376 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.2537052631378174, loss=2.271897315979004
I0129 11:41:26.379820 140005322254080 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.1332927942276, loss=2.367722749710083
I0129 11:41:59.799141 140005313861376 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.3229358196258545, loss=2.1973235607147217
I0129 11:42:33.237225 140005322254080 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.083449363708496, loss=2.26481294631958
I0129 11:43:06.661269 140005313861376 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.27242112159729, loss=2.20477294921875
I0129 11:43:40.082579 140005322254080 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.4153648614883423, loss=2.3560967445373535
I0129 11:44:13.556731 140005313861376 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.3505914211273193, loss=2.164673089981079
I0129 11:44:46.994880 140005322254080 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.243263602256775, loss=2.3838701248168945
I0129 11:45:20.427609 140005313861376 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.2359083890914917, loss=2.440159797668457
I0129 11:45:53.870662 140005322254080 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1740833520889282, loss=2.2968130111694336
I0129 11:46:27.302747 140005313861376 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.2895147800445557, loss=2.322993278503418
I0129 11:47:00.741520 140005322254080 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.1484556198120117, loss=2.3262526988983154
I0129 11:47:29.978778 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:47:36.436876 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:47:45.406625 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:47:48.096895 140169137129280 submission_runner.py:408] Time since start: 14311.14s, 	Step: 41089, 	{'train/accuracy': 0.0666852667927742, 'train/loss': 6.601611614227295, 'validation/accuracy': 0.05983999744057655, 'validation/loss': 6.742616653442383, 'validation/num_examples': 50000, 'test/accuracy': 0.04020000249147415, 'test/loss': 7.291037082672119, 'test/num_examples': 10000, 'score': 13806.275705337524, 'total_duration': 14311.14434671402, 'accumulated_submission_time': 13806.275705337524, 'accumulated_eval_time': 502.60431265830994, 'accumulated_logging_time': 0.9239518642425537}
I0129 11:47:48.124179 140005288683264 logging_writer.py:48] [41089] accumulated_eval_time=502.604313, accumulated_logging_time=0.923952, accumulated_submission_time=13806.275705, global_step=41089, preemption_count=0, score=13806.275705, test/accuracy=0.040200, test/loss=7.291037, test/num_examples=10000, total_duration=14311.144347, train/accuracy=0.066685, train/loss=6.601612, validation/accuracy=0.059840, validation/loss=6.742617, validation/num_examples=50000
I0129 11:47:52.157161 140005297075968 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.2548906803131104, loss=2.274226188659668
I0129 11:48:25.536365 140005288683264 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1768797636032104, loss=2.3633382320404053
I0129 11:48:58.944983 140005297075968 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2030441761016846, loss=2.2836570739746094
I0129 11:49:32.371630 140005288683264 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.2423346042633057, loss=2.1927056312561035
I0129 11:50:05.809488 140005297075968 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2322149276733398, loss=2.3289589881896973
I0129 11:50:39.255295 140005288683264 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1978532075881958, loss=2.3524608612060547
I0129 11:51:12.690598 140005297075968 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1326466798782349, loss=2.278825521469116
I0129 11:51:46.120635 140005288683264 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.3296425342559814, loss=2.182687282562256
I0129 11:52:19.548993 140005297075968 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.2448360919952393, loss=2.3362340927124023
I0129 11:52:52.979629 140005288683264 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.2845516204833984, loss=2.278836727142334
I0129 11:53:26.410732 140005297075968 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1537458896636963, loss=2.1217544078826904
I0129 11:53:59.846480 140005288683264 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.4852294921875, loss=2.3119707107543945
I0129 11:54:33.375309 140005297075968 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1858289241790771, loss=2.187267541885376
I0129 11:55:06.813984 140005288683264 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.2311761379241943, loss=2.3197832107543945
I0129 11:55:40.256371 140005297075968 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2730462551116943, loss=2.2139487266540527
I0129 11:56:13.697732 140005288683264 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.1548926830291748, loss=2.271578550338745
I0129 11:56:18.200094 140169137129280 spec.py:321] Evaluating on the training split.
I0129 11:56:24.612294 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 11:56:33.463690 140169137129280 spec.py:349] Evaluating on the test split.
I0129 11:56:36.070302 140169137129280 submission_runner.py:408] Time since start: 14839.12s, 	Step: 42615, 	{'train/accuracy': 0.322963148355484, 'train/loss': 3.35842227935791, 'validation/accuracy': 0.3021000027656555, 'validation/loss': 3.5063271522521973, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.310806751251221, 'test/num_examples': 10000, 'score': 14316.291334152222, 'total_duration': 14839.117769956589, 'accumulated_submission_time': 14316.291334152222, 'accumulated_eval_time': 520.4744794368744, 'accumulated_logging_time': 0.9617166519165039}
I0129 11:56:36.096066 140004624946944 logging_writer.py:48] [42615] accumulated_eval_time=520.474479, accumulated_logging_time=0.961717, accumulated_submission_time=14316.291334, global_step=42615, preemption_count=0, score=14316.291334, test/accuracy=0.220100, test/loss=4.310807, test/num_examples=10000, total_duration=14839.117770, train/accuracy=0.322963, train/loss=3.358422, validation/accuracy=0.302100, validation/loss=3.506327, validation/num_examples=50000
I0129 11:57:04.810577 140005288683264 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.273023009300232, loss=2.210261106491089
I0129 11:57:38.199184 140004624946944 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.380398154258728, loss=2.257747173309326
I0129 11:58:11.615463 140005288683264 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2710237503051758, loss=2.2808616161346436
I0129 11:58:45.023200 140004624946944 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1615192890167236, loss=2.3318276405334473
I0129 11:59:48.410485 140005288683264 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.3520598411560059, loss=2.3478801250457764
I0129 12:00:23.067880 140004624946944 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.2319672107696533, loss=2.3375730514526367
I0129 12:00:56.552170 140005288683264 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.1808717250823975, loss=2.25205135345459
I0129 12:01:29.981254 140004624946944 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.2812689542770386, loss=2.2021732330322266
I0129 12:02:03.399931 140005288683264 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.3726788759231567, loss=2.256510019302368
I0129 12:02:36.824077 140004624946944 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2988393306732178, loss=2.3736233711242676
I0129 12:03:10.236696 140005288683264 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.2129828929901123, loss=2.202479362487793
I0129 12:03:43.677501 140004624946944 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1248974800109863, loss=2.3117482662200928
I0129 12:04:17.095870 140005288683264 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1687616109848022, loss=2.3067963123321533
I0129 12:04:50.534832 140004624946944 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1762315034866333, loss=2.2895331382751465
I0129 12:05:06.393526 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:05:12.851783 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:05:21.704790 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:05:24.434340 140169137129280 submission_runner.py:408] Time since start: 15367.48s, 	Step: 44049, 	{'train/accuracy': 0.2813097834587097, 'train/loss': 3.6863856315612793, 'validation/accuracy': 0.2552799880504608, 'validation/loss': 3.8958096504211426, 'validation/num_examples': 50000, 'test/accuracy': 0.18490001559257507, 'test/loss': 4.619652271270752, 'test/num_examples': 10000, 'score': 14826.530678033829, 'total_duration': 15367.481809616089, 'accumulated_submission_time': 14826.530678033829, 'accumulated_eval_time': 538.5152575969696, 'accumulated_logging_time': 0.9980008602142334}
I0129 12:05:24.463709 140004616554240 logging_writer.py:48] [44049] accumulated_eval_time=538.515258, accumulated_logging_time=0.998001, accumulated_submission_time=14826.530678, global_step=44049, preemption_count=0, score=14826.530678, test/accuracy=0.184900, test/loss=4.619652, test/num_examples=10000, total_duration=15367.481810, train/accuracy=0.281310, train/loss=3.686386, validation/accuracy=0.255280, validation/loss=3.895810, validation/num_examples=50000
I0129 12:05:41.842982 140004624946944 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.2933403253555298, loss=2.300489664077759
I0129 12:06:15.224356 140004616554240 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.2510485649108887, loss=2.3872838020324707
I0129 12:06:48.623465 140004624946944 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.4705390930175781, loss=2.2951974868774414
I0129 12:07:22.063589 140004616554240 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.3874163627624512, loss=2.3409488201141357
I0129 12:07:55.575322 140004624946944 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4191484451293945, loss=2.4380173683166504
I0129 12:08:29.011125 140004616554240 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1887775659561157, loss=2.273167371749878
I0129 12:09:02.440373 140004624946944 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.293411135673523, loss=2.2918004989624023
I0129 12:09:35.880205 140004616554240 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.2597377300262451, loss=2.2291018962860107
I0129 12:10:09.312587 140004624946944 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.471458077430725, loss=2.2627296447753906
I0129 12:10:42.740331 140004616554240 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.184262990951538, loss=2.143240451812744
I0129 12:11:16.167020 140004624946944 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.2144936323165894, loss=2.301591396331787
I0129 12:11:49.594747 140004616554240 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.1415830850601196, loss=2.1381707191467285
I0129 12:12:23.001214 140004624946944 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.2424460649490356, loss=2.3629860877990723
I0129 12:12:56.421848 140004616554240 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1575895547866821, loss=2.138685464859009
I0129 12:13:29.867181 140004624946944 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.2406576871871948, loss=2.159710645675659
I0129 12:13:54.503656 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:14:01.016198 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:14:09.640317 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:14:12.321837 140169137129280 submission_runner.py:408] Time since start: 15895.37s, 	Step: 45575, 	{'train/accuracy': 0.3519212305545807, 'train/loss': 3.07643723487854, 'validation/accuracy': 0.3336399793624878, 'validation/loss': 3.2434942722320557, 'validation/num_examples': 50000, 'test/accuracy': 0.24820001423358917, 'test/loss': 4.0240983963012695, 'test/num_examples': 10000, 'score': 15336.509346485138, 'total_duration': 15895.369309186935, 'accumulated_submission_time': 15336.509346485138, 'accumulated_eval_time': 556.3334038257599, 'accumulated_logging_time': 1.0387768745422363}
I0129 12:14:12.348523 140005313861376 logging_writer.py:48] [45575] accumulated_eval_time=556.333404, accumulated_logging_time=1.038777, accumulated_submission_time=15336.509346, global_step=45575, preemption_count=0, score=15336.509346, test/accuracy=0.248200, test/loss=4.024098, test/num_examples=10000, total_duration=15895.369309, train/accuracy=0.351921, train/loss=3.076437, validation/accuracy=0.333640, validation/loss=3.243494, validation/num_examples=50000
I0129 12:14:21.045817 140005322254080 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.3438392877578735, loss=2.2904069423675537
I0129 12:14:54.413967 140005313861376 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1519789695739746, loss=2.2584855556488037
I0129 12:15:27.811906 140005322254080 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.2979414463043213, loss=2.250812292098999
I0129 12:16:01.262628 140005313861376 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1792398691177368, loss=2.3494856357574463
I0129 12:16:34.706057 140005322254080 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.3822598457336426, loss=2.3314902782440186
I0129 12:17:08.126019 140005313861376 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.3855750560760498, loss=2.2656404972076416
I0129 12:17:41.546820 140005322254080 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.2220120429992676, loss=2.340449333190918
I0129 12:18:14.961302 140005313861376 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.2275816202163696, loss=2.0826101303100586
I0129 12:18:48.384249 140005322254080 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.204797625541687, loss=2.131159543991089
I0129 12:19:21.803635 140005313861376 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1910876035690308, loss=2.201552391052246
I0129 12:19:55.246671 140005322254080 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2730134725570679, loss=2.2772574424743652
I0129 12:20:28.762927 140005313861376 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.1304820775985718, loss=2.230238914489746
I0129 12:21:02.190052 140005322254080 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.3958138227462769, loss=2.296407699584961
I0129 12:21:35.626091 140005313861376 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.2825098037719727, loss=2.2189595699310303
I0129 12:22:09.050101 140005322254080 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.2390367984771729, loss=2.2586092948913574
I0129 12:22:42.504437 140005313861376 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.321668267250061, loss=2.2585091590881348
I0129 12:22:42.512434 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:22:48.866539 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:22:57.345112 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:23:00.061277 140169137129280 submission_runner.py:408] Time since start: 16423.11s, 	Step: 47101, 	{'train/accuracy': 0.20083306729793549, 'train/loss': 4.566847324371338, 'validation/accuracy': 0.1885399967432022, 'validation/loss': 4.639566898345947, 'validation/num_examples': 50000, 'test/accuracy': 0.13790000975131989, 'test/loss': 5.32618522644043, 'test/num_examples': 10000, 'score': 15846.612461805344, 'total_duration': 16423.10874414444, 'accumulated_submission_time': 15846.612461805344, 'accumulated_eval_time': 573.8821873664856, 'accumulated_logging_time': 1.0766091346740723}
I0129 12:23:00.088729 140004624946944 logging_writer.py:48] [47101] accumulated_eval_time=573.882187, accumulated_logging_time=1.076609, accumulated_submission_time=15846.612462, global_step=47101, preemption_count=0, score=15846.612462, test/accuracy=0.137900, test/loss=5.326185, test/num_examples=10000, total_duration=16423.108744, train/accuracy=0.200833, train/loss=4.566847, validation/accuracy=0.188540, validation/loss=4.639567, validation/num_examples=50000
I0129 12:23:33.474596 140005288683264 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.1903318166732788, loss=2.300034761428833
I0129 12:24:06.873373 140004624946944 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.3461264371871948, loss=2.293436050415039
I0129 12:24:40.287116 140005288683264 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.286025047302246, loss=2.290550708770752
I0129 12:25:13.716563 140004624946944 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.3495985269546509, loss=2.3786821365356445
I0129 12:25:47.146844 140005288683264 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.2091002464294434, loss=2.1678168773651123
I0129 12:26:20.572740 140004624946944 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.316954255104065, loss=2.241680383682251
I0129 12:26:54.118680 140005288683264 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.2997467517852783, loss=2.353349208831787
I0129 12:27:27.551950 140004624946944 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.3072636127471924, loss=2.2153751850128174
I0129 12:28:00.992980 140005288683264 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.3292611837387085, loss=2.248342275619507
I0129 12:28:34.415282 140004624946944 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.1686769723892212, loss=2.127312421798706
I0129 12:29:07.834105 140005288683264 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.128601312637329, loss=2.306427240371704
I0129 12:29:41.263329 140004624946944 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.3155041933059692, loss=2.396545648574829
I0129 12:30:14.712924 140005288683264 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.300563931465149, loss=2.434048891067505
I0129 12:30:48.163275 140004624946944 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2593774795532227, loss=2.2012386322021484
I0129 12:31:21.581590 140005288683264 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.1758708953857422, loss=2.150243043899536
I0129 12:31:30.093680 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:31:36.525915 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:31:45.149515 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:31:47.867984 140169137129280 submission_runner.py:408] Time since start: 16950.92s, 	Step: 48627, 	{'train/accuracy': 0.2834024131298065, 'train/loss': 3.6968536376953125, 'validation/accuracy': 0.2582399845123291, 'validation/loss': 3.9303948879241943, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.547379016876221, 'test/num_examples': 10000, 'score': 16356.553442955017, 'total_duration': 16950.91544485092, 'accumulated_submission_time': 16356.553442955017, 'accumulated_eval_time': 591.6564452648163, 'accumulated_logging_time': 1.1180686950683594}
I0129 12:31:47.896844 140005313861376 logging_writer.py:48] [48627] accumulated_eval_time=591.656445, accumulated_logging_time=1.118069, accumulated_submission_time=16356.553443, global_step=48627, preemption_count=0, score=16356.553443, test/accuracy=0.203500, test/loss=4.547379, test/num_examples=10000, total_duration=16950.915445, train/accuracy=0.283402, train/loss=3.696854, validation/accuracy=0.258240, validation/loss=3.930395, validation/num_examples=50000
I0129 12:32:12.601596 140005322254080 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.246574878692627, loss=2.199619770050049
I0129 12:32:45.973495 140005313861376 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2123339176177979, loss=2.119882345199585
I0129 12:33:19.461668 140005322254080 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2007789611816406, loss=2.2756459712982178
I0129 12:33:52.878589 140005313861376 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3362013101577759, loss=2.2246499061584473
I0129 12:34:26.305153 140005322254080 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.4034870862960815, loss=2.197995901107788
I0129 12:34:59.738691 140005313861376 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.2699624300003052, loss=2.0895321369171143
I0129 12:35:33.164968 140005322254080 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2819762229919434, loss=2.2587671279907227
I0129 12:36:06.592750 140005313861376 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.3029956817626953, loss=2.364995002746582
I0129 12:36:40.015674 140005322254080 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.2560303211212158, loss=2.139629602432251
I0129 12:37:13.447352 140005313861376 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2437206506729126, loss=2.2842869758605957
I0129 12:37:46.873420 140005322254080 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2298749685287476, loss=2.289397716522217
I0129 12:38:20.301117 140005313861376 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.3101747035980225, loss=2.255192279815674
I0129 12:38:53.726961 140005322254080 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2836445569992065, loss=2.2865700721740723
I0129 12:39:27.239692 140005313861376 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.2785084247589111, loss=2.1506454944610596
I0129 12:40:00.656685 140005322254080 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.359175205230713, loss=2.2376511096954346
I0129 12:40:18.191671 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:40:24.644439 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:40:33.472696 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:40:36.149357 140169137129280 submission_runner.py:408] Time since start: 17479.20s, 	Step: 50154, 	{'train/accuracy': 0.27758291363716125, 'train/loss': 3.7395527362823486, 'validation/accuracy': 0.2506999969482422, 'validation/loss': 3.980403423309326, 'validation/num_examples': 50000, 'test/accuracy': 0.19620001316070557, 'test/loss': 4.586901664733887, 'test/num_examples': 10000, 'score': 16866.786379098892, 'total_duration': 17479.196828603745, 'accumulated_submission_time': 16866.786379098892, 'accumulated_eval_time': 609.6140928268433, 'accumulated_logging_time': 1.1585710048675537}
I0129 12:40:36.176352 140004608161536 logging_writer.py:48] [50154] accumulated_eval_time=609.614093, accumulated_logging_time=1.158571, accumulated_submission_time=16866.786379, global_step=50154, preemption_count=0, score=16866.786379, test/accuracy=0.196200, test/loss=4.586902, test/num_examples=10000, total_duration=17479.196829, train/accuracy=0.277583, train/loss=3.739553, validation/accuracy=0.250700, validation/loss=3.980403, validation/num_examples=50000
I0129 12:40:51.871203 140004616554240 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.3278766870498657, loss=2.321380376815796
I0129 12:41:25.227378 140004608161536 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2103495597839355, loss=2.2705843448638916
I0129 12:41:58.622198 140004616554240 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2912653684616089, loss=2.128640651702881
I0129 12:42:32.032420 140004608161536 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.3542981147766113, loss=2.4074440002441406
I0129 12:43:05.483803 140004616554240 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.5225738286972046, loss=2.2711923122406006
I0129 12:43:38.907385 140004608161536 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.226810336112976, loss=2.1941239833831787
I0129 12:44:12.317458 140004616554240 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.4012446403503418, loss=2.3383629322052
I0129 12:44:45.738592 140004608161536 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.3249081373214722, loss=2.2147035598754883
I0129 12:45:19.155486 140004616554240 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.394096851348877, loss=2.2997562885284424
I0129 12:45:52.706475 140004608161536 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.345094084739685, loss=2.3335249423980713
I0129 12:46:26.138466 140004616554240 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.3999241590499878, loss=2.2724084854125977
I0129 12:46:59.562071 140004608161536 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2664965391159058, loss=2.3906643390655518
I0129 12:47:32.988012 140004616554240 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2186022996902466, loss=2.2436792850494385
I0129 12:48:06.409792 140004608161536 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.335890769958496, loss=2.2899973392486572
I0129 12:48:39.829348 140004616554240 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2973203659057617, loss=2.178025722503662
I0129 12:49:06.381934 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:49:12.733283 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:49:21.789837 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:49:24.525393 140169137129280 submission_runner.py:408] Time since start: 18007.57s, 	Step: 51681, 	{'train/accuracy': 0.3650350570678711, 'train/loss': 3.0436782836914062, 'validation/accuracy': 0.3366599977016449, 'validation/loss': 3.240746259689331, 'validation/num_examples': 50000, 'test/accuracy': 0.2612999975681305, 'test/loss': 3.977926254272461, 'test/num_examples': 10000, 'score': 17376.93217921257, 'total_duration': 18007.572848796844, 'accumulated_submission_time': 17376.93217921257, 'accumulated_eval_time': 627.7574996948242, 'accumulated_logging_time': 1.1959059238433838}
I0129 12:49:24.554588 140004616554240 logging_writer.py:48] [51681] accumulated_eval_time=627.757500, accumulated_logging_time=1.195906, accumulated_submission_time=17376.932179, global_step=51681, preemption_count=0, score=17376.932179, test/accuracy=0.261300, test/loss=3.977926, test/num_examples=10000, total_duration=18007.572849, train/accuracy=0.365035, train/loss=3.043678, validation/accuracy=0.336660, validation/loss=3.240746, validation/num_examples=50000
I0129 12:49:31.238826 140005305468672 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.2857393026351929, loss=2.296292781829834
I0129 12:50:04.613579 140004616554240 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.4649834632873535, loss=2.2411179542541504
I0129 12:50:38.010796 140005305468672 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2987263202667236, loss=2.3780486583709717
I0129 12:51:11.431168 140004616554240 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2805192470550537, loss=2.280034065246582
I0129 12:51:44.849359 140005305468672 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.4322870969772339, loss=2.11387038230896
I0129 12:52:18.362255 140004616554240 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.257243037223816, loss=2.1118650436401367
I0129 12:52:51.797294 140005305468672 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.348548173904419, loss=2.2160563468933105
I0129 12:53:25.215886 140004616554240 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.2158000469207764, loss=2.2356882095336914
I0129 12:53:58.666266 140005305468672 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2362327575683594, loss=2.195591926574707
I0129 12:54:32.097722 140004616554240 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.3735227584838867, loss=2.0846474170684814
I0129 12:55:05.511993 140005305468672 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.3419357538223267, loss=2.3257577419281006
I0129 12:55:38.933279 140004616554240 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.3638741970062256, loss=2.200451374053955
I0129 12:56:12.372581 140005305468672 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.208116888999939, loss=2.3133890628814697
I0129 12:56:45.795244 140004616554240 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.329018235206604, loss=2.128016471862793
I0129 12:57:19.225206 140005305468672 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.3027814626693726, loss=2.425443649291992
I0129 12:57:52.635351 140004616554240 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.3304308652877808, loss=2.178133726119995
I0129 12:57:54.786648 140169137129280 spec.py:321] Evaluating on the training split.
I0129 12:58:01.233682 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 12:58:09.796876 140169137129280 spec.py:349] Evaluating on the test split.
I0129 12:58:12.504276 140169137129280 submission_runner.py:408] Time since start: 18535.55s, 	Step: 53208, 	{'train/accuracy': 0.33207109570503235, 'train/loss': 3.289456367492676, 'validation/accuracy': 0.3113200068473816, 'validation/loss': 3.4361612796783447, 'validation/num_examples': 50000, 'test/accuracy': 0.23360000550746918, 'test/loss': 4.169589042663574, 'test/num_examples': 10000, 'score': 17887.1039853096, 'total_duration': 18535.551746606827, 'accumulated_submission_time': 17887.1039853096, 'accumulated_eval_time': 645.4750876426697, 'accumulated_logging_time': 1.2352380752563477}
I0129 12:58:12.535492 140005313861376 logging_writer.py:48] [53208] accumulated_eval_time=645.475088, accumulated_logging_time=1.235238, accumulated_submission_time=17887.103985, global_step=53208, preemption_count=0, score=17887.103985, test/accuracy=0.233600, test/loss=4.169589, test/num_examples=10000, total_duration=18535.551747, train/accuracy=0.332071, train/loss=3.289456, validation/accuracy=0.311320, validation/loss=3.436161, validation/num_examples=50000
I0129 12:58:43.676737 140005322254080 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2272329330444336, loss=2.235675096511841
I0129 12:59:17.058610 140005313861376 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2946501970291138, loss=2.170246124267578
I0129 12:59:50.467013 140005322254080 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.337505578994751, loss=2.2017104625701904
I0129 13:00:23.873814 140005313861376 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.22080659866333, loss=2.192106008529663
I0129 13:00:57.327324 140005322254080 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.3769315481185913, loss=2.2533113956451416
I0129 13:01:30.741084 140005313861376 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2418650388717651, loss=2.3313002586364746
I0129 13:02:04.165051 140005322254080 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.464587688446045, loss=2.219477415084839
I0129 13:02:37.568483 140005313861376 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.4207046031951904, loss=2.21625018119812
I0129 13:03:10.980629 140005322254080 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.3681753873825073, loss=2.3956356048583984
I0129 13:03:44.406556 140005313861376 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2026491165161133, loss=2.2323055267333984
I0129 13:04:17.822428 140005322254080 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.3435617685317993, loss=2.374464988708496
I0129 13:04:51.247226 140005313861376 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2365810871124268, loss=2.2783122062683105
I0129 13:05:24.764044 140005322254080 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2556949853897095, loss=2.199924945831299
I0129 13:05:58.182174 140005313861376 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.3902875185012817, loss=2.1663882732391357
I0129 13:06:31.610507 140005322254080 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.1716043949127197, loss=2.199310779571533
I0129 13:06:42.795524 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:06:49.228680 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:06:58.125256 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:07:00.821318 140169137129280 submission_runner.py:408] Time since start: 19063.87s, 	Step: 54735, 	{'train/accuracy': 0.22729989886283875, 'train/loss': 4.260939121246338, 'validation/accuracy': 0.22429999709129333, 'validation/loss': 4.226137161254883, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.019404411315918, 'test/num_examples': 10000, 'score': 18397.301684379578, 'total_duration': 19063.868786096573, 'accumulated_submission_time': 18397.301684379578, 'accumulated_eval_time': 663.5008449554443, 'accumulated_logging_time': 1.2788963317871094}
I0129 13:07:00.854673 140004608161536 logging_writer.py:48] [54735] accumulated_eval_time=663.500845, accumulated_logging_time=1.278896, accumulated_submission_time=18397.301684, global_step=54735, preemption_count=0, score=18397.301684, test/accuracy=0.155100, test/loss=5.019404, test/num_examples=10000, total_duration=19063.868786, train/accuracy=0.227300, train/loss=4.260939, validation/accuracy=0.224300, validation/loss=4.226137, validation/num_examples=50000
I0129 13:07:22.901545 140004616554240 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.3403888940811157, loss=2.3229920864105225
I0129 13:07:56.259165 140004608161536 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.5376633405685425, loss=2.2729434967041016
I0129 13:08:29.661704 140004616554240 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2695232629776, loss=2.2386341094970703
I0129 13:09:03.062557 140004608161536 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2374029159545898, loss=2.259542226791382
I0129 13:09:36.483645 140004616554240 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.4008049964904785, loss=2.2601075172424316
I0129 13:10:09.905518 140004608161536 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.3186883926391602, loss=2.1534242630004883
I0129 13:10:43.327091 140004616554240 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2907085418701172, loss=2.1353769302368164
I0129 13:11:16.857565 140004608161536 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.3136638402938843, loss=2.236246109008789
I0129 13:11:50.295319 140004616554240 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.3004056215286255, loss=2.1179559230804443
I0129 13:12:23.708980 140004608161536 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.3986457586288452, loss=2.242884874343872
I0129 13:12:57.125802 140004616554240 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.354927897453308, loss=2.2216291427612305
I0129 13:13:30.550661 140004608161536 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.300682783126831, loss=2.165900945663452
I0129 13:14:03.966614 140004616554240 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.4561516046524048, loss=2.1928205490112305
I0129 13:14:37.386024 140004608161536 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.229539394378662, loss=2.117645263671875
I0129 13:15:10.808229 140004616554240 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.319383978843689, loss=2.1946825981140137
I0129 13:15:31.026689 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:15:37.410663 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:15:46.308074 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:15:48.992110 140169137129280 submission_runner.py:408] Time since start: 19592.04s, 	Step: 56262, 	{'train/accuracy': 0.19854113459587097, 'train/loss': 4.842617511749268, 'validation/accuracy': 0.1861799955368042, 'validation/loss': 5.003037452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.14670000970363617, 'test/loss': 5.59663200378418, 'test/num_examples': 10000, 'score': 18907.414145231247, 'total_duration': 19592.039578437805, 'accumulated_submission_time': 18907.414145231247, 'accumulated_eval_time': 681.4662253856659, 'accumulated_logging_time': 1.322425127029419}
I0129 13:15:49.021972 140004608161536 logging_writer.py:48] [56262] accumulated_eval_time=681.466225, accumulated_logging_time=1.322425, accumulated_submission_time=18907.414145, global_step=56262, preemption_count=0, score=18907.414145, test/accuracy=0.146700, test/loss=5.596632, test/num_examples=10000, total_duration=19592.039578, train/accuracy=0.198541, train/loss=4.842618, validation/accuracy=0.186180, validation/loss=5.003037, validation/num_examples=50000
I0129 13:16:02.060717 140005305468672 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.3973139524459839, loss=2.1117141246795654
I0129 13:16:35.410982 140004608161536 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.3501434326171875, loss=2.2737104892730713
I0129 13:17:08.806124 140005305468672 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.360130786895752, loss=2.289273262023926
I0129 13:17:42.321395 140004608161536 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.3879051208496094, loss=2.3904056549072266
I0129 13:18:15.741179 140005305468672 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.4037487506866455, loss=2.317845106124878
I0129 13:18:49.161747 140004608161536 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2944282293319702, loss=2.3037984371185303
I0129 13:19:22.583791 140005305468672 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2005903720855713, loss=2.220003366470337
I0129 13:19:56.004371 140004608161536 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.3679693937301636, loss=2.3152308464050293
I0129 13:20:29.441183 140005305468672 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2948582172393799, loss=2.1686208248138428
I0129 13:21:02.844331 140004608161536 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.3292611837387085, loss=2.214977741241455
I0129 13:21:36.255162 140005305468672 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2946168184280396, loss=2.2929539680480957
I0129 13:22:09.676758 140004608161536 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.465526819229126, loss=2.172757148742676
I0129 13:22:43.097335 140005305468672 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.1989240646362305, loss=2.176879405975342
I0129 13:23:16.518091 140004608161536 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.3275967836380005, loss=2.238947868347168
I0129 13:23:49.933090 140005305468672 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.3136897087097168, loss=2.1387147903442383
I0129 13:24:19.250979 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:24:25.639199 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:24:34.496866 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:24:37.176825 140169137129280 submission_runner.py:408] Time since start: 20120.22s, 	Step: 57789, 	{'train/accuracy': 0.40565210580825806, 'train/loss': 2.6804370880126953, 'validation/accuracy': 0.3668600022792816, 'validation/loss': 2.9475936889648438, 'validation/num_examples': 50000, 'test/accuracy': 0.27880001068115234, 'test/loss': 3.6488375663757324, 'test/num_examples': 10000, 'score': 19417.582760095596, 'total_duration': 20120.224281072617, 'accumulated_submission_time': 19417.582760095596, 'accumulated_eval_time': 699.392019033432, 'accumulated_logging_time': 1.3630115985870361}
I0129 13:24:37.206534 140005297075968 logging_writer.py:48] [57789] accumulated_eval_time=699.392019, accumulated_logging_time=1.363012, accumulated_submission_time=19417.582760, global_step=57789, preemption_count=0, score=19417.582760, test/accuracy=0.278800, test/loss=3.648838, test/num_examples=10000, total_duration=20120.224281, train/accuracy=0.405652, train/loss=2.680437, validation/accuracy=0.366860, validation/loss=2.947594, validation/num_examples=50000
I0129 13:24:41.219348 140005313861376 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.4382649660110474, loss=2.1896843910217285
I0129 13:25:14.587685 140005297075968 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.3857687711715698, loss=2.236762285232544
I0129 13:25:47.959234 140005313861376 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.348602533340454, loss=2.190619468688965
I0129 13:26:21.380879 140005297075968 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.216179370880127, loss=2.0119972229003906
I0129 13:26:54.793460 140005313861376 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.394790530204773, loss=2.291867256164551
I0129 13:27:28.195405 140005297075968 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.3777718544006348, loss=2.112621545791626
I0129 13:28:01.606709 140005313861376 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.466705322265625, loss=2.181385040283203
I0129 13:28:35.040829 140005297075968 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.3626692295074463, loss=2.1641077995300293
I0129 13:29:08.444196 140005313861376 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.1896542310714722, loss=2.162560224533081
I0129 13:29:41.875932 140005297075968 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1878902912139893, loss=2.203875780105591
I0129 13:30:15.292714 140005313861376 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3567116260528564, loss=2.225743293762207
I0129 13:30:48.829381 140005297075968 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.3189620971679688, loss=2.089708089828491
I0129 13:31:22.245363 140005313861376 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.433959722518921, loss=2.2891008853912354
I0129 13:31:55.666233 140005297075968 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.286145567893982, loss=2.24713397026062
I0129 13:32:29.082498 140005313861376 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1964432001113892, loss=2.1552202701568604
I0129 13:33:02.512618 140005297075968 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.4499588012695312, loss=2.1135525703430176
I0129 13:33:07.338066 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:33:13.736138 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:33:22.527439 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:33:25.236374 140169137129280 submission_runner.py:408] Time since start: 20648.28s, 	Step: 59316, 	{'train/accuracy': 0.4073062837123871, 'train/loss': 2.710867166519165, 'validation/accuracy': 0.3699599802494049, 'validation/loss': 2.9452648162841797, 'validation/num_examples': 50000, 'test/accuracy': 0.28460001945495605, 'test/loss': 3.6405067443847656, 'test/num_examples': 10000, 'score': 19927.652045965195, 'total_duration': 20648.283844470978, 'accumulated_submission_time': 19927.652045965195, 'accumulated_eval_time': 717.2902994155884, 'accumulated_logging_time': 1.4048044681549072}
I0129 13:33:25.266243 140004624946944 logging_writer.py:48] [59316] accumulated_eval_time=717.290299, accumulated_logging_time=1.404804, accumulated_submission_time=19927.652046, global_step=59316, preemption_count=0, score=19927.652046, test/accuracy=0.284600, test/loss=3.640507, test/num_examples=10000, total_duration=20648.283844, train/accuracy=0.407306, train/loss=2.710867, validation/accuracy=0.369960, validation/loss=2.945265, validation/num_examples=50000
I0129 13:33:53.631977 140005288683264 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.2949249744415283, loss=2.185875415802002
I0129 13:34:27.002774 140004624946944 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2181905508041382, loss=2.1636950969696045
I0129 13:35:00.437021 140005288683264 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.58647620677948, loss=2.3232035636901855
I0129 13:35:33.844078 140004624946944 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.220723271369934, loss=2.274235486984253
I0129 13:36:07.273358 140005288683264 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.3891849517822266, loss=2.182896375656128
I0129 13:36:40.690605 140004624946944 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.366106629371643, loss=2.2535009384155273
I0129 13:37:14.195549 140005288683264 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.4909640550613403, loss=2.3469629287719727
I0129 13:37:47.602183 140004624946944 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3020484447479248, loss=2.313225507736206
I0129 13:38:21.009035 140005288683264 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.4478496313095093, loss=2.2829949855804443
I0129 13:38:54.417468 140004624946944 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.2583791017532349, loss=2.236706495285034
I0129 13:39:27.814826 140005288683264 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.3073492050170898, loss=2.135392904281616
I0129 13:40:01.221896 140004624946944 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.6465343236923218, loss=2.210411548614502
I0129 13:40:34.624984 140005288683264 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.4587953090667725, loss=2.0387301445007324
I0129 13:41:08.046942 140004624946944 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2646851539611816, loss=2.2045328617095947
I0129 13:41:41.472648 140005288683264 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.3864144086837769, loss=2.3248202800750732
I0129 13:41:55.337973 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:42:01.704216 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:42:10.461051 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:42:13.151847 140169137129280 submission_runner.py:408] Time since start: 21176.20s, 	Step: 60843, 	{'train/accuracy': 0.23230229318141937, 'train/loss': 4.145813941955566, 'validation/accuracy': 0.22111999988555908, 'validation/loss': 4.304041862487793, 'validation/num_examples': 50000, 'test/accuracy': 0.1575000137090683, 'test/loss': 5.075519561767578, 'test/num_examples': 10000, 'score': 20437.66402554512, 'total_duration': 21176.199315071106, 'accumulated_submission_time': 20437.66402554512, 'accumulated_eval_time': 735.1041345596313, 'accumulated_logging_time': 1.4447991847991943}
I0129 13:42:13.182697 140004624946944 logging_writer.py:48] [60843] accumulated_eval_time=735.104135, accumulated_logging_time=1.444799, accumulated_submission_time=20437.664026, global_step=60843, preemption_count=0, score=20437.664026, test/accuracy=0.157500, test/loss=5.075520, test/num_examples=10000, total_duration=21176.199315, train/accuracy=0.232302, train/loss=4.145814, validation/accuracy=0.221120, validation/loss=4.304042, validation/num_examples=50000
I0129 13:42:32.559978 140005313861376 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.4433871507644653, loss=2.133742332458496
I0129 13:43:05.913814 140004624946944 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.4815995693206787, loss=2.2512402534484863
I0129 13:43:39.418394 140005313861376 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2356358766555786, loss=2.1589720249176025
I0129 13:44:12.813993 140004624946944 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.3490434885025024, loss=2.1863527297973633
I0129 13:44:46.237132 140005313861376 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.348564624786377, loss=2.122532367706299
I0129 13:45:19.647029 140004624946944 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.4680919647216797, loss=2.186575412750244
I0129 13:45:53.054781 140005313861376 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.2577311992645264, loss=2.31205677986145
I0129 13:46:26.459583 140004624946944 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.2619282007217407, loss=2.1148831844329834
I0129 13:46:59.879313 140005313861376 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.39055597782135, loss=2.267881155014038
I0129 13:47:33.298782 140004624946944 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.6329057216644287, loss=2.262652635574341
I0129 13:48:06.725680 140005313861376 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.4215229749679565, loss=2.3402698040008545
I0129 13:48:40.127167 140004624946944 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.3038769960403442, loss=2.2796263694763184
I0129 13:49:13.558965 140005313861376 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.551352620124817, loss=2.1646528244018555
I0129 13:49:46.968093 140004624946944 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.4143465757369995, loss=2.359920024871826
I0129 13:50:20.482342 140005313861376 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.202802062034607, loss=2.165884017944336
I0129 13:50:43.347807 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:50:49.700778 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:50:58.615043 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:51:01.283858 140169137129280 submission_runner.py:408] Time since start: 21704.33s, 	Step: 62370, 	{'train/accuracy': 0.2048588991165161, 'train/loss': 4.66219425201416, 'validation/accuracy': 0.1911199986934662, 'validation/loss': 4.752766132354736, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.52890157699585, 'test/num_examples': 10000, 'score': 20947.766759634018, 'total_duration': 21704.33132839203, 'accumulated_submission_time': 20947.766759634018, 'accumulated_eval_time': 753.0401477813721, 'accumulated_logging_time': 1.4879536628723145}
I0129 13:51:01.319609 140004616554240 logging_writer.py:48] [62370] accumulated_eval_time=753.040148, accumulated_logging_time=1.487954, accumulated_submission_time=20947.766760, global_step=62370, preemption_count=0, score=20947.766760, test/accuracy=0.139900, test/loss=5.528902, test/num_examples=10000, total_duration=21704.331328, train/accuracy=0.204859, train/loss=4.662194, validation/accuracy=0.191120, validation/loss=4.752766, validation/num_examples=50000
I0129 13:51:11.683908 140005288683264 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.4026942253112793, loss=2.180060625076294
I0129 13:51:45.024425 140004616554240 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2521382570266724, loss=2.167154550552368
I0129 13:52:18.413154 140005288683264 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4349722862243652, loss=2.215472459793091
I0129 13:52:51.802481 140004616554240 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2870347499847412, loss=2.349041223526001
I0129 13:53:25.222437 140005288683264 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3369042873382568, loss=2.1796860694885254
I0129 13:53:58.620798 140004616554240 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.281310796737671, loss=2.1281938552856445
I0129 13:54:32.039656 140005288683264 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2392878532409668, loss=2.1769747734069824
I0129 13:55:05.481930 140004616554240 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.4247554540634155, loss=2.288182497024536
I0129 13:55:38.873173 140005288683264 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2134824991226196, loss=2.137050151824951
I0129 13:56:12.366199 140004616554240 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.505750298500061, loss=2.149319887161255
I0129 13:56:45.783231 140005288683264 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.413198471069336, loss=2.170041799545288
I0129 13:57:19.192658 140004616554240 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.5140408277511597, loss=2.1073763370513916
I0129 13:57:52.602198 140005288683264 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.41010582447052, loss=2.254795789718628
I0129 13:58:26.019403 140004616554240 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2761071920394897, loss=2.0191962718963623
I0129 13:58:59.449951 140005288683264 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.315855622291565, loss=2.13107967376709
I0129 13:59:31.325641 140169137129280 spec.py:321] Evaluating on the training split.
I0129 13:59:37.695974 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 13:59:46.218366 140169137129280 spec.py:349] Evaluating on the test split.
I0129 13:59:48.909666 140169137129280 submission_runner.py:408] Time since start: 22231.96s, 	Step: 63897, 	{'train/accuracy': 0.37432238459587097, 'train/loss': 2.886444091796875, 'validation/accuracy': 0.3484399914741516, 'validation/loss': 3.0589442253112793, 'validation/num_examples': 50000, 'test/accuracy': 0.2681000232696533, 'test/loss': 3.77394700050354, 'test/num_examples': 10000, 'score': 21457.712785243988, 'total_duration': 22231.957134723663, 'accumulated_submission_time': 21457.712785243988, 'accumulated_eval_time': 770.624137878418, 'accumulated_logging_time': 1.533905029296875}
I0129 13:59:48.941301 140004608161536 logging_writer.py:48] [63897] accumulated_eval_time=770.624138, accumulated_logging_time=1.533905, accumulated_submission_time=21457.712785, global_step=63897, preemption_count=0, score=21457.712785, test/accuracy=0.268100, test/loss=3.773947, test/num_examples=10000, total_duration=22231.957135, train/accuracy=0.374322, train/loss=2.886444, validation/accuracy=0.348440, validation/loss=3.058944, validation/num_examples=50000
I0129 13:59:50.278702 140004616554240 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.2712851762771606, loss=2.164463996887207
I0129 14:00:23.651908 140004608161536 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3952507972717285, loss=2.2507269382476807
I0129 14:00:57.044862 140004616554240 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.4604295492172241, loss=2.0249454975128174
I0129 14:01:30.440099 140004608161536 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2911906242370605, loss=2.1941072940826416
I0129 14:02:03.852274 140004616554240 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.3672384023666382, loss=2.2492713928222656
I0129 14:02:37.254874 140004608161536 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.5236501693725586, loss=2.2304656505584717
I0129 14:03:10.771193 140004616554240 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3763136863708496, loss=2.277495861053467
I0129 14:03:44.185009 140004608161536 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.4006998538970947, loss=2.1244349479675293
I0129 14:04:17.615575 140004616554240 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.4468023777008057, loss=2.137667655944824
I0129 14:04:51.025575 140004608161536 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.5792049169540405, loss=2.2353873252868652
I0129 14:05:24.437718 140004616554240 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.5354435443878174, loss=2.290182590484619
I0129 14:05:57.845242 140004608161536 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3440227508544922, loss=2.10168719291687
I0129 14:06:31.254145 140004616554240 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.433112382888794, loss=2.1751701831817627
I0129 14:07:04.668665 140004608161536 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.3596842288970947, loss=2.247250556945801
I0129 14:07:38.089060 140004616554240 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3645468950271606, loss=2.241370916366577
I0129 14:08:11.499395 140004608161536 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.4411200284957886, loss=2.123079776763916
I0129 14:08:19.001663 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:08:25.479232 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:08:33.875159 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:08:36.593398 140169137129280 submission_runner.py:408] Time since start: 22759.64s, 	Step: 65424, 	{'train/accuracy': 0.30799585580825806, 'train/loss': 3.498842716217041, 'validation/accuracy': 0.28487998247146606, 'validation/loss': 3.6848952770233154, 'validation/num_examples': 50000, 'test/accuracy': 0.21580001711845398, 'test/loss': 4.417821884155273, 'test/num_examples': 10000, 'score': 21967.712792396545, 'total_duration': 22759.64086675644, 'accumulated_submission_time': 21967.712792396545, 'accumulated_eval_time': 788.2158498764038, 'accumulated_logging_time': 1.5760498046875}
I0129 14:08:36.624181 140005305468672 logging_writer.py:48] [65424] accumulated_eval_time=788.215850, accumulated_logging_time=1.576050, accumulated_submission_time=21967.712792, global_step=65424, preemption_count=0, score=21967.712792, test/accuracy=0.215800, test/loss=4.417822, test/num_examples=10000, total_duration=22759.640867, train/accuracy=0.307996, train/loss=3.498843, validation/accuracy=0.284880, validation/loss=3.684895, validation/num_examples=50000
I0129 14:09:02.321952 140005322254080 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3818491697311401, loss=2.220086097717285
I0129 14:09:35.781182 140005305468672 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3625452518463135, loss=2.15524959564209
I0129 14:10:09.165634 140005322254080 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.295883297920227, loss=2.071286916732788
I0129 14:10:42.564926 140005305468672 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.4422835111618042, loss=2.142444610595703
I0129 14:11:15.970169 140005322254080 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.4464612007141113, loss=2.20113468170166
I0129 14:11:49.379077 140005305468672 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.3362741470336914, loss=2.230642318725586
I0129 14:12:22.807401 140005322254080 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.376807451248169, loss=2.0954177379608154
I0129 14:12:56.203768 140005305468672 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.431706428527832, loss=2.1331899166107178
I0129 14:13:29.590538 140005322254080 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3134559392929077, loss=2.1780569553375244
I0129 14:14:02.968196 140005305468672 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.243607759475708, loss=2.259556770324707
I0129 14:14:36.362058 140005322254080 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.369014859199524, loss=2.191457509994507
I0129 14:15:09.780579 140005305468672 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.2984282970428467, loss=2.1496503353118896
I0129 14:15:43.283806 140005322254080 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.4746017456054688, loss=2.29189133644104
I0129 14:16:16.682724 140005305468672 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.3317879438400269, loss=2.2106213569641113
I0129 14:16:50.097027 140005322254080 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.4235584735870361, loss=2.244359016418457
I0129 14:17:06.614854 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:17:13.040781 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:17:21.568191 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:17:24.261694 140169137129280 submission_runner.py:408] Time since start: 23287.31s, 	Step: 66951, 	{'train/accuracy': 0.3006616532802582, 'train/loss': 3.679957628250122, 'validation/accuracy': 0.27539998292922974, 'validation/loss': 3.886249542236328, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.604314804077148, 'test/num_examples': 10000, 'score': 22477.6426115036, 'total_duration': 23287.309163093567, 'accumulated_submission_time': 22477.6426115036, 'accumulated_eval_time': 805.8626515865326, 'accumulated_logging_time': 1.61775541305542}
I0129 14:17:24.293232 140004616554240 logging_writer.py:48] [66951] accumulated_eval_time=805.862652, accumulated_logging_time=1.617755, accumulated_submission_time=22477.642612, global_step=66951, preemption_count=0, score=22477.642612, test/accuracy=0.202500, test/loss=4.604315, test/num_examples=10000, total_duration=23287.309163, train/accuracy=0.300662, train/loss=3.679958, validation/accuracy=0.275400, validation/loss=3.886250, validation/num_examples=50000
I0129 14:17:40.974572 140004624946944 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.3320180177688599, loss=2.1945900917053223
I0129 14:18:14.335992 140004616554240 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3949177265167236, loss=2.2006373405456543
I0129 14:18:47.714941 140004624946944 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.289104700088501, loss=2.2096147537231445
I0129 14:19:21.101740 140004616554240 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3481255769729614, loss=2.179612636566162
I0129 14:19:54.506464 140004624946944 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2407522201538086, loss=2.1691222190856934
I0129 14:20:27.930264 140004616554240 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3716243505477905, loss=2.1797537803649902
I0129 14:21:01.325675 140004624946944 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.4275716543197632, loss=2.0899171829223633
I0129 14:21:34.721266 140004616554240 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.4413621425628662, loss=2.1432957649230957
I0129 14:22:08.226201 140004624946944 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.399476170539856, loss=2.208341598510742
I0129 14:22:41.636023 140004616554240 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.3669064044952393, loss=2.0679876804351807
I0129 14:23:15.042105 140004624946944 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.403455376625061, loss=2.2645249366760254
I0129 14:23:48.440345 140004616554240 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.4049654006958008, loss=2.268155574798584
I0129 14:24:21.853466 140004624946944 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.389076828956604, loss=2.17126727104187
I0129 14:24:55.267601 140004616554240 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3876526355743408, loss=2.2026145458221436
I0129 14:25:28.674857 140004624946944 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.5248651504516602, loss=2.1873090267181396
I0129 14:25:54.537075 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:26:01.023517 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:26:09.921089 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:26:12.630871 140169137129280 submission_runner.py:408] Time since start: 23815.68s, 	Step: 68479, 	{'train/accuracy': 0.35837849974632263, 'train/loss': 3.008211612701416, 'validation/accuracy': 0.33601999282836914, 'validation/loss': 3.1703059673309326, 'validation/num_examples': 50000, 'test/accuracy': 0.2540000081062317, 'test/loss': 3.8654260635375977, 'test/num_examples': 10000, 'score': 22987.82523560524, 'total_duration': 23815.678339004517, 'accumulated_submission_time': 22987.82523560524, 'accumulated_eval_time': 823.9564123153687, 'accumulated_logging_time': 1.6605603694915771}
I0129 14:26:12.664793 140004624946944 logging_writer.py:48] [68479] accumulated_eval_time=823.956412, accumulated_logging_time=1.660560, accumulated_submission_time=22987.825236, global_step=68479, preemption_count=0, score=22987.825236, test/accuracy=0.254000, test/loss=3.865426, test/num_examples=10000, total_duration=23815.678339, train/accuracy=0.358378, train/loss=3.008212, validation/accuracy=0.336020, validation/loss=3.170306, validation/num_examples=50000
I0129 14:26:20.019228 140005322254080 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.481024980545044, loss=2.267364978790283
I0129 14:26:53.382184 140004624946944 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.4532907009124756, loss=2.084697723388672
I0129 14:27:26.774874 140005322254080 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.3817187547683716, loss=2.1129891872406006
I0129 14:28:00.184643 140004624946944 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.436149001121521, loss=2.1305999755859375
I0129 14:28:33.702595 140005322254080 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.389436960220337, loss=2.2626545429229736
I0129 14:29:07.099736 140004624946944 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.5797487497329712, loss=2.204469680786133
I0129 14:29:40.510820 140005322254080 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.5484459400177002, loss=2.2407920360565186
I0129 14:30:13.906002 140004624946944 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4479166269302368, loss=2.157006025314331
I0129 14:30:47.297202 140005322254080 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.3321839570999146, loss=2.1293423175811768
I0129 14:31:20.695724 140004624946944 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.4983090162277222, loss=2.092648506164551
I0129 14:31:54.102998 140005322254080 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.4487155675888062, loss=2.1256890296936035
I0129 14:32:27.492486 140004624946944 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.5701028108596802, loss=2.081533670425415
I0129 14:33:00.919874 140005322254080 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.3465501070022583, loss=2.046593189239502
I0129 14:33:34.325477 140004624946944 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.3500094413757324, loss=2.1662237644195557
I0129 14:34:07.751760 140005322254080 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3934907913208008, loss=2.1778573989868164
I0129 14:34:41.240422 140004624946944 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3498979806900024, loss=2.1208419799804688
I0129 14:34:42.731688 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:34:49.059078 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:34:57.700518 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:35:00.386919 140169137129280 submission_runner.py:408] Time since start: 24343.43s, 	Step: 70006, 	{'train/accuracy': 0.31164300441741943, 'train/loss': 3.471470355987549, 'validation/accuracy': 0.2888000011444092, 'validation/loss': 3.6245996952056885, 'validation/num_examples': 50000, 'test/accuracy': 0.21480001509189606, 'test/loss': 4.404978275299072, 'test/num_examples': 10000, 'score': 23497.832139492035, 'total_duration': 24343.43438887596, 'accumulated_submission_time': 23497.832139492035, 'accumulated_eval_time': 841.6116020679474, 'accumulated_logging_time': 1.704545497894287}
I0129 14:35:00.418017 140004624946944 logging_writer.py:48] [70006] accumulated_eval_time=841.611602, accumulated_logging_time=1.704545, accumulated_submission_time=23497.832139, global_step=70006, preemption_count=0, score=23497.832139, test/accuracy=0.214800, test/loss=4.404978, test/num_examples=10000, total_duration=24343.434389, train/accuracy=0.311643, train/loss=3.471470, validation/accuracy=0.288800, validation/loss=3.624600, validation/num_examples=50000
I0129 14:35:32.100701 140005288683264 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.4097864627838135, loss=2.26400089263916
I0129 14:36:05.481749 140004624946944 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.409097671508789, loss=2.1223809719085693
I0129 14:36:38.881705 140005288683264 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.5546752214431763, loss=2.0695769786834717
I0129 14:37:12.289970 140004624946944 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.6327531337738037, loss=2.0705292224884033
I0129 14:37:45.685024 140005288683264 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.448899745941162, loss=2.2360165119171143
I0129 14:38:19.099211 140004624946944 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.4411066770553589, loss=2.193268299102783
I0129 14:38:52.520061 140005288683264 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3688122034072876, loss=2.2199084758758545
I0129 14:39:25.914402 140004624946944 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3901925086975098, loss=2.16324520111084
I0129 14:39:59.336844 140005288683264 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4503449201583862, loss=2.1598570346832275
I0129 14:40:32.746135 140004624946944 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3870147466659546, loss=2.0859789848327637
I0129 14:41:06.160919 140005288683264 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3856173753738403, loss=2.1578176021575928
I0129 14:41:39.690404 140004624946944 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.439903736114502, loss=2.335451126098633
I0129 14:42:13.087899 140005288683264 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.5199525356292725, loss=2.1748204231262207
I0129 14:42:46.511335 140004624946944 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3703992366790771, loss=2.139756679534912
I0129 14:43:19.908174 140005288683264 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3273651599884033, loss=2.04900860786438
I0129 14:43:30.403184 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:43:36.753501 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:43:45.506393 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:43:48.193686 140169137129280 submission_runner.py:408] Time since start: 24871.24s, 	Step: 71533, 	{'train/accuracy': 0.21960698068141937, 'train/loss': 4.325572967529297, 'validation/accuracy': 0.20805999636650085, 'validation/loss': 4.440552234649658, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 5.040404319763184, 'test/num_examples': 10000, 'score': 24007.757329940796, 'total_duration': 24871.24115753174, 'accumulated_submission_time': 24007.757329940796, 'accumulated_eval_time': 859.4020702838898, 'accumulated_logging_time': 1.7458949089050293}
I0129 14:43:48.223825 140004624946944 logging_writer.py:48] [71533] accumulated_eval_time=859.402070, accumulated_logging_time=1.745895, accumulated_submission_time=24007.757330, global_step=71533, preemption_count=0, score=24007.757330, test/accuracy=0.163400, test/loss=5.040404, test/num_examples=10000, total_duration=24871.241158, train/accuracy=0.219607, train/loss=4.325573, validation/accuracy=0.208060, validation/loss=4.440552, validation/num_examples=50000
I0129 14:44:10.906483 140005313861376 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3547101020812988, loss=2.128800868988037
I0129 14:44:44.255404 140004624946944 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.386850357055664, loss=2.230973482131958
I0129 14:45:17.648007 140005313861376 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.821959376335144, loss=2.2213077545166016
I0129 14:45:51.054305 140004624946944 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3528926372528076, loss=2.1917905807495117
I0129 14:46:24.456035 140005313861376 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3327341079711914, loss=2.0188968181610107
I0129 14:46:57.855576 140004624946944 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.503113865852356, loss=2.228773832321167
I0129 14:47:31.251940 140005313861376 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.4963716268539429, loss=2.163611888885498
I0129 14:48:04.719714 140004624946944 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3549644947052002, loss=2.184068441390991
I0129 14:48:38.123003 140005313861376 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.467721700668335, loss=2.140793561935425
I0129 14:49:11.516731 140004624946944 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.4072214365005493, loss=2.122708797454834
I0129 14:49:44.929324 140005313861376 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.5572359561920166, loss=2.2531509399414062
I0129 14:50:18.338295 140004624946944 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.5302304029464722, loss=2.2720940113067627
I0129 14:50:51.742352 140005313861376 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.481872320175171, loss=2.0739905834198
I0129 14:51:25.148005 140004624946944 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.4530413150787354, loss=2.194955587387085
I0129 14:51:58.554715 140005313861376 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3208211660385132, loss=2.0983359813690186
I0129 14:52:18.414202 140169137129280 spec.py:321] Evaluating on the training split.
I0129 14:52:24.791522 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 14:52:33.473917 140169137129280 spec.py:349] Evaluating on the test split.
I0129 14:52:36.254676 140169137129280 submission_runner.py:408] Time since start: 25399.30s, 	Step: 73061, 	{'train/accuracy': 0.36025190353393555, 'train/loss': 3.1127405166625977, 'validation/accuracy': 0.33861997723579407, 'validation/loss': 3.271209239959717, 'validation/num_examples': 50000, 'test/accuracy': 0.25390002131462097, 'test/loss': 4.007626056671143, 'test/num_examples': 10000, 'score': 24517.887838363647, 'total_duration': 25399.302145957947, 'accumulated_submission_time': 24517.887838363647, 'accumulated_eval_time': 877.2425088882446, 'accumulated_logging_time': 1.785841703414917}
I0129 14:52:36.287162 140005288683264 logging_writer.py:48] [73061] accumulated_eval_time=877.242509, accumulated_logging_time=1.785842, accumulated_submission_time=24517.887838, global_step=73061, preemption_count=0, score=24517.887838, test/accuracy=0.253900, test/loss=4.007626, test/num_examples=10000, total_duration=25399.302146, train/accuracy=0.360252, train/loss=3.112741, validation/accuracy=0.338620, validation/loss=3.271209, validation/num_examples=50000
I0129 14:52:49.646663 140005297075968 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.601301908493042, loss=2.0519461631774902
I0129 14:53:23.011796 140005288683264 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.376839518547058, loss=2.1360621452331543
I0129 14:53:56.402781 140005297075968 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.476525068283081, loss=2.148505687713623
I0129 14:54:29.874044 140005288683264 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.3970855474472046, loss=1.9549579620361328
I0129 14:55:03.268651 140005297075968 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.3998039960861206, loss=2.0442264080047607
I0129 14:55:36.676701 140005288683264 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.3836305141448975, loss=2.0383520126342773
I0129 14:56:10.065774 140005297075968 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.7707993984222412, loss=2.3118553161621094
I0129 14:56:43.472980 140005288683264 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.41776442527771, loss=2.071685314178467
I0129 14:57:16.866108 140005297075968 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4848514795303345, loss=2.1555416584014893
I0129 14:57:50.270257 140005288683264 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.3671436309814453, loss=2.036447286605835
I0129 14:58:23.668853 140005297075968 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.4114981889724731, loss=2.0691304206848145
I0129 14:58:57.074098 140005288683264 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3798447847366333, loss=2.112492084503174
I0129 14:59:30.480889 140005297075968 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4357837438583374, loss=2.1472887992858887
I0129 15:00:03.897720 140005288683264 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.3123717308044434, loss=2.0225956439971924
I0129 15:00:37.404480 140005297075968 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.5879769325256348, loss=2.1239254474639893
I0129 15:01:06.279597 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:01:12.670926 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:01:21.279574 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:01:23.970057 140169137129280 submission_runner.py:408] Time since start: 25927.02s, 	Step: 74588, 	{'train/accuracy': 0.28748804330825806, 'train/loss': 3.622225284576416, 'validation/accuracy': 0.262719988822937, 'validation/loss': 3.8419241905212402, 'validation/num_examples': 50000, 'test/accuracy': 0.1949000060558319, 'test/loss': 4.602164268493652, 'test/num_examples': 10000, 'score': 25027.81852698326, 'total_duration': 25927.017527341843, 'accumulated_submission_time': 25027.81852698326, 'accumulated_eval_time': 894.9329364299774, 'accumulated_logging_time': 1.829528570175171}
I0129 15:01:24.005525 140004624946944 logging_writer.py:48] [74588] accumulated_eval_time=894.932936, accumulated_logging_time=1.829529, accumulated_submission_time=25027.818527, global_step=74588, preemption_count=0, score=25027.818527, test/accuracy=0.194900, test/loss=4.602164, test/num_examples=10000, total_duration=25927.017527, train/accuracy=0.287488, train/loss=3.622225, validation/accuracy=0.262720, validation/loss=3.841924, validation/num_examples=50000
I0129 15:01:28.357128 140005313861376 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.9724352359771729, loss=2.206192970275879
I0129 15:02:01.701310 140004624946944 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3685431480407715, loss=2.12406063079834
I0129 15:02:35.080898 140005313861376 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.4176493883132935, loss=2.1267879009246826
I0129 15:03:08.464974 140004624946944 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.2483314275741577, loss=2.1440370082855225
I0129 15:03:41.865172 140005313861376 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.5215708017349243, loss=2.0877134799957275
I0129 15:04:15.276018 140004624946944 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.5164966583251953, loss=2.135416269302368
I0129 15:04:48.682456 140005313861376 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.479092001914978, loss=2.0464718341827393
I0129 15:05:22.086816 140004624946944 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4851347208023071, loss=2.1458895206451416
I0129 15:05:55.495856 140005313861376 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.478317141532898, loss=2.0283262729644775
I0129 15:06:28.899221 140004624946944 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4282881021499634, loss=2.1663970947265625
I0129 15:07:02.415061 140005313861376 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.5224403142929077, loss=2.1304731369018555
I0129 15:07:35.809914 140004624946944 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.437168836593628, loss=2.232025384902954
I0129 15:08:09.205263 140005313861376 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.4506800174713135, loss=2.075371265411377
I0129 15:08:42.606671 140004624946944 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.4099777936935425, loss=2.029283285140991
I0129 15:09:16.010362 140005313861376 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3455942869186401, loss=2.012777328491211
I0129 15:09:49.404525 140004624946944 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.5579869747161865, loss=2.206630229949951
I0129 15:09:54.230629 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:10:00.632932 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:10:09.149662 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:10:11.882562 140169137129280 submission_runner.py:408] Time since start: 26454.93s, 	Step: 76116, 	{'train/accuracy': 0.38954877853393555, 'train/loss': 2.8953745365142822, 'validation/accuracy': 0.36695998907089233, 'validation/loss': 3.0703601837158203, 'validation/num_examples': 50000, 'test/accuracy': 0.2672000229358673, 'test/loss': 3.9029479026794434, 'test/num_examples': 10000, 'score': 25537.982943058014, 'total_duration': 26454.930029153824, 'accumulated_submission_time': 25537.982943058014, 'accumulated_eval_time': 912.58482670784, 'accumulated_logging_time': 1.876232624053955}
I0129 15:10:11.916233 140005305468672 logging_writer.py:48] [76116] accumulated_eval_time=912.584827, accumulated_logging_time=1.876233, accumulated_submission_time=25537.982943, global_step=76116, preemption_count=0, score=25537.982943, test/accuracy=0.267200, test/loss=3.902948, test/num_examples=10000, total_duration=26454.930029, train/accuracy=0.389549, train/loss=2.895375, validation/accuracy=0.366960, validation/loss=3.070360, validation/num_examples=50000
I0129 15:10:40.274631 140005330646784 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.6388862133026123, loss=2.160939931869507
I0129 15:11:13.635882 140005305468672 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.4149318933486938, loss=2.121553897857666
I0129 15:11:47.023068 140005330646784 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.5746265649795532, loss=2.038376808166504
I0129 15:12:20.426847 140005305468672 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4134011268615723, loss=2.267988681793213
I0129 15:12:53.829071 140005330646784 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.6417534351348877, loss=2.2181577682495117
I0129 15:13:27.310619 140005305468672 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4937198162078857, loss=2.122279167175293
I0129 15:14:00.714069 140005330646784 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.5286238193511963, loss=2.023665428161621
I0129 15:14:34.125144 140005305468672 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.4273688793182373, loss=2.1883909702301025
I0129 15:15:07.542884 140005330646784 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.3931653499603271, loss=2.1200132369995117
I0129 15:15:40.942849 140005305468672 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.7613731622695923, loss=2.0408105850219727
I0129 15:16:14.358807 140005330646784 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.5118823051452637, loss=2.1339967250823975
I0129 15:16:47.750869 140005305468672 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4640685319900513, loss=2.1915409564971924
I0129 15:17:21.156861 140005330646784 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4817638397216797, loss=2.090533971786499
I0129 15:17:54.572151 140005305468672 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.5532562732696533, loss=2.202690839767456
I0129 15:18:27.979092 140005330646784 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.40463387966156, loss=2.0431089401245117
I0129 15:18:42.153555 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:18:48.542179 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:18:57.149261 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:18:59.845115 140169137129280 submission_runner.py:408] Time since start: 26982.89s, 	Step: 77644, 	{'train/accuracy': 0.4274752736091614, 'train/loss': 2.7035293579101562, 'validation/accuracy': 0.3989799916744232, 'validation/loss': 2.8653476238250732, 'validation/num_examples': 50000, 'test/accuracy': 0.30330002307891846, 'test/loss': 3.6137118339538574, 'test/num_examples': 10000, 'score': 26048.157845020294, 'total_duration': 26982.892572402954, 'accumulated_submission_time': 26048.157845020294, 'accumulated_eval_time': 930.2763559818268, 'accumulated_logging_time': 1.9222619533538818}
I0129 15:18:59.878438 140004624946944 logging_writer.py:48] [77644] accumulated_eval_time=930.276356, accumulated_logging_time=1.922262, accumulated_submission_time=26048.157845, global_step=77644, preemption_count=0, score=26048.157845, test/accuracy=0.303300, test/loss=3.613712, test/num_examples=10000, total_duration=26982.892572, train/accuracy=0.427475, train/loss=2.703529, validation/accuracy=0.398980, validation/loss=2.865348, validation/num_examples=50000
I0129 15:19:18.907543 140005288683264 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.4802570343017578, loss=2.1378753185272217
I0129 15:19:52.360552 140004624946944 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.4737712144851685, loss=2.1250991821289062
I0129 15:20:25.753087 140005288683264 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.4477638006210327, loss=2.1227171421051025
I0129 15:20:59.164628 140004624946944 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.5573538541793823, loss=2.0893216133117676
I0129 15:21:32.547743 140005288683264 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.683357834815979, loss=2.108036756515503
I0129 15:22:05.949408 140004624946944 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.5020742416381836, loss=1.9282443523406982
I0129 15:22:39.352198 140005288683264 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.5109634399414062, loss=2.2042341232299805
I0129 15:23:12.757299 140004624946944 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.552391767501831, loss=2.097654342651367
I0129 15:23:46.170419 140005288683264 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4709367752075195, loss=2.1214137077331543
I0129 15:24:19.564644 140004624946944 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.579133152961731, loss=2.042912006378174
I0129 15:24:52.962490 140005288683264 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.552534818649292, loss=2.220421552658081
I0129 15:25:26.355369 140004624946944 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.379989504814148, loss=2.1732804775238037
I0129 15:25:59.873485 140005288683264 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.473206877708435, loss=2.0615828037261963
I0129 15:26:33.288540 140004624946944 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.4269803762435913, loss=2.201232433319092
I0129 15:27:06.692642 140005288683264 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.5569473505020142, loss=2.0475990772247314
I0129 15:27:29.883944 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:27:36.318228 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:27:45.177269 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:27:47.866474 140169137129280 submission_runner.py:408] Time since start: 27510.91s, 	Step: 79171, 	{'train/accuracy': 0.44569116830825806, 'train/loss': 2.4578397274017334, 'validation/accuracy': 0.41425999999046326, 'validation/loss': 2.646533966064453, 'validation/num_examples': 50000, 'test/accuracy': 0.31520000100135803, 'test/loss': 3.3972880840301514, 'test/num_examples': 10000, 'score': 26558.103238105774, 'total_duration': 27510.913942098618, 'accumulated_submission_time': 26558.103238105774, 'accumulated_eval_time': 948.2588489055634, 'accumulated_logging_time': 1.9660618305206299}
I0129 15:27:47.898523 140004616554240 logging_writer.py:48] [79171] accumulated_eval_time=948.258849, accumulated_logging_time=1.966062, accumulated_submission_time=26558.103238, global_step=79171, preemption_count=0, score=26558.103238, test/accuracy=0.315200, test/loss=3.397288, test/num_examples=10000, total_duration=27510.913942, train/accuracy=0.445691, train/loss=2.457840, validation/accuracy=0.414260, validation/loss=2.646534, validation/num_examples=50000
I0129 15:27:57.924528 140005313861376 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.4233286380767822, loss=2.15602970123291
I0129 15:28:31.282199 140004616554240 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5304718017578125, loss=2.1396923065185547
I0129 15:29:04.666244 140005313861376 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.7480223178863525, loss=2.262943744659424
I0129 15:29:38.060013 140004616554240 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.6812881231307983, loss=2.139216661453247
I0129 15:30:11.461069 140005313861376 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.6001474857330322, loss=2.0951688289642334
I0129 15:30:44.865482 140004616554240 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4542899131774902, loss=1.9401767253875732
I0129 15:31:18.270466 140005313861376 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.4393033981323242, loss=2.143576145172119
I0129 15:31:51.678710 140004616554240 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4857261180877686, loss=2.055828809738159
I0129 15:32:25.171208 140005313861376 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5004515647888184, loss=2.1292226314544678
I0129 15:32:58.586981 140004616554240 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.435468316078186, loss=2.146003246307373
I0129 15:33:31.989495 140005313861376 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.5337623357772827, loss=2.171193838119507
I0129 15:34:05.395726 140004616554240 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3872137069702148, loss=2.0581064224243164
I0129 15:34:38.794391 140005313861376 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.5337631702423096, loss=2.132211208343506
I0129 15:35:12.200621 140004616554240 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.6683521270751953, loss=2.189542770385742
I0129 15:35:45.610136 140005313861376 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.4471932649612427, loss=2.0664937496185303
I0129 15:36:18.145923 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:36:24.617043 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:36:33.445901 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:36:36.045971 140169137129280 submission_runner.py:408] Time since start: 28039.09s, 	Step: 80699, 	{'train/accuracy': 0.4790935814380646, 'train/loss': 2.3329217433929443, 'validation/accuracy': 0.4555400013923645, 'validation/loss': 2.4908533096313477, 'validation/num_examples': 50000, 'test/accuracy': 0.3410000205039978, 'test/loss': 3.3224074840545654, 'test/num_examples': 10000, 'score': 27068.290987730026, 'total_duration': 28039.09343481064, 'accumulated_submission_time': 27068.290987730026, 'accumulated_eval_time': 966.1588563919067, 'accumulated_logging_time': 2.0083022117614746}
I0129 15:36:36.077786 140004616554240 logging_writer.py:48] [80699] accumulated_eval_time=966.158856, accumulated_logging_time=2.008302, accumulated_submission_time=27068.290988, global_step=80699, preemption_count=0, score=27068.290988, test/accuracy=0.341000, test/loss=3.322407, test/num_examples=10000, total_duration=28039.093435, train/accuracy=0.479094, train/loss=2.332922, validation/accuracy=0.455540, validation/loss=2.490853, validation/num_examples=50000
I0129 15:36:36.779323 140004624946944 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4565792083740234, loss=2.060478925704956
I0129 15:37:10.121502 140004616554240 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.8259989023208618, loss=2.2385754585266113
I0129 15:37:43.492238 140004624946944 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.7517815828323364, loss=2.1334025859832764
I0129 15:38:16.873295 140004616554240 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.653029203414917, loss=1.969052791595459
I0129 15:38:50.376964 140004624946944 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.5410910844802856, loss=2.1784777641296387
I0129 15:39:23.791794 140004616554240 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.7824465036392212, loss=2.074154853820801
I0129 15:39:57.186228 140004624946944 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.896742343902588, loss=2.1253082752227783
I0129 15:40:30.589168 140004616554240 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.6290034055709839, loss=2.028606414794922
I0129 15:41:03.992684 140004624946944 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.504004716873169, loss=2.1055757999420166
I0129 15:41:37.408064 140004616554240 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.7567092180252075, loss=2.0348000526428223
I0129 15:42:10.790019 140004624946944 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.518711805343628, loss=2.0080487728118896
I0129 15:42:44.181735 140004616554240 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.4948413372039795, loss=2.138122320175171
I0129 15:43:17.585453 140004624946944 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.6568424701690674, loss=2.0013785362243652
I0129 15:43:50.981007 140004616554240 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.4850380420684814, loss=2.052233934402466
I0129 15:44:24.392425 140004624946944 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.440495252609253, loss=2.0229101181030273
I0129 15:44:57.800203 140004616554240 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.5810850858688354, loss=2.0357701778411865
I0129 15:45:06.053383 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:45:12.466662 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:45:21.036256 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:45:23.739848 140169137129280 submission_runner.py:408] Time since start: 28566.79s, 	Step: 82226, 	{'train/accuracy': 0.30674025416374207, 'train/loss': 3.739010810852051, 'validation/accuracy': 0.2848399877548218, 'validation/loss': 3.9017951488494873, 'validation/num_examples': 50000, 'test/accuracy': 0.20160001516342163, 'test/loss': 4.77598237991333, 'test/num_examples': 10000, 'score': 27578.205619335175, 'total_duration': 28566.787320137024, 'accumulated_submission_time': 27578.205619335175, 'accumulated_eval_time': 983.8452835083008, 'accumulated_logging_time': 2.0514883995056152}
I0129 15:45:23.771252 140005305468672 logging_writer.py:48] [82226] accumulated_eval_time=983.845284, accumulated_logging_time=2.051488, accumulated_submission_time=27578.205619, global_step=82226, preemption_count=0, score=27578.205619, test/accuracy=0.201600, test/loss=4.775982, test/num_examples=10000, total_duration=28566.787320, train/accuracy=0.306740, train/loss=3.739011, validation/accuracy=0.284840, validation/loss=3.901795, validation/num_examples=50000
I0129 15:45:48.791366 140005313861376 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.7767704725265503, loss=2.162386178970337
I0129 15:46:22.156055 140005305468672 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.6328719854354858, loss=2.259996175765991
I0129 15:46:55.543629 140005313861376 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.6808245182037354, loss=2.0147972106933594
I0129 15:47:28.944948 140005305468672 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.5256184339523315, loss=2.2479546070098877
I0129 15:48:02.340493 140005313861376 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.4844774007797241, loss=2.148855447769165
I0129 15:48:35.736083 140005305468672 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.995628833770752, loss=2.1646742820739746
I0129 15:49:09.148788 140005313861376 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.5811073780059814, loss=2.006161689758301
I0129 15:49:42.553681 140005305468672 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.7883315086364746, loss=2.0823378562927246
I0129 15:50:15.957152 140005313861376 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.5013856887817383, loss=1.9667725563049316
I0129 15:50:49.344421 140005305468672 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.7120702266693115, loss=2.2127163410186768
I0129 15:51:22.750365 140005313861376 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.589477777481079, loss=2.0227339267730713
I0129 15:51:56.253616 140005305468672 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.4973351955413818, loss=2.129823684692383
I0129 15:52:29.659317 140005313861376 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.6431900262832642, loss=2.141160488128662
I0129 15:53:03.077130 140005305468672 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.575537085533142, loss=2.118800163269043
I0129 15:53:36.470306 140005313861376 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.5389913320541382, loss=2.044428825378418
I0129 15:53:53.989500 140169137129280 spec.py:321] Evaluating on the training split.
I0129 15:54:00.390990 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 15:54:09.360843 140169137129280 spec.py:349] Evaluating on the test split.
I0129 15:54:12.035532 140169137129280 submission_runner.py:408] Time since start: 29095.08s, 	Step: 83754, 	{'train/accuracy': 0.40670838952064514, 'train/loss': 2.7435925006866455, 'validation/accuracy': 0.37073999643325806, 'validation/loss': 3.0049564838409424, 'validation/num_examples': 50000, 'test/accuracy': 0.2864000201225281, 'test/loss': 3.7166709899902344, 'test/num_examples': 10000, 'score': 28088.363196611404, 'total_duration': 29095.082992315292, 'accumulated_submission_time': 28088.363196611404, 'accumulated_eval_time': 1001.8912694454193, 'accumulated_logging_time': 2.0932278633117676}
I0129 15:54:12.070299 140004616554240 logging_writer.py:48] [83754] accumulated_eval_time=1001.891269, accumulated_logging_time=2.093228, accumulated_submission_time=28088.363197, global_step=83754, preemption_count=0, score=28088.363197, test/accuracy=0.286400, test/loss=3.716671, test/num_examples=10000, total_duration=29095.082992, train/accuracy=0.406708, train/loss=2.743593, validation/accuracy=0.370740, validation/loss=3.004956, validation/num_examples=50000
I0129 15:54:27.765008 140004624946944 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.5491714477539062, loss=2.1398935317993164
I0129 15:55:01.117381 140004616554240 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.6560527086257935, loss=2.159886360168457
I0129 15:55:34.513102 140004624946944 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.642204761505127, loss=2.052145004272461
I0129 15:56:07.907169 140004616554240 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.3970104455947876, loss=2.0047478675842285
I0129 15:56:41.307437 140004624946944 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.5199778079986572, loss=2.151881456375122
I0129 15:57:14.691863 140004616554240 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.7788037061691284, loss=2.086376667022705
I0129 15:57:48.200248 140004624946944 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.602777361869812, loss=2.0778207778930664
I0129 15:58:21.592477 140004616554240 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.468692421913147, loss=2.0765559673309326
I0129 15:58:54.982509 140004624946944 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.5877848863601685, loss=2.1539812088012695
I0129 15:59:28.387949 140004616554240 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.405479073524475, loss=2.0598340034484863
I0129 16:00:01.792374 140004624946944 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.616837978363037, loss=2.188164234161377
I0129 16:00:35.204052 140004616554240 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.54886794090271, loss=2.151047468185425
I0129 16:01:08.608848 140004624946944 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.6610466241836548, loss=2.1127538681030273
I0129 16:01:42.005992 140004616554240 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.56227445602417, loss=2.057736873626709
I0129 16:02:15.395260 140004624946944 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.6481469869613647, loss=2.0381743907928467
I0129 16:02:42.259752 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:02:48.762306 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:02:57.567920 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:03:00.283617 140169137129280 submission_runner.py:408] Time since start: 29623.33s, 	Step: 85282, 	{'train/accuracy': 0.2871890962123871, 'train/loss': 3.671375036239624, 'validation/accuracy': 0.26642000675201416, 'validation/loss': 3.8343710899353027, 'validation/num_examples': 50000, 'test/accuracy': 0.18240000307559967, 'test/loss': 4.670032978057861, 'test/num_examples': 10000, 'score': 28598.490182876587, 'total_duration': 29623.33106446266, 'accumulated_submission_time': 28598.490182876587, 'accumulated_eval_time': 1019.915079832077, 'accumulated_logging_time': 2.1403162479400635}
I0129 16:03:00.329241 140005313861376 logging_writer.py:48] [85282] accumulated_eval_time=1019.915080, accumulated_logging_time=2.140316, accumulated_submission_time=28598.490183, global_step=85282, preemption_count=0, score=28598.490183, test/accuracy=0.182400, test/loss=4.670033, test/num_examples=10000, total_duration=29623.331064, train/accuracy=0.287189, train/loss=3.671375, validation/accuracy=0.266420, validation/loss=3.834371, validation/num_examples=50000
I0129 16:03:06.685319 140005322254080 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.4999821186065674, loss=2.0153279304504395
I0129 16:03:40.026478 140005313861376 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.5708272457122803, loss=2.108593225479126
I0129 16:04:13.493830 140005322254080 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.7636134624481201, loss=2.0828311443328857
I0129 16:04:46.895992 140005313861376 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.753912329673767, loss=1.9823856353759766
I0129 16:05:20.307238 140005322254080 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.874859094619751, loss=2.0901687145233154
I0129 16:05:53.712181 140005313861376 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.627246379852295, loss=2.0497334003448486
I0129 16:06:27.108494 140005322254080 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.58589768409729, loss=2.0354342460632324
I0129 16:07:00.499432 140005313861376 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.5150574445724487, loss=1.9889551401138306
I0129 16:07:33.899356 140005322254080 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.6657073497772217, loss=2.1165788173675537
I0129 16:08:07.306881 140005313861376 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.531456708908081, loss=2.0628480911254883
I0129 16:08:40.718594 140005322254080 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.974377989768982, loss=2.169032096862793
I0129 16:09:14.106677 140005313861376 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.6644524335861206, loss=2.1104769706726074
I0129 16:09:47.507528 140005322254080 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.6330313682556152, loss=2.210564613342285
I0129 16:10:20.927359 140005313861376 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.6375389099121094, loss=2.0262558460235596
I0129 16:10:54.426955 140005322254080 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.689814567565918, loss=2.063106060028076
I0129 16:11:27.844997 140005313861376 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.5979019403457642, loss=2.11104679107666
I0129 16:11:30.340134 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:11:36.776744 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:11:45.610183 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:11:48.249431 140169137129280 submission_runner.py:408] Time since start: 30151.30s, 	Step: 86809, 	{'train/accuracy': 0.3782086968421936, 'train/loss': 3.034822463989258, 'validation/accuracy': 0.35117998719215393, 'validation/loss': 3.2767865657806396, 'validation/num_examples': 50000, 'test/accuracy': 0.27880001068115234, 'test/loss': 3.9043896198272705, 'test/num_examples': 10000, 'score': 29108.441494226456, 'total_duration': 30151.29687857628, 'accumulated_submission_time': 29108.441494226456, 'accumulated_eval_time': 1037.8243143558502, 'accumulated_logging_time': 2.196073055267334}
I0129 16:11:48.290344 140004616554240 logging_writer.py:48] [86809] accumulated_eval_time=1037.824314, accumulated_logging_time=2.196073, accumulated_submission_time=29108.441494, global_step=86809, preemption_count=0, score=29108.441494, test/accuracy=0.278800, test/loss=3.904390, test/num_examples=10000, total_duration=30151.296879, train/accuracy=0.378209, train/loss=3.034822, validation/accuracy=0.351180, validation/loss=3.276787, validation/num_examples=50000
I0129 16:12:18.972546 140004624946944 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.6146427392959595, loss=2.007189989089966
I0129 16:12:52.340477 140004616554240 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.621726632118225, loss=2.1207189559936523
I0129 16:13:25.741887 140004624946944 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5077791213989258, loss=2.015449047088623
I0129 16:13:59.150538 140004616554240 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.7755004167556763, loss=2.0805277824401855
I0129 16:14:32.541622 140004624946944 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.9188649654388428, loss=2.1378653049468994
I0129 16:15:05.944412 140004616554240 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.690525770187378, loss=2.0470871925354004
I0129 16:15:39.346208 140004624946944 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.610244870185852, loss=2.058825731277466
I0129 16:16:12.750585 140004616554240 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.8249059915542603, loss=2.134249448776245
I0129 16:16:46.147102 140004624946944 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.542862057685852, loss=2.000007390975952
I0129 16:17:19.647271 140004616554240 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.5417964458465576, loss=1.9692692756652832
I0129 16:17:53.036939 140004624946944 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.5804579257965088, loss=2.0971016883850098
I0129 16:18:26.454717 140004616554240 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.7036243677139282, loss=2.023838996887207
I0129 16:18:59.855917 140004624946944 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.7830884456634521, loss=1.9945714473724365
I0129 16:19:33.273865 140004616554240 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.7519272565841675, loss=2.0989952087402344
I0129 16:20:06.675436 140004624946944 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.700634479522705, loss=2.058946132659912
I0129 16:20:18.532012 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:20:24.864371 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:20:33.681751 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:20:36.348541 140169137129280 submission_runner.py:408] Time since start: 30679.40s, 	Step: 88337, 	{'train/accuracy': 0.3022560477256775, 'train/loss': 3.6190991401672363, 'validation/accuracy': 0.2789599895477295, 'validation/loss': 3.844160556793213, 'validation/num_examples': 50000, 'test/accuracy': 0.21980001032352448, 'test/loss': 4.452763080596924, 'test/num_examples': 10000, 'score': 29618.62277984619, 'total_duration': 30679.39599967003, 'accumulated_submission_time': 29618.62277984619, 'accumulated_eval_time': 1055.6407935619354, 'accumulated_logging_time': 2.2476541996002197}
I0129 16:20:36.383784 140004624946944 logging_writer.py:48] [88337] accumulated_eval_time=1055.640794, accumulated_logging_time=2.247654, accumulated_submission_time=29618.622780, global_step=88337, preemption_count=0, score=29618.622780, test/accuracy=0.219800, test/loss=4.452763, test/num_examples=10000, total_duration=30679.396000, train/accuracy=0.302256, train/loss=3.619099, validation/accuracy=0.278960, validation/loss=3.844161, validation/num_examples=50000
I0129 16:20:57.745600 140005305468672 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.5318900346755981, loss=2.05859637260437
I0129 16:21:31.104257 140004624946944 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.6239923238754272, loss=2.0292088985443115
I0129 16:22:04.476287 140005305468672 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.6972947120666504, loss=2.0368669033050537
I0129 16:22:37.874310 140004624946944 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.8344906568527222, loss=2.046624183654785
I0129 16:23:11.274960 140005305468672 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.637323021888733, loss=1.8555521965026855
I0129 16:23:44.751704 140004624946944 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.5772123336791992, loss=1.9833462238311768
I0129 16:24:18.162621 140005305468672 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.6213982105255127, loss=2.0476701259613037
I0129 16:24:51.562432 140004624946944 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.591898798942566, loss=2.002187490463257
I0129 16:25:24.969212 140005305468672 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.4955755472183228, loss=2.0533711910247803
I0129 16:25:58.362543 140004624946944 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.6430697441101074, loss=2.011660575866699
I0129 16:26:31.780596 140005305468672 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.694446086883545, loss=2.012542724609375
I0129 16:27:05.170384 140004624946944 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5713884830474854, loss=1.8984267711639404
I0129 16:27:38.580937 140005305468672 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.6112926006317139, loss=2.1063730716705322
I0129 16:28:12.412672 140004624946944 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.634934663772583, loss=2.116530179977417
I0129 16:28:45.794031 140005305468672 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.7298659086227417, loss=2.0608716011047363
I0129 16:29:06.647167 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:29:13.096069 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:29:21.918915 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:29:24.601183 140169137129280 submission_runner.py:408] Time since start: 31207.65s, 	Step: 89864, 	{'train/accuracy': 0.5006975531578064, 'train/loss': 2.200382947921753, 'validation/accuracy': 0.47029998898506165, 'validation/loss': 2.4046542644500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.230496644973755, 'test/num_examples': 10000, 'score': 30128.824651002884, 'total_duration': 31207.648649930954, 'accumulated_submission_time': 30128.824651002884, 'accumulated_eval_time': 1073.5947699546814, 'accumulated_logging_time': 2.2941675186157227}
I0129 16:29:24.641230 140005288683264 logging_writer.py:48] [89864] accumulated_eval_time=1073.594770, accumulated_logging_time=2.294168, accumulated_submission_time=30128.824651, global_step=89864, preemption_count=0, score=30128.824651, test/accuracy=0.361400, test/loss=3.230497, test/num_examples=10000, total_duration=31207.648650, train/accuracy=0.500698, train/loss=2.200383, validation/accuracy=0.470300, validation/loss=2.404654, validation/num_examples=50000
I0129 16:29:36.986707 140005297075968 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.8274693489074707, loss=2.0483665466308594
I0129 16:30:10.409305 140005288683264 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.745816946029663, loss=2.0000438690185547
I0129 16:30:43.796256 140005297075968 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6956461668014526, loss=2.068830966949463
I0129 16:31:17.197042 140005288683264 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.509092092514038, loss=1.9952876567840576
I0129 16:31:50.609017 140005297075968 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.5053318738937378, loss=2.131664752960205
I0129 16:32:24.015349 140005288683264 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5491807460784912, loss=2.1133053302764893
I0129 16:32:57.417688 140005297075968 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.6739166975021362, loss=2.08176851272583
I0129 16:33:30.833852 140005288683264 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.8084793090820312, loss=2.0296144485473633
I0129 16:34:04.239394 140005297075968 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.664646863937378, loss=1.861386775970459
I0129 16:34:37.645242 140005288683264 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.5601592063903809, loss=2.0364108085632324
I0129 16:35:11.063614 140005297075968 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.6752135753631592, loss=1.9768190383911133
I0129 16:35:44.446396 140005288683264 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.733044147491455, loss=2.0610544681549072
I0129 16:36:17.928205 140005297075968 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.615964412689209, loss=2.0911240577697754
I0129 16:36:51.322104 140005288683264 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.7258647680282593, loss=2.119231939315796
I0129 16:37:24.732558 140005297075968 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.615529179573059, loss=2.097555637359619
I0129 16:37:54.609580 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:38:01.820627 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:38:10.718289 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:38:13.391691 140169137129280 submission_runner.py:408] Time since start: 31736.44s, 	Step: 91391, 	{'train/accuracy': 0.3496890962123871, 'train/loss': 3.236147165298462, 'validation/accuracy': 0.3291199803352356, 'validation/loss': 3.4062445163726807, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 4.2228899002075195, 'test/num_examples': 10000, 'score': 30638.732491970062, 'total_duration': 31736.439160823822, 'accumulated_submission_time': 30638.732491970062, 'accumulated_eval_time': 1092.3768472671509, 'accumulated_logging_time': 2.3449578285217285}
I0129 16:38:13.428461 140004608161536 logging_writer.py:48] [91391] accumulated_eval_time=1092.376847, accumulated_logging_time=2.344958, accumulated_submission_time=30638.732492, global_step=91391, preemption_count=0, score=30638.732492, test/accuracy=0.234600, test/loss=4.222890, test/num_examples=10000, total_duration=31736.439161, train/accuracy=0.349689, train/loss=3.236147, validation/accuracy=0.329120, validation/loss=3.406245, validation/num_examples=50000
I0129 16:38:16.768749 140004616554240 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.9060295820236206, loss=1.9052786827087402
I0129 16:38:50.105505 140004608161536 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.710099220275879, loss=2.1215097904205322
I0129 16:39:23.464544 140004616554240 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.7495442628860474, loss=2.1302521228790283
I0129 16:39:56.868774 140004608161536 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.6482211351394653, loss=2.0435891151428223
I0129 16:40:30.268608 140004616554240 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.6001967191696167, loss=2.064038038253784
I0129 16:41:03.677496 140004608161536 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.8606303930282593, loss=1.9962174892425537
I0129 16:41:37.063942 140004616554240 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.2878096103668213, loss=2.0205395221710205
I0129 16:42:10.487607 140004608161536 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.9118461608886719, loss=2.097776174545288
I0129 16:42:44.000511 140004616554240 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.121656656265259, loss=2.076662302017212
I0129 16:43:17.397950 140004608161536 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6763793230056763, loss=1.9696812629699707
I0129 16:43:50.788356 140004616554240 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.6412382125854492, loss=2.0458362102508545
I0129 16:44:24.178041 140004608161536 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.6687917709350586, loss=2.008464813232422
I0129 16:44:57.593694 140004616554240 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.7152451276779175, loss=1.988321304321289
I0129 16:45:30.989722 140004608161536 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.629189372062683, loss=2.073674440383911
I0129 16:46:04.385471 140004616554240 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.6586413383483887, loss=2.0707690715789795
I0129 16:46:37.774749 140004608161536 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.6835917234420776, loss=1.9638912677764893
I0129 16:46:43.607031 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:46:50.027678 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:46:58.829757 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:47:01.510865 140169137129280 submission_runner.py:408] Time since start: 32264.56s, 	Step: 92919, 	{'train/accuracy': 0.40118780732154846, 'train/loss': 2.894664764404297, 'validation/accuracy': 0.37257999181747437, 'validation/loss': 3.125652313232422, 'validation/num_examples': 50000, 'test/accuracy': 0.2875000238418579, 'test/loss': 3.9007530212402344, 'test/num_examples': 10000, 'score': 31148.847939491272, 'total_duration': 32264.55832004547, 'accumulated_submission_time': 31148.847939491272, 'accumulated_eval_time': 1110.28062915802, 'accumulated_logging_time': 2.3948986530303955}
I0129 16:47:01.545046 140005297075968 logging_writer.py:48] [92919] accumulated_eval_time=1110.280629, accumulated_logging_time=2.394899, accumulated_submission_time=31148.847939, global_step=92919, preemption_count=0, score=31148.847939, test/accuracy=0.287500, test/loss=3.900753, test/num_examples=10000, total_duration=32264.558320, train/accuracy=0.401188, train/loss=2.894665, validation/accuracy=0.372580, validation/loss=3.125652, validation/num_examples=50000
I0129 16:47:28.933969 140005313861376 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.7100439071655273, loss=2.0117409229278564
I0129 16:48:02.303662 140005297075968 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.564063310623169, loss=2.0281999111175537
I0129 16:48:35.693647 140005313861376 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4986414909362793, loss=1.900400996208191
I0129 16:49:09.307364 140005297075968 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7639225721359253, loss=2.0022497177124023
I0129 16:49:42.712541 140005313861376 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.6651363372802734, loss=1.8877336978912354
I0129 16:50:16.100314 140005297075968 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.7465084791183472, loss=2.025777578353882
I0129 16:50:49.500387 140005313861376 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.6870520114898682, loss=1.9971036911010742
I0129 16:51:22.891038 140005297075968 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.6504989862442017, loss=2.11689829826355
I0129 16:51:56.279854 140005313861376 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.7889134883880615, loss=2.0269558429718018
I0129 16:52:29.674462 140005297075968 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.839070200920105, loss=1.897444725036621
I0129 16:53:03.062766 140005313861376 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.763390302658081, loss=2.043674945831299
I0129 16:53:36.454473 140005297075968 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.8356225490570068, loss=2.04022216796875
I0129 16:54:09.855242 140005313861376 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.6366815567016602, loss=1.9675906896591187
I0129 16:54:43.263047 140005297075968 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.7309162616729736, loss=2.0725457668304443
I0129 16:55:16.652749 140005313861376 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.721277117729187, loss=2.1004021167755127
I0129 16:55:31.623305 140169137129280 spec.py:321] Evaluating on the training split.
I0129 16:55:37.974663 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 16:55:46.697886 140169137129280 spec.py:349] Evaluating on the test split.
I0129 16:55:49.360201 140169137129280 submission_runner.py:408] Time since start: 32792.41s, 	Step: 94446, 	{'train/accuracy': 0.46033960580825806, 'train/loss': 2.431842565536499, 'validation/accuracy': 0.4266199767589569, 'validation/loss': 2.6566004753112793, 'validation/num_examples': 50000, 'test/accuracy': 0.325300008058548, 'test/loss': 3.4505057334899902, 'test/num_examples': 10000, 'score': 31658.8655025959, 'total_duration': 32792.40767073631, 'accumulated_submission_time': 31658.8655025959, 'accumulated_eval_time': 1128.0174877643585, 'accumulated_logging_time': 2.43977689743042}
I0129 16:55:49.394739 140005288683264 logging_writer.py:48] [94446] accumulated_eval_time=1128.017488, accumulated_logging_time=2.439777, accumulated_submission_time=31658.865503, global_step=94446, preemption_count=0, score=31658.865503, test/accuracy=0.325300, test/loss=3.450506, test/num_examples=10000, total_duration=32792.407671, train/accuracy=0.460340, train/loss=2.431843, validation/accuracy=0.426620, validation/loss=2.656600, validation/num_examples=50000
I0129 16:56:07.753517 140005297075968 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.7627910375595093, loss=1.9857234954833984
I0129 16:56:41.098649 140005288683264 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.6726484298706055, loss=1.999265432357788
I0129 16:57:14.473995 140005297075968 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.679961085319519, loss=1.9799915552139282
I0129 16:57:47.856778 140005288683264 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.692064642906189, loss=1.9336750507354736
I0129 16:58:21.248298 140005297075968 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.1179635524749756, loss=2.157917022705078
I0129 16:58:54.650217 140005288683264 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.7856229543685913, loss=2.0965347290039062
I0129 16:59:28.058601 140005297075968 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.6615382432937622, loss=2.0291168689727783
I0129 17:00:01.465217 140005288683264 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.6608812808990479, loss=2.063669443130493
I0129 17:00:34.862584 140005297075968 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.9566166400909424, loss=2.001981258392334
I0129 17:01:08.267094 140005288683264 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.7652066946029663, loss=2.0680880546569824
I0129 17:01:41.654287 140005297075968 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.640820026397705, loss=2.0348923206329346
I0129 17:02:15.119196 140005288683264 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.6975480318069458, loss=2.069869041442871
I0129 17:02:48.517357 140005297075968 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.886543869972229, loss=2.028250217437744
I0129 17:03:21.925339 140005288683264 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.6550800800323486, loss=1.9271764755249023
I0129 17:03:55.323065 140005297075968 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.6391459703445435, loss=2.029538154602051
I0129 17:04:19.518646 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:04:26.091514 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:04:34.406929 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:04:37.116218 140169137129280 submission_runner.py:408] Time since start: 33320.16s, 	Step: 95974, 	{'train/accuracy': 0.20230786502361298, 'train/loss': 4.5234880447387695, 'validation/accuracy': 0.19153998792171478, 'validation/loss': 4.669929027557373, 'validation/num_examples': 50000, 'test/accuracy': 0.14240001142024994, 'test/loss': 5.283186912536621, 'test/num_examples': 10000, 'score': 32168.929438829422, 'total_duration': 33320.16366028786, 'accumulated_submission_time': 32168.929438829422, 'accumulated_eval_time': 1145.614995956421, 'accumulated_logging_time': 2.484170436859131}
I0129 17:04:37.154652 140005313861376 logging_writer.py:48] [95974] accumulated_eval_time=1145.614996, accumulated_logging_time=2.484170, accumulated_submission_time=32168.929439, global_step=95974, preemption_count=0, score=32168.929439, test/accuracy=0.142400, test/loss=5.283187, test/num_examples=10000, total_duration=33320.163660, train/accuracy=0.202308, train/loss=4.523488, validation/accuracy=0.191540, validation/loss=4.669929, validation/num_examples=50000
I0129 17:04:46.176720 140005322254080 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.9762409925460815, loss=2.0087194442749023
I0129 17:05:19.513039 140005313861376 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.616969108581543, loss=2.035672187805176
I0129 17:05:52.881592 140005322254080 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.8544803857803345, loss=2.13162899017334
I0129 17:06:26.264078 140005313861376 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.7434360980987549, loss=2.0677120685577393
I0129 17:06:59.658884 140005322254080 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.6878019571304321, loss=1.9307976961135864
I0129 17:07:33.065713 140005313861376 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.870178461074829, loss=2.0511906147003174
I0129 17:08:06.471669 140005322254080 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.7281197309494019, loss=2.0795459747314453
I0129 17:08:39.963154 140005313861376 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.735422134399414, loss=1.9325475692749023
I0129 17:09:13.360590 140005322254080 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.5956543684005737, loss=1.9153649806976318
I0129 17:09:46.767431 140005313861376 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.8412526845932007, loss=2.0490105152130127
I0129 17:10:20.156327 140005322254080 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.920168399810791, loss=2.0611813068389893
I0129 17:10:53.564895 140005313861376 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.7555344104766846, loss=2.0230607986450195
I0129 17:11:26.977392 140005322254080 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.04917049407959, loss=2.0715997219085693
I0129 17:12:00.385321 140005313861376 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6989009380340576, loss=2.060985565185547
I0129 17:12:33.801120 140005322254080 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.11493182182312, loss=1.8970201015472412
I0129 17:13:07.198636 140005313861376 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.8506510257720947, loss=1.9279358386993408
I0129 17:13:07.207088 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:13:13.649142 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:13:22.497729 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:13:25.212615 140169137129280 submission_runner.py:408] Time since start: 33848.26s, 	Step: 97501, 	{'train/accuracy': 0.39953362941741943, 'train/loss': 2.832772731781006, 'validation/accuracy': 0.37498000264167786, 'validation/loss': 3.0190296173095703, 'validation/num_examples': 50000, 'test/accuracy': 0.2884000241756439, 'test/loss': 3.7778568267822266, 'test/num_examples': 10000, 'score': 32678.92079949379, 'total_duration': 33848.26008415222, 'accumulated_submission_time': 32678.92079949379, 'accumulated_eval_time': 1163.6204626560211, 'accumulated_logging_time': 2.5338714122772217}
I0129 17:13:25.250339 140004624946944 logging_writer.py:48] [97501] accumulated_eval_time=1163.620463, accumulated_logging_time=2.533871, accumulated_submission_time=32678.920799, global_step=97501, preemption_count=0, score=32678.920799, test/accuracy=0.288400, test/loss=3.777857, test/num_examples=10000, total_duration=33848.260084, train/accuracy=0.399534, train/loss=2.832773, validation/accuracy=0.374980, validation/loss=3.019030, validation/num_examples=50000
I0129 17:13:58.598822 140005288683264 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.6092610359191895, loss=2.043161153793335
I0129 17:14:31.978829 140004624946944 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.8835340738296509, loss=2.017117738723755
I0129 17:15:05.442083 140005288683264 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.9016568660736084, loss=1.9841499328613281
I0129 17:15:38.840443 140004624946944 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.876139521598816, loss=1.9546515941619873
I0129 17:16:12.238508 140005288683264 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.8706040382385254, loss=1.9593570232391357
I0129 17:16:45.646288 140004624946944 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.8444377183914185, loss=1.9508888721466064
I0129 17:17:19.052596 140005288683264 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.9663679599761963, loss=1.958277940750122
I0129 17:17:52.470641 140004624946944 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.7959115505218506, loss=2.039520025253296
I0129 17:18:25.864393 140005288683264 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.773551344871521, loss=1.9428406953811646
I0129 17:18:59.260690 140004624946944 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.9121023416519165, loss=1.9670950174331665
I0129 17:19:32.683504 140005288683264 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.710219144821167, loss=1.9649300575256348
I0129 17:20:06.077840 140004624946944 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.97818922996521, loss=1.886108636856079
I0129 17:20:39.487695 140005288683264 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.8937631845474243, loss=2.0052099227905273
I0129 17:21:12.960439 140004624946944 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.9517425298690796, loss=1.9592976570129395
I0129 17:21:46.348510 140005288683264 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.7169296741485596, loss=1.9579417705535889
I0129 17:21:55.519782 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:22:01.921111 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:22:10.669922 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:22:13.402760 140169137129280 submission_runner.py:408] Time since start: 34376.45s, 	Step: 99029, 	{'train/accuracy': 0.4629504084587097, 'train/loss': 2.4320366382598877, 'validation/accuracy': 0.4339199960231781, 'validation/loss': 2.634920597076416, 'validation/num_examples': 50000, 'test/accuracy': 0.32440000772476196, 'test/loss': 3.5730552673339844, 'test/num_examples': 10000, 'score': 33189.129398584366, 'total_duration': 34376.450227975845, 'accumulated_submission_time': 33189.129398584366, 'accumulated_eval_time': 1181.5033974647522, 'accumulated_logging_time': 2.582381010055542}
I0129 17:22:13.440085 140004616554240 logging_writer.py:48] [99029] accumulated_eval_time=1181.503397, accumulated_logging_time=2.582381, accumulated_submission_time=33189.129399, global_step=99029, preemption_count=0, score=33189.129399, test/accuracy=0.324400, test/loss=3.573055, test/num_examples=10000, total_duration=34376.450228, train/accuracy=0.462950, train/loss=2.432037, validation/accuracy=0.433920, validation/loss=2.634921, validation/num_examples=50000
I0129 17:22:37.477512 140004624946944 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.8239394426345825, loss=2.0098211765289307
I0129 17:23:10.824394 140004616554240 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.6981465816497803, loss=2.010383129119873
I0129 17:23:44.203611 140004624946944 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.7361539602279663, loss=1.9555325508117676
I0129 17:24:17.613082 140004616554240 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.7699453830718994, loss=1.8983019590377808
I0129 17:24:51.003960 140004624946944 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.050732374191284, loss=2.1287331581115723
I0129 17:25:24.414685 140004616554240 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.9014363288879395, loss=2.0253231525421143
I0129 17:25:57.812278 140004624946944 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.9274256229400635, loss=2.064751386642456
I0129 17:26:31.202922 140004616554240 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.8893868923187256, loss=1.8392372131347656
I0129 17:27:04.596582 140004624946944 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.6807392835617065, loss=1.9477953910827637
I0129 17:27:38.093672 140004616554240 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.8086568117141724, loss=1.8980920314788818
I0129 17:28:11.482407 140004624946944 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.9718941450119019, loss=2.0044426918029785
I0129 17:28:44.883097 140004616554240 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.7648552656173706, loss=1.9774644374847412
I0129 17:29:18.301415 140004624946944 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.6239560842514038, loss=1.9810982942581177
I0129 17:29:51.704560 140004616554240 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.947217345237732, loss=1.9963963031768799
I0129 17:30:25.102415 140004624946944 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.8571999073028564, loss=1.9890204668045044
I0129 17:30:43.613143 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:30:49.976351 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:30:58.957303 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:31:01.699280 140169137129280 submission_runner.py:408] Time since start: 34904.75s, 	Step: 100557, 	{'train/accuracy': 0.41095343232154846, 'train/loss': 2.8825113773345947, 'validation/accuracy': 0.38245999813079834, 'validation/loss': 3.089705467224121, 'validation/num_examples': 50000, 'test/accuracy': 0.27900001406669617, 'test/loss': 4.0157599449157715, 'test/num_examples': 10000, 'score': 33699.24235534668, 'total_duration': 34904.74674367905, 'accumulated_submission_time': 33699.24235534668, 'accumulated_eval_time': 1199.5894927978516, 'accumulated_logging_time': 2.630051374435425}
I0129 17:31:01.739794 140004608161536 logging_writer.py:48] [100557] accumulated_eval_time=1199.589493, accumulated_logging_time=2.630051, accumulated_submission_time=33699.242355, global_step=100557, preemption_count=0, score=33699.242355, test/accuracy=0.279000, test/loss=4.015760, test/num_examples=10000, total_duration=34904.746744, train/accuracy=0.410953, train/loss=2.882511, validation/accuracy=0.382460, validation/loss=3.089705, validation/num_examples=50000
I0129 17:31:16.426683 140004616554240 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.8353859186172485, loss=1.8598066568374634
I0129 17:31:49.780241 140004608161536 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.7423596382141113, loss=1.902761697769165
I0129 17:32:23.174882 140004616554240 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.7145359516143799, loss=1.8568761348724365
I0129 17:32:56.573353 140004608161536 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.9534507989883423, loss=1.982661485671997
I0129 17:33:29.976774 140004616554240 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.9252904653549194, loss=1.945432424545288
I0129 17:34:03.472228 140004608161536 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.7664217948913574, loss=1.913764476776123
I0129 17:34:36.885973 140004616554240 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.6775473356246948, loss=1.8582007884979248
I0129 17:35:10.298674 140004608161536 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.8687392473220825, loss=1.9344863891601562
I0129 17:35:43.700347 140004616554240 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.7566096782684326, loss=1.937854290008545
I0129 17:36:17.094448 140004608161536 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.89578115940094, loss=2.02426815032959
I0129 17:36:50.501712 140004616554240 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.9334062337875366, loss=1.8849650621414185
I0129 17:37:23.898184 140004608161536 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.7542965412139893, loss=2.0354132652282715
I0129 17:37:57.299613 140004616554240 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.8551759719848633, loss=1.9774831533432007
I0129 17:38:30.702997 140004608161536 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.85753333568573, loss=2.0996177196502686
I0129 17:39:04.112071 140004616554240 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.08015775680542, loss=1.9641715288162231
I0129 17:39:31.977175 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:39:38.478935 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:39:46.895407 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:39:49.567545 140169137129280 submission_runner.py:408] Time since start: 35432.62s, 	Step: 102085, 	{'train/accuracy': 0.49404096603393555, 'train/loss': 2.256666421890259, 'validation/accuracy': 0.4540799856185913, 'validation/loss': 2.4757895469665527, 'validation/num_examples': 50000, 'test/accuracy': 0.34710001945495605, 'test/loss': 3.2543532848358154, 'test/num_examples': 10000, 'score': 34209.41844010353, 'total_duration': 35432.615013599396, 'accumulated_submission_time': 34209.41844010353, 'accumulated_eval_time': 1217.1798260211945, 'accumulated_logging_time': 2.681713342666626}
I0129 17:39:49.606723 140004624946944 logging_writer.py:48] [102085] accumulated_eval_time=1217.179826, accumulated_logging_time=2.681713, accumulated_submission_time=34209.418440, global_step=102085, preemption_count=0, score=34209.418440, test/accuracy=0.347100, test/loss=3.254353, test/num_examples=10000, total_duration=35432.615014, train/accuracy=0.494041, train/loss=2.256666, validation/accuracy=0.454080, validation/loss=2.475790, validation/num_examples=50000
I0129 17:39:54.969390 140005288683264 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.014997959136963, loss=2.065716505050659
I0129 17:40:28.423020 140004624946944 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.7904407978057861, loss=1.9724605083465576
I0129 17:41:01.793300 140005288683264 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.8683210611343384, loss=2.040719985961914
I0129 17:41:35.171679 140004624946944 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.8760350942611694, loss=1.844184160232544
I0129 17:42:08.567876 140005288683264 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.8371928930282593, loss=2.00406813621521
I0129 17:42:41.979443 140004624946944 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.9138957262039185, loss=1.9596893787384033
I0129 17:43:15.388853 140005288683264 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.7860867977142334, loss=1.9096956253051758
I0129 17:43:48.781505 140004624946944 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.084104061126709, loss=2.052339553833008
I0129 17:44:22.191313 140005288683264 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.9607834815979004, loss=2.004422187805176
I0129 17:44:55.608801 140004624946944 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.929274082183838, loss=2.0862131118774414
I0129 17:45:29.022470 140005288683264 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.79275381565094, loss=1.918804407119751
I0129 17:46:02.426124 140004624946944 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.8926109075546265, loss=1.9789152145385742
I0129 17:46:35.834671 140005288683264 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.798143982887268, loss=1.927803635597229
I0129 17:47:09.317781 140004624946944 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.9150124788284302, loss=1.9595180749893188
I0129 17:47:42.723562 140005288683264 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.03948974609375, loss=1.8968931436538696
I0129 17:48:16.125647 140004624946944 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.9098470211029053, loss=1.9391529560089111
I0129 17:48:19.617641 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:48:25.941915 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:48:34.794710 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:48:37.537724 140169137129280 submission_runner.py:408] Time since start: 35960.59s, 	Step: 103612, 	{'train/accuracy': 0.34476640820503235, 'train/loss': 3.2952005863189697, 'validation/accuracy': 0.3284199833869934, 'validation/loss': 3.4189095497131348, 'validation/num_examples': 50000, 'test/accuracy': 0.23850001394748688, 'test/loss': 4.211745738983154, 'test/num_examples': 10000, 'score': 34719.36843562126, 'total_duration': 35960.58519363403, 'accumulated_submission_time': 34719.36843562126, 'accumulated_eval_time': 1235.0998673439026, 'accumulated_logging_time': 2.732177972793579}
I0129 17:48:37.576422 140005297075968 logging_writer.py:48] [103612] accumulated_eval_time=1235.099867, accumulated_logging_time=2.732178, accumulated_submission_time=34719.368436, global_step=103612, preemption_count=0, score=34719.368436, test/accuracy=0.238500, test/loss=4.211746, test/num_examples=10000, total_duration=35960.585194, train/accuracy=0.344766, train/loss=3.295201, validation/accuracy=0.328420, validation/loss=3.418910, validation/num_examples=50000
I0129 17:49:07.256587 140005305468672 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.8539799451828003, loss=1.8523279428482056
I0129 17:49:40.623492 140005297075968 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.9466835260391235, loss=1.9163230657577515
I0129 17:50:14.026308 140005305468672 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.7623729705810547, loss=1.8932101726531982
I0129 17:50:47.434844 140005297075968 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.9658896923065186, loss=2.0276920795440674
I0129 17:51:20.812204 140005305468672 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.8308846950531006, loss=1.9491229057312012
I0129 17:51:54.196009 140005297075968 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.019402503967285, loss=1.8025561571121216
I0129 17:52:27.593705 140005305468672 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.9395736455917358, loss=2.0149776935577393
I0129 17:53:01.004856 140005297075968 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.8300325870513916, loss=1.9636895656585693
I0129 17:53:34.501403 140005305468672 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.674847960472107, loss=1.8950769901275635
I0129 17:54:07.888452 140005297075968 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.9151734113693237, loss=1.913991928100586
I0129 17:54:41.287067 140005305468672 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.8066481351852417, loss=1.8910839557647705
I0129 17:55:14.690319 140005297075968 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.8851858377456665, loss=1.8810064792633057
I0129 17:55:48.092986 140005305468672 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.8404947519302368, loss=1.9215961694717407
I0129 17:56:21.494877 140005297075968 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.8696707487106323, loss=1.899109959602356
I0129 17:56:54.895069 140005305468672 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.9210286140441895, loss=2.069000720977783
I0129 17:57:07.735057 140169137129280 spec.py:321] Evaluating on the training split.
I0129 17:57:14.204762 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 17:57:22.895193 140169137129280 spec.py:349] Evaluating on the test split.
I0129 17:57:25.584955 140169137129280 submission_runner.py:408] Time since start: 36488.63s, 	Step: 105140, 	{'train/accuracy': 0.4314213991165161, 'train/loss': 2.7033612728118896, 'validation/accuracy': 0.39800000190734863, 'validation/loss': 2.949122190475464, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.649909496307373, 'test/num_examples': 10000, 'score': 35229.46646118164, 'total_duration': 36488.632420539856, 'accumulated_submission_time': 35229.46646118164, 'accumulated_eval_time': 1252.949723482132, 'accumulated_logging_time': 2.7811789512634277}
I0129 17:57:25.621517 140004624946944 logging_writer.py:48] [105140] accumulated_eval_time=1252.949723, accumulated_logging_time=2.781179, accumulated_submission_time=35229.466461, global_step=105140, preemption_count=0, score=35229.466461, test/accuracy=0.319400, test/loss=3.649909, test/num_examples=10000, total_duration=36488.632421, train/accuracy=0.431421, train/loss=2.703361, validation/accuracy=0.398000, validation/loss=2.949122, validation/num_examples=50000
I0129 17:57:45.998509 140005288683264 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.8424152135849, loss=1.908068060874939
I0129 17:58:19.341483 140004624946944 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.9277952909469604, loss=1.9294867515563965
I0129 17:58:52.729176 140005288683264 logging_writer.py:48] [105400] global_step=105400, grad_norm=3.635288715362549, loss=2.0212128162384033
I0129 17:59:26.125429 140004624946944 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.9045418500900269, loss=1.909395456314087
I0129 17:59:59.620774 140005288683264 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.0281240940093994, loss=1.9207487106323242
I0129 18:00:33.013345 140004624946944 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.947042465209961, loss=2.0509250164031982
I0129 18:01:06.401349 140005288683264 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.845113754272461, loss=1.85469651222229
I0129 18:01:39.800900 140004624946944 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.820001482963562, loss=1.8355374336242676
I0129 18:02:13.198320 140005288683264 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.083793878555298, loss=1.884764313697815
I0129 18:02:46.602223 140004624946944 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.8209609985351562, loss=2.0675017833709717
I0129 18:03:20.011408 140005288683264 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.0691988468170166, loss=1.914710283279419
I0129 18:03:53.407860 140004624946944 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.955511450767517, loss=1.9889795780181885
I0129 18:04:26.814151 140005288683264 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.9784327745437622, loss=1.9214460849761963
I0129 18:05:00.210588 140004624946944 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.976146936416626, loss=1.9618334770202637
I0129 18:05:33.604095 140005288683264 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.055452346801758, loss=1.9847303628921509
I0129 18:05:55.787297 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:06:02.630187 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:06:11.338975 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:06:14.022930 140169137129280 submission_runner.py:408] Time since start: 37017.07s, 	Step: 106668, 	{'train/accuracy': 0.562898576259613, 'train/loss': 1.828281283378601, 'validation/accuracy': 0.5299199819564819, 'validation/loss': 2.0261363983154297, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7543745040893555, 'test/num_examples': 10000, 'score': 35739.570628643036, 'total_duration': 37017.070395708084, 'accumulated_submission_time': 35739.570628643036, 'accumulated_eval_time': 1271.185317993164, 'accumulated_logging_time': 2.8301868438720703}
I0129 18:06:14.062193 140004616554240 logging_writer.py:48] [106668] accumulated_eval_time=1271.185318, accumulated_logging_time=2.830187, accumulated_submission_time=35739.570629, global_step=106668, preemption_count=0, score=35739.570629, test/accuracy=0.407500, test/loss=2.754375, test/num_examples=10000, total_duration=37017.070396, train/accuracy=0.562899, train/loss=1.828281, validation/accuracy=0.529920, validation/loss=2.026136, validation/num_examples=50000
I0129 18:06:25.073308 140005297075968 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.344855546951294, loss=1.847299575805664
I0129 18:06:58.408910 140004616554240 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.8929810523986816, loss=1.8161253929138184
I0129 18:07:31.774746 140005297075968 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.1538474559783936, loss=2.0267622470855713
I0129 18:08:05.176036 140004616554240 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.924293041229248, loss=1.8266329765319824
I0129 18:08:38.579183 140005297075968 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.006190538406372, loss=1.915380597114563
I0129 18:09:11.996478 140004616554240 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.8330070972442627, loss=1.8991460800170898
I0129 18:09:45.384626 140005297075968 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.3132500648498535, loss=1.988581657409668
I0129 18:10:18.785372 140004616554240 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.0564584732055664, loss=2.016055107116699
I0129 18:10:52.188799 140005297075968 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.8300243616104126, loss=1.8793126344680786
I0129 18:11:25.595867 140004616554240 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.9634335041046143, loss=2.0036306381225586
I0129 18:11:58.999053 140005297075968 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.9721177816390991, loss=1.9380799531936646
I0129 18:12:32.505193 140004616554240 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.950653314590454, loss=2.039071559906006
I0129 18:13:05.893333 140005297075968 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.323934316635132, loss=1.9715349674224854
I0129 18:13:39.281694 140004616554240 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.2345356941223145, loss=2.032078742980957
I0129 18:14:12.687415 140005297075968 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.827377200126648, loss=1.9464972019195557
I0129 18:14:44.224676 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:14:50.580437 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:14:59.433646 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:15:02.145376 140169137129280 submission_runner.py:408] Time since start: 37545.19s, 	Step: 108196, 	{'train/accuracy': 0.5349768996238708, 'train/loss': 1.9775022268295288, 'validation/accuracy': 0.48151999711990356, 'validation/loss': 2.32653546333313, 'validation/num_examples': 50000, 'test/accuracy': 0.359000027179718, 'test/loss': 3.246283531188965, 'test/num_examples': 10000, 'score': 36249.67180633545, 'total_duration': 37545.1928293705, 'accumulated_submission_time': 36249.67180633545, 'accumulated_eval_time': 1289.1059653759003, 'accumulated_logging_time': 2.8810999393463135}
I0129 18:15:02.185102 140004624946944 logging_writer.py:48] [108196] accumulated_eval_time=1289.105965, accumulated_logging_time=2.881100, accumulated_submission_time=36249.671806, global_step=108196, preemption_count=0, score=36249.671806, test/accuracy=0.359000, test/loss=3.246284, test/num_examples=10000, total_duration=37545.192829, train/accuracy=0.534977, train/loss=1.977502, validation/accuracy=0.481520, validation/loss=2.326535, validation/num_examples=50000
I0129 18:15:03.872120 140005313861376 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.8446648120880127, loss=1.8732274770736694
I0129 18:15:37.200072 140004624946944 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.9227319955825806, loss=1.7525649070739746
I0129 18:16:10.575895 140005313861376 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.925022840499878, loss=1.9074127674102783
I0129 18:16:43.973170 140004624946944 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.0090179443359375, loss=1.8340274095535278
I0129 18:17:17.378308 140005313861376 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.8576602935791016, loss=1.852948546409607
I0129 18:17:50.763729 140004624946944 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.8858565092086792, loss=1.857940435409546
I0129 18:18:24.169487 140005313861376 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.1011416912078857, loss=1.876670479774475
I0129 18:18:57.660547 140004624946944 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.4733188152313232, loss=2.01470685005188
I0129 18:19:31.050566 140005313861376 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.8258333206176758, loss=1.9202733039855957
I0129 18:20:04.458299 140004624946944 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.8935933113098145, loss=1.8581210374832153
I0129 18:20:37.851921 140005313861376 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.1010005474090576, loss=1.854245901107788
I0129 18:21:11.250110 140004624946944 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.9344927072525024, loss=1.8971104621887207
I0129 18:21:44.651679 140005313861376 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.9658399820327759, loss=1.8493865728378296
I0129 18:22:18.041214 140004624946944 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.9590550661087036, loss=1.8261686563491821
I0129 18:22:51.441259 140005313861376 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.1347110271453857, loss=1.9147166013717651
I0129 18:23:24.838734 140004624946944 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.9324321746826172, loss=1.9414103031158447
I0129 18:23:32.334129 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:23:38.711695 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:23:47.423775 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:23:50.113748 140169137129280 submission_runner.py:408] Time since start: 38073.16s, 	Step: 109724, 	{'train/accuracy': 0.5242546200752258, 'train/loss': 2.0457510948181152, 'validation/accuracy': 0.48155999183654785, 'validation/loss': 2.3079276084899902, 'validation/num_examples': 50000, 'test/accuracy': 0.3776000142097473, 'test/loss': 3.0594658851623535, 'test/num_examples': 10000, 'score': 36759.76053881645, 'total_duration': 38073.16121888161, 'accumulated_submission_time': 36759.76053881645, 'accumulated_eval_time': 1306.8855466842651, 'accumulated_logging_time': 2.9313416481018066}
I0129 18:23:50.153112 140004616554240 logging_writer.py:48] [109724] accumulated_eval_time=1306.885547, accumulated_logging_time=2.931342, accumulated_submission_time=36759.760539, global_step=109724, preemption_count=0, score=36759.760539, test/accuracy=0.377600, test/loss=3.059466, test/num_examples=10000, total_duration=38073.161219, train/accuracy=0.524255, train/loss=2.045751, validation/accuracy=0.481560, validation/loss=2.307928, validation/num_examples=50000
I0129 18:24:15.850046 140005297075968 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.2244181632995605, loss=1.8969677686691284
I0129 18:24:49.205868 140004616554240 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.050075054168701, loss=2.05916428565979
I0129 18:25:22.694165 140005297075968 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.9216886758804321, loss=1.993064045906067
I0129 18:25:56.098861 140004616554240 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.8992315530776978, loss=1.8879616260528564
I0129 18:26:29.498019 140005297075968 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.9435867071151733, loss=1.7939457893371582
I0129 18:27:02.909632 140004616554240 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.9920393228530884, loss=2.0024170875549316
I0129 18:27:36.291827 140005297075968 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.253606081008911, loss=1.8862382173538208
I0129 18:28:09.685978 140004616554240 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9497971534729004, loss=1.8245433568954468
I0129 18:28:43.091458 140005297075968 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.9494411945343018, loss=2.0059471130371094
I0129 18:29:16.480396 140004616554240 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.4815986156463623, loss=1.8233802318572998
I0129 18:29:49.875509 140005297075968 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.9374494552612305, loss=1.7678446769714355
I0129 18:30:23.277185 140004616554240 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.2389471530914307, loss=1.9654953479766846
I0129 18:30:56.688635 140005297075968 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.1234190464019775, loss=1.8317772150039673
I0129 18:31:30.165044 140004616554240 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.1524572372436523, loss=1.8622065782546997
I0129 18:32:03.564354 140005297075968 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.188311815261841, loss=1.8165969848632812
I0129 18:32:20.412453 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:32:26.903169 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:32:35.586831 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:32:38.294070 140169137129280 submission_runner.py:408] Time since start: 38601.34s, 	Step: 111252, 	{'train/accuracy': 0.5350366830825806, 'train/loss': 1.9782723188400269, 'validation/accuracy': 0.5010600090026855, 'validation/loss': 2.1930699348449707, 'validation/num_examples': 50000, 'test/accuracy': 0.3883000314235687, 'test/loss': 2.9401986598968506, 'test/num_examples': 10000, 'score': 37269.95954012871, 'total_duration': 38601.34153342247, 'accumulated_submission_time': 37269.95954012871, 'accumulated_eval_time': 1324.7671279907227, 'accumulated_logging_time': 2.9814400672912598}
I0129 18:32:38.332064 140005305468672 logging_writer.py:48] [111252] accumulated_eval_time=1324.767128, accumulated_logging_time=2.981440, accumulated_submission_time=37269.959540, global_step=111252, preemption_count=0, score=37269.959540, test/accuracy=0.388300, test/loss=2.940199, test/num_examples=10000, total_duration=38601.341533, train/accuracy=0.535037, train/loss=1.978272, validation/accuracy=0.501060, validation/loss=2.193070, validation/num_examples=50000
I0129 18:32:54.687458 140005313861376 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.9951567649841309, loss=1.8463501930236816
I0129 18:33:28.025655 140005305468672 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.8889001607894897, loss=1.9503873586654663
I0129 18:34:01.417805 140005313861376 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.9536200761795044, loss=1.8823364973068237
I0129 18:34:34.813674 140005305468672 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.2011759281158447, loss=1.8605782985687256
I0129 18:35:08.205462 140005313861376 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.155393362045288, loss=1.9166172742843628
I0129 18:35:41.602809 140005305468672 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.11285138130188, loss=1.9529149532318115
I0129 18:36:14.995633 140005313861376 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.0934886932373047, loss=1.7485018968582153
I0129 18:36:48.399594 140005305468672 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.205267906188965, loss=1.846488356590271
I0129 18:37:21.790084 140005313861376 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.097604274749756, loss=1.9803048372268677
I0129 18:37:55.283963 140005305468672 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.9090499877929688, loss=1.8839339017868042
I0129 18:38:28.690546 140005313861376 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.027954578399658, loss=1.8355934619903564
I0129 18:39:02.084457 140005305468672 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.1712982654571533, loss=1.9535436630249023
I0129 18:39:35.506555 140005313861376 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.111738920211792, loss=1.824021577835083
I0129 18:40:08.913254 140005305468672 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.4175667762756348, loss=2.022397994995117
I0129 18:40:42.329549 140005313861376 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.1536800861358643, loss=2.0318589210510254
I0129 18:41:08.521573 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:41:14.912768 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:41:23.437771 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:41:26.167828 140169137129280 submission_runner.py:408] Time since start: 39129.22s, 	Step: 112780, 	{'train/accuracy': 0.5105628371238708, 'train/loss': 2.158193826675415, 'validation/accuracy': 0.4708399772644043, 'validation/loss': 2.4166712760925293, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.1187973022460938, 'test/num_examples': 10000, 'score': 37780.089233636856, 'total_duration': 39129.21529483795, 'accumulated_submission_time': 37780.089233636856, 'accumulated_eval_time': 1342.4133460521698, 'accumulated_logging_time': 3.0294137001037598}
I0129 18:41:26.209013 140004624946944 logging_writer.py:48] [112780] accumulated_eval_time=1342.413346, accumulated_logging_time=3.029414, accumulated_submission_time=37780.089234, global_step=112780, preemption_count=0, score=37780.089234, test/accuracy=0.368000, test/loss=3.118797, test/num_examples=10000, total_duration=39129.215295, train/accuracy=0.510563, train/loss=2.158194, validation/accuracy=0.470840, validation/loss=2.416671, validation/num_examples=50000
I0129 18:41:33.220905 140005288683264 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.9211066961288452, loss=1.8694032430648804
I0129 18:42:06.571472 140004624946944 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.3800501823425293, loss=1.9630253314971924
I0129 18:42:39.948225 140005288683264 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.2577946186065674, loss=1.9567664861679077
I0129 18:43:13.354153 140004624946944 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.1981828212738037, loss=1.8895947933197021
I0129 18:43:46.745293 140005288683264 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.4154696464538574, loss=1.8651340007781982
I0129 18:44:20.236859 140004624946944 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.1055495738983154, loss=1.9190574884414673
I0129 18:44:53.630776 140005288683264 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.3498785495758057, loss=1.9814640283584595
I0129 18:45:27.030241 140004624946944 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.278313159942627, loss=1.903399109840393
I0129 18:46:00.436950 140005288683264 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.1315317153930664, loss=1.8996593952178955
I0129 18:46:33.820150 140004624946944 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.247844696044922, loss=1.8890693187713623
I0129 18:47:07.206147 140005288683264 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.1310739517211914, loss=1.8309109210968018
I0129 18:47:40.600814 140004624946944 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.1172842979431152, loss=1.928574800491333
I0129 18:48:13.990324 140005288683264 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.199026584625244, loss=1.911358118057251
I0129 18:48:47.402955 140004624946944 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.185664415359497, loss=1.8812861442565918
I0129 18:49:20.801730 140005288683264 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.1198744773864746, loss=1.7611355781555176
I0129 18:49:54.192497 140004624946944 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.258958578109741, loss=1.7953730821609497
I0129 18:49:56.352112 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:50:02.790432 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:50:11.317501 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:50:13.984294 140169137129280 submission_runner.py:408] Time since start: 39657.03s, 	Step: 114308, 	{'train/accuracy': 0.5438257455825806, 'train/loss': 1.9540691375732422, 'validation/accuracy': 0.5044999718666077, 'validation/loss': 2.177619695663452, 'validation/num_examples': 50000, 'test/accuracy': 0.38130003213882446, 'test/loss': 3.057910680770874, 'test/num_examples': 10000, 'score': 38290.16973924637, 'total_duration': 39657.031764507294, 'accumulated_submission_time': 38290.16973924637, 'accumulated_eval_time': 1360.0454897880554, 'accumulated_logging_time': 3.0832841396331787}
I0129 18:50:14.023496 140005313861376 logging_writer.py:48] [114308] accumulated_eval_time=1360.045490, accumulated_logging_time=3.083284, accumulated_submission_time=38290.169739, global_step=114308, preemption_count=0, score=38290.169739, test/accuracy=0.381300, test/loss=3.057911, test/num_examples=10000, total_duration=39657.031765, train/accuracy=0.543826, train/loss=1.954069, validation/accuracy=0.504500, validation/loss=2.177620, validation/num_examples=50000
I0129 18:50:45.134331 140005322254080 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.24562931060791, loss=1.898266077041626
I0129 18:51:18.486465 140005313861376 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.263169765472412, loss=1.878007411956787
I0129 18:51:51.866229 140005322254080 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.1819217205047607, loss=1.8915916681289673
I0129 18:52:25.260312 140005313861376 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.3066022396087646, loss=1.8174571990966797
I0129 18:52:58.670249 140005322254080 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.4533321857452393, loss=1.8597131967544556
I0129 18:53:32.066107 140005313861376 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.2196853160858154, loss=1.859020709991455
I0129 18:54:05.470968 140005322254080 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.9957860708236694, loss=1.744562029838562
I0129 18:54:38.861841 140005313861376 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.040835380554199, loss=1.9086074829101562
I0129 18:55:12.262721 140005322254080 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.3758609294891357, loss=1.9108917713165283
I0129 18:55:45.674571 140005313861376 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.025238037109375, loss=1.828296422958374
I0129 18:56:19.091681 140005322254080 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.047412633895874, loss=1.884442925453186
I0129 18:56:52.501866 140005313861376 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.114121437072754, loss=1.7952507734298706
I0129 18:57:25.987117 140005322254080 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.2364094257354736, loss=1.8701874017715454
I0129 18:57:59.367025 140005313861376 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.1041922569274902, loss=1.9106838703155518
I0129 18:58:32.768074 140005322254080 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.388563632965088, loss=1.8022397756576538
I0129 18:58:44.278628 140169137129280 spec.py:321] Evaluating on the training split.
I0129 18:58:50.692293 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 18:58:59.500938 140169137129280 spec.py:349] Evaluating on the test split.
I0129 18:59:02.261514 140169137129280 submission_runner.py:408] Time since start: 40185.31s, 	Step: 115836, 	{'train/accuracy': 0.6219108700752258, 'train/loss': 1.5522531270980835, 'validation/accuracy': 0.5729599595069885, 'validation/loss': 1.8273431062698364, 'validation/num_examples': 50000, 'test/accuracy': 0.45750001072883606, 'test/loss': 2.5451416969299316, 'test/num_examples': 10000, 'score': 38800.36287164688, 'total_duration': 40185.308972120285, 'accumulated_submission_time': 38800.36287164688, 'accumulated_eval_time': 1378.0283389091492, 'accumulated_logging_time': 3.134504556655884}
I0129 18:59:02.299069 140005305468672 logging_writer.py:48] [115836] accumulated_eval_time=1378.028339, accumulated_logging_time=3.134505, accumulated_submission_time=38800.362872, global_step=115836, preemption_count=0, score=38800.362872, test/accuracy=0.457500, test/loss=2.545142, test/num_examples=10000, total_duration=40185.308972, train/accuracy=0.621911, train/loss=1.552253, validation/accuracy=0.572960, validation/loss=1.827343, validation/num_examples=50000
I0129 18:59:23.971152 140005330646784 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.1898627281188965, loss=1.8347488641738892
I0129 18:59:57.296282 140005305468672 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.101161241531372, loss=1.731162190437317
I0129 19:00:30.664645 140005330646784 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.2764077186584473, loss=1.795405387878418
I0129 19:01:04.061230 140005305468672 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.2489967346191406, loss=1.7884769439697266
I0129 19:01:37.467651 140005330646784 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.9242736101150513, loss=1.7482547760009766
I0129 19:02:10.859193 140005305468672 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.194453477859497, loss=1.8614617586135864
I0129 19:02:44.254061 140005330646784 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.1054258346557617, loss=1.7550467252731323
I0129 19:03:17.646724 140005305468672 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.0487892627716064, loss=1.8358334302902222
I0129 19:03:51.138204 140005330646784 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.9168343544006348, loss=1.7821389436721802
I0129 19:04:24.529028 140005305468672 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.0910120010375977, loss=1.7962946891784668
I0129 19:04:57.936570 140005330646784 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.1598117351531982, loss=1.857614278793335
I0129 19:05:31.328258 140005305468672 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.2262401580810547, loss=1.7755985260009766
I0129 19:06:04.719940 140005330646784 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.2506844997406006, loss=1.8493685722351074
I0129 19:06:38.110151 140005305468672 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.115663766860962, loss=1.8329812288284302
I0129 19:07:11.500857 140005330646784 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.4734363555908203, loss=1.8752126693725586
I0129 19:07:32.354132 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:07:38.707294 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:07:47.542982 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:07:50.245126 140169137129280 submission_runner.py:408] Time since start: 40713.29s, 	Step: 117364, 	{'train/accuracy': 0.6141780614852905, 'train/loss': 1.578540563583374, 'validation/accuracy': 0.551800012588501, 'validation/loss': 1.9139325618743896, 'validation/num_examples': 50000, 'test/accuracy': 0.435200035572052, 'test/loss': 2.6567647457122803, 'test/num_examples': 10000, 'score': 39310.3577773571, 'total_duration': 40713.292598724365, 'accumulated_submission_time': 39310.3577773571, 'accumulated_eval_time': 1395.919328212738, 'accumulated_logging_time': 3.18233060836792}
I0129 19:07:50.286310 140004624946944 logging_writer.py:48] [117364] accumulated_eval_time=1395.919328, accumulated_logging_time=3.182331, accumulated_submission_time=39310.357777, global_step=117364, preemption_count=0, score=39310.357777, test/accuracy=0.435200, test/loss=2.656765, test/num_examples=10000, total_duration=40713.292599, train/accuracy=0.614178, train/loss=1.578541, validation/accuracy=0.551800, validation/loss=1.913933, validation/num_examples=50000
I0129 19:08:02.636717 140005288683264 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.5331413745880127, loss=1.8484241962432861
I0129 19:08:35.998651 140004624946944 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.152506113052368, loss=1.7789709568023682
I0129 19:09:09.383596 140005288683264 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.2518484592437744, loss=1.9630976915359497
I0129 19:09:42.770082 140004624946944 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.2028543949127197, loss=1.9026223421096802
I0129 19:10:16.234307 140005288683264 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.2598090171813965, loss=1.815091609954834
I0129 19:10:49.640619 140004624946944 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.7349984645843506, loss=1.8602163791656494
I0129 19:11:23.041373 140005288683264 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.341344118118286, loss=1.8525454998016357
I0129 19:11:56.438120 140004624946944 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.498023271560669, loss=1.871985673904419
I0129 19:12:29.847926 140005288683264 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.3695068359375, loss=1.8932886123657227
I0129 19:13:03.266458 140004624946944 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.199840784072876, loss=1.784880518913269
I0129 19:13:36.661114 140005288683264 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.2413864135742188, loss=1.7030750513076782
I0129 19:14:10.048298 140004624946944 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.25065279006958, loss=1.8393018245697021
I0129 19:14:43.454747 140005288683264 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.1806392669677734, loss=1.7680634260177612
I0129 19:15:16.857239 140004624946944 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.0993874073028564, loss=1.6135433912277222
I0129 19:15:50.255848 140005288683264 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.3287665843963623, loss=1.7703378200531006
I0129 19:16:20.540952 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:16:26.877634 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:16:35.407992 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:16:38.118457 140169137129280 submission_runner.py:408] Time since start: 41241.17s, 	Step: 118892, 	{'train/accuracy': 0.5724250674247742, 'train/loss': 1.7684458494186401, 'validation/accuracy': 0.526639997959137, 'validation/loss': 2.0348665714263916, 'validation/num_examples': 50000, 'test/accuracy': 0.4172000288963318, 'test/loss': 2.772226333618164, 'test/num_examples': 10000, 'score': 39820.552035331726, 'total_duration': 41241.165909051895, 'accumulated_submission_time': 39820.552035331726, 'accumulated_eval_time': 1413.4967761039734, 'accumulated_logging_time': 3.233930826187134}
I0129 19:16:38.158468 140004616554240 logging_writer.py:48] [118892] accumulated_eval_time=1413.496776, accumulated_logging_time=3.233931, accumulated_submission_time=39820.552035, global_step=118892, preemption_count=0, score=39820.552035, test/accuracy=0.417200, test/loss=2.772226, test/num_examples=10000, total_duration=41241.165909, train/accuracy=0.572425, train/loss=1.768446, validation/accuracy=0.526640, validation/loss=2.034867, validation/num_examples=50000
I0129 19:16:41.172159 140005313861376 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.2752363681793213, loss=1.7952077388763428
I0129 19:17:14.508895 140004616554240 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.2384605407714844, loss=1.7329304218292236
I0129 19:17:47.880729 140005313861376 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.0595405101776123, loss=1.79616379737854
I0129 19:18:21.266210 140004616554240 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.199928045272827, loss=1.8690149784088135
I0129 19:18:54.656314 140005313861376 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.203827142715454, loss=1.8020178079605103
I0129 19:19:28.051487 140004616554240 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.292062520980835, loss=1.791682481765747
I0129 19:20:01.453306 140005313861376 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.426537036895752, loss=1.7536160945892334
I0129 19:20:34.855235 140004616554240 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.252739191055298, loss=1.892991542816162
I0129 19:21:08.241893 140005313861376 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.3528616428375244, loss=1.877722144126892
I0129 19:21:41.646729 140004616554240 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.3390026092529297, loss=1.8719497919082642
I0129 19:22:15.038341 140005313861376 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.342629909515381, loss=1.7690010070800781
I0129 19:22:48.535176 140004616554240 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.145081043243408, loss=1.8770725727081299
I0129 19:23:21.935987 140005313861376 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.214664936065674, loss=1.8280669450759888
I0129 19:23:55.333747 140004616554240 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.5124645233154297, loss=1.8162363767623901
I0129 19:24:28.723371 140005313861376 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.333124876022339, loss=1.7296873331069946
I0129 19:25:02.116673 140004616554240 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.4567079544067383, loss=1.7886430025100708
I0129 19:25:08.282515 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:25:14.708645 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:25:23.169798 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:25:25.869294 140169137129280 submission_runner.py:408] Time since start: 41768.92s, 	Step: 120420, 	{'train/accuracy': 0.6161710619926453, 'train/loss': 1.5671641826629639, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.838868498802185, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5782828330993652, 'test/num_examples': 10000, 'score': 40330.61541700363, 'total_duration': 41768.916761636734, 'accumulated_submission_time': 40330.61541700363, 'accumulated_eval_time': 1431.0835175514221, 'accumulated_logging_time': 3.284346342086792}
I0129 19:25:25.909731 140004624946944 logging_writer.py:48] [120420] accumulated_eval_time=1431.083518, accumulated_logging_time=3.284346, accumulated_submission_time=40330.615417, global_step=120420, preemption_count=0, score=40330.615417, test/accuracy=0.443200, test/loss=2.578283, test/num_examples=10000, total_duration=41768.916762, train/accuracy=0.616171, train/loss=1.567164, validation/accuracy=0.566400, validation/loss=1.838868, validation/num_examples=50000
I0129 19:25:52.900055 140005288683264 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.2529895305633545, loss=1.7675209045410156
I0129 19:26:26.242315 140004624946944 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.3582098484039307, loss=1.7942733764648438
I0129 19:26:59.622281 140005288683264 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.392436981201172, loss=1.8680155277252197
I0129 19:27:33.015678 140004624946944 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.3671462535858154, loss=1.839528203010559
I0129 19:28:06.412347 140005288683264 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.1837399005889893, loss=1.9170924425125122
I0129 19:28:39.806964 140004624946944 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.42814302444458, loss=1.7684259414672852
I0129 19:29:13.361213 140005288683264 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.3208327293395996, loss=1.8041046857833862
I0129 19:29:46.738351 140004624946944 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.4787468910217285, loss=1.769911766052246
I0129 19:30:20.120384 140005288683264 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.430192470550537, loss=1.819732904434204
I0129 19:30:53.490003 140004624946944 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.1457107067108154, loss=1.8362518548965454
I0129 19:31:26.885529 140005288683264 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.2642385959625244, loss=1.8646377325057983
I0129 19:32:00.284909 140004624946944 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.4465131759643555, loss=1.7919425964355469
I0129 19:32:33.682162 140005288683264 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.0882701873779297, loss=1.7450275421142578
I0129 19:33:07.078775 140004624946944 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.541215181350708, loss=1.9629318714141846
I0129 19:33:40.473714 140005288683264 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.5620765686035156, loss=1.8199048042297363
I0129 19:33:55.993306 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:34:02.430752 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:34:11.092647 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:34:13.784185 140169137129280 submission_runner.py:408] Time since start: 42296.83s, 	Step: 121948, 	{'train/accuracy': 0.5903021097183228, 'train/loss': 1.6958752870559692, 'validation/accuracy': 0.5471799969673157, 'validation/loss': 1.95624577999115, 'validation/num_examples': 50000, 'test/accuracy': 0.43160003423690796, 'test/loss': 2.7272050380706787, 'test/num_examples': 10000, 'score': 40840.63898897171, 'total_duration': 42296.831644296646, 'accumulated_submission_time': 40840.63898897171, 'accumulated_eval_time': 1448.8743512630463, 'accumulated_logging_time': 3.3349971771240234}
I0129 19:34:13.824151 140005313861376 logging_writer.py:48] [121948] accumulated_eval_time=1448.874351, accumulated_logging_time=3.334997, accumulated_submission_time=40840.638989, global_step=121948, preemption_count=0, score=40840.638989, test/accuracy=0.431600, test/loss=2.727205, test/num_examples=10000, total_duration=42296.831644, train/accuracy=0.590302, train/loss=1.695875, validation/accuracy=0.547180, validation/loss=1.956246, validation/num_examples=50000
I0129 19:34:31.525774 140005322254080 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.2579195499420166, loss=1.727744698524475
I0129 19:35:04.872375 140005313861376 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.474665403366089, loss=1.7505342960357666
I0129 19:35:38.363498 140005322254080 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.235775947570801, loss=1.7821625471115112
I0129 19:36:11.763027 140005313861376 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.3624091148376465, loss=1.8191182613372803
I0129 19:36:45.165703 140005322254080 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.5895118713378906, loss=1.76445734500885
I0129 19:37:18.549509 140005313861376 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.4328651428222656, loss=1.8248294591903687
I0129 19:37:51.959244 140005322254080 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.441477060317993, loss=1.834171175956726
I0129 19:38:25.350158 140005313861376 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.733853340148926, loss=1.9117600917816162
I0129 19:38:58.760889 140005322254080 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.3049583435058594, loss=1.720592737197876
I0129 19:39:32.165066 140005313861376 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.242480516433716, loss=1.751947283744812
I0129 19:40:05.577247 140005322254080 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.424320936203003, loss=1.8679250478744507
I0129 19:40:38.969503 140005313861376 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.2707533836364746, loss=1.6386390924453735
I0129 19:41:12.362690 140005322254080 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.4941937923431396, loss=1.692978024482727
I0129 19:41:45.855731 140005313861376 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.5078396797180176, loss=1.747329831123352
I0129 19:42:19.245726 140005322254080 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.4995081424713135, loss=1.7702138423919678
I0129 19:42:44.101492 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:42:50.601919 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:42:59.034525 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:43:01.744194 140169137129280 submission_runner.py:408] Time since start: 42824.79s, 	Step: 123476, 	{'train/accuracy': 0.6090362071990967, 'train/loss': 1.6039897203445435, 'validation/accuracy': 0.5696600079536438, 'validation/loss': 1.830997109413147, 'validation/num_examples': 50000, 'test/accuracy': 0.43780001997947693, 'test/loss': 2.6645984649658203, 'test/num_examples': 10000, 'score': 41350.856478214264, 'total_duration': 42824.79166054726, 'accumulated_submission_time': 41350.856478214264, 'accumulated_eval_time': 1466.5170137882233, 'accumulated_logging_time': 3.384669065475464}
I0129 19:43:01.786633 140005288683264 logging_writer.py:48] [123476] accumulated_eval_time=1466.517014, accumulated_logging_time=3.384669, accumulated_submission_time=41350.856478, global_step=123476, preemption_count=0, score=41350.856478, test/accuracy=0.437800, test/loss=2.664598, test/num_examples=10000, total_duration=42824.791661, train/accuracy=0.609036, train/loss=1.603990, validation/accuracy=0.569660, validation/loss=1.830997, validation/num_examples=50000
I0129 19:43:10.138324 140005297075968 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.2244932651519775, loss=1.8318207263946533
I0129 19:43:43.477880 140005288683264 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.399540662765503, loss=1.7427445650100708
I0129 19:44:16.846758 140005297075968 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.3410258293151855, loss=1.8297297954559326
I0129 19:44:50.229433 140005288683264 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.3675193786621094, loss=1.8337557315826416
I0129 19:45:23.638079 140005297075968 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.2021002769470215, loss=1.7660245895385742
I0129 19:45:57.033322 140005288683264 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.2913427352905273, loss=1.812426209449768
I0129 19:46:30.439977 140005297075968 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.328967332839966, loss=1.7691895961761475
I0129 19:47:03.829128 140005288683264 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.641664981842041, loss=1.8063786029815674
I0129 19:47:37.223078 140005297075968 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.327831983566284, loss=1.7796728610992432
I0129 19:48:10.780110 140005288683264 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.372558832168579, loss=1.7891130447387695
I0129 19:48:44.179144 140005297075968 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.521240234375, loss=1.8205287456512451
I0129 19:49:17.590824 140005288683264 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.4200985431671143, loss=1.72357177734375
I0129 19:49:50.983964 140005297075968 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.418902635574341, loss=1.7859305143356323
I0129 19:50:24.372390 140005288683264 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.878248929977417, loss=1.852880597114563
I0129 19:50:57.776143 140005297075968 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.441108226776123, loss=1.7058913707733154
I0129 19:51:31.165111 140005288683264 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.64428448677063, loss=1.6437740325927734
I0129 19:51:31.986541 140169137129280 spec.py:321] Evaluating on the training split.
I0129 19:51:38.391838 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 19:51:46.947179 140169137129280 spec.py:349] Evaluating on the test split.
I0129 19:51:49.626481 140169137129280 submission_runner.py:408] Time since start: 43352.67s, 	Step: 125004, 	{'train/accuracy': 0.6317163705825806, 'train/loss': 1.4943809509277344, 'validation/accuracy': 0.5685999989509583, 'validation/loss': 1.8384133577346802, 'validation/num_examples': 50000, 'test/accuracy': 0.44460001587867737, 'test/loss': 2.6121327877044678, 'test/num_examples': 10000, 'score': 41860.99411845207, 'total_duration': 43352.67394042015, 'accumulated_submission_time': 41860.99411845207, 'accumulated_eval_time': 1484.1569118499756, 'accumulated_logging_time': 3.4393198490142822}
I0129 19:51:49.667408 140004624946944 logging_writer.py:48] [125004] accumulated_eval_time=1484.156912, accumulated_logging_time=3.439320, accumulated_submission_time=41860.994118, global_step=125004, preemption_count=0, score=41860.994118, test/accuracy=0.444600, test/loss=2.612133, test/num_examples=10000, total_duration=43352.673940, train/accuracy=0.631716, train/loss=1.494381, validation/accuracy=0.568600, validation/loss=1.838413, validation/num_examples=50000
I0129 19:52:22.021234 140005288683264 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.4638893604278564, loss=1.850243091583252
I0129 19:52:55.389379 140004624946944 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.3914713859558105, loss=1.7383065223693848
I0129 19:53:28.775276 140005288683264 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.2958977222442627, loss=1.7917581796646118
I0129 19:54:02.171007 140004624946944 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.2790513038635254, loss=1.7017781734466553
I0129 19:54:35.564037 140005288683264 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.6710336208343506, loss=1.9166730642318726
I0129 19:55:09.068696 140004624946944 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.501052141189575, loss=1.7983887195587158
I0129 19:55:42.457896 140005288683264 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.4868221282958984, loss=1.7595903873443604
I0129 19:56:15.850683 140004624946944 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.571255922317505, loss=1.7299586534500122
I0129 19:56:49.245765 140005288683264 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.705568790435791, loss=1.7694480419158936
I0129 19:57:22.643877 140004624946944 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.680391550064087, loss=1.736564040184021
I0129 19:57:56.053675 140005288683264 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.47387433052063, loss=1.6971435546875
I0129 19:58:29.453389 140004624946944 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.539015293121338, loss=1.8277335166931152
I0129 19:59:02.854869 140005288683264 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.3865060806274414, loss=1.6924620866775513
I0129 19:59:36.247422 140004624946944 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.3391451835632324, loss=1.678734540939331
I0129 20:00:09.668691 140005288683264 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.5383048057556152, loss=1.7401273250579834
I0129 20:00:19.830498 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:00:26.161113 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:00:34.826992 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:00:37.487820 140169137129280 submission_runner.py:408] Time since start: 43880.54s, 	Step: 126532, 	{'train/accuracy': 0.5991111397743225, 'train/loss': 1.6552882194519043, 'validation/accuracy': 0.5497399568557739, 'validation/loss': 1.9696252346038818, 'validation/num_examples': 50000, 'test/accuracy': 0.42260003089904785, 'test/loss': 2.7985992431640625, 'test/num_examples': 10000, 'score': 42371.095455646515, 'total_duration': 43880.535278081894, 'accumulated_submission_time': 42371.095455646515, 'accumulated_eval_time': 1501.814185142517, 'accumulated_logging_time': 3.4921960830688477}
I0129 20:00:37.533704 140004608161536 logging_writer.py:48] [126532] accumulated_eval_time=1501.814185, accumulated_logging_time=3.492196, accumulated_submission_time=42371.095456, global_step=126532, preemption_count=0, score=42371.095456, test/accuracy=0.422600, test/loss=2.798599, test/num_examples=10000, total_duration=43880.535278, train/accuracy=0.599111, train/loss=1.655288, validation/accuracy=0.549740, validation/loss=1.969625, validation/num_examples=50000
I0129 20:01:00.568055 140004616554240 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.4085426330566406, loss=1.7266976833343506
I0129 20:01:34.018217 140004608161536 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.658048629760742, loss=1.6969164609909058
I0129 20:02:07.399271 140004616554240 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.4349868297576904, loss=1.6528730392456055
I0129 20:02:40.773169 140004608161536 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.7902069091796875, loss=1.8327089548110962
I0129 20:03:14.162507 140004616554240 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.693596363067627, loss=1.8264412879943848
I0129 20:03:47.558171 140004608161536 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.471872091293335, loss=1.6963995695114136
I0129 20:04:20.948369 140004616554240 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.3330376148223877, loss=1.7657792568206787
I0129 20:04:54.344734 140004608161536 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.402754783630371, loss=1.7426724433898926
I0129 20:05:27.737294 140004616554240 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.5526857376098633, loss=1.6941850185394287
I0129 20:06:01.146188 140004608161536 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.5311005115509033, loss=1.6430078744888306
I0129 20:06:34.535978 140004616554240 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.697476387023926, loss=1.6222362518310547
I0129 20:07:07.924499 140004608161536 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.3827168941497803, loss=1.7690119743347168
I0129 20:07:41.412920 140004616554240 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.773698091506958, loss=1.826034665107727
I0129 20:08:14.799621 140004608161536 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.6637229919433594, loss=1.7589665651321411
I0129 20:08:48.184140 140004616554240 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.7249503135681152, loss=1.769075870513916
I0129 20:09:07.697011 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:09:14.091565 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:09:22.752582 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:09:25.389402 140169137129280 submission_runner.py:408] Time since start: 44408.44s, 	Step: 128060, 	{'train/accuracy': 0.6313177347183228, 'train/loss': 1.4843790531158447, 'validation/accuracy': 0.5854200124740601, 'validation/loss': 1.7508049011230469, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.504871129989624, 'test/num_examples': 10000, 'score': 42881.19850087166, 'total_duration': 44408.43687057495, 'accumulated_submission_time': 42881.19850087166, 'accumulated_eval_time': 1519.5065422058105, 'accumulated_logging_time': 3.5481600761413574}
I0129 20:09:25.430851 140004616554240 logging_writer.py:48] [128060] accumulated_eval_time=1519.506542, accumulated_logging_time=3.548160, accumulated_submission_time=42881.198501, global_step=128060, preemption_count=0, score=42881.198501, test/accuracy=0.462900, test/loss=2.504871, test/num_examples=10000, total_duration=44408.436871, train/accuracy=0.631318, train/loss=1.484379, validation/accuracy=0.585420, validation/loss=1.750805, validation/num_examples=50000
I0129 20:09:39.111419 140004624946944 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.4385223388671875, loss=1.6614744663238525
I0129 20:10:12.456701 140004616554240 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.533914089202881, loss=1.7775455713272095
I0129 20:10:45.832376 140004624946944 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.59033465385437, loss=1.7252968549728394
I0129 20:11:19.217636 140004616554240 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.686695098876953, loss=1.6885616779327393
I0129 20:11:52.615816 140004624946944 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.3811843395233154, loss=1.601824402809143
I0129 20:12:26.007859 140004616554240 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.700359344482422, loss=1.7788524627685547
I0129 20:12:59.408218 140004624946944 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.4444944858551025, loss=1.7740617990493774
I0129 20:13:32.802103 140004616554240 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.4407026767730713, loss=1.7075707912445068
I0129 20:14:06.287145 140004624946944 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.4503228664398193, loss=1.7660913467407227
I0129 20:14:39.664341 140004616554240 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.6095399856567383, loss=1.6784675121307373
I0129 20:15:13.051712 140004624946944 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.7461512088775635, loss=1.692238211631775
I0129 20:15:46.446317 140004616554240 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.6418986320495605, loss=1.7056915760040283
I0129 20:16:19.830077 140004624946944 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.525179147720337, loss=1.8542962074279785
I0129 20:16:53.216641 140004616554240 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.59439754486084, loss=1.6405504941940308
I0129 20:17:26.630877 140004624946944 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.5679242610931396, loss=1.6085588932037354
I0129 20:17:55.497257 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:18:01.963871 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:18:10.358882 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:18:13.049367 140169137129280 submission_runner.py:408] Time since start: 44936.10s, 	Step: 129588, 	{'train/accuracy': 0.620137095451355, 'train/loss': 1.5721157789230347, 'validation/accuracy': 0.5705400109291077, 'validation/loss': 1.8495970964431763, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.617003917694092, 'test/num_examples': 10000, 'score': 43391.20353627205, 'total_duration': 44936.096828222275, 'accumulated_submission_time': 43391.20353627205, 'accumulated_eval_time': 1537.0586075782776, 'accumulated_logging_time': 3.6010334491729736}
I0129 20:18:13.094247 140004608161536 logging_writer.py:48] [129588] accumulated_eval_time=1537.058608, accumulated_logging_time=3.601033, accumulated_submission_time=43391.203536, global_step=129588, preemption_count=0, score=43391.203536, test/accuracy=0.450200, test/loss=2.617004, test/num_examples=10000, total_duration=44936.096828, train/accuracy=0.620137, train/loss=1.572116, validation/accuracy=0.570540, validation/loss=1.849597, validation/num_examples=50000
I0129 20:18:17.442654 140005305468672 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.4722137451171875, loss=1.6368250846862793
I0129 20:18:50.798136 140004608161536 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.741581439971924, loss=1.6865941286087036
I0129 20:19:24.153031 140005305468672 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.4949238300323486, loss=1.7645056247711182
I0129 20:19:57.548052 140004608161536 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.555419921875, loss=1.685853123664856
I0129 20:20:31.037677 140005305468672 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.5377931594848633, loss=1.725298523902893
I0129 20:21:04.433625 140004608161536 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.761897325515747, loss=1.5834155082702637
I0129 20:21:37.811575 140005305468672 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.422048807144165, loss=1.6711755990982056
I0129 20:22:11.199598 140004608161536 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.6700997352600098, loss=1.7875853776931763
I0129 20:22:44.578139 140005305468672 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.6466712951660156, loss=1.6941592693328857
I0129 20:23:17.975468 140004608161536 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.574549674987793, loss=1.7490410804748535
I0129 20:23:51.354551 140005305468672 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.744396924972534, loss=1.684706687927246
I0129 20:24:24.737805 140004608161536 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.612015962600708, loss=1.6152070760726929
I0129 20:24:58.114130 140005305468672 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.6954402923583984, loss=1.6692839860916138
I0129 20:25:31.528215 140004608161536 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.460129499435425, loss=1.7911880016326904
I0129 20:26:04.928767 140005305468672 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.702845335006714, loss=1.6651949882507324
I0129 20:26:38.331122 140004608161536 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.8108863830566406, loss=1.7726620435714722
I0129 20:26:43.241970 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:26:49.679372 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:26:58.375124 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:27:01.086818 140169137129280 submission_runner.py:408] Time since start: 45464.13s, 	Step: 131116, 	{'train/accuracy': 0.6050900816917419, 'train/loss': 1.635583519935608, 'validation/accuracy': 0.5597000122070312, 'validation/loss': 1.9223034381866455, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.7411255836486816, 'test/num_examples': 10000, 'score': 43901.29053258896, 'total_duration': 45464.13428735733, 'accumulated_submission_time': 43901.29053258896, 'accumulated_eval_time': 1554.9034173488617, 'accumulated_logging_time': 3.656316041946411}
I0129 20:27:01.130576 140005297075968 logging_writer.py:48] [131116] accumulated_eval_time=1554.903417, accumulated_logging_time=3.656316, accumulated_submission_time=43901.290533, global_step=131116, preemption_count=0, score=43901.290533, test/accuracy=0.433100, test/loss=2.741126, test/num_examples=10000, total_duration=45464.134287, train/accuracy=0.605090, train/loss=1.635584, validation/accuracy=0.559700, validation/loss=1.922303, validation/num_examples=50000
I0129 20:27:29.502021 140005330646784 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.6736135482788086, loss=1.675291657447815
I0129 20:28:02.865808 140005297075968 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.698554754257202, loss=1.7034070491790771
I0129 20:28:36.245239 140005330646784 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.6114501953125, loss=1.673464298248291
I0129 20:29:09.622652 140005297075968 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.243840217590332, loss=1.8729568719863892
I0129 20:29:43.042304 140005330646784 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.6802515983581543, loss=1.711124062538147
I0129 20:30:16.430122 140005297075968 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.944066047668457, loss=1.6830742359161377
I0129 20:30:49.831015 140005330646784 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.620690107345581, loss=1.6994380950927734
I0129 20:31:23.241262 140005297075968 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.6351027488708496, loss=1.6181285381317139
I0129 20:31:56.641979 140005330646784 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.5685315132141113, loss=1.6732264757156372
I0129 20:32:30.040979 140005297075968 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.597252368927002, loss=1.5896506309509277
I0129 20:33:03.446449 140005330646784 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.7336647510528564, loss=1.643268346786499
I0129 20:33:36.933021 140005297075968 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.902219533920288, loss=1.652612566947937
I0129 20:34:10.331988 140005330646784 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.8917236328125, loss=1.8369494676589966
I0129 20:34:43.714402 140005297075968 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.5403664112091064, loss=1.6030254364013672
I0129 20:35:17.105626 140005330646784 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.8270251750946045, loss=1.617287278175354
I0129 20:35:31.289785 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:35:37.650993 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:35:46.152339 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:35:48.961746 140169137129280 submission_runner.py:408] Time since start: 45992.01s, 	Step: 132644, 	{'train/accuracy': 0.6350247263908386, 'train/loss': 1.4889466762542725, 'validation/accuracy': 0.5819799900054932, 'validation/loss': 1.7952096462249756, 'validation/num_examples': 50000, 'test/accuracy': 0.45750001072883606, 'test/loss': 2.617147445678711, 'test/num_examples': 10000, 'score': 44411.385907649994, 'total_duration': 45992.00921392441, 'accumulated_submission_time': 44411.385907649994, 'accumulated_eval_time': 1572.5753400325775, 'accumulated_logging_time': 3.7136833667755127}
I0129 20:35:49.006488 140004624946944 logging_writer.py:48] [132644] accumulated_eval_time=1572.575340, accumulated_logging_time=3.713683, accumulated_submission_time=44411.385908, global_step=132644, preemption_count=0, score=44411.385908, test/accuracy=0.457500, test/loss=2.617147, test/num_examples=10000, total_duration=45992.009214, train/accuracy=0.635025, train/loss=1.488947, validation/accuracy=0.581980, validation/loss=1.795210, validation/num_examples=50000
I0129 20:36:08.019821 140005288683264 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.0006587505340576, loss=1.6462651491165161
I0129 20:36:41.360790 140004624946944 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.5167551040649414, loss=1.6104328632354736
I0129 20:37:14.734302 140005288683264 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.9655354022979736, loss=1.8279807567596436
I0129 20:37:48.114474 140004624946944 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.704012632369995, loss=1.6518722772598267
I0129 20:38:21.505676 140005288683264 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.650188684463501, loss=1.6540558338165283
I0129 20:38:54.899032 140004624946944 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.730038642883301, loss=1.704635739326477
I0129 20:39:28.303061 140005288683264 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.729687213897705, loss=1.681550145149231
I0129 20:40:01.793337 140004624946944 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.9106593132019043, loss=1.591285228729248
I0129 20:40:35.198892 140005288683264 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.692889451980591, loss=1.6887097358703613
I0129 20:41:08.598590 140004624946944 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.1483938694000244, loss=1.7430099248886108
I0129 20:41:41.999700 140005288683264 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.5913503170013428, loss=1.6464532613754272
I0129 20:42:15.392685 140004624946944 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.474371910095215, loss=1.6167556047439575
I0129 20:42:48.789013 140005288683264 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.8275699615478516, loss=1.707364797592163
I0129 20:43:22.184991 140004624946944 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.8551509380340576, loss=1.5872570276260376
I0129 20:43:55.587077 140005288683264 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.821739912033081, loss=1.7322986125946045
I0129 20:44:19.101577 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:44:25.455476 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:44:33.900259 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:44:36.625187 140169137129280 submission_runner.py:408] Time since start: 46519.67s, 	Step: 134172, 	{'train/accuracy': 0.6775948405265808, 'train/loss': 1.2720587253570557, 'validation/accuracy': 0.6111199855804443, 'validation/loss': 1.6209615468978882, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.4012057781219482, 'test/num_examples': 10000, 'score': 44921.41864657402, 'total_duration': 46519.672657966614, 'accumulated_submission_time': 44921.41864657402, 'accumulated_eval_time': 1590.098914861679, 'accumulated_logging_time': 3.7708654403686523}
I0129 20:44:36.665323 140005313861376 logging_writer.py:48] [134172] accumulated_eval_time=1590.098915, accumulated_logging_time=3.770865, accumulated_submission_time=44921.418647, global_step=134172, preemption_count=0, score=44921.418647, test/accuracy=0.480900, test/loss=2.401206, test/num_examples=10000, total_duration=46519.672658, train/accuracy=0.677595, train/loss=1.272059, validation/accuracy=0.611120, validation/loss=1.620962, validation/num_examples=50000
I0129 20:44:46.341898 140005322254080 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.521254301071167, loss=1.561093807220459
I0129 20:45:19.682895 140005313861376 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.7972970008850098, loss=1.6864889860153198
I0129 20:45:53.055800 140005322254080 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.6236886978149414, loss=1.6375960111618042
I0129 20:46:26.532087 140005313861376 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.9265565872192383, loss=1.6764318943023682
I0129 20:46:59.911968 140005322254080 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.887561082839966, loss=1.7263648509979248
I0129 20:47:33.299375 140005313861376 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.941040515899658, loss=1.6756573915481567
I0129 20:48:06.689370 140005322254080 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.7205028533935547, loss=1.601492166519165
I0129 20:48:40.088809 140005313861376 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.6284472942352295, loss=1.565549612045288
I0129 20:49:13.479062 140005322254080 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.9346859455108643, loss=1.6998305320739746
I0129 20:49:46.854201 140005313861376 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.0201950073242188, loss=1.7438994646072388
I0129 20:50:20.243458 140005322254080 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.654452323913574, loss=1.6203407049179077
I0129 20:50:53.629024 140005313861376 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.8188438415527344, loss=1.6151609420776367
I0129 20:51:27.000152 140005322254080 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.7800838947296143, loss=1.6210001707077026
I0129 20:52:00.383757 140005313861376 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.7856407165527344, loss=1.5499248504638672
I0129 20:52:33.894445 140005322254080 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.91445255279541, loss=1.6022586822509766
I0129 20:53:06.770143 140169137129280 spec.py:321] Evaluating on the training split.
I0129 20:53:13.118886 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 20:53:21.956249 140169137129280 spec.py:349] Evaluating on the test split.
I0129 20:53:24.653019 140169137129280 submission_runner.py:408] Time since start: 47047.70s, 	Step: 135700, 	{'train/accuracy': 0.6362802982330322, 'train/loss': 1.4815256595611572, 'validation/accuracy': 0.5810799598693848, 'validation/loss': 1.7779004573822021, 'validation/num_examples': 50000, 'test/accuracy': 0.4642000198364258, 'test/loss': 2.5052618980407715, 'test/num_examples': 10000, 'score': 45431.459854364395, 'total_duration': 47047.70048522949, 'accumulated_submission_time': 45431.459854364395, 'accumulated_eval_time': 1607.9817507266998, 'accumulated_logging_time': 3.824446439743042}
I0129 20:53:24.694231 140005297075968 logging_writer.py:48] [135700] accumulated_eval_time=1607.981751, accumulated_logging_time=3.824446, accumulated_submission_time=45431.459854, global_step=135700, preemption_count=0, score=45431.459854, test/accuracy=0.464200, test/loss=2.505262, test/num_examples=10000, total_duration=47047.700485, train/accuracy=0.636280, train/loss=1.481526, validation/accuracy=0.581080, validation/loss=1.777900, validation/num_examples=50000
I0129 20:53:25.038052 140005305468672 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.028710126876831, loss=1.7160193920135498
I0129 20:53:58.383672 140005297075968 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.890288829803467, loss=1.5998505353927612
I0129 20:54:31.737902 140005305468672 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.7498910427093506, loss=1.5730273723602295
I0129 20:55:05.122587 140005297075968 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.210444688796997, loss=1.6955876350402832
I0129 20:55:38.504098 140005305468672 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.0323851108551025, loss=1.6380376815795898
I0129 20:56:11.917519 140005297075968 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.8773679733276367, loss=1.6165714263916016
I0129 20:56:45.319483 140005305468672 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.025669574737549, loss=1.6798251867294312
I0129 20:57:18.721649 140005297075968 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.8195292949676514, loss=1.6482946872711182
I0129 20:57:52.119515 140005305468672 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.0675501823425293, loss=1.7023882865905762
I0129 20:58:25.517753 140005297075968 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.1075501441955566, loss=1.6096314191818237
I0129 20:58:58.998134 140005305468672 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.072282314300537, loss=1.6124114990234375
I0129 20:59:32.387889 140005297075968 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.7554666996002197, loss=1.6107347011566162
I0129 21:00:05.784754 140005305468672 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.229810953140259, loss=1.654658555984497
I0129 21:00:39.172710 140005297075968 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.9152932167053223, loss=1.5777735710144043
I0129 21:01:12.565017 140005305468672 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.0282952785491943, loss=1.5781432390213013
I0129 21:01:45.961358 140005297075968 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.735203742980957, loss=1.5484806299209595
I0129 21:01:54.786434 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:02:01.188185 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:02:09.961056 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:02:12.664105 140169137129280 submission_runner.py:408] Time since start: 47575.71s, 	Step: 137228, 	{'train/accuracy': 0.672273576259613, 'train/loss': 1.3039746284484863, 'validation/accuracy': 0.6099199652671814, 'validation/loss': 1.6179298162460327, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.3209965229034424, 'test/num_examples': 10000, 'score': 45941.489257097244, 'total_duration': 47575.711570978165, 'accumulated_submission_time': 45941.489257097244, 'accumulated_eval_time': 1625.8593764305115, 'accumulated_logging_time': 3.8787858486175537}
I0129 21:02:12.705760 140005288683264 logging_writer.py:48] [137228] accumulated_eval_time=1625.859376, accumulated_logging_time=3.878786, accumulated_submission_time=45941.489257, global_step=137228, preemption_count=0, score=45941.489257, test/accuracy=0.497700, test/loss=2.320997, test/num_examples=10000, total_duration=47575.711571, train/accuracy=0.672274, train/loss=1.303975, validation/accuracy=0.609920, validation/loss=1.617930, validation/num_examples=50000
I0129 21:02:37.086862 140005313861376 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.0687851905822754, loss=1.5479539632797241
I0129 21:03:10.416952 140005288683264 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.8690598011016846, loss=1.5930771827697754
I0129 21:03:43.803085 140005313861376 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.133786916732788, loss=1.7102640867233276
I0129 21:04:17.202279 140005288683264 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.9677863121032715, loss=1.5554593801498413
I0129 21:04:50.605539 140005313861376 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.8986847400665283, loss=1.5872472524642944
I0129 21:05:24.074721 140005288683264 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.0010671615600586, loss=1.5887539386749268
I0129 21:05:57.473701 140005313861376 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.9849560260772705, loss=1.5116217136383057
I0129 21:06:30.861654 140005288683264 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.8934366703033447, loss=1.513198971748352
I0129 21:07:04.239975 140005313861376 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.198629379272461, loss=1.6975802183151245
I0129 21:07:37.646560 140005288683264 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.019249200820923, loss=1.5790504217147827
I0129 21:08:11.045037 140005313861376 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.7523250579833984, loss=1.5195503234863281
I0129 21:08:44.429363 140005288683264 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.9425301551818848, loss=1.604344367980957
I0129 21:09:17.817234 140005313861376 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.3937063217163086, loss=1.6465522050857544
I0129 21:09:51.206825 140005288683264 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.07232666015625, loss=1.5904159545898438
I0129 21:10:24.604455 140005313861376 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.729465961456299, loss=1.57301664352417
I0129 21:10:42.779570 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:10:49.196033 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:10:57.973634 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:11:00.631069 140169137129280 submission_runner.py:408] Time since start: 48103.68s, 	Step: 138756, 	{'train/accuracy': 0.6704400181770325, 'train/loss': 1.3004449605941772, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.5722438097000122, 'validation/num_examples': 50000, 'test/accuracy': 0.49250003695487976, 'test/loss': 2.3523638248443604, 'test/num_examples': 10000, 'score': 46451.5012075901, 'total_duration': 48103.67853784561, 'accumulated_submission_time': 46451.5012075901, 'accumulated_eval_time': 1643.7108445167542, 'accumulated_logging_time': 3.9324073791503906}
I0129 21:11:00.673367 140005288683264 logging_writer.py:48] [138756] accumulated_eval_time=1643.710845, accumulated_logging_time=3.932407, accumulated_submission_time=46451.501208, global_step=138756, preemption_count=0, score=46451.501208, test/accuracy=0.492500, test/loss=2.352364, test/num_examples=10000, total_duration=48103.678538, train/accuracy=0.670440, train/loss=1.300445, validation/accuracy=0.620280, validation/loss=1.572244, validation/num_examples=50000
I0129 21:11:15.684271 140005297075968 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.2241759300231934, loss=1.650346279144287
I0129 21:11:49.120367 140005288683264 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.2829267978668213, loss=1.6736527681350708
I0129 21:12:22.501325 140005297075968 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.118553400039673, loss=1.6455347537994385
I0129 21:12:55.883518 140005288683264 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.9189329147338867, loss=1.547815203666687
I0129 21:13:29.274931 140005297075968 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.982459545135498, loss=1.6109216213226318
I0129 21:14:02.682312 140005288683264 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.2245030403137207, loss=1.468900203704834
I0129 21:14:36.083538 140005297075968 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.0840041637420654, loss=1.5871223211288452
I0129 21:15:09.477563 140005288683264 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.8597140312194824, loss=1.4961333274841309
I0129 21:15:42.853012 140005297075968 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.039530038833618, loss=1.5673233270645142
I0129 21:16:16.268155 140005288683264 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.8846921920776367, loss=1.5069371461868286
I0129 21:16:49.660741 140005297075968 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.39578914642334, loss=1.5485113859176636
I0129 21:17:23.489616 140005288683264 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.8742246627807617, loss=1.5341131687164307
I0129 21:17:56.918731 140005297075968 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.1050286293029785, loss=1.4994721412658691
I0129 21:18:30.409236 140005288683264 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.1964495182037354, loss=1.5959445238113403
I0129 21:19:03.813815 140005297075968 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.9866130352020264, loss=1.5149459838867188
I0129 21:19:30.673383 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:19:37.053478 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:19:45.561494 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:19:48.225302 140169137129280 submission_runner.py:408] Time since start: 48631.27s, 	Step: 140282, 	{'train/accuracy': 0.49563536047935486, 'train/loss': 2.2361655235290527, 'validation/accuracy': 0.46785998344421387, 'validation/loss': 2.429154634475708, 'validation/num_examples': 50000, 'test/accuracy': 0.355400025844574, 'test/loss': 3.248229503631592, 'test/num_examples': 10000, 'score': 46961.43891119957, 'total_duration': 48631.272773981094, 'accumulated_submission_time': 46961.43891119957, 'accumulated_eval_time': 1661.262745141983, 'accumulated_logging_time': 3.98689866065979}
I0129 21:19:48.271741 140004624946944 logging_writer.py:48] [140282] accumulated_eval_time=1661.262745, accumulated_logging_time=3.986899, accumulated_submission_time=46961.438911, global_step=140282, preemption_count=0, score=46961.438911, test/accuracy=0.355400, test/loss=3.248230, test/num_examples=10000, total_duration=48631.272774, train/accuracy=0.495635, train/loss=2.236166, validation/accuracy=0.467860, validation/loss=2.429155, validation/num_examples=50000
I0129 21:19:54.629544 140005288683264 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.7780184745788574, loss=1.4859116077423096
I0129 21:20:27.949843 140004624946944 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.408048629760742, loss=1.5779979228973389
I0129 21:21:01.308077 140005288683264 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.93870210647583, loss=1.5028879642486572
I0129 21:21:34.706061 140004624946944 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.158808708190918, loss=1.6381274461746216
I0129 21:22:08.105724 140005288683264 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.195026159286499, loss=1.5736265182495117
I0129 21:22:41.486522 140004624946944 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.1951308250427246, loss=1.441434383392334
I0129 21:23:14.874105 140005288683264 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.538135290145874, loss=1.5287736654281616
I0129 21:23:48.256164 140004624946944 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.9650931358337402, loss=1.3954194784164429
I0129 21:24:21.646311 140005288683264 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.2475504875183105, loss=1.511665940284729
I0129 21:24:55.108926 140004624946944 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.076810598373413, loss=1.4522498846054077
I0129 21:25:28.505164 140005288683264 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.9234039783477783, loss=1.680352807044983
I0129 21:26:01.915275 140004624946944 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.4352619647979736, loss=1.6692614555358887
I0129 21:26:35.317635 140005288683264 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.1158576011657715, loss=1.6364991664886475
I0129 21:27:08.713007 140004624946944 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.1172373294830322, loss=1.6109730005264282
I0129 21:27:42.121085 140005288683264 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.215980291366577, loss=1.5509103536605835
I0129 21:28:15.554393 140004624946944 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.428753137588501, loss=1.5762399435043335
I0129 21:28:18.375256 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:28:24.749567 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:28:33.581569 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:28:36.267777 140169137129280 submission_runner.py:408] Time since start: 49159.32s, 	Step: 141810, 	{'train/accuracy': 0.7058752775192261, 'train/loss': 1.1420633792877197, 'validation/accuracy': 0.6265599727630615, 'validation/loss': 1.5559496879577637, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.3541271686553955, 'test/num_examples': 10000, 'score': 47471.4825797081, 'total_duration': 49159.315249443054, 'accumulated_submission_time': 47471.4825797081, 'accumulated_eval_time': 1679.1552288532257, 'accumulated_logging_time': 4.043623924255371}
I0129 21:28:36.313333 140005305468672 logging_writer.py:48] [141810] accumulated_eval_time=1679.155229, accumulated_logging_time=4.043624, accumulated_submission_time=47471.482580, global_step=141810, preemption_count=0, score=47471.482580, test/accuracy=0.493400, test/loss=2.354127, test/num_examples=10000, total_duration=49159.315249, train/accuracy=0.705875, train/loss=1.142063, validation/accuracy=0.626560, validation/loss=1.555950, validation/num_examples=50000
I0129 21:29:06.651833 140005313861376 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.2625434398651123, loss=1.5410797595977783
I0129 21:29:40.008992 140005305468672 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.1668708324432373, loss=1.5876755714416504
I0129 21:30:13.421571 140005313861376 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.4441077709198, loss=1.6255956888198853
I0129 21:30:46.807959 140005305468672 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.0837106704711914, loss=1.5403069257736206
I0129 21:31:20.305123 140005313861376 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.077241897583008, loss=1.4373021125793457
I0129 21:31:53.708012 140005305468672 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.05317759513855, loss=1.5480588674545288
I0129 21:32:27.098626 140005313861376 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.992938756942749, loss=1.4051802158355713
I0129 21:33:00.486501 140005305468672 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.0052287578582764, loss=1.3977092504501343
I0129 21:33:33.879425 140005313861376 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.9647154808044434, loss=1.5311460494995117
I0129 21:34:07.273192 140005305468672 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.155895471572876, loss=1.4243168830871582
I0129 21:34:40.674603 140005313861376 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.0340495109558105, loss=1.5966418981552124
I0129 21:35:14.059191 140005305468672 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.399007558822632, loss=1.5710649490356445
I0129 21:35:47.460692 140005313861376 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.2170798778533936, loss=1.478090524673462
I0129 21:36:20.850628 140005305468672 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.254114866256714, loss=1.535590410232544
I0129 21:36:54.247731 140005313861376 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.1281025409698486, loss=1.515116810798645
I0129 21:37:06.423439 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:37:13.052616 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:37:21.799499 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:37:24.471131 140169137129280 submission_runner.py:408] Time since start: 49687.52s, 	Step: 143338, 	{'train/accuracy': 0.7002949714660645, 'train/loss': 1.1588937044143677, 'validation/accuracy': 0.6348199844360352, 'validation/loss': 1.5233947038650513, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2824220657348633, 'test/num_examples': 10000, 'score': 47981.5309278965, 'total_duration': 49687.51860022545, 'accumulated_submission_time': 47981.5309278965, 'accumulated_eval_time': 1697.2029082775116, 'accumulated_logging_time': 4.101156234741211}
I0129 21:37:24.518498 140004624946944 logging_writer.py:48] [143338] accumulated_eval_time=1697.202908, accumulated_logging_time=4.101156, accumulated_submission_time=47981.530928, global_step=143338, preemption_count=0, score=47981.530928, test/accuracy=0.505900, test/loss=2.282422, test/num_examples=10000, total_duration=49687.518600, train/accuracy=0.700295, train/loss=1.158894, validation/accuracy=0.634820, validation/loss=1.523395, validation/num_examples=50000
I0129 21:37:45.538692 140005288683264 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.1848583221435547, loss=1.4828304052352905
I0129 21:38:18.873809 140004624946944 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.4767470359802246, loss=1.4816968441009521
I0129 21:38:52.270720 140005288683264 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.034149169921875, loss=1.4101357460021973
I0129 21:39:25.648723 140004624946944 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.282733917236328, loss=1.5391709804534912
I0129 21:39:59.056011 140005288683264 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.2678942680358887, loss=1.5590553283691406
I0129 21:40:32.470226 140004624946944 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.3507790565490723, loss=1.4846694469451904
I0129 21:41:05.862307 140005288683264 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.9933836460113525, loss=1.5442922115325928
I0129 21:41:39.258958 140004624946944 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.143263816833496, loss=1.5062042474746704
I0129 21:42:12.646846 140005288683264 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.2906734943389893, loss=1.5293048620224
I0129 21:42:46.052898 140004624946944 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.35566782951355, loss=1.5622212886810303
I0129 21:43:19.443303 140005288683264 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.388535499572754, loss=1.5063717365264893
I0129 21:43:52.919915 140004624946944 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.404257297515869, loss=1.541118860244751
I0129 21:44:26.311913 140005288683264 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.235579490661621, loss=1.4264023303985596
I0129 21:44:59.722903 140004624946944 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.281923770904541, loss=1.5983836650848389
I0129 21:45:33.120283 140005288683264 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.410214900970459, loss=1.6434413194656372
I0129 21:45:54.639572 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:46:01.081318 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:46:09.703842 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:46:12.433055 140169137129280 submission_runner.py:408] Time since start: 50215.48s, 	Step: 144866, 	{'train/accuracy': 0.7127311825752258, 'train/loss': 1.1116129159927368, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4744572639465332, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.2296669483184814, 'test/num_examples': 10000, 'score': 48491.59139537811, 'total_duration': 50215.48052382469, 'accumulated_submission_time': 48491.59139537811, 'accumulated_eval_time': 1714.9963533878326, 'accumulated_logging_time': 4.159027576446533}
I0129 21:46:12.479021 140005313861376 logging_writer.py:48] [144866] accumulated_eval_time=1714.996353, accumulated_logging_time=4.159028, accumulated_submission_time=48491.591395, global_step=144866, preemption_count=0, score=48491.591395, test/accuracy=0.516200, test/loss=2.229667, test/num_examples=10000, total_duration=50215.480524, train/accuracy=0.712731, train/loss=1.111613, validation/accuracy=0.642300, validation/loss=1.474457, validation/num_examples=50000
I0129 21:46:24.166523 140005322254080 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.250657081604004, loss=1.4988652467727661
I0129 21:46:57.517893 140005313861376 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.4350228309631348, loss=1.5415045022964478
I0129 21:47:30.884896 140005322254080 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.499678134918213, loss=1.4565573930740356
I0129 21:48:04.269371 140005313861376 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.4637088775634766, loss=1.4780255556106567
I0129 21:48:37.643636 140005322254080 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.0885422229766846, loss=1.477702021598816
I0129 21:49:11.037263 140005313861376 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.151883602142334, loss=1.4620752334594727
I0129 21:49:44.448554 140005322254080 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.155303478240967, loss=1.4767465591430664
I0129 21:50:17.958856 140005313861376 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.3956618309020996, loss=1.402901291847229
I0129 21:50:51.341103 140005322254080 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.4405007362365723, loss=1.4726066589355469
I0129 21:51:24.734459 140005313861376 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.304988384246826, loss=1.561659812927246
I0129 21:51:58.123770 140005322254080 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.053593873977661, loss=1.3365952968597412
I0129 21:52:31.518797 140005313861376 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.4679582118988037, loss=1.4650412797927856
I0129 21:53:04.924005 140005322254080 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.596770763397217, loss=1.5322575569152832
I0129 21:53:38.331104 140005313861376 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.601675510406494, loss=1.500391960144043
I0129 21:54:11.729981 140005322254080 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.536088705062866, loss=1.4785473346710205
I0129 21:54:42.592660 140169137129280 spec.py:321] Evaluating on the training split.
I0129 21:54:49.000692 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 21:54:57.398815 140169137129280 spec.py:349] Evaluating on the test split.
I0129 21:55:00.090615 140169137129280 submission_runner.py:408] Time since start: 50743.14s, 	Step: 146394, 	{'train/accuracy': 0.7197863459587097, 'train/loss': 1.077343225479126, 'validation/accuracy': 0.6534799933433533, 'validation/loss': 1.424755334854126, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.159235954284668, 'test/num_examples': 10000, 'score': 49001.642828941345, 'total_duration': 50743.13808107376, 'accumulated_submission_time': 49001.642828941345, 'accumulated_eval_time': 1732.4942715168, 'accumulated_logging_time': 4.217477798461914}
I0129 21:55:00.135095 140005288683264 logging_writer.py:48] [146394] accumulated_eval_time=1732.494272, accumulated_logging_time=4.217478, accumulated_submission_time=49001.642829, global_step=146394, preemption_count=0, score=49001.642829, test/accuracy=0.526000, test/loss=2.159236, test/num_examples=10000, total_duration=50743.138081, train/accuracy=0.719786, train/loss=1.077343, validation/accuracy=0.653480, validation/loss=1.424755, validation/num_examples=50000
I0129 21:55:02.488763 140005297075968 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.5183238983154297, loss=1.4960554838180542
I0129 21:55:35.837676 140005288683264 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.3005669116973877, loss=1.4081264734268188
I0129 21:56:09.198662 140005297075968 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.3151168823242188, loss=1.5334563255310059
I0129 21:56:42.678045 140005288683264 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.380687713623047, loss=1.493918538093567
I0129 21:57:16.087007 140005297075968 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.74615216255188, loss=1.4559478759765625
I0129 21:57:49.488766 140005288683264 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.513478994369507, loss=1.5617820024490356
I0129 21:58:22.882710 140005297075968 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.438803195953369, loss=1.4713237285614014
I0129 21:58:56.285889 140005288683264 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.514808177947998, loss=1.5564608573913574
I0129 21:59:29.685076 140005297075968 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.3997302055358887, loss=1.3924046754837036
I0129 22:00:03.089051 140005288683264 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.3816332817077637, loss=1.495204210281372
I0129 22:00:36.482140 140005297075968 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.513261556625366, loss=1.5768444538116455
I0129 22:01:09.889033 140005288683264 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.8737986087799072, loss=1.5095688104629517
I0129 22:01:43.297342 140005297075968 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.3912875652313232, loss=1.427467703819275
I0129 22:02:16.684322 140005288683264 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.4420671463012695, loss=1.465423822402954
I0129 22:02:50.152258 140005297075968 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.4524002075195312, loss=1.4238340854644775
I0129 22:03:23.564647 140005288683264 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.403242826461792, loss=1.449625015258789
I0129 22:03:30.396718 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:03:36.781131 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:03:45.542398 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:03:48.214539 140169137129280 submission_runner.py:408] Time since start: 51271.26s, 	Step: 147922, 	{'train/accuracy': 0.7180524468421936, 'train/loss': 1.0938856601715088, 'validation/accuracy': 0.6541599631309509, 'validation/loss': 1.4333339929580688, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1575634479522705, 'test/num_examples': 10000, 'score': 49511.839173316956, 'total_duration': 51271.2620010376, 'accumulated_submission_time': 49511.839173316956, 'accumulated_eval_time': 1750.3120419979095, 'accumulated_logging_time': 4.277265548706055}
I0129 22:03:48.263964 140004624946944 logging_writer.py:48] [147922] accumulated_eval_time=1750.312042, accumulated_logging_time=4.277266, accumulated_submission_time=49511.839173, global_step=147922, preemption_count=0, score=49511.839173, test/accuracy=0.526900, test/loss=2.157563, test/num_examples=10000, total_duration=51271.262001, train/accuracy=0.718052, train/loss=1.093886, validation/accuracy=0.654160, validation/loss=1.433334, validation/num_examples=50000
I0129 22:04:14.630457 140005288683264 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.5539443492889404, loss=1.4342670440673828
I0129 22:04:47.990674 140004624946944 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.508265972137451, loss=1.4902406930923462
I0129 22:05:21.372307 140005288683264 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.343405246734619, loss=1.4287384748458862
I0129 22:05:54.760528 140004624946944 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.680936098098755, loss=1.4042925834655762
I0129 22:06:28.139456 140005288683264 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.5886032581329346, loss=1.4997988939285278
I0129 22:07:01.526005 140004624946944 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.4613146781921387, loss=1.36912202835083
I0129 22:07:34.936003 140005288683264 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.7533152103424072, loss=1.4943296909332275
I0129 22:08:08.317242 140004624946944 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.2644481658935547, loss=1.4369494915008545
I0129 22:08:41.725793 140005288683264 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.304659366607666, loss=1.5347111225128174
I0129 22:09:15.184998 140004624946944 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.5151607990264893, loss=1.385239839553833
I0129 22:09:48.575841 140005288683264 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.4899981021881104, loss=1.3944354057312012
I0129 22:10:21.958916 140004624946944 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.2897274494171143, loss=1.3264379501342773
I0129 22:10:55.362814 140005288683264 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.7942748069763184, loss=1.3770233392715454
I0129 22:11:28.765093 140004624946944 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.2812514305114746, loss=1.435673475265503
I0129 22:12:02.171509 140005288683264 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.6674091815948486, loss=1.4805586338043213
I0129 22:12:18.347282 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:12:24.845473 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:12:33.658643 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:12:36.342701 140169137129280 submission_runner.py:408] Time since start: 51799.39s, 	Step: 149450, 	{'train/accuracy': 0.7408322691917419, 'train/loss': 0.9889224767684937, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.3719351291656494, 'validation/num_examples': 50000, 'test/accuracy': 0.5319000482559204, 'test/loss': 2.132917881011963, 'test/num_examples': 10000, 'score': 50021.862213134766, 'total_duration': 51799.39016842842, 'accumulated_submission_time': 50021.862213134766, 'accumulated_eval_time': 1768.307421207428, 'accumulated_logging_time': 4.336929559707642}
I0129 22:12:36.387083 140004616554240 logging_writer.py:48] [149450] accumulated_eval_time=1768.307421, accumulated_logging_time=4.336930, accumulated_submission_time=50021.862213, global_step=149450, preemption_count=0, score=50021.862213, test/accuracy=0.531900, test/loss=2.132918, test/num_examples=10000, total_duration=51799.390168, train/accuracy=0.740832, train/loss=0.988922, validation/accuracy=0.667020, validation/loss=1.371935, validation/num_examples=50000
I0129 22:12:53.401738 140004624946944 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.7283992767333984, loss=1.4748318195343018
I0129 22:13:26.735000 140004616554240 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.8876302242279053, loss=1.5195013284683228
I0129 22:14:00.106319 140004624946944 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.8208913803100586, loss=1.4122936725616455
I0129 22:14:33.490129 140004616554240 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.757199287414551, loss=1.4993703365325928
I0129 22:15:06.885153 140004624946944 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.9060583114624023, loss=1.4073344469070435
I0129 22:15:40.358164 140004616554240 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.5660605430603027, loss=1.4733083248138428
I0129 22:16:13.753728 140004624946944 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.6334331035614014, loss=1.36833655834198
I0129 22:16:47.157684 140004616554240 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.4719667434692383, loss=1.4832606315612793
I0129 22:17:20.562884 140004624946944 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.667989253997803, loss=1.540286660194397
I0129 22:17:53.963590 140004616554240 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.512021064758301, loss=1.3888542652130127
I0129 22:18:27.374138 140004624946944 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.432368755340576, loss=1.4082741737365723
I0129 22:19:00.757274 140004616554240 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.7565016746520996, loss=1.4712297916412354
I0129 22:19:34.149280 140004624946944 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.7625272274017334, loss=1.4056087732315063
I0129 22:20:07.547727 140004616554240 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.726414442062378, loss=1.3866496086120605
I0129 22:20:40.934647 140004624946944 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.598306179046631, loss=1.4209002256393433
I0129 22:21:06.461663 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:21:12.821023 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:21:21.332401 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:21:24.037078 140169137129280 submission_runner.py:408] Time since start: 52327.08s, 	Step: 150978, 	{'train/accuracy': 0.7679368257522583, 'train/loss': 0.8675883412361145, 'validation/accuracy': 0.6796999573707581, 'validation/loss': 1.3259252309799194, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0598459243774414, 'test/num_examples': 10000, 'score': 50531.87656021118, 'total_duration': 52327.08454442024, 'accumulated_submission_time': 50531.87656021118, 'accumulated_eval_time': 1785.8828003406525, 'accumulated_logging_time': 4.391806602478027}
I0129 22:21:24.084447 140005297075968 logging_writer.py:48] [150978] accumulated_eval_time=1785.882800, accumulated_logging_time=4.391807, accumulated_submission_time=50531.876560, global_step=150978, preemption_count=0, score=50531.876560, test/accuracy=0.546900, test/loss=2.059846, test/num_examples=10000, total_duration=52327.084544, train/accuracy=0.767937, train/loss=0.867588, validation/accuracy=0.679700, validation/loss=1.325925, validation/num_examples=50000
I0129 22:21:31.762762 140005313861376 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.043655872344971, loss=1.3759742975234985
I0129 22:22:05.186886 140005297075968 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.5824644565582275, loss=1.352811336517334
I0129 22:22:38.567825 140005313861376 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.787785291671753, loss=1.3745571374893188
I0129 22:23:11.957259 140005297075968 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.6744296550750732, loss=1.3667988777160645
I0129 22:23:45.356457 140005313861376 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.6386544704437256, loss=1.3836939334869385
I0129 22:24:18.772496 140005297075968 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.624890089035034, loss=1.4899107217788696
I0129 22:24:52.167014 140005313861376 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.8844194412231445, loss=1.535995364189148
I0129 22:25:25.562872 140005297075968 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.7461397647857666, loss=1.4369648694992065
I0129 22:25:58.958309 140005313861376 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.6948087215423584, loss=1.3534832000732422
I0129 22:26:32.363094 140005297075968 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.196871757507324, loss=1.3883863687515259
I0129 22:27:05.765101 140005313861376 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.834442377090454, loss=1.3769217729568481
I0129 22:27:39.150017 140005297075968 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.7820000648498535, loss=1.368071436882019
I0129 22:28:12.532455 140005313861376 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.778367519378662, loss=1.3006086349487305
I0129 22:28:46.024609 140005297075968 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.888094902038574, loss=1.360124111175537
I0129 22:29:19.425791 140005313861376 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.7810192108154297, loss=1.2759848833084106
I0129 22:29:52.828631 140005297075968 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.896928548812866, loss=1.3053723573684692
I0129 22:29:54.328215 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:30:00.808050 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:30:09.551069 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:30:12.238735 140169137129280 submission_runner.py:408] Time since start: 52855.29s, 	Step: 152506, 	{'train/accuracy': 0.7592275142669678, 'train/loss': 0.9073374271392822, 'validation/accuracy': 0.6794599890708923, 'validation/loss': 1.3260866403579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.066560745239258, 'test/num_examples': 10000, 'score': 51042.059888124466, 'total_duration': 52855.28619623184, 'accumulated_submission_time': 51042.059888124466, 'accumulated_eval_time': 1803.7932722568512, 'accumulated_logging_time': 4.44978928565979}
I0129 22:30:12.286753 140005305468672 logging_writer.py:48] [152506] accumulated_eval_time=1803.793272, accumulated_logging_time=4.449789, accumulated_submission_time=51042.059888, global_step=152506, preemption_count=0, score=51042.059888, test/accuracy=0.554100, test/loss=2.066561, test/num_examples=10000, total_duration=52855.286196, train/accuracy=0.759228, train/loss=0.907337, validation/accuracy=0.679460, validation/loss=1.326087, validation/num_examples=50000
I0129 22:30:43.974970 140005322254080 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.883924722671509, loss=1.5083210468292236
I0129 22:31:17.335339 140005305468672 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.97147536277771, loss=1.412508487701416
I0129 22:31:50.740100 140005322254080 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.587141275405884, loss=1.415130376815796
I0129 22:32:24.142270 140005305468672 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.6861562728881836, loss=1.4018638134002686
I0129 22:32:57.542276 140005322254080 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.252994537353516, loss=1.372875452041626
I0129 22:33:30.925995 140005305468672 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.691162347793579, loss=1.381038784980774
I0129 22:34:04.343057 140005322254080 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.9042809009552, loss=1.3915622234344482
I0129 22:34:37.754333 140005305468672 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.958547830581665, loss=1.5480401515960693
I0129 22:35:11.256185 140005322254080 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.993316888809204, loss=1.341636300086975
I0129 22:35:44.645181 140005305468672 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.8594305515289307, loss=1.4030263423919678
I0129 22:36:18.047497 140005322254080 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.8284220695495605, loss=1.2335264682769775
I0129 22:36:51.449411 140005305468672 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.7013423442840576, loss=1.3216660022735596
I0129 22:37:24.868454 140005322254080 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.607677698135376, loss=1.3624358177185059
I0129 22:37:58.266861 140005305468672 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.803061008453369, loss=1.3114787340164185
I0129 22:38:31.671322 140005322254080 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.913113594055176, loss=1.311480164527893
I0129 22:38:42.512466 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:38:48.962436 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:38:57.675210 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:39:00.369909 140169137129280 submission_runner.py:408] Time since start: 53383.42s, 	Step: 154034, 	{'train/accuracy': 0.7609016299247742, 'train/loss': 0.9098778963088989, 'validation/accuracy': 0.6835599541664124, 'validation/loss': 1.3010308742523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.03236985206604, 'test/num_examples': 10000, 'score': 51552.22501087189, 'total_duration': 53383.41737794876, 'accumulated_submission_time': 51552.22501087189, 'accumulated_eval_time': 1821.6506774425507, 'accumulated_logging_time': 4.508504867553711}
I0129 22:39:00.414640 140004624946944 logging_writer.py:48] [154034] accumulated_eval_time=1821.650677, accumulated_logging_time=4.508505, accumulated_submission_time=51552.225011, global_step=154034, preemption_count=0, score=51552.225011, test/accuracy=0.552400, test/loss=2.032370, test/num_examples=10000, total_duration=53383.417378, train/accuracy=0.760902, train/loss=0.909878, validation/accuracy=0.683560, validation/loss=1.301031, validation/num_examples=50000
I0129 22:39:22.779363 140005288683264 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.881312370300293, loss=1.400647521018982
I0129 22:39:56.138786 140004624946944 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.131985664367676, loss=1.5192383527755737
I0129 22:40:29.534253 140005288683264 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.635141134262085, loss=1.3359373807907104
I0129 22:41:02.919085 140004624946944 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.930943012237549, loss=1.3020153045654297
I0129 22:41:36.424830 140005288683264 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.872072458267212, loss=1.3398537635803223
I0129 22:42:09.813707 140004624946944 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.052874565124512, loss=1.3676412105560303
I0129 22:42:43.220724 140005288683264 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.1604905128479, loss=1.460705280303955
I0129 22:43:16.617063 140004624946944 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.860194683074951, loss=1.2671022415161133
I0129 22:43:50.027376 140005288683264 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.9038901329040527, loss=1.341963529586792
I0129 22:44:23.423656 140004624946944 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.070132732391357, loss=1.4188138246536255
I0129 22:44:56.822065 140005288683264 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.199036598205566, loss=1.4218562841415405
I0129 22:45:30.226097 140004624946944 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.27139949798584, loss=1.3749239444732666
I0129 22:46:03.627169 140005288683264 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.9476852416992188, loss=1.303036093711853
I0129 22:46:37.017360 140004624946944 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.192364692687988, loss=1.3530240058898926
I0129 22:47:10.412378 140005288683264 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.158835411071777, loss=1.268662929534912
I0129 22:47:30.606428 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:47:37.210045 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:47:45.987240 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:47:48.680749 140169137129280 submission_runner.py:408] Time since start: 53911.73s, 	Step: 155562, 	{'train/accuracy': 0.7763671875, 'train/loss': 0.8441674709320068, 'validation/accuracy': 0.6982199549674988, 'validation/loss': 1.2281646728515625, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.9566088914871216, 'test/num_examples': 10000, 'score': 52062.35582947731, 'total_duration': 53911.72821640968, 'accumulated_submission_time': 52062.35582947731, 'accumulated_eval_time': 1839.7249593734741, 'accumulated_logging_time': 4.563934326171875}
I0129 22:47:48.729056 140004608161536 logging_writer.py:48] [155562] accumulated_eval_time=1839.724959, accumulated_logging_time=4.563934, accumulated_submission_time=52062.355829, global_step=155562, preemption_count=0, score=52062.355829, test/accuracy=0.566300, test/loss=1.956609, test/num_examples=10000, total_duration=53911.728216, train/accuracy=0.776367, train/loss=0.844167, validation/accuracy=0.698220, validation/loss=1.228165, validation/num_examples=50000
I0129 22:48:01.742447 140004616554240 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.0957112312316895, loss=1.3191545009613037
I0129 22:48:35.073512 140004608161536 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.6911091804504395, loss=1.3614243268966675
I0129 22:49:08.462616 140004616554240 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.760514497756958, loss=1.2530776262283325
I0129 22:49:41.841486 140004608161536 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.831388473510742, loss=1.3365578651428223
I0129 22:50:15.233224 140004616554240 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.675962448120117, loss=1.3623392581939697
I0129 22:50:48.628945 140004608161536 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.931514263153076, loss=1.409364104270935
I0129 22:51:22.032902 140004616554240 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.325929164886475, loss=1.3769683837890625
I0129 22:51:55.430551 140004608161536 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.02163553237915, loss=1.2471715211868286
I0129 22:52:28.827919 140004616554240 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.182555675506592, loss=1.2819265127182007
I0129 22:53:02.228864 140004608161536 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.079771995544434, loss=1.218376874923706
I0129 22:53:35.632724 140004616554240 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.9217936992645264, loss=1.2853256464004517
I0129 22:54:09.133029 140004608161536 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.98040771484375, loss=1.2343261241912842
I0129 22:54:42.524099 140004616554240 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.140771865844727, loss=1.3843040466308594
I0129 22:55:15.911124 140004608161536 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.9696640968322754, loss=1.317432165145874
I0129 22:55:49.309937 140004616554240 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.365544319152832, loss=1.4178423881530762
I0129 22:56:18.845297 140169137129280 spec.py:321] Evaluating on the training split.
I0129 22:56:25.291229 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 22:56:33.726945 140169137129280 spec.py:349] Evaluating on the test split.
I0129 22:56:36.312294 140169137129280 submission_runner.py:408] Time since start: 54439.36s, 	Step: 157090, 	{'train/accuracy': 0.7760881781578064, 'train/loss': 0.8410550951957703, 'validation/accuracy': 0.6970399618148804, 'validation/loss': 1.2410316467285156, 'validation/num_examples': 50000, 'test/accuracy': 0.5633000135421753, 'test/loss': 1.9879528284072876, 'test/num_examples': 10000, 'score': 52572.40886282921, 'total_duration': 54439.35976409912, 'accumulated_submission_time': 52572.40886282921, 'accumulated_eval_time': 1857.1919219493866, 'accumulated_logging_time': 4.625617980957031}
I0129 22:56:36.360584 140004608161536 logging_writer.py:48] [157090] accumulated_eval_time=1857.191922, accumulated_logging_time=4.625618, accumulated_submission_time=52572.408863, global_step=157090, preemption_count=0, score=52572.408863, test/accuracy=0.563300, test/loss=1.987953, test/num_examples=10000, total_duration=54439.359764, train/accuracy=0.776088, train/loss=0.841055, validation/accuracy=0.697040, validation/loss=1.241032, validation/num_examples=50000
I0129 22:56:40.039140 140004616554240 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.925905466079712, loss=1.2304542064666748
I0129 22:57:13.384576 140004608161536 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.483663082122803, loss=1.331212043762207
I0129 22:57:46.744301 140004616554240 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.471752166748047, loss=1.394331455230713
I0129 22:58:20.139802 140004608161536 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.170481204986572, loss=1.2577478885650635
I0129 22:58:53.529064 140004616554240 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.93021297454834, loss=1.164792537689209
I0129 22:59:26.911168 140004608161536 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.144679069519043, loss=1.3138959407806396
I0129 23:00:00.295837 140004616554240 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.184872150421143, loss=1.1450941562652588
I0129 23:00:33.760377 140004608161536 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.164085388183594, loss=1.3246119022369385
I0129 23:01:07.146419 140004616554240 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.044880390167236, loss=1.2910460233688354
I0129 23:01:40.535743 140004608161536 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.971102476119995, loss=1.309586524963379
I0129 23:02:13.924346 140004616554240 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.193828582763672, loss=1.2567408084869385
I0129 23:02:47.319729 140004608161536 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.249486923217773, loss=1.3732589483261108
I0129 23:03:20.714139 140004616554240 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.184720516204834, loss=1.2766553163528442
I0129 23:03:54.103094 140004608161536 logging_writer.py:48] [158400] global_step=158400, grad_norm=3.9026246070861816, loss=1.19379723072052
I0129 23:04:27.484273 140004616554240 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.052756309509277, loss=1.2436131238937378
I0129 23:05:00.889867 140004608161536 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.498453140258789, loss=1.22600257396698
I0129 23:05:06.378846 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:05:12.747478 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:05:21.576129 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:05:24.443585 140169137129280 submission_runner.py:408] Time since start: 54967.49s, 	Step: 158618, 	{'train/accuracy': 0.8107461333274841, 'train/loss': 0.691912055015564, 'validation/accuracy': 0.7024999856948853, 'validation/loss': 1.2118477821350098, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.921416163444519, 'test/num_examples': 10000, 'score': 53082.36680340767, 'total_duration': 54967.49105381966, 'accumulated_submission_time': 53082.36680340767, 'accumulated_eval_time': 1875.2566223144531, 'accumulated_logging_time': 4.6845269203186035}
I0129 23:05:24.489427 140005305468672 logging_writer.py:48] [158618] accumulated_eval_time=1875.256622, accumulated_logging_time=4.684527, accumulated_submission_time=53082.366803, global_step=158618, preemption_count=0, score=53082.366803, test/accuracy=0.579800, test/loss=1.921416, test/num_examples=10000, total_duration=54967.491054, train/accuracy=0.810746, train/loss=0.691912, validation/accuracy=0.702500, validation/loss=1.211848, validation/num_examples=50000
I0129 23:05:52.209478 140005322254080 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.036801338195801, loss=1.2421889305114746
I0129 23:06:25.560395 140005305468672 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.401575088500977, loss=1.328586459159851
I0129 23:06:59.044639 140005322254080 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.226438522338867, loss=1.4050310850143433
I0129 23:07:32.418147 140005305468672 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.126402378082275, loss=1.259103775024414
I0129 23:08:05.813682 140005322254080 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.422523498535156, loss=1.335010290145874
I0129 23:08:39.204831 140005305468672 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.305379390716553, loss=1.2553856372833252
I0129 23:09:12.598289 140005322254080 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.825385570526123, loss=1.2092127799987793
I0129 23:09:46.013829 140005305468672 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.456833362579346, loss=1.2992209196090698
I0129 23:10:19.409811 140005322254080 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.358603000640869, loss=1.2301892042160034
I0129 23:10:52.813966 140005305468672 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.146446704864502, loss=1.1799451112747192
I0129 23:11:26.209699 140005322254080 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.2042012214660645, loss=1.1749714612960815
I0129 23:11:59.611243 140005305468672 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.049915313720703, loss=1.2384814023971558
I0129 23:12:33.001505 140005322254080 logging_writer.py:48] [159900] global_step=159900, grad_norm=3.9817898273468018, loss=1.1614770889282227
I0129 23:13:06.515381 140005305468672 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.292135715484619, loss=1.3155021667480469
I0129 23:13:39.930893 140005322254080 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.1019768714904785, loss=1.203129529953003
I0129 23:13:54.772401 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:14:01.169567 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:14:09.850122 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:14:12.644094 140169137129280 submission_runner.py:408] Time since start: 55495.69s, 	Step: 160146, 	{'train/accuracy': 0.8052654266357422, 'train/loss': 0.7151588201522827, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.1937520503997803, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.909941554069519, 'test/num_examples': 10000, 'score': 53592.58766222, 'total_duration': 55495.69156050682, 'accumulated_submission_time': 53592.58766222, 'accumulated_eval_time': 1893.1282756328583, 'accumulated_logging_time': 4.742824554443359}
I0129 23:14:12.690798 140004616554240 logging_writer.py:48] [160146] accumulated_eval_time=1893.128276, accumulated_logging_time=4.742825, accumulated_submission_time=53592.587662, global_step=160146, preemption_count=0, score=53592.587662, test/accuracy=0.577600, test/loss=1.909942, test/num_examples=10000, total_duration=55495.691561, train/accuracy=0.805265, train/loss=0.715159, validation/accuracy=0.706040, validation/loss=1.193752, validation/num_examples=50000
I0129 23:14:31.044897 140004624946944 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.4771342277526855, loss=1.4479424953460693
I0129 23:15:04.380428 140004616554240 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.87099552154541, loss=1.3243764638900757
I0129 23:15:37.763089 140004624946944 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.889297962188721, loss=1.2805920839309692
I0129 23:16:11.146802 140004616554240 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.765478610992432, loss=1.2024202346801758
I0129 23:16:44.547677 140004624946944 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.570700645446777, loss=1.31172776222229
I0129 23:17:17.933599 140004616554240 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.317563533782959, loss=1.2784022092819214
I0129 23:17:51.326255 140004624946944 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.594933986663818, loss=1.2573676109313965
I0129 23:18:24.713353 140004616554240 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.331704139709473, loss=1.24006986618042
I0129 23:18:58.145578 140004624946944 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.333440780639648, loss=1.1471598148345947
I0129 23:19:31.621305 140004616554240 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.403886318206787, loss=1.201521873474121
I0129 23:20:05.021122 140004624946944 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.380925178527832, loss=1.2885507345199585
I0129 23:20:38.406962 140004616554240 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.615591049194336, loss=1.2905070781707764
I0129 23:21:11.810665 140004624946944 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.335944652557373, loss=1.1579129695892334
I0129 23:21:45.190826 140004616554240 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.304495334625244, loss=1.197167158126831
I0129 23:22:18.591657 140004624946944 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.259661674499512, loss=1.1873799562454224
I0129 23:22:42.783111 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:22:49.206232 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:22:57.999312 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:23:00.680450 140169137129280 submission_runner.py:408] Time since start: 56023.73s, 	Step: 161674, 	{'train/accuracy': 0.8055843114852905, 'train/loss': 0.7204368710517883, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.1815768480300903, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.902944564819336, 'test/num_examples': 10000, 'score': 54102.61943149567, 'total_duration': 56023.72791719437, 'accumulated_submission_time': 54102.61943149567, 'accumulated_eval_time': 1911.025577545166, 'accumulated_logging_time': 4.800848007202148}
I0129 23:23:00.730078 140005322254080 logging_writer.py:48] [161674] accumulated_eval_time=1911.025578, accumulated_logging_time=4.800848, accumulated_submission_time=54102.619431, global_step=161674, preemption_count=0, score=54102.619431, test/accuracy=0.581500, test/loss=1.902945, test/num_examples=10000, total_duration=56023.727917, train/accuracy=0.805584, train/loss=0.720437, validation/accuracy=0.708420, validation/loss=1.181577, validation/num_examples=50000
I0129 23:23:09.760586 140005330646784 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.514255523681641, loss=1.275400161743164
I0129 23:23:43.103742 140005322254080 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.720082759857178, loss=1.2013978958129883
I0129 23:24:16.471726 140005330646784 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.232346057891846, loss=1.1841106414794922
I0129 23:24:49.857067 140005322254080 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.485413551330566, loss=1.2464772462844849
I0129 23:25:23.263436 140005330646784 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.417578220367432, loss=1.1704609394073486
I0129 23:25:56.745948 140005322254080 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.461259841918945, loss=1.2374573945999146
I0129 23:26:30.137855 140005330646784 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.411190986633301, loss=1.252853274345398
I0129 23:27:03.521168 140005322254080 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.206425189971924, loss=1.2079501152038574
I0129 23:27:36.918230 140005330646784 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.859926223754883, loss=1.265479564666748
I0129 23:28:10.317086 140005322254080 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.076648235321045, loss=1.085827112197876
I0129 23:28:43.704236 140005330646784 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.951345920562744, loss=1.2912368774414062
I0129 23:29:17.101819 140005322254080 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.048275470733643, loss=1.232574462890625
I0129 23:29:50.499255 140005330646784 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.603699684143066, loss=1.2325061559677124
I0129 23:30:23.908697 140005322254080 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.813451766967773, loss=1.2078500986099243
I0129 23:30:57.310920 140005330646784 logging_writer.py:48] [163100] global_step=163100, grad_norm=5.093906402587891, loss=1.1955960988998413
I0129 23:31:30.704373 140005322254080 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.467838287353516, loss=1.131805181503296
I0129 23:31:30.713797 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:31:37.135681 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:31:45.904258 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:31:48.590137 140169137129280 submission_runner.py:408] Time since start: 56551.64s, 	Step: 163201, 	{'train/accuracy': 0.8162667155265808, 'train/loss': 0.6751604676246643, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.1474189758300781, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.8701530694961548, 'test/num_examples': 10000, 'score': 54612.54327106476, 'total_duration': 56551.63760781288, 'accumulated_submission_time': 54612.54327106476, 'accumulated_eval_time': 1928.901858329773, 'accumulated_logging_time': 4.860100746154785}
I0129 23:31:48.635822 140004616554240 logging_writer.py:48] [163201] accumulated_eval_time=1928.901858, accumulated_logging_time=4.860101, accumulated_submission_time=54612.543271, global_step=163201, preemption_count=0, score=54612.543271, test/accuracy=0.590100, test/loss=1.870153, test/num_examples=10000, total_duration=56551.637608, train/accuracy=0.816267, train/loss=0.675160, validation/accuracy=0.718200, validation/loss=1.147419, validation/num_examples=50000
I0129 23:32:22.077475 140005288683264 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.515406608581543, loss=1.1413201093673706
I0129 23:32:55.451197 140004616554240 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.85308313369751, loss=1.2573158740997314
I0129 23:33:28.832311 140005288683264 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.346007347106934, loss=1.179801344871521
I0129 23:34:02.221771 140004616554240 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.915834426879883, loss=1.1533524990081787
I0129 23:34:35.619884 140005288683264 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.496633052825928, loss=1.1982316970825195
I0129 23:35:09.018975 140004616554240 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.850324630737305, loss=1.1864665746688843
I0129 23:35:42.414721 140005288683264 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.60823917388916, loss=1.1592910289764404
I0129 23:36:15.801462 140004616554240 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.615998268127441, loss=1.2171556949615479
I0129 23:36:49.200947 140005288683264 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.925199031829834, loss=1.2555515766143799
I0129 23:37:22.610283 140004616554240 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.256984233856201, loss=1.292271614074707
I0129 23:37:56.016294 140005288683264 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.561747074127197, loss=1.103331446647644
I0129 23:38:29.422127 140004616554240 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.70671272277832, loss=1.2032222747802734
I0129 23:39:02.911190 140005288683264 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.445768356323242, loss=1.0474096536636353
I0129 23:39:36.301478 140004616554240 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.627869606018066, loss=1.1521658897399902
I0129 23:40:09.693021 140005288683264 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.722068786621094, loss=1.2324273586273193
I0129 23:40:18.868021 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:40:25.225389 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:40:33.794350 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:40:36.478553 140169137129280 submission_runner.py:408] Time since start: 57079.53s, 	Step: 164729, 	{'train/accuracy': 0.8210498690605164, 'train/loss': 0.6503438353538513, 'validation/accuracy': 0.7199400067329407, 'validation/loss': 1.1440032720565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5978000164031982, 'test/loss': 1.8753104209899902, 'test/num_examples': 10000, 'score': 55122.71467757225, 'total_duration': 57079.52602314949, 'accumulated_submission_time': 55122.71467757225, 'accumulated_eval_time': 1946.5123527050018, 'accumulated_logging_time': 4.916692018508911}
I0129 23:40:36.526778 140004616554240 logging_writer.py:48] [164729] accumulated_eval_time=1946.512353, accumulated_logging_time=4.916692, accumulated_submission_time=55122.714678, global_step=164729, preemption_count=0, score=55122.714678, test/accuracy=0.597800, test/loss=1.875310, test/num_examples=10000, total_duration=57079.526023, train/accuracy=0.821050, train/loss=0.650344, validation/accuracy=0.719940, validation/loss=1.144003, validation/num_examples=50000
I0129 23:41:00.539167 140004624946944 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.702068328857422, loss=1.2454816102981567
I0129 23:41:33.889565 140004616554240 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.843299865722656, loss=1.2008605003356934
I0129 23:42:07.273475 140004624946944 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.3970136642456055, loss=1.1310360431671143
I0129 23:42:40.654374 140004616554240 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.641317844390869, loss=1.2163841724395752
I0129 23:43:14.042105 140004624946944 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.7483720779418945, loss=1.1099375486373901
I0129 23:43:47.438796 140004616554240 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.46359920501709, loss=1.192684531211853
I0129 23:44:20.826797 140004624946944 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.650483131408691, loss=1.1894340515136719
I0129 23:44:54.243713 140004616554240 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.131153583526611, loss=1.1818469762802124
I0129 23:45:27.854649 140004624946944 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.628191947937012, loss=1.0988510847091675
I0129 23:46:01.241894 140004616554240 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.337611198425293, loss=1.1853420734405518
I0129 23:46:34.663689 140004624946944 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.605799198150635, loss=1.104007363319397
I0129 23:47:08.075639 140004616554240 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.01340389251709, loss=1.1674162149429321
I0129 23:47:41.485565 140004624946944 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.792986869812012, loss=1.190929651260376
I0129 23:48:14.885801 140004616554240 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.904243469238281, loss=1.0685018301010132
I0129 23:48:48.276893 140004624946944 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.901716709136963, loss=1.1532268524169922
I0129 23:49:06.783753 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:49:13.168481 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:49:21.760470 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:49:24.446521 140169137129280 submission_runner.py:408] Time since start: 57607.49s, 	Step: 166257, 	{'train/accuracy': 0.8339245915412903, 'train/loss': 0.5999334454536438, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1352955102920532, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8515959978103638, 'test/num_examples': 10000, 'score': 55632.91140437126, 'total_duration': 57607.49398231506, 'accumulated_submission_time': 55632.91140437126, 'accumulated_eval_time': 1964.1750729084015, 'accumulated_logging_time': 4.975133895874023}
I0129 23:49:24.495146 140004616554240 logging_writer.py:48] [166257] accumulated_eval_time=1964.175073, accumulated_logging_time=4.975134, accumulated_submission_time=55632.911404, global_step=166257, preemption_count=0, score=55632.911404, test/accuracy=0.596200, test/loss=1.851596, test/num_examples=10000, total_duration=57607.493982, train/accuracy=0.833925, train/loss=0.599933, validation/accuracy=0.720580, validation/loss=1.135296, validation/num_examples=50000
I0129 23:49:39.189357 140004624946944 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.111662864685059, loss=1.1283152103424072
I0129 23:50:12.518652 140004616554240 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.45767879486084, loss=1.072159767150879
I0129 23:50:45.898679 140004624946944 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.836368560791016, loss=1.1198962926864624
I0129 23:51:19.277161 140004616554240 logging_writer.py:48] [166600] global_step=166600, grad_norm=5.245491027832031, loss=1.1807570457458496
I0129 23:51:52.799164 140004624946944 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.848936557769775, loss=1.2143738269805908
I0129 23:52:26.205930 140004616554240 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.9173102378845215, loss=1.149039626121521
I0129 23:52:59.616154 140004624946944 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.236068248748779, loss=1.1289241313934326
I0129 23:53:33.005128 140004616554240 logging_writer.py:48] [167000] global_step=167000, grad_norm=5.335538387298584, loss=1.1976373195648193
I0129 23:54:06.410337 140004624946944 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.2364935874938965, loss=1.1329002380371094
I0129 23:54:39.805001 140004616554240 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.403057098388672, loss=1.1509339809417725
I0129 23:55:13.210909 140004624946944 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.354716777801514, loss=1.2099597454071045
I0129 23:55:46.621287 140004616554240 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.656888484954834, loss=1.0737718343734741
I0129 23:56:20.035649 140004624946944 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.470584392547607, loss=1.0637913942337036
I0129 23:56:53.444281 140004616554240 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.868363380432129, loss=1.1606450080871582
I0129 23:57:26.833526 140004624946944 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.557321548461914, loss=1.1225849390029907
I0129 23:57:54.457165 140169137129280 spec.py:321] Evaluating on the training split.
I0129 23:58:00.818365 140169137129280 spec.py:333] Evaluating on the validation split.
I0129 23:58:09.661441 140169137129280 spec.py:349] Evaluating on the test split.
I0129 23:58:12.519189 140169137129280 submission_runner.py:408] Time since start: 58135.57s, 	Step: 167784, 	{'train/accuracy': 0.8476362824440002, 'train/loss': 0.5580589175224304, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.1028032302856445, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.822930932044983, 'test/num_examples': 10000, 'score': 56142.81165266037, 'total_duration': 58135.56666016579, 'accumulated_submission_time': 56142.81165266037, 'accumulated_eval_time': 1982.2370581626892, 'accumulated_logging_time': 5.035584211349487}
I0129 23:58:12.568411 140004624946944 logging_writer.py:48] [167784] accumulated_eval_time=1982.237058, accumulated_logging_time=5.035584, accumulated_submission_time=56142.811653, global_step=167784, preemption_count=0, score=56142.811653, test/accuracy=0.599500, test/loss=1.822931, test/num_examples=10000, total_duration=58135.566660, train/accuracy=0.847636, train/loss=0.558059, validation/accuracy=0.726340, validation/loss=1.102803, validation/num_examples=50000
I0129 23:58:18.253709 140005322254080 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.19123649597168, loss=1.17421555519104
I0129 23:58:51.579552 140004624946944 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.16763162612915, loss=1.1734421253204346
I0129 23:59:24.939884 140005322254080 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.368926048278809, loss=1.1626635789871216
I0129 23:59:58.319433 140004624946944 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.429323673248291, loss=1.0582116842269897
I0130 00:00:31.706557 140005322254080 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.605987071990967, loss=1.1575453281402588
I0130 00:01:05.113023 140004624946944 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.88287878036499, loss=1.1053547859191895
I0130 00:01:38.518895 140005322254080 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.993947505950928, loss=1.038921594619751
I0130 00:02:11.930482 140004624946944 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.885244846343994, loss=1.0670897960662842
I0130 00:02:45.318752 140005322254080 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.972613334655762, loss=1.0731343030929565
I0130 00:03:18.731446 140004624946944 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.728001117706299, loss=0.9633769989013672
I0130 00:03:52.126040 140005322254080 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.249002456665039, loss=1.1042389869689941
I0130 00:04:25.614741 140004624946944 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.920899391174316, loss=1.1415648460388184
I0130 00:04:59.018057 140005322254080 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.838613033294678, loss=1.02938711643219
I0130 00:05:32.410437 140004624946944 logging_writer.py:48] [169100] global_step=169100, grad_norm=5.204543590545654, loss=1.0827587842941284
I0130 00:06:05.808911 140005322254080 logging_writer.py:48] [169200] global_step=169200, grad_norm=5.217094421386719, loss=1.1123273372650146
I0130 00:06:39.212388 140004624946944 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.157055854797363, loss=1.0833089351654053
I0130 00:06:42.703851 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:06:49.801267 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:06:58.628954 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:07:01.273635 140169137129280 submission_runner.py:408] Time since start: 58664.32s, 	Step: 169312, 	{'train/accuracy': 0.8518813848495483, 'train/loss': 0.5361858010292053, 'validation/accuracy': 0.7312799692153931, 'validation/loss': 1.0846476554870605, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.800598382949829, 'test/num_examples': 10000, 'score': 56652.887028455734, 'total_duration': 58664.32110500336, 'accumulated_submission_time': 56652.887028455734, 'accumulated_eval_time': 2000.8068022727966, 'accumulated_logging_time': 5.0948405265808105}
I0130 00:07:01.323287 140005288683264 logging_writer.py:48] [169312] accumulated_eval_time=2000.806802, accumulated_logging_time=5.094841, accumulated_submission_time=56652.887028, global_step=169312, preemption_count=0, score=56652.887028, test/accuracy=0.607700, test/loss=1.800598, test/num_examples=10000, total_duration=58664.321105, train/accuracy=0.851881, train/loss=0.536186, validation/accuracy=0.731280, validation/loss=1.084648, validation/num_examples=50000
I0130 00:07:31.001760 140005297075968 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.986576080322266, loss=1.0328348875045776
I0130 00:08:04.372642 140005288683264 logging_writer.py:48] [169500] global_step=169500, grad_norm=5.434738636016846, loss=1.0205836296081543
I0130 00:08:37.767617 140005297075968 logging_writer.py:48] [169600] global_step=169600, grad_norm=5.49185037612915, loss=1.1537214517593384
I0130 00:09:11.157309 140005288683264 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.969000816345215, loss=1.0728747844696045
I0130 00:09:44.538649 140005297075968 logging_writer.py:48] [169800] global_step=169800, grad_norm=5.2855939865112305, loss=1.2143305540084839
I0130 00:10:17.930937 140005288683264 logging_writer.py:48] [169900] global_step=169900, grad_norm=5.338712692260742, loss=1.0897868871688843
I0130 00:10:51.434840 140005297075968 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.221977710723877, loss=1.1326583623886108
I0130 00:11:24.842488 140005288683264 logging_writer.py:48] [170100] global_step=170100, grad_norm=5.344996452331543, loss=1.046789526939392
I0130 00:11:58.236168 140005297075968 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.909469127655029, loss=0.9799069166183472
I0130 00:12:31.631592 140005288683264 logging_writer.py:48] [170300] global_step=170300, grad_norm=5.44926643371582, loss=1.166595220565796
I0130 00:13:05.025694 140005297075968 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.787754058837891, loss=1.0255746841430664
I0130 00:13:38.415438 140005288683264 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.324460983276367, loss=1.07939875125885
I0130 00:14:11.806827 140005297075968 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.445817470550537, loss=1.134128451347351
I0130 00:14:45.203398 140005288683264 logging_writer.py:48] [170700] global_step=170700, grad_norm=5.250943183898926, loss=1.0384864807128906
I0130 00:15:18.594599 140005297075968 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.797802925109863, loss=1.0746033191680908
I0130 00:15:31.438103 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:15:37.827739 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:15:46.612262 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:15:49.290920 140169137129280 submission_runner.py:408] Time since start: 59192.34s, 	Step: 170840, 	{'train/accuracy': 0.8516421914100647, 'train/loss': 0.5271843671798706, 'validation/accuracy': 0.7345199584960938, 'validation/loss': 1.0806338787078857, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.7768418788909912, 'test/num_examples': 10000, 'score': 57162.94032907486, 'total_duration': 59192.33838939667, 'accumulated_submission_time': 57162.94032907486, 'accumulated_eval_time': 2018.65958237648, 'accumulated_logging_time': 5.155996322631836}
I0130 00:15:49.341300 140004624946944 logging_writer.py:48] [170840] accumulated_eval_time=2018.659582, accumulated_logging_time=5.155996, accumulated_submission_time=57162.940329, global_step=170840, preemption_count=0, score=57162.940329, test/accuracy=0.613300, test/loss=1.776842, test/num_examples=10000, total_duration=59192.338389, train/accuracy=0.851642, train/loss=0.527184, validation/accuracy=0.734520, validation/loss=1.080634, validation/num_examples=50000
I0130 00:16:09.711796 140005313861376 logging_writer.py:48] [170900] global_step=170900, grad_norm=5.675334930419922, loss=1.0487091541290283
I0130 00:16:43.077990 140004624946944 logging_writer.py:48] [171000] global_step=171000, grad_norm=5.363602638244629, loss=1.1458779573440552
I0130 00:17:16.570619 140005313861376 logging_writer.py:48] [171100] global_step=171100, grad_norm=5.183666706085205, loss=1.0549215078353882
I0130 00:17:49.968411 140004624946944 logging_writer.py:48] [171200] global_step=171200, grad_norm=5.113490104675293, loss=1.0485577583312988
I0130 00:18:23.358819 140005313861376 logging_writer.py:48] [171300] global_step=171300, grad_norm=5.333756446838379, loss=1.0320590734481812
I0130 00:18:56.757220 140004624946944 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.839941024780273, loss=0.9786468744277954
I0130 00:19:30.145290 140005313861376 logging_writer.py:48] [171500] global_step=171500, grad_norm=5.244892597198486, loss=0.9839503765106201
I0130 00:20:03.533864 140004624946944 logging_writer.py:48] [171600] global_step=171600, grad_norm=5.1146559715271, loss=0.9765967726707458
I0130 00:20:36.945777 140005313861376 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.270953178405762, loss=1.0657711029052734
I0130 00:21:10.343033 140004624946944 logging_writer.py:48] [171800] global_step=171800, grad_norm=5.707864761352539, loss=1.062036395072937
I0130 00:21:43.738026 140005313861376 logging_writer.py:48] [171900] global_step=171900, grad_norm=5.171184539794922, loss=1.0572617053985596
I0130 00:22:17.133611 140004624946944 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.987387657165527, loss=0.9239448308944702
I0130 00:22:50.527312 140005313861376 logging_writer.py:48] [172100] global_step=172100, grad_norm=5.121994495391846, loss=1.0118836164474487
I0130 00:23:23.946462 140004624946944 logging_writer.py:48] [172200] global_step=172200, grad_norm=5.823935031890869, loss=0.9956915974617004
I0130 00:23:57.433154 140005313861376 logging_writer.py:48] [172300] global_step=172300, grad_norm=5.525052547454834, loss=1.162672519683838
I0130 00:24:19.617299 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:24:25.992777 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:24:34.595631 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:24:37.340775 140169137129280 submission_runner.py:408] Time since start: 59720.39s, 	Step: 172368, 	{'train/accuracy': 0.8561064600944519, 'train/loss': 0.51559978723526, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0692557096481323, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.7621028423309326, 'test/num_examples': 10000, 'score': 57673.15584611893, 'total_duration': 59720.38824558258, 'accumulated_submission_time': 57673.15584611893, 'accumulated_eval_time': 2036.3830211162567, 'accumulated_logging_time': 5.2170140743255615}
I0130 00:24:37.389991 140005305468672 logging_writer.py:48] [172368] accumulated_eval_time=2036.383021, accumulated_logging_time=5.217014, accumulated_submission_time=57673.155846, global_step=172368, preemption_count=0, score=57673.155846, test/accuracy=0.611400, test/loss=1.762103, test/num_examples=10000, total_duration=59720.388246, train/accuracy=0.856106, train/loss=0.515600, validation/accuracy=0.735840, validation/loss=1.069256, validation/num_examples=50000
I0130 00:24:48.395721 140005330646784 logging_writer.py:48] [172400] global_step=172400, grad_norm=5.165672302246094, loss=1.1251204013824463
I0130 00:25:21.742246 140005305468672 logging_writer.py:48] [172500] global_step=172500, grad_norm=5.331632137298584, loss=1.0048229694366455
I0130 00:25:55.119770 140005330646784 logging_writer.py:48] [172600] global_step=172600, grad_norm=5.404834270477295, loss=1.0561559200286865
I0130 00:26:28.507577 140005305468672 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.959089279174805, loss=1.0132631063461304
I0130 00:27:01.891444 140005330646784 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.435436248779297, loss=0.9841921925544739
I0130 00:27:35.298446 140005305468672 logging_writer.py:48] [172900] global_step=172900, grad_norm=5.059493064880371, loss=0.9591082334518433
I0130 00:28:08.698817 140005330646784 logging_writer.py:48] [173000] global_step=173000, grad_norm=5.211077690124512, loss=1.0736911296844482
I0130 00:28:42.090081 140005305468672 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.999037742614746, loss=0.9524005055427551
I0130 00:29:15.486569 140005330646784 logging_writer.py:48] [173200] global_step=173200, grad_norm=5.394623756408691, loss=1.0595176219940186
I0130 00:29:48.883163 140005305468672 logging_writer.py:48] [173300] global_step=173300, grad_norm=5.549783229827881, loss=0.9810669422149658
I0130 00:30:22.364506 140005330646784 logging_writer.py:48] [173400] global_step=173400, grad_norm=5.428184986114502, loss=1.1017448902130127
I0130 00:30:55.757150 140005305468672 logging_writer.py:48] [173500] global_step=173500, grad_norm=5.224533557891846, loss=1.0436592102050781
I0130 00:31:29.152331 140005330646784 logging_writer.py:48] [173600] global_step=173600, grad_norm=5.034130573272705, loss=1.034085750579834
I0130 00:32:02.540680 140005305468672 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.619845390319824, loss=1.0522618293762207
I0130 00:32:35.957832 140005330646784 logging_writer.py:48] [173800] global_step=173800, grad_norm=5.6227126121521, loss=1.0430612564086914
I0130 00:33:07.501854 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:33:13.872648 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:33:22.524494 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:33:25.208274 140169137129280 submission_runner.py:408] Time since start: 60248.26s, 	Step: 173896, 	{'train/accuracy': 0.8584582209587097, 'train/loss': 0.5045436024665833, 'validation/accuracy': 0.7406799793243408, 'validation/loss': 1.059166669845581, 'validation/num_examples': 50000, 'test/accuracy': 0.6181000471115112, 'test/loss': 1.763548493385315, 'test/num_examples': 10000, 'score': 58183.20809054375, 'total_duration': 60248.25574231148, 'accumulated_submission_time': 58183.20809054375, 'accumulated_eval_time': 2054.0894026756287, 'accumulated_logging_time': 5.276186227798462}
I0130 00:33:25.261889 140005297075968 logging_writer.py:48] [173896] accumulated_eval_time=2054.089403, accumulated_logging_time=5.276186, accumulated_submission_time=58183.208091, global_step=173896, preemption_count=0, score=58183.208091, test/accuracy=0.618100, test/loss=1.763548, test/num_examples=10000, total_duration=60248.255742, train/accuracy=0.858458, train/loss=0.504544, validation/accuracy=0.740680, validation/loss=1.059167, validation/num_examples=50000
I0130 00:33:26.942146 140005313861376 logging_writer.py:48] [173900] global_step=173900, grad_norm=5.148875713348389, loss=1.0450897216796875
I0130 00:34:00.283037 140005297075968 logging_writer.py:48] [174000] global_step=174000, grad_norm=5.129101753234863, loss=1.0031671524047852
I0130 00:34:33.636183 140005313861376 logging_writer.py:48] [174100] global_step=174100, grad_norm=5.0706048011779785, loss=1.0004689693450928
I0130 00:35:07.027659 140005297075968 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.15554141998291, loss=1.0127346515655518
I0130 00:35:40.440058 140005313861376 logging_writer.py:48] [174300] global_step=174300, grad_norm=5.60591459274292, loss=1.10203218460083
I0130 00:36:13.828341 140005297075968 logging_writer.py:48] [174400] global_step=174400, grad_norm=5.7813334465026855, loss=1.1062872409820557
I0130 00:36:47.323542 140005313861376 logging_writer.py:48] [174500] global_step=174500, grad_norm=5.201047420501709, loss=1.0400079488754272
I0130 00:37:20.737048 140005297075968 logging_writer.py:48] [174600] global_step=174600, grad_norm=5.445153713226318, loss=1.0639827251434326
I0130 00:37:54.149224 140005313861376 logging_writer.py:48] [174700] global_step=174700, grad_norm=5.194220542907715, loss=1.0068984031677246
I0130 00:38:27.574545 140005297075968 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.1113080978393555, loss=1.013014793395996
I0130 00:39:00.971238 140005313861376 logging_writer.py:48] [174900] global_step=174900, grad_norm=5.389519214630127, loss=0.9850751757621765
I0130 00:39:34.376651 140005297075968 logging_writer.py:48] [175000] global_step=175000, grad_norm=6.050407886505127, loss=1.0654834508895874
I0130 00:40:07.777275 140005313861376 logging_writer.py:48] [175100] global_step=175100, grad_norm=5.189108848571777, loss=1.0863792896270752
I0130 00:40:41.186899 140005297075968 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.2627716064453125, loss=1.043212890625
I0130 00:41:14.589416 140005313861376 logging_writer.py:48] [175300] global_step=175300, grad_norm=5.177178859710693, loss=1.0208230018615723
I0130 00:41:48.006275 140005297075968 logging_writer.py:48] [175400] global_step=175400, grad_norm=5.113115310668945, loss=1.0343616008758545
I0130 00:41:55.505621 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:42:01.955502 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:42:10.498719 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:42:13.268378 140169137129280 submission_runner.py:408] Time since start: 60776.32s, 	Step: 175424, 	{'train/accuracy': 0.8757573366165161, 'train/loss': 0.44802698493003845, 'validation/accuracy': 0.7421999573707581, 'validation/loss': 1.0490195751190186, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7469100952148438, 'test/num_examples': 10000, 'score': 58693.388276576996, 'total_duration': 60776.315844774246, 'accumulated_submission_time': 58693.388276576996, 'accumulated_eval_time': 2071.8521168231964, 'accumulated_logging_time': 5.343605995178223}
I0130 00:42:13.319048 140005313861376 logging_writer.py:48] [175424] accumulated_eval_time=2071.852117, accumulated_logging_time=5.343606, accumulated_submission_time=58693.388277, global_step=175424, preemption_count=0, score=58693.388277, test/accuracy=0.617700, test/loss=1.746910, test/num_examples=10000, total_duration=60776.315845, train/accuracy=0.875757, train/loss=0.448027, validation/accuracy=0.742200, validation/loss=1.049020, validation/num_examples=50000
I0130 00:42:39.005908 140005322254080 logging_writer.py:48] [175500] global_step=175500, grad_norm=5.2896809577941895, loss=0.9678075909614563
I0130 00:43:12.460930 140005313861376 logging_writer.py:48] [175600] global_step=175600, grad_norm=5.436795234680176, loss=1.0227500200271606
I0130 00:43:45.858643 140005322254080 logging_writer.py:48] [175700] global_step=175700, grad_norm=5.888881206512451, loss=1.0694407224655151
I0130 00:44:19.257894 140005313861376 logging_writer.py:48] [175800] global_step=175800, grad_norm=5.960964679718018, loss=1.082008957862854
I0130 00:44:52.645376 140005322254080 logging_writer.py:48] [175900] global_step=175900, grad_norm=5.4063005447387695, loss=1.0579774379730225
I0130 00:45:26.031566 140005313861376 logging_writer.py:48] [176000] global_step=176000, grad_norm=5.029891014099121, loss=0.9530896544456482
I0130 00:45:59.419548 140005322254080 logging_writer.py:48] [176100] global_step=176100, grad_norm=5.4935383796691895, loss=0.9983649849891663
I0130 00:46:32.807754 140005313861376 logging_writer.py:48] [176200] global_step=176200, grad_norm=5.44744873046875, loss=0.9834848642349243
I0130 00:47:06.218284 140005322254080 logging_writer.py:48] [176300] global_step=176300, grad_norm=5.684891700744629, loss=0.9888945817947388
I0130 00:47:39.608797 140005313861376 logging_writer.py:48] [176400] global_step=176400, grad_norm=5.964603900909424, loss=0.9278184771537781
I0130 00:48:13.017375 140005322254080 logging_writer.py:48] [176500] global_step=176500, grad_norm=5.658052444458008, loss=0.9664390683174133
I0130 00:48:46.402479 140005313861376 logging_writer.py:48] [176600] global_step=176600, grad_norm=5.041365623474121, loss=0.9706841707229614
I0130 00:49:19.906718 140005322254080 logging_writer.py:48] [176700] global_step=176700, grad_norm=5.178955078125, loss=0.9871461391448975
I0130 00:49:53.315693 140005313861376 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.328479290008545, loss=1.00294828414917
I0130 00:50:26.711496 140005322254080 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.4735331535339355, loss=1.0599567890167236
I0130 00:50:43.553051 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:50:49.922397 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:50:58.360836 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:51:01.062593 140169137129280 submission_runner.py:408] Time since start: 61304.11s, 	Step: 176952, 	{'train/accuracy': 0.8742027878761292, 'train/loss': 0.44698426127433777, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.0434011220932007, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.7508008480072021, 'test/num_examples': 10000, 'score': 59203.56213974953, 'total_duration': 61304.11006069183, 'accumulated_submission_time': 59203.56213974953, 'accumulated_eval_time': 2089.3616197109222, 'accumulated_logging_time': 5.404559135437012}
I0130 00:51:01.115229 140004616554240 logging_writer.py:48] [176952] accumulated_eval_time=2089.361620, accumulated_logging_time=5.404559, accumulated_submission_time=59203.562140, global_step=176952, preemption_count=0, score=59203.562140, test/accuracy=0.620800, test/loss=1.750801, test/num_examples=10000, total_duration=61304.110061, train/accuracy=0.874203, train/loss=0.446984, validation/accuracy=0.744800, validation/loss=1.043401, validation/num_examples=50000
I0130 00:51:17.471531 140004624946944 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.57144021987915, loss=1.002289891242981
I0130 00:51:50.823410 140004616554240 logging_writer.py:48] [177100] global_step=177100, grad_norm=5.669480800628662, loss=0.9753943681716919
I0130 00:52:24.208976 140004624946944 logging_writer.py:48] [177200] global_step=177200, grad_norm=5.53573751449585, loss=1.0449541807174683
I0130 00:52:57.591245 140004616554240 logging_writer.py:48] [177300] global_step=177300, grad_norm=5.358236312866211, loss=0.9824521541595459
I0130 00:53:30.974304 140004624946944 logging_writer.py:48] [177400] global_step=177400, grad_norm=5.6414408683776855, loss=1.0141922235488892
I0130 00:54:04.377671 140004616554240 logging_writer.py:48] [177500] global_step=177500, grad_norm=5.469202518463135, loss=0.9590184688568115
I0130 00:54:37.781716 140004624946944 logging_writer.py:48] [177600] global_step=177600, grad_norm=5.400401592254639, loss=0.969322144985199
I0130 00:55:11.174489 140004616554240 logging_writer.py:48] [177700] global_step=177700, grad_norm=5.526714324951172, loss=1.0452640056610107
I0130 00:55:44.643154 140004624946944 logging_writer.py:48] [177800] global_step=177800, grad_norm=5.793558120727539, loss=1.035723328590393
I0130 00:56:18.048128 140004616554240 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.0220794677734375, loss=0.9808598160743713
I0130 00:56:51.470455 140004624946944 logging_writer.py:48] [178000] global_step=178000, grad_norm=5.099466323852539, loss=0.979272723197937
I0130 00:57:24.857808 140004616554240 logging_writer.py:48] [178100] global_step=178100, grad_norm=5.360412120819092, loss=0.9476553797721863
I0130 00:57:58.266910 140004624946944 logging_writer.py:48] [178200] global_step=178200, grad_norm=5.804401397705078, loss=1.0097496509552002
I0130 00:58:31.671852 140004616554240 logging_writer.py:48] [178300] global_step=178300, grad_norm=6.3030571937561035, loss=1.034654974937439
I0130 00:59:05.070942 140004624946944 logging_writer.py:48] [178400] global_step=178400, grad_norm=5.323147296905518, loss=0.9020612239837646
I0130 00:59:31.275600 140169137129280 spec.py:321] Evaluating on the training split.
I0130 00:59:37.638775 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 00:59:46.146691 140169137129280 spec.py:349] Evaluating on the test split.
I0130 00:59:48.826373 140169137129280 submission_runner.py:408] Time since start: 61831.87s, 	Step: 178480, 	{'train/accuracy': 0.878348171710968, 'train/loss': 0.4332602918148041, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.0316156148910522, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.740649938583374, 'test/num_examples': 10000, 'score': 59713.66224193573, 'total_duration': 61831.87384080887, 'accumulated_submission_time': 59713.66224193573, 'accumulated_eval_time': 2106.9123561382294, 'accumulated_logging_time': 5.46753716468811}
I0130 00:59:48.878241 140005313861376 logging_writer.py:48] [178480] accumulated_eval_time=2106.912356, accumulated_logging_time=5.467537, accumulated_submission_time=59713.662242, global_step=178480, preemption_count=0, score=59713.662242, test/accuracy=0.623200, test/loss=1.740650, test/num_examples=10000, total_duration=61831.873841, train/accuracy=0.878348, train/loss=0.433260, validation/accuracy=0.747060, validation/loss=1.031616, validation/num_examples=50000
I0130 00:59:55.905480 140005322254080 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.634751796722412, loss=1.0512287616729736
I0130 01:00:29.253260 140005313861376 logging_writer.py:48] [178600] global_step=178600, grad_norm=5.152803421020508, loss=0.8685947060585022
I0130 01:01:02.616706 140005322254080 logging_writer.py:48] [178700] global_step=178700, grad_norm=5.2180705070495605, loss=1.0384045839309692
I0130 01:01:36.016861 140005313861376 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.1769819259643555, loss=0.9540355205535889
I0130 01:02:09.547380 140005322254080 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.941167831420898, loss=0.9986248016357422
I0130 01:02:42.946236 140005313861376 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.624844551086426, loss=0.9763817191123962
I0130 01:03:16.341983 140005322254080 logging_writer.py:48] [179100] global_step=179100, grad_norm=5.20147705078125, loss=0.985324501991272
I0130 01:03:49.757287 140005313861376 logging_writer.py:48] [179200] global_step=179200, grad_norm=5.181643009185791, loss=1.034480094909668
I0130 01:04:23.165462 140005322254080 logging_writer.py:48] [179300] global_step=179300, grad_norm=5.059471130371094, loss=0.8628179430961609
I0130 01:04:56.577843 140005313861376 logging_writer.py:48] [179400] global_step=179400, grad_norm=5.348206520080566, loss=1.0196770429611206
I0130 01:05:29.967158 140005322254080 logging_writer.py:48] [179500] global_step=179500, grad_norm=5.4424052238464355, loss=0.9450011253356934
I0130 01:06:03.397573 140005313861376 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.560178756713867, loss=0.946563184261322
I0130 01:06:36.791444 140005322254080 logging_writer.py:48] [179700] global_step=179700, grad_norm=5.267796516418457, loss=0.8827740550041199
I0130 01:07:10.214406 140005313861376 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.480559349060059, loss=0.9371414184570312
I0130 01:07:43.618109 140005322254080 logging_writer.py:48] [179900] global_step=179900, grad_norm=5.362651348114014, loss=1.0234955549240112
I0130 01:08:17.117596 140005313861376 logging_writer.py:48] [180000] global_step=180000, grad_norm=5.7984771728515625, loss=0.880406379699707
I0130 01:08:18.941376 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:08:25.384926 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:08:33.762803 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:08:36.455879 140169137129280 submission_runner.py:408] Time since start: 62359.50s, 	Step: 180007, 	{'train/accuracy': 0.8775510191917419, 'train/loss': 0.4316553771495819, 'validation/accuracy': 0.7484999895095825, 'validation/loss': 1.0303864479064941, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7318052053451538, 'test/num_examples': 10000, 'score': 60223.664409160614, 'total_duration': 62359.50334787369, 'accumulated_submission_time': 60223.664409160614, 'accumulated_eval_time': 2124.426818370819, 'accumulated_logging_time': 5.530123233795166}
I0130 01:08:36.508350 140004616554240 logging_writer.py:48] [180007] accumulated_eval_time=2124.426818, accumulated_logging_time=5.530123, accumulated_submission_time=60223.664409, global_step=180007, preemption_count=0, score=60223.664409, test/accuracy=0.626200, test/loss=1.731805, test/num_examples=10000, total_duration=62359.503348, train/accuracy=0.877551, train/loss=0.431655, validation/accuracy=0.748500, validation/loss=1.030386, validation/num_examples=50000
I0130 01:09:07.882334 140004624946944 logging_writer.py:48] [180100] global_step=180100, grad_norm=6.229683876037598, loss=0.9474802017211914
I0130 01:09:41.262980 140004616554240 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.6753740310668945, loss=1.0800331830978394
I0130 01:10:14.654152 140004624946944 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.416143417358398, loss=0.9877035617828369
I0130 01:10:48.033657 140004616554240 logging_writer.py:48] [180400] global_step=180400, grad_norm=5.274722576141357, loss=0.8871811032295227
I0130 01:11:21.420099 140004624946944 logging_writer.py:48] [180500] global_step=180500, grad_norm=5.222515106201172, loss=0.9059088230133057
I0130 01:11:54.827407 140004616554240 logging_writer.py:48] [180600] global_step=180600, grad_norm=5.735132694244385, loss=1.0287443399429321
I0130 01:12:28.241182 140004624946944 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.382953643798828, loss=0.9849204421043396
I0130 01:13:01.645718 140004616554240 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.977066516876221, loss=0.9436463117599487
I0130 01:13:35.049346 140004624946944 logging_writer.py:48] [180900] global_step=180900, grad_norm=5.494945526123047, loss=0.9891460537910461
I0130 01:14:08.456939 140004616554240 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.594130992889404, loss=0.9677215218544006
I0130 01:14:41.855579 140004624946944 logging_writer.py:48] [181100] global_step=181100, grad_norm=5.058695316314697, loss=0.938197135925293
I0130 01:15:15.354200 140004616554240 logging_writer.py:48] [181200] global_step=181200, grad_norm=5.658289432525635, loss=1.014918327331543
I0130 01:15:48.755947 140004624946944 logging_writer.py:48] [181300] global_step=181300, grad_norm=5.658545970916748, loss=0.9511335492134094
I0130 01:16:22.147628 140004616554240 logging_writer.py:48] [181400] global_step=181400, grad_norm=5.813507080078125, loss=0.9066545367240906
I0130 01:16:55.556472 140004624946944 logging_writer.py:48] [181500] global_step=181500, grad_norm=5.431168079376221, loss=0.9938000440597534
I0130 01:17:06.718644 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:17:13.180004 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:17:21.863038 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:17:24.571955 140169137129280 submission_runner.py:408] Time since start: 62887.62s, 	Step: 181535, 	{'train/accuracy': 0.883211076259613, 'train/loss': 0.4166988730430603, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 1.027388334274292, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.7263579368591309, 'test/num_examples': 10000, 'score': 60733.81407546997, 'total_duration': 62887.61942386627, 'accumulated_submission_time': 60733.81407546997, 'accumulated_eval_time': 2142.2800900936127, 'accumulated_logging_time': 5.592935800552368}
I0130 01:17:24.621281 140005305468672 logging_writer.py:48] [181535] accumulated_eval_time=2142.280090, accumulated_logging_time=5.592936, accumulated_submission_time=60733.814075, global_step=181535, preemption_count=0, score=60733.814075, test/accuracy=0.626500, test/loss=1.726358, test/num_examples=10000, total_duration=62887.619424, train/accuracy=0.883211, train/loss=0.416699, validation/accuracy=0.749520, validation/loss=1.027388, validation/num_examples=50000
I0130 01:17:46.635141 140005313861376 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.604098796844482, loss=0.8560179471969604
I0130 01:18:19.979078 140005305468672 logging_writer.py:48] [181700] global_step=181700, grad_norm=5.556429386138916, loss=0.9943071603775024
I0130 01:18:53.361381 140005313861376 logging_writer.py:48] [181800] global_step=181800, grad_norm=6.1377787590026855, loss=1.0169637203216553
I0130 01:19:26.767540 140005305468672 logging_writer.py:48] [181900] global_step=181900, grad_norm=5.4520039558410645, loss=0.8841993808746338
I0130 01:20:00.168964 140005313861376 logging_writer.py:48] [182000] global_step=182000, grad_norm=5.813045501708984, loss=0.9743918180465698
I0130 01:20:33.577760 140005305468672 logging_writer.py:48] [182100] global_step=182100, grad_norm=5.3226165771484375, loss=0.9819693565368652
I0130 01:21:06.967084 140005313861376 logging_writer.py:48] [182200] global_step=182200, grad_norm=6.083547592163086, loss=0.9687281250953674
I0130 01:21:40.464118 140005305468672 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.474062919616699, loss=0.9724991321563721
I0130 01:22:13.854902 140005313861376 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.859579563140869, loss=0.9013878703117371
I0130 01:22:47.277833 140005305468672 logging_writer.py:48] [182500] global_step=182500, grad_norm=5.878027439117432, loss=0.969122588634491
I0130 01:23:20.696799 140005313861376 logging_writer.py:48] [182600] global_step=182600, grad_norm=5.081784248352051, loss=0.919801652431488
I0130 01:23:54.105196 140005305468672 logging_writer.py:48] [182700] global_step=182700, grad_norm=5.791828632354736, loss=0.9946483373641968
I0130 01:24:27.513954 140005313861376 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.7438883781433105, loss=0.9789021015167236
I0130 01:25:00.922793 140005305468672 logging_writer.py:48] [182900] global_step=182900, grad_norm=5.706108093261719, loss=0.9575722813606262
I0130 01:25:34.324025 140005313861376 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.504880905151367, loss=0.952712893486023
I0130 01:25:54.851130 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:26:01.251929 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:26:09.856539 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:26:12.473861 140169137129280 submission_runner.py:408] Time since start: 63415.52s, 	Step: 183063, 	{'train/accuracy': 0.8834103941917419, 'train/loss': 0.41619643568992615, 'validation/accuracy': 0.7491599917411804, 'validation/loss': 1.0240086317062378, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.7243411540985107, 'test/num_examples': 10000, 'score': 61243.983364105225, 'total_duration': 63415.52132463455, 'accumulated_submission_time': 61243.983364105225, 'accumulated_eval_time': 2159.9027767181396, 'accumulated_logging_time': 5.653109788894653}
I0130 01:26:12.526195 140004616554240 logging_writer.py:48] [183063] accumulated_eval_time=2159.902777, accumulated_logging_time=5.653110, accumulated_submission_time=61243.983364, global_step=183063, preemption_count=0, score=61243.983364, test/accuracy=0.626900, test/loss=1.724341, test/num_examples=10000, total_duration=63415.521325, train/accuracy=0.883410, train/loss=0.416196, validation/accuracy=0.749160, validation/loss=1.024009, validation/num_examples=50000
I0130 01:26:25.195782 140004624946944 logging_writer.py:48] [183100] global_step=183100, grad_norm=5.814831256866455, loss=0.8984689116477966
I0130 01:26:58.542765 140004616554240 logging_writer.py:48] [183200] global_step=183200, grad_norm=5.330656051635742, loss=0.9011502861976624
I0130 01:27:31.924122 140004624946944 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.509353160858154, loss=0.972603440284729
I0130 01:28:05.424493 140004616554240 logging_writer.py:48] [183400] global_step=183400, grad_norm=5.834644317626953, loss=0.9548548460006714
I0130 01:28:38.846201 140004624946944 logging_writer.py:48] [183500] global_step=183500, grad_norm=5.491370677947998, loss=0.9068182110786438
I0130 01:29:12.249829 140004616554240 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.62469482421875, loss=0.9493815302848816
I0130 01:29:45.661741 140004624946944 logging_writer.py:48] [183700] global_step=183700, grad_norm=5.599499702453613, loss=0.8364865779876709
I0130 01:30:19.068892 140004616554240 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.443830490112305, loss=0.9207172989845276
I0130 01:30:52.480721 140004624946944 logging_writer.py:48] [183900] global_step=183900, grad_norm=5.686797142028809, loss=0.953035295009613
I0130 01:31:25.882653 140004616554240 logging_writer.py:48] [184000] global_step=184000, grad_norm=5.69807767868042, loss=0.9048853516578674
I0130 01:31:59.297068 140004624946944 logging_writer.py:48] [184100] global_step=184100, grad_norm=5.576448440551758, loss=1.1055545806884766
I0130 01:32:32.695604 140004616554240 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.522330284118652, loss=0.9774848818778992
I0130 01:33:06.094218 140004624946944 logging_writer.py:48] [184300] global_step=184300, grad_norm=5.649609565734863, loss=1.0755455493927002
I0130 01:33:39.494598 140004616554240 logging_writer.py:48] [184400] global_step=184400, grad_norm=5.383692741394043, loss=0.9405544996261597
I0130 01:34:12.990820 140004624946944 logging_writer.py:48] [184500] global_step=184500, grad_norm=5.465181350708008, loss=0.9012302160263062
I0130 01:34:42.543414 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:34:48.982623 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:34:57.654971 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:35:00.334345 140169137129280 submission_runner.py:408] Time since start: 63943.38s, 	Step: 184590, 	{'train/accuracy': 0.8830117583274841, 'train/loss': 0.41594430804252625, 'validation/accuracy': 0.7496199607849121, 'validation/loss': 1.0223222970962524, 'validation/num_examples': 50000, 'test/accuracy': 0.6244000196456909, 'test/loss': 1.7232787609100342, 'test/num_examples': 10000, 'score': 61753.94016075134, 'total_duration': 63943.381813287735, 'accumulated_submission_time': 61753.94016075134, 'accumulated_eval_time': 2177.6936724185944, 'accumulated_logging_time': 5.716378927230835}
I0130 01:35:00.386231 140005305468672 logging_writer.py:48] [184590] accumulated_eval_time=2177.693672, accumulated_logging_time=5.716379, accumulated_submission_time=61753.940161, global_step=184590, preemption_count=0, score=61753.940161, test/accuracy=0.624400, test/loss=1.723279, test/num_examples=10000, total_duration=63943.381813, train/accuracy=0.883012, train/loss=0.415944, validation/accuracy=0.749620, validation/loss=1.022322, validation/num_examples=50000
I0130 01:35:04.080760 140005313861376 logging_writer.py:48] [184600] global_step=184600, grad_norm=5.568680763244629, loss=0.953603208065033
I0130 01:35:37.441498 140005305468672 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.276055812835693, loss=0.9586228132247925
I0130 01:36:10.818036 140005313861376 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.415563106536865, loss=0.8575637340545654
I0130 01:36:44.214519 140005305468672 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.450569152832031, loss=0.9496586918830872
I0130 01:37:17.625590 140005313861376 logging_writer.py:48] [185000] global_step=185000, grad_norm=5.277897357940674, loss=0.9273945689201355
I0130 01:37:51.022330 140005305468672 logging_writer.py:48] [185100] global_step=185100, grad_norm=6.082137107849121, loss=0.9718806743621826
I0130 01:38:24.417198 140005313861376 logging_writer.py:48] [185200] global_step=185200, grad_norm=5.609554290771484, loss=0.9568398594856262
I0130 01:38:57.822866 140005305468672 logging_writer.py:48] [185300] global_step=185300, grad_norm=6.114452838897705, loss=0.9134925007820129
I0130 01:39:31.211177 140005313861376 logging_writer.py:48] [185400] global_step=185400, grad_norm=6.073182106018066, loss=0.96608567237854
I0130 01:40:04.609479 140005305468672 logging_writer.py:48] [185500] global_step=185500, grad_norm=6.039203643798828, loss=1.0028553009033203
I0130 01:40:38.096220 140005313861376 logging_writer.py:48] [185600] global_step=185600, grad_norm=5.557847499847412, loss=0.9396361112594604
I0130 01:41:11.491059 140005305468672 logging_writer.py:48] [185700] global_step=185700, grad_norm=5.417049407958984, loss=0.9848435521125793
I0130 01:41:44.906528 140005313861376 logging_writer.py:48] [185800] global_step=185800, grad_norm=5.502354621887207, loss=0.9535594582557678
I0130 01:42:18.313490 140005305468672 logging_writer.py:48] [185900] global_step=185900, grad_norm=5.583215236663818, loss=0.9914416074752808
I0130 01:42:51.714646 140005313861376 logging_writer.py:48] [186000] global_step=186000, grad_norm=5.213094711303711, loss=0.9709552526473999
I0130 01:43:25.106189 140005305468672 logging_writer.py:48] [186100] global_step=186100, grad_norm=5.707249164581299, loss=1.0169646739959717
I0130 01:43:30.596774 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:43:36.994609 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:43:45.470094 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:43:48.108869 140169137129280 submission_runner.py:408] Time since start: 64471.16s, 	Step: 186118, 	{'train/accuracy': 0.8834103941917419, 'train/loss': 0.41534796357154846, 'validation/accuracy': 0.7500199675559998, 'validation/loss': 1.0227417945861816, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.7218387126922607, 'test/num_examples': 10000, 'score': 62264.0898706913, 'total_duration': 64471.15631699562, 'accumulated_submission_time': 62264.0898706913, 'accumulated_eval_time': 2195.205705881119, 'accumulated_logging_time': 5.779400110244751}
I0130 01:43:48.162541 140005297075968 logging_writer.py:48] [186118] accumulated_eval_time=2195.205706, accumulated_logging_time=5.779400, accumulated_submission_time=62264.089871, global_step=186118, preemption_count=0, score=62264.089871, test/accuracy=0.625200, test/loss=1.721839, test/num_examples=10000, total_duration=64471.156317, train/accuracy=0.883410, train/loss=0.415348, validation/accuracy=0.750020, validation/loss=1.022742, validation/num_examples=50000
I0130 01:44:15.847046 140005330646784 logging_writer.py:48] [186200] global_step=186200, grad_norm=5.484187126159668, loss=0.9580122232437134
I0130 01:44:49.194277 140005297075968 logging_writer.py:48] [186300] global_step=186300, grad_norm=5.577254772186279, loss=0.9671863317489624
I0130 01:45:22.557597 140005330646784 logging_writer.py:48] [186400] global_step=186400, grad_norm=5.741357326507568, loss=0.9778427481651306
I0130 01:45:55.942521 140005297075968 logging_writer.py:48] [186500] global_step=186500, grad_norm=5.923628330230713, loss=1.0089800357818604
I0130 01:46:29.336780 140005330646784 logging_writer.py:48] [186600] global_step=186600, grad_norm=5.53933048248291, loss=0.910815954208374
I0130 01:46:50.967290 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:46:57.368263 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:47:06.117424 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:47:08.792928 140169137129280 submission_runner.py:408] Time since start: 64671.84s, 	Step: 186666, 	{'train/accuracy': 0.8849848508834839, 'train/loss': 0.41121968626976013, 'validation/accuracy': 0.7501599788665771, 'validation/loss': 1.0221025943756104, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.7213950157165527, 'test/num_examples': 10000, 'score': 62446.866736888885, 'total_duration': 64671.84037780762, 'accumulated_submission_time': 62446.866736888885, 'accumulated_eval_time': 2213.0312852859497, 'accumulated_logging_time': 5.842752933502197}
I0130 01:47:08.847829 140005288683264 logging_writer.py:48] [186666] accumulated_eval_time=2213.031285, accumulated_logging_time=5.842753, accumulated_submission_time=62446.866737, global_step=186666, preemption_count=0, score=62446.866737, test/accuracy=0.625800, test/loss=1.721395, test/num_examples=10000, total_duration=64671.840378, train/accuracy=0.884985, train/loss=0.411220, validation/accuracy=0.750160, validation/loss=1.022103, validation/num_examples=50000
I0130 01:47:08.892244 140005305468672 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62446.866737
I0130 01:47:09.237060 140169137129280 checkpoints.py:490] Saving checkpoint at step: 186666
I0130 01:47:10.382205 140169137129280 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4/checkpoint_186666
I0130 01:47:10.405599 140169137129280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_4/checkpoint_186666.
I0130 01:47:11.141939 140169137129280 submission_runner.py:583] Tuning trial 4/5
I0130 01:47:11.142181 140169137129280 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0130 01:47:11.151308 140169137129280 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009167729294858873, 'train/loss': 6.91040563583374, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 33.11548709869385, 'total_duration': 51.11273193359375, 'accumulated_submission_time': 33.11548709869385, 'accumulated_eval_time': 17.99710965156555, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1515, {'train/accuracy': 0.19355866312980652, 'train/loss': 4.136880397796631, 'validation/accuracy': 0.18143999576568604, 'validation/loss': 4.266019344329834, 'validation/num_examples': 50000, 'test/accuracy': 0.13379999995231628, 'test/loss': 4.854593276977539, 'test/num_examples': 10000, 'score': 543.3378028869629, 'total_duration': 579.1754071712494, 'accumulated_submission_time': 543.3378028869629, 'accumulated_eval_time': 35.76667332649231, 'accumulated_logging_time': 0.02084040641784668, 'global_step': 1515, 'preemption_count': 0}), (3030, {'train/accuracy': 0.3678053319454193, 'train/loss': 2.9227986335754395, 'validation/accuracy': 0.34421998262405396, 'validation/loss': 3.063002109527588, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.755113124847412, 'test/num_examples': 10000, 'score': 1053.532371044159, 'total_duration': 1107.5142476558685, 'accumulated_submission_time': 1053.532371044159, 'accumulated_eval_time': 53.829830169677734, 'accumulated_logging_time': 0.05278611183166504, 'global_step': 3030, 'preemption_count': 0}), (4547, {'train/accuracy': 0.409877210855484, 'train/loss': 2.6684727668762207, 'validation/accuracy': 0.38561999797821045, 'validation/loss': 2.8410749435424805, 'validation/num_examples': 50000, 'test/accuracy': 0.28300002217292786, 'test/loss': 3.582597494125366, 'test/num_examples': 10000, 'score': 1563.7822008132935, 'total_duration': 1635.7310452461243, 'accumulated_submission_time': 1563.7822008132935, 'accumulated_eval_time': 71.71818590164185, 'accumulated_logging_time': 0.08233404159545898, 'global_step': 4547, 'preemption_count': 0}), (6064, {'train/accuracy': 0.3488520383834839, 'train/loss': 3.0972437858581543, 'validation/accuracy': 0.3138200044631958, 'validation/loss': 3.3472726345062256, 'validation/num_examples': 50000, 'test/accuracy': 0.22520001232624054, 'test/loss': 4.131947994232178, 'test/num_examples': 10000, 'score': 2073.916249513626, 'total_duration': 2164.014529466629, 'accumulated_submission_time': 2073.916249513626, 'accumulated_eval_time': 89.78872632980347, 'accumulated_logging_time': 0.11142849922180176, 'global_step': 6064, 'preemption_count': 0}), (7581, {'train/accuracy': 0.29169324040412903, 'train/loss': 3.627654552459717, 'validation/accuracy': 0.27017998695373535, 'validation/loss': 3.8371315002441406, 'validation/num_examples': 50000, 'test/accuracy': 0.20400001108646393, 'test/loss': 4.534960746765137, 'test/num_examples': 10000, 'score': 2583.992983341217, 'total_duration': 2692.171564102173, 'accumulated_submission_time': 2583.992983341217, 'accumulated_eval_time': 107.7860050201416, 'accumulated_logging_time': 0.14529061317443848, 'global_step': 7581, 'preemption_count': 0}), (9099, {'train/accuracy': 0.0675223171710968, 'train/loss': 6.560290813446045, 'validation/accuracy': 0.060920000076293945, 'validation/loss': 6.683966159820557, 'validation/num_examples': 50000, 'test/accuracy': 0.047300003468990326, 'test/loss': 7.12650203704834, 'test/num_examples': 10000, 'score': 3094.0958971977234, 'total_duration': 3220.553163051605, 'accumulated_submission_time': 3094.0958971977234, 'accumulated_eval_time': 125.98636031150818, 'accumulated_logging_time': 0.17412614822387695, 'global_step': 9099, 'preemption_count': 0}), (10618, {'train/accuracy': 0.36471620202064514, 'train/loss': 3.049172878265381, 'validation/accuracy': 0.34261998534202576, 'validation/loss': 3.222270965576172, 'validation/num_examples': 50000, 'test/accuracy': 0.25460001826286316, 'test/loss': 3.9828693866729736, 'test/num_examples': 10000, 'score': 3604.2537105083466, 'total_duration': 3748.795606613159, 'accumulated_submission_time': 3604.2537105083466, 'accumulated_eval_time': 143.99125719070435, 'accumulated_logging_time': 0.2043311595916748, 'global_step': 10618, 'preemption_count': 0}), (12138, {'train/accuracy': 0.26538583636283875, 'train/loss': 3.7979533672332764, 'validation/accuracy': 0.24609999358654022, 'validation/loss': 3.9674124717712402, 'validation/num_examples': 50000, 'test/accuracy': 0.18620000779628754, 'test/loss': 4.577045440673828, 'test/num_examples': 10000, 'score': 4114.4356808662415, 'total_duration': 4277.1030423641205, 'accumulated_submission_time': 4114.4356808662415, 'accumulated_eval_time': 162.03544116020203, 'accumulated_logging_time': 0.23630690574645996, 'global_step': 12138, 'preemption_count': 0}), (13658, {'train/accuracy': 0.14144212007522583, 'train/loss': 5.066132068634033, 'validation/accuracy': 0.1360199898481369, 'validation/loss': 5.142930030822754, 'validation/num_examples': 50000, 'test/accuracy': 0.0982000082731247, 'test/loss': 5.658753395080566, 'test/num_examples': 10000, 'score': 4624.410368680954, 'total_duration': 4805.513270378113, 'accumulated_submission_time': 4624.410368680954, 'accumulated_eval_time': 180.38438057899475, 'accumulated_logging_time': 0.27373385429382324, 'global_step': 13658, 'preemption_count': 0}), (15178, {'train/accuracy': 0.29649633169174194, 'train/loss': 3.4820408821105957, 'validation/accuracy': 0.26763999462127686, 'validation/loss': 3.7009527683258057, 'validation/num_examples': 50000, 'test/accuracy': 0.20770001411437988, 'test/loss': 4.33635950088501, 'test/num_examples': 10000, 'score': 5134.47070813179, 'total_duration': 5333.5263023376465, 'accumulated_submission_time': 5134.47070813179, 'accumulated_eval_time': 198.250910282135, 'accumulated_logging_time': 0.31057238578796387, 'global_step': 15178, 'preemption_count': 0}), (16699, {'train/accuracy': 0.2660634517669678, 'train/loss': 3.838587760925293, 'validation/accuracy': 0.2495799958705902, 'validation/loss': 3.9884297847747803, 'validation/num_examples': 50000, 'test/accuracy': 0.17820000648498535, 'test/loss': 4.823949813842773, 'test/num_examples': 10000, 'score': 5644.488646507263, 'total_duration': 5861.574481487274, 'accumulated_submission_time': 5644.488646507263, 'accumulated_eval_time': 216.19500756263733, 'accumulated_logging_time': 0.34738874435424805, 'global_step': 16699, 'preemption_count': 0}), (18220, {'train/accuracy': 0.18536750972270966, 'train/loss': 4.6938252449035645, 'validation/accuracy': 0.17095999419689178, 'validation/loss': 4.821983337402344, 'validation/num_examples': 50000, 'test/accuracy': 0.12280000746250153, 'test/loss': 5.50702428817749, 'test/num_examples': 10000, 'score': 6154.5932059288025, 'total_duration': 6389.560833454132, 'accumulated_submission_time': 6154.5932059288025, 'accumulated_eval_time': 233.99508047103882, 'accumulated_logging_time': 0.37943339347839355, 'global_step': 18220, 'preemption_count': 0}), (19743, {'train/accuracy': 0.3005022406578064, 'train/loss': 3.5624327659606934, 'validation/accuracy': 0.2856999933719635, 'validation/loss': 3.7141499519348145, 'validation/num_examples': 50000, 'test/accuracy': 0.20440000295639038, 'test/loss': 4.545632839202881, 'test/num_examples': 10000, 'score': 6664.773620843887, 'total_duration': 6917.872552871704, 'accumulated_submission_time': 6664.773620843887, 'accumulated_eval_time': 252.04337310791016, 'accumulated_logging_time': 0.4124011993408203, 'global_step': 19743, 'preemption_count': 0}), (21265, {'train/accuracy': 0.24615353345870972, 'train/loss': 4.195694923400879, 'validation/accuracy': 0.2303600013256073, 'validation/loss': 4.367095470428467, 'validation/num_examples': 50000, 'test/accuracy': 0.16830000281333923, 'test/loss': 5.043724060058594, 'test/num_examples': 10000, 'score': 7174.803897380829, 'total_duration': 7445.765621185303, 'accumulated_submission_time': 7174.803897380829, 'accumulated_eval_time': 269.8241550922394, 'accumulated_logging_time': 0.4445338249206543, 'global_step': 21265, 'preemption_count': 0}), (22789, {'train/accuracy': 0.006437340285629034, 'train/loss': 10.417054176330566, 'validation/accuracy': 0.00571999978274107, 'validation/loss': 10.540876388549805, 'validation/num_examples': 50000, 'test/accuracy': 0.00430000014603138, 'test/loss': 10.739754676818848, 'test/num_examples': 10000, 'score': 7685.032256126404, 'total_duration': 7973.8614938259125, 'accumulated_submission_time': 7685.032256126404, 'accumulated_eval_time': 287.60385155677795, 'accumulated_logging_time': 0.48259806632995605, 'global_step': 22789, 'preemption_count': 0}), (24313, {'train/accuracy': 0.33952486515045166, 'train/loss': 3.138223648071289, 'validation/accuracy': 0.3097600042819977, 'validation/loss': 3.3382935523986816, 'validation/num_examples': 50000, 'test/accuracy': 0.22700001299381256, 'test/loss': 4.0918073654174805, 'test/num_examples': 10000, 'score': 8195.057115793228, 'total_duration': 8501.879445791245, 'accumulated_submission_time': 8195.057115793228, 'accumulated_eval_time': 305.51457262039185, 'accumulated_logging_time': 0.5152654647827148, 'global_step': 24313, 'preemption_count': 0}), (25837, {'train/accuracy': 0.30123963952064514, 'train/loss': 3.451519012451172, 'validation/accuracy': 0.2791000008583069, 'validation/loss': 3.6457788944244385, 'validation/num_examples': 50000, 'test/accuracy': 0.2086000144481659, 'test/loss': 4.288328170776367, 'test/num_examples': 10000, 'score': 8705.275005102158, 'total_duration': 9029.854134559631, 'accumulated_submission_time': 8705.275005102158, 'accumulated_eval_time': 323.1879549026489, 'accumulated_logging_time': 0.5490307807922363, 'global_step': 25837, 'preemption_count': 0}), (27362, {'train/accuracy': 0.07280372828245163, 'train/loss': 8.18244743347168, 'validation/accuracy': 0.06663999706506729, 'validation/loss': 8.396821975708008, 'validation/num_examples': 50000, 'test/accuracy': 0.04800000041723251, 'test/loss': 9.050444602966309, 'test/num_examples': 10000, 'score': 9215.481248617172, 'total_duration': 9558.021826505661, 'accumulated_submission_time': 9215.481248617172, 'accumulated_eval_time': 341.062353849411, 'accumulated_logging_time': 0.5862481594085693, 'global_step': 27362, 'preemption_count': 0}), (28887, {'train/accuracy': 0.2673788070678711, 'train/loss': 4.047645092010498, 'validation/accuracy': 0.24875999987125397, 'validation/loss': 4.2008957862854, 'validation/num_examples': 50000, 'test/accuracy': 0.17830000817775726, 'test/loss': 5.061367988586426, 'test/num_examples': 10000, 'score': 9725.596413373947, 'total_duration': 10086.723033189774, 'accumulated_submission_time': 9725.596413373947, 'accumulated_eval_time': 359.5661907196045, 'accumulated_logging_time': 0.6190822124481201, 'global_step': 28887, 'preemption_count': 0}), (30412, {'train/accuracy': 0.17727598547935486, 'train/loss': 5.089120864868164, 'validation/accuracy': 0.16142000257968903, 'validation/loss': 5.261129379272461, 'validation/num_examples': 50000, 'test/accuracy': 0.12190000712871552, 'test/loss': 5.848219871520996, 'test/num_examples': 10000, 'score': 10235.736080169678, 'total_duration': 10614.611628293991, 'accumulated_submission_time': 10235.736080169678, 'accumulated_eval_time': 377.23164319992065, 'accumulated_logging_time': 0.6528291702270508, 'global_step': 30412, 'preemption_count': 0}), (31937, {'train/accuracy': 0.18742027878761292, 'train/loss': 4.548458099365234, 'validation/accuracy': 0.17667999863624573, 'validation/loss': 4.695439338684082, 'validation/num_examples': 50000, 'test/accuracy': 0.1275000125169754, 'test/loss': 5.495242595672607, 'test/num_examples': 10000, 'score': 10745.85445523262, 'total_duration': 11142.47832274437, 'accumulated_submission_time': 10745.85445523262, 'accumulated_eval_time': 394.89136147499084, 'accumulated_logging_time': 0.6915583610534668, 'global_step': 31937, 'preemption_count': 0}), (33462, {'train/accuracy': 0.17797352373600006, 'train/loss': 5.004164218902588, 'validation/accuracy': 0.17418000102043152, 'validation/loss': 5.03352689743042, 'validation/num_examples': 50000, 'test/accuracy': 0.11430000513792038, 'test/loss': 6.097242832183838, 'test/num_examples': 10000, 'score': 11255.838641881943, 'total_duration': 11670.20167684555, 'accumulated_submission_time': 11255.838641881943, 'accumulated_eval_time': 412.54050064086914, 'accumulated_logging_time': 0.7317273616790771, 'global_step': 33462, 'preemption_count': 0}), (34987, {'train/accuracy': 0.18648357689380646, 'train/loss': 4.9204301834106445, 'validation/accuracy': 0.179639995098114, 'validation/loss': 5.009549140930176, 'validation/num_examples': 50000, 'test/accuracy': 0.1218000054359436, 'test/loss': 5.849211692810059, 'test/num_examples': 10000, 'score': 11765.846685171127, 'total_duration': 12198.48053264618, 'accumulated_submission_time': 11765.846685171127, 'accumulated_eval_time': 430.7237157821655, 'accumulated_logging_time': 0.7693831920623779, 'global_step': 34987, 'preemption_count': 0}), (36513, {'train/accuracy': 0.30430883169174194, 'train/loss': 3.4593262672424316, 'validation/accuracy': 0.29071998596191406, 'validation/loss': 3.5893800258636475, 'validation/num_examples': 50000, 'test/accuracy': 0.22230000793933868, 'test/loss': 4.251679420471191, 'test/num_examples': 10000, 'score': 12275.982605934143, 'total_duration': 12726.636957645416, 'accumulated_submission_time': 12275.982605934143, 'accumulated_eval_time': 448.6538984775543, 'accumulated_logging_time': 0.8097381591796875, 'global_step': 36513, 'preemption_count': 0}), (38039, {'train/accuracy': 0.2504782974720001, 'train/loss': 3.9678797721862793, 'validation/accuracy': 0.23197999596595764, 'validation/loss': 4.1183319091796875, 'validation/num_examples': 50000, 'test/accuracy': 0.1794000118970871, 'test/loss': 4.738935470581055, 'test/num_examples': 10000, 'score': 12786.193783521652, 'total_duration': 13254.94819188118, 'accumulated_submission_time': 12786.193783521652, 'accumulated_eval_time': 466.66515278816223, 'accumulated_logging_time': 0.8490216732025146, 'global_step': 38039, 'preemption_count': 0}), (39563, {'train/accuracy': 0.09614157676696777, 'train/loss': 5.963015556335449, 'validation/accuracy': 0.08895999938249588, 'validation/loss': 6.041711807250977, 'validation/num_examples': 50000, 'test/accuracy': 0.06270000338554382, 'test/loss': 6.510247230529785, 'test/num_examples': 10000, 'score': 13296.231772899628, 'total_duration': 13782.895034313202, 'accumulated_submission_time': 13296.231772899628, 'accumulated_eval_time': 484.48624563217163, 'accumulated_logging_time': 0.8864550590515137, 'global_step': 39563, 'preemption_count': 0}), (41089, {'train/accuracy': 0.0666852667927742, 'train/loss': 6.601611614227295, 'validation/accuracy': 0.05983999744057655, 'validation/loss': 6.742616653442383, 'validation/num_examples': 50000, 'test/accuracy': 0.04020000249147415, 'test/loss': 7.291037082672119, 'test/num_examples': 10000, 'score': 13806.275705337524, 'total_duration': 14311.14434671402, 'accumulated_submission_time': 13806.275705337524, 'accumulated_eval_time': 502.60431265830994, 'accumulated_logging_time': 0.9239518642425537, 'global_step': 41089, 'preemption_count': 0}), (42615, {'train/accuracy': 0.322963148355484, 'train/loss': 3.35842227935791, 'validation/accuracy': 0.3021000027656555, 'validation/loss': 3.5063271522521973, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.310806751251221, 'test/num_examples': 10000, 'score': 14316.291334152222, 'total_duration': 14839.117769956589, 'accumulated_submission_time': 14316.291334152222, 'accumulated_eval_time': 520.4744794368744, 'accumulated_logging_time': 0.9617166519165039, 'global_step': 42615, 'preemption_count': 0}), (44049, {'train/accuracy': 0.2813097834587097, 'train/loss': 3.6863856315612793, 'validation/accuracy': 0.2552799880504608, 'validation/loss': 3.8958096504211426, 'validation/num_examples': 50000, 'test/accuracy': 0.18490001559257507, 'test/loss': 4.619652271270752, 'test/num_examples': 10000, 'score': 14826.530678033829, 'total_duration': 15367.481809616089, 'accumulated_submission_time': 14826.530678033829, 'accumulated_eval_time': 538.5152575969696, 'accumulated_logging_time': 0.9980008602142334, 'global_step': 44049, 'preemption_count': 0}), (45575, {'train/accuracy': 0.3519212305545807, 'train/loss': 3.07643723487854, 'validation/accuracy': 0.3336399793624878, 'validation/loss': 3.2434942722320557, 'validation/num_examples': 50000, 'test/accuracy': 0.24820001423358917, 'test/loss': 4.0240983963012695, 'test/num_examples': 10000, 'score': 15336.509346485138, 'total_duration': 15895.369309186935, 'accumulated_submission_time': 15336.509346485138, 'accumulated_eval_time': 556.3334038257599, 'accumulated_logging_time': 1.0387768745422363, 'global_step': 45575, 'preemption_count': 0}), (47101, {'train/accuracy': 0.20083306729793549, 'train/loss': 4.566847324371338, 'validation/accuracy': 0.1885399967432022, 'validation/loss': 4.639566898345947, 'validation/num_examples': 50000, 'test/accuracy': 0.13790000975131989, 'test/loss': 5.32618522644043, 'test/num_examples': 10000, 'score': 15846.612461805344, 'total_duration': 16423.10874414444, 'accumulated_submission_time': 15846.612461805344, 'accumulated_eval_time': 573.8821873664856, 'accumulated_logging_time': 1.0766091346740723, 'global_step': 47101, 'preemption_count': 0}), (48627, {'train/accuracy': 0.2834024131298065, 'train/loss': 3.6968536376953125, 'validation/accuracy': 0.2582399845123291, 'validation/loss': 3.9303948879241943, 'validation/num_examples': 50000, 'test/accuracy': 0.20350000262260437, 'test/loss': 4.547379016876221, 'test/num_examples': 10000, 'score': 16356.553442955017, 'total_duration': 16950.91544485092, 'accumulated_submission_time': 16356.553442955017, 'accumulated_eval_time': 591.6564452648163, 'accumulated_logging_time': 1.1180686950683594, 'global_step': 48627, 'preemption_count': 0}), (50154, {'train/accuracy': 0.27758291363716125, 'train/loss': 3.7395527362823486, 'validation/accuracy': 0.2506999969482422, 'validation/loss': 3.980403423309326, 'validation/num_examples': 50000, 'test/accuracy': 0.19620001316070557, 'test/loss': 4.586901664733887, 'test/num_examples': 10000, 'score': 16866.786379098892, 'total_duration': 17479.196828603745, 'accumulated_submission_time': 16866.786379098892, 'accumulated_eval_time': 609.6140928268433, 'accumulated_logging_time': 1.1585710048675537, 'global_step': 50154, 'preemption_count': 0}), (51681, {'train/accuracy': 0.3650350570678711, 'train/loss': 3.0436782836914062, 'validation/accuracy': 0.3366599977016449, 'validation/loss': 3.240746259689331, 'validation/num_examples': 50000, 'test/accuracy': 0.2612999975681305, 'test/loss': 3.977926254272461, 'test/num_examples': 10000, 'score': 17376.93217921257, 'total_duration': 18007.572848796844, 'accumulated_submission_time': 17376.93217921257, 'accumulated_eval_time': 627.7574996948242, 'accumulated_logging_time': 1.1959059238433838, 'global_step': 51681, 'preemption_count': 0}), (53208, {'train/accuracy': 0.33207109570503235, 'train/loss': 3.289456367492676, 'validation/accuracy': 0.3113200068473816, 'validation/loss': 3.4361612796783447, 'validation/num_examples': 50000, 'test/accuracy': 0.23360000550746918, 'test/loss': 4.169589042663574, 'test/num_examples': 10000, 'score': 17887.1039853096, 'total_duration': 18535.551746606827, 'accumulated_submission_time': 17887.1039853096, 'accumulated_eval_time': 645.4750876426697, 'accumulated_logging_time': 1.2352380752563477, 'global_step': 53208, 'preemption_count': 0}), (54735, {'train/accuracy': 0.22729989886283875, 'train/loss': 4.260939121246338, 'validation/accuracy': 0.22429999709129333, 'validation/loss': 4.226137161254883, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.019404411315918, 'test/num_examples': 10000, 'score': 18397.301684379578, 'total_duration': 19063.868786096573, 'accumulated_submission_time': 18397.301684379578, 'accumulated_eval_time': 663.5008449554443, 'accumulated_logging_time': 1.2788963317871094, 'global_step': 54735, 'preemption_count': 0}), (56262, {'train/accuracy': 0.19854113459587097, 'train/loss': 4.842617511749268, 'validation/accuracy': 0.1861799955368042, 'validation/loss': 5.003037452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.14670000970363617, 'test/loss': 5.59663200378418, 'test/num_examples': 10000, 'score': 18907.414145231247, 'total_duration': 19592.039578437805, 'accumulated_submission_time': 18907.414145231247, 'accumulated_eval_time': 681.4662253856659, 'accumulated_logging_time': 1.322425127029419, 'global_step': 56262, 'preemption_count': 0}), (57789, {'train/accuracy': 0.40565210580825806, 'train/loss': 2.6804370880126953, 'validation/accuracy': 0.3668600022792816, 'validation/loss': 2.9475936889648438, 'validation/num_examples': 50000, 'test/accuracy': 0.27880001068115234, 'test/loss': 3.6488375663757324, 'test/num_examples': 10000, 'score': 19417.582760095596, 'total_duration': 20120.224281072617, 'accumulated_submission_time': 19417.582760095596, 'accumulated_eval_time': 699.392019033432, 'accumulated_logging_time': 1.3630115985870361, 'global_step': 57789, 'preemption_count': 0}), (59316, {'train/accuracy': 0.4073062837123871, 'train/loss': 2.710867166519165, 'validation/accuracy': 0.3699599802494049, 'validation/loss': 2.9452648162841797, 'validation/num_examples': 50000, 'test/accuracy': 0.28460001945495605, 'test/loss': 3.6405067443847656, 'test/num_examples': 10000, 'score': 19927.652045965195, 'total_duration': 20648.283844470978, 'accumulated_submission_time': 19927.652045965195, 'accumulated_eval_time': 717.2902994155884, 'accumulated_logging_time': 1.4048044681549072, 'global_step': 59316, 'preemption_count': 0}), (60843, {'train/accuracy': 0.23230229318141937, 'train/loss': 4.145813941955566, 'validation/accuracy': 0.22111999988555908, 'validation/loss': 4.304041862487793, 'validation/num_examples': 50000, 'test/accuracy': 0.1575000137090683, 'test/loss': 5.075519561767578, 'test/num_examples': 10000, 'score': 20437.66402554512, 'total_duration': 21176.199315071106, 'accumulated_submission_time': 20437.66402554512, 'accumulated_eval_time': 735.1041345596313, 'accumulated_logging_time': 1.4447991847991943, 'global_step': 60843, 'preemption_count': 0}), (62370, {'train/accuracy': 0.2048588991165161, 'train/loss': 4.66219425201416, 'validation/accuracy': 0.1911199986934662, 'validation/loss': 4.752766132354736, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.52890157699585, 'test/num_examples': 10000, 'score': 20947.766759634018, 'total_duration': 21704.33132839203, 'accumulated_submission_time': 20947.766759634018, 'accumulated_eval_time': 753.0401477813721, 'accumulated_logging_time': 1.4879536628723145, 'global_step': 62370, 'preemption_count': 0}), (63897, {'train/accuracy': 0.37432238459587097, 'train/loss': 2.886444091796875, 'validation/accuracy': 0.3484399914741516, 'validation/loss': 3.0589442253112793, 'validation/num_examples': 50000, 'test/accuracy': 0.2681000232696533, 'test/loss': 3.77394700050354, 'test/num_examples': 10000, 'score': 21457.712785243988, 'total_duration': 22231.957134723663, 'accumulated_submission_time': 21457.712785243988, 'accumulated_eval_time': 770.624137878418, 'accumulated_logging_time': 1.533905029296875, 'global_step': 63897, 'preemption_count': 0}), (65424, {'train/accuracy': 0.30799585580825806, 'train/loss': 3.498842716217041, 'validation/accuracy': 0.28487998247146606, 'validation/loss': 3.6848952770233154, 'validation/num_examples': 50000, 'test/accuracy': 0.21580001711845398, 'test/loss': 4.417821884155273, 'test/num_examples': 10000, 'score': 21967.712792396545, 'total_duration': 22759.64086675644, 'accumulated_submission_time': 21967.712792396545, 'accumulated_eval_time': 788.2158498764038, 'accumulated_logging_time': 1.5760498046875, 'global_step': 65424, 'preemption_count': 0}), (66951, {'train/accuracy': 0.3006616532802582, 'train/loss': 3.679957628250122, 'validation/accuracy': 0.27539998292922974, 'validation/loss': 3.886249542236328, 'validation/num_examples': 50000, 'test/accuracy': 0.20250001549720764, 'test/loss': 4.604314804077148, 'test/num_examples': 10000, 'score': 22477.6426115036, 'total_duration': 23287.309163093567, 'accumulated_submission_time': 22477.6426115036, 'accumulated_eval_time': 805.8626515865326, 'accumulated_logging_time': 1.61775541305542, 'global_step': 66951, 'preemption_count': 0}), (68479, {'train/accuracy': 0.35837849974632263, 'train/loss': 3.008211612701416, 'validation/accuracy': 0.33601999282836914, 'validation/loss': 3.1703059673309326, 'validation/num_examples': 50000, 'test/accuracy': 0.2540000081062317, 'test/loss': 3.8654260635375977, 'test/num_examples': 10000, 'score': 22987.82523560524, 'total_duration': 23815.678339004517, 'accumulated_submission_time': 22987.82523560524, 'accumulated_eval_time': 823.9564123153687, 'accumulated_logging_time': 1.6605603694915771, 'global_step': 68479, 'preemption_count': 0}), (70006, {'train/accuracy': 0.31164300441741943, 'train/loss': 3.471470355987549, 'validation/accuracy': 0.2888000011444092, 'validation/loss': 3.6245996952056885, 'validation/num_examples': 50000, 'test/accuracy': 0.21480001509189606, 'test/loss': 4.404978275299072, 'test/num_examples': 10000, 'score': 23497.832139492035, 'total_duration': 24343.43438887596, 'accumulated_submission_time': 23497.832139492035, 'accumulated_eval_time': 841.6116020679474, 'accumulated_logging_time': 1.704545497894287, 'global_step': 70006, 'preemption_count': 0}), (71533, {'train/accuracy': 0.21960698068141937, 'train/loss': 4.325572967529297, 'validation/accuracy': 0.20805999636650085, 'validation/loss': 4.440552234649658, 'validation/num_examples': 50000, 'test/accuracy': 0.16340000927448273, 'test/loss': 5.040404319763184, 'test/num_examples': 10000, 'score': 24007.757329940796, 'total_duration': 24871.24115753174, 'accumulated_submission_time': 24007.757329940796, 'accumulated_eval_time': 859.4020702838898, 'accumulated_logging_time': 1.7458949089050293, 'global_step': 71533, 'preemption_count': 0}), (73061, {'train/accuracy': 0.36025190353393555, 'train/loss': 3.1127405166625977, 'validation/accuracy': 0.33861997723579407, 'validation/loss': 3.271209239959717, 'validation/num_examples': 50000, 'test/accuracy': 0.25390002131462097, 'test/loss': 4.007626056671143, 'test/num_examples': 10000, 'score': 24517.887838363647, 'total_duration': 25399.302145957947, 'accumulated_submission_time': 24517.887838363647, 'accumulated_eval_time': 877.2425088882446, 'accumulated_logging_time': 1.785841703414917, 'global_step': 73061, 'preemption_count': 0}), (74588, {'train/accuracy': 0.28748804330825806, 'train/loss': 3.622225284576416, 'validation/accuracy': 0.262719988822937, 'validation/loss': 3.8419241905212402, 'validation/num_examples': 50000, 'test/accuracy': 0.1949000060558319, 'test/loss': 4.602164268493652, 'test/num_examples': 10000, 'score': 25027.81852698326, 'total_duration': 25927.017527341843, 'accumulated_submission_time': 25027.81852698326, 'accumulated_eval_time': 894.9329364299774, 'accumulated_logging_time': 1.829528570175171, 'global_step': 74588, 'preemption_count': 0}), (76116, {'train/accuracy': 0.38954877853393555, 'train/loss': 2.8953745365142822, 'validation/accuracy': 0.36695998907089233, 'validation/loss': 3.0703601837158203, 'validation/num_examples': 50000, 'test/accuracy': 0.2672000229358673, 'test/loss': 3.9029479026794434, 'test/num_examples': 10000, 'score': 25537.982943058014, 'total_duration': 26454.930029153824, 'accumulated_submission_time': 25537.982943058014, 'accumulated_eval_time': 912.58482670784, 'accumulated_logging_time': 1.876232624053955, 'global_step': 76116, 'preemption_count': 0}), (77644, {'train/accuracy': 0.4274752736091614, 'train/loss': 2.7035293579101562, 'validation/accuracy': 0.3989799916744232, 'validation/loss': 2.8653476238250732, 'validation/num_examples': 50000, 'test/accuracy': 0.30330002307891846, 'test/loss': 3.6137118339538574, 'test/num_examples': 10000, 'score': 26048.157845020294, 'total_duration': 26982.892572402954, 'accumulated_submission_time': 26048.157845020294, 'accumulated_eval_time': 930.2763559818268, 'accumulated_logging_time': 1.9222619533538818, 'global_step': 77644, 'preemption_count': 0}), (79171, {'train/accuracy': 0.44569116830825806, 'train/loss': 2.4578397274017334, 'validation/accuracy': 0.41425999999046326, 'validation/loss': 2.646533966064453, 'validation/num_examples': 50000, 'test/accuracy': 0.31520000100135803, 'test/loss': 3.3972880840301514, 'test/num_examples': 10000, 'score': 26558.103238105774, 'total_duration': 27510.913942098618, 'accumulated_submission_time': 26558.103238105774, 'accumulated_eval_time': 948.2588489055634, 'accumulated_logging_time': 1.9660618305206299, 'global_step': 79171, 'preemption_count': 0}), (80699, {'train/accuracy': 0.4790935814380646, 'train/loss': 2.3329217433929443, 'validation/accuracy': 0.4555400013923645, 'validation/loss': 2.4908533096313477, 'validation/num_examples': 50000, 'test/accuracy': 0.3410000205039978, 'test/loss': 3.3224074840545654, 'test/num_examples': 10000, 'score': 27068.290987730026, 'total_duration': 28039.09343481064, 'accumulated_submission_time': 27068.290987730026, 'accumulated_eval_time': 966.1588563919067, 'accumulated_logging_time': 2.0083022117614746, 'global_step': 80699, 'preemption_count': 0}), (82226, {'train/accuracy': 0.30674025416374207, 'train/loss': 3.739010810852051, 'validation/accuracy': 0.2848399877548218, 'validation/loss': 3.9017951488494873, 'validation/num_examples': 50000, 'test/accuracy': 0.20160001516342163, 'test/loss': 4.77598237991333, 'test/num_examples': 10000, 'score': 27578.205619335175, 'total_duration': 28566.787320137024, 'accumulated_submission_time': 27578.205619335175, 'accumulated_eval_time': 983.8452835083008, 'accumulated_logging_time': 2.0514883995056152, 'global_step': 82226, 'preemption_count': 0}), (83754, {'train/accuracy': 0.40670838952064514, 'train/loss': 2.7435925006866455, 'validation/accuracy': 0.37073999643325806, 'validation/loss': 3.0049564838409424, 'validation/num_examples': 50000, 'test/accuracy': 0.2864000201225281, 'test/loss': 3.7166709899902344, 'test/num_examples': 10000, 'score': 28088.363196611404, 'total_duration': 29095.082992315292, 'accumulated_submission_time': 28088.363196611404, 'accumulated_eval_time': 1001.8912694454193, 'accumulated_logging_time': 2.0932278633117676, 'global_step': 83754, 'preemption_count': 0}), (85282, {'train/accuracy': 0.2871890962123871, 'train/loss': 3.671375036239624, 'validation/accuracy': 0.26642000675201416, 'validation/loss': 3.8343710899353027, 'validation/num_examples': 50000, 'test/accuracy': 0.18240000307559967, 'test/loss': 4.670032978057861, 'test/num_examples': 10000, 'score': 28598.490182876587, 'total_duration': 29623.33106446266, 'accumulated_submission_time': 28598.490182876587, 'accumulated_eval_time': 1019.915079832077, 'accumulated_logging_time': 2.1403162479400635, 'global_step': 85282, 'preemption_count': 0}), (86809, {'train/accuracy': 0.3782086968421936, 'train/loss': 3.034822463989258, 'validation/accuracy': 0.35117998719215393, 'validation/loss': 3.2767865657806396, 'validation/num_examples': 50000, 'test/accuracy': 0.27880001068115234, 'test/loss': 3.9043896198272705, 'test/num_examples': 10000, 'score': 29108.441494226456, 'total_duration': 30151.29687857628, 'accumulated_submission_time': 29108.441494226456, 'accumulated_eval_time': 1037.8243143558502, 'accumulated_logging_time': 2.196073055267334, 'global_step': 86809, 'preemption_count': 0}), (88337, {'train/accuracy': 0.3022560477256775, 'train/loss': 3.6190991401672363, 'validation/accuracy': 0.2789599895477295, 'validation/loss': 3.844160556793213, 'validation/num_examples': 50000, 'test/accuracy': 0.21980001032352448, 'test/loss': 4.452763080596924, 'test/num_examples': 10000, 'score': 29618.62277984619, 'total_duration': 30679.39599967003, 'accumulated_submission_time': 29618.62277984619, 'accumulated_eval_time': 1055.6407935619354, 'accumulated_logging_time': 2.2476541996002197, 'global_step': 88337, 'preemption_count': 0}), (89864, {'train/accuracy': 0.5006975531578064, 'train/loss': 2.200382947921753, 'validation/accuracy': 0.47029998898506165, 'validation/loss': 2.4046542644500732, 'validation/num_examples': 50000, 'test/accuracy': 0.3614000082015991, 'test/loss': 3.230496644973755, 'test/num_examples': 10000, 'score': 30128.824651002884, 'total_duration': 31207.648649930954, 'accumulated_submission_time': 30128.824651002884, 'accumulated_eval_time': 1073.5947699546814, 'accumulated_logging_time': 2.2941675186157227, 'global_step': 89864, 'preemption_count': 0}), (91391, {'train/accuracy': 0.3496890962123871, 'train/loss': 3.236147165298462, 'validation/accuracy': 0.3291199803352356, 'validation/loss': 3.4062445163726807, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 4.2228899002075195, 'test/num_examples': 10000, 'score': 30638.732491970062, 'total_duration': 31736.439160823822, 'accumulated_submission_time': 30638.732491970062, 'accumulated_eval_time': 1092.3768472671509, 'accumulated_logging_time': 2.3449578285217285, 'global_step': 91391, 'preemption_count': 0}), (92919, {'train/accuracy': 0.40118780732154846, 'train/loss': 2.894664764404297, 'validation/accuracy': 0.37257999181747437, 'validation/loss': 3.125652313232422, 'validation/num_examples': 50000, 'test/accuracy': 0.2875000238418579, 'test/loss': 3.9007530212402344, 'test/num_examples': 10000, 'score': 31148.847939491272, 'total_duration': 32264.55832004547, 'accumulated_submission_time': 31148.847939491272, 'accumulated_eval_time': 1110.28062915802, 'accumulated_logging_time': 2.3948986530303955, 'global_step': 92919, 'preemption_count': 0}), (94446, {'train/accuracy': 0.46033960580825806, 'train/loss': 2.431842565536499, 'validation/accuracy': 0.4266199767589569, 'validation/loss': 2.6566004753112793, 'validation/num_examples': 50000, 'test/accuracy': 0.325300008058548, 'test/loss': 3.4505057334899902, 'test/num_examples': 10000, 'score': 31658.8655025959, 'total_duration': 32792.40767073631, 'accumulated_submission_time': 31658.8655025959, 'accumulated_eval_time': 1128.0174877643585, 'accumulated_logging_time': 2.43977689743042, 'global_step': 94446, 'preemption_count': 0}), (95974, {'train/accuracy': 0.20230786502361298, 'train/loss': 4.5234880447387695, 'validation/accuracy': 0.19153998792171478, 'validation/loss': 4.669929027557373, 'validation/num_examples': 50000, 'test/accuracy': 0.14240001142024994, 'test/loss': 5.283186912536621, 'test/num_examples': 10000, 'score': 32168.929438829422, 'total_duration': 33320.16366028786, 'accumulated_submission_time': 32168.929438829422, 'accumulated_eval_time': 1145.614995956421, 'accumulated_logging_time': 2.484170436859131, 'global_step': 95974, 'preemption_count': 0}), (97501, {'train/accuracy': 0.39953362941741943, 'train/loss': 2.832772731781006, 'validation/accuracy': 0.37498000264167786, 'validation/loss': 3.0190296173095703, 'validation/num_examples': 50000, 'test/accuracy': 0.2884000241756439, 'test/loss': 3.7778568267822266, 'test/num_examples': 10000, 'score': 32678.92079949379, 'total_duration': 33848.26008415222, 'accumulated_submission_time': 32678.92079949379, 'accumulated_eval_time': 1163.6204626560211, 'accumulated_logging_time': 2.5338714122772217, 'global_step': 97501, 'preemption_count': 0}), (99029, {'train/accuracy': 0.4629504084587097, 'train/loss': 2.4320366382598877, 'validation/accuracy': 0.4339199960231781, 'validation/loss': 2.634920597076416, 'validation/num_examples': 50000, 'test/accuracy': 0.32440000772476196, 'test/loss': 3.5730552673339844, 'test/num_examples': 10000, 'score': 33189.129398584366, 'total_duration': 34376.450227975845, 'accumulated_submission_time': 33189.129398584366, 'accumulated_eval_time': 1181.5033974647522, 'accumulated_logging_time': 2.582381010055542, 'global_step': 99029, 'preemption_count': 0}), (100557, {'train/accuracy': 0.41095343232154846, 'train/loss': 2.8825113773345947, 'validation/accuracy': 0.38245999813079834, 'validation/loss': 3.089705467224121, 'validation/num_examples': 50000, 'test/accuracy': 0.27900001406669617, 'test/loss': 4.0157599449157715, 'test/num_examples': 10000, 'score': 33699.24235534668, 'total_duration': 34904.74674367905, 'accumulated_submission_time': 33699.24235534668, 'accumulated_eval_time': 1199.5894927978516, 'accumulated_logging_time': 2.630051374435425, 'global_step': 100557, 'preemption_count': 0}), (102085, {'train/accuracy': 0.49404096603393555, 'train/loss': 2.256666421890259, 'validation/accuracy': 0.4540799856185913, 'validation/loss': 2.4757895469665527, 'validation/num_examples': 50000, 'test/accuracy': 0.34710001945495605, 'test/loss': 3.2543532848358154, 'test/num_examples': 10000, 'score': 34209.41844010353, 'total_duration': 35432.615013599396, 'accumulated_submission_time': 34209.41844010353, 'accumulated_eval_time': 1217.1798260211945, 'accumulated_logging_time': 2.681713342666626, 'global_step': 102085, 'preemption_count': 0}), (103612, {'train/accuracy': 0.34476640820503235, 'train/loss': 3.2952005863189697, 'validation/accuracy': 0.3284199833869934, 'validation/loss': 3.4189095497131348, 'validation/num_examples': 50000, 'test/accuracy': 0.23850001394748688, 'test/loss': 4.211745738983154, 'test/num_examples': 10000, 'score': 34719.36843562126, 'total_duration': 35960.58519363403, 'accumulated_submission_time': 34719.36843562126, 'accumulated_eval_time': 1235.0998673439026, 'accumulated_logging_time': 2.732177972793579, 'global_step': 103612, 'preemption_count': 0}), (105140, {'train/accuracy': 0.4314213991165161, 'train/loss': 2.7033612728118896, 'validation/accuracy': 0.39800000190734863, 'validation/loss': 2.949122190475464, 'validation/num_examples': 50000, 'test/accuracy': 0.31940001249313354, 'test/loss': 3.649909496307373, 'test/num_examples': 10000, 'score': 35229.46646118164, 'total_duration': 36488.632420539856, 'accumulated_submission_time': 35229.46646118164, 'accumulated_eval_time': 1252.949723482132, 'accumulated_logging_time': 2.7811789512634277, 'global_step': 105140, 'preemption_count': 0}), (106668, {'train/accuracy': 0.562898576259613, 'train/loss': 1.828281283378601, 'validation/accuracy': 0.5299199819564819, 'validation/loss': 2.0261363983154297, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7543745040893555, 'test/num_examples': 10000, 'score': 35739.570628643036, 'total_duration': 37017.070395708084, 'accumulated_submission_time': 35739.570628643036, 'accumulated_eval_time': 1271.185317993164, 'accumulated_logging_time': 2.8301868438720703, 'global_step': 106668, 'preemption_count': 0}), (108196, {'train/accuracy': 0.5349768996238708, 'train/loss': 1.9775022268295288, 'validation/accuracy': 0.48151999711990356, 'validation/loss': 2.32653546333313, 'validation/num_examples': 50000, 'test/accuracy': 0.359000027179718, 'test/loss': 3.246283531188965, 'test/num_examples': 10000, 'score': 36249.67180633545, 'total_duration': 37545.1928293705, 'accumulated_submission_time': 36249.67180633545, 'accumulated_eval_time': 1289.1059653759003, 'accumulated_logging_time': 2.8810999393463135, 'global_step': 108196, 'preemption_count': 0}), (109724, {'train/accuracy': 0.5242546200752258, 'train/loss': 2.0457510948181152, 'validation/accuracy': 0.48155999183654785, 'validation/loss': 2.3079276084899902, 'validation/num_examples': 50000, 'test/accuracy': 0.3776000142097473, 'test/loss': 3.0594658851623535, 'test/num_examples': 10000, 'score': 36759.76053881645, 'total_duration': 38073.16121888161, 'accumulated_submission_time': 36759.76053881645, 'accumulated_eval_time': 1306.8855466842651, 'accumulated_logging_time': 2.9313416481018066, 'global_step': 109724, 'preemption_count': 0}), (111252, {'train/accuracy': 0.5350366830825806, 'train/loss': 1.9782723188400269, 'validation/accuracy': 0.5010600090026855, 'validation/loss': 2.1930699348449707, 'validation/num_examples': 50000, 'test/accuracy': 0.3883000314235687, 'test/loss': 2.9401986598968506, 'test/num_examples': 10000, 'score': 37269.95954012871, 'total_duration': 38601.34153342247, 'accumulated_submission_time': 37269.95954012871, 'accumulated_eval_time': 1324.7671279907227, 'accumulated_logging_time': 2.9814400672912598, 'global_step': 111252, 'preemption_count': 0}), (112780, {'train/accuracy': 0.5105628371238708, 'train/loss': 2.158193826675415, 'validation/accuracy': 0.4708399772644043, 'validation/loss': 2.4166712760925293, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.1187973022460938, 'test/num_examples': 10000, 'score': 37780.089233636856, 'total_duration': 39129.21529483795, 'accumulated_submission_time': 37780.089233636856, 'accumulated_eval_time': 1342.4133460521698, 'accumulated_logging_time': 3.0294137001037598, 'global_step': 112780, 'preemption_count': 0}), (114308, {'train/accuracy': 0.5438257455825806, 'train/loss': 1.9540691375732422, 'validation/accuracy': 0.5044999718666077, 'validation/loss': 2.177619695663452, 'validation/num_examples': 50000, 'test/accuracy': 0.38130003213882446, 'test/loss': 3.057910680770874, 'test/num_examples': 10000, 'score': 38290.16973924637, 'total_duration': 39657.031764507294, 'accumulated_submission_time': 38290.16973924637, 'accumulated_eval_time': 1360.0454897880554, 'accumulated_logging_time': 3.0832841396331787, 'global_step': 114308, 'preemption_count': 0}), (115836, {'train/accuracy': 0.6219108700752258, 'train/loss': 1.5522531270980835, 'validation/accuracy': 0.5729599595069885, 'validation/loss': 1.8273431062698364, 'validation/num_examples': 50000, 'test/accuracy': 0.45750001072883606, 'test/loss': 2.5451416969299316, 'test/num_examples': 10000, 'score': 38800.36287164688, 'total_duration': 40185.308972120285, 'accumulated_submission_time': 38800.36287164688, 'accumulated_eval_time': 1378.0283389091492, 'accumulated_logging_time': 3.134504556655884, 'global_step': 115836, 'preemption_count': 0}), (117364, {'train/accuracy': 0.6141780614852905, 'train/loss': 1.578540563583374, 'validation/accuracy': 0.551800012588501, 'validation/loss': 1.9139325618743896, 'validation/num_examples': 50000, 'test/accuracy': 0.435200035572052, 'test/loss': 2.6567647457122803, 'test/num_examples': 10000, 'score': 39310.3577773571, 'total_duration': 40713.292598724365, 'accumulated_submission_time': 39310.3577773571, 'accumulated_eval_time': 1395.919328212738, 'accumulated_logging_time': 3.18233060836792, 'global_step': 117364, 'preemption_count': 0}), (118892, {'train/accuracy': 0.5724250674247742, 'train/loss': 1.7684458494186401, 'validation/accuracy': 0.526639997959137, 'validation/loss': 2.0348665714263916, 'validation/num_examples': 50000, 'test/accuracy': 0.4172000288963318, 'test/loss': 2.772226333618164, 'test/num_examples': 10000, 'score': 39820.552035331726, 'total_duration': 41241.165909051895, 'accumulated_submission_time': 39820.552035331726, 'accumulated_eval_time': 1413.4967761039734, 'accumulated_logging_time': 3.233930826187134, 'global_step': 118892, 'preemption_count': 0}), (120420, {'train/accuracy': 0.6161710619926453, 'train/loss': 1.5671641826629639, 'validation/accuracy': 0.5663999915122986, 'validation/loss': 1.838868498802185, 'validation/num_examples': 50000, 'test/accuracy': 0.443200021982193, 'test/loss': 2.5782828330993652, 'test/num_examples': 10000, 'score': 40330.61541700363, 'total_duration': 41768.916761636734, 'accumulated_submission_time': 40330.61541700363, 'accumulated_eval_time': 1431.0835175514221, 'accumulated_logging_time': 3.284346342086792, 'global_step': 120420, 'preemption_count': 0}), (121948, {'train/accuracy': 0.5903021097183228, 'train/loss': 1.6958752870559692, 'validation/accuracy': 0.5471799969673157, 'validation/loss': 1.95624577999115, 'validation/num_examples': 50000, 'test/accuracy': 0.43160003423690796, 'test/loss': 2.7272050380706787, 'test/num_examples': 10000, 'score': 40840.63898897171, 'total_duration': 42296.831644296646, 'accumulated_submission_time': 40840.63898897171, 'accumulated_eval_time': 1448.8743512630463, 'accumulated_logging_time': 3.3349971771240234, 'global_step': 121948, 'preemption_count': 0}), (123476, {'train/accuracy': 0.6090362071990967, 'train/loss': 1.6039897203445435, 'validation/accuracy': 0.5696600079536438, 'validation/loss': 1.830997109413147, 'validation/num_examples': 50000, 'test/accuracy': 0.43780001997947693, 'test/loss': 2.6645984649658203, 'test/num_examples': 10000, 'score': 41350.856478214264, 'total_duration': 42824.79166054726, 'accumulated_submission_time': 41350.856478214264, 'accumulated_eval_time': 1466.5170137882233, 'accumulated_logging_time': 3.384669065475464, 'global_step': 123476, 'preemption_count': 0}), (125004, {'train/accuracy': 0.6317163705825806, 'train/loss': 1.4943809509277344, 'validation/accuracy': 0.5685999989509583, 'validation/loss': 1.8384133577346802, 'validation/num_examples': 50000, 'test/accuracy': 0.44460001587867737, 'test/loss': 2.6121327877044678, 'test/num_examples': 10000, 'score': 41860.99411845207, 'total_duration': 43352.67394042015, 'accumulated_submission_time': 41860.99411845207, 'accumulated_eval_time': 1484.1569118499756, 'accumulated_logging_time': 3.4393198490142822, 'global_step': 125004, 'preemption_count': 0}), (126532, {'train/accuracy': 0.5991111397743225, 'train/loss': 1.6552882194519043, 'validation/accuracy': 0.5497399568557739, 'validation/loss': 1.9696252346038818, 'validation/num_examples': 50000, 'test/accuracy': 0.42260003089904785, 'test/loss': 2.7985992431640625, 'test/num_examples': 10000, 'score': 42371.095455646515, 'total_duration': 43880.535278081894, 'accumulated_submission_time': 42371.095455646515, 'accumulated_eval_time': 1501.814185142517, 'accumulated_logging_time': 3.4921960830688477, 'global_step': 126532, 'preemption_count': 0}), (128060, {'train/accuracy': 0.6313177347183228, 'train/loss': 1.4843790531158447, 'validation/accuracy': 0.5854200124740601, 'validation/loss': 1.7508049011230469, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.504871129989624, 'test/num_examples': 10000, 'score': 42881.19850087166, 'total_duration': 44408.43687057495, 'accumulated_submission_time': 42881.19850087166, 'accumulated_eval_time': 1519.5065422058105, 'accumulated_logging_time': 3.5481600761413574, 'global_step': 128060, 'preemption_count': 0}), (129588, {'train/accuracy': 0.620137095451355, 'train/loss': 1.5721157789230347, 'validation/accuracy': 0.5705400109291077, 'validation/loss': 1.8495970964431763, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.617003917694092, 'test/num_examples': 10000, 'score': 43391.20353627205, 'total_duration': 44936.096828222275, 'accumulated_submission_time': 43391.20353627205, 'accumulated_eval_time': 1537.0586075782776, 'accumulated_logging_time': 3.6010334491729736, 'global_step': 129588, 'preemption_count': 0}), (131116, {'train/accuracy': 0.6050900816917419, 'train/loss': 1.635583519935608, 'validation/accuracy': 0.5597000122070312, 'validation/loss': 1.9223034381866455, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.7411255836486816, 'test/num_examples': 10000, 'score': 43901.29053258896, 'total_duration': 45464.13428735733, 'accumulated_submission_time': 43901.29053258896, 'accumulated_eval_time': 1554.9034173488617, 'accumulated_logging_time': 3.656316041946411, 'global_step': 131116, 'preemption_count': 0}), (132644, {'train/accuracy': 0.6350247263908386, 'train/loss': 1.4889466762542725, 'validation/accuracy': 0.5819799900054932, 'validation/loss': 1.7952096462249756, 'validation/num_examples': 50000, 'test/accuracy': 0.45750001072883606, 'test/loss': 2.617147445678711, 'test/num_examples': 10000, 'score': 44411.385907649994, 'total_duration': 45992.00921392441, 'accumulated_submission_time': 44411.385907649994, 'accumulated_eval_time': 1572.5753400325775, 'accumulated_logging_time': 3.7136833667755127, 'global_step': 132644, 'preemption_count': 0}), (134172, {'train/accuracy': 0.6775948405265808, 'train/loss': 1.2720587253570557, 'validation/accuracy': 0.6111199855804443, 'validation/loss': 1.6209615468978882, 'validation/num_examples': 50000, 'test/accuracy': 0.48090001940727234, 'test/loss': 2.4012057781219482, 'test/num_examples': 10000, 'score': 44921.41864657402, 'total_duration': 46519.672657966614, 'accumulated_submission_time': 44921.41864657402, 'accumulated_eval_time': 1590.098914861679, 'accumulated_logging_time': 3.7708654403686523, 'global_step': 134172, 'preemption_count': 0}), (135700, {'train/accuracy': 0.6362802982330322, 'train/loss': 1.4815256595611572, 'validation/accuracy': 0.5810799598693848, 'validation/loss': 1.7779004573822021, 'validation/num_examples': 50000, 'test/accuracy': 0.4642000198364258, 'test/loss': 2.5052618980407715, 'test/num_examples': 10000, 'score': 45431.459854364395, 'total_duration': 47047.70048522949, 'accumulated_submission_time': 45431.459854364395, 'accumulated_eval_time': 1607.9817507266998, 'accumulated_logging_time': 3.824446439743042, 'global_step': 135700, 'preemption_count': 0}), (137228, {'train/accuracy': 0.672273576259613, 'train/loss': 1.3039746284484863, 'validation/accuracy': 0.6099199652671814, 'validation/loss': 1.6179298162460327, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.3209965229034424, 'test/num_examples': 10000, 'score': 45941.489257097244, 'total_duration': 47575.711570978165, 'accumulated_submission_time': 45941.489257097244, 'accumulated_eval_time': 1625.8593764305115, 'accumulated_logging_time': 3.8787858486175537, 'global_step': 137228, 'preemption_count': 0}), (138756, {'train/accuracy': 0.6704400181770325, 'train/loss': 1.3004449605941772, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.5722438097000122, 'validation/num_examples': 50000, 'test/accuracy': 0.49250003695487976, 'test/loss': 2.3523638248443604, 'test/num_examples': 10000, 'score': 46451.5012075901, 'total_duration': 48103.67853784561, 'accumulated_submission_time': 46451.5012075901, 'accumulated_eval_time': 1643.7108445167542, 'accumulated_logging_time': 3.9324073791503906, 'global_step': 138756, 'preemption_count': 0}), (140282, {'train/accuracy': 0.49563536047935486, 'train/loss': 2.2361655235290527, 'validation/accuracy': 0.46785998344421387, 'validation/loss': 2.429154634475708, 'validation/num_examples': 50000, 'test/accuracy': 0.355400025844574, 'test/loss': 3.248229503631592, 'test/num_examples': 10000, 'score': 46961.43891119957, 'total_duration': 48631.272773981094, 'accumulated_submission_time': 46961.43891119957, 'accumulated_eval_time': 1661.262745141983, 'accumulated_logging_time': 3.98689866065979, 'global_step': 140282, 'preemption_count': 0}), (141810, {'train/accuracy': 0.7058752775192261, 'train/loss': 1.1420633792877197, 'validation/accuracy': 0.6265599727630615, 'validation/loss': 1.5559496879577637, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.3541271686553955, 'test/num_examples': 10000, 'score': 47471.4825797081, 'total_duration': 49159.315249443054, 'accumulated_submission_time': 47471.4825797081, 'accumulated_eval_time': 1679.1552288532257, 'accumulated_logging_time': 4.043623924255371, 'global_step': 141810, 'preemption_count': 0}), (143338, {'train/accuracy': 0.7002949714660645, 'train/loss': 1.1588937044143677, 'validation/accuracy': 0.6348199844360352, 'validation/loss': 1.5233947038650513, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2824220657348633, 'test/num_examples': 10000, 'score': 47981.5309278965, 'total_duration': 49687.51860022545, 'accumulated_submission_time': 47981.5309278965, 'accumulated_eval_time': 1697.2029082775116, 'accumulated_logging_time': 4.101156234741211, 'global_step': 143338, 'preemption_count': 0}), (144866, {'train/accuracy': 0.7127311825752258, 'train/loss': 1.1116129159927368, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4744572639465332, 'validation/num_examples': 50000, 'test/accuracy': 0.5162000060081482, 'test/loss': 2.2296669483184814, 'test/num_examples': 10000, 'score': 48491.59139537811, 'total_duration': 50215.48052382469, 'accumulated_submission_time': 48491.59139537811, 'accumulated_eval_time': 1714.9963533878326, 'accumulated_logging_time': 4.159027576446533, 'global_step': 144866, 'preemption_count': 0}), (146394, {'train/accuracy': 0.7197863459587097, 'train/loss': 1.077343225479126, 'validation/accuracy': 0.6534799933433533, 'validation/loss': 1.424755334854126, 'validation/num_examples': 50000, 'test/accuracy': 0.5260000228881836, 'test/loss': 2.159235954284668, 'test/num_examples': 10000, 'score': 49001.642828941345, 'total_duration': 50743.13808107376, 'accumulated_submission_time': 49001.642828941345, 'accumulated_eval_time': 1732.4942715168, 'accumulated_logging_time': 4.217477798461914, 'global_step': 146394, 'preemption_count': 0}), (147922, {'train/accuracy': 0.7180524468421936, 'train/loss': 1.0938856601715088, 'validation/accuracy': 0.6541599631309509, 'validation/loss': 1.4333339929580688, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1575634479522705, 'test/num_examples': 10000, 'score': 49511.839173316956, 'total_duration': 51271.2620010376, 'accumulated_submission_time': 49511.839173316956, 'accumulated_eval_time': 1750.3120419979095, 'accumulated_logging_time': 4.277265548706055, 'global_step': 147922, 'preemption_count': 0}), (149450, {'train/accuracy': 0.7408322691917419, 'train/loss': 0.9889224767684937, 'validation/accuracy': 0.6670199632644653, 'validation/loss': 1.3719351291656494, 'validation/num_examples': 50000, 'test/accuracy': 0.5319000482559204, 'test/loss': 2.132917881011963, 'test/num_examples': 10000, 'score': 50021.862213134766, 'total_duration': 51799.39016842842, 'accumulated_submission_time': 50021.862213134766, 'accumulated_eval_time': 1768.307421207428, 'accumulated_logging_time': 4.336929559707642, 'global_step': 149450, 'preemption_count': 0}), (150978, {'train/accuracy': 0.7679368257522583, 'train/loss': 0.8675883412361145, 'validation/accuracy': 0.6796999573707581, 'validation/loss': 1.3259252309799194, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.0598459243774414, 'test/num_examples': 10000, 'score': 50531.87656021118, 'total_duration': 52327.08454442024, 'accumulated_submission_time': 50531.87656021118, 'accumulated_eval_time': 1785.8828003406525, 'accumulated_logging_time': 4.391806602478027, 'global_step': 150978, 'preemption_count': 0}), (152506, {'train/accuracy': 0.7592275142669678, 'train/loss': 0.9073374271392822, 'validation/accuracy': 0.6794599890708923, 'validation/loss': 1.3260866403579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.066560745239258, 'test/num_examples': 10000, 'score': 51042.059888124466, 'total_duration': 52855.28619623184, 'accumulated_submission_time': 51042.059888124466, 'accumulated_eval_time': 1803.7932722568512, 'accumulated_logging_time': 4.44978928565979, 'global_step': 152506, 'preemption_count': 0}), (154034, {'train/accuracy': 0.7609016299247742, 'train/loss': 0.9098778963088989, 'validation/accuracy': 0.6835599541664124, 'validation/loss': 1.3010308742523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5524000525474548, 'test/loss': 2.03236985206604, 'test/num_examples': 10000, 'score': 51552.22501087189, 'total_duration': 53383.41737794876, 'accumulated_submission_time': 51552.22501087189, 'accumulated_eval_time': 1821.6506774425507, 'accumulated_logging_time': 4.508504867553711, 'global_step': 154034, 'preemption_count': 0}), (155562, {'train/accuracy': 0.7763671875, 'train/loss': 0.8441674709320068, 'validation/accuracy': 0.6982199549674988, 'validation/loss': 1.2281646728515625, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.9566088914871216, 'test/num_examples': 10000, 'score': 52062.35582947731, 'total_duration': 53911.72821640968, 'accumulated_submission_time': 52062.35582947731, 'accumulated_eval_time': 1839.7249593734741, 'accumulated_logging_time': 4.563934326171875, 'global_step': 155562, 'preemption_count': 0}), (157090, {'train/accuracy': 0.7760881781578064, 'train/loss': 0.8410550951957703, 'validation/accuracy': 0.6970399618148804, 'validation/loss': 1.2410316467285156, 'validation/num_examples': 50000, 'test/accuracy': 0.5633000135421753, 'test/loss': 1.9879528284072876, 'test/num_examples': 10000, 'score': 52572.40886282921, 'total_duration': 54439.35976409912, 'accumulated_submission_time': 52572.40886282921, 'accumulated_eval_time': 1857.1919219493866, 'accumulated_logging_time': 4.625617980957031, 'global_step': 157090, 'preemption_count': 0}), (158618, {'train/accuracy': 0.8107461333274841, 'train/loss': 0.691912055015564, 'validation/accuracy': 0.7024999856948853, 'validation/loss': 1.2118477821350098, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.921416163444519, 'test/num_examples': 10000, 'score': 53082.36680340767, 'total_duration': 54967.49105381966, 'accumulated_submission_time': 53082.36680340767, 'accumulated_eval_time': 1875.2566223144531, 'accumulated_logging_time': 4.6845269203186035, 'global_step': 158618, 'preemption_count': 0}), (160146, {'train/accuracy': 0.8052654266357422, 'train/loss': 0.7151588201522827, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.1937520503997803, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.909941554069519, 'test/num_examples': 10000, 'score': 53592.58766222, 'total_duration': 55495.69156050682, 'accumulated_submission_time': 53592.58766222, 'accumulated_eval_time': 1893.1282756328583, 'accumulated_logging_time': 4.742824554443359, 'global_step': 160146, 'preemption_count': 0}), (161674, {'train/accuracy': 0.8055843114852905, 'train/loss': 0.7204368710517883, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.1815768480300903, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.902944564819336, 'test/num_examples': 10000, 'score': 54102.61943149567, 'total_duration': 56023.72791719437, 'accumulated_submission_time': 54102.61943149567, 'accumulated_eval_time': 1911.025577545166, 'accumulated_logging_time': 4.800848007202148, 'global_step': 161674, 'preemption_count': 0}), (163201, {'train/accuracy': 0.8162667155265808, 'train/loss': 0.6751604676246643, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.1474189758300781, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.8701530694961548, 'test/num_examples': 10000, 'score': 54612.54327106476, 'total_duration': 56551.63760781288, 'accumulated_submission_time': 54612.54327106476, 'accumulated_eval_time': 1928.901858329773, 'accumulated_logging_time': 4.860100746154785, 'global_step': 163201, 'preemption_count': 0}), (164729, {'train/accuracy': 0.8210498690605164, 'train/loss': 0.6503438353538513, 'validation/accuracy': 0.7199400067329407, 'validation/loss': 1.1440032720565796, 'validation/num_examples': 50000, 'test/accuracy': 0.5978000164031982, 'test/loss': 1.8753104209899902, 'test/num_examples': 10000, 'score': 55122.71467757225, 'total_duration': 57079.52602314949, 'accumulated_submission_time': 55122.71467757225, 'accumulated_eval_time': 1946.5123527050018, 'accumulated_logging_time': 4.916692018508911, 'global_step': 164729, 'preemption_count': 0}), (166257, {'train/accuracy': 0.8339245915412903, 'train/loss': 0.5999334454536438, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1352955102920532, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.8515959978103638, 'test/num_examples': 10000, 'score': 55632.91140437126, 'total_duration': 57607.49398231506, 'accumulated_submission_time': 55632.91140437126, 'accumulated_eval_time': 1964.1750729084015, 'accumulated_logging_time': 4.975133895874023, 'global_step': 166257, 'preemption_count': 0}), (167784, {'train/accuracy': 0.8476362824440002, 'train/loss': 0.5580589175224304, 'validation/accuracy': 0.7263399958610535, 'validation/loss': 1.1028032302856445, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.822930932044983, 'test/num_examples': 10000, 'score': 56142.81165266037, 'total_duration': 58135.56666016579, 'accumulated_submission_time': 56142.81165266037, 'accumulated_eval_time': 1982.2370581626892, 'accumulated_logging_time': 5.035584211349487, 'global_step': 167784, 'preemption_count': 0}), (169312, {'train/accuracy': 0.8518813848495483, 'train/loss': 0.5361858010292053, 'validation/accuracy': 0.7312799692153931, 'validation/loss': 1.0846476554870605, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.800598382949829, 'test/num_examples': 10000, 'score': 56652.887028455734, 'total_duration': 58664.32110500336, 'accumulated_submission_time': 56652.887028455734, 'accumulated_eval_time': 2000.8068022727966, 'accumulated_logging_time': 5.0948405265808105, 'global_step': 169312, 'preemption_count': 0}), (170840, {'train/accuracy': 0.8516421914100647, 'train/loss': 0.5271843671798706, 'validation/accuracy': 0.7345199584960938, 'validation/loss': 1.0806338787078857, 'validation/num_examples': 50000, 'test/accuracy': 0.6133000254631042, 'test/loss': 1.7768418788909912, 'test/num_examples': 10000, 'score': 57162.94032907486, 'total_duration': 59192.33838939667, 'accumulated_submission_time': 57162.94032907486, 'accumulated_eval_time': 2018.65958237648, 'accumulated_logging_time': 5.155996322631836, 'global_step': 170840, 'preemption_count': 0}), (172368, {'train/accuracy': 0.8561064600944519, 'train/loss': 0.51559978723526, 'validation/accuracy': 0.7358399629592896, 'validation/loss': 1.0692557096481323, 'validation/num_examples': 50000, 'test/accuracy': 0.6114000082015991, 'test/loss': 1.7621028423309326, 'test/num_examples': 10000, 'score': 57673.15584611893, 'total_duration': 59720.38824558258, 'accumulated_submission_time': 57673.15584611893, 'accumulated_eval_time': 2036.3830211162567, 'accumulated_logging_time': 5.2170140743255615, 'global_step': 172368, 'preemption_count': 0}), (173896, {'train/accuracy': 0.8584582209587097, 'train/loss': 0.5045436024665833, 'validation/accuracy': 0.7406799793243408, 'validation/loss': 1.059166669845581, 'validation/num_examples': 50000, 'test/accuracy': 0.6181000471115112, 'test/loss': 1.763548493385315, 'test/num_examples': 10000, 'score': 58183.20809054375, 'total_duration': 60248.25574231148, 'accumulated_submission_time': 58183.20809054375, 'accumulated_eval_time': 2054.0894026756287, 'accumulated_logging_time': 5.276186227798462, 'global_step': 173896, 'preemption_count': 0}), (175424, {'train/accuracy': 0.8757573366165161, 'train/loss': 0.44802698493003845, 'validation/accuracy': 0.7421999573707581, 'validation/loss': 1.0490195751190186, 'validation/num_examples': 50000, 'test/accuracy': 0.6177000403404236, 'test/loss': 1.7469100952148438, 'test/num_examples': 10000, 'score': 58693.388276576996, 'total_duration': 60776.315844774246, 'accumulated_submission_time': 58693.388276576996, 'accumulated_eval_time': 2071.8521168231964, 'accumulated_logging_time': 5.343605995178223, 'global_step': 175424, 'preemption_count': 0}), (176952, {'train/accuracy': 0.8742027878761292, 'train/loss': 0.44698426127433777, 'validation/accuracy': 0.7447999715805054, 'validation/loss': 1.0434011220932007, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.7508008480072021, 'test/num_examples': 10000, 'score': 59203.56213974953, 'total_duration': 61304.11006069183, 'accumulated_submission_time': 59203.56213974953, 'accumulated_eval_time': 2089.3616197109222, 'accumulated_logging_time': 5.404559135437012, 'global_step': 176952, 'preemption_count': 0}), (178480, {'train/accuracy': 0.878348171710968, 'train/loss': 0.4332602918148041, 'validation/accuracy': 0.7470600008964539, 'validation/loss': 1.0316156148910522, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.740649938583374, 'test/num_examples': 10000, 'score': 59713.66224193573, 'total_duration': 61831.87384080887, 'accumulated_submission_time': 59713.66224193573, 'accumulated_eval_time': 2106.9123561382294, 'accumulated_logging_time': 5.46753716468811, 'global_step': 178480, 'preemption_count': 0}), (180007, {'train/accuracy': 0.8775510191917419, 'train/loss': 0.4316553771495819, 'validation/accuracy': 0.7484999895095825, 'validation/loss': 1.0303864479064941, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7318052053451538, 'test/num_examples': 10000, 'score': 60223.664409160614, 'total_duration': 62359.50334787369, 'accumulated_submission_time': 60223.664409160614, 'accumulated_eval_time': 2124.426818370819, 'accumulated_logging_time': 5.530123233795166, 'global_step': 180007, 'preemption_count': 0}), (181535, {'train/accuracy': 0.883211076259613, 'train/loss': 0.4166988730430603, 'validation/accuracy': 0.7495200037956238, 'validation/loss': 1.027388334274292, 'validation/num_examples': 50000, 'test/accuracy': 0.6265000104904175, 'test/loss': 1.7263579368591309, 'test/num_examples': 10000, 'score': 60733.81407546997, 'total_duration': 62887.61942386627, 'accumulated_submission_time': 60733.81407546997, 'accumulated_eval_time': 2142.2800900936127, 'accumulated_logging_time': 5.592935800552368, 'global_step': 181535, 'preemption_count': 0}), (183063, {'train/accuracy': 0.8834103941917419, 'train/loss': 0.41619643568992615, 'validation/accuracy': 0.7491599917411804, 'validation/loss': 1.0240086317062378, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.7243411540985107, 'test/num_examples': 10000, 'score': 61243.983364105225, 'total_duration': 63415.52132463455, 'accumulated_submission_time': 61243.983364105225, 'accumulated_eval_time': 2159.9027767181396, 'accumulated_logging_time': 5.653109788894653, 'global_step': 183063, 'preemption_count': 0}), (184590, {'train/accuracy': 0.8830117583274841, 'train/loss': 0.41594430804252625, 'validation/accuracy': 0.7496199607849121, 'validation/loss': 1.0223222970962524, 'validation/num_examples': 50000, 'test/accuracy': 0.6244000196456909, 'test/loss': 1.7232787609100342, 'test/num_examples': 10000, 'score': 61753.94016075134, 'total_duration': 63943.381813287735, 'accumulated_submission_time': 61753.94016075134, 'accumulated_eval_time': 2177.6936724185944, 'accumulated_logging_time': 5.716378927230835, 'global_step': 184590, 'preemption_count': 0}), (186118, {'train/accuracy': 0.8834103941917419, 'train/loss': 0.41534796357154846, 'validation/accuracy': 0.7500199675559998, 'validation/loss': 1.0227417945861816, 'validation/num_examples': 50000, 'test/accuracy': 0.6252000331878662, 'test/loss': 1.7218387126922607, 'test/num_examples': 10000, 'score': 62264.0898706913, 'total_duration': 64471.15631699562, 'accumulated_submission_time': 62264.0898706913, 'accumulated_eval_time': 2195.205705881119, 'accumulated_logging_time': 5.779400110244751, 'global_step': 186118, 'preemption_count': 0}), (186666, {'train/accuracy': 0.8849848508834839, 'train/loss': 0.41121968626976013, 'validation/accuracy': 0.7501599788665771, 'validation/loss': 1.0221025943756104, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.7213950157165527, 'test/num_examples': 10000, 'score': 62446.866736888885, 'total_duration': 64671.84037780762, 'accumulated_submission_time': 62446.866736888885, 'accumulated_eval_time': 2213.0312852859497, 'accumulated_logging_time': 5.842752933502197, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0130 01:47:11.151747 140169137129280 submission_runner.py:586] Timing: 62446.866736888885
I0130 01:47:11.151820 140169137129280 submission_runner.py:588] Total number of evals: 124
I0130 01:47:11.151861 140169137129280 submission_runner.py:589] ====================
I0130 01:47:11.151905 140169137129280 submission_runner.py:542] Using RNG seed 3614520272
I0130 01:47:11.153624 140169137129280 submission_runner.py:551] --- Tuning run 5/5 ---
I0130 01:47:11.153761 140169137129280 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5.
I0130 01:47:11.154703 140169137129280 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5/hparams.json.
I0130 01:47:11.155541 140169137129280 submission_runner.py:206] Initializing dataset.
I0130 01:47:11.165036 140169137129280 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 01:47:11.175315 140169137129280 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 01:47:11.355719 140169137129280 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 01:47:12.215876 140169137129280 submission_runner.py:213] Initializing model.
I0130 01:47:18.018300 140169137129280 submission_runner.py:255] Initializing optimizer.
I0130 01:47:18.414615 140169137129280 submission_runner.py:262] Initializing metrics bundle.
I0130 01:47:18.414760 140169137129280 submission_runner.py:280] Initializing checkpoint and logger.
I0130 01:47:18.430140 140169137129280 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0130 01:47:18.430268 140169137129280 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 01:47:30.959480 140169137129280 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 01:47:43.298024 140169137129280 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5/flags_0.json.
I0130 01:47:43.302444 140169137129280 submission_runner.py:314] Starting training loop.
I0130 01:48:17.446246 140004608161536 logging_writer.py:48] [0] global_step=0, grad_norm=0.6634446382522583, loss=6.919498443603516
I0130 01:48:17.462388 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:48:23.848963 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:48:32.671088 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:48:35.379037 140169137129280 submission_runner.py:408] Time since start: 52.08s, 	Step: 1, 	{'train/accuracy': 0.0007772640092298388, 'train/loss': 6.909999847412109, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 34.15979290008545, 'total_duration': 52.07652568817139, 'accumulated_submission_time': 34.15979290008545, 'accumulated_eval_time': 17.916585683822632, 'accumulated_logging_time': 0}
I0130 01:48:35.389302 140004624946944 logging_writer.py:48] [1] accumulated_eval_time=17.916586, accumulated_logging_time=0, accumulated_submission_time=34.159793, global_step=1, preemption_count=0, score=34.159793, test/accuracy=0.000600, test/loss=6.910250, test/num_examples=10000, total_duration=52.076526, train/accuracy=0.000777, train/loss=6.910000, validation/accuracy=0.000960, validation/loss=6.910244, validation/num_examples=50000
I0130 01:49:08.961274 140005322254080 logging_writer.py:48] [100] global_step=100, grad_norm=0.662411093711853, loss=6.806985378265381
I0130 01:49:42.613485 140004624946944 logging_writer.py:48] [200] global_step=200, grad_norm=0.7921142578125, loss=6.5287065505981445
I0130 01:50:16.261746 140005322254080 logging_writer.py:48] [300] global_step=300, grad_norm=0.9370889663696289, loss=6.253746509552002
I0130 01:50:50.080489 140004624946944 logging_writer.py:48] [400] global_step=400, grad_norm=1.5437570810317993, loss=5.973962306976318
I0130 01:51:23.825854 140005322254080 logging_writer.py:48] [500] global_step=500, grad_norm=2.9322285652160645, loss=5.76228666305542
I0130 01:51:57.547817 140004624946944 logging_writer.py:48] [600] global_step=600, grad_norm=2.3782432079315186, loss=5.613275051116943
I0130 01:52:31.278769 140005322254080 logging_writer.py:48] [700] global_step=700, grad_norm=3.767035961151123, loss=5.424404621124268
I0130 01:53:05.019121 140004624946944 logging_writer.py:48] [800] global_step=800, grad_norm=9.465176582336426, loss=5.208443641662598
I0130 01:53:38.768971 140005322254080 logging_writer.py:48] [900] global_step=900, grad_norm=5.5914130210876465, loss=5.034794330596924
I0130 01:54:12.545339 140004624946944 logging_writer.py:48] [1000] global_step=1000, grad_norm=5.047776699066162, loss=5.03773832321167
I0130 01:54:46.298865 140005322254080 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.9609932899475098, loss=4.933892726898193
I0130 01:55:20.052014 140004624946944 logging_writer.py:48] [1200] global_step=1200, grad_norm=5.883011341094971, loss=4.87423849105835
I0130 01:55:53.808987 140005322254080 logging_writer.py:48] [1300] global_step=1300, grad_norm=5.939958095550537, loss=4.66336727142334
I0130 01:56:27.504446 140004624946944 logging_writer.py:48] [1400] global_step=1400, grad_norm=7.277763843536377, loss=4.615407943725586
I0130 01:57:01.378420 140005322254080 logging_writer.py:48] [1500] global_step=1500, grad_norm=8.378456115722656, loss=4.503154754638672
I0130 01:57:05.583908 140169137129280 spec.py:321] Evaluating on the training split.
I0130 01:57:12.012544 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 01:57:20.486901 140169137129280 spec.py:349] Evaluating on the test split.
I0130 01:57:23.252559 140169137129280 submission_runner.py:408] Time since start: 579.95s, 	Step: 1514, 	{'train/accuracy': 0.1859654039144516, 'train/loss': 4.123931407928467, 'validation/accuracy': 0.1693599969148636, 'validation/loss': 4.246126651763916, 'validation/num_examples': 50000, 'test/accuracy': 0.11990000307559967, 'test/loss': 4.764464855194092, 'test/num_examples': 10000, 'score': 544.2960352897644, 'total_duration': 579.9500658512115, 'accumulated_submission_time': 544.2960352897644, 'accumulated_eval_time': 35.58520984649658, 'accumulated_logging_time': 0.019943952560424805}
I0130 01:57:23.270137 140004608161536 logging_writer.py:48] [1514] accumulated_eval_time=35.585210, accumulated_logging_time=0.019944, accumulated_submission_time=544.296035, global_step=1514, preemption_count=0, score=544.296035, test/accuracy=0.119900, test/loss=4.764465, test/num_examples=10000, total_duration=579.950066, train/accuracy=0.185965, train/loss=4.123931, validation/accuracy=0.169360, validation/loss=4.246127, validation/num_examples=50000
I0130 01:57:52.567814 140004616554240 logging_writer.py:48] [1600] global_step=1600, grad_norm=9.112589836120605, loss=4.4334492683410645
I0130 01:58:26.236436 140004608161536 logging_writer.py:48] [1700] global_step=1700, grad_norm=9.97410774230957, loss=4.334758758544922
I0130 01:58:59.943854 140004616554240 logging_writer.py:48] [1800] global_step=1800, grad_norm=6.342648983001709, loss=4.1260881423950195
I0130 01:59:33.680235 140004608161536 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.134328603744507, loss=4.228733062744141
I0130 02:00:07.411195 140004616554240 logging_writer.py:48] [2000] global_step=2000, grad_norm=8.294041633605957, loss=3.975506067276001
I0130 02:00:41.167243 140004608161536 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.6185455322265625, loss=3.9138875007629395
I0130 02:01:14.915955 140004616554240 logging_writer.py:48] [2200] global_step=2200, grad_norm=7.4012956619262695, loss=3.8930373191833496
I0130 02:01:48.646867 140004608161536 logging_writer.py:48] [2300] global_step=2300, grad_norm=6.4673261642456055, loss=3.919340133666992
I0130 02:02:22.384627 140004616554240 logging_writer.py:48] [2400] global_step=2400, grad_norm=5.109313011169434, loss=3.6129631996154785
I0130 02:02:56.131299 140004608161536 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.180124759674072, loss=3.68550968170166
I0130 02:03:29.937295 140004616554240 logging_writer.py:48] [2600] global_step=2600, grad_norm=5.696465969085693, loss=3.6703975200653076
I0130 02:04:03.681150 140004608161536 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.598977088928223, loss=3.64896297454834
I0130 02:04:37.425544 140004616554240 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.782868385314941, loss=3.582191228866577
I0130 02:05:11.130844 140004608161536 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.7168326377868652, loss=3.5163869857788086
I0130 02:05:44.852671 140004616554240 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.646510362625122, loss=3.2684319019317627
I0130 02:05:53.425244 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:05:59.861667 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:06:08.419617 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:06:11.130319 140169137129280 submission_runner.py:408] Time since start: 1107.83s, 	Step: 3027, 	{'train/accuracy': 0.3240991532802582, 'train/loss': 3.1349141597747803, 'validation/accuracy': 0.30215999484062195, 'validation/loss': 3.293057441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.22670000791549683, 'test/loss': 3.9505279064178467, 'test/num_examples': 10000, 'score': 1054.3922047615051, 'total_duration': 1107.8278141021729, 'accumulated_submission_time': 1054.3922047615051, 'accumulated_eval_time': 53.290247440338135, 'accumulated_logging_time': 0.047429561614990234}
I0130 02:06:11.151666 140005305468672 logging_writer.py:48] [3027] accumulated_eval_time=53.290247, accumulated_logging_time=0.047430, accumulated_submission_time=1054.392205, global_step=3027, preemption_count=0, score=1054.392205, test/accuracy=0.226700, test/loss=3.950528, test/num_examples=10000, total_duration=1107.827814, train/accuracy=0.324099, train/loss=3.134914, validation/accuracy=0.302160, validation/loss=3.293057, validation/num_examples=50000
I0130 02:06:36.073811 140005313861376 logging_writer.py:48] [3100] global_step=3100, grad_norm=5.346083164215088, loss=3.360872268676758
I0130 02:07:09.736508 140005305468672 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.6739566326141357, loss=3.2096168994903564
I0130 02:07:43.439833 140005313861376 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.431290864944458, loss=3.232347249984741
I0130 02:08:17.126886 140005305468672 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.3343088626861572, loss=3.318723201751709
I0130 02:08:50.865492 140005313861376 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.647507667541504, loss=3.0643811225891113
I0130 02:09:24.549268 140005305468672 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.8464179039001465, loss=3.103618860244751
I0130 02:09:58.349385 140005313861376 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.5820045471191406, loss=3.0617446899414062
I0130 02:10:32.053016 140005305468672 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.209566116333008, loss=2.925579071044922
I0130 02:11:05.776487 140005313861376 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.639815092086792, loss=2.9792439937591553
I0130 02:11:39.464445 140005305468672 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.1272454261779785, loss=2.9588394165039062
I0130 02:12:13.160108 140005313861376 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.9611477851867676, loss=3.0068111419677734
I0130 02:12:46.848167 140005305468672 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.8221182823181152, loss=2.841334342956543
I0130 02:13:20.552222 140005313861376 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.2046875953674316, loss=2.8464927673339844
I0130 02:13:54.228021 140005305468672 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.796657085418701, loss=2.809908866882324
I0130 02:14:27.908720 140005313861376 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.673626184463501, loss=2.750340700149536
I0130 02:14:41.188046 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:14:47.643624 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:14:56.035666 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:14:58.734660 140169137129280 submission_runner.py:408] Time since start: 1635.43s, 	Step: 4541, 	{'train/accuracy': 0.5005580186843872, 'train/loss': 2.1658248901367188, 'validation/accuracy': 0.4260199964046478, 'validation/loss': 2.557955503463745, 'validation/num_examples': 50000, 'test/accuracy': 0.3175000250339508, 'test/loss': 3.3052544593811035, 'test/num_examples': 10000, 'score': 1564.3699560165405, 'total_duration': 1635.4321525096893, 'accumulated_submission_time': 1564.3699560165405, 'accumulated_eval_time': 70.83682298660278, 'accumulated_logging_time': 0.07883095741271973}
I0130 02:14:58.753314 140004624946944 logging_writer.py:48] [4541] accumulated_eval_time=70.836823, accumulated_logging_time=0.078831, accumulated_submission_time=1564.369956, global_step=4541, preemption_count=0, score=1564.369956, test/accuracy=0.317500, test/loss=3.305254, test/num_examples=10000, total_duration=1635.432153, train/accuracy=0.500558, train/loss=2.165825, validation/accuracy=0.426020, validation/loss=2.557956, validation/num_examples=50000
I0130 02:15:18.920398 140005288683264 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.5113956928253174, loss=2.695680856704712
I0130 02:15:52.489840 140004624946944 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.8868377208709717, loss=2.7234127521514893
I0130 02:16:26.265847 140005288683264 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.6611104011535645, loss=2.6802220344543457
I0130 02:16:59.899451 140004624946944 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.2165415287017822, loss=2.655442237854004
I0130 02:17:33.561243 140005288683264 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.492279052734375, loss=2.5548555850982666
I0130 02:18:07.182351 140004624946944 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.4955806732177734, loss=2.5565929412841797
I0130 02:18:40.802884 140005288683264 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.2575483322143555, loss=2.602560043334961
I0130 02:19:14.465650 140004624946944 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.270808696746826, loss=2.639178991317749
I0130 02:19:48.138226 140005288683264 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.5401742458343506, loss=2.5697948932647705
I0130 02:20:21.821022 140004624946944 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.0794336795806885, loss=2.4784319400787354
I0130 02:20:55.471065 140005288683264 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.160959243774414, loss=2.5947353839874268
I0130 02:21:29.136444 140004624946944 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.5806814432144165, loss=2.3928747177124023
I0130 02:22:02.816825 140005288683264 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.8063722848892212, loss=2.450971841812134
I0130 02:22:36.439033 140004624946944 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.0052826404571533, loss=2.4172446727752686
I0130 02:23:10.195605 140005288683264 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.2140109539031982, loss=2.3786487579345703
I0130 02:23:28.853573 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:23:35.246902 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:23:43.943510 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:23:46.670198 140169137129280 submission_runner.py:408] Time since start: 2163.37s, 	Step: 6057, 	{'train/accuracy': 0.554109513759613, 'train/loss': 1.864499568939209, 'validation/accuracy': 0.5052399635314941, 'validation/loss': 2.1542959213256836, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.921358346939087, 'test/num_examples': 10000, 'score': 2074.408198595047, 'total_duration': 2163.3676924705505, 'accumulated_submission_time': 2074.408198595047, 'accumulated_eval_time': 88.65340995788574, 'accumulated_logging_time': 0.11072349548339844}
I0130 02:23:46.691115 140004616554240 logging_writer.py:48] [6057] accumulated_eval_time=88.653410, accumulated_logging_time=0.110723, accumulated_submission_time=2074.408199, global_step=6057, preemption_count=0, score=2074.408199, test/accuracy=0.382700, test/loss=2.921358, test/num_examples=10000, total_duration=2163.367692, train/accuracy=0.554110, train/loss=1.864500, validation/accuracy=0.505240, validation/loss=2.154296, validation/num_examples=50000
I0130 02:24:01.497760 140004624946944 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.7314910888671875, loss=2.338953733444214
I0130 02:24:35.036671 140004616554240 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.1435608863830566, loss=2.3900105953216553
I0130 02:25:08.612981 140004624946944 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1886587142944336, loss=2.540639638900757
I0130 02:25:42.262326 140004616554240 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.068972587585449, loss=2.478400707244873
I0130 02:26:15.886501 140004624946944 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.02006459236145, loss=2.419764995574951
I0130 02:26:49.584275 140004616554240 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.4602367877960205, loss=2.342879295349121
I0130 02:27:23.248632 140004624946944 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.430586099624634, loss=2.3999438285827637
I0130 02:27:56.941368 140004616554240 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.3869214057922363, loss=2.319611072540283
I0130 02:28:30.611552 140004624946944 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.2819912433624268, loss=2.287858247756958
I0130 02:29:04.319218 140004616554240 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.4657785892486572, loss=2.3830020427703857
I0130 02:29:38.041712 140004624946944 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.786687970161438, loss=2.3659508228302
I0130 02:30:11.665963 140004616554240 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.8617151975631714, loss=2.378058433532715
I0130 02:30:45.316192 140004624946944 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.420892596244812, loss=2.241773843765259
I0130 02:31:18.994675 140004616554240 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.2170019149780273, loss=2.3265328407287598
I0130 02:31:52.622841 140004624946944 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.8320767879486084, loss=2.315429210662842
I0130 02:32:16.980473 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:32:23.385236 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:32:32.340878 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:32:35.070337 140169137129280 submission_runner.py:408] Time since start: 2691.77s, 	Step: 7574, 	{'train/accuracy': 0.5734215378761292, 'train/loss': 1.780144214630127, 'validation/accuracy': 0.5266799926757812, 'validation/loss': 2.0440306663513184, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.817870855331421, 'test/num_examples': 10000, 'score': 2584.638957977295, 'total_duration': 2691.767830848694, 'accumulated_submission_time': 2584.638957977295, 'accumulated_eval_time': 106.74323630332947, 'accumulated_logging_time': 0.14153456687927246}
I0130 02:32:35.089432 140004616554240 logging_writer.py:48] [7574] accumulated_eval_time=106.743236, accumulated_logging_time=0.141535, accumulated_submission_time=2584.638958, global_step=7574, preemption_count=0, score=2584.638958, test/accuracy=0.402000, test/loss=2.817871, test/num_examples=10000, total_duration=2691.767831, train/accuracy=0.573422, train/loss=1.780144, validation/accuracy=0.526680, validation/loss=2.044031, validation/num_examples=50000
I0130 02:32:44.183305 140004624946944 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.6779605150222778, loss=2.2715110778808594
I0130 02:33:17.802580 140004616554240 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.1261391639709473, loss=2.197476387023926
I0130 02:33:51.447341 140004624946944 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.349780559539795, loss=2.148977518081665
I0130 02:34:25.081190 140004616554240 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.094796657562256, loss=2.233301877975464
I0130 02:34:58.741213 140004624946944 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.0676827430725098, loss=2.1769189834594727
I0130 02:35:32.421510 140004616554240 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.898856282234192, loss=2.2424509525299072
I0130 02:36:06.146726 140004624946944 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.3989789485931396, loss=2.2393453121185303
I0130 02:36:39.815555 140004616554240 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.5630947351455688, loss=2.1480469703674316
I0130 02:37:13.458125 140004624946944 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.698141098022461, loss=2.2412593364715576
I0130 02:37:47.093105 140004616554240 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.7169671058654785, loss=2.15915846824646
I0130 02:38:20.765413 140004624946944 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.9160553216934204, loss=2.1612014770507812
I0130 02:38:54.430690 140004616554240 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.6090511083602905, loss=2.129800796508789
I0130 02:39:28.090244 140004624946944 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.8163502216339111, loss=2.2138545513153076
I0130 02:40:01.699332 140004616554240 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.9520241022109985, loss=2.142529010772705
I0130 02:40:35.356644 140004624946944 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.069413423538208, loss=2.207778215408325
I0130 02:41:05.127777 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:41:11.604036 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:41:20.445313 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:41:23.212359 140169137129280 submission_runner.py:408] Time since start: 3219.91s, 	Step: 9090, 	{'train/accuracy': 0.5981544852256775, 'train/loss': 1.6540687084197998, 'validation/accuracy': 0.5498999953269958, 'validation/loss': 1.9097604751586914, 'validation/num_examples': 50000, 'test/accuracy': 0.42760002613067627, 'test/loss': 2.6754543781280518, 'test/num_examples': 10000, 'score': 3094.6172511577606, 'total_duration': 3219.9098541736603, 'accumulated_submission_time': 3094.6172511577606, 'accumulated_eval_time': 124.82778286933899, 'accumulated_logging_time': 0.17210912704467773}
I0130 02:41:23.228721 140004616554240 logging_writer.py:48] [9090] accumulated_eval_time=124.827783, accumulated_logging_time=0.172109, accumulated_submission_time=3094.617251, global_step=9090, preemption_count=0, score=3094.617251, test/accuracy=0.427600, test/loss=2.675454, test/num_examples=10000, total_duration=3219.909854, train/accuracy=0.598154, train/loss=1.654069, validation/accuracy=0.549900, validation/loss=1.909760, validation/num_examples=50000
I0130 02:41:26.935695 140004624946944 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.072014570236206, loss=1.9992705583572388
I0130 02:42:00.524684 140004616554240 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.6421903371810913, loss=2.0575642585754395
I0130 02:42:34.149557 140004624946944 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.7905800342559814, loss=2.20491886138916
I0130 02:43:07.808590 140004616554240 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.8403804302215576, loss=2.0604007244110107
I0130 02:43:41.476737 140004624946944 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.9472613334655762, loss=2.018840789794922
I0130 02:44:15.122931 140004616554240 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.6302846670150757, loss=2.1441330909729004
I0130 02:44:48.776449 140004624946944 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.406374931335449, loss=2.181311845779419
I0130 02:45:22.382381 140004616554240 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.7310781478881836, loss=2.00235915184021
I0130 02:45:56.025227 140004624946944 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.114835739135742, loss=2.0967206954956055
I0130 02:46:29.653851 140004616554240 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.801115870475769, loss=2.0901148319244385
I0130 02:47:03.309472 140004624946944 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.611653208732605, loss=2.0566508769989014
I0130 02:47:36.961511 140004616554240 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.440297842025757, loss=2.228543996810913
I0130 02:48:10.625101 140004624946944 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5504189729690552, loss=2.1220908164978027
I0130 02:48:44.266404 140004616554240 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.0045924186706543, loss=2.15228271484375
I0130 02:49:18.035284 140004624946944 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4904695749282837, loss=1.9428311586380005
I0130 02:49:51.702311 140004616554240 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.4582160711288452, loss=1.934867024421692
I0130 02:49:53.542899 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:49:59.921859 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:50:08.828102 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:50:11.492007 140169137129280 submission_runner.py:408] Time since start: 3748.19s, 	Step: 10607, 	{'train/accuracy': 0.6220703125, 'train/loss': 1.550428032875061, 'validation/accuracy': 0.5730400085449219, 'validation/loss': 1.805853247642517, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.5722270011901855, 'test/num_examples': 10000, 'score': 3604.871292591095, 'total_duration': 3748.189495563507, 'accumulated_submission_time': 3604.871292591095, 'accumulated_eval_time': 142.7768428325653, 'accumulated_logging_time': 0.1999351978302002}
I0130 02:50:11.513307 140004624946944 logging_writer.py:48] [10607] accumulated_eval_time=142.776843, accumulated_logging_time=0.199935, accumulated_submission_time=3604.871293, global_step=10607, preemption_count=0, score=3604.871293, test/accuracy=0.443000, test/loss=2.572227, test/num_examples=10000, total_duration=3748.189496, train/accuracy=0.622070, train/loss=1.550428, validation/accuracy=0.573040, validation/loss=1.805853, validation/num_examples=50000
I0130 02:50:43.047670 140005305468672 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.4409064054489136, loss=2.0088746547698975
I0130 02:51:16.677445 140004624946944 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.7649856805801392, loss=2.119152307510376
I0130 02:51:50.315566 140005305468672 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.32635498046875, loss=1.973078727722168
I0130 02:52:23.930735 140004624946944 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.998329758644104, loss=1.9704539775848389
I0130 02:52:57.571564 140005305468672 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.4808112382888794, loss=2.0005199909210205
I0130 02:53:31.222958 140004624946944 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.570359468460083, loss=1.9087674617767334
I0130 02:54:04.859663 140005305468672 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6704204082489014, loss=2.062840461730957
I0130 02:54:38.504076 140004624946944 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.9557422399520874, loss=2.071816921234131
I0130 02:55:12.114686 140005305468672 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.6124359369277954, loss=2.018315076828003
I0130 02:55:45.847014 140004624946944 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.3612239360809326, loss=2.1294968128204346
I0130 02:56:19.477444 140005305468672 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.9638371467590332, loss=2.0376267433166504
I0130 02:56:53.102484 140004624946944 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.0451478958129883, loss=1.9492851495742798
I0130 02:57:26.762566 140005305468672 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6652535200119019, loss=2.0165584087371826
I0130 02:58:00.410957 140004624946944 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8643689155578613, loss=1.8876579999923706
I0130 02:58:34.047906 140005305468672 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.4087145328521729, loss=1.83218514919281
I0130 02:58:41.598935 140169137129280 spec.py:321] Evaluating on the training split.
I0130 02:58:47.991906 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 02:58:56.450909 140169137129280 spec.py:349] Evaluating on the test split.
I0130 02:58:59.175747 140169137129280 submission_runner.py:408] Time since start: 4275.87s, 	Step: 12124, 	{'train/accuracy': 0.6409637928009033, 'train/loss': 1.4560710191726685, 'validation/accuracy': 0.5910199880599976, 'validation/loss': 1.7010043859481812, 'validation/num_examples': 50000, 'test/accuracy': 0.4637000262737274, 'test/loss': 2.4511096477508545, 'test/num_examples': 10000, 'score': 4114.898060321808, 'total_duration': 4275.873243093491, 'accumulated_submission_time': 4114.898060321808, 'accumulated_eval_time': 160.3536171913147, 'accumulated_logging_time': 0.23119378089904785}
I0130 02:58:59.195327 140004624946944 logging_writer.py:48] [12124] accumulated_eval_time=160.353617, accumulated_logging_time=0.231194, accumulated_submission_time=4114.898060, global_step=12124, preemption_count=0, score=4114.898060, test/accuracy=0.463700, test/loss=2.451110, test/num_examples=10000, total_duration=4275.873243, train/accuracy=0.640964, train/loss=1.456071, validation/accuracy=0.591020, validation/loss=1.701004, validation/num_examples=50000
I0130 02:59:25.081672 140005288683264 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.4792150259017944, loss=1.8312458992004395
I0130 02:59:58.700484 140004624946944 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.390480399131775, loss=1.797843337059021
I0130 03:00:32.362208 140005288683264 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.6568143367767334, loss=2.0053141117095947
I0130 03:01:06.000718 140004624946944 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.7820321321487427, loss=2.020848274230957
I0130 03:01:39.658941 140005288683264 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.8933857679367065, loss=2.0123281478881836
I0130 03:02:13.364157 140004624946944 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4318681955337524, loss=1.8865737915039062
I0130 03:02:46.957195 140005288683264 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.4032509326934814, loss=1.932695746421814
I0130 03:03:20.560638 140004624946944 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.3853039741516113, loss=2.0155513286590576
I0130 03:03:54.209714 140005288683264 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2322574853897095, loss=2.0349771976470947
I0130 03:04:27.864208 140004624946944 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.1984184980392456, loss=2.0205235481262207
I0130 03:05:01.530955 140005288683264 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4748128652572632, loss=1.8524373769760132
I0130 03:05:35.140533 140004624946944 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.6486515998840332, loss=1.8630954027175903
I0130 03:06:08.773499 140005288683264 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.53644859790802, loss=2.004591941833496
I0130 03:06:42.414567 140004624946944 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6635022163391113, loss=1.8040292263031006
I0130 03:07:16.072509 140005288683264 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6419620513916016, loss=1.8703646659851074
I0130 03:07:29.330325 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:07:35.746301 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:07:44.262513 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:07:46.968563 140169137129280 submission_runner.py:408] Time since start: 4803.67s, 	Step: 13641, 	{'train/accuracy': 0.684988796710968, 'train/loss': 1.2643846273422241, 'validation/accuracy': 0.5950599908828735, 'validation/loss': 1.6960663795471191, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.450937509536743, 'test/num_examples': 10000, 'score': 4624.97420334816, 'total_duration': 4803.666056632996, 'accumulated_submission_time': 4624.97420334816, 'accumulated_eval_time': 177.99181604385376, 'accumulated_logging_time': 0.2608301639556885}
I0130 03:07:46.989393 140004616554240 logging_writer.py:48] [13641] accumulated_eval_time=177.991816, accumulated_logging_time=0.260830, accumulated_submission_time=4624.974203, global_step=13641, preemption_count=0, score=4624.974203, test/accuracy=0.470200, test/loss=2.450938, test/num_examples=10000, total_duration=4803.666057, train/accuracy=0.684989, train/loss=1.264385, validation/accuracy=0.595060, validation/loss=1.696066, validation/num_examples=50000
I0130 03:08:07.146101 140004624946944 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5292739868164062, loss=1.926525592803955
I0130 03:08:40.843112 140004616554240 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.441989541053772, loss=1.8732852935791016
I0130 03:09:14.495430 140004624946944 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.5493744611740112, loss=1.9776479005813599
I0130 03:09:48.138814 140004616554240 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.7391728162765503, loss=1.991881012916565
I0130 03:10:21.791077 140004624946944 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.4758542776107788, loss=2.034520149230957
I0130 03:10:55.460380 140004616554240 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.4859619140625, loss=2.026775360107422
I0130 03:11:29.114916 140004624946944 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.3742003440856934, loss=1.952818751335144
I0130 03:12:02.760638 140004616554240 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.564207673072815, loss=1.7999677658081055
I0130 03:12:36.401545 140004624946944 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4734119176864624, loss=1.8920437097549438
I0130 03:13:10.024032 140004616554240 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.9096205234527588, loss=2.0110890865325928
I0130 03:13:43.673776 140004624946944 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.6115074157714844, loss=2.095099687576294
I0130 03:14:17.302389 140004616554240 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.7018667459487915, loss=1.8088452816009521
I0130 03:14:50.981633 140004624946944 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.638207197189331, loss=1.8409315347671509
I0130 03:15:24.723269 140004616554240 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.674965262413025, loss=1.9071767330169678
I0130 03:15:58.398074 140004624946944 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.588201642036438, loss=1.9903554916381836
I0130 03:16:17.035063 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:16:23.555684 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:16:32.328944 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:16:35.131585 140169137129280 submission_runner.py:408] Time since start: 5331.83s, 	Step: 15157, 	{'train/accuracy': 0.671894907951355, 'train/loss': 1.3063687086105347, 'validation/accuracy': 0.6003199815750122, 'validation/loss': 1.6621453762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.469400018453598, 'test/loss': 2.4176716804504395, 'test/num_examples': 10000, 'score': 5134.960388422012, 'total_duration': 5331.829082727432, 'accumulated_submission_time': 5134.960388422012, 'accumulated_eval_time': 196.0883026123047, 'accumulated_logging_time': 0.2926368713378906}
I0130 03:16:35.155721 140005297075968 logging_writer.py:48] [15157] accumulated_eval_time=196.088303, accumulated_logging_time=0.292637, accumulated_submission_time=5134.960388, global_step=15157, preemption_count=0, score=5134.960388, test/accuracy=0.469400, test/loss=2.417672, test/num_examples=10000, total_duration=5331.829083, train/accuracy=0.671895, train/loss=1.306369, validation/accuracy=0.600320, validation/loss=1.662145, validation/num_examples=50000
I0130 03:16:49.951292 140005305468672 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.4817895889282227, loss=1.9261894226074219
I0130 03:17:23.493797 140005297075968 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.5778268575668335, loss=1.925027847290039
I0130 03:17:57.132115 140005305468672 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.4223394393920898, loss=1.8219542503356934
I0130 03:18:30.777198 140005297075968 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4438282251358032, loss=1.8685801029205322
I0130 03:19:04.437941 140005305468672 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.842989206314087, loss=1.9696787595748901
I0130 03:19:38.072324 140005297075968 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.540855884552002, loss=1.8115867376327515
I0130 03:20:11.684618 140005305468672 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.7811825275421143, loss=1.8655763864517212
I0130 03:20:45.316843 140005297075968 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.501161813735962, loss=1.8922861814498901
I0130 03:21:18.930285 140005305468672 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5349509716033936, loss=1.8418033123016357
I0130 03:21:52.700639 140005297075968 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.6132334470748901, loss=1.8680834770202637
I0130 03:22:26.356209 140005305468672 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.0649819374084473, loss=1.9738123416900635
I0130 03:22:59.983603 140005297075968 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7201730012893677, loss=1.8970513343811035
I0130 03:23:33.610062 140005305468672 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.631113052368164, loss=1.8643431663513184
I0130 03:24:07.256184 140005297075968 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.6353826522827148, loss=1.886117696762085
I0130 03:24:40.874331 140005305468672 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.9627670049667358, loss=1.7768820524215698
I0130 03:25:05.244268 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:25:11.641184 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:25:20.442640 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:25:23.231930 140169137129280 submission_runner.py:408] Time since start: 5859.93s, 	Step: 16674, 	{'train/accuracy': 0.6599569320678711, 'train/loss': 1.3636835813522339, 'validation/accuracy': 0.5981000065803528, 'validation/loss': 1.6931911706924438, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.433629035949707, 'test/num_examples': 10000, 'score': 5644.989506959915, 'total_duration': 5859.929432630539, 'accumulated_submission_time': 5644.989506959915, 'accumulated_eval_time': 214.07593870162964, 'accumulated_logging_time': 0.32698750495910645}
I0130 03:25:23.252117 140004616554240 logging_writer.py:48] [16674] accumulated_eval_time=214.075939, accumulated_logging_time=0.326988, accumulated_submission_time=5644.989507, global_step=16674, preemption_count=0, score=5644.989507, test/accuracy=0.472400, test/loss=2.433629, test/num_examples=10000, total_duration=5859.929433, train/accuracy=0.659957, train/loss=1.363684, validation/accuracy=0.598100, validation/loss=1.693191, validation/num_examples=50000
I0130 03:25:32.310300 140004624946944 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.294734239578247, loss=1.8753600120544434
I0130 03:26:05.884501 140004616554240 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.462741494178772, loss=1.9153505563735962
I0130 03:26:39.512529 140004624946944 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6346954107284546, loss=1.791763186454773
I0130 03:27:13.119127 140004616554240 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.960818886756897, loss=1.9785046577453613
I0130 03:27:46.766084 140004624946944 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4457879066467285, loss=1.8591094017028809
I0130 03:28:20.496264 140004616554240 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.1580631732940674, loss=1.8850300312042236
I0130 03:28:54.074042 140004624946944 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.7560476064682007, loss=1.842078447341919
I0130 03:29:27.676177 140004616554240 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.5518769025802612, loss=1.8623167276382446
I0130 03:30:01.345955 140004624946944 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.6193959712982178, loss=1.8341622352600098
I0130 03:30:35.012323 140004616554240 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6555371284484863, loss=1.8853628635406494
I0130 03:31:08.623761 140004624946944 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.773891568183899, loss=1.874064564704895
I0130 03:31:42.241586 140004616554240 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.4917712211608887, loss=1.8989458084106445
I0130 03:32:15.887735 140004624946944 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.626299262046814, loss=1.7964662313461304
I0130 03:32:49.550786 140004616554240 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.9979616403579712, loss=1.8853013515472412
I0130 03:33:23.146743 140004624946944 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.692962408065796, loss=1.7398126125335693
I0130 03:33:53.255155 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:33:59.637204 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:34:08.376786 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:34:11.091120 140169137129280 submission_runner.py:408] Time since start: 6387.79s, 	Step: 18191, 	{'train/accuracy': 0.6560705900192261, 'train/loss': 1.3798298835754395, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.6763943433761597, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4417972564697266, 'test/num_examples': 10000, 'score': 6154.933459997177, 'total_duration': 6387.788614749908, 'accumulated_submission_time': 6154.933459997177, 'accumulated_eval_time': 231.9118676185608, 'accumulated_logging_time': 0.35696887969970703}
I0130 03:34:11.112488 140005313861376 logging_writer.py:48] [18191] accumulated_eval_time=231.911868, accumulated_logging_time=0.356969, accumulated_submission_time=6154.933460, global_step=18191, preemption_count=0, score=6154.933460, test/accuracy=0.473500, test/loss=2.441797, test/num_examples=10000, total_duration=6387.788615, train/accuracy=0.656071, train/loss=1.379830, validation/accuracy=0.599900, validation/loss=1.676394, validation/num_examples=50000
I0130 03:34:14.521262 140005322254080 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.7593797445297241, loss=1.918588399887085
I0130 03:34:48.223784 140005313861376 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6201738119125366, loss=1.7491366863250732
I0130 03:35:21.764927 140005322254080 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.6866326332092285, loss=1.7325525283813477
I0130 03:35:55.379602 140005313861376 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.633273720741272, loss=1.8243913650512695
I0130 03:36:29.010219 140005322254080 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.7979323863983154, loss=1.8454487323760986
I0130 03:37:02.632481 140005313861376 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.7104209661483765, loss=1.784769892692566
I0130 03:37:36.261224 140005322254080 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.6306641101837158, loss=1.9588494300842285
I0130 03:38:09.889555 140005313861376 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.5796931982040405, loss=1.8452285528182983
I0130 03:38:43.522910 140005322254080 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.6870386600494385, loss=1.7270548343658447
I0130 03:39:17.126351 140005313861376 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5426409244537354, loss=1.7528190612792969
I0130 03:39:50.785942 140005322254080 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7069323062896729, loss=1.827036738395691
I0130 03:40:24.416093 140005313861376 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4800068140029907, loss=1.8790228366851807
I0130 03:40:58.053308 140005322254080 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.7020825147628784, loss=1.7150434255599976
I0130 03:41:31.783501 140005313861376 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7130017280578613, loss=1.7554596662521362
I0130 03:42:05.436434 140005322254080 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.578566551208496, loss=1.7288498878479004
I0130 03:42:39.070089 140005313861376 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.4516241550445557, loss=1.8475779294967651
I0130 03:42:41.251276 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:42:48.546777 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:42:57.439598 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:43:00.183860 140169137129280 submission_runner.py:408] Time since start: 6916.88s, 	Step: 19708, 	{'train/accuracy': 0.6623684763908386, 'train/loss': 1.3440089225769043, 'validation/accuracy': 0.6084399819374084, 'validation/loss': 1.626311182975769, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.366896867752075, 'test/num_examples': 10000, 'score': 6665.012505054474, 'total_duration': 6916.881340265274, 'accumulated_submission_time': 6665.012505054474, 'accumulated_eval_time': 250.8443946838379, 'accumulated_logging_time': 0.3887760639190674}
I0130 03:43:00.204653 140004608161536 logging_writer.py:48] [19708] accumulated_eval_time=250.844395, accumulated_logging_time=0.388776, accumulated_submission_time=6665.012505, global_step=19708, preemption_count=0, score=6665.012505, test/accuracy=0.488300, test/loss=2.366897, test/num_examples=10000, total_duration=6916.881340, train/accuracy=0.662368, train/loss=1.344009, validation/accuracy=0.608440, validation/loss=1.626311, validation/num_examples=50000
I0130 03:43:31.440017 140004616554240 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.722589373588562, loss=1.887991189956665
I0130 03:44:05.050362 140004608161536 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5688942670822144, loss=1.7361806631088257
I0130 03:44:38.693502 140004616554240 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.7550044059753418, loss=1.861768126487732
I0130 03:45:12.352150 140004608161536 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6782768964767456, loss=1.8440109491348267
I0130 03:45:45.976006 140004616554240 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.7124577760696411, loss=1.9933736324310303
I0130 03:46:19.642085 140004608161536 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.0009076595306396, loss=1.7610687017440796
I0130 03:46:53.259724 140004616554240 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.000218152999878, loss=1.7883955240249634
I0130 03:47:26.905343 140004608161536 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.614899754524231, loss=1.8169670104980469
I0130 03:48:00.644794 140004616554240 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6884875297546387, loss=1.7524558305740356
I0130 03:48:34.323669 140004608161536 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.1044938564300537, loss=1.8345742225646973
I0130 03:49:07.935160 140004616554240 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.8021198511123657, loss=1.9537928104400635
I0130 03:49:41.588578 140004608161536 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.8955414295196533, loss=1.809861183166504
I0130 03:50:15.218255 140004616554240 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.7120397090911865, loss=1.8797067403793335
I0130 03:50:48.867163 140004608161536 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.728386640548706, loss=1.9044232368469238
I0130 03:51:22.489530 140004616554240 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.5935943126678467, loss=1.780529260635376
I0130 03:51:30.395982 140169137129280 spec.py:321] Evaluating on the training split.
I0130 03:51:36.766921 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 03:51:45.318191 140169137129280 spec.py:349] Evaluating on the test split.
I0130 03:51:48.039586 140169137129280 submission_runner.py:408] Time since start: 7444.74s, 	Step: 21225, 	{'train/accuracy': 0.6512874364852905, 'train/loss': 1.397564172744751, 'validation/accuracy': 0.6033200025558472, 'validation/loss': 1.6574549674987793, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.4436450004577637, 'test/num_examples': 10000, 'score': 7175.144921779633, 'total_duration': 7444.737069368362, 'accumulated_submission_time': 7175.144921779633, 'accumulated_eval_time': 268.487957239151, 'accumulated_logging_time': 0.41996049880981445}
I0130 03:51:48.060773 140004616554240 logging_writer.py:48] [21225] accumulated_eval_time=268.487957, accumulated_logging_time=0.419960, accumulated_submission_time=7175.144922, global_step=21225, preemption_count=0, score=7175.144922, test/accuracy=0.477200, test/loss=2.443645, test/num_examples=10000, total_duration=7444.737069, train/accuracy=0.651287, train/loss=1.397564, validation/accuracy=0.603320, validation/loss=1.657455, validation/num_examples=50000
I0130 03:52:13.595276 140005313861376 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.6306225061416626, loss=1.7278196811676025
I0130 03:52:47.186656 140004616554240 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.8954517841339111, loss=1.712104082107544
I0130 03:53:20.833938 140005313861376 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.5503946542739868, loss=1.7916500568389893
I0130 03:53:54.483066 140004616554240 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.523908257484436, loss=1.7786540985107422
I0130 03:54:28.122962 140005313861376 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6466169357299805, loss=1.8331416845321655
I0130 03:55:01.826035 140004616554240 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.6027143001556396, loss=1.7564492225646973
I0130 03:55:35.473794 140005313861376 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.6931686401367188, loss=1.681165099143982
I0130 03:56:09.120874 140004616554240 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5900224447250366, loss=1.7220077514648438
I0130 03:56:42.766620 140005313861376 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.009737730026245, loss=1.7083522081375122
I0130 03:57:16.431055 140004616554240 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.9630852937698364, loss=1.8135875463485718
I0130 03:57:50.050106 140005313861376 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7339732646942139, loss=1.7593252658843994
I0130 03:58:23.692433 140004616554240 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.3990163803100586, loss=1.7484283447265625
I0130 03:58:57.315518 140005313861376 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.7634221315383911, loss=1.7676734924316406
I0130 03:59:30.930132 140004616554240 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.6935054063796997, loss=1.7202680110931396
I0130 04:00:04.574404 140005313861376 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.8258785009384155, loss=1.8042455911636353
I0130 04:00:18.189725 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:00:24.584634 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:00:33.023582 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:00:35.741520 140169137129280 submission_runner.py:408] Time since start: 7972.44s, 	Step: 22742, 	{'train/accuracy': 0.6950533986091614, 'train/loss': 1.1915576457977295, 'validation/accuracy': 0.6091399788856506, 'validation/loss': 1.6365070343017578, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.4041829109191895, 'test/num_examples': 10000, 'score': 7685.214646100998, 'total_duration': 7972.439017057419, 'accumulated_submission_time': 7685.214646100998, 'accumulated_eval_time': 286.0397162437439, 'accumulated_logging_time': 0.4515516757965088}
I0130 04:00:35.766005 140005297075968 logging_writer.py:48] [22742] accumulated_eval_time=286.039716, accumulated_logging_time=0.451552, accumulated_submission_time=7685.214646, global_step=22742, preemption_count=0, score=7685.214646, test/accuracy=0.481600, test/loss=2.404183, test/num_examples=10000, total_duration=7972.439017, train/accuracy=0.695053, train/loss=1.191558, validation/accuracy=0.609140, validation/loss=1.636507, validation/num_examples=50000
I0130 04:00:55.550701 140005305468672 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.581986904144287, loss=1.632710337638855
I0130 04:01:29.174443 140005297075968 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.5471214056015015, loss=1.7213401794433594
I0130 04:02:02.716345 140005305468672 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.8179137706756592, loss=1.7974714040756226
I0130 04:02:36.284250 140005297075968 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7469619512557983, loss=1.9609081745147705
I0130 04:03:09.915135 140005305468672 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.9007699489593506, loss=1.908287525177002
I0130 04:03:43.534558 140005297075968 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.8474897146224976, loss=1.8042699098587036
I0130 04:04:17.163572 140005305468672 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.7355382442474365, loss=1.720562219619751
I0130 04:04:50.825744 140005297075968 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.6805202960968018, loss=1.7360950708389282
I0130 04:05:24.477750 140005305468672 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.5588756799697876, loss=1.8662081956863403
I0130 04:05:58.100062 140005297075968 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.6498388051986694, loss=1.7501858472824097
I0130 04:06:31.682237 140005305468672 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.612215280532837, loss=1.8028780221939087
I0130 04:07:05.258761 140005297075968 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.7782896757125854, loss=1.7618223428726196
I0130 04:07:38.941190 140005305468672 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.545954942703247, loss=1.6701089143753052
I0130 04:08:12.525113 140005297075968 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.564337134361267, loss=1.858365774154663
I0130 04:08:46.139650 140005305468672 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.7018429040908813, loss=1.841440200805664
I0130 04:09:05.805673 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:09:12.218338 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:09:20.994966 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:09:23.771863 140169137129280 submission_runner.py:408] Time since start: 8500.47s, 	Step: 24260, 	{'train/accuracy': 0.6743263602256775, 'train/loss': 1.2909504175186157, 'validation/accuracy': 0.6072799563407898, 'validation/loss': 1.6269196271896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.375473737716675, 'test/num_examples': 10000, 'score': 8195.194686412811, 'total_duration': 8500.469371795654, 'accumulated_submission_time': 8195.194686412811, 'accumulated_eval_time': 304.0058841705322, 'accumulated_logging_time': 0.4865305423736572}
I0130 04:09:23.790332 140005288683264 logging_writer.py:48] [24260] accumulated_eval_time=304.005884, accumulated_logging_time=0.486531, accumulated_submission_time=8195.194686, global_step=24260, preemption_count=0, score=8195.194686, test/accuracy=0.482100, test/loss=2.375474, test/num_examples=10000, total_duration=8500.469372, train/accuracy=0.674326, train/loss=1.290950, validation/accuracy=0.607280, validation/loss=1.626920, validation/num_examples=50000
I0130 04:09:37.591676 140005313861376 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6976120471954346, loss=1.839519739151001
I0130 04:10:11.185032 140005288683264 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.5076960325241089, loss=1.804872989654541
I0130 04:10:44.806265 140005313861376 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.703249216079712, loss=1.785123348236084
I0130 04:11:18.423824 140005288683264 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.619272232055664, loss=1.717498779296875
I0130 04:11:52.066160 140005313861376 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.8405749797821045, loss=1.8916125297546387
I0130 04:12:25.689023 140005288683264 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.7779461145401, loss=1.7646433115005493
I0130 04:12:59.339617 140005313861376 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.7757594585418701, loss=1.737648367881775
I0130 04:13:32.974020 140005288683264 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.8883161544799805, loss=1.8524221181869507
I0130 04:14:06.677062 140005313861376 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.6728451251983643, loss=1.674109935760498
I0130 04:14:40.305851 140005288683264 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.8736565113067627, loss=1.7201344966888428
I0130 04:15:13.992132 140005313861376 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.8873649835586548, loss=1.7679275274276733
I0130 04:15:47.617699 140005288683264 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.8361811637878418, loss=1.747551679611206
I0130 04:16:21.227284 140005313861376 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.6433230638504028, loss=1.6739736795425415
I0130 04:16:54.798169 140005288683264 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.6087387800216675, loss=1.7418158054351807
I0130 04:17:28.414574 140005313861376 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.7214710712432861, loss=1.6238303184509277
I0130 04:17:53.799288 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:18:00.268349 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:18:09.078590 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:18:11.786085 140169137129280 submission_runner.py:408] Time since start: 9028.48s, 	Step: 25777, 	{'train/accuracy': 0.6813815236091614, 'train/loss': 1.2523821592330933, 'validation/accuracy': 0.6219599843025208, 'validation/loss': 1.5588935613632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.291343927383423, 'test/num_examples': 10000, 'score': 8705.145479679108, 'total_duration': 9028.483579874039, 'accumulated_submission_time': 8705.145479679108, 'accumulated_eval_time': 321.99264454841614, 'accumulated_logging_time': 0.5148470401763916}
I0130 04:18:11.809036 140004624946944 logging_writer.py:48] [25777] accumulated_eval_time=321.992645, accumulated_logging_time=0.514847, accumulated_submission_time=8705.145480, global_step=25777, preemption_count=0, score=8705.145480, test/accuracy=0.500400, test/loss=2.291344, test/num_examples=10000, total_duration=9028.483580, train/accuracy=0.681382, train/loss=1.252382, validation/accuracy=0.621960, validation/loss=1.558894, validation/num_examples=50000
I0130 04:18:19.867208 140005297075968 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.9731874465942383, loss=1.8020730018615723
I0130 04:18:53.440520 140004624946944 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7477867603302002, loss=1.8126914501190186
I0130 04:19:27.073655 140005297075968 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8245376348495483, loss=1.7439852952957153
I0130 04:20:00.727350 140004624946944 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.6964384317398071, loss=1.759995937347412
I0130 04:20:34.354621 140005297075968 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6922187805175781, loss=1.6838890314102173
I0130 04:21:08.113530 140004624946944 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6797670125961304, loss=1.7908211946487427
I0130 04:21:41.742217 140005297075968 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.679548978805542, loss=1.7853834629058838
I0130 04:22:15.386729 140004624946944 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.8519631624221802, loss=1.76706862449646
I0130 04:22:49.015461 140005297075968 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6306707859039307, loss=1.851375937461853
I0130 04:23:22.657301 140004624946944 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8973711729049683, loss=1.8429371118545532
I0130 04:23:56.289846 140005297075968 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.706807255744934, loss=1.705580472946167
I0130 04:24:29.933616 140004624946944 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.6229832172393799, loss=1.6789591312408447
I0130 04:25:03.562025 140005297075968 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7151203155517578, loss=1.567615032196045
I0130 04:25:37.186349 140004624946944 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.72966730594635, loss=1.892920970916748
I0130 04:26:10.827895 140005297075968 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.6331790685653687, loss=1.6643874645233154
I0130 04:26:41.928569 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:26:48.329622 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:26:56.889703 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:26:59.583956 140169137129280 submission_runner.py:408] Time since start: 9556.28s, 	Step: 27294, 	{'train/accuracy': 0.6639030575752258, 'train/loss': 1.3441171646118164, 'validation/accuracy': 0.6081399917602539, 'validation/loss': 1.6323024034500122, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.400562047958374, 'test/num_examples': 10000, 'score': 9215.205620288849, 'total_duration': 9556.281441688538, 'accumulated_submission_time': 9215.205620288849, 'accumulated_eval_time': 339.64798951148987, 'accumulated_logging_time': 0.5484886169433594}
I0130 04:26:59.606926 140005288683264 logging_writer.py:48] [27294] accumulated_eval_time=339.647990, accumulated_logging_time=0.548489, accumulated_submission_time=9215.205620, global_step=27294, preemption_count=0, score=9215.205620, test/accuracy=0.477200, test/loss=2.400562, test/num_examples=10000, total_duration=9556.281442, train/accuracy=0.663903, train/loss=1.344117, validation/accuracy=0.608140, validation/loss=1.632302, validation/num_examples=50000
I0130 04:27:01.965557 140005313861376 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.7209911346435547, loss=1.6903640031814575
I0130 04:27:35.536595 140005288683264 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.7125943899154663, loss=1.7304377555847168
I0130 04:28:09.131029 140005313861376 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5966582298278809, loss=1.7584421634674072
I0130 04:28:42.706949 140005288683264 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.9689799547195435, loss=1.7311300039291382
I0130 04:29:16.261829 140005313861376 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.7755308151245117, loss=1.745973825454712
I0130 04:29:49.887383 140005288683264 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.680444598197937, loss=1.8004182577133179
I0130 04:30:23.523859 140005313861376 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.229512929916382, loss=1.6577577590942383
I0130 04:30:57.164086 140005288683264 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.613448977470398, loss=1.7273305654525757
I0130 04:31:30.797191 140005313861376 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.701870083808899, loss=1.8422260284423828
I0130 04:32:04.439319 140005288683264 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.8569433689117432, loss=1.732136607170105
I0130 04:32:38.057845 140005313861376 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.9311035871505737, loss=1.6994867324829102
I0130 04:33:11.642082 140005288683264 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.887332797050476, loss=1.8178033828735352
I0130 04:33:45.304977 140005313861376 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.6704626083374023, loss=1.6435996294021606
I0130 04:34:18.899483 140005288683264 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.7626471519470215, loss=1.6413453817367554
I0130 04:34:52.458005 140005313861376 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.038271903991699, loss=1.7974008321762085
I0130 04:35:26.065007 140005288683264 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.879499912261963, loss=1.7636005878448486
I0130 04:35:29.586970 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:35:35.977139 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:35:44.853618 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:35:47.579312 140169137129280 submission_runner.py:408] Time since start: 10084.28s, 	Step: 28812, 	{'train/accuracy': 0.6672313213348389, 'train/loss': 1.3261438608169556, 'validation/accuracy': 0.6168999671936035, 'validation/loss': 1.6034150123596191, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.371846914291382, 'test/num_examples': 10000, 'score': 9725.125751495361, 'total_duration': 10084.276794195175, 'accumulated_submission_time': 9725.125751495361, 'accumulated_eval_time': 357.6402759552002, 'accumulated_logging_time': 0.5823171138763428}
I0130 04:35:47.601238 140004608161536 logging_writer.py:48] [28812] accumulated_eval_time=357.640276, accumulated_logging_time=0.582317, accumulated_submission_time=9725.125751, global_step=28812, preemption_count=0, score=9725.125751, test/accuracy=0.486000, test/loss=2.371847, test/num_examples=10000, total_duration=10084.276794, train/accuracy=0.667231, train/loss=1.326144, validation/accuracy=0.616900, validation/loss=1.603415, validation/num_examples=50000
I0130 04:36:17.473030 140004616554240 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.824644923210144, loss=1.773034691810608
I0130 04:36:51.108760 140004608161536 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.7055045366287231, loss=1.7131215333938599
I0130 04:37:24.745659 140004616554240 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.8613131046295166, loss=1.6210658550262451
I0130 04:37:58.382081 140004608161536 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.5365341901779175, loss=1.64093017578125
I0130 04:38:32.037270 140004616554240 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.6848435401916504, loss=1.625530481338501
I0130 04:39:05.681379 140004608161536 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.973893642425537, loss=1.7090095281600952
I0130 04:39:39.301192 140004616554240 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.7433578968048096, loss=1.7818748950958252
I0130 04:40:13.051957 140004608161536 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.7980527877807617, loss=1.6869642734527588
I0130 04:40:46.704237 140004616554240 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.666485071182251, loss=1.770626425743103
I0130 04:41:20.351397 140004608161536 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.780643105506897, loss=1.7704691886901855
I0130 04:41:53.979990 140004616554240 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.7421278953552246, loss=1.7487914562225342
I0130 04:42:27.630643 140004608161536 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.8130629062652588, loss=1.7664326429367065
I0130 04:43:01.258181 140004616554240 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.8790650367736816, loss=1.703492283821106
I0130 04:43:34.886465 140004608161536 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.121685743331909, loss=1.6373385190963745
I0130 04:44:08.497794 140004616554240 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7389684915542603, loss=1.8370122909545898
I0130 04:44:17.732136 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:44:24.115110 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:44:32.828999 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:44:35.557882 140169137129280 submission_runner.py:408] Time since start: 10612.26s, 	Step: 30329, 	{'train/accuracy': 0.6734893321990967, 'train/loss': 1.2919286489486694, 'validation/accuracy': 0.610260009765625, 'validation/loss': 1.616947889328003, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.3278603553771973, 'test/num_examples': 10000, 'score': 10235.195373535156, 'total_duration': 10612.255376338959, 'accumulated_submission_time': 10235.195373535156, 'accumulated_eval_time': 375.4659821987152, 'accumulated_logging_time': 0.6163196563720703}
I0130 04:44:35.583028 140004616554240 logging_writer.py:48] [30329] accumulated_eval_time=375.465982, accumulated_logging_time=0.616320, accumulated_submission_time=10235.195374, global_step=30329, preemption_count=0, score=10235.195374, test/accuracy=0.487800, test/loss=2.327860, test/num_examples=10000, total_duration=10612.255376, train/accuracy=0.673489, train/loss=1.291929, validation/accuracy=0.610260, validation/loss=1.616948, validation/num_examples=50000
I0130 04:44:59.714869 140005305468672 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.6676462888717651, loss=1.7094343900680542
I0130 04:45:33.250008 140004616554240 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.0218968391418457, loss=1.7180321216583252
I0130 04:46:06.798238 140005305468672 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.6565768718719482, loss=1.637356162071228
I0130 04:46:40.344335 140004616554240 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7092136144638062, loss=1.7936038970947266
I0130 04:47:14.092946 140005305468672 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.736282467842102, loss=1.683839201927185
I0130 04:47:47.751552 140004616554240 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.8735954761505127, loss=1.7634124755859375
I0130 04:48:21.383442 140005305468672 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.9905239343643188, loss=1.6756678819656372
I0130 04:48:55.035023 140004616554240 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.69333016872406, loss=1.7775442600250244
I0130 04:49:28.640662 140005305468672 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.683361291885376, loss=1.592121958732605
I0130 04:50:02.288099 140004616554240 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.735798954963684, loss=1.642956256866455
I0130 04:50:35.883447 140005305468672 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6845059394836426, loss=1.7396137714385986
I0130 04:51:09.523324 140004616554240 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.63550865650177, loss=1.6259706020355225
I0130 04:51:43.149410 140005305468672 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.7621625661849976, loss=1.6161433458328247
I0130 04:52:16.785390 140004616554240 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.864574670791626, loss=1.7337960004806519
I0130 04:52:50.421528 140005305468672 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.9096862077713013, loss=1.8430609703063965
I0130 04:53:05.699633 140169137129280 spec.py:321] Evaluating on the training split.
I0130 04:53:12.089306 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 04:53:20.682274 140169137129280 spec.py:349] Evaluating on the test split.
I0130 04:53:23.836708 140169137129280 submission_runner.py:408] Time since start: 11140.53s, 	Step: 31847, 	{'train/accuracy': 0.7060347199440002, 'train/loss': 1.1288166046142578, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.5537689924240112, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.2625062465667725, 'test/num_examples': 10000, 'score': 10745.252250671387, 'total_duration': 11140.534215211868, 'accumulated_submission_time': 10745.252250671387, 'accumulated_eval_time': 393.60303115844727, 'accumulated_logging_time': 0.6519203186035156}
I0130 04:53:23.859344 140005288683264 logging_writer.py:48] [31847] accumulated_eval_time=393.603031, accumulated_logging_time=0.651920, accumulated_submission_time=10745.252251, global_step=31847, preemption_count=0, score=10745.252251, test/accuracy=0.499500, test/loss=2.262506, test/num_examples=10000, total_duration=11140.534215, train/accuracy=0.706035, train/loss=1.128817, validation/accuracy=0.624800, validation/loss=1.553769, validation/num_examples=50000
I0130 04:53:41.995458 140005297075968 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.7807997465133667, loss=1.809692621231079
I0130 04:54:15.593038 140005288683264 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.7268800735473633, loss=1.83298659324646
I0130 04:54:49.234533 140005297075968 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.784283995628357, loss=1.6061663627624512
I0130 04:55:22.862318 140005288683264 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.7388286590576172, loss=1.686065673828125
I0130 04:55:56.497044 140005297075968 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.7277469635009766, loss=1.612163782119751
I0130 04:56:30.128405 140005288683264 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.7352904081344604, loss=1.6093049049377441
I0130 04:57:03.779172 140005297075968 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.9213762283325195, loss=1.7129713296890259
I0130 04:57:37.409708 140005288683264 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.7940866947174072, loss=1.7149341106414795
I0130 04:58:11.053302 140005297075968 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.1239869594573975, loss=1.6638092994689941
I0130 04:58:44.686314 140005288683264 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.1220288276672363, loss=1.6846832036972046
I0130 04:59:18.306627 140005297075968 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.6646995544433594, loss=1.658566951751709
I0130 04:59:52.024937 140005288683264 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.6462947130203247, loss=1.667785406112671
I0130 05:00:25.578524 140005297075968 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.8668807744979858, loss=1.7344578504562378
I0130 05:00:59.129604 140005288683264 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.6072256565093994, loss=1.694603443145752
I0130 05:01:32.680662 140005297075968 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.192270517349243, loss=1.751281976699829
I0130 05:01:53.976512 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:02:00.400643 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:02:09.143800 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:02:11.851595 140169137129280 submission_runner.py:408] Time since start: 11668.55s, 	Step: 33365, 	{'train/accuracy': 0.6925222873687744, 'train/loss': 1.2043423652648926, 'validation/accuracy': 0.6267799735069275, 'validation/loss': 1.553866982460022, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.2885379791259766, 'test/num_examples': 10000, 'score': 11255.309514045715, 'total_duration': 11668.54907798767, 'accumulated_submission_time': 11255.309514045715, 'accumulated_eval_time': 411.4780659675598, 'accumulated_logging_time': 0.6850986480712891}
I0130 05:02:11.878220 140004624946944 logging_writer.py:48] [33365] accumulated_eval_time=411.478066, accumulated_logging_time=0.685099, accumulated_submission_time=11255.309514, global_step=33365, preemption_count=0, score=11255.309514, test/accuracy=0.497600, test/loss=2.288538, test/num_examples=10000, total_duration=11668.549078, train/accuracy=0.692522, train/loss=1.204342, validation/accuracy=0.626780, validation/loss=1.553867, validation/num_examples=50000
I0130 05:02:23.975112 140005305468672 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.735129952430725, loss=1.6448254585266113
I0130 05:02:57.499558 140004624946944 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.694847583770752, loss=1.6539506912231445
I0130 05:03:31.046286 140005305468672 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.7356150150299072, loss=1.6839487552642822
I0130 05:04:04.614713 140004624946944 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.650559663772583, loss=1.813359260559082
I0130 05:04:38.246427 140005305468672 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8093507289886475, loss=1.6373754739761353
I0130 05:05:11.891834 140004624946944 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.659066915512085, loss=1.6750737428665161
I0130 05:05:45.550723 140005305468672 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.999196171760559, loss=1.7741703987121582
I0130 05:06:19.187656 140004624946944 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.5272059440612793, loss=1.7118405103683472
I0130 05:06:52.823597 140005305468672 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.8562350273132324, loss=1.7407872676849365
I0130 05:07:26.396484 140004624946944 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.830846905708313, loss=1.8272075653076172
I0130 05:08:00.043475 140005305468672 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.109743118286133, loss=1.7605868577957153
I0130 05:08:33.656497 140004624946944 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.74723219871521, loss=1.7186483144760132
I0130 05:09:07.281244 140005305468672 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.721479892730713, loss=1.7362732887268066
I0130 05:09:40.890345 140004624946944 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.8045293092727661, loss=1.6518476009368896
I0130 05:10:14.527110 140005305468672 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.6805174350738525, loss=1.7123576402664185
I0130 05:10:41.932153 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:10:48.406483 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:10:56.993466 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:10:59.701200 140169137129280 submission_runner.py:408] Time since start: 12196.40s, 	Step: 34883, 	{'train/accuracy': 0.6701012253761292, 'train/loss': 1.305039405822754, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.6147181987762451, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.387573003768921, 'test/num_examples': 10000, 'score': 11765.304506778717, 'total_duration': 12196.398695707321, 'accumulated_submission_time': 11765.304506778717, 'accumulated_eval_time': 429.24707651138306, 'accumulated_logging_time': 0.7219088077545166}
I0130 05:10:59.727901 140005297075968 logging_writer.py:48] [34883] accumulated_eval_time=429.247077, accumulated_logging_time=0.721909, accumulated_submission_time=11765.304507, global_step=34883, preemption_count=0, score=11765.304507, test/accuracy=0.485000, test/loss=2.387573, test/num_examples=10000, total_duration=12196.398696, train/accuracy=0.670101, train/loss=1.305039, validation/accuracy=0.614480, validation/loss=1.614718, validation/num_examples=50000
I0130 05:11:05.771311 140005322254080 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.9160685539245605, loss=1.7992949485778809
I0130 05:11:39.299113 140005297075968 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.9356876611709595, loss=1.7624611854553223
I0130 05:12:12.937562 140005322254080 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.931112289428711, loss=1.69938325881958
I0130 05:12:46.567198 140005297075968 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.8119288682937622, loss=1.6528775691986084
I0130 05:13:20.226701 140005322254080 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.9235286712646484, loss=1.6560423374176025
I0130 05:13:53.796503 140005297075968 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.748733401298523, loss=1.7052192687988281
I0130 05:14:27.394948 140005322254080 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.803290843963623, loss=1.6342955827713013
I0130 05:15:01.039137 140005297075968 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.8849290609359741, loss=1.6681292057037354
I0130 05:15:34.685720 140005322254080 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.8254733085632324, loss=1.6241095066070557
I0130 05:16:08.304140 140005297075968 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.6002496480941772, loss=1.699272871017456
I0130 05:16:41.888580 140005322254080 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.7959043979644775, loss=1.678071141242981
I0130 05:17:15.528664 140005297075968 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7188032865524292, loss=1.7135982513427734
I0130 05:17:49.176391 140005322254080 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.711848258972168, loss=1.6261179447174072
I0130 05:18:22.814289 140005297075968 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.6172764301300049, loss=1.6327565908432007
I0130 05:18:56.417567 140005322254080 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.6491869688034058, loss=1.7158399820327759
I0130 05:19:30.072758 140005297075968 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.7616302967071533, loss=1.753985047340393
I0130 05:19:30.080635 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:19:36.443763 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:19:44.893100 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:19:47.579106 140169137129280 submission_runner.py:408] Time since start: 12724.28s, 	Step: 36401, 	{'train/accuracy': 0.6705994606018066, 'train/loss': 1.3002781867980957, 'validation/accuracy': 0.6158999800682068, 'validation/loss': 1.5967798233032227, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.386909008026123, 'test/num_examples': 10000, 'score': 12275.594941139221, 'total_duration': 12724.276602745056, 'accumulated_submission_time': 12275.594941139221, 'accumulated_eval_time': 446.7454869747162, 'accumulated_logging_time': 0.7614481449127197}
I0130 05:19:47.605151 140004616554240 logging_writer.py:48] [36401] accumulated_eval_time=446.745487, accumulated_logging_time=0.761448, accumulated_submission_time=12275.594941, global_step=36401, preemption_count=0, score=12275.594941, test/accuracy=0.480400, test/loss=2.386909, test/num_examples=10000, total_duration=12724.276603, train/accuracy=0.670599, train/loss=1.300278, validation/accuracy=0.615900, validation/loss=1.596780, validation/num_examples=50000
I0130 05:20:21.180994 140005288683264 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.9768197536468506, loss=1.8079735040664673
I0130 05:20:54.803328 140004616554240 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.6071268320083618, loss=1.5172948837280273
I0130 05:21:28.439407 140005288683264 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.9462801218032837, loss=1.6929413080215454
I0130 05:22:02.076683 140004616554240 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.6878058910369873, loss=1.627326250076294
I0130 05:22:35.724014 140005288683264 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7803113460540771, loss=1.7005152702331543
I0130 05:23:09.328748 140004616554240 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.6075676679611206, loss=1.6626639366149902
I0130 05:23:42.912204 140005288683264 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.547997236251831, loss=1.5868744850158691
I0130 05:24:16.568783 140004616554240 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.741477370262146, loss=1.7103257179260254
I0130 05:24:50.221518 140005288683264 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.8225609064102173, loss=1.701253056526184
I0130 05:25:23.846111 140004616554240 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.126115083694458, loss=1.6613259315490723
I0130 05:25:57.593071 140005288683264 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.8479835987091064, loss=1.696563959121704
I0130 05:26:31.252608 140004616554240 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.6955461502075195, loss=1.6536225080490112
I0130 05:27:04.901849 140005288683264 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8279145956039429, loss=1.7127841711044312
I0130 05:27:38.516138 140004616554240 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.784851312637329, loss=1.5867429971694946
I0130 05:28:12.129078 140005288683264 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.791313886642456, loss=1.751209020614624
I0130 05:28:17.659561 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:28:24.090173 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:28:32.848419 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:28:35.556322 140169137129280 submission_runner.py:408] Time since start: 13252.25s, 	Step: 37918, 	{'train/accuracy': 0.6964285373687744, 'train/loss': 1.1979633569717407, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.4970052242279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.2210662364959717, 'test/num_examples': 10000, 'score': 12785.589760780334, 'total_duration': 13252.253804206848, 'accumulated_submission_time': 12785.589760780334, 'accumulated_eval_time': 464.64219307899475, 'accumulated_logging_time': 0.7978343963623047}
I0130 05:28:35.584046 140004616554240 logging_writer.py:48] [37918] accumulated_eval_time=464.642193, accumulated_logging_time=0.797834, accumulated_submission_time=12785.589761, global_step=37918, preemption_count=0, score=12785.589761, test/accuracy=0.510700, test/loss=2.221066, test/num_examples=10000, total_duration=13252.253804, train/accuracy=0.696429, train/loss=1.197963, validation/accuracy=0.637240, validation/loss=1.497005, validation/num_examples=50000
I0130 05:29:03.448336 140004624946944 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.700325846672058, loss=1.6549994945526123
I0130 05:29:36.936440 140004616554240 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.88162100315094, loss=1.5806872844696045
I0130 05:30:10.529082 140004624946944 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.617972731590271, loss=1.7126377820968628
I0130 05:30:44.144733 140004616554240 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.911781668663025, loss=1.7331981658935547
I0130 05:31:17.800176 140004624946944 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7906215190887451, loss=1.6153695583343506
I0130 05:31:51.423561 140004616554240 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.8614137172698975, loss=1.6481211185455322
I0130 05:32:25.064476 140004624946944 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8177703619003296, loss=1.6691720485687256
I0130 05:32:58.770303 140004616554240 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.7094353437423706, loss=1.5970267057418823
I0130 05:33:32.427943 140004624946944 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.9109296798706055, loss=1.661874532699585
I0130 05:34:06.054568 140004616554240 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.9571170806884766, loss=1.6760995388031006
I0130 05:34:39.668570 140004624946944 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.8331557512283325, loss=1.4864003658294678
I0130 05:35:13.301758 140004616554240 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.8346036672592163, loss=1.7435393333435059
I0130 05:35:46.934373 140004624946944 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.7326117753982544, loss=1.6501926183700562
I0130 05:36:20.554942 140004616554240 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.9925565719604492, loss=1.626197099685669
I0130 05:36:54.200283 140004624946944 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.8841301202774048, loss=1.6308763027191162
I0130 05:37:05.776211 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:37:12.318066 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:37:20.952276 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:37:23.705366 140169137129280 submission_runner.py:408] Time since start: 13780.40s, 	Step: 39436, 	{'train/accuracy': 0.7492027878761292, 'train/loss': 0.9707934260368347, 'validation/accuracy': 0.644320011138916, 'validation/loss': 1.4667904376983643, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.183366060256958, 'test/num_examples': 10000, 'score': 13295.721585988998, 'total_duration': 13780.402867794037, 'accumulated_submission_time': 13295.721585988998, 'accumulated_eval_time': 482.57131838798523, 'accumulated_logging_time': 0.8367643356323242}
I0130 05:37:23.728440 140004608161536 logging_writer.py:48] [39436] accumulated_eval_time=482.571318, accumulated_logging_time=0.836764, accumulated_submission_time=13295.721586, global_step=39436, preemption_count=0, score=13295.721586, test/accuracy=0.515100, test/loss=2.183366, test/num_examples=10000, total_duration=13780.402868, train/accuracy=0.749203, train/loss=0.970793, validation/accuracy=0.644320, validation/loss=1.466790, validation/num_examples=50000
I0130 05:37:45.534379 140004616554240 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.816120982170105, loss=1.6335819959640503
I0130 05:38:19.099596 140004608161536 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.1246962547302246, loss=1.6638566255569458
I0130 05:38:52.746842 140004616554240 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.8872382640838623, loss=1.6879768371582031
I0130 05:39:26.441796 140004608161536 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.0558862686157227, loss=1.781168818473816
I0130 05:40:00.001167 140004616554240 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.9134314060211182, loss=1.6393429040908813
I0130 05:40:33.525389 140004608161536 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.688912034034729, loss=1.6934318542480469
I0130 05:41:07.120853 140004616554240 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.8283771276474, loss=1.6093144416809082
I0130 05:41:40.746641 140004608161536 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.7486984729766846, loss=1.6710097789764404
I0130 05:42:14.379098 140004616554240 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.83822762966156, loss=1.565422773361206
I0130 05:42:48.024108 140004608161536 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.9143402576446533, loss=1.7296336889266968
I0130 05:43:21.636688 140004616554240 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.7294456958770752, loss=1.5671277046203613
I0130 05:43:55.276415 140004608161536 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.812181234359741, loss=1.7038949728012085
I0130 05:44:28.898714 140004616554240 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.7524245977401733, loss=1.7723321914672852
I0130 05:45:02.488080 140004608161536 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.010658025741577, loss=1.7209893465042114
I0130 05:45:36.118296 140004616554240 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.9718998670578003, loss=1.6829065084457397
I0130 05:45:53.826667 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:46:00.279374 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:46:08.758258 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:46:11.490574 140169137129280 submission_runner.py:408] Time since start: 14308.19s, 	Step: 40954, 	{'train/accuracy': 0.711933970451355, 'train/loss': 1.1202871799468994, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.5269010066986084, 'validation/num_examples': 50000, 'test/accuracy': 0.5002000331878662, 'test/loss': 2.254570484161377, 'test/num_examples': 10000, 'score': 13805.760528564453, 'total_duration': 14308.188065290451, 'accumulated_submission_time': 13805.760528564453, 'accumulated_eval_time': 500.2351813316345, 'accumulated_logging_time': 0.8699560165405273}
I0130 05:46:11.517309 140005313861376 logging_writer.py:48] [40954] accumulated_eval_time=500.235181, accumulated_logging_time=0.869956, accumulated_submission_time=13805.760529, global_step=40954, preemption_count=0, score=13805.760529, test/accuracy=0.500200, test/loss=2.254570, test/num_examples=10000, total_duration=14308.188065, train/accuracy=0.711934, train/loss=1.120287, validation/accuracy=0.630820, validation/loss=1.526901, validation/num_examples=50000
I0130 05:46:27.288422 140005322254080 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.8214948177337646, loss=1.7265092134475708
I0130 05:47:00.809510 140005313861376 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7897287607192993, loss=1.6288436651229858
I0130 05:47:34.390064 140005322254080 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.9011043310165405, loss=1.75819730758667
I0130 05:48:07.979079 140005313861376 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.0290939807891846, loss=1.7139332294464111
I0130 05:48:41.623143 140005322254080 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.879817008972168, loss=1.60551917552948
I0130 05:49:15.244335 140005313861376 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.825717568397522, loss=1.7609553337097168
I0130 05:49:48.867823 140005322254080 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.15073299407959, loss=1.6562199592590332
I0130 05:50:22.447168 140005313861376 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.0246827602386475, loss=1.6738098859786987
I0130 05:50:56.082510 140005322254080 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.9385210275650024, loss=1.5657685995101929
I0130 05:51:29.714808 140005313861376 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8390135765075684, loss=1.7076178789138794
I0130 05:52:03.432801 140005322254080 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7758952379226685, loss=1.599550485610962
I0130 05:52:37.006839 140005313861376 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7834445238113403, loss=1.5689777135849
I0130 05:53:10.604903 140005322254080 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.035588264465332, loss=1.6516592502593994
I0130 05:53:44.174004 140005313861376 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.5845879316329956, loss=1.5628235340118408
I0130 05:54:17.723738 140005322254080 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.1180782318115234, loss=1.710591197013855
I0130 05:54:41.715030 140169137129280 spec.py:321] Evaluating on the training split.
I0130 05:54:48.118048 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 05:54:56.767301 140169137129280 spec.py:349] Evaluating on the test split.
I0130 05:54:59.475409 140169137129280 submission_runner.py:408] Time since start: 14836.17s, 	Step: 42473, 	{'train/accuracy': 0.7022480964660645, 'train/loss': 1.17045259475708, 'validation/accuracy': 0.6351400017738342, 'validation/loss': 1.5160480737686157, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2341115474700928, 'test/num_examples': 10000, 'score': 14315.898855924606, 'total_duration': 14836.172902822495, 'accumulated_submission_time': 14315.898855924606, 'accumulated_eval_time': 517.9955246448517, 'accumulated_logging_time': 0.9070932865142822}
I0130 05:54:59.501748 140004608161536 logging_writer.py:48] [42473] accumulated_eval_time=517.995525, accumulated_logging_time=0.907093, accumulated_submission_time=14315.898856, global_step=42473, preemption_count=0, score=14315.898856, test/accuracy=0.504900, test/loss=2.234112, test/num_examples=10000, total_duration=14836.172903, train/accuracy=0.702248, train/loss=1.170453, validation/accuracy=0.635140, validation/loss=1.516048, validation/num_examples=50000
I0130 05:55:08.926674 140004616554240 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.7195850610733032, loss=1.575180172920227
I0130 05:55:42.459168 140004608161536 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.859474539756775, loss=1.6071126461029053
I0130 05:56:15.993283 140004616554240 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7778595685958862, loss=1.6511588096618652
I0130 05:56:49.601640 140004608161536 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.240818738937378, loss=1.65596342086792
I0130 05:57:23.217077 140004616554240 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.9815247058868408, loss=1.6542226076126099
I0130 05:57:56.852407 140004608161536 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.836538553237915, loss=1.6915680170059204
I0130 05:58:30.484072 140004616554240 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.835843563079834, loss=1.6624337434768677
I0130 05:59:04.143512 140004608161536 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.8407047986984253, loss=1.6293237209320068
I0130 05:59:37.751139 140004616554240 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.679298758506775, loss=1.6657096147537231
I0130 06:00:11.369045 140004608161536 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.072962999343872, loss=1.5834758281707764
I0130 06:00:44.956681 140004616554240 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7668277025222778, loss=1.576503038406372
I0130 06:01:19.003692 140004608161536 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.8191838264465332, loss=1.708371639251709
I0130 06:01:52.500334 140004616554240 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.070064067840576, loss=1.604162335395813
I0130 06:02:26.078496 140004608161536 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.7123714685440063, loss=1.7297735214233398
I0130 06:02:59.693347 140004616554240 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.97849440574646, loss=1.6289337873458862
I0130 06:03:29.783886 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:03:36.172530 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:03:44.886337 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:03:47.618475 140169137129280 submission_runner.py:408] Time since start: 15364.32s, 	Step: 43991, 	{'train/accuracy': 0.6902702450752258, 'train/loss': 1.219701886177063, 'validation/accuracy': 0.6301400065422058, 'validation/loss': 1.5335793495178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.266105890274048, 'test/num_examples': 10000, 'score': 14826.120551109314, 'total_duration': 15364.315973997116, 'accumulated_submission_time': 14826.120551109314, 'accumulated_eval_time': 535.8300864696503, 'accumulated_logging_time': 0.9444942474365234}
I0130 06:03:47.644198 140004608161536 logging_writer.py:48] [43991] accumulated_eval_time=535.830086, accumulated_logging_time=0.944494, accumulated_submission_time=14826.120551, global_step=43991, preemption_count=0, score=14826.120551, test/accuracy=0.503200, test/loss=2.266106, test/num_examples=10000, total_duration=15364.315974, train/accuracy=0.690270, train/loss=1.219702, validation/accuracy=0.630140, validation/loss=1.533579, validation/num_examples=50000
I0130 06:03:51.019064 140004616554240 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.966048002243042, loss=1.681596279144287
I0130 06:04:24.555956 140004608161536 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.781638503074646, loss=1.6417632102966309
I0130 06:04:58.111602 140004616554240 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.9497296810150146, loss=1.7377212047576904
I0130 06:05:31.753333 140004608161536 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.1530797481536865, loss=1.6360632181167603
I0130 06:06:05.370227 140004616554240 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.9753865003585815, loss=1.7046793699264526
I0130 06:06:39.001389 140004608161536 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.2057666778564453, loss=1.8028934001922607
I0130 06:07:12.614080 140004616554240 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7903287410736084, loss=1.6129138469696045
I0130 06:07:46.227748 140004608161536 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.924560308456421, loss=1.6838548183441162
I0130 06:08:19.831385 140004616554240 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.7328712940216064, loss=1.6023086309432983
I0130 06:08:53.463898 140004608161536 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.8551894426345825, loss=1.6201343536376953
I0130 06:09:27.084250 140004616554240 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.7211261987686157, loss=1.572898268699646
I0130 06:10:00.701541 140004608161536 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.978211760520935, loss=1.685571312904358
I0130 06:10:34.336619 140004616554240 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7358585596084595, loss=1.5461361408233643
I0130 06:11:07.965387 140004608161536 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8042832612991333, loss=1.715915560722351
I0130 06:11:41.691761 140004616554240 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6740447282791138, loss=1.5291489362716675
I0130 06:12:15.191597 140004608161536 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.778577446937561, loss=1.5076704025268555
I0130 06:12:17.705467 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:12:24.155767 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:12:32.816875 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:12:35.481051 140169137129280 submission_runner.py:408] Time since start: 15892.18s, 	Step: 45509, 	{'train/accuracy': 0.7076291441917419, 'train/loss': 1.1384071111679077, 'validation/accuracy': 0.6442599892616272, 'validation/loss': 1.483494520187378, 'validation/num_examples': 50000, 'test/accuracy': 0.5153000354766846, 'test/loss': 2.2475781440734863, 'test/num_examples': 10000, 'score': 15336.121745824814, 'total_duration': 15892.178544044495, 'accumulated_submission_time': 15336.121745824814, 'accumulated_eval_time': 553.6056270599365, 'accumulated_logging_time': 0.9802684783935547}
I0130 06:12:35.509193 140005297075968 logging_writer.py:48] [45509] accumulated_eval_time=553.605627, accumulated_logging_time=0.980268, accumulated_submission_time=15336.121746, global_step=45509, preemption_count=0, score=15336.121746, test/accuracy=0.515300, test/loss=2.247578, test/num_examples=10000, total_duration=15892.178544, train/accuracy=0.707629, train/loss=1.138407, validation/accuracy=0.644260, validation/loss=1.483495, validation/num_examples=50000
I0130 06:13:06.357907 140005305468672 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.7985115051269531, loss=1.6356900930404663
I0130 06:13:39.943387 140005297075968 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.849288821220398, loss=1.635148525238037
I0130 06:14:13.591809 140005305468672 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.0969760417938232, loss=1.6190664768218994
I0130 06:14:47.181408 140005297075968 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.9388296604156494, loss=1.6724772453308105
I0130 06:15:20.813783 140005305468672 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7999402284622192, loss=1.6571645736694336
I0130 06:15:54.447050 140005297075968 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7687987089157104, loss=1.5642033815383911
I0130 06:16:28.099617 140005305468672 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.083786725997925, loss=1.736226201057434
I0130 06:17:01.736320 140005297075968 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.735681176185608, loss=1.463836669921875
I0130 06:17:35.367492 140005305468672 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.6477364301681519, loss=1.527469515800476
I0130 06:18:09.075112 140005297075968 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.62363862991333, loss=1.61838960647583
I0130 06:18:42.633558 140005305468672 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.7876174449920654, loss=1.6330490112304688
I0130 06:19:16.222499 140005297075968 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.7187285423278809, loss=1.5544157028198242
I0130 06:19:49.867472 140005305468672 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.8138600587844849, loss=1.6284823417663574
I0130 06:20:23.481769 140005297075968 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8013828992843628, loss=1.6039683818817139
I0130 06:20:57.100595 140005305468672 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8012444972991943, loss=1.7190122604370117
I0130 06:21:05.655670 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:21:12.067754 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:21:20.470854 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:21:23.254790 140169137129280 submission_runner.py:408] Time since start: 16419.95s, 	Step: 47027, 	{'train/accuracy': 0.6973453164100647, 'train/loss': 1.1850770711898804, 'validation/accuracy': 0.6376000046730042, 'validation/loss': 1.4864510297775269, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.221822500228882, 'test/num_examples': 10000, 'score': 15846.208704471588, 'total_duration': 16419.95223212242, 'accumulated_submission_time': 15846.208704471588, 'accumulated_eval_time': 571.2046520709991, 'accumulated_logging_time': 1.0184855461120605}
I0130 06:21:23.282665 140004624946944 logging_writer.py:48] [47027] accumulated_eval_time=571.204652, accumulated_logging_time=1.018486, accumulated_submission_time=15846.208704, global_step=47027, preemption_count=0, score=15846.208704, test/accuracy=0.510300, test/loss=2.221823, test/num_examples=10000, total_duration=16419.952232, train/accuracy=0.697345, train/loss=1.185077, validation/accuracy=0.637600, validation/loss=1.486451, validation/num_examples=50000
I0130 06:21:48.115310 140005288683264 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.9407838582992554, loss=1.5982251167297363
I0130 06:22:21.728428 140004624946944 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.830472707748413, loss=1.6540662050247192
I0130 06:22:55.368115 140005288683264 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.0440900325775146, loss=1.6757183074951172
I0130 06:23:28.982364 140004624946944 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.9528738260269165, loss=1.6660196781158447
I0130 06:24:02.624101 140005288683264 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7958173751831055, loss=1.6824921369552612
I0130 06:24:36.245541 140004624946944 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.668045997619629, loss=1.5499529838562012
I0130 06:25:09.894088 140005288683264 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.9422770738601685, loss=1.5733249187469482
I0130 06:25:43.478885 140004624946944 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.9118326902389526, loss=1.6743714809417725
I0130 06:26:17.084909 140005288683264 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.883685827255249, loss=1.5789135694503784
I0130 06:26:50.717894 140004624946944 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.819126009941101, loss=1.562517762184143
I0130 06:27:24.358054 140005288683264 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.659855842590332, loss=1.503044605255127
I0130 06:27:57.960799 140004624946944 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.0352704524993896, loss=1.6405949592590332
I0130 06:28:31.570044 140005288683264 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.909236192703247, loss=1.768694281578064
I0130 06:29:05.203690 140004624946944 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.8148770332336426, loss=1.7898212671279907
I0130 06:29:38.840766 140005288683264 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.0199737548828125, loss=1.5373607873916626
I0130 06:29:53.445394 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:29:59.915392 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:30:08.362280 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:30:11.098741 140169137129280 submission_runner.py:408] Time since start: 16947.80s, 	Step: 48545, 	{'train/accuracy': 0.7341358065605164, 'train/loss': 1.0244120359420776, 'validation/accuracy': 0.6324599981307983, 'validation/loss': 1.5245925188064575, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.224776268005371, 'test/num_examples': 10000, 'score': 16356.312096595764, 'total_duration': 16947.796236276627, 'accumulated_submission_time': 16356.312096595764, 'accumulated_eval_time': 588.8579633235931, 'accumulated_logging_time': 1.0557844638824463}
I0130 06:30:11.130358 140004608161536 logging_writer.py:48] [48545] accumulated_eval_time=588.857963, accumulated_logging_time=1.055784, accumulated_submission_time=16356.312097, global_step=48545, preemption_count=0, score=16356.312097, test/accuracy=0.506700, test/loss=2.224776, test/num_examples=10000, total_duration=16947.796236, train/accuracy=0.734136, train/loss=1.024412, validation/accuracy=0.632460, validation/loss=1.524593, validation/num_examples=50000
I0130 06:30:29.930871 140005305468672 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.679945945739746, loss=1.5551211833953857
I0130 06:31:03.532437 140004608161536 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.9481996297836304, loss=1.5584940910339355
I0130 06:31:37.183108 140005305468672 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.9209867715835571, loss=1.5895919799804688
I0130 06:32:10.741564 140004608161536 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.7899909019470215, loss=1.641105055809021
I0130 06:32:44.345332 140005305468672 logging_writer.py:48] [49000] global_step=49000, grad_norm=2.115196466445923, loss=1.55612313747406
I0130 06:33:17.976738 140004608161536 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.8841975927352905, loss=1.6094777584075928
I0130 06:33:51.598943 140005305468672 logging_writer.py:48] [49200] global_step=49200, grad_norm=2.125192642211914, loss=1.5148872137069702
I0130 06:34:25.228419 140004608161536 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.8746124505996704, loss=1.6593708992004395
I0130 06:34:58.873055 140005305468672 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.913198709487915, loss=1.7263903617858887
I0130 06:35:32.488293 140004608161536 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.868554949760437, loss=1.5140020847320557
I0130 06:36:06.103676 140005305468672 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.9097665548324585, loss=1.6674343347549438
I0130 06:36:39.717991 140004608161536 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.915666937828064, loss=1.6698825359344482
I0130 06:37:13.326395 140005305468672 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8777658939361572, loss=1.6795103549957275
I0130 06:37:47.058038 140004608161536 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.8176186084747314, loss=1.6121222972869873
I0130 06:38:20.659005 140005305468672 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7071094512939453, loss=1.521069049835205
I0130 06:38:41.263218 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:38:47.664081 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:38:56.494507 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:38:59.199728 140169137129280 submission_runner.py:408] Time since start: 17475.90s, 	Step: 50063, 	{'train/accuracy': 0.7353116869926453, 'train/loss': 1.0175830125808716, 'validation/accuracy': 0.6514399647712708, 'validation/loss': 1.4259246587753296, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.150914430618286, 'test/num_examples': 10000, 'score': 16866.38495707512, 'total_duration': 17475.897213935852, 'accumulated_submission_time': 16866.38495707512, 'accumulated_eval_time': 606.7944264411926, 'accumulated_logging_time': 1.0985126495361328}
I0130 06:38:59.228195 140004624946944 logging_writer.py:48] [50063] accumulated_eval_time=606.794426, accumulated_logging_time=1.098513, accumulated_submission_time=16866.384957, global_step=50063, preemption_count=0, score=16866.384957, test/accuracy=0.522900, test/loss=2.150914, test/num_examples=10000, total_duration=17475.897214, train/accuracy=0.735312, train/loss=1.017583, validation/accuracy=0.651440, validation/loss=1.425925, validation/num_examples=50000
I0130 06:39:11.955566 140005288683264 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.992708683013916, loss=1.6448075771331787
I0130 06:39:45.490726 140004624946944 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.716522216796875, loss=1.6835981607437134
I0130 06:40:19.089046 140005288683264 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.062650680541992, loss=1.6527342796325684
I0130 06:40:52.685525 140004624946944 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.932921290397644, loss=1.5129984617233276
I0130 06:41:26.304775 140005288683264 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.9157923460006714, loss=1.7693347930908203
I0130 06:41:59.872042 140004624946944 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.051694869995117, loss=1.6056069135665894
I0130 06:42:33.420713 140005288683264 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.818686842918396, loss=1.609506607055664
I0130 06:43:07.031826 140004624946944 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.9114404916763306, loss=1.626479983329773
I0130 06:43:40.674880 140005288683264 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.9175920486450195, loss=1.5499327182769775
I0130 06:44:14.310393 140004624946944 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.757972240447998, loss=1.6576883792877197
I0130 06:44:47.984791 140005288683264 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.723699688911438, loss=1.7061959505081177
I0130 06:45:21.526849 140004624946944 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.898023009300232, loss=1.6214499473571777
I0130 06:45:55.080701 140005288683264 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.700769066810608, loss=1.7067152261734009
I0130 06:46:28.627771 140004624946944 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.829573392868042, loss=1.6305526494979858
I0130 06:47:02.215776 140005288683264 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.98766028881073, loss=1.6422622203826904
I0130 06:47:29.525525 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:47:35.916536 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:47:44.722427 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:47:47.396260 140169137129280 submission_runner.py:408] Time since start: 18004.09s, 	Step: 51583, 	{'train/accuracy': 0.7122129797935486, 'train/loss': 1.1027354001998901, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.4719308614730835, 'validation/num_examples': 50000, 'test/accuracy': 0.5144000053405762, 'test/loss': 2.2002127170562744, 'test/num_examples': 10000, 'score': 17376.622447252274, 'total_duration': 18004.093755483627, 'accumulated_submission_time': 17376.622447252274, 'accumulated_eval_time': 624.6651320457458, 'accumulated_logging_time': 1.1381235122680664}
I0130 06:47:47.423358 140004616554240 logging_writer.py:48] [51583] accumulated_eval_time=624.665132, accumulated_logging_time=1.138124, accumulated_submission_time=17376.622447, global_step=51583, preemption_count=0, score=17376.622447, test/accuracy=0.514400, test/loss=2.200213, test/num_examples=10000, total_duration=18004.093755, train/accuracy=0.712213, train/loss=1.102735, validation/accuracy=0.641460, validation/loss=1.471931, validation/num_examples=50000
I0130 06:47:53.475219 140004624946944 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.8889617919921875, loss=1.5175142288208008
I0130 06:48:27.035458 140004616554240 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.0152993202209473, loss=1.656203269958496
I0130 06:49:00.654128 140004624946944 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8129956722259521, loss=1.598834753036499
I0130 06:49:34.284078 140004616554240 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.004020929336548, loss=1.6531946659088135
I0130 06:50:07.918705 140004624946944 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.8759596347808838, loss=1.6533195972442627
I0130 06:50:41.515183 140004616554240 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.7328273057937622, loss=1.5228571891784668
I0130 06:51:15.176318 140004624946944 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.0832042694091797, loss=1.4951058626174927
I0130 06:51:48.729508 140004616554240 logging_writer.py:48] [52300] global_step=52300, grad_norm=2.063225030899048, loss=1.6621959209442139
I0130 06:52:22.340751 140004624946944 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.78606116771698, loss=1.5941473245620728
I0130 06:52:55.962842 140004616554240 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.99428129196167, loss=1.5585781335830688
I0130 06:53:29.586542 140004624946944 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.8669718503952026, loss=1.4744772911071777
I0130 06:54:03.230779 140004616554240 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8276782035827637, loss=1.6894586086273193
I0130 06:54:36.850508 140004624946944 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8370414972305298, loss=1.5531466007232666
I0130 06:55:10.466537 140004616554240 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7580897808074951, loss=1.5866844654083252
I0130 06:55:44.123383 140004624946944 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8062585592269897, loss=1.528389573097229
I0130 06:56:17.728269 140004616554240 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.9336962699890137, loss=1.7311959266662598
I0130 06:56:17.737301 140169137129280 spec.py:321] Evaluating on the training split.
I0130 06:56:24.168744 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 06:56:32.516986 140169137129280 spec.py:349] Evaluating on the test split.
I0130 06:56:35.231595 140169137129280 submission_runner.py:408] Time since start: 18531.93s, 	Step: 53101, 	{'train/accuracy': 0.7161391973495483, 'train/loss': 1.1013281345367432, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.445090889930725, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.178856134414673, 'test/num_examples': 10000, 'score': 17886.875101804733, 'total_duration': 18531.929088115692, 'accumulated_submission_time': 17886.875101804733, 'accumulated_eval_time': 642.1593663692474, 'accumulated_logging_time': 1.176835536956787}
I0130 06:56:35.260947 140004608161536 logging_writer.py:48] [53101] accumulated_eval_time=642.159366, accumulated_logging_time=1.176836, accumulated_submission_time=17886.875102, global_step=53101, preemption_count=0, score=17886.875102, test/accuracy=0.520100, test/loss=2.178856, test/num_examples=10000, total_duration=18531.929088, train/accuracy=0.716139, train/loss=1.101328, validation/accuracy=0.650340, validation/loss=1.445091, validation/num_examples=50000
I0130 06:57:08.783282 140005297075968 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.8229728937149048, loss=1.5193724632263184
I0130 06:57:42.411278 140004608161536 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.8500069379806519, loss=1.6421709060668945
I0130 06:58:15.968197 140005297075968 logging_writer.py:48] [53400] global_step=53400, grad_norm=2.027278184890747, loss=1.511204719543457
I0130 06:58:49.614336 140004608161536 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.9760464429855347, loss=1.5363162755966187
I0130 06:59:23.238035 140005297075968 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.0567336082458496, loss=1.5518910884857178
I0130 06:59:56.874016 140004608161536 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.9180502891540527, loss=1.6439671516418457
I0130 07:00:30.461635 140005297075968 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.984893560409546, loss=1.686424970626831
I0130 07:01:04.100632 140004608161536 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.6902505159378052, loss=1.5536993741989136
I0130 07:01:37.710606 140005297075968 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7580223083496094, loss=1.5581872463226318
I0130 07:02:11.344258 140004608161536 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8852720260620117, loss=1.6816940307617188
I0130 07:02:44.983009 140005297075968 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8173495531082153, loss=1.628866195678711
I0130 07:03:18.633170 140004608161536 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.9961462020874023, loss=1.6732003688812256
I0130 07:03:52.249686 140005297075968 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.088261604309082, loss=1.5976645946502686
I0130 07:04:25.932643 140004608161536 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.855518102645874, loss=1.5435411930084229
I0130 07:04:59.538439 140005297075968 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7660192251205444, loss=1.496751308441162
I0130 07:05:05.403917 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:05:11.790989 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:05:20.409374 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:05:23.132769 140169137129280 submission_runner.py:408] Time since start: 19059.83s, 	Step: 54619, 	{'train/accuracy': 0.7102997303009033, 'train/loss': 1.1230549812316895, 'validation/accuracy': 0.6488800048828125, 'validation/loss': 1.4407885074615479, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.1671206951141357, 'test/num_examples': 10000, 'score': 18396.955702781677, 'total_duration': 19059.830214738846, 'accumulated_submission_time': 18396.955702781677, 'accumulated_eval_time': 659.8881301879883, 'accumulated_logging_time': 1.2191162109375}
I0130 07:05:23.165043 140004616554240 logging_writer.py:48] [54619] accumulated_eval_time=659.888130, accumulated_logging_time=1.219116, accumulated_submission_time=18396.955703, global_step=54619, preemption_count=0, score=18396.955703, test/accuracy=0.521300, test/loss=2.167121, test/num_examples=10000, total_duration=19059.830215, train/accuracy=0.710300, train/loss=1.123055, validation/accuracy=0.648880, validation/loss=1.440789, validation/num_examples=50000
I0130 07:05:50.743503 140005288683264 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.1254968643188477, loss=1.4984102249145508
I0130 07:06:24.312570 140004616554240 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.892488718032837, loss=1.6953006982803345
I0130 07:06:57.840933 140005288683264 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.7515668869018555, loss=1.6191034317016602
I0130 07:07:31.370301 140004616554240 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.817168951034546, loss=1.5965656042099
I0130 07:08:04.921357 140005288683264 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.0102996826171875, loss=1.6537611484527588
I0130 07:08:38.547235 140004616554240 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.841758131980896, loss=1.5828036069869995
I0130 07:09:12.165872 140005288683264 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.7404966354370117, loss=1.499509334564209
I0130 07:09:45.791830 140004616554240 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.812474250793457, loss=1.586684226989746
I0130 07:10:19.401378 140005288683264 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.7575953006744385, loss=1.5190844535827637
I0130 07:10:53.135047 140004616554240 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.867783784866333, loss=1.4847050905227661
I0130 07:11:26.771286 140005288683264 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.9437090158462524, loss=1.5792341232299805
I0130 07:12:00.396145 140004616554240 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.0155551433563232, loss=1.5846648216247559
I0130 07:12:34.032404 140005288683264 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.0768284797668457, loss=1.6379094123840332
I0130 07:13:07.623780 140004616554240 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.0814337730407715, loss=1.567024827003479
I0130 07:13:41.264070 140005288683264 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.931565523147583, loss=1.4911069869995117
I0130 07:13:53.183643 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:13:59.582056 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:14:08.073130 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:14:10.763375 140169137129280 submission_runner.py:408] Time since start: 19587.46s, 	Step: 56137, 	{'train/accuracy': 0.6968669891357422, 'train/loss': 1.1886688470840454, 'validation/accuracy': 0.6329799890518188, 'validation/loss': 1.519242763519287, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2319228649139404, 'test/num_examples': 10000, 'score': 18906.910097837448, 'total_duration': 19587.460858106613, 'accumulated_submission_time': 18906.910097837448, 'accumulated_eval_time': 677.467814207077, 'accumulated_logging_time': 1.266390085220337}
I0130 07:14:10.793844 140004616554240 logging_writer.py:48] [56137] accumulated_eval_time=677.467814, accumulated_logging_time=1.266390, accumulated_submission_time=18906.910098, global_step=56137, preemption_count=0, score=18906.910098, test/accuracy=0.510500, test/loss=2.231923, test/num_examples=10000, total_duration=19587.460858, train/accuracy=0.696867, train/loss=1.188669, validation/accuracy=0.632980, validation/loss=1.519243, validation/num_examples=50000
I0130 07:14:32.241481 140005297075968 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.022704601287842, loss=1.5789272785186768
I0130 07:15:05.742108 140004616554240 logging_writer.py:48] [56300] global_step=56300, grad_norm=2.053976058959961, loss=1.4771984815597534
I0130 07:15:39.366322 140005297075968 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.8874778747558594, loss=1.624180555343628
I0130 07:16:12.988090 140004616554240 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8760840892791748, loss=1.6148030757904053
I0130 07:16:46.637993 140005297075968 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8286705017089844, loss=1.711578369140625
I0130 07:17:20.312518 140004616554240 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.000135660171509, loss=1.6666216850280762
I0130 07:17:53.863902 140005297075968 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.8866556882858276, loss=1.7032045125961304
I0130 07:18:27.427936 140004616554240 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.7601248025894165, loss=1.535033941268921
I0130 07:19:00.936747 140005297075968 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9514344930648804, loss=1.6769838333129883
I0130 07:19:34.530016 140004616554240 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.099628448486328, loss=1.530637502670288
I0130 07:20:08.148884 140005297075968 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.7814412117004395, loss=1.5631110668182373
I0130 07:20:41.786606 140004616554240 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.976352334022522, loss=1.6285507678985596
I0130 07:21:15.437749 140005297075968 logging_writer.py:48] [57400] global_step=57400, grad_norm=2.0942909717559814, loss=1.5768249034881592
I0130 07:21:49.070028 140004616554240 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.056279182434082, loss=1.5738525390625
I0130 07:22:22.711161 140005297075968 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.951242446899414, loss=1.6041629314422607
I0130 07:22:41.004390 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:22:48.127603 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:22:56.550851 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:22:59.273002 140169137129280 submission_runner.py:408] Time since start: 20115.97s, 	Step: 57656, 	{'train/accuracy': 0.7419084906578064, 'train/loss': 0.977180004119873, 'validation/accuracy': 0.6464999914169312, 'validation/loss': 1.4487738609313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2348201274871826, 'test/num_examples': 10000, 'score': 19417.059530496597, 'total_duration': 20115.970502853394, 'accumulated_submission_time': 19417.059530496597, 'accumulated_eval_time': 695.7363994121552, 'accumulated_logging_time': 1.3087108135223389}
I0130 07:22:59.303578 140005288683264 logging_writer.py:48] [57656] accumulated_eval_time=695.736399, accumulated_logging_time=1.308711, accumulated_submission_time=19417.059530, global_step=57656, preemption_count=0, score=19417.059530, test/accuracy=0.508500, test/loss=2.234820, test/num_examples=10000, total_duration=20115.970503, train/accuracy=0.741908, train/loss=0.977180, validation/accuracy=0.646500, validation/loss=1.448774, validation/num_examples=50000
I0130 07:23:14.406697 140005305468672 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9013516902923584, loss=1.5629897117614746
I0130 07:23:48.032014 140005288683264 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.7672137022018433, loss=1.5264984369277954
I0130 07:24:21.655608 140005305468672 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.8602349758148193, loss=1.5512160062789917
I0130 07:24:55.284492 140005288683264 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.7884398698806763, loss=1.644179105758667
I0130 07:25:28.936552 140005305468672 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.0097968578338623, loss=1.4680641889572144
I0130 07:26:02.553149 140005288683264 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.065983295440674, loss=1.6179300546646118
I0130 07:26:36.142489 140005305468672 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8681378364562988, loss=1.4611029624938965
I0130 07:27:09.702971 140005288683264 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9919142723083496, loss=1.5241479873657227
I0130 07:27:43.348025 140005305468672 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.128929376602173, loss=1.4933615922927856
I0130 07:28:16.983323 140005288683264 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.0725975036621094, loss=1.575146198272705
I0130 07:28:50.574311 140005305468672 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.777625322341919, loss=1.6427311897277832
I0130 07:29:24.128927 140005288683264 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.942483901977539, loss=1.618043303489685
I0130 07:29:57.753154 140005305468672 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.801780104637146, loss=1.4858161211013794
I0130 07:30:31.435224 140005288683264 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8783987760543823, loss=1.6473677158355713
I0130 07:31:04.961761 140005305468672 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.794101357460022, loss=1.546118974685669
I0130 07:31:29.602412 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:31:36.056380 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:31:44.558867 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:31:47.272159 140169137129280 submission_runner.py:408] Time since start: 20643.97s, 	Step: 59175, 	{'train/accuracy': 0.7238121628761292, 'train/loss': 1.0625795125961304, 'validation/accuracy': 0.6496399641036987, 'validation/loss': 1.435991883277893, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.1798503398895264, 'test/num_examples': 10000, 'score': 19927.296014785767, 'total_duration': 20643.969654798508, 'accumulated_submission_time': 19927.296014785767, 'accumulated_eval_time': 713.406112909317, 'accumulated_logging_time': 1.3524727821350098}
I0130 07:31:47.300122 140004624946944 logging_writer.py:48] [59175] accumulated_eval_time=713.406113, accumulated_logging_time=1.352473, accumulated_submission_time=19927.296015, global_step=59175, preemption_count=0, score=19927.296015, test/accuracy=0.519000, test/loss=2.179850, test/num_examples=10000, total_duration=20643.969655, train/accuracy=0.723812, train/loss=1.062580, validation/accuracy=0.649640, validation/loss=1.435992, validation/num_examples=50000
I0130 07:31:56.013701 140005288683264 logging_writer.py:48] [59200] global_step=59200, grad_norm=2.3041787147521973, loss=1.5929805040359497
I0130 07:32:29.573909 140004624946944 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8007851839065552, loss=1.4787352085113525
I0130 07:33:03.174395 140005288683264 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.846621036529541, loss=1.5419460535049438
I0130 07:33:36.716609 140004624946944 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.8187731504440308, loss=1.4665836095809937
I0130 07:34:10.275081 140005288683264 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.280160903930664, loss=1.6391334533691406
I0130 07:34:43.803481 140004624946944 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.794455885887146, loss=1.5806713104248047
I0130 07:35:17.393932 140005288683264 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.1407229900360107, loss=1.5535497665405273
I0130 07:35:51.028872 140004624946944 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.803442120552063, loss=1.559463381767273
I0130 07:36:24.644057 140005288683264 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.93545401096344, loss=1.6432397365570068
I0130 07:36:58.362636 140004624946944 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.1777560710906982, loss=1.6326745748519897
I0130 07:37:32.023523 140005288683264 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.1350150108337402, loss=1.63340425491333
I0130 07:38:05.651211 140004624946944 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9557210206985474, loss=1.5197705030441284
I0130 07:38:39.281708 140005288683264 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8731353282928467, loss=1.5224905014038086
I0130 07:39:12.903537 140004624946944 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.123638391494751, loss=1.5294146537780762
I0130 07:39:46.512467 140005288683264 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.0718882083892822, loss=1.4481754302978516
I0130 07:40:17.512447 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:40:23.933426 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:40:32.553121 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:40:35.231584 140169137129280 submission_runner.py:408] Time since start: 21171.93s, 	Step: 60694, 	{'train/accuracy': 0.7162587642669678, 'train/loss': 1.1006048917770386, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4462645053863525, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1441636085510254, 'test/num_examples': 10000, 'score': 20437.448969364166, 'total_duration': 21171.929075479507, 'accumulated_submission_time': 20437.448969364166, 'accumulated_eval_time': 731.1252071857452, 'accumulated_logging_time': 1.3902521133422852}
I0130 07:40:35.260873 140005322254080 logging_writer.py:48] [60694] accumulated_eval_time=731.125207, accumulated_logging_time=1.390252, accumulated_submission_time=20437.448969, global_step=60694, preemption_count=0, score=20437.448969, test/accuracy=0.526700, test/loss=2.144164, test/num_examples=10000, total_duration=21171.929075, train/accuracy=0.716259, train/loss=1.100605, validation/accuracy=0.648800, validation/loss=1.446265, validation/num_examples=50000
I0130 07:40:37.628467 140005330646784 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.7568312883377075, loss=1.5478527545928955
I0130 07:41:11.164020 140005322254080 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.14095139503479, loss=1.718178153038025
I0130 07:41:44.705960 140005330646784 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8553075790405273, loss=1.5383764505386353
I0130 07:42:18.234186 140005322254080 logging_writer.py:48] [61000] global_step=61000, grad_norm=2.003208637237549, loss=1.510551929473877
I0130 07:42:51.831990 140005330646784 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.9750217199325562, loss=1.5151124000549316
I0130 07:43:25.526911 140005322254080 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.1521246433258057, loss=1.5358085632324219
I0130 07:43:59.061740 140005330646784 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.7718050479888916, loss=1.4592764377593994
I0130 07:44:32.592247 140005322254080 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.916746735572815, loss=1.623765468597412
I0130 07:45:06.179381 140005330646784 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.1287894248962402, loss=1.706705093383789
I0130 07:45:39.791381 140005322254080 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9920355081558228, loss=1.4917054176330566
I0130 07:46:13.410224 140005330646784 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.1277987957000732, loss=1.6442151069641113
I0130 07:46:47.023390 140005322254080 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.184678792953491, loss=1.6145671606063843
I0130 07:47:20.652752 140005330646784 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.0438852310180664, loss=1.6863815784454346
I0130 07:47:54.266144 140005322254080 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.096682071685791, loss=1.6029913425445557
I0130 07:48:27.900230 140005330646784 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.0214433670043945, loss=1.51885986328125
I0130 07:49:01.524003 140005322254080 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.0360822677612305, loss=1.6702916622161865
I0130 07:49:05.380958 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:49:11.785799 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:49:20.251874 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:49:23.011163 140169137129280 submission_runner.py:408] Time since start: 21699.71s, 	Step: 62213, 	{'train/accuracy': 0.7052375674247742, 'train/loss': 1.142183542251587, 'validation/accuracy': 0.6395800113677979, 'validation/loss': 1.4831955432891846, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2172904014587402, 'test/num_examples': 10000, 'score': 20947.50677704811, 'total_duration': 21699.70861840248, 'accumulated_submission_time': 20947.50677704811, 'accumulated_eval_time': 748.7553527355194, 'accumulated_logging_time': 1.4329195022583008}
I0130 07:49:23.057133 140004616554240 logging_writer.py:48] [62213] accumulated_eval_time=748.755353, accumulated_logging_time=1.432920, accumulated_submission_time=20947.506777, global_step=62213, preemption_count=0, score=20947.506777, test/accuracy=0.511900, test/loss=2.217290, test/num_examples=10000, total_duration=21699.708618, train/accuracy=0.705238, train/loss=1.142184, validation/accuracy=0.639580, validation/loss=1.483196, validation/num_examples=50000
I0130 07:49:52.585983 140004624946944 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.8716028928756714, loss=1.5230052471160889
I0130 07:50:26.206010 140004616554240 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9092687368392944, loss=1.5547494888305664
I0130 07:50:59.812718 140004624946944 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.8304930925369263, loss=1.5421134233474731
I0130 07:51:33.440024 140004616554240 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.0530431270599365, loss=1.5391623973846436
I0130 07:52:07.053431 140004624946944 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.185596227645874, loss=1.677089810371399
I0130 07:52:40.682657 140004616554240 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8911501169204712, loss=1.5706243515014648
I0130 07:53:14.296718 140004624946944 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.869749665260315, loss=1.4810208082199097
I0130 07:53:47.936621 140004616554240 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.910952091217041, loss=1.4685697555541992
I0130 07:54:21.576620 140004624946944 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.0953474044799805, loss=1.6992120742797852
I0130 07:54:55.205322 140004616554240 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8026152849197388, loss=1.5138523578643799
I0130 07:55:28.802350 140004624946944 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9341018199920654, loss=1.5335302352905273
I0130 07:56:02.354609 140004616554240 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.9636191129684448, loss=1.5411783456802368
I0130 07:56:36.012320 140004624946944 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.9349510669708252, loss=1.457804799079895
I0130 07:57:09.571110 140004616554240 logging_writer.py:48] [63600] global_step=63600, grad_norm=2.2474513053894043, loss=1.666710615158081
I0130 07:57:43.160844 140004624946944 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9840762615203857, loss=1.4511299133300781
I0130 07:57:53.055070 140169137129280 spec.py:321] Evaluating on the training split.
I0130 07:57:59.505224 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 07:58:08.007296 140169137129280 spec.py:349] Evaluating on the test split.
I0130 07:58:10.732915 140169137129280 submission_runner.py:408] Time since start: 22227.43s, 	Step: 63731, 	{'train/accuracy': 0.7198262214660645, 'train/loss': 1.0845756530761719, 'validation/accuracy': 0.657039999961853, 'validation/loss': 1.4061697721481323, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.1699273586273193, 'test/num_examples': 10000, 'score': 21457.440640211105, 'total_duration': 22227.43039250374, 'accumulated_submission_time': 21457.440640211105, 'accumulated_eval_time': 766.4331395626068, 'accumulated_logging_time': 1.4937834739685059}
I0130 07:58:10.764631 140005313861376 logging_writer.py:48] [63731] accumulated_eval_time=766.433140, accumulated_logging_time=1.493783, accumulated_submission_time=21457.440640, global_step=63731, preemption_count=0, score=21457.440640, test/accuracy=0.525000, test/loss=2.169927, test/num_examples=10000, total_duration=22227.430393, train/accuracy=0.719826, train/loss=1.084576, validation/accuracy=0.657040, validation/loss=1.406170, validation/num_examples=50000
I0130 07:58:34.234575 140005322254080 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.9844640493392944, loss=1.5607945919036865
I0130 07:59:07.842782 140005313861376 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0674965381622314, loss=1.4696283340454102
I0130 07:59:41.448529 140005322254080 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.7573747634887695, loss=1.6158154010772705
I0130 08:00:15.066415 140005313861376 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.0280582904815674, loss=1.4481943845748901
I0130 08:00:48.678813 140005322254080 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.8030364513397217, loss=1.5208135843276978
I0130 08:01:22.318335 140005313861376 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9931782484054565, loss=1.587339162826538
I0130 08:01:55.927528 140005322254080 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.873409390449524, loss=1.5483068227767944
I0130 08:02:29.513101 140005313861376 logging_writer.py:48] [64500] global_step=64500, grad_norm=2.2308640480041504, loss=1.5674870014190674
I0130 08:03:03.242110 140005322254080 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.9926079511642456, loss=1.429057240486145
I0130 08:03:36.778537 140005313861376 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.8684262037277222, loss=1.5223166942596436
I0130 08:04:10.343734 140005322254080 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0835230350494385, loss=1.5207245349884033
I0130 08:04:43.973152 140005313861376 logging_writer.py:48] [64900] global_step=64900, grad_norm=2.056138515472412, loss=1.6467479467391968
I0130 08:05:17.590624 140005322254080 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.830629825592041, loss=1.5049346685409546
I0130 08:05:51.229050 140005313861376 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.136131763458252, loss=1.5783382654190063
I0130 08:06:24.826417 140005322254080 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8502641916275024, loss=1.5477066040039062
I0130 08:06:40.764105 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:06:47.174534 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:06:55.730545 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:06:58.437175 140169137129280 submission_runner.py:408] Time since start: 22755.13s, 	Step: 65249, 	{'train/accuracy': 0.732421875, 'train/loss': 1.0268968343734741, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.399552345275879, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.1229050159454346, 'test/num_examples': 10000, 'score': 21967.380613327026, 'total_duration': 22755.134654521942, 'accumulated_submission_time': 21967.380613327026, 'accumulated_eval_time': 784.1061565876007, 'accumulated_logging_time': 1.5358808040618896}
I0130 08:06:58.468456 140005297075968 logging_writer.py:48] [65249] accumulated_eval_time=784.106157, accumulated_logging_time=1.535881, accumulated_submission_time=21967.380613, global_step=65249, preemption_count=0, score=21967.380613, test/accuracy=0.527000, test/loss=2.122905, test/num_examples=10000, total_duration=22755.134655, train/accuracy=0.732422, train/loss=1.026897, validation/accuracy=0.657600, validation/loss=1.399552, validation/num_examples=50000
I0130 08:07:15.909525 140005305468672 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9202160835266113, loss=1.5455687046051025
I0130 08:07:49.442925 140005297075968 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.8716740608215332, loss=1.4964063167572021
I0130 08:08:23.025834 140005305468672 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.917150616645813, loss=1.5723650455474854
I0130 08:08:56.636744 140005297075968 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.9290282726287842, loss=1.5506387948989868
I0130 08:09:30.277762 140005305468672 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.072381019592285, loss=1.4236328601837158
I0130 08:10:03.874412 140005297075968 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.247755289077759, loss=1.5541378259658813
I0130 08:10:37.480191 140005305468672 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.113050699234009, loss=1.576298475265503
I0130 08:11:11.073694 140005297075968 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.931409478187561, loss=1.523648738861084
I0130 08:11:44.692116 140005305468672 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0308821201324463, loss=1.5128874778747559
I0130 08:12:18.311139 140005297075968 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.9959449768066406, loss=1.493957281112671
I0130 08:12:51.941755 140005305468672 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.8104848861694336, loss=1.5770230293273926
I0130 08:13:25.532857 140005297075968 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9831150770187378, loss=1.6309975385665894
I0130 08:13:59.063906 140005305468672 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.022056818008423, loss=1.5490487813949585
I0130 08:14:32.613634 140005297075968 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.0369105339050293, loss=1.4837229251861572
I0130 08:15:06.170321 140005305468672 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0631744861602783, loss=1.6482155323028564
I0130 08:15:28.500182 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:15:34.928734 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:15:43.704600 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:15:46.418598 140169137129280 submission_runner.py:408] Time since start: 23283.12s, 	Step: 66768, 	{'train/accuracy': 0.7518733739852905, 'train/loss': 0.9370354413986206, 'validation/accuracy': 0.6610400080680847, 'validation/loss': 1.3839858770370483, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.1156187057495117, 'test/num_examples': 10000, 'score': 22477.35139322281, 'total_duration': 23283.116079568863, 'accumulated_submission_time': 22477.35139322281, 'accumulated_eval_time': 802.0245227813721, 'accumulated_logging_time': 1.5785470008850098}
I0130 08:15:46.449891 140004608161536 logging_writer.py:48] [66768] accumulated_eval_time=802.024523, accumulated_logging_time=1.578547, accumulated_submission_time=22477.351393, global_step=66768, preemption_count=0, score=22477.351393, test/accuracy=0.533100, test/loss=2.115619, test/num_examples=10000, total_duration=23283.116080, train/accuracy=0.751873, train/loss=0.937035, validation/accuracy=0.661040, validation/loss=1.383986, validation/num_examples=50000
I0130 08:15:57.541029 140004616554240 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.1544344425201416, loss=1.555944800376892
I0130 08:16:31.202196 140004608161536 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.8060113191604614, loss=1.527250051498413
I0130 08:17:04.808845 140004616554240 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.2240357398986816, loss=1.575451374053955
I0130 08:17:38.391830 140004608161536 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.8227533102035522, loss=1.54210364818573
I0130 08:18:11.930689 140004616554240 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.1517274379730225, loss=1.5681121349334717
I0130 08:18:45.517328 140004608161536 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.89009428024292, loss=1.5447020530700684
I0130 08:19:19.151069 140004616554240 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.0693581104278564, loss=1.5674406290054321
I0130 08:19:52.768464 140004608161536 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.8678011894226074, loss=1.5026490688323975
I0130 08:20:26.398574 140004616554240 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9609330892562866, loss=1.5227211713790894
I0130 08:21:00.023154 140004608161536 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9903563261032104, loss=1.4838950634002686
I0130 08:21:33.579330 140004616554240 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.109302520751953, loss=1.5601780414581299
I0130 08:22:07.125108 140004608161536 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.072448492050171, loss=1.4287548065185547
I0130 08:22:40.800576 140004616554240 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.2120795249938965, loss=1.6264337301254272
I0130 08:23:14.427309 140004608161536 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.117487668991089, loss=1.5597094297409058
I0130 08:23:48.082358 140004616554240 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.8852449655532837, loss=1.4724397659301758
I0130 08:24:16.459301 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:24:22.999872 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:24:31.286056 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:24:34.013830 140169137129280 submission_runner.py:408] Time since start: 23810.71s, 	Step: 68286, 	{'train/accuracy': 0.7399752736091614, 'train/loss': 0.9882227778434753, 'validation/accuracy': 0.6643999814987183, 'validation/loss': 1.3710088729858398, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.0881545543670654, 'test/num_examples': 10000, 'score': 22987.300876379013, 'total_duration': 23810.711325645447, 'accumulated_submission_time': 22987.300876379013, 'accumulated_eval_time': 819.579030752182, 'accumulated_logging_time': 1.6207971572875977}
I0130 08:24:34.047072 140005313861376 logging_writer.py:48] [68286] accumulated_eval_time=819.579031, accumulated_logging_time=1.620797, accumulated_submission_time=22987.300876, global_step=68286, preemption_count=0, score=22987.300876, test/accuracy=0.539600, test/loss=2.088155, test/num_examples=10000, total_duration=23810.711326, train/accuracy=0.739975, train/loss=0.988223, validation/accuracy=0.664400, validation/loss=1.371009, validation/num_examples=50000
I0130 08:24:39.103332 140005322254080 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.069899082183838, loss=1.548197865486145
I0130 08:25:12.577837 140005313861376 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.052459955215454, loss=1.5638114213943481
I0130 08:25:46.142156 140005322254080 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.120304822921753, loss=1.680086612701416
I0130 08:26:19.751209 140005313861376 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.7959915399551392, loss=1.4738885164260864
I0130 08:26:53.361713 140005322254080 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.8715858459472656, loss=1.501458764076233
I0130 08:27:26.982648 140005313861376 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0400137901306152, loss=1.4787040948867798
I0130 08:28:00.625751 140005322254080 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0392332077026367, loss=1.5877405405044556
I0130 08:28:34.239873 140005313861376 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.126387119293213, loss=1.5544251203536987
I0130 08:29:07.902373 140005322254080 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.2040793895721436, loss=1.5612103939056396
I0130 08:29:41.468704 140005313861376 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.9414787292480469, loss=1.5534391403198242
I0130 08:30:15.055796 140005322254080 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9790130853652954, loss=1.517499327659607
I0130 08:30:48.679640 140005313861376 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.9304349422454834, loss=1.4738428592681885
I0130 08:31:22.325631 140005322254080 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.0946664810180664, loss=1.533290982246399
I0130 08:31:55.942253 140005313861376 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.975812315940857, loss=1.4376580715179443
I0130 08:32:29.584992 140005322254080 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0326035022735596, loss=1.4924397468566895
I0130 08:33:03.206450 140005313861376 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.2176477909088135, loss=1.5215175151824951
I0130 08:33:04.034982 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:33:10.481085 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:33:19.242334 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:33:21.967137 140169137129280 submission_runner.py:408] Time since start: 24338.66s, 	Step: 69804, 	{'train/accuracy': 0.736726701259613, 'train/loss': 1.0046941041946411, 'validation/accuracy': 0.661359965801239, 'validation/loss': 1.3887338638305664, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.137101888656616, 'test/num_examples': 10000, 'score': 23497.229088783264, 'total_duration': 24338.664605140686, 'accumulated_submission_time': 23497.229088783264, 'accumulated_eval_time': 837.5111167430878, 'accumulated_logging_time': 1.6648674011230469}
I0130 08:33:21.999230 140004624946944 logging_writer.py:48] [69804] accumulated_eval_time=837.511117, accumulated_logging_time=1.664867, accumulated_submission_time=23497.229089, global_step=69804, preemption_count=0, score=23497.229089, test/accuracy=0.530100, test/loss=2.137102, test/num_examples=10000, total_duration=24338.664605, train/accuracy=0.736727, train/loss=1.004694, validation/accuracy=0.661360, validation/loss=1.388734, validation/num_examples=50000
I0130 08:33:54.581025 140005288683264 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.8290431499481201, loss=1.5047714710235596
I0130 08:34:28.186243 140004624946944 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.1830050945281982, loss=1.5170197486877441
I0130 08:35:01.828951 140005288683264 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0741288661956787, loss=1.6206247806549072
I0130 08:35:35.532091 140004624946944 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.2224624156951904, loss=1.5325521230697632
I0130 08:36:09.066686 140005288683264 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.9889599084854126, loss=1.4068857431411743
I0130 08:36:42.669275 140004624946944 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.062955141067505, loss=1.4029500484466553
I0130 08:37:16.254865 140005288683264 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.0620529651641846, loss=1.6028823852539062
I0130 08:37:49.892503 140004624946944 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0403242111206055, loss=1.5363997220993042
I0130 08:38:23.522546 140005288683264 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.9259265661239624, loss=1.5561350584030151
I0130 08:38:57.150675 140004624946944 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.9269471168518066, loss=1.5249207019805908
I0130 08:39:30.794386 140005288683264 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.9014947414398193, loss=1.518314003944397
I0130 08:40:04.429142 140004624946944 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.9587589502334595, loss=1.4840259552001953
I0130 08:40:38.063225 140005288683264 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.1238675117492676, loss=1.513750672340393
I0130 08:41:11.671023 140004624946944 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0985379219055176, loss=1.6354141235351562
I0130 08:41:45.236406 140005288683264 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.1584205627441406, loss=1.5153666734695435
I0130 08:41:52.103055 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:41:58.531133 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:42:07.283135 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:42:09.997498 140169137129280 submission_runner.py:408] Time since start: 24866.69s, 	Step: 71322, 	{'train/accuracy': 0.7341557741165161, 'train/loss': 1.0238789319992065, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.3770734071731567, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.085169553756714, 'test/num_examples': 10000, 'score': 24007.27220749855, 'total_duration': 24866.694973945618, 'accumulated_submission_time': 24007.27220749855, 'accumulated_eval_time': 855.4055006504059, 'accumulated_logging_time': 1.7085967063903809}
I0130 08:42:10.028946 140005288683264 logging_writer.py:48] [71322] accumulated_eval_time=855.405501, accumulated_logging_time=1.708597, accumulated_submission_time=24007.272207, global_step=71322, preemption_count=0, score=24007.272207, test/accuracy=0.532400, test/loss=2.085170, test/num_examples=10000, total_duration=24866.694974, train/accuracy=0.734156, train/loss=1.023879, validation/accuracy=0.662120, validation/loss=1.377073, validation/num_examples=50000
I0130 08:42:36.683445 140005313861376 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.1451423168182373, loss=1.5300889015197754
I0130 08:43:10.266040 140005288683264 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.0903754234313965, loss=1.458295226097107
I0130 08:43:43.913130 140005313861376 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.910292148590088, loss=1.5192883014678955
I0130 08:44:17.541516 140005288683264 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.3106541633605957, loss=1.6003857851028442
I0130 08:44:51.187861 140005313861376 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.8692858219146729, loss=1.4940383434295654
I0130 08:45:24.796699 140005288683264 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.1448354721069336, loss=1.6096117496490479
I0130 08:45:58.423053 140005313861376 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.8497827053070068, loss=1.3750274181365967
I0130 08:46:32.041974 140005288683264 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.010197877883911, loss=1.5544835329055786
I0130 08:47:05.648775 140005313861376 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.0302910804748535, loss=1.5937803983688354
I0130 08:47:39.282538 140005288683264 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.9041310548782349, loss=1.523287296295166
I0130 08:48:12.933887 140005313861376 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.05653977394104, loss=1.4922350645065308
I0130 08:48:46.605890 140005288683264 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.123371124267578, loss=1.4700195789337158
I0130 08:49:20.160791 140005313861376 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.053354024887085, loss=1.5749398469924927
I0130 08:49:53.773409 140005288683264 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.029635429382324, loss=1.5978397130966187
I0130 08:50:27.353910 140005313861376 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9401370286941528, loss=1.4584929943084717
I0130 08:50:40.249728 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:50:46.724311 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:50:55.505849 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:50:58.236749 140169137129280 submission_runner.py:408] Time since start: 25394.93s, 	Step: 72840, 	{'train/accuracy': 0.7183912396430969, 'train/loss': 1.0752381086349487, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.404468059539795, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.1446402072906494, 'test/num_examples': 10000, 'score': 24517.43322777748, 'total_duration': 25394.934241056442, 'accumulated_submission_time': 24517.43322777748, 'accumulated_eval_time': 873.3924815654755, 'accumulated_logging_time': 1.7509582042694092}
I0130 08:50:58.269451 140004624946944 logging_writer.py:48] [72840] accumulated_eval_time=873.392482, accumulated_logging_time=1.750958, accumulated_submission_time=24517.433228, global_step=72840, preemption_count=0, score=24517.433228, test/accuracy=0.525600, test/loss=2.144640, test/num_examples=10000, total_duration=25394.934241, train/accuracy=0.718391, train/loss=1.075238, validation/accuracy=0.658500, validation/loss=1.404468, validation/num_examples=50000
I0130 08:51:18.750445 140005288683264 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.028163433074951, loss=1.5762698650360107
I0130 08:51:52.233286 140004624946944 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.9807960987091064, loss=1.431229591369629
I0130 08:52:25.774524 140005288683264 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.119532346725464, loss=1.4607813358306885
I0130 08:52:59.375405 140004624946944 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.9711036682128906, loss=1.4644581079483032
I0130 08:53:32.991215 140005288683264 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.078172445297241, loss=1.4623321294784546
I0130 08:54:06.532204 140004624946944 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.24204421043396, loss=1.340578317642212
I0130 08:54:40.098617 140005288683264 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.9411547183990479, loss=1.4265540838241577
I0130 08:55:13.787144 140004624946944 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.140669584274292, loss=1.4530781507492065
I0130 08:55:47.374287 140005288683264 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.329658269882202, loss=1.5791118144989014
I0130 08:56:21.006300 140004624946944 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.2056777477264404, loss=1.4629000425338745
I0130 08:56:54.593461 140005288683264 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.151256799697876, loss=1.5087997913360596
I0130 08:57:28.196124 140004624946944 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.9768563508987427, loss=1.4519996643066406
I0130 08:58:01.843170 140005288683264 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.9644193649291992, loss=1.4588181972503662
I0130 08:58:35.475613 140004624946944 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.1269116401672363, loss=1.5031237602233887
I0130 08:59:09.092495 140005288683264 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.0644333362579346, loss=1.5408728122711182
I0130 08:59:28.398946 140169137129280 spec.py:321] Evaluating on the training split.
I0130 08:59:34.800647 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 08:59:43.479594 140169137129280 spec.py:349] Evaluating on the test split.
I0130 08:59:46.221050 140169137129280 submission_runner.py:408] Time since start: 25922.92s, 	Step: 74359, 	{'train/accuracy': 0.7762077450752258, 'train/loss': 0.8492001891136169, 'validation/accuracy': 0.660539984703064, 'validation/loss': 1.3998095989227295, 'validation/num_examples': 50000, 'test/accuracy': 0.531000018119812, 'test/loss': 2.1150431632995605, 'test/num_examples': 10000, 'score': 25027.502433538437, 'total_duration': 25922.918542146683, 'accumulated_submission_time': 25027.502433538437, 'accumulated_eval_time': 891.2145557403564, 'accumulated_logging_time': 1.7948503494262695}
I0130 08:59:46.254495 140004624946944 logging_writer.py:48] [74359] accumulated_eval_time=891.214556, accumulated_logging_time=1.794850, accumulated_submission_time=25027.502434, global_step=74359, preemption_count=0, score=25027.502434, test/accuracy=0.531000, test/loss=2.115043, test/num_examples=10000, total_duration=25922.918542, train/accuracy=0.776208, train/loss=0.849200, validation/accuracy=0.660540, validation/loss=1.399810, validation/num_examples=50000
I0130 09:00:00.326980 140005288683264 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.1238789558410645, loss=1.4478726387023926
I0130 09:00:33.843980 140004624946944 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.068438768386841, loss=1.4742182493209839
I0130 09:01:07.445242 140005288683264 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.092693567276001, loss=1.5398858785629272
I0130 09:01:41.082536 140004624946944 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.129523754119873, loss=1.4463518857955933
I0130 09:02:14.687204 140005288683264 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.158453941345215, loss=1.5360113382339478
I0130 09:02:48.290114 140004624946944 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.044304609298706, loss=1.562010407447815
I0130 09:03:21.897851 140005288683264 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.2523622512817383, loss=1.4309806823730469
I0130 09:03:55.497868 140004624946944 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.166027307510376, loss=1.4359543323516846
I0130 09:04:29.113467 140005288683264 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.0139193534851074, loss=1.4346601963043213
I0130 09:05:02.746417 140004624946944 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.1048433780670166, loss=1.4928127527236938
I0130 09:05:36.359255 140005288683264 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.9249826669692993, loss=1.4193803071975708
I0130 09:06:10.004781 140004624946944 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.3861382007598877, loss=1.5265772342681885
I0130 09:06:43.615865 140005288683264 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.142145872116089, loss=1.4485639333724976
I0130 09:07:17.246560 140004624946944 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.295405149459839, loss=1.5492500066757202
I0130 09:07:50.866770 140005288683264 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9644864797592163, loss=1.443110466003418
I0130 09:08:16.294149 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:08:22.667320 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:08:31.287033 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:08:33.973389 140169137129280 submission_runner.py:408] Time since start: 26450.67s, 	Step: 75877, 	{'train/accuracy': 0.7547233700752258, 'train/loss': 0.9260692000389099, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.3541022539138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.06687068939209, 'test/num_examples': 10000, 'score': 25537.48215198517, 'total_duration': 26450.67085123062, 'accumulated_submission_time': 25537.48215198517, 'accumulated_eval_time': 908.893723487854, 'accumulated_logging_time': 1.8392260074615479}
I0130 09:08:34.005873 140005297075968 logging_writer.py:48] [75877] accumulated_eval_time=908.893723, accumulated_logging_time=1.839226, accumulated_submission_time=25537.482152, global_step=75877, preemption_count=0, score=25537.482152, test/accuracy=0.539700, test/loss=2.066871, test/num_examples=10000, total_duration=26450.670851, train/accuracy=0.754723, train/loss=0.926069, validation/accuracy=0.668940, validation/loss=1.354102, validation/num_examples=50000
I0130 09:08:42.052053 140005305468672 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.1546854972839355, loss=1.459585428237915
I0130 09:09:15.562695 140005297075968 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.1409904956817627, loss=1.3936998844146729
I0130 09:09:49.124258 140005305468672 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.3308887481689453, loss=1.5687001943588257
I0130 09:10:22.762149 140005297075968 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.1112282276153564, loss=1.4928405284881592
I0130 09:10:56.389511 140005305468672 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.2368786334991455, loss=1.4717543125152588
I0130 09:11:29.997460 140005297075968 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.3932037353515625, loss=1.4377546310424805
I0130 09:12:03.604089 140005305468672 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1821672916412354, loss=1.6088635921478271
I0130 09:12:37.209782 140005297075968 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1306331157684326, loss=1.4645568132400513
I0130 09:13:10.755238 140005305468672 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.066617250442505, loss=1.4809322357177734
I0130 09:13:44.319466 140005297075968 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.1956608295440674, loss=1.4129852056503296
I0130 09:14:17.937858 140005305468672 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.086215019226074, loss=1.5318249464035034
I0130 09:14:51.657561 140005297075968 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1039133071899414, loss=1.498417854309082
I0130 09:15:25.201987 140005305468672 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.120248556137085, loss=1.4107716083526611
I0130 09:15:58.812723 140005297075968 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.2112648487091064, loss=1.441372275352478
I0130 09:16:32.431690 140005305468672 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.053335666656494, loss=1.501067042350769
I0130 09:17:04.194845 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:17:10.550510 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:17:19.159545 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:17:21.875343 140169137129280 submission_runner.py:408] Time since start: 26978.57s, 	Step: 77396, 	{'train/accuracy': 0.7463727593421936, 'train/loss': 0.9656037092208862, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.3637856245040894, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.0874693393707275, 'test/num_examples': 10000, 'score': 26047.611650943756, 'total_duration': 26978.572833299637, 'accumulated_submission_time': 26047.611650943756, 'accumulated_eval_time': 926.574179649353, 'accumulated_logging_time': 1.88204026222229}
I0130 09:17:21.911982 140004624946944 logging_writer.py:48] [77396] accumulated_eval_time=926.574180, accumulated_logging_time=1.882040, accumulated_submission_time=26047.611651, global_step=77396, preemption_count=0, score=26047.611651, test/accuracy=0.533700, test/loss=2.087469, test/num_examples=10000, total_duration=26978.572833, train/accuracy=0.746373, train/loss=0.965604, validation/accuracy=0.666600, validation/loss=1.363786, validation/num_examples=50000
I0130 09:17:23.606965 140005288683264 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.164280652999878, loss=1.465628743171692
I0130 09:17:57.125895 140004624946944 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.254011631011963, loss=1.5367733240127563
I0130 09:18:30.757849 140005288683264 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.199629545211792, loss=1.4493674039840698
I0130 09:19:04.379374 140004624946944 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.0885207653045654, loss=1.470773458480835
I0130 09:19:38.000270 140005288683264 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.1496479511260986, loss=1.487363338470459
I0130 09:20:11.616362 140004624946944 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.035855770111084, loss=1.5259606838226318
I0130 09:20:45.249285 140005288683264 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.3764426708221436, loss=1.467106819152832
I0130 09:21:18.975042 140004624946944 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.270667314529419, loss=1.4360584020614624
I0130 09:21:52.507957 140005288683264 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.9438668489456177, loss=1.3233146667480469
I0130 09:22:26.088134 140004624946944 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.267223358154297, loss=1.4992873668670654
I0130 09:22:59.648933 140005288683264 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.3095932006835938, loss=1.456595540046692
I0130 09:23:33.170846 140004624946944 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.421489953994751, loss=1.5029596090316772
I0130 09:24:06.706583 140005288683264 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.1280517578125, loss=1.4665169715881348
I0130 09:24:40.274574 140004624946944 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.154822826385498, loss=1.5342533588409424
I0130 09:25:13.892487 140005288683264 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.503756523132324, loss=1.4883724451065063
I0130 09:25:47.502518 140004624946944 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.14729380607605, loss=1.4173455238342285
I0130 09:25:52.013022 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:25:58.432141 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:26:06.945783 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:26:09.669730 140169137129280 submission_runner.py:408] Time since start: 27506.37s, 	Step: 78915, 	{'train/accuracy': 0.724609375, 'train/loss': 1.0671521425247192, 'validation/accuracy': 0.6492800116539001, 'validation/loss': 1.4398260116577148, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.150360107421875, 'test/num_examples': 10000, 'score': 26557.652361154556, 'total_duration': 27506.367225170135, 'accumulated_submission_time': 26557.652361154556, 'accumulated_eval_time': 944.2308526039124, 'accumulated_logging_time': 1.929426908493042}
I0130 09:26:09.703702 140004624946944 logging_writer.py:48] [78915] accumulated_eval_time=944.230853, accumulated_logging_time=1.929427, accumulated_submission_time=26557.652361, global_step=78915, preemption_count=0, score=26557.652361, test/accuracy=0.525700, test/loss=2.150360, test/num_examples=10000, total_duration=27506.367225, train/accuracy=0.724609, train/loss=1.067152, validation/accuracy=0.649280, validation/loss=1.439826, validation/num_examples=50000
I0130 09:26:38.526004 140005305468672 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.2496635913848877, loss=1.534208059310913
I0130 09:27:12.077564 140004624946944 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.0414819717407227, loss=1.4331374168395996
I0130 09:27:45.562147 140005305468672 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.1791739463806152, loss=1.5460566282272339
I0130 09:28:19.296325 140004624946944 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.3789052963256836, loss=1.509965181350708
I0130 09:28:52.933794 140005305468672 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.1925628185272217, loss=1.5332728624343872
I0130 09:29:26.524658 140004624946944 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.1921539306640625, loss=1.5851938724517822
I0130 09:30:00.128392 140005305468672 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.31207537651062, loss=1.4742891788482666
I0130 09:30:33.722631 140004624946944 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.0203542709350586, loss=1.3330143690109253
I0130 09:31:07.387020 140005305468672 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.2696783542633057, loss=1.4738273620605469
I0130 09:31:40.994141 140004624946944 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1633293628692627, loss=1.3956332206726074
I0130 09:32:14.629417 140005305468672 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.257413387298584, loss=1.5090551376342773
I0130 09:32:48.262343 140004624946944 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.027531862258911, loss=1.4724235534667969
I0130 09:33:21.881925 140005305468672 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.2538280487060547, loss=1.5143836736679077
I0130 09:33:55.507424 140004624946944 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.18267822265625, loss=1.5260531902313232
I0130 09:34:29.185227 140005305468672 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.0627152919769287, loss=1.5286180973052979
I0130 09:34:39.722785 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:34:46.089503 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:34:54.706111 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:34:57.461615 140169137129280 submission_runner.py:408] Time since start: 28034.16s, 	Step: 80433, 	{'train/accuracy': 0.7413902878761292, 'train/loss': 0.9853048324584961, 'validation/accuracy': 0.6678000092506409, 'validation/loss': 1.3506805896759033, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.104771375656128, 'test/num_examples': 10000, 'score': 27067.61016345024, 'total_duration': 28034.159109592438, 'accumulated_submission_time': 27067.61016345024, 'accumulated_eval_time': 961.969643831253, 'accumulated_logging_time': 1.9752976894378662}
I0130 09:34:57.501709 140004608161536 logging_writer.py:48] [80433] accumulated_eval_time=961.969644, accumulated_logging_time=1.975298, accumulated_submission_time=27067.610163, global_step=80433, preemption_count=0, score=27067.610163, test/accuracy=0.536000, test/loss=2.104771, test/num_examples=10000, total_duration=28034.159110, train/accuracy=0.741390, train/loss=0.985305, validation/accuracy=0.667800, validation/loss=1.350681, validation/num_examples=50000
I0130 09:35:20.309417 140004616554240 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.16461181640625, loss=1.5878877639770508
I0130 09:35:53.911446 140004608161536 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.3889825344085693, loss=1.4446945190429688
I0130 09:36:27.548287 140004616554240 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.066756248474121, loss=1.511889934539795
I0130 09:37:01.162073 140004608161536 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.1530163288116455, loss=1.5814356803894043
I0130 09:37:34.804151 140004616554240 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.2921907901763916, loss=1.4494436979293823
I0130 09:38:08.414983 140004608161536 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.0307836532592773, loss=1.3678631782531738
I0130 09:38:42.053162 140004616554240 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.34004545211792, loss=1.456435203552246
I0130 09:39:15.663238 140004608161536 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.176954984664917, loss=1.3882797956466675
I0130 09:39:49.201843 140004616554240 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.260977029800415, loss=1.5270346403121948
I0130 09:40:22.734024 140004608161536 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.572826862335205, loss=1.413681149482727
I0130 09:40:56.378692 140004616554240 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.211268424987793, loss=1.4161146879196167
I0130 09:41:30.015568 140004608161536 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.265187978744507, loss=1.4124505519866943
I0130 09:42:03.655704 140004616554240 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.3215980529785156, loss=1.3759305477142334
I0130 09:42:37.262653 140004608161536 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.0825400352478027, loss=1.5236914157867432
I0130 09:43:10.875057 140004616554240 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1204936504364014, loss=1.4203773736953735
I0130 09:43:27.497903 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:43:33.901874 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:43:42.622211 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:43:45.300159 140169137129280 submission_runner.py:408] Time since start: 28562.00s, 	Step: 81951, 	{'train/accuracy': 0.7299306392669678, 'train/loss': 1.0427826642990112, 'validation/accuracy': 0.6603999733924866, 'validation/loss': 1.401990294456482, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.1657660007476807, 'test/num_examples': 10000, 'score': 27577.544088840485, 'total_duration': 28561.997648715973, 'accumulated_submission_time': 27577.544088840485, 'accumulated_eval_time': 979.7718670368195, 'accumulated_logging_time': 2.0286335945129395}
I0130 09:43:45.333745 140004616554240 logging_writer.py:48] [81951] accumulated_eval_time=979.771867, accumulated_logging_time=2.028634, accumulated_submission_time=27577.544089, global_step=81951, preemption_count=0, score=27577.544089, test/accuracy=0.530700, test/loss=2.165766, test/num_examples=10000, total_duration=28561.997649, train/accuracy=0.729931, train/loss=1.042783, validation/accuracy=0.660400, validation/loss=1.401990, validation/num_examples=50000
I0130 09:44:02.140533 140004624946944 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1607768535614014, loss=1.4261155128479004
I0130 09:44:35.749208 140004616554240 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.4125008583068848, loss=1.3848600387573242
I0130 09:45:09.330830 140004624946944 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.1966311931610107, loss=1.4278671741485596
I0130 09:45:42.865661 140004616554240 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.1732661724090576, loss=1.509220004081726
I0130 09:46:16.393728 140004624946944 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.2393178939819336, loss=1.5645838975906372
I0130 09:46:49.959868 140004616554240 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2584855556488037, loss=1.40237295627594
I0130 09:47:23.539745 140004624946944 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.2453033924102783, loss=1.623711109161377
I0130 09:47:57.193179 140004616554240 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.226736068725586, loss=1.4752740859985352
I0130 09:48:30.852962 140004624946944 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.1497702598571777, loss=1.4324088096618652
I0130 09:49:04.469959 140004616554240 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.3639612197875977, loss=1.392159342765808
I0130 09:49:38.101778 140004624946944 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.221538543701172, loss=1.382198452949524
I0130 09:50:11.721198 140004616554240 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.07399845123291, loss=1.3387938737869263
I0130 09:50:45.369145 140004624946944 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.2072556018829346, loss=1.507011890411377
I0130 09:51:18.998063 140004616554240 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.2878496646881104, loss=1.4005670547485352
I0130 09:51:52.614441 140004624946944 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.56131649017334, loss=1.4884052276611328
I0130 09:52:15.628774 140169137129280 spec.py:321] Evaluating on the training split.
I0130 09:52:22.059508 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 09:52:30.570694 140169137129280 spec.py:349] Evaluating on the test split.
I0130 09:52:33.314150 140169137129280 submission_runner.py:408] Time since start: 29090.01s, 	Step: 83470, 	{'train/accuracy': 0.7867506146430969, 'train/loss': 0.7846525311470032, 'validation/accuracy': 0.6734799742698669, 'validation/loss': 1.3287465572357178, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.0561273097991943, 'test/num_examples': 10000, 'score': 28087.777733802795, 'total_duration': 29090.011644124985, 'accumulated_submission_time': 28087.777733802795, 'accumulated_eval_time': 997.4572043418884, 'accumulated_logging_time': 2.0746822357177734}
I0130 09:52:33.348047 140004616554240 logging_writer.py:48] [83470] accumulated_eval_time=997.457204, accumulated_logging_time=2.074682, accumulated_submission_time=28087.777734, global_step=83470, preemption_count=0, score=28087.777734, test/accuracy=0.542500, test/loss=2.056127, test/num_examples=10000, total_duration=29090.011644, train/accuracy=0.786751, train/loss=0.784653, validation/accuracy=0.673480, validation/loss=1.328747, validation/num_examples=50000
I0130 09:52:43.760432 140005297075968 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.2211098670959473, loss=1.5404506921768188
I0130 09:53:17.273567 140004616554240 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.16752290725708, loss=1.5014894008636475
I0130 09:53:50.885497 140005297075968 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.138390064239502, loss=1.3768728971481323
I0130 09:54:24.544836 140004616554240 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.1463782787323, loss=1.5183385610580444
I0130 09:54:58.120674 140005297075968 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.233382225036621, loss=1.5079725980758667
I0130 09:55:31.742863 140004616554240 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.1816370487213135, loss=1.4191431999206543
I0130 09:56:05.370417 140005297075968 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1693813800811768, loss=1.4213156700134277
I0130 09:56:38.996756 140004616554240 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.243133306503296, loss=1.4733405113220215
I0130 09:57:12.620241 140005297075968 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.1689701080322266, loss=1.413112998008728
I0130 09:57:46.191580 140004616554240 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.2862918376922607, loss=1.447335958480835
I0130 09:58:19.759685 140005297075968 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.9738706350326538, loss=1.4555145502090454
I0130 09:58:53.390377 140004616554240 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.2594876289367676, loss=1.4841880798339844
I0130 09:59:27.023750 140005297075968 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.2086069583892822, loss=1.4441730976104736
I0130 10:00:00.653031 140004616554240 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.295210599899292, loss=1.571446418762207
I0130 10:00:34.377332 140005297075968 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.4556972980499268, loss=1.5299310684204102
I0130 10:01:03.359639 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:01:09.777049 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:01:18.558602 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:01:21.341493 140169137129280 submission_runner.py:408] Time since start: 29618.04s, 	Step: 84988, 	{'train/accuracy': 0.7635124325752258, 'train/loss': 0.8853896856307983, 'validation/accuracy': 0.6748999953269958, 'validation/loss': 1.3213706016540527, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.043912649154663, 'test/num_examples': 10000, 'score': 28597.729063987732, 'total_duration': 29618.0389854908, 'accumulated_submission_time': 28597.729063987732, 'accumulated_eval_time': 1015.4390184879303, 'accumulated_logging_time': 2.119399070739746}
I0130 10:01:21.374666 140005288683264 logging_writer.py:48] [84988] accumulated_eval_time=1015.439018, accumulated_logging_time=2.119399, accumulated_submission_time=28597.729064, global_step=84988, preemption_count=0, score=28597.729064, test/accuracy=0.552100, test/loss=2.043913, test/num_examples=10000, total_duration=29618.038985, train/accuracy=0.763512, train/loss=0.885390, validation/accuracy=0.674900, validation/loss=1.321371, validation/num_examples=50000
I0130 10:01:25.743008 140005297075968 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2289376258850098, loss=1.4665426015853882
I0130 10:01:59.262325 140005288683264 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1462650299072266, loss=1.4336283206939697
I0130 10:02:32.876402 140005297075968 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.2940046787261963, loss=1.403951644897461
I0130 10:03:06.507612 140005288683264 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.192007303237915, loss=1.410065770149231
I0130 10:03:40.135251 140005297075968 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.2288730144500732, loss=1.4120101928710938
I0130 10:04:13.708853 140005288683264 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.2849009037017822, loss=1.4842517375946045
I0130 10:04:47.308913 140005297075968 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2994203567504883, loss=1.3792368173599243
I0130 10:05:20.952286 140005288683264 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.283649444580078, loss=1.499677300453186
I0130 10:05:54.574275 140005297075968 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.333975315093994, loss=1.4572705030441284
I0130 10:06:28.206970 140005288683264 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.139153242111206, loss=1.3857529163360596
I0130 10:07:01.838866 140005297075968 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.0296266078948975, loss=1.3752022981643677
I0130 10:07:35.478676 140005288683264 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.1657397747039795, loss=1.4100111722946167
I0130 10:08:09.030462 140005297075968 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.265256643295288, loss=1.4129102230072021
I0130 10:08:42.630494 140005288683264 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.5990302562713623, loss=1.4737660884857178
I0130 10:09:16.261096 140005297075968 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.3647217750549316, loss=1.4579495191574097
I0130 10:09:49.870779 140005288683264 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.420701503753662, loss=1.5280284881591797
I0130 10:09:51.369628 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:09:57.756696 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:10:06.218369 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:10:08.898905 140169137129280 submission_runner.py:408] Time since start: 30145.60s, 	Step: 86506, 	{'train/accuracy': 0.7604631781578064, 'train/loss': 0.9026753902435303, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.3243170976638794, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.0270941257476807, 'test/num_examples': 10000, 'score': 29107.663843154907, 'total_duration': 30145.59640312195, 'accumulated_submission_time': 29107.663843154907, 'accumulated_eval_time': 1032.9682595729828, 'accumulated_logging_time': 2.1635024547576904}
I0130 10:10:08.934866 140004624946944 logging_writer.py:48] [86506] accumulated_eval_time=1032.968260, accumulated_logging_time=2.163502, accumulated_submission_time=29107.663843, global_step=86506, preemption_count=0, score=29107.663843, test/accuracy=0.548300, test/loss=2.027094, test/num_examples=10000, total_duration=30145.596403, train/accuracy=0.760463, train/loss=0.902675, validation/accuracy=0.675760, validation/loss=1.324317, validation/num_examples=50000
I0130 10:10:40.754194 140005288683264 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.1826603412628174, loss=1.4454407691955566
I0130 10:11:14.340316 140004624946944 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3646159172058105, loss=1.4209362268447876
I0130 10:11:47.935871 140005288683264 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.217777967453003, loss=1.5340718030929565
I0130 10:12:21.564249 140004624946944 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.003974437713623, loss=1.40218186378479
I0130 10:12:55.178155 140005288683264 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.237997531890869, loss=1.4651868343353271
I0130 10:13:28.815128 140004624946944 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.1424646377563477, loss=1.392114281654358
I0130 10:14:02.485302 140005288683264 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.407139778137207, loss=1.3662739992141724
I0130 10:14:36.092499 140004624946944 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.395681381225586, loss=1.532857894897461
I0130 10:15:09.722547 140005288683264 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.3597142696380615, loss=1.453677773475647
I0130 10:15:43.355943 140004624946944 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.3130571842193604, loss=1.472050428390503
I0130 10:16:16.966749 140005288683264 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.229266405105591, loss=1.491072177886963
I0130 10:16:50.596463 140004624946944 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.448085069656372, loss=1.3628804683685303
I0130 10:17:24.227699 140005288683264 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.3196020126342773, loss=1.3749593496322632
I0130 10:17:57.864498 140004624946944 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.4806947708129883, loss=1.4907410144805908
I0130 10:18:31.480282 140005288683264 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.1754369735717773, loss=1.4233649969100952
I0130 10:18:39.024740 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:18:45.389427 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:18:54.061349 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:18:56.771097 140169137129280 submission_runner.py:408] Time since start: 30673.47s, 	Step: 88024, 	{'train/accuracy': 0.753926157951355, 'train/loss': 0.9288226366043091, 'validation/accuracy': 0.6761999726295471, 'validation/loss': 1.318540334701538, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.064603328704834, 'test/num_examples': 10000, 'score': 29617.69456934929, 'total_duration': 30673.468591213226, 'accumulated_submission_time': 29617.69456934929, 'accumulated_eval_time': 1050.7145743370056, 'accumulated_logging_time': 2.2094473838806152}
I0130 10:18:56.805560 140005297075968 logging_writer.py:48] [88024] accumulated_eval_time=1050.714574, accumulated_logging_time=2.209447, accumulated_submission_time=29617.694569, global_step=88024, preemption_count=0, score=29617.694569, test/accuracy=0.541100, test/loss=2.064603, test/num_examples=10000, total_duration=30673.468591, train/accuracy=0.753926, train/loss=0.928823, validation/accuracy=0.676200, validation/loss=1.318540, validation/num_examples=50000
I0130 10:19:22.608972 140005313861376 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3075430393218994, loss=1.381049394607544
I0130 10:19:56.168505 140005297075968 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3297362327575684, loss=1.4317736625671387
I0130 10:20:29.800330 140005313861376 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.2312304973602295, loss=1.4608752727508545
I0130 10:21:03.412380 140005297075968 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.2398762702941895, loss=1.4081299304962158
I0130 10:21:37.038127 140005313861376 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.443962812423706, loss=1.4047412872314453
I0130 10:22:10.681591 140005297075968 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.383221387863159, loss=1.3739680051803589
I0130 10:22:44.316699 140005313861376 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.146362543106079, loss=1.46151602268219
I0130 10:23:17.945476 140005297075968 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.4575159549713135, loss=1.2944802045822144
I0130 10:23:51.565878 140005313861376 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.201662063598633, loss=1.3243770599365234
I0130 10:24:25.172053 140005297075968 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.251006603240967, loss=1.4168891906738281
I0130 10:24:58.792312 140005313861376 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3470077514648438, loss=1.4031563997268677
I0130 10:25:32.367732 140005297075968 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.166659116744995, loss=1.4141130447387695
I0130 10:26:05.909867 140005313861376 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.540328025817871, loss=1.4145534038543701
I0130 10:26:39.598862 140005297075968 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.225682497024536, loss=1.293433427810669
I0130 10:27:13.228622 140005313861376 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.4054672718048096, loss=1.2516027688980103
I0130 10:27:26.831829 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:27:33.269061 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:27:42.033358 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:27:44.786443 140169137129280 submission_runner.py:408] Time since start: 31201.48s, 	Step: 89542, 	{'train/accuracy': 0.7547233700752258, 'train/loss': 0.9210852384567261, 'validation/accuracy': 0.6802200078964233, 'validation/loss': 1.296499252319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.026613473892212, 'test/num_examples': 10000, 'score': 30127.660895824432, 'total_duration': 31201.483937740326, 'accumulated_submission_time': 30127.660895824432, 'accumulated_eval_time': 1068.669147491455, 'accumulated_logging_time': 2.254591703414917}
I0130 10:27:44.825942 140004624946944 logging_writer.py:48] [89542] accumulated_eval_time=1068.669147, accumulated_logging_time=2.254592, accumulated_submission_time=30127.660896, global_step=89542, preemption_count=0, score=30127.660896, test/accuracy=0.551200, test/loss=2.026613, test/num_examples=10000, total_duration=31201.483938, train/accuracy=0.754723, train/loss=0.921085, validation/accuracy=0.680220, validation/loss=1.296499, validation/num_examples=50000
I0130 10:28:04.633187 140005288683264 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.41955304145813, loss=1.4420921802520752
I0130 10:28:38.178929 140004624946944 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.3822836875915527, loss=1.4913382530212402
I0130 10:29:11.682151 140005288683264 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.3049051761627197, loss=1.4589526653289795
I0130 10:29:45.248717 140004624946944 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.3775928020477295, loss=1.3359642028808594
I0130 10:30:18.800822 140005288683264 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.223748207092285, loss=1.3495519161224365
I0130 10:30:52.403946 140004624946944 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.4642302989959717, loss=1.4326021671295166
I0130 10:31:26.020922 140005288683264 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.2444305419921875, loss=1.3931745290756226
I0130 10:31:59.628564 140004624946944 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.4016826152801514, loss=1.4295265674591064
I0130 10:32:33.279365 140005288683264 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.4881505966186523, loss=1.446791410446167
I0130 10:33:06.897742 140004624946944 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.3345725536346436, loss=1.451206088066101
I0130 10:33:40.527185 140005288683264 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.3856348991394043, loss=1.3883206844329834
I0130 10:34:14.093153 140004624946944 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.2874886989593506, loss=1.3055819272994995
I0130 10:34:47.704245 140005288683264 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4509856700897217, loss=1.411128282546997
I0130 10:35:21.360657 140004624946944 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2167181968688965, loss=1.3088788986206055
I0130 10:35:54.987572 140005288683264 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.2690072059631348, loss=1.416881799697876
I0130 10:36:14.962531 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:36:21.419964 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:36:29.797373 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:36:32.563613 140169137129280 submission_runner.py:408] Time since start: 31729.26s, 	Step: 91061, 	{'train/accuracy': 0.7591477632522583, 'train/loss': 0.9124574065208435, 'validation/accuracy': 0.6842399835586548, 'validation/loss': 1.2905629873275757, 'validation/num_examples': 50000, 'test/accuracy': 0.5562000274658203, 'test/loss': 1.9942518472671509, 'test/num_examples': 10000, 'score': 30637.73820257187, 'total_duration': 31729.2611079216, 'accumulated_submission_time': 30637.73820257187, 'accumulated_eval_time': 1086.2701907157898, 'accumulated_logging_time': 2.303988218307495}
I0130 10:36:32.599838 140005305468672 logging_writer.py:48] [91061] accumulated_eval_time=1086.270191, accumulated_logging_time=2.303988, accumulated_submission_time=30637.738203, global_step=91061, preemption_count=0, score=30637.738203, test/accuracy=0.556200, test/loss=1.994252, test/num_examples=10000, total_duration=31729.261108, train/accuracy=0.759148, train/loss=0.912457, validation/accuracy=0.684240, validation/loss=1.290563, validation/num_examples=50000
I0130 10:36:46.042202 140005313861376 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.143249034881592, loss=1.4330296516418457
I0130 10:37:19.603121 140005305468672 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.4238598346710205, loss=1.4342976808547974
I0130 10:37:53.175077 140005313861376 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.507861375808716, loss=1.450523018836975
I0130 10:38:26.768998 140005305468672 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.22623610496521, loss=1.2845364809036255
I0130 10:39:00.376871 140005313861376 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.3495469093322754, loss=1.5383076667785645
I0130 10:39:34.023585 140005305468672 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.449270009994507, loss=1.4133363962173462
I0130 10:40:07.736567 140005313861376 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3395280838012695, loss=1.4072866439819336
I0130 10:40:41.366416 140005305468672 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.4276225566864014, loss=1.4445090293884277
I0130 10:41:15.010814 140005313861376 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.610144853591919, loss=1.434483289718628
I0130 10:41:48.635293 140005305468672 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.46189284324646, loss=1.4083608388900757
I0130 10:42:22.234532 140005313861376 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.2723233699798584, loss=1.4441062211990356
I0130 10:42:55.860265 140005305468672 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.1716439723968506, loss=1.3844361305236816
I0130 10:43:29.451784 140005313861376 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.294799327850342, loss=1.3652360439300537
I0130 10:44:03.104801 140005305468672 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.3840408325195312, loss=1.4421110153198242
I0130 10:44:36.745880 140005313861376 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.387256145477295, loss=1.400294303894043
I0130 10:45:02.783441 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:45:09.236854 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:45:17.955882 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:45:20.690310 140169137129280 submission_runner.py:408] Time since start: 32257.39s, 	Step: 92579, 	{'train/accuracy': 0.7926697731018066, 'train/loss': 0.767687976360321, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.2874754667282104, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.0014238357543945, 'test/num_examples': 10000, 'score': 31147.860745429993, 'total_duration': 32257.387805223465, 'accumulated_submission_time': 31147.860745429993, 'accumulated_eval_time': 1104.1770284175873, 'accumulated_logging_time': 2.3518271446228027}
I0130 10:45:20.724385 140004616554240 logging_writer.py:48] [92579] accumulated_eval_time=1104.177028, accumulated_logging_time=2.351827, accumulated_submission_time=31147.860745, global_step=92579, preemption_count=0, score=31147.860745, test/accuracy=0.551600, test/loss=2.001424, test/num_examples=10000, total_duration=32257.387805, train/accuracy=0.792670, train/loss=0.767688, validation/accuracy=0.684560, validation/loss=1.287475, validation/num_examples=50000
I0130 10:45:28.124852 140004624946944 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3658835887908936, loss=1.4110817909240723
I0130 10:46:01.730440 140004616554240 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.6081833839416504, loss=1.4913207292556763
I0130 10:46:35.405167 140004624946944 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.330171585083008, loss=1.4480891227722168
I0130 10:47:09.014484 140004616554240 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.5107390880584717, loss=1.3896676301956177
I0130 10:47:42.640368 140004624946944 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.250497817993164, loss=1.3828861713409424
I0130 10:48:16.270598 140004616554240 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.190633535385132, loss=1.3789043426513672
I0130 10:48:49.881806 140004624946944 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.3372528553009033, loss=1.3166592121124268
I0130 10:49:23.453908 140004616554240 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.2697391510009766, loss=1.389095664024353
I0130 10:49:56.986982 140004624946944 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.420517921447754, loss=1.2891157865524292
I0130 10:50:30.603752 140004616554240 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.433337688446045, loss=1.3903279304504395
I0130 10:51:04.189133 140004624946944 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.4852726459503174, loss=1.4091116189956665
I0130 10:51:37.723280 140004616554240 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.55706524848938, loss=1.4148215055465698
I0130 10:52:11.766629 140004624946944 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.2492237091064453, loss=1.345968246459961
I0130 10:52:45.381745 140004616554240 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.1827545166015625, loss=1.2937692403793335
I0130 10:53:19.041838 140004624946944 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.2496211528778076, loss=1.454504132270813
I0130 10:53:50.696299 140169137129280 spec.py:321] Evaluating on the training split.
I0130 10:53:57.186686 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 10:54:05.659502 140169137129280 spec.py:349] Evaluating on the test split.
I0130 10:54:08.355596 140169137129280 submission_runner.py:408] Time since start: 32785.05s, 	Step: 94096, 	{'train/accuracy': 0.7694913744926453, 'train/loss': 0.8633120656013489, 'validation/accuracy': 0.6793199777603149, 'validation/loss': 1.320208191871643, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 2.048421621322632, 'test/num_examples': 10000, 'score': 31657.77154326439, 'total_duration': 32785.05307340622, 'accumulated_submission_time': 31657.77154326439, 'accumulated_eval_time': 1121.8362724781036, 'accumulated_logging_time': 2.3979732990264893}
I0130 10:54:08.390774 140004616554240 logging_writer.py:48] [94096] accumulated_eval_time=1121.836272, accumulated_logging_time=2.397973, accumulated_submission_time=31657.771543, global_step=94096, preemption_count=0, score=31657.771543, test/accuracy=0.553600, test/loss=2.048422, test/num_examples=10000, total_duration=32785.053073, train/accuracy=0.769491, train/loss=0.863312, validation/accuracy=0.679320, validation/loss=1.320208, validation/num_examples=50000
I0130 10:54:10.080232 140004624946944 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.4046969413757324, loss=1.4199579954147339
I0130 10:54:43.613596 140004616554240 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.2900078296661377, loss=1.3427693843841553
I0130 10:55:17.210072 140004624946944 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4198880195617676, loss=1.4068955183029175
I0130 10:55:50.790531 140004616554240 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.3525917530059814, loss=1.3859561681747437
I0130 10:56:24.423472 140004624946944 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.193800926208496, loss=1.3692930936813354
I0130 10:56:58.041152 140004616554240 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.4867324829101562, loss=1.339690923690796
I0130 10:57:31.645414 140004624946944 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.5861971378326416, loss=1.3506052494049072
I0130 10:58:05.261368 140004616554240 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.6970417499542236, loss=1.330221176147461
I0130 10:58:38.898051 140004624946944 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.3891079425811768, loss=1.4725927114486694
I0130 10:59:12.542687 140004616554240 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.318424701690674, loss=1.41428542137146
I0130 10:59:46.203525 140004624946944 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.4377665519714355, loss=1.431125283241272
I0130 11:00:19.749646 140004616554240 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.582301378250122, loss=1.4481124877929688
I0130 11:00:53.316620 140004624946944 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.3337209224700928, loss=1.3118900060653687
I0130 11:01:26.953990 140004616554240 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.435783863067627, loss=1.4540109634399414
I0130 11:02:00.571144 140004624946944 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.359610080718994, loss=1.4134595394134521
I0130 11:02:34.192405 140004616554240 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.426745653152466, loss=1.4651885032653809
I0130 11:02:38.378297 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:02:44.765950 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:02:53.442252 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:02:56.159406 140169137129280 submission_runner.py:408] Time since start: 33312.86s, 	Step: 95614, 	{'train/accuracy': 0.7730388641357422, 'train/loss': 0.8482105135917664, 'validation/accuracy': 0.6853799819946289, 'validation/loss': 1.288509726524353, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.018871784210205, 'test/num_examples': 10000, 'score': 32167.696942329407, 'total_duration': 33312.856902360916, 'accumulated_submission_time': 32167.696942329407, 'accumulated_eval_time': 1139.6173412799835, 'accumulated_logging_time': 2.44614315032959}
I0130 11:02:56.192735 140005297075968 logging_writer.py:48] [95614] accumulated_eval_time=1139.617341, accumulated_logging_time=2.446143, accumulated_submission_time=32167.696942, global_step=95614, preemption_count=0, score=32167.696942, test/accuracy=0.556100, test/loss=2.018872, test/num_examples=10000, total_duration=33312.856902, train/accuracy=0.773039, train/loss=0.848211, validation/accuracy=0.685380, validation/loss=1.288510, validation/num_examples=50000
I0130 11:03:25.341671 140005322254080 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.382037878036499, loss=1.3871804475784302
I0130 11:03:58.880026 140005297075968 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.4593989849090576, loss=1.381221890449524
I0130 11:04:32.449011 140005322254080 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.2389779090881348, loss=1.395327091217041
I0130 11:05:06.079711 140005297075968 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.30582332611084, loss=1.3343325853347778
I0130 11:05:39.723697 140005322254080 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.3100106716156006, loss=1.392525315284729
I0130 11:06:13.445101 140005297075968 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.560634136199951, loss=1.452210783958435
I0130 11:06:47.049340 140005322254080 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.535590887069702, loss=1.4625165462493896
I0130 11:07:20.684265 140005297075968 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.22959566116333, loss=1.2749677896499634
I0130 11:07:54.291135 140005322254080 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.6131036281585693, loss=1.461233377456665
I0130 11:08:27.928862 140005297075968 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4329416751861572, loss=1.4596997499465942
I0130 11:09:01.520802 140005322254080 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.5261998176574707, loss=1.3201640844345093
I0130 11:09:35.155002 140005297075968 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.3211920261383057, loss=1.3080387115478516
I0130 11:10:08.787019 140005322254080 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.468069314956665, loss=1.3819540739059448
I0130 11:10:42.415373 140005297075968 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.5832722187042236, loss=1.4505996704101562
I0130 11:11:16.058595 140005322254080 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.346130609512329, loss=1.3783042430877686
I0130 11:11:26.297031 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:11:33.549073 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:11:42.368426 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:11:45.093832 140169137129280 submission_runner.py:408] Time since start: 33841.79s, 	Step: 97132, 	{'train/accuracy': 0.751375138759613, 'train/loss': 0.9182205200195312, 'validation/accuracy': 0.6756399869918823, 'validation/loss': 1.318800449371338, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.0723519325256348, 'test/num_examples': 10000, 'score': 32677.73971581459, 'total_duration': 33841.79132437706, 'accumulated_submission_time': 32677.73971581459, 'accumulated_eval_time': 1158.4141011238098, 'accumulated_logging_time': 2.492197275161743}
I0130 11:11:45.130587 140004616554240 logging_writer.py:48] [97132] accumulated_eval_time=1158.414101, accumulated_logging_time=2.492197, accumulated_submission_time=32677.739716, global_step=97132, preemption_count=0, score=32677.739716, test/accuracy=0.542500, test/loss=2.072352, test/num_examples=10000, total_duration=33841.791324, train/accuracy=0.751375, train/loss=0.918221, validation/accuracy=0.675640, validation/loss=1.318800, validation/num_examples=50000
I0130 11:12:08.281033 140004624946944 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.4137966632843018, loss=1.4329092502593994
I0130 11:12:41.942171 140004616554240 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.5210530757904053, loss=1.3680219650268555
I0130 11:13:15.479355 140004624946944 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.4364070892333984, loss=1.298840045928955
I0130 11:13:49.073183 140004616554240 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.2228496074676514, loss=1.2634975910186768
I0130 11:14:22.708759 140004624946944 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.3129305839538574, loss=1.4053422212600708
I0130 11:14:56.337157 140004616554240 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4944021701812744, loss=1.4295135736465454
I0130 11:15:29.960843 140004624946944 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.6416594982147217, loss=1.3695443868637085
I0130 11:16:03.518773 140004616554240 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.3109753131866455, loss=1.3234294652938843
I0130 11:16:37.075188 140004624946944 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.344836711883545, loss=1.3273049592971802
I0130 11:17:10.619786 140004616554240 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.388745069503784, loss=1.3039755821228027
I0130 11:17:44.202309 140004624946944 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.267564058303833, loss=1.3088033199310303
I0130 11:18:17.817496 140004616554240 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.38842511177063, loss=1.3636311292648315
I0130 11:18:51.434386 140004624946944 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.369626760482788, loss=1.3220785856246948
I0130 11:19:25.075818 140004616554240 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4112229347229004, loss=1.3461503982543945
I0130 11:19:58.627275 140004624946944 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.304461717605591, loss=1.3182601928710938
I0130 11:20:15.245430 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:20:21.655100 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:20:30.236702 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:20:33.023070 140169137129280 submission_runner.py:408] Time since start: 34369.72s, 	Step: 98651, 	{'train/accuracy': 0.7703284025192261, 'train/loss': 0.8573641180992126, 'validation/accuracy': 0.6903600096702576, 'validation/loss': 1.2503340244293213, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.983777403831482, 'test/num_examples': 10000, 'score': 33187.793897628784, 'total_duration': 34369.720563173294, 'accumulated_submission_time': 33187.793897628784, 'accumulated_eval_time': 1176.1916980743408, 'accumulated_logging_time': 2.540599822998047}
I0130 11:20:33.060135 140004608161536 logging_writer.py:48] [98651] accumulated_eval_time=1176.191698, accumulated_logging_time=2.540600, accumulated_submission_time=33187.793898, global_step=98651, preemption_count=0, score=33187.793898, test/accuracy=0.562300, test/loss=1.983777, test/num_examples=10000, total_duration=34369.720563, train/accuracy=0.770328, train/loss=0.857364, validation/accuracy=0.690360, validation/loss=1.250334, validation/num_examples=50000
I0130 11:20:49.875575 140005305468672 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.3701415061950684, loss=1.2633135318756104
I0130 11:21:23.451584 140004608161536 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.5540690422058105, loss=1.3651223182678223
I0130 11:21:57.066654 140005305468672 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.380852222442627, loss=1.3482674360275269
I0130 11:22:30.684603 140004608161536 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.5457053184509277, loss=1.3441996574401855
I0130 11:23:04.314500 140005305468672 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.3289339542388916, loss=1.3770884275436401
I0130 11:23:37.906875 140004608161536 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.28255558013916, loss=1.395254135131836
I0130 11:24:11.474090 140005305468672 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.8785510063171387, loss=1.3157267570495605
I0130 11:24:45.020724 140004608161536 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.563206672668457, loss=1.3335599899291992
I0130 11:25:18.553970 140005305468672 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.5090036392211914, loss=1.4908775091171265
I0130 11:25:52.234818 140004608161536 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.326113224029541, loss=1.3565187454223633
I0130 11:26:25.859625 140005305468672 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.441927909851074, loss=1.42732834815979
I0130 11:26:59.490449 140004608161536 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.273855209350586, loss=1.2157256603240967
I0130 11:27:33.116203 140005305468672 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.3118960857391357, loss=1.31636381149292
I0130 11:28:06.776957 140004608161536 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.4290385246276855, loss=1.3095749616622925
I0130 11:28:40.389000 140005305468672 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.414585828781128, loss=1.3387384414672852
I0130 11:29:03.059346 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:29:09.465426 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:29:18.039731 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:29:20.760621 140169137129280 submission_runner.py:408] Time since start: 34897.46s, 	Step: 100169, 	{'train/accuracy': 0.7879264950752258, 'train/loss': 0.7837117314338684, 'validation/accuracy': 0.6886000037193298, 'validation/loss': 1.271813988685608, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9868594408035278, 'test/num_examples': 10000, 'score': 33697.73100566864, 'total_duration': 34897.45811223984, 'accumulated_submission_time': 33697.73100566864, 'accumulated_eval_time': 1193.8929476737976, 'accumulated_logging_time': 2.5904340744018555}
I0130 11:29:20.799531 140005288683264 logging_writer.py:48] [100169] accumulated_eval_time=1193.892948, accumulated_logging_time=2.590434, accumulated_submission_time=33697.731006, global_step=100169, preemption_count=0, score=33697.731006, test/accuracy=0.562100, test/loss=1.986859, test/num_examples=10000, total_duration=34897.458112, train/accuracy=0.787926, train/loss=0.783712, validation/accuracy=0.688600, validation/loss=1.271814, validation/num_examples=50000
I0130 11:29:31.538071 140005297075968 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.548342227935791, loss=1.3184459209442139
I0130 11:30:05.039911 140005288683264 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.3888890743255615, loss=1.3854095935821533
I0130 11:30:38.653994 140005297075968 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4837844371795654, loss=1.383470058441162
I0130 11:31:12.277495 140005288683264 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.2770581245422363, loss=1.3374872207641602
I0130 11:31:45.890702 140005297075968 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.6284732818603516, loss=1.2583523988723755
I0130 11:32:19.579962 140005288683264 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.362522602081299, loss=1.2727539539337158
I0130 11:32:53.126561 140005297075968 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.377852439880371, loss=1.279889464378357
I0130 11:33:26.700345 140005288683264 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.510974407196045, loss=1.3472676277160645
I0130 11:34:00.333934 140005297075968 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.468257427215576, loss=1.2838983535766602
I0130 11:34:33.955887 140005288683264 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.544496536254883, loss=1.2274610996246338
I0130 11:35:07.583500 140005297075968 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.490450143814087, loss=1.238417625427246
I0130 11:35:41.207887 140005288683264 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.4150261878967285, loss=1.290138602256775
I0130 11:36:14.846931 140005297075968 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.578259229660034, loss=1.3421318531036377
I0130 11:36:48.472641 140005288683264 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.4919538497924805, loss=1.3584120273590088
I0130 11:37:22.073745 140005297075968 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.6594624519348145, loss=1.3030987977981567
I0130 11:37:50.815353 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:37:57.224798 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:38:06.007551 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:38:08.745379 140169137129280 submission_runner.py:408] Time since start: 35425.44s, 	Step: 101687, 	{'train/accuracy': 0.7952805757522583, 'train/loss': 0.7596470713615417, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.2509621381759644, 'validation/num_examples': 50000, 'test/accuracy': 0.5557000041007996, 'test/loss': 1.9959415197372437, 'test/num_examples': 10000, 'score': 34207.68478536606, 'total_duration': 35425.44287323952, 'accumulated_submission_time': 34207.68478536606, 'accumulated_eval_time': 1211.8229558467865, 'accumulated_logging_time': 2.642286777496338}
I0130 11:38:08.783631 140005305468672 logging_writer.py:48] [101687] accumulated_eval_time=1211.822956, accumulated_logging_time=2.642287, accumulated_submission_time=34207.684785, global_step=101687, preemption_count=0, score=34207.684785, test/accuracy=0.555700, test/loss=1.995942, test/num_examples=10000, total_duration=35425.442873, train/accuracy=0.795281, train/loss=0.759647, validation/accuracy=0.690480, validation/loss=1.250962, validation/num_examples=50000
I0130 11:38:13.510560 140005313861376 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.5491292476654053, loss=1.3836040496826172
I0130 11:38:47.163424 140005305468672 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.624654769897461, loss=1.4128423929214478
I0130 11:39:20.748704 140005313861376 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.6081690788269043, loss=1.4941139221191406
I0130 11:39:54.360451 140005305468672 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.4608099460601807, loss=1.2991552352905273
I0130 11:40:27.954022 140005313861376 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.4112958908081055, loss=1.3640096187591553
I0130 11:41:01.493483 140005305468672 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.56986141204834, loss=1.2942163944244385
I0130 11:41:35.019281 140005313861376 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.470801591873169, loss=1.3965928554534912
I0130 11:42:08.572718 140005305468672 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.5958914756774902, loss=1.2835570573806763
I0130 11:42:42.193971 140005313861376 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.554255723953247, loss=1.377224087715149
I0130 11:43:15.823998 140005305468672 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.425839424133301, loss=1.312664270401001
I0130 11:43:49.395131 140005313861376 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.445676565170288, loss=1.2653753757476807
I0130 11:44:22.921045 140005305468672 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.8537681102752686, loss=1.391819953918457
I0130 11:44:56.450817 140005313861376 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.490349054336548, loss=1.3613072633743286
I0130 11:45:30.153028 140005305468672 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.4135007858276367, loss=1.4256349802017212
I0130 11:46:03.788081 140005313861376 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.514059543609619, loss=1.372229814529419
I0130 11:46:37.423379 140005305468672 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.9887630939483643, loss=1.3606069087982178
I0130 11:46:38.922291 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:46:45.388965 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:46:53.845691 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:46:56.582440 140169137129280 submission_runner.py:408] Time since start: 35953.28s, 	Step: 103206, 	{'train/accuracy': 0.7831433415412903, 'train/loss': 0.8068510293960571, 'validation/accuracy': 0.6899600028991699, 'validation/loss': 1.2710587978363037, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 2.0179336071014404, 'test/num_examples': 10000, 'score': 34717.76279425621, 'total_duration': 35953.27993154526, 'accumulated_submission_time': 34717.76279425621, 'accumulated_eval_time': 1229.4830603599548, 'accumulated_logging_time': 2.692025899887085}
I0130 11:46:56.618925 140004616554240 logging_writer.py:48] [103206] accumulated_eval_time=1229.483060, accumulated_logging_time=2.692026, accumulated_submission_time=34717.762794, global_step=103206, preemption_count=0, score=34717.762794, test/accuracy=0.563900, test/loss=2.017934, test/num_examples=10000, total_duration=35953.279932, train/accuracy=0.783143, train/loss=0.806851, validation/accuracy=0.689960, validation/loss=1.271059, validation/num_examples=50000
I0130 11:47:28.485992 140004624946944 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.389430522918701, loss=1.285001277923584
I0130 11:48:02.079689 140004616554240 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.5253565311431885, loss=1.3565983772277832
I0130 11:48:35.599535 140004624946944 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.536957025527954, loss=1.252393364906311
I0130 11:49:09.133583 140004616554240 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.47918438911438, loss=1.2968730926513672
I0130 11:49:42.679229 140004624946944 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.6500651836395264, loss=1.2360899448394775
I0130 11:50:16.278893 140004616554240 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.532377004623413, loss=1.2774633169174194
I0130 11:50:49.834264 140004624946944 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.585564374923706, loss=1.2818526029586792
I0130 11:51:23.427479 140004616554240 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.7327094078063965, loss=1.3697936534881592
I0130 11:51:57.093445 140004624946944 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.508002519607544, loss=1.3467618227005005
I0130 11:52:30.651269 140004616554240 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.367340564727783, loss=1.1964359283447266
I0130 11:53:04.163968 140004624946944 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.950547695159912, loss=1.379190444946289
I0130 11:53:37.736875 140004616554240 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.6570868492126465, loss=1.3557913303375244
I0130 11:54:11.357623 140004624946944 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.566589593887329, loss=1.2494369745254517
I0130 11:54:44.992625 140004616554240 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.7013230323791504, loss=1.2689449787139893
I0130 11:55:18.609625 140004624946944 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.5984628200531006, loss=1.282282829284668
I0130 11:55:26.819916 140169137129280 spec.py:321] Evaluating on the training split.
I0130 11:55:33.219741 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 11:55:42.071277 140169137129280 spec.py:349] Evaluating on the test split.
I0130 11:55:44.786216 140169137129280 submission_runner.py:408] Time since start: 36481.48s, 	Step: 104726, 	{'train/accuracy': 0.7667809128761292, 'train/loss': 0.8686758279800415, 'validation/accuracy': 0.6767399907112122, 'validation/loss': 1.3189637660980225, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.056422472000122, 'test/num_examples': 10000, 'score': 35227.90310502052, 'total_duration': 36481.48370862007, 'accumulated_submission_time': 35227.90310502052, 'accumulated_eval_time': 1247.4493174552917, 'accumulated_logging_time': 2.7401788234710693}
I0130 11:55:44.826586 140004616554240 logging_writer.py:48] [104726] accumulated_eval_time=1247.449317, accumulated_logging_time=2.740179, accumulated_submission_time=35227.903105, global_step=104726, preemption_count=0, score=35227.903105, test/accuracy=0.552500, test/loss=2.056422, test/num_examples=10000, total_duration=36481.483709, train/accuracy=0.766781, train/loss=0.868676, validation/accuracy=0.676740, validation/loss=1.318964, validation/num_examples=50000
I0130 11:56:09.972143 140005313861376 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.379756212234497, loss=1.310509204864502
I0130 11:56:43.444471 140004616554240 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.4116809368133545, loss=1.2870354652404785
I0130 11:57:17.043735 140005313861376 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.4519758224487305, loss=1.2980726957321167
I0130 11:57:50.646826 140004616554240 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.5945944786071777, loss=1.351002812385559
I0130 11:58:24.316368 140005313861376 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.460869550704956, loss=1.2618664503097534
I0130 11:58:57.844408 140004616554240 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.3218016624450684, loss=1.3201427459716797
I0130 11:59:31.420861 140005313861376 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.8112330436706543, loss=1.4056041240692139
I0130 12:00:05.037981 140004616554240 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.56949782371521, loss=1.313847541809082
I0130 12:00:38.673914 140005313861376 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.5038857460021973, loss=1.2401659488677979
I0130 12:01:12.293900 140004616554240 logging_writer.py:48] [105700] global_step=105700, grad_norm=3.1733601093292236, loss=1.3665368556976318
I0130 12:01:45.909035 140005313861376 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.639650821685791, loss=1.2721806764602661
I0130 12:02:19.531323 140004616554240 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.487670421600342, loss=1.2638370990753174
I0130 12:02:53.171906 140005313861376 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.545757532119751, loss=1.2471975088119507
I0130 12:03:26.800665 140004616554240 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.7588400840759277, loss=1.348474383354187
I0130 12:04:00.433989 140005313861376 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.7558038234710693, loss=1.2656487226486206
I0130 12:04:15.038585 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:04:21.594176 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:04:29.995802 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:04:32.741931 140169137129280 submission_runner.py:408] Time since start: 37009.44s, 	Step: 106245, 	{'train/accuracy': 0.7792769074440002, 'train/loss': 0.8142668008804321, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.2566287517547607, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9923611879348755, 'test/num_examples': 10000, 'score': 35738.055804252625, 'total_duration': 37009.439423561096, 'accumulated_submission_time': 35738.055804252625, 'accumulated_eval_time': 1265.152621269226, 'accumulated_logging_time': 2.7908036708831787}
I0130 12:04:32.780913 140004616554240 logging_writer.py:48] [106245] accumulated_eval_time=1265.152621, accumulated_logging_time=2.790804, accumulated_submission_time=35738.055804, global_step=106245, preemption_count=0, score=35738.055804, test/accuracy=0.567600, test/loss=1.992361, test/num_examples=10000, total_duration=37009.439424, train/accuracy=0.779277, train/loss=0.814267, validation/accuracy=0.694940, validation/loss=1.256629, validation/num_examples=50000
I0130 12:04:51.542830 140004624946944 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.840729236602783, loss=1.3540124893188477
I0130 12:05:25.158722 140004616554240 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.590733528137207, loss=1.3099356889724731
I0130 12:05:58.736727 140004624946944 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.5412180423736572, loss=1.33793044090271
I0130 12:06:32.260033 140004616554240 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.907214641571045, loss=1.3320088386535645
I0130 12:07:05.807208 140004624946944 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.357483386993408, loss=1.1746207475662231
I0130 12:07:39.422418 140004616554240 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.455965995788574, loss=1.2667317390441895
I0130 12:08:13.046551 140004624946944 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.7632763385772705, loss=1.4051783084869385
I0130 12:08:46.667024 140004616554240 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.3221328258514404, loss=1.2164714336395264
I0130 12:09:20.310016 140004624946944 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.435826539993286, loss=1.199426531791687
I0130 12:09:53.935692 140004616554240 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.5588488578796387, loss=1.2819907665252686
I0130 12:10:27.550910 140004624946944 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.7845375537872314, loss=1.3519327640533447
I0130 12:11:01.097656 140004616554240 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.708761692047119, loss=1.3453153371810913
I0130 12:11:34.768539 140004624946944 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.568516969680786, loss=1.2728195190429688
I0130 12:12:08.401162 140004616554240 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.9068105220794678, loss=1.3663148880004883
I0130 12:12:42.002049 140004624946944 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.4647717475891113, loss=1.3425592184066772
I0130 12:13:02.970675 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:13:09.939765 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:13:18.174654 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:13:20.910179 140169137129280 submission_runner.py:408] Time since start: 37537.61s, 	Step: 107764, 	{'train/accuracy': 0.7824258208274841, 'train/loss': 0.7984607219696045, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.2417380809783936, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9687929153442383, 'test/num_examples': 10000, 'score': 36248.185584783554, 'total_duration': 37537.60767388344, 'accumulated_submission_time': 36248.185584783554, 'accumulated_eval_time': 1283.092089176178, 'accumulated_logging_time': 2.8411149978637695}
I0130 12:13:20.948788 140005313861376 logging_writer.py:48] [107764] accumulated_eval_time=1283.092089, accumulated_logging_time=2.841115, accumulated_submission_time=36248.185585, global_step=107764, preemption_count=0, score=36248.185585, test/accuracy=0.566200, test/loss=1.968793, test/num_examples=10000, total_duration=37537.607674, train/accuracy=0.782426, train/loss=0.798461, validation/accuracy=0.694940, validation/loss=1.241738, validation/num_examples=50000
I0130 12:13:33.376274 140005322254080 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.826451301574707, loss=1.3725078105926514
I0130 12:14:06.896638 140005313861376 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.7099671363830566, loss=1.335465908050537
I0130 12:14:40.460689 140005322254080 logging_writer.py:48] [108000] global_step=108000, grad_norm=3.112067699432373, loss=1.3945332765579224
I0130 12:15:14.012598 140005313861376 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.7289419174194336, loss=1.386885643005371
I0130 12:15:47.544353 140005322254080 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.558049440383911, loss=1.3043081760406494
I0130 12:16:21.083973 140005313861376 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.4376332759857178, loss=1.146023154258728
I0130 12:16:54.641635 140005322254080 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.625994920730591, loss=1.3275620937347412
I0130 12:17:28.265394 140005313861376 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.594160795211792, loss=1.2871164083480835
I0130 12:18:01.961612 140005322254080 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.5629119873046875, loss=1.2166857719421387
I0130 12:18:35.579638 140005313861376 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.8259291648864746, loss=1.281278133392334
I0130 12:19:09.195578 140005322254080 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.5240073204040527, loss=1.3259029388427734
I0130 12:19:42.814304 140005313861376 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.8195183277130127, loss=1.4459800720214844
I0130 12:20:16.457897 140005322254080 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.6946492195129395, loss=1.2699099779129028
I0130 12:20:50.086026 140005313861376 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.510101795196533, loss=1.2461501359939575
I0130 12:21:23.713791 140005322254080 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6425418853759766, loss=1.2316430807113647
I0130 12:21:51.117915 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:21:57.486197 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:22:06.102933 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:22:08.794829 140169137129280 submission_runner.py:408] Time since start: 38065.49s, 	Step: 109283, 	{'train/accuracy': 0.8298588991165161, 'train/loss': 0.6174313426017761, 'validation/accuracy': 0.7004799842834473, 'validation/loss': 1.2328135967254639, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9671730995178223, 'test/num_examples': 10000, 'score': 36758.29444336891, 'total_duration': 38065.49232196808, 'accumulated_submission_time': 36758.29444336891, 'accumulated_eval_time': 1300.768966436386, 'accumulated_logging_time': 2.890819549560547}
I0130 12:22:08.834687 140004676327168 logging_writer.py:48] [109283] accumulated_eval_time=1300.768966, accumulated_logging_time=2.890820, accumulated_submission_time=36758.294443, global_step=109283, preemption_count=0, score=36758.294443, test/accuracy=0.570700, test/loss=1.967173, test/num_examples=10000, total_duration=38065.492322, train/accuracy=0.829859, train/loss=0.617431, validation/accuracy=0.700480, validation/loss=1.232814, validation/num_examples=50000
I0130 12:22:14.882948 140005288683264 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.786590337753296, loss=1.3160512447357178
I0130 12:22:48.388528 140004676327168 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.722421169281006, loss=1.1985026597976685
I0130 12:23:21.986812 140005288683264 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.5450704097747803, loss=1.247879147529602
I0130 12:23:55.593829 140004676327168 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.731855630874634, loss=1.2810113430023193
I0130 12:24:29.278982 140005288683264 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.77144455909729, loss=1.3161464929580688
I0130 12:25:02.842608 140004676327168 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.756103038787842, loss=1.2955113649368286
I0130 12:25:36.445747 140005288683264 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.1241979598999023, loss=1.411531925201416
I0130 12:26:10.033096 140004676327168 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.610661506652832, loss=1.4026128053665161
I0130 12:26:43.669533 140005288683264 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.435602903366089, loss=1.2300080060958862
I0130 12:27:17.272962 140004676327168 logging_writer.py:48] [110200] global_step=110200, grad_norm=3.0671746730804443, loss=1.151871681213379
I0130 12:27:50.817759 140005288683264 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.8600997924804688, loss=1.372199535369873
I0130 12:28:24.435521 140004676327168 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.7922332286834717, loss=1.259498119354248
I0130 12:28:57.995670 140005288683264 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.5333340167999268, loss=1.1925368309020996
I0130 12:29:31.630511 140004676327168 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.9390974044799805, loss=1.403875708580017
I0130 12:30:05.264191 140005288683264 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.7909367084503174, loss=1.2242165803909302
I0130 12:30:38.857429 140004676327168 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.45123553276062, loss=1.1228623390197754
I0130 12:30:38.865957 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:30:45.361272 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:30:54.142229 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:30:56.833386 140169137129280 submission_runner.py:408] Time since start: 38593.53s, 	Step: 110801, 	{'train/accuracy': 0.8005221486091614, 'train/loss': 0.7223421931266785, 'validation/accuracy': 0.695580005645752, 'validation/loss': 1.2395744323730469, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.978838562965393, 'test/num_examples': 10000, 'score': 37268.264142751694, 'total_duration': 38593.53087067604, 'accumulated_submission_time': 37268.264142751694, 'accumulated_eval_time': 1318.7363233566284, 'accumulated_logging_time': 2.9423089027404785}
I0130 12:30:56.871063 140005313861376 logging_writer.py:48] [110801] accumulated_eval_time=1318.736323, accumulated_logging_time=2.942309, accumulated_submission_time=37268.264143, global_step=110801, preemption_count=0, score=37268.264143, test/accuracy=0.566300, test/loss=1.978839, test/num_examples=10000, total_duration=38593.530871, train/accuracy=0.800522, train/loss=0.722342, validation/accuracy=0.695580, validation/loss=1.239574, validation/num_examples=50000
I0130 12:31:30.502604 140005322254080 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.833211660385132, loss=1.349357008934021
I0130 12:32:04.045236 140005313861376 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.7618987560272217, loss=1.2644813060760498
I0130 12:32:37.662768 140005322254080 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.630056619644165, loss=1.2429238557815552
I0130 12:33:11.189966 140005313861376 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.7190818786621094, loss=1.2272918224334717
I0130 12:33:44.768914 140005322254080 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.537578821182251, loss=1.2414531707763672
I0130 12:34:18.336534 140005313861376 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.735088348388672, loss=1.388667345046997
I0130 12:34:51.861541 140005322254080 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7089788913726807, loss=1.3063373565673828
I0130 12:35:25.402062 140005313861376 logging_writer.py:48] [111600] global_step=111600, grad_norm=3.108224868774414, loss=1.2861814498901367
I0130 12:35:58.965900 140005322254080 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.762749195098877, loss=1.250319242477417
I0130 12:36:32.596983 140005313861376 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.717715263366699, loss=1.3297934532165527
I0130 12:37:06.202939 140005322254080 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.539016008377075, loss=1.176445484161377
I0130 12:37:39.921103 140005313861376 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.6231415271759033, loss=1.2391412258148193
I0130 12:38:13.543879 140005322254080 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.9416706562042236, loss=1.3359878063201904
I0130 12:38:47.193341 140005313861376 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.6422102451324463, loss=1.3454017639160156
I0130 12:39:20.820324 140005322254080 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.705155849456787, loss=1.2277123928070068
I0130 12:39:27.025158 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:39:33.429979 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:39:42.196312 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:39:44.890088 140169137129280 submission_runner.py:408] Time since start: 39121.59s, 	Step: 112320, 	{'train/accuracy': 0.800223171710968, 'train/loss': 0.7208084464073181, 'validation/accuracy': 0.6976799964904785, 'validation/loss': 1.2322026491165161, 'validation/num_examples': 50000, 'test/accuracy': 0.5700000524520874, 'test/loss': 1.9467543363571167, 'test/num_examples': 10000, 'score': 37778.35658097267, 'total_duration': 39121.58758187294, 'accumulated_submission_time': 37778.35658097267, 'accumulated_eval_time': 1336.601214170456, 'accumulated_logging_time': 2.9928505420684814}
I0130 12:39:44.927457 140004676327168 logging_writer.py:48] [112320] accumulated_eval_time=1336.601214, accumulated_logging_time=2.992851, accumulated_submission_time=37778.356581, global_step=112320, preemption_count=0, score=37778.356581, test/accuracy=0.570000, test/loss=1.946754, test/num_examples=10000, total_duration=39121.587582, train/accuracy=0.800223, train/loss=0.720808, validation/accuracy=0.697680, validation/loss=1.232203, validation/num_examples=50000
I0130 12:40:12.098177 140005288683264 logging_writer.py:48] [112400] global_step=112400, grad_norm=3.2405765056610107, loss=1.3579329252243042
I0130 12:40:45.621532 140004676327168 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.625859260559082, loss=1.2495653629302979
I0130 12:41:19.192879 140005288683264 logging_writer.py:48] [112600] global_step=112600, grad_norm=3.0002338886260986, loss=1.3219547271728516
I0130 12:41:52.810781 140004676327168 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.8860018253326416, loss=1.3782533407211304
I0130 12:42:26.440119 140005288683264 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.882265329360962, loss=1.2659449577331543
I0130 12:43:00.064490 140004676327168 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.993211507797241, loss=1.3234376907348633
I0130 12:43:33.699996 140005288683264 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.9888899326324463, loss=1.3494843244552612
I0130 12:44:07.436198 140004676327168 logging_writer.py:48] [113100] global_step=113100, grad_norm=3.11637806892395, loss=1.2712043523788452
I0130 12:44:40.945872 140005288683264 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.7447190284729004, loss=1.2099518775939941
I0130 12:45:14.517143 140004676327168 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.7103023529052734, loss=1.2876996994018555
I0130 12:45:48.118600 140005288683264 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.9481894969940186, loss=1.3269290924072266
I0130 12:46:21.707525 140004676327168 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.6993234157562256, loss=1.2596158981323242
I0130 12:46:55.300433 140005288683264 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.8359248638153076, loss=1.2973415851593018
I0130 12:47:28.928472 140004676327168 logging_writer.py:48] [113700] global_step=113700, grad_norm=3.0775859355926514, loss=1.288589358329773
I0130 12:48:02.556020 140005288683264 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.954183340072632, loss=1.1868596076965332
I0130 12:48:15.148434 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:48:21.601130 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:48:30.282026 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:48:33.000596 140169137129280 submission_runner.py:408] Time since start: 39649.70s, 	Step: 113839, 	{'train/accuracy': 0.8005420565605164, 'train/loss': 0.7312487959861755, 'validation/accuracy': 0.7041599750518799, 'validation/loss': 1.2149962186813354, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.942642331123352, 'test/num_examples': 10000, 'score': 38288.51729607582, 'total_duration': 39649.69808101654, 'accumulated_submission_time': 38288.51729607582, 'accumulated_eval_time': 1354.4533264636993, 'accumulated_logging_time': 3.0412895679473877}
I0130 12:48:33.036273 140004676327168 logging_writer.py:48] [113839] accumulated_eval_time=1354.453326, accumulated_logging_time=3.041290, accumulated_submission_time=38288.517296, global_step=113839, preemption_count=0, score=38288.517296, test/accuracy=0.577000, test/loss=1.942642, test/num_examples=10000, total_duration=39649.698081, train/accuracy=0.800542, train/loss=0.731249, validation/accuracy=0.704160, validation/loss=1.214996, validation/num_examples=50000
I0130 12:48:53.819609 140005288683264 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.878480911254883, loss=1.2993714809417725
I0130 12:49:27.388596 140004676327168 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.87406325340271, loss=1.257871150970459
I0130 12:50:01.022301 140005288683264 logging_writer.py:48] [114100] global_step=114100, grad_norm=3.1421494483947754, loss=1.2825913429260254
I0130 12:50:34.650763 140004676327168 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.806051254272461, loss=1.2433208227157593
I0130 12:51:08.288460 140005288683264 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.837965250015259, loss=1.204116940498352
I0130 12:51:41.898720 140004676327168 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.7560389041900635, loss=1.2994096279144287
I0130 12:52:15.515376 140005288683264 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.9547760486602783, loss=1.263076901435852
I0130 12:52:49.145772 140004676327168 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.7031466960906982, loss=1.3016290664672852
I0130 12:53:22.706557 140005288683264 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.933220624923706, loss=1.216737985610962
I0130 12:53:56.262967 140004676327168 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.88063645362854, loss=1.2502453327178955
I0130 12:54:29.885558 140005288683264 logging_writer.py:48] [114900] global_step=114900, grad_norm=3.1964168548583984, loss=1.2783421277999878
I0130 12:55:03.509812 140004676327168 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.562260627746582, loss=1.1155132055282593
I0130 12:55:37.157690 140005288683264 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.815190076828003, loss=1.320556402206421
I0130 12:56:10.771150 140004676327168 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.8502085208892822, loss=1.2771117687225342
I0130 12:56:44.403926 140005288683264 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.652719259262085, loss=1.1567655801773071
I0130 12:57:03.044554 140169137129280 spec.py:321] Evaluating on the training split.
I0130 12:57:09.739778 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 12:57:18.511745 140169137129280 spec.py:349] Evaluating on the test split.
I0130 12:57:21.189491 140169137129280 submission_runner.py:408] Time since start: 40177.89s, 	Step: 115357, 	{'train/accuracy': 0.7947823405265808, 'train/loss': 0.752173125743866, 'validation/accuracy': 0.6983799934387207, 'validation/loss': 1.2410300970077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9666895866394043, 'test/num_examples': 10000, 'score': 38798.464625597, 'total_duration': 40177.886984825134, 'accumulated_submission_time': 38798.464625597, 'accumulated_eval_time': 1372.5982236862183, 'accumulated_logging_time': 3.089034080505371}
I0130 12:57:21.229516 140005313861376 logging_writer.py:48] [115357] accumulated_eval_time=1372.598224, accumulated_logging_time=3.089034, accumulated_submission_time=38798.464626, global_step=115357, preemption_count=0, score=38798.464626, test/accuracy=0.570700, test/loss=1.966690, test/num_examples=10000, total_duration=40177.886985, train/accuracy=0.794782, train/loss=0.752173, validation/accuracy=0.698380, validation/loss=1.241030, validation/num_examples=50000
I0130 12:57:36.002708 140005322254080 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.851419448852539, loss=1.2544457912445068
I0130 12:58:09.605970 140005313861376 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.7599740028381348, loss=1.1436662673950195
I0130 12:58:43.216902 140005322254080 logging_writer.py:48] [115600] global_step=115600, grad_norm=3.099266529083252, loss=1.2790039777755737
I0130 12:59:16.850271 140005313861376 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.816486358642578, loss=1.2002379894256592
I0130 12:59:50.449152 140005322254080 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.218374013900757, loss=1.1541295051574707
I0130 13:00:24.074579 140005313861376 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.944387197494507, loss=1.2714762687683105
I0130 13:00:57.682788 140005322254080 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.795426368713379, loss=1.1429212093353271
I0130 13:01:31.312621 140005313861376 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.8038341999053955, loss=1.134408950805664
I0130 13:02:04.949319 140005322254080 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.784651756286621, loss=1.163848638534546
I0130 13:02:38.575877 140005313861376 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.988266706466675, loss=1.1608690023422241
I0130 13:03:12.174384 140005322254080 logging_writer.py:48] [116400] global_step=116400, grad_norm=3.1126883029937744, loss=1.3054397106170654
I0130 13:03:45.893852 140005313861376 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.7138559818267822, loss=1.1771379709243774
I0130 13:04:19.468518 140005322254080 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.8228085041046143, loss=1.2347825765609741
I0130 13:04:53.084079 140005313861376 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.797755479812622, loss=1.218335747718811
I0130 13:05:26.705582 140005322254080 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.9639930725097656, loss=1.2028851509094238
I0130 13:05:51.410413 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:05:57.934297 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:06:06.540954 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:06:09.247248 140169137129280 submission_runner.py:408] Time since start: 40705.94s, 	Step: 116875, 	{'train/accuracy': 0.7978315949440002, 'train/loss': 0.7409703135490417, 'validation/accuracy': 0.7010599970817566, 'validation/loss': 1.2206995487213135, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 1.9554057121276855, 'test/num_examples': 10000, 'score': 39308.584755182266, 'total_duration': 40705.944717884064, 'accumulated_submission_time': 39308.584755182266, 'accumulated_eval_time': 1390.4349954128265, 'accumulated_logging_time': 3.141059637069702}
I0130 13:06:09.289096 140005297075968 logging_writer.py:48] [116875] accumulated_eval_time=1390.434995, accumulated_logging_time=3.141060, accumulated_submission_time=39308.584755, global_step=116875, preemption_count=0, score=39308.584755, test/accuracy=0.569800, test/loss=1.955406, test/num_examples=10000, total_duration=40705.944718, train/accuracy=0.797832, train/loss=0.740970, validation/accuracy=0.701060, validation/loss=1.220700, validation/num_examples=50000
I0130 13:06:18.015974 140005305468672 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.7320969104766846, loss=1.2371962070465088
I0130 13:06:51.532350 140005297075968 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9216294288635254, loss=1.1757208108901978
I0130 13:07:25.036023 140005305468672 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.866952419281006, loss=1.1851147413253784
I0130 13:07:58.646450 140005297075968 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.6750991344451904, loss=1.22687566280365
I0130 13:08:32.242745 140005305468672 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.7803354263305664, loss=1.2168830633163452
I0130 13:09:05.847972 140005297075968 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.209993839263916, loss=1.2448091506958008
I0130 13:09:39.473504 140005305468672 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.8229384422302246, loss=1.1820040941238403
I0130 13:10:13.104587 140005297075968 logging_writer.py:48] [117600] global_step=117600, grad_norm=3.0314338207244873, loss=1.2857595682144165
I0130 13:10:46.823258 140005305468672 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.8549110889434814, loss=1.216785192489624
I0130 13:11:20.440508 140005297075968 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.941790819168091, loss=1.2327630519866943
I0130 13:11:54.058606 140005305468672 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.925457239151001, loss=1.2024204730987549
I0130 13:12:27.673785 140005297075968 logging_writer.py:48] [118000] global_step=118000, grad_norm=3.349400758743286, loss=1.244994878768921
I0130 13:13:01.284835 140005305468672 logging_writer.py:48] [118100] global_step=118100, grad_norm=3.1286957263946533, loss=1.2319742441177368
I0130 13:13:34.899214 140005297075968 logging_writer.py:48] [118200] global_step=118200, grad_norm=3.0646743774414062, loss=1.2733274698257446
I0130 13:14:08.529193 140005305468672 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.909639596939087, loss=1.2086565494537354
I0130 13:14:39.282216 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:14:45.709454 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:14:54.439242 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:14:57.269442 140169137129280 submission_runner.py:408] Time since start: 41233.97s, 	Step: 118393, 	{'train/accuracy': 0.8346220850944519, 'train/loss': 0.5950406193733215, 'validation/accuracy': 0.7032999992370605, 'validation/loss': 1.2153681516647339, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9618397951126099, 'test/num_examples': 10000, 'score': 39818.51747059822, 'total_duration': 41233.966715574265, 'accumulated_submission_time': 39818.51747059822, 'accumulated_eval_time': 1408.42196559906, 'accumulated_logging_time': 3.193824052810669}
I0130 13:14:57.310688 140004676327168 logging_writer.py:48] [118393] accumulated_eval_time=1408.421966, accumulated_logging_time=3.193824, accumulated_submission_time=39818.517471, global_step=118393, preemption_count=0, score=39818.517471, test/accuracy=0.574900, test/loss=1.961840, test/num_examples=10000, total_duration=41233.966716, train/accuracy=0.834622, train/loss=0.595041, validation/accuracy=0.703300, validation/loss=1.215368, validation/num_examples=50000
I0130 13:15:00.003514 140005288683264 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.6599245071411133, loss=1.098771333694458
I0130 13:15:33.576492 140004676327168 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.997797727584839, loss=1.2064521312713623
I0130 13:16:07.199230 140005288683264 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.1309471130371094, loss=1.1492247581481934
I0130 13:16:40.828742 140004676327168 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.5838992595672607, loss=1.0366172790527344
I0130 13:17:14.550814 140005288683264 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.6953177452087402, loss=1.170045018196106
I0130 13:17:48.117480 140004676327168 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.737250328063965, loss=1.2086153030395508
I0130 13:18:21.628122 140005288683264 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.8323519229888916, loss=1.1004836559295654
I0130 13:18:55.166404 140004676327168 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.0205039978027344, loss=1.2327585220336914
I0130 13:19:28.768784 140005288683264 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.912675142288208, loss=1.2606791257858276
I0130 13:20:02.354541 140004676327168 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.0495686531066895, loss=1.2097268104553223
I0130 13:20:35.911294 140005288683264 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.7633631229400635, loss=1.2117877006530762
I0130 13:21:09.532534 140004676327168 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.7554988861083984, loss=1.118445873260498
I0130 13:21:43.147260 140005288683264 logging_writer.py:48] [119600] global_step=119600, grad_norm=3.21492600440979, loss=1.3289676904678345
I0130 13:22:16.761384 140004676327168 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.896557331085205, loss=1.2015864849090576
I0130 13:22:50.380974 140005288683264 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.9694595336914062, loss=1.2228959798812866
I0130 13:23:24.076902 140004676327168 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.8919243812561035, loss=1.198822021484375
I0130 13:23:27.586036 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:23:34.001735 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:23:42.566846 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:23:45.314852 140169137129280 submission_runner.py:408] Time since start: 41762.01s, 	Step: 119912, 	{'train/accuracy': 0.8252750039100647, 'train/loss': 0.6352033615112305, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.1854684352874756, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8898450136184692, 'test/num_examples': 10000, 'score': 40328.73139810562, 'total_duration': 41762.01231408119, 'accumulated_submission_time': 40328.73139810562, 'accumulated_eval_time': 1426.150707244873, 'accumulated_logging_time': 3.247591495513916}
I0130 13:23:45.356157 140004659541760 logging_writer.py:48] [119912] accumulated_eval_time=1426.150707, accumulated_logging_time=3.247591, accumulated_submission_time=40328.731398, global_step=119912, preemption_count=0, score=40328.731398, test/accuracy=0.581900, test/loss=1.889845, test/num_examples=10000, total_duration=41762.012314, train/accuracy=0.825275, train/loss=0.635203, validation/accuracy=0.706880, validation/loss=1.185468, validation/num_examples=50000
I0130 13:24:15.228415 140004667934464 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.0414161682128906, loss=1.240033745765686
I0130 13:24:48.830424 140004659541760 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.961799144744873, loss=1.2269248962402344
I0130 13:25:22.481961 140004667934464 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.158320665359497, loss=1.1833415031433105
I0130 13:25:56.090371 140004659541760 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.016270637512207, loss=1.170102834701538
I0130 13:26:29.678059 140004667934464 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.2329678535461426, loss=1.1742541790008545
I0130 13:27:03.307334 140004659541760 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.814059019088745, loss=1.1528371572494507
I0130 13:27:36.919100 140004667934464 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.985898494720459, loss=1.2213034629821777
I0130 13:28:10.554868 140004659541760 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.7429327964782715, loss=1.188279628753662
I0130 13:28:44.159259 140004667934464 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.137523651123047, loss=1.1890974044799805
I0130 13:29:17.790149 140004659541760 logging_writer.py:48] [120900] global_step=120900, grad_norm=3.2418081760406494, loss=1.2589538097381592
I0130 13:29:51.415776 140004667934464 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.234510660171509, loss=1.189910650253296
I0130 13:30:25.076255 140004659541760 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.880526065826416, loss=1.1821221113204956
I0130 13:30:58.648473 140004667934464 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.973759651184082, loss=1.1926119327545166
I0130 13:31:32.200601 140004659541760 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.1307144165039062, loss=1.1792941093444824
I0130 13:32:05.755469 140004667934464 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.231687307357788, loss=1.209507942199707
I0130 13:32:15.637324 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:32:22.046745 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:32:30.633397 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:32:33.355556 140169137129280 submission_runner.py:408] Time since start: 42290.05s, 	Step: 121431, 	{'train/accuracy': 0.8082947731018066, 'train/loss': 0.6803027391433716, 'validation/accuracy': 0.7044599652290344, 'validation/loss': 1.2126598358154297, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.9775440692901611, 'test/num_examples': 10000, 'score': 40838.95229148865, 'total_duration': 42290.05305337906, 'accumulated_submission_time': 40838.95229148865, 'accumulated_eval_time': 1443.8689014911652, 'accumulated_logging_time': 3.300241231918335}
I0130 13:32:33.394573 140005322254080 logging_writer.py:48] [121431] accumulated_eval_time=1443.868901, accumulated_logging_time=3.300241, accumulated_submission_time=40838.952291, global_step=121431, preemption_count=0, score=40838.952291, test/accuracy=0.574200, test/loss=1.977544, test/num_examples=10000, total_duration=42290.053053, train/accuracy=0.808295, train/loss=0.680303, validation/accuracy=0.704460, validation/loss=1.212660, validation/num_examples=50000
I0130 13:32:56.876304 140005330646784 logging_writer.py:48] [121500] global_step=121500, grad_norm=3.297024965286255, loss=1.2533425092697144
I0130 13:33:30.436556 140005322254080 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.8350260257720947, loss=1.15876305103302
I0130 13:34:04.070251 140005330646784 logging_writer.py:48] [121700] global_step=121700, grad_norm=3.073490858078003, loss=1.1669012308120728
I0130 13:34:37.718369 140005322254080 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.0734899044036865, loss=1.27347993850708
I0130 13:35:11.340961 140005330646784 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.871727228164673, loss=1.1903679370880127
I0130 13:35:44.963074 140005322254080 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.072434186935425, loss=1.148975133895874
I0130 13:36:18.602643 140005330646784 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.0978620052337646, loss=1.1466747522354126
I0130 13:36:52.271856 140005322254080 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0039560794830322, loss=1.1825032234191895
I0130 13:37:25.855828 140005330646784 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.021267890930176, loss=1.2349199056625366
I0130 13:37:59.420202 140005322254080 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.873203754425049, loss=1.1101219654083252
I0130 13:38:32.950669 140005330646784 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.4523160457611084, loss=1.2332476377487183
I0130 13:39:06.483992 140005322254080 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.861753463745117, loss=1.1504993438720703
I0130 13:39:40.050554 140005330646784 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.2941839694976807, loss=1.2586157321929932
I0130 13:40:13.671217 140005322254080 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.968421220779419, loss=1.133312702178955
I0130 13:40:47.292474 140005330646784 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.3313145637512207, loss=1.163672924041748
I0130 13:41:03.548211 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:41:09.947316 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:41:18.774027 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:41:21.486430 140169137129280 submission_runner.py:408] Time since start: 42818.18s, 	Step: 122950, 	{'train/accuracy': 0.8224449753761292, 'train/loss': 0.6331303119659424, 'validation/accuracy': 0.7143599987030029, 'validation/loss': 1.1689563989639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5859000086784363, 'test/loss': 1.9121652841567993, 'test/num_examples': 10000, 'score': 41349.047131061554, 'total_duration': 42818.18391633034, 'accumulated_submission_time': 41349.047131061554, 'accumulated_eval_time': 1461.8070714473724, 'accumulated_logging_time': 3.3489990234375}
I0130 13:41:21.525831 140004676327168 logging_writer.py:48] [122950] accumulated_eval_time=1461.807071, accumulated_logging_time=3.348999, accumulated_submission_time=41349.047131, global_step=122950, preemption_count=0, score=41349.047131, test/accuracy=0.585900, test/loss=1.912165, test/num_examples=10000, total_duration=42818.183916, train/accuracy=0.822445, train/loss=0.633130, validation/accuracy=0.714360, validation/loss=1.168956, validation/num_examples=50000
I0130 13:41:38.642277 140005288683264 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.1161415576934814, loss=1.275469422340393
I0130 13:42:12.127635 140004676327168 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.0235443115234375, loss=1.0783201456069946
I0130 13:42:45.698274 140005288683264 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.848422050476074, loss=1.05025053024292
I0130 13:43:19.370841 140004676327168 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.3084053993225098, loss=1.1106435060501099
I0130 13:43:52.947757 140005288683264 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.4739272594451904, loss=1.1511895656585693
I0130 13:44:26.554354 140004676327168 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.9141640663146973, loss=1.19230318069458
I0130 13:45:00.181870 140005288683264 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.0266520977020264, loss=1.1336324214935303
I0130 13:45:33.814985 140004676327168 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.275312662124634, loss=1.1778792142868042
I0130 13:46:07.440875 140005288683264 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.0066487789154053, loss=1.201759696006775
I0130 13:46:41.052676 140004676327168 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.0002264976501465, loss=1.1566706895828247
I0130 13:47:14.662475 140005288683264 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.958507537841797, loss=1.1767561435699463
I0130 13:47:48.266095 140004676327168 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.9131598472595215, loss=1.1707687377929688
I0130 13:48:21.796676 140005288683264 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.1606805324554443, loss=1.1739293336868286
I0130 13:48:55.327036 140004676327168 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.952101469039917, loss=1.1744441986083984
I0130 13:49:28.874644 140005288683264 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.0805180072784424, loss=1.1686519384384155
I0130 13:49:51.586487 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:49:57.981957 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:50:06.451224 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:50:09.123211 140169137129280 submission_runner.py:408] Time since start: 43345.82s, 	Step: 124469, 	{'train/accuracy': 0.8160474896430969, 'train/loss': 0.6538271903991699, 'validation/accuracy': 0.7134999632835388, 'validation/loss': 1.174975872039795, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.8961846828460693, 'test/num_examples': 10000, 'score': 41859.047733068466, 'total_duration': 43345.82070159912, 'accumulated_submission_time': 41859.047733068466, 'accumulated_eval_time': 1479.3437526226044, 'accumulated_logging_time': 3.399578094482422}
I0130 13:50:09.171966 140004676327168 logging_writer.py:48] [124469] accumulated_eval_time=1479.343753, accumulated_logging_time=3.399578, accumulated_submission_time=41859.047733, global_step=124469, preemption_count=0, score=41859.047733, test/accuracy=0.586600, test/loss=1.896185, test/num_examples=10000, total_duration=43345.820702, train/accuracy=0.816047, train/loss=0.653827, validation/accuracy=0.713500, validation/loss=1.174976, validation/num_examples=50000
I0130 13:50:19.923776 140005288683264 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.1099376678466797, loss=1.209031581878662
I0130 13:50:53.494544 140004676327168 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.097221851348877, loss=1.0723702907562256
I0130 13:51:27.007348 140005288683264 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.4704415798187256, loss=1.260202407836914
I0130 13:52:00.510977 140004676327168 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.0962131023406982, loss=1.2168413400650024
I0130 13:52:34.109735 140005288683264 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.3669943809509277, loss=1.1568603515625
I0130 13:53:07.743242 140004676327168 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.7023468017578125, loss=1.0973191261291504
I0130 13:53:41.374989 140005288683264 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.2981979846954346, loss=1.2101330757141113
I0130 13:54:14.997407 140004676327168 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.150796890258789, loss=1.2026349306106567
I0130 13:54:48.629368 140005288683264 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.24481463432312, loss=1.1933790445327759
I0130 13:55:22.253026 140004676327168 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.041614532470703, loss=1.1327314376831055
I0130 13:55:55.882946 140005288683264 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.361132860183716, loss=1.2941372394561768
I0130 13:56:29.559418 140004676327168 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.31815505027771, loss=1.1631076335906982
I0130 13:57:03.100955 140005288683264 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.171206474304199, loss=1.1594643592834473
I0130 13:57:36.641957 140004676327168 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.03139591217041, loss=1.1420512199401855
I0130 13:58:10.190457 140005288683264 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.35207200050354, loss=1.2090823650360107
I0130 13:58:39.149246 140169137129280 spec.py:321] Evaluating on the training split.
I0130 13:58:45.550271 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 13:58:54.313973 140169137129280 spec.py:349] Evaluating on the test split.
I0130 13:58:57.012005 140169137129280 submission_runner.py:408] Time since start: 43873.71s, 	Step: 125988, 	{'train/accuracy': 0.8224050998687744, 'train/loss': 0.6366180181503296, 'validation/accuracy': 0.7099800109863281, 'validation/loss': 1.189315915107727, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9083479642868042, 'test/num_examples': 10000, 'score': 42368.96591067314, 'total_duration': 43873.70950007439, 'accumulated_submission_time': 42368.96591067314, 'accumulated_eval_time': 1497.206482887268, 'accumulated_logging_time': 3.4583938121795654}
I0130 13:58:57.051309 140004676327168 logging_writer.py:48] [125988] accumulated_eval_time=1497.206483, accumulated_logging_time=3.458394, accumulated_submission_time=42368.965911, global_step=125988, preemption_count=0, score=42368.965911, test/accuracy=0.586800, test/loss=1.908348, test/num_examples=10000, total_duration=43873.709500, train/accuracy=0.822405, train/loss=0.636618, validation/accuracy=0.709980, validation/loss=1.189316, validation/num_examples=50000
I0130 13:59:01.434459 140005288683264 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.9857592582702637, loss=1.110830545425415
I0130 13:59:35.034942 140004676327168 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.9426212310791016, loss=1.0983402729034424
I0130 14:00:08.668832 140005288683264 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.262341022491455, loss=1.2291752099990845
I0130 14:00:42.286032 140004676327168 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.078442096710205, loss=1.1094567775726318
I0130 14:01:15.913051 140005288683264 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.149293899536133, loss=1.044540286064148
I0130 14:01:49.546647 140004676327168 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.0852890014648438, loss=1.1054421663284302
I0130 14:02:23.169092 140005288683264 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.346531391143799, loss=1.1611336469650269
I0130 14:02:56.828714 140004676327168 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.026526689529419, loss=1.0914764404296875
I0130 14:03:30.397053 140005288683264 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.2601280212402344, loss=1.0936871767044067
I0130 14:04:03.998962 140004676327168 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.4185800552368164, loss=1.2230119705200195
I0130 14:04:37.599811 140005288683264 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.3959970474243164, loss=1.1476573944091797
I0130 14:05:11.218708 140004676327168 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.280088424682617, loss=1.0887703895568848
I0130 14:05:44.798715 140005288683264 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.3666207790374756, loss=1.1409353017807007
I0130 14:06:18.436466 140004676327168 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.14296293258667, loss=1.148605465888977
I0130 14:06:52.047099 140005288683264 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.3534862995147705, loss=1.1101343631744385
I0130 14:07:25.660119 140004676327168 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.239630937576294, loss=1.0513460636138916
I0130 14:07:27.152970 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:07:33.742756 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:07:42.104254 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:07:44.818120 140169137129280 submission_runner.py:408] Time since start: 44401.52s, 	Step: 127506, 	{'train/accuracy': 0.8494299650192261, 'train/loss': 0.529978334903717, 'validation/accuracy': 0.7148199677467346, 'validation/loss': 1.1761623620986938, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.8952957391738892, 'test/num_examples': 10000, 'score': 42879.00650596619, 'total_duration': 44401.515615940094, 'accumulated_submission_time': 42879.00650596619, 'accumulated_eval_time': 1514.8715977668762, 'accumulated_logging_time': 3.5093042850494385}
I0130 14:07:44.859645 140004676327168 logging_writer.py:48] [127506] accumulated_eval_time=1514.871598, accumulated_logging_time=3.509304, accumulated_submission_time=42879.006506, global_step=127506, preemption_count=0, score=42879.006506, test/accuracy=0.589800, test/loss=1.895296, test/num_examples=10000, total_duration=44401.515616, train/accuracy=0.849430, train/loss=0.529978, validation/accuracy=0.714820, validation/loss=1.176162, validation/num_examples=50000
I0130 14:08:16.692915 140005313861376 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.087158441543579, loss=1.0751110315322876
I0130 14:08:50.258796 140004676327168 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.423144817352295, loss=1.1535706520080566
I0130 14:09:23.908865 140005313861376 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.39898681640625, loss=1.196282148361206
I0130 14:09:57.466701 140004676327168 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.26396107673645, loss=1.1217442750930786
I0130 14:10:31.093792 140005313861376 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.294987916946411, loss=1.1046850681304932
I0130 14:11:04.687757 140004676327168 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.152435302734375, loss=1.096482753753662
I0130 14:11:38.293690 140005313861376 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.4295284748077393, loss=1.146824836730957
I0130 14:12:11.906420 140004676327168 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.5289392471313477, loss=1.1404348611831665
I0130 14:12:45.470334 140005313861376 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.0809154510498047, loss=1.1144554615020752
I0130 14:13:18.996580 140004676327168 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.209993839263916, loss=1.022217869758606
I0130 14:13:52.534328 140005313861376 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.051189422607422, loss=1.131150484085083
I0130 14:14:26.153079 140004676327168 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.4589407444000244, loss=1.113563060760498
I0130 14:14:59.778622 140005313861376 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1263139247894287, loss=1.118323564529419
I0130 14:15:33.407488 140004676327168 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1771790981292725, loss=1.1708985567092896
I0130 14:16:07.064643 140005313861376 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.1499509811401367, loss=1.0577199459075928
I0130 14:16:14.934907 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:16:21.320503 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:16:29.992059 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:16:32.730997 140169137129280 submission_runner.py:408] Time since start: 44929.43s, 	Step: 129025, 	{'train/accuracy': 0.8404814600944519, 'train/loss': 0.564034104347229, 'validation/accuracy': 0.7139599919319153, 'validation/loss': 1.1836059093475342, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.9311414957046509, 'test/num_examples': 10000, 'score': 43389.02200245857, 'total_duration': 44929.42847776413, 'accumulated_submission_time': 43389.02200245857, 'accumulated_eval_time': 1532.6676306724548, 'accumulated_logging_time': 3.56146502494812}
I0130 14:16:32.772236 140005305468672 logging_writer.py:48] [129025] accumulated_eval_time=1532.667631, accumulated_logging_time=3.561465, accumulated_submission_time=43389.022002, global_step=129025, preemption_count=0, score=43389.022002, test/accuracy=0.589300, test/loss=1.931141, test/num_examples=10000, total_duration=44929.428478, train/accuracy=0.840481, train/loss=0.564034, validation/accuracy=0.713960, validation/loss=1.183606, validation/num_examples=50000
I0130 14:16:58.250835 140005330646784 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.2176318168640137, loss=1.0640814304351807
I0130 14:17:31.765336 140005305468672 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.6992857456207275, loss=1.1019017696380615
I0130 14:18:05.291674 140005330646784 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.3925318717956543, loss=1.2035675048828125
I0130 14:18:38.844131 140005305468672 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.293461561203003, loss=1.037671685218811
I0130 14:19:12.447748 140005330646784 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.367825984954834, loss=1.073233962059021
I0130 14:19:46.076856 140005305468672 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.2716572284698486, loss=1.0105881690979004
I0130 14:20:19.659390 140005330646784 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.191789388656616, loss=1.102954626083374
I0130 14:20:53.291902 140005305468672 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.279571771621704, loss=1.144120693206787
I0130 14:21:26.914350 140005330646784 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.345004081726074, loss=1.109938144683838
I0130 14:22:00.559841 140005305468672 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.4761126041412354, loss=1.0616271495819092
I0130 14:22:34.211830 140005330646784 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.0834462642669678, loss=0.992548942565918
I0130 14:23:07.768491 140005305468672 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.3431003093719482, loss=1.11737060546875
I0130 14:23:41.378380 140005330646784 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.4542040824890137, loss=1.14351487159729
I0130 14:24:15.007977 140005305468672 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.4589405059814453, loss=1.063234567642212
I0130 14:24:48.600629 140005330646784 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.0938830375671387, loss=1.1330186128616333
I0130 14:25:02.889476 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:25:09.298221 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:25:17.747213 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:25:20.478008 140169137129280 submission_runner.py:408] Time since start: 45457.18s, 	Step: 130544, 	{'train/accuracy': 0.8401227593421936, 'train/loss': 0.5696372985839844, 'validation/accuracy': 0.7194199562072754, 'validation/loss': 1.1422617435455322, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.8697551488876343, 'test/num_examples': 10000, 'score': 43899.07996249199, 'total_duration': 45457.17548465729, 'accumulated_submission_time': 43899.07996249199, 'accumulated_eval_time': 1550.256118774414, 'accumulated_logging_time': 3.6132612228393555}
I0130 14:25:20.533715 140004667934464 logging_writer.py:48] [130544] accumulated_eval_time=1550.256119, accumulated_logging_time=3.613261, accumulated_submission_time=43899.079962, global_step=130544, preemption_count=0, score=43899.079962, test/accuracy=0.592300, test/loss=1.869755, test/num_examples=10000, total_duration=45457.175485, train/accuracy=0.840123, train/loss=0.569637, validation/accuracy=0.719420, validation/loss=1.142262, validation/num_examples=50000
I0130 14:25:39.660743 140004676327168 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.786865472793579, loss=1.0485023260116577
I0130 14:26:13.193722 140004667934464 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.1181960105895996, loss=1.0448594093322754
I0130 14:26:46.752821 140004676327168 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.273953437805176, loss=1.0348716974258423
I0130 14:27:20.271824 140004667934464 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.4200611114501953, loss=1.1864084005355835
I0130 14:27:53.851367 140004676327168 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.2849812507629395, loss=1.1044893264770508
I0130 14:28:27.479486 140004667934464 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.1488707065582275, loss=1.1142783164978027
I0130 14:29:01.151446 140004676327168 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.0248663425445557, loss=1.0768237113952637
I0130 14:29:34.698277 140004667934464 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.2786741256713867, loss=1.0940403938293457
I0130 14:30:08.264120 140004676327168 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.061586618423462, loss=1.0873870849609375
I0130 14:30:41.878785 140004667934464 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.852468490600586, loss=1.1398762464523315
I0130 14:31:15.500355 140004676327168 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.5729782581329346, loss=1.0500376224517822
I0130 14:31:49.120723 140004667934464 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.308053493499756, loss=1.0504889488220215
I0130 14:32:22.724239 140004676327168 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.187556266784668, loss=1.0534191131591797
I0130 14:32:56.282719 140004667934464 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.2543387413024902, loss=1.0181505680084229
I0130 14:33:29.811903 140004676327168 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.319809913635254, loss=1.0730247497558594
I0130 14:33:50.757706 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:33:57.188558 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:34:05.858680 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:34:08.542973 140169137129280 submission_runner.py:408] Time since start: 45985.24s, 	Step: 132064, 	{'train/accuracy': 0.8375318646430969, 'train/loss': 0.5773704051971436, 'validation/accuracy': 0.7168599963188171, 'validation/loss': 1.1653879880905151, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8888368606567383, 'test/num_examples': 10000, 'score': 44409.24393892288, 'total_duration': 45985.240468502045, 'accumulated_submission_time': 44409.24393892288, 'accumulated_eval_time': 1568.0413491725922, 'accumulated_logging_time': 3.679718255996704}
I0130 14:34:08.590703 140005313861376 logging_writer.py:48] [132064] accumulated_eval_time=1568.041349, accumulated_logging_time=3.679718, accumulated_submission_time=44409.243939, global_step=132064, preemption_count=0, score=44409.243939, test/accuracy=0.588200, test/loss=1.888837, test/num_examples=10000, total_duration=45985.240469, train/accuracy=0.837532, train/loss=0.577370, validation/accuracy=0.716860, validation/loss=1.165388, validation/num_examples=50000
I0130 14:34:20.993211 140005322254080 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.211273670196533, loss=0.9890416860580444
I0130 14:34:54.568430 140005313861376 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.4018802642822266, loss=1.078934669494629
I0130 14:35:28.258358 140005322254080 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.3016068935394287, loss=1.0186492204666138
I0130 14:36:01.827632 140005313861376 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.5352096557617188, loss=1.1927838325500488
I0130 14:36:35.357656 140005322254080 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.3917644023895264, loss=1.0709114074707031
I0130 14:37:08.923139 140005313861376 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.526472806930542, loss=1.082840919494629
I0130 14:37:42.536406 140005322254080 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.186506748199463, loss=0.9966832995414734
I0130 14:38:16.176391 140005313861376 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.113086462020874, loss=0.9735976457595825
I0130 14:38:49.812574 140005322254080 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.96736478805542, loss=1.216784954071045
I0130 14:39:23.460482 140005313861376 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.25715708732605, loss=1.1021438837051392
I0130 14:39:57.078398 140005322254080 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.3322103023529053, loss=1.0360182523727417
I0130 14:40:30.707950 140005313861376 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5359385013580322, loss=1.1137417554855347
I0130 14:41:04.340734 140005322254080 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.5464491844177246, loss=1.0585687160491943
I0130 14:41:37.970318 140005313861376 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.2888400554656982, loss=1.0008137226104736
I0130 14:42:11.634229 140005322254080 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.2921645641326904, loss=1.1267168521881104
I0130 14:42:38.658791 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:42:45.151438 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:42:53.885658 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:42:56.578635 140169137129280 submission_runner.py:408] Time since start: 46513.28s, 	Step: 133582, 	{'train/accuracy': 0.8428332209587097, 'train/loss': 0.5535558462142944, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1483376026153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.894065499305725, 'test/num_examples': 10000, 'score': 44919.25254154205, 'total_duration': 46513.27612757683, 'accumulated_submission_time': 44919.25254154205, 'accumulated_eval_time': 1585.9611542224884, 'accumulated_logging_time': 3.7378854751586914}
I0130 14:42:56.618458 140004676327168 logging_writer.py:48] [133582] accumulated_eval_time=1585.961154, accumulated_logging_time=3.737885, accumulated_submission_time=44919.252542, global_step=133582, preemption_count=0, score=44919.252542, test/accuracy=0.595800, test/loss=1.894065, test/num_examples=10000, total_duration=46513.276128, train/accuracy=0.842833, train/loss=0.553556, validation/accuracy=0.720580, validation/loss=1.148338, validation/num_examples=50000
I0130 14:43:02.996364 140005288683264 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.446821451187134, loss=1.0747780799865723
I0130 14:43:36.545133 140004676327168 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.4950294494628906, loss=1.0991543531417847
I0130 14:44:10.053259 140005288683264 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.2673966884613037, loss=1.0047619342803955
I0130 14:44:43.594626 140004676327168 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.4884471893310547, loss=1.0560210943222046
I0130 14:45:17.141772 140005288683264 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.3485915660858154, loss=1.036539077758789
I0130 14:45:50.768685 140004676327168 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.32210373878479, loss=1.0446052551269531
I0130 14:46:24.384692 140005288683264 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.1709437370300293, loss=0.9943915605545044
I0130 14:46:58.015579 140004676327168 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.452059268951416, loss=1.053745150566101
I0130 14:47:31.645556 140005288683264 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.2946102619171143, loss=1.088904619216919
I0130 14:48:05.259614 140004676327168 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.2857162952423096, loss=1.0514507293701172
I0130 14:48:38.884595 140005288683264 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.6306025981903076, loss=1.092734456062317
I0130 14:49:12.476012 140004676327168 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.4841372966766357, loss=1.019782304763794
I0130 14:49:46.096573 140005288683264 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.5523781776428223, loss=0.9910297989845276
I0130 14:50:19.682282 140004676327168 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.5235157012939453, loss=1.0068036317825317
I0130 14:50:53.296979 140005288683264 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.4997472763061523, loss=1.0652672052383423
I0130 14:51:26.905831 140004676327168 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.385601043701172, loss=1.095033884048462
I0130 14:51:26.913547 140169137129280 spec.py:321] Evaluating on the training split.
I0130 14:51:33.999244 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 14:51:42.719630 140169137129280 spec.py:349] Evaluating on the test split.
I0130 14:51:45.450262 140169137129280 submission_runner.py:408] Time since start: 47042.15s, 	Step: 135101, 	{'train/accuracy': 0.8835100531578064, 'train/loss': 0.4169529676437378, 'validation/accuracy': 0.72461998462677, 'validation/loss': 1.1391019821166992, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.867887020111084, 'test/num_examples': 10000, 'score': 45429.48771595955, 'total_duration': 47042.147736787796, 'accumulated_submission_time': 45429.48771595955, 'accumulated_eval_time': 1604.497786283493, 'accumulated_logging_time': 3.788525342941284}
I0130 14:51:45.511858 140004676327168 logging_writer.py:48] [135101] accumulated_eval_time=1604.497786, accumulated_logging_time=3.788525, accumulated_submission_time=45429.487716, global_step=135101, preemption_count=0, score=45429.487716, test/accuracy=0.599800, test/loss=1.867887, test/num_examples=10000, total_duration=47042.147737, train/accuracy=0.883510, train/loss=0.416953, validation/accuracy=0.724620, validation/loss=1.139102, validation/num_examples=50000
I0130 14:52:18.970619 140005288683264 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.557861328125, loss=0.9689830541610718
I0130 14:52:52.550404 140004676327168 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.4640755653381348, loss=1.0263361930847168
I0130 14:53:26.169578 140005288683264 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.7158737182617188, loss=1.0476964712142944
I0130 14:53:59.812614 140004676327168 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.4975740909576416, loss=0.9639980792999268
I0130 14:54:33.438629 140005288683264 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.2911458015441895, loss=1.0315783023834229
I0130 14:55:07.160804 140004676327168 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.4402294158935547, loss=1.0900346040725708
I0130 14:55:40.686103 140005288683264 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.3275983333587646, loss=1.0328679084777832
I0130 14:56:14.301105 140004676327168 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.336646318435669, loss=1.0185672044754028
I0130 14:56:47.912111 140005288683264 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.8241019248962402, loss=1.0388582944869995
I0130 14:57:21.564034 140004676327168 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.3589367866516113, loss=1.018349051475525
I0130 14:57:55.175237 140005288683264 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.6047003269195557, loss=0.9813322424888611
I0130 14:58:28.771230 140004676327168 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.377326488494873, loss=1.0389139652252197
I0130 14:59:02.407345 140005288683264 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.4724557399749756, loss=1.0454779863357544
I0130 14:59:36.036409 140004676327168 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.641296148300171, loss=1.0511471033096313
I0130 15:00:09.672732 140005288683264 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.527275323867798, loss=1.0405484437942505
I0130 15:00:15.549314 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:00:21.965505 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:00:30.752544 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:00:33.498876 140169137129280 submission_runner.py:408] Time since start: 47570.20s, 	Step: 136619, 	{'train/accuracy': 0.8668088316917419, 'train/loss': 0.46367859840393066, 'validation/accuracy': 0.724399983882904, 'validation/loss': 1.1427175998687744, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.8947100639343262, 'test/num_examples': 10000, 'score': 45939.46535348892, 'total_duration': 47570.19637298584, 'accumulated_submission_time': 45939.46535348892, 'accumulated_eval_time': 1622.4473087787628, 'accumulated_logging_time': 3.860964298248291}
I0130 15:00:33.540864 140004667934464 logging_writer.py:48] [136619] accumulated_eval_time=1622.447309, accumulated_logging_time=3.860964, accumulated_submission_time=45939.465353, global_step=136619, preemption_count=0, score=45939.465353, test/accuracy=0.594300, test/loss=1.894710, test/num_examples=10000, total_duration=47570.196373, train/accuracy=0.866809, train/loss=0.463679, validation/accuracy=0.724400, validation/loss=1.142718, validation/num_examples=50000
I0130 15:01:01.052000 140004676327168 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.489896059036255, loss=1.0546832084655762
I0130 15:01:34.612074 140004667934464 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.5486600399017334, loss=1.0209219455718994
I0130 15:02:08.255623 140004676327168 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.622941732406616, loss=1.1254245042800903
I0130 15:02:41.862197 140004667934464 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.4350576400756836, loss=0.9645974040031433
I0130 15:03:15.472894 140004676327168 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.640587329864502, loss=1.0306543111801147
I0130 15:03:49.062468 140004667934464 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.6536383628845215, loss=1.009233832359314
I0130 15:04:22.698876 140004676327168 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.56900691986084, loss=0.9533385038375854
I0130 15:04:56.301631 140004667934464 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.3756327629089355, loss=0.9158540964126587
I0130 15:05:29.841147 140004676327168 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.680304765701294, loss=1.0796557664871216
I0130 15:06:03.406381 140004667934464 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.404707193374634, loss=0.9382963180541992
I0130 15:06:36.937356 140004676327168 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.9652464389801025, loss=0.9789695739746094
I0130 15:07:10.491850 140004667934464 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.6442432403564453, loss=0.9761413335800171
I0130 15:07:44.031156 140004676327168 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.373460292816162, loss=0.9354397654533386
I0130 15:08:17.688320 140004667934464 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.300283670425415, loss=0.947104811668396
I0130 15:08:51.329570 140004676327168 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.5696427822113037, loss=1.0447328090667725
I0130 15:09:03.574043 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:09:10.020488 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:09:18.820578 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:09:21.541631 140169137129280 submission_runner.py:408] Time since start: 48098.24s, 	Step: 138138, 	{'train/accuracy': 0.8622449040412903, 'train/loss': 0.4757066071033478, 'validation/accuracy': 0.7257999777793884, 'validation/loss': 1.1301318407058716, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.837876796722412, 'test/num_examples': 10000, 'score': 46449.43815970421, 'total_duration': 48098.23912549019, 'accumulated_submission_time': 46449.43815970421, 'accumulated_eval_time': 1640.4148676395416, 'accumulated_logging_time': 3.913733720779419}
I0130 15:09:21.588991 140005297075968 logging_writer.py:48] [138138] accumulated_eval_time=1640.414868, accumulated_logging_time=3.913734, accumulated_submission_time=46449.438160, global_step=138138, preemption_count=0, score=46449.438160, test/accuracy=0.604100, test/loss=1.837877, test/num_examples=10000, total_duration=48098.239125, train/accuracy=0.862245, train/loss=0.475707, validation/accuracy=0.725800, validation/loss=1.130132, validation/num_examples=50000
I0130 15:09:42.736042 140005305468672 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.4302730560302734, loss=0.9720983505249023
I0130 15:10:16.302747 140005297075968 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.405898094177246, loss=0.9577349424362183
I0130 15:10:49.891488 140005305468672 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.4427480697631836, loss=0.9413354992866516
I0130 15:11:23.518042 140005297075968 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.7722673416137695, loss=1.1089428663253784
I0130 15:11:57.155873 140005305468672 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.723097801208496, loss=1.0296785831451416
I0130 15:12:30.761884 140005297075968 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.511415719985962, loss=1.011897325515747
I0130 15:13:04.352528 140005305468672 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.703126907348633, loss=1.0156654119491577
I0130 15:13:37.980495 140005297075968 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.960080146789551, loss=1.046989917755127
I0130 15:14:11.627282 140005305468672 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.9134767055511475, loss=1.0631111860275269
I0130 15:14:45.307479 140005297075968 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.5056002140045166, loss=1.0050292015075684
I0130 15:15:18.842291 140005305468672 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.490993022918701, loss=0.9891859889030457
I0130 15:15:52.407907 140005297075968 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.6667962074279785, loss=0.9518375992774963
I0130 15:16:26.040565 140005305468672 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.5752696990966797, loss=1.0108181238174438
I0130 15:16:59.663796 140005297075968 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.5886895656585693, loss=0.9505568742752075
I0130 15:17:33.285743 140005305468672 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.432286024093628, loss=1.0119318962097168
I0130 15:17:51.601473 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:17:57.977268 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:18:06.824130 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:18:09.547255 140169137129280 submission_runner.py:408] Time since start: 48626.24s, 	Step: 139656, 	{'train/accuracy': 0.8647361397743225, 'train/loss': 0.466713011264801, 'validation/accuracy': 0.7309799790382385, 'validation/loss': 1.1106938123703003, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.8522108793258667, 'test/num_examples': 10000, 'score': 46959.3910138607, 'total_duration': 48626.24474787712, 'accumulated_submission_time': 46959.3910138607, 'accumulated_eval_time': 1658.3606095314026, 'accumulated_logging_time': 3.971649646759033}
I0130 15:18:09.593013 140005288683264 logging_writer.py:48] [139656] accumulated_eval_time=1658.360610, accumulated_logging_time=3.971650, accumulated_submission_time=46959.391014, global_step=139656, preemption_count=0, score=46959.391014, test/accuracy=0.604500, test/loss=1.852211, test/num_examples=10000, total_duration=48626.244748, train/accuracy=0.864736, train/loss=0.466713, validation/accuracy=0.730980, validation/loss=1.110694, validation/num_examples=50000
I0130 15:18:24.733547 140005322254080 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.2784390449523926, loss=0.9536707997322083
I0130 15:18:58.334428 140005288683264 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.8651626110076904, loss=0.9649908542633057
I0130 15:19:31.976377 140005322254080 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.314924478530884, loss=0.9632304310798645
I0130 15:20:05.602729 140005288683264 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.52040696144104, loss=0.968257486820221
I0130 15:20:39.225348 140005322254080 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.846971273422241, loss=0.9854961037635803
I0130 15:21:12.842128 140005288683264 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.7186620235443115, loss=0.9600977897644043
I0130 15:21:46.637055 140005322254080 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.667907476425171, loss=0.9501896500587463
I0130 15:22:20.263397 140005288683264 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.608887195587158, loss=0.9779395461082458
I0130 15:22:53.877793 140005322254080 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.4444711208343506, loss=0.9746530055999756
I0130 15:23:27.507724 140005288683264 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.997859239578247, loss=1.0011365413665771
I0130 15:24:01.133052 140005322254080 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.6342456340789795, loss=0.9368607997894287
I0130 15:24:34.759979 140005288683264 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.3748245239257812, loss=0.9107054471969604
I0130 15:25:08.367427 140005322254080 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.7536239624023438, loss=0.9213955998420715
I0130 15:25:42.004481 140005288683264 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.684800624847412, loss=0.8550924062728882
I0130 15:26:15.636888 140005322254080 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.7906546592712402, loss=0.9251247644424438
I0130 15:26:39.667733 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:26:46.021509 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:26:54.670306 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:26:57.371995 140169137129280 submission_runner.py:408] Time since start: 49154.07s, 	Step: 141173, 	{'train/accuracy': 0.8649353981018066, 'train/loss': 0.4697670340538025, 'validation/accuracy': 0.7278599739074707, 'validation/loss': 1.1257988214492798, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8480082750320435, 'test/num_examples': 10000, 'score': 47469.405690431595, 'total_duration': 49154.069472551346, 'accumulated_submission_time': 47469.405690431595, 'accumulated_eval_time': 1676.064817905426, 'accumulated_logging_time': 4.028349161148071}
I0130 15:26:57.414319 140004676327168 logging_writer.py:48] [141173] accumulated_eval_time=1676.064818, accumulated_logging_time=4.028349, accumulated_submission_time=47469.405690, global_step=141173, preemption_count=0, score=47469.405690, test/accuracy=0.600800, test/loss=1.848008, test/num_examples=10000, total_duration=49154.069473, train/accuracy=0.864935, train/loss=0.469767, validation/accuracy=0.727860, validation/loss=1.125799, validation/num_examples=50000
I0130 15:27:06.815315 140005297075968 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.62658953666687, loss=0.8878211379051208
I0130 15:27:40.308642 140004676327168 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.6788341999053955, loss=1.1216844320297241
I0130 15:28:14.010262 140005297075968 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.726602554321289, loss=1.067290186882019
I0130 15:28:47.620250 140004676327168 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.8802735805511475, loss=1.0929231643676758
I0130 15:29:21.257238 140005297075968 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.6316287517547607, loss=1.016219139099121
I0130 15:29:54.863984 140004676327168 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.606584072113037, loss=0.9469282627105713
I0130 15:30:28.494953 140005297075968 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.796513557434082, loss=0.9788229465484619
I0130 15:31:02.138715 140004676327168 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.7905023097991943, loss=0.9423234462738037
I0130 15:31:35.749718 140005297075968 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.3917391300201416, loss=0.9759438037872314
I0130 15:32:09.362509 140004676327168 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.602827310562134, loss=1.0047705173492432
I0130 15:32:42.998083 140005297075968 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4696688652038574, loss=1.010553240776062
I0130 15:33:16.569341 140004676327168 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.6018922328948975, loss=0.9151257276535034
I0130 15:33:50.197022 140005297075968 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.7919065952301025, loss=0.9902867674827576
I0130 15:34:23.921753 140004676327168 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.7727932929992676, loss=0.8322610259056091
I0130 15:34:57.453088 140005297075968 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.538679361343384, loss=0.8680388927459717
I0130 15:35:27.510124 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:35:34.018515 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:35:42.758999 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:35:45.484637 140169137129280 submission_runner.py:408] Time since start: 49682.18s, 	Step: 142691, 	{'train/accuracy': 0.8581592440605164, 'train/loss': 0.48604610562324524, 'validation/accuracy': 0.7240399718284607, 'validation/loss': 1.1514825820922852, 'validation/num_examples': 50000, 'test/accuracy': 0.5978000164031982, 'test/loss': 1.939948320388794, 'test/num_examples': 10000, 'score': 47979.4405105114, 'total_duration': 49682.18212604523, 'accumulated_submission_time': 47979.4405105114, 'accumulated_eval_time': 1694.0392887592316, 'accumulated_logging_time': 4.082364082336426}
I0130 15:35:45.528936 140004667934464 logging_writer.py:48] [142691] accumulated_eval_time=1694.039289, accumulated_logging_time=4.082364, accumulated_submission_time=47979.440511, global_step=142691, preemption_count=0, score=47979.440511, test/accuracy=0.597800, test/loss=1.939948, test/num_examples=10000, total_duration=49682.182126, train/accuracy=0.858159, train/loss=0.486046, validation/accuracy=0.724040, validation/loss=1.151483, validation/num_examples=50000
I0130 15:35:48.896187 140004676327168 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.810375452041626, loss=0.9871906638145447
I0130 15:36:22.423581 140004667934464 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.781484842300415, loss=0.8966809511184692
I0130 15:36:55.954932 140004676327168 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.815490484237671, loss=0.999525785446167
I0130 15:37:29.553754 140004667934464 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.735520124435425, loss=0.9983644485473633
I0130 15:38:03.192192 140004676327168 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.7245142459869385, loss=0.9190706014633179
I0130 15:38:36.808696 140004667934464 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.7719550132751465, loss=0.9910025000572205
I0130 15:39:10.429255 140004676327168 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.91306209564209, loss=0.9746201634407043
I0130 15:39:44.045966 140004667934464 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.968088150024414, loss=0.9053555130958557
I0130 15:40:17.669332 140004676327168 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.8124382495880127, loss=0.8939052820205688
I0130 15:40:51.317104 140004667934464 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.760507822036743, loss=0.9270954728126526
I0130 15:41:24.935883 140004676327168 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.773580312728882, loss=0.9810467958450317
I0130 15:41:58.489504 140004667934464 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.9207065105438232, loss=0.9951609373092651
I0130 15:42:32.117774 140004676327168 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.9969496726989746, loss=0.9276065826416016
I0130 15:43:06.170925 140004667934464 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.6293234825134277, loss=0.9875917434692383
I0130 15:43:39.771843 140004676327168 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.5945584774017334, loss=0.9449195861816406
I0130 15:44:13.381985 140004667934464 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.8990490436553955, loss=0.9107858538627625
I0130 15:44:15.564777 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:44:22.031257 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:44:30.425812 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:44:33.216818 140169137129280 submission_runner.py:408] Time since start: 50209.91s, 	Step: 144208, 	{'train/accuracy': 0.8985769748687744, 'train/loss': 0.3564726412296295, 'validation/accuracy': 0.7285599708557129, 'validation/loss': 1.129584789276123, 'validation/num_examples': 50000, 'test/accuracy': 0.6085000038146973, 'test/loss': 1.8798846006393433, 'test/num_examples': 10000, 'score': 48489.41442799568, 'total_duration': 50209.914311885834, 'accumulated_submission_time': 48489.41442799568, 'accumulated_eval_time': 1711.6912882328033, 'accumulated_logging_time': 4.139669179916382}
I0130 15:44:33.260727 140004667934464 logging_writer.py:48] [144208] accumulated_eval_time=1711.691288, accumulated_logging_time=4.139669, accumulated_submission_time=48489.414428, global_step=144208, preemption_count=0, score=48489.414428, test/accuracy=0.608500, test/loss=1.879885, test/num_examples=10000, total_duration=50209.914312, train/accuracy=0.898577, train/loss=0.356473, validation/accuracy=0.728560, validation/loss=1.129585, validation/num_examples=50000
I0130 15:45:04.449267 140005305468672 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.9106338024139404, loss=0.9637927412986755
I0130 15:45:38.033635 140004667934464 logging_writer.py:48] [144400] global_step=144400, grad_norm=4.05573034286499, loss=0.9373338222503662
I0130 15:46:11.674572 140005305468672 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.7317545413970947, loss=0.9428465366363525
I0130 15:46:45.311610 140004667934464 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.586287498474121, loss=0.8560709953308105
I0130 15:47:18.913724 140005305468672 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.7377443313598633, loss=1.057611346244812
I0130 15:47:52.575211 140004667934464 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.08034086227417, loss=1.043359637260437
I0130 15:48:26.131579 140005305468672 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.81467342376709, loss=0.9470725059509277
I0130 15:48:59.764050 140004667934464 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.0244903564453125, loss=0.91486656665802
I0130 15:49:33.372283 140005305468672 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6986212730407715, loss=0.9316282868385315
I0130 15:50:07.011803 140004667934464 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.7781717777252197, loss=0.9184451103210449
I0130 15:50:40.609906 140005305468672 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.8674330711364746, loss=0.859042227268219
I0130 15:51:14.236500 140004667934464 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.8285982608795166, loss=0.8787930011749268
I0130 15:51:47.848843 140005305468672 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.9818739891052246, loss=0.9741588234901428
I0130 15:52:21.444581 140004667934464 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.4513370990753174, loss=0.8428425192832947
I0130 15:52:55.087084 140005305468672 logging_writer.py:48] [145700] global_step=145700, grad_norm=4.012144088745117, loss=0.895409345626831
I0130 15:53:03.302797 140169137129280 spec.py:321] Evaluating on the training split.
I0130 15:53:09.725286 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 15:53:18.214660 140169137129280 spec.py:349] Evaluating on the test split.
I0130 15:53:20.932792 140169137129280 submission_runner.py:408] Time since start: 50737.63s, 	Step: 145726, 	{'train/accuracy': 0.8902861475944519, 'train/loss': 0.38217586278915405, 'validation/accuracy': 0.7315999865531921, 'validation/loss': 1.1145838499069214, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.8463207483291626, 'test/num_examples': 10000, 'score': 48999.39666056633, 'total_duration': 50737.630281448364, 'accumulated_submission_time': 48999.39666056633, 'accumulated_eval_time': 1729.3212552070618, 'accumulated_logging_time': 4.194288969039917}
I0130 15:53:20.975043 140005288683264 logging_writer.py:48] [145726] accumulated_eval_time=1729.321255, accumulated_logging_time=4.194289, accumulated_submission_time=48999.396661, global_step=145726, preemption_count=0, score=48999.396661, test/accuracy=0.606600, test/loss=1.846321, test/num_examples=10000, total_duration=50737.630281, train/accuracy=0.890286, train/loss=0.382176, validation/accuracy=0.731600, validation/loss=1.114584, validation/num_examples=50000
I0130 15:53:46.140318 140005297075968 logging_writer.py:48] [145800] global_step=145800, grad_norm=4.010339260101318, loss=0.9889053702354431
I0130 15:54:19.792489 140005288683264 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.3962976932525635, loss=0.7696273326873779
I0130 15:54:53.392108 140005297075968 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.9169678688049316, loss=0.927146852016449
I0130 15:55:27.001042 140005288683264 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.722928524017334, loss=0.9947071671485901
I0130 15:56:00.638138 140005297075968 logging_writer.py:48] [146200] global_step=146200, grad_norm=4.071855068206787, loss=0.9251410961151123
I0130 15:56:34.256666 140005288683264 logging_writer.py:48] [146300] global_step=146300, grad_norm=4.154697418212891, loss=0.9039280414581299
I0130 15:57:07.879589 140005297075968 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.9346060752868652, loss=0.9328056573867798
I0130 15:57:41.503432 140005288683264 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.7524452209472656, loss=0.8893352150917053
I0130 15:58:15.122080 140005297075968 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.043342113494873, loss=0.9302719831466675
I0130 15:58:48.766124 140005288683264 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.961702585220337, loss=0.8975067734718323
I0130 15:59:22.390321 140005297075968 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.900700330734253, loss=0.9426926970481873
I0130 15:59:56.000948 140005288683264 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.9071590900421143, loss=0.9421272873878479
I0130 16:00:29.620482 140005297075968 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.808544635772705, loss=0.9405890107154846
I0130 16:01:03.259355 140005288683264 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.800346851348877, loss=1.02262544631958
I0130 16:01:36.837288 140005297075968 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.7255454063415527, loss=0.8243873119354248
I0130 16:01:51.098896 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:01:57.484866 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:02:06.149609 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:02:08.750839 140169137129280 submission_runner.py:408] Time since start: 51265.45s, 	Step: 147244, 	{'train/accuracy': 0.8928371667861938, 'train/loss': 0.36683279275894165, 'validation/accuracy': 0.7371399998664856, 'validation/loss': 1.0903639793395996, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.8396549224853516, 'test/num_examples': 10000, 'score': 49509.461168289185, 'total_duration': 51265.44833254814, 'accumulated_submission_time': 49509.461168289185, 'accumulated_eval_time': 1746.9731595516205, 'accumulated_logging_time': 4.246668100357056}
I0130 16:02:08.797153 140005305468672 logging_writer.py:48] [147244] accumulated_eval_time=1746.973160, accumulated_logging_time=4.246668, accumulated_submission_time=49509.461168, global_step=147244, preemption_count=0, score=49509.461168, test/accuracy=0.605500, test/loss=1.839655, test/num_examples=10000, total_duration=51265.448333, train/accuracy=0.892837, train/loss=0.366833, validation/accuracy=0.737140, validation/loss=1.090364, validation/num_examples=50000
I0130 16:02:27.931674 140005313861376 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.908116102218628, loss=0.9078510403633118
I0130 16:03:01.516932 140005305468672 logging_writer.py:48] [147400] global_step=147400, grad_norm=4.09946346282959, loss=1.0054861307144165
I0130 16:03:35.163233 140005313861376 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.90727162361145, loss=0.8772978782653809
I0130 16:04:08.784311 140005305468672 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.8237154483795166, loss=0.8410035371780396
I0130 16:04:42.384536 140005313861376 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.6976747512817383, loss=0.9052324295043945
I0130 16:05:15.986227 140005305468672 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.780517101287842, loss=0.8897470235824585
I0130 16:05:49.624199 140005313861376 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.029708385467529, loss=0.8966047167778015
I0130 16:06:23.254930 140005305468672 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.7129595279693604, loss=0.8726128339767456
I0130 16:06:56.873993 140005313861376 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.9180917739868164, loss=0.8901943564414978
I0130 16:07:30.531999 140005305468672 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.384202003479004, loss=0.855171799659729
I0130 16:08:04.091261 140005313861376 logging_writer.py:48] [148300] global_step=148300, grad_norm=4.129692554473877, loss=0.8863398432731628
I0130 16:08:37.707238 140005305468672 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.7862207889556885, loss=0.9172539710998535
I0130 16:09:11.334098 140005313861376 logging_writer.py:48] [148500] global_step=148500, grad_norm=4.179633617401123, loss=0.8986194133758545
I0130 16:09:44.968486 140005305468672 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.9064018726348877, loss=0.9778115749359131
I0130 16:10:18.579691 140005313861376 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.0018439292907715, loss=0.8960148096084595
I0130 16:10:38.897001 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:10:45.300981 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:10:54.001187 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:10:56.691109 140169137129280 submission_runner.py:408] Time since start: 51793.39s, 	Step: 148762, 	{'train/accuracy': 0.8951091766357422, 'train/loss': 0.3621106743812561, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.090329647064209, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8255805969238281, 'test/num_examples': 10000, 'score': 50019.50054001808, 'total_duration': 51793.38860464096, 'accumulated_submission_time': 50019.50054001808, 'accumulated_eval_time': 1764.7672312259674, 'accumulated_logging_time': 4.303528308868408}
I0130 16:10:56.734573 140004676327168 logging_writer.py:48] [148762] accumulated_eval_time=1764.767231, accumulated_logging_time=4.303528, accumulated_submission_time=50019.500540, global_step=148762, preemption_count=0, score=50019.500540, test/accuracy=0.609600, test/loss=1.825581, test/num_examples=10000, total_duration=51793.388605, train/accuracy=0.895109, train/loss=0.362111, validation/accuracy=0.738600, validation/loss=1.090330, validation/num_examples=50000
I0130 16:11:09.828708 140005288683264 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.085366249084473, loss=0.9693298935890198
I0130 16:11:43.339173 140004676327168 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.769578456878662, loss=0.8316463828086853
I0130 16:12:16.871499 140005288683264 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8538942337036133, loss=0.8583837151527405
I0130 16:12:50.419916 140004676327168 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.539456844329834, loss=0.775470495223999
I0130 16:13:24.051024 140005288683264 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.5758285522460938, loss=0.8142902851104736
I0130 16:13:57.788991 140004676327168 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.147621154785156, loss=0.8663743138313293
I0130 16:14:31.432959 140005288683264 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.10561466217041, loss=0.9030801057815552
I0130 16:15:05.053351 140004676327168 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.011641979217529, loss=0.892162024974823
I0130 16:15:38.686079 140005288683264 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.95913028717041, loss=0.9025286436080933
I0130 16:16:12.303291 140004676327168 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.7661643028259277, loss=0.8463478684425354
I0130 16:16:45.928614 140005288683264 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.154026508331299, loss=0.9209551215171814
I0130 16:17:19.563279 140004676327168 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.072783470153809, loss=0.894607424736023
I0130 16:17:53.200881 140005288683264 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.8213953971862793, loss=0.9331250786781311
I0130 16:18:26.812498 140004676327168 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.852250099182129, loss=0.8282331228256226
I0130 16:19:00.398252 140005288683264 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.2804388999938965, loss=0.9256818294525146
I0130 16:19:26.699801 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:19:33.096360 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:19:41.551455 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:19:44.280411 140169137129280 submission_runner.py:408] Time since start: 52320.98s, 	Step: 150280, 	{'train/accuracy': 0.8956672549247742, 'train/loss': 0.356880247592926, 'validation/accuracy': 0.7396799921989441, 'validation/loss': 1.1052157878875732, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.8790510892868042, 'test/num_examples': 10000, 'score': 50529.406017541885, 'total_duration': 52320.97790455818, 'accumulated_submission_time': 50529.406017541885, 'accumulated_eval_time': 1782.3478038311005, 'accumulated_logging_time': 4.357873916625977}
I0130 16:19:44.326195 140005313861376 logging_writer.py:48] [150280] accumulated_eval_time=1782.347804, accumulated_logging_time=4.357874, accumulated_submission_time=50529.406018, global_step=150280, preemption_count=0, score=50529.406018, test/accuracy=0.611100, test/loss=1.879051, test/num_examples=10000, total_duration=52320.977905, train/accuracy=0.895667, train/loss=0.356880, validation/accuracy=0.739680, validation/loss=1.105216, validation/num_examples=50000
I0130 16:19:51.368368 140005322254080 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.290597915649414, loss=0.9432742595672607
I0130 16:20:24.971215 140005313861376 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.161630153656006, loss=0.8724563121795654
I0130 16:20:58.504641 140005322254080 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.9830079078674316, loss=0.8951746821403503
I0130 16:21:32.048779 140005313861376 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.041537761688232, loss=0.8790030479431152
I0130 16:22:05.615571 140005322254080 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.7473766803741455, loss=0.8326034545898438
I0130 16:22:39.239356 140005313861376 logging_writer.py:48] [150800] global_step=150800, grad_norm=4.21255350112915, loss=0.8789591193199158
I0130 16:23:12.867538 140005322254080 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.115099906921387, loss=0.8866163492202759
I0130 16:23:46.501751 140005313861376 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.220881938934326, loss=0.826323926448822
I0130 16:24:20.127262 140005322254080 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.205372333526611, loss=0.8006559610366821
I0130 16:24:53.762804 140005313861376 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.977201461791992, loss=0.8566974401473999
I0130 16:25:27.373734 140005322254080 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.853623151779175, loss=0.7760706543922424
I0130 16:26:01.006142 140005313861376 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.992709159851074, loss=0.8287720680236816
I0130 16:26:34.632921 140005322254080 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.0649590492248535, loss=0.8930925130844116
I0130 16:27:08.285263 140005313861376 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.216803073883057, loss=0.9538761377334595
I0130 16:27:41.813917 140005322254080 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.7365612983703613, loss=0.8464183807373047
I0130 16:28:14.539309 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:28:20.973627 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:28:29.764791 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:28:32.510259 140169137129280 submission_runner.py:408] Time since start: 52849.21s, 	Step: 151799, 	{'train/accuracy': 0.904715359210968, 'train/loss': 0.33212369680404663, 'validation/accuracy': 0.7416599988937378, 'validation/loss': 1.0797576904296875, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.824073076248169, 'test/num_examples': 10000, 'score': 51039.55972290039, 'total_duration': 52849.2077562809, 'accumulated_submission_time': 51039.55972290039, 'accumulated_eval_time': 1800.3187172412872, 'accumulated_logging_time': 4.414421319961548}
I0130 16:28:32.556547 140004667934464 logging_writer.py:48] [151799] accumulated_eval_time=1800.318717, accumulated_logging_time=4.414421, accumulated_submission_time=51039.559723, global_step=151799, preemption_count=0, score=51039.559723, test/accuracy=0.613800, test/loss=1.824073, test/num_examples=10000, total_duration=52849.207756, train/accuracy=0.904715, train/loss=0.332124, validation/accuracy=0.741660, validation/loss=1.079758, validation/num_examples=50000
I0130 16:28:33.257977 140004676327168 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.8555378913879395, loss=0.8133271336555481
I0130 16:29:06.791089 140004667934464 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.868704319000244, loss=0.8208678960800171
I0130 16:29:40.397565 140004676327168 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.7649571895599365, loss=0.8159201741218567
I0130 16:30:13.971344 140004667934464 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.066066741943359, loss=0.8284200429916382
I0130 16:30:47.617482 140004676327168 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.9405648708343506, loss=0.8235859870910645
I0130 16:31:21.258402 140004667934464 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.10739803314209, loss=0.8727949857711792
I0130 16:31:54.877345 140004676327168 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.9415013790130615, loss=0.778127908706665
I0130 16:32:28.450831 140004667934464 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.7091610431671143, loss=0.7444339990615845
I0130 16:33:01.999543 140004676327168 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.433929920196533, loss=0.883851170539856
I0130 16:33:35.669630 140004667934464 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.156305313110352, loss=0.8639402389526367
I0130 16:34:09.285573 140004676327168 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.53813362121582, loss=0.8227208852767944
I0130 16:34:42.882505 140004667934464 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.8135886192321777, loss=0.8483772277832031
I0130 16:35:16.541162 140004676327168 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.078121662139893, loss=0.8622615933418274
I0130 16:35:50.181795 140004667934464 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.034343719482422, loss=0.8443503379821777
I0130 16:36:23.791244 140004676327168 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.004466533660889, loss=0.8235663771629333
I0130 16:36:57.372276 140004667934464 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.123006343841553, loss=0.9400124549865723
I0130 16:37:02.559560 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:37:09.022249 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:37:17.399413 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:37:20.136966 140169137129280 submission_runner.py:408] Time since start: 53376.83s, 	Step: 153317, 	{'train/accuracy': 0.9268175959587097, 'train/loss': 0.26082730293273926, 'validation/accuracy': 0.7423999905586243, 'validation/loss': 1.080202579498291, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.8277418613433838, 'test/num_examples': 10000, 'score': 51549.500520944595, 'total_duration': 53376.834444761276, 'accumulated_submission_time': 51549.500520944595, 'accumulated_eval_time': 1817.896065711975, 'accumulated_logging_time': 4.474432706832886}
I0130 16:37:20.183075 140004659541760 logging_writer.py:48] [153317] accumulated_eval_time=1817.896066, accumulated_logging_time=4.474433, accumulated_submission_time=51549.500521, global_step=153317, preemption_count=0, score=51549.500521, test/accuracy=0.615300, test/loss=1.827742, test/num_examples=10000, total_duration=53376.834445, train/accuracy=0.926818, train/loss=0.260827, validation/accuracy=0.742400, validation/loss=1.080203, validation/num_examples=50000
I0130 16:37:48.313971 140004667934464 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.275948524475098, loss=0.8519272208213806
I0130 16:38:21.879240 140004659541760 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.879087209701538, loss=0.8562290072441101
I0130 16:38:55.478958 140004667934464 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.9923112392425537, loss=0.7815638780593872
I0130 16:39:29.112177 140004659541760 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.095101356506348, loss=0.8411757349967957
I0130 16:40:02.805216 140004667934464 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.244831562042236, loss=0.868377685546875
I0130 16:40:36.358736 140004659541760 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.883261203765869, loss=0.7960700392723083
I0130 16:41:09.972194 140004667934464 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.168440818786621, loss=0.8186694979667664
I0130 16:41:43.560775 140004659541760 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.387170314788818, loss=0.9131020903587341
I0130 16:42:17.187126 140004667934464 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.25138521194458, loss=0.9565491676330566
I0130 16:42:50.829313 140004659541760 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.829845905303955, loss=0.7896857857704163
I0130 16:43:24.447422 140004667934464 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.304081439971924, loss=0.8371951580047607
I0130 16:43:58.062251 140004659541760 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.051117897033691, loss=0.8066361546516418
I0130 16:44:31.675625 140004667934464 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.898046016693115, loss=0.8629440069198608
I0130 16:45:05.292958 140004659541760 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.040180206298828, loss=0.9076749086380005
I0130 16:45:38.949818 140004667934464 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.975752115249634, loss=0.7645061016082764
I0130 16:45:50.204302 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:45:56.649262 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:46:05.162230 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:46:07.874490 140169137129280 submission_runner.py:408] Time since start: 53904.57s, 	Step: 154835, 	{'train/accuracy': 0.9178690910339355, 'train/loss': 0.28511112928390503, 'validation/accuracy': 0.7413199543952942, 'validation/loss': 1.083351969718933, 'validation/num_examples': 50000, 'test/accuracy': 0.6127000451087952, 'test/loss': 1.845734715461731, 'test/num_examples': 10000, 'score': 52059.46150302887, 'total_duration': 53904.571982860565, 'accumulated_submission_time': 52059.46150302887, 'accumulated_eval_time': 1835.5662310123444, 'accumulated_logging_time': 4.531611204147339}
I0130 16:46:07.921740 140005313861376 logging_writer.py:48] [154835] accumulated_eval_time=1835.566231, accumulated_logging_time=4.531611, accumulated_submission_time=52059.461503, global_step=154835, preemption_count=0, score=52059.461503, test/accuracy=0.612700, test/loss=1.845735, test/num_examples=10000, total_duration=53904.571983, train/accuracy=0.917869, train/loss=0.285111, validation/accuracy=0.741320, validation/loss=1.083352, validation/num_examples=50000
I0130 16:46:30.142575 140005322254080 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.058164596557617, loss=0.8130056262016296
I0130 16:47:03.721518 140005313861376 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.031909465789795, loss=0.8649240732192993
I0130 16:47:37.360959 140005322254080 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.625422477722168, loss=0.883860170841217
I0130 16:48:10.976395 140005313861376 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.182600498199463, loss=0.8516055345535278
I0130 16:48:44.595101 140005322254080 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.892080307006836, loss=0.8052372932434082
I0130 16:49:18.223680 140005313861376 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.198256969451904, loss=0.8324632048606873
I0130 16:49:51.856721 140005322254080 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.265555381774902, loss=0.7510985136032104
I0130 16:50:25.500853 140005313861376 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.064442157745361, loss=0.7447083592414856
I0130 16:50:59.125880 140005322254080 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.112456798553467, loss=0.8001348972320557
I0130 16:51:32.760046 140005313861376 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.8388946056365967, loss=0.7789154052734375
I0130 16:52:06.367995 140005322254080 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.2438740730285645, loss=0.8281035423278809
I0130 16:52:39.928779 140005313861376 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.310240745544434, loss=0.7954724431037903
I0130 16:53:13.616509 140005322254080 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.359411716461182, loss=0.8213168978691101
I0130 16:53:47.183288 140005313861376 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.3890862464904785, loss=0.8396924734115601
I0130 16:54:20.806428 140005322254080 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.342993259429932, loss=0.7830017805099487
I0130 16:54:38.115380 140169137129280 spec.py:321] Evaluating on the training split.
I0130 16:54:44.539304 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 16:54:52.992263 140169137129280 spec.py:349] Evaluating on the test split.
I0130 16:54:55.692221 140169137129280 submission_runner.py:408] Time since start: 54432.39s, 	Step: 156353, 	{'train/accuracy': 0.9176298975944519, 'train/loss': 0.2818046808242798, 'validation/accuracy': 0.7437999844551086, 'validation/loss': 1.084424614906311, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.8379580974578857, 'test/num_examples': 10000, 'score': 52569.59485697746, 'total_duration': 54432.38970851898, 'accumulated_submission_time': 52569.59485697746, 'accumulated_eval_time': 1853.1430249214172, 'accumulated_logging_time': 4.589997291564941}
I0130 16:54:55.739755 140005297075968 logging_writer.py:48] [156353] accumulated_eval_time=1853.143025, accumulated_logging_time=4.589997, accumulated_submission_time=52569.594857, global_step=156353, preemption_count=0, score=52569.594857, test/accuracy=0.622000, test/loss=1.837958, test/num_examples=10000, total_duration=54432.389709, train/accuracy=0.917630, train/loss=0.281805, validation/accuracy=0.743800, validation/loss=1.084425, validation/num_examples=50000
I0130 16:55:11.880431 140005305468672 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.147407054901123, loss=0.7713611125946045
I0130 16:55:45.463792 140005297075968 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.067698955535889, loss=0.729301929473877
I0130 16:56:19.077888 140005305468672 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.317819118499756, loss=0.7557427883148193
I0130 16:56:52.713558 140005297075968 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.9191150665283203, loss=0.7239665985107422
I0130 16:57:26.336181 140005305468672 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.533175468444824, loss=0.856570303440094
I0130 16:57:59.926145 140005297075968 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.2857208251953125, loss=0.7920734882354736
I0130 16:58:33.458488 140005305468672 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.603734970092773, loss=0.8960344791412354
I0130 16:59:06.991652 140005297075968 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.307884216308594, loss=0.7551557421684265
I0130 16:59:40.627032 140005305468672 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.2591657638549805, loss=0.8203617930412292
I0130 17:00:14.146701 140005297075968 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.415821075439453, loss=0.8228210210800171
I0130 17:00:47.698561 140005305468672 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.029234886169434, loss=0.7354068756103516
I0130 17:01:21.256736 140005297075968 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.9225547313690186, loss=0.6951901912689209
I0130 17:01:54.767297 140005305468672 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.7792158126831055, loss=0.8118714094161987
I0130 17:02:28.320128 140005297075968 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.080790042877197, loss=0.7028302550315857
I0130 17:03:01.913415 140005305468672 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.225633144378662, loss=0.7922396659851074
I0130 17:03:25.934173 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:03:32.349844 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:03:40.898073 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:03:43.638050 140169137129280 submission_runner.py:408] Time since start: 54960.34s, 	Step: 157873, 	{'train/accuracy': 0.9197026491165161, 'train/loss': 0.27838096022605896, 'validation/accuracy': 0.7439000010490417, 'validation/loss': 1.0915087461471558, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8537700176239014, 'test/num_examples': 10000, 'score': 53079.72857952118, 'total_duration': 54960.33554697037, 'accumulated_submission_time': 53079.72857952118, 'accumulated_eval_time': 1870.8468675613403, 'accumulated_logging_time': 4.649376630783081}
I0130 17:03:43.685281 140004659541760 logging_writer.py:48] [157873] accumulated_eval_time=1870.846868, accumulated_logging_time=4.649377, accumulated_submission_time=53079.728580, global_step=157873, preemption_count=0, score=53079.728580, test/accuracy=0.615600, test/loss=1.853770, test/num_examples=10000, total_duration=54960.335547, train/accuracy=0.919703, train/loss=0.278381, validation/accuracy=0.743900, validation/loss=1.091509, validation/num_examples=50000
I0130 17:03:53.092570 140004667934464 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.384730815887451, loss=0.7979421615600586
I0130 17:04:26.615781 140004659541760 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.422631740570068, loss=0.8565075397491455
I0130 17:05:00.165556 140004667934464 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.644817352294922, loss=0.8183109760284424
I0130 17:05:33.711233 140004659541760 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.292172908782959, loss=0.80950927734375
I0130 17:06:07.360400 140004667934464 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.1106181144714355, loss=0.7366523146629333
I0130 17:06:40.988547 140004659541760 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.02522611618042, loss=0.7774794697761536
I0130 17:07:14.629669 140004667934464 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.047755718231201, loss=0.7707107663154602
I0130 17:07:48.252018 140004659541760 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.167478084564209, loss=0.7849535942077637
I0130 17:08:21.898561 140004667934464 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.1791300773620605, loss=0.7805870771408081
I0130 17:08:55.520017 140004659541760 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.561737537384033, loss=0.7777721881866455
I0130 17:09:29.112933 140004667934464 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.795452117919922, loss=0.8954006433486938
I0130 17:10:02.687893 140004659541760 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.201141357421875, loss=0.7948183417320251
I0130 17:10:36.327430 140004667934464 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.5320258140563965, loss=0.8431606888771057
I0130 17:11:09.954558 140004659541760 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.465049743652344, loss=0.8012545108795166
I0130 17:11:43.607635 140004667934464 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.978557586669922, loss=0.7171425819396973
I0130 17:12:13.649175 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:12:20.084880 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:12:28.949239 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:12:31.689934 140169137129280 submission_runner.py:408] Time since start: 55488.39s, 	Step: 159391, 	{'train/accuracy': 0.9197424650192261, 'train/loss': 0.27487990260124207, 'validation/accuracy': 0.7453799843788147, 'validation/loss': 1.0846729278564453, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8550739288330078, 'test/num_examples': 10000, 'score': 53589.63150548935, 'total_duration': 55488.387427806854, 'accumulated_submission_time': 53589.63150548935, 'accumulated_eval_time': 1888.8875906467438, 'accumulated_logging_time': 4.708525896072388}
I0130 17:12:31.736510 140004667934464 logging_writer.py:48] [159391] accumulated_eval_time=1888.887591, accumulated_logging_time=4.708526, accumulated_submission_time=53589.631505, global_step=159391, preemption_count=0, score=53589.631505, test/accuracy=0.616500, test/loss=1.855074, test/num_examples=10000, total_duration=55488.387428, train/accuracy=0.919742, train/loss=0.274880, validation/accuracy=0.745380, validation/loss=1.084673, validation/num_examples=50000
I0130 17:12:35.088670 140005305468672 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.171354293823242, loss=0.7674000859260559
I0130 17:13:08.728165 140004667934464 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.327895164489746, loss=0.7515153884887695
I0130 17:13:42.293913 140005305468672 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.124542236328125, loss=0.7333321571350098
I0130 17:14:15.945980 140004667934464 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.9971370697021484, loss=0.7328810691833496
I0130 17:14:49.559099 140005305468672 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.122027397155762, loss=0.762088418006897
I0130 17:15:23.187377 140004667934464 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.071446895599365, loss=0.7078292965888977
I0130 17:15:56.825559 140005305468672 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.250325679779053, loss=0.8408807516098022
I0130 17:16:30.446161 140004667934464 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.047764301300049, loss=0.7274858951568604
I0130 17:17:04.074913 140005305468672 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.795702934265137, loss=0.9230432510375977
I0130 17:17:37.715702 140004667934464 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.568604469299316, loss=0.7820178866386414
I0130 17:18:11.347569 140005305468672 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.685811519622803, loss=0.7985904812812805
I0130 17:18:44.986672 140004667934464 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.3144612312316895, loss=0.7327218055725098
I0130 17:19:18.674599 140005305468672 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.187613487243652, loss=0.7820706963539124
I0130 17:19:52.279667 140004667934464 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.513282775878906, loss=0.8523547649383545
I0130 17:20:25.923553 140005305468672 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.070046424865723, loss=0.7403671741485596
I0130 17:20:59.553163 140004667934464 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.200387001037598, loss=0.7511006593704224
I0130 17:21:01.729784 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:21:08.132933 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:21:16.576677 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:21:19.328272 140169137129280 submission_runner.py:408] Time since start: 56016.03s, 	Step: 160908, 	{'train/accuracy': 0.9289500713348389, 'train/loss': 0.24715971946716309, 'validation/accuracy': 0.7472599744796753, 'validation/loss': 1.0752619504928589, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.851715087890625, 'test/num_examples': 10000, 'score': 54099.56414723396, 'total_duration': 56016.02576851845, 'accumulated_submission_time': 54099.56414723396, 'accumulated_eval_time': 1906.48606300354, 'accumulated_logging_time': 4.766141414642334}
I0130 17:21:19.375007 140004676327168 logging_writer.py:48] [160908] accumulated_eval_time=1906.486063, accumulated_logging_time=4.766141, accumulated_submission_time=54099.564147, global_step=160908, preemption_count=0, score=54099.564147, test/accuracy=0.620800, test/loss=1.851715, test/num_examples=10000, total_duration=56016.025769, train/accuracy=0.928950, train/loss=0.247160, validation/accuracy=0.747260, validation/loss=1.075262, validation/num_examples=50000
I0130 17:21:50.615881 140005288683264 logging_writer.py:48] [161000] global_step=161000, grad_norm=3.931161403656006, loss=0.6836766600608826
I0130 17:22:24.227709 140004676327168 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.255308628082275, loss=0.7325652241706848
I0130 17:22:57.888193 140005288683264 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.3772454261779785, loss=0.7681658864021301
I0130 17:23:31.515319 140004676327168 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.279567241668701, loss=0.7853658199310303
I0130 17:24:05.149824 140005288683264 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.420239448547363, loss=0.7343190908432007
I0130 17:24:38.779089 140004676327168 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.451258659362793, loss=0.7043362855911255
I0130 17:25:12.422794 140005288683264 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.179540634155273, loss=0.7101361751556396
I0130 17:25:46.135916 140004676327168 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.626213550567627, loss=0.8107891082763672
I0130 17:26:19.659501 140005288683264 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.353177547454834, loss=0.7734124660491943
I0130 17:26:53.259309 140004676327168 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.167749404907227, loss=0.7639459371566772
I0130 17:27:26.817076 140005288683264 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.534788131713867, loss=0.7823407649993896
I0130 17:28:00.382557 140004676327168 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.66396427154541, loss=0.7019062042236328
I0130 17:28:33.987565 140005288683264 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.348323345184326, loss=0.7756993174552917
I0130 17:29:07.633202 140004676327168 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.597057342529297, loss=0.7646979689598083
I0130 17:29:41.273337 140005288683264 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.656219005584717, loss=0.7465953230857849
I0130 17:29:49.491440 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:29:56.054746 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:30:04.732145 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:30:07.521343 140169137129280 submission_runner.py:408] Time since start: 56544.22s, 	Step: 162426, 	{'train/accuracy': 0.9414859414100647, 'train/loss': 0.20910905301570892, 'validation/accuracy': 0.7475999593734741, 'validation/loss': 1.0705816745758057, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.8417563438415527, 'test/num_examples': 10000, 'score': 54609.62043738365, 'total_duration': 56544.218824863434, 'accumulated_submission_time': 54609.62043738365, 'accumulated_eval_time': 1924.5159137248993, 'accumulated_logging_time': 4.82317328453064}
I0130 17:30:07.567162 140005313861376 logging_writer.py:48] [162426] accumulated_eval_time=1924.515914, accumulated_logging_time=4.823173, accumulated_submission_time=54609.620437, global_step=162426, preemption_count=0, score=54609.620437, test/accuracy=0.624500, test/loss=1.841756, test/num_examples=10000, total_duration=56544.218825, train/accuracy=0.941486, train/loss=0.209109, validation/accuracy=0.747600, validation/loss=1.070582, validation/num_examples=50000
I0130 17:30:32.737806 140005322254080 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.852631092071533, loss=0.8092783689498901
I0130 17:31:06.300138 140005313861376 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.110376358032227, loss=0.6872817873954773
I0130 17:31:39.878218 140005322254080 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.886370658874512, loss=0.8088526129722595
I0130 17:32:13.515300 140005313861376 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.383410453796387, loss=0.7433696389198303
I0130 17:32:47.155186 140005322254080 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.516010284423828, loss=0.719393789768219
I0130 17:33:20.705271 140005313861376 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.589043617248535, loss=0.7671894431114197
I0130 17:33:54.330813 140005322254080 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.437735557556152, loss=0.7084686756134033
I0130 17:34:27.941521 140005313861376 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.254344940185547, loss=0.6697124242782593
I0130 17:35:01.530758 140005322254080 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.504047393798828, loss=0.7066946625709534
I0130 17:35:35.157160 140005313861376 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.585265159606934, loss=0.7421858310699463
I0130 17:36:08.736141 140005322254080 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.0451226234436035, loss=0.7320449948310852
I0130 17:36:42.260185 140005313861376 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.364791393280029, loss=0.7070022821426392
I0130 17:37:15.809575 140005322254080 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.42970609664917, loss=0.7313296794891357
I0130 17:37:49.407947 140005313861376 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.5016632080078125, loss=0.723617434501648
I0130 17:38:23.036675 140005322254080 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.292842388153076, loss=0.6859384179115295
I0130 17:38:37.639582 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:38:44.393686 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:38:53.039071 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:38:55.736162 140169137129280 submission_runner.py:408] Time since start: 57072.43s, 	Step: 163945, 	{'train/accuracy': 0.9399114847183228, 'train/loss': 0.21189145743846893, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.0750255584716797, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.8490190505981445, 'test/num_examples': 10000, 'score': 55119.633002758026, 'total_duration': 57072.43365240097, 'accumulated_submission_time': 55119.633002758026, 'accumulated_eval_time': 1942.612450838089, 'accumulated_logging_time': 4.879745721817017}
I0130 17:38:55.785307 140004676327168 logging_writer.py:48] [163945] accumulated_eval_time=1942.612451, accumulated_logging_time=4.879746, accumulated_submission_time=55119.633003, global_step=163945, preemption_count=0, score=55119.633003, test/accuracy=0.620000, test/loss=1.849019, test/num_examples=10000, total_duration=57072.433652, train/accuracy=0.939911, train/loss=0.211891, validation/accuracy=0.747460, validation/loss=1.075026, validation/num_examples=50000
I0130 17:39:14.567405 140005288683264 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.472168922424316, loss=0.7867624759674072
I0130 17:39:48.078177 140004676327168 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.357560157775879, loss=0.7651987075805664
I0130 17:40:21.699309 140005288683264 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.963367938995361, loss=0.7511570453643799
I0130 17:40:55.344974 140004676327168 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.210118293762207, loss=0.652082085609436
I0130 17:41:28.965924 140005288683264 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.624988079071045, loss=0.7339078187942505
I0130 17:42:02.604139 140004676327168 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.190750598907471, loss=0.6378064751625061
I0130 17:42:36.236031 140005288683264 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.647703647613525, loss=0.7348669171333313
I0130 17:43:09.879793 140004676327168 logging_writer.py:48] [164700] global_step=164700, grad_norm=3.928642511367798, loss=0.7315573692321777
I0130 17:43:43.496607 140005288683264 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.528963088989258, loss=0.7725135087966919
I0130 17:44:17.104095 140004676327168 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.241030693054199, loss=0.7349116802215576
I0130 17:44:50.749591 140005288683264 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.160160541534424, loss=0.7152376174926758
I0130 17:45:24.478500 140004676327168 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.526528358459473, loss=0.7872635722160339
I0130 17:45:58.124881 140005288683264 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.659551620483398, loss=0.6596933603286743
I0130 17:46:31.760381 140004676327168 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.754220485687256, loss=0.7553113698959351
I0130 17:47:05.376943 140005288683264 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.499911785125732, loss=0.7385239005088806
I0130 17:47:26.022455 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:47:32.395880 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:47:40.959781 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:47:43.670390 140169137129280 submission_runner.py:408] Time since start: 57600.37s, 	Step: 165463, 	{'train/accuracy': 0.9395328164100647, 'train/loss': 0.20963868498802185, 'validation/accuracy': 0.7483199834823608, 'validation/loss': 1.0747122764587402, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.847937822341919, 'test/num_examples': 10000, 'score': 55629.809804201126, 'total_duration': 57600.36788249016, 'accumulated_submission_time': 55629.809804201126, 'accumulated_eval_time': 1960.2603447437286, 'accumulated_logging_time': 4.9395411014556885}
I0130 17:47:43.716725 140005322254080 logging_writer.py:48] [165463] accumulated_eval_time=1960.260345, accumulated_logging_time=4.939541, accumulated_submission_time=55629.809804, global_step=165463, preemption_count=0, score=55629.809804, test/accuracy=0.624100, test/loss=1.847938, test/num_examples=10000, total_duration=57600.367882, train/accuracy=0.939533, train/loss=0.209639, validation/accuracy=0.748320, validation/loss=1.074712, validation/num_examples=50000
I0130 17:47:56.458243 140005330646784 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.17555046081543, loss=0.738179624080658
I0130 17:48:30.056977 140005322254080 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.3950347900390625, loss=0.665460467338562
I0130 17:49:03.682141 140005330646784 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.543226718902588, loss=0.6979079842567444
I0130 17:49:37.318733 140005322254080 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.195668697357178, loss=0.7162882685661316
I0130 17:50:10.929607 140005330646784 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.597966194152832, loss=0.7260038256645203
I0130 17:50:44.539048 140005322254080 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.440418243408203, loss=0.7894721627235413
I0130 17:51:18.158025 140005330646784 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.268871307373047, loss=0.661342978477478
I0130 17:51:51.851622 140005322254080 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.457751274108887, loss=0.7050557136535645
I0130 17:52:25.386386 140005330646784 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.260931015014648, loss=0.6720606684684753
I0130 17:52:58.950009 140005322254080 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.315149307250977, loss=0.7035108208656311
I0130 17:53:32.501479 140005330646784 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.548940181732178, loss=0.6634525656700134
I0130 17:54:06.113602 140005322254080 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.435088157653809, loss=0.7128220796585083
I0130 17:54:39.743524 140005330646784 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.587426662445068, loss=0.7429647445678711
I0130 17:55:13.383417 140005322254080 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.2504754066467285, loss=0.6909855604171753
I0130 17:55:47.000274 140005330646784 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.3750200271606445, loss=0.6646246314048767
I0130 17:56:13.687772 140169137129280 spec.py:321] Evaluating on the training split.
I0130 17:56:20.403193 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 17:56:28.903164 140169137129280 spec.py:349] Evaluating on the test split.
I0130 17:56:31.611131 140169137129280 submission_runner.py:408] Time since start: 58128.31s, 	Step: 166981, 	{'train/accuracy': 0.9429607391357422, 'train/loss': 0.20566825568675995, 'validation/accuracy': 0.750059962272644, 'validation/loss': 1.0746914148330688, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.840732455253601, 'test/num_examples': 10000, 'score': 56139.720806121826, 'total_duration': 58128.30862569809, 'accumulated_submission_time': 56139.720806121826, 'accumulated_eval_time': 1978.183670282364, 'accumulated_logging_time': 4.996540307998657}
I0130 17:56:31.660259 140004659541760 logging_writer.py:48] [166981] accumulated_eval_time=1978.183670, accumulated_logging_time=4.996540, accumulated_submission_time=56139.720806, global_step=166981, preemption_count=0, score=56139.720806, test/accuracy=0.624700, test/loss=1.840732, test/num_examples=10000, total_duration=58128.308626, train/accuracy=0.942961, train/loss=0.205668, validation/accuracy=0.750060, validation/loss=1.074691, validation/num_examples=50000
I0130 17:56:38.385886 140004667934464 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.639017581939697, loss=0.7016943693161011
I0130 17:57:11.910521 140004659541760 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.573324680328369, loss=0.6997667551040649
I0130 17:57:45.521130 140004667934464 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.4268364906311035, loss=0.690942108631134
I0130 17:58:19.287168 140004659541760 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.111729145050049, loss=0.8119328022003174
I0130 17:58:52.898575 140004667934464 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.280455112457275, loss=0.6560621857643127
I0130 17:59:26.548069 140004659541760 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.761508941650391, loss=0.6313135623931885
I0130 18:00:00.176839 140004667934464 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.358669757843018, loss=0.7675455808639526
I0130 18:00:33.803016 140004659541760 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.476064682006836, loss=0.732414186000824
I0130 18:01:07.434642 140004667934464 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.668605327606201, loss=0.7320044636726379
I0130 18:01:41.061530 140004659541760 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.384848594665527, loss=0.7520102262496948
I0130 18:02:14.658046 140004667934464 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.305427074432373, loss=0.7161015272140503
I0130 18:02:48.312768 140004659541760 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.680081367492676, loss=0.6520640254020691
I0130 18:03:21.953543 140004667934464 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.287255764007568, loss=0.7144152522087097
I0130 18:03:55.568434 140004659541760 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.50811767578125, loss=0.727401077747345
I0130 18:04:29.216361 140004667934464 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.388319492340088, loss=0.6723927855491638
I0130 18:05:01.691453 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:05:08.138494 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:05:16.783430 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:05:19.502394 140169137129280 submission_runner.py:408] Time since start: 58656.20s, 	Step: 168498, 	{'train/accuracy': 0.9451330900192261, 'train/loss': 0.19239141047000885, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.0681169033050537, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8350611925125122, 'test/num_examples': 10000, 'score': 56649.69009900093, 'total_duration': 58656.19987845421, 'accumulated_submission_time': 56649.69009900093, 'accumulated_eval_time': 1995.9945611953735, 'accumulated_logging_time': 5.058220386505127}
I0130 18:05:19.551413 140005305468672 logging_writer.py:48] [168498] accumulated_eval_time=1995.994561, accumulated_logging_time=5.058220, accumulated_submission_time=56649.690099, global_step=168498, preemption_count=0, score=56649.690099, test/accuracy=0.625700, test/loss=1.835061, test/num_examples=10000, total_duration=58656.199878, train/accuracy=0.945133, train/loss=0.192391, validation/accuracy=0.751200, validation/loss=1.068117, validation/num_examples=50000
I0130 18:05:20.569589 140005313861376 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.1898884773254395, loss=0.6566721200942993
I0130 18:05:54.116088 140005305468672 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.452723979949951, loss=0.6882482171058655
I0130 18:06:27.640601 140005313861376 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.415311336517334, loss=0.5495691299438477
I0130 18:07:01.201625 140005305468672 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.581070899963379, loss=0.667434811592102
I0130 18:07:34.840879 140005313861376 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.678370952606201, loss=0.7470332980155945
I0130 18:08:08.464703 140005305468672 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.244548797607422, loss=0.6371549367904663
I0130 18:08:42.070971 140005313861376 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.298595905303955, loss=0.6793176531791687
I0130 18:09:15.698088 140005305468672 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.311736106872559, loss=0.6888518333435059
I0130 18:09:49.336954 140005313861376 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.009629249572754, loss=0.6994400024414062
I0130 18:10:22.948060 140005305468672 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.481568336486816, loss=0.6562243700027466
I0130 18:10:56.563090 140005313861376 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.114253520965576, loss=0.6024757623672485
I0130 18:11:30.251080 140005305468672 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.758956432342529, loss=0.7527282238006592
I0130 18:12:03.856352 140005313861376 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.059105396270752, loss=0.6759418249130249
I0130 18:12:37.499026 140005305468672 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.990083694458008, loss=0.7732256650924683
I0130 18:13:11.136250 140005313861376 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.673542022705078, loss=0.7201662659645081
I0130 18:13:44.758877 140005305468672 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.540226459503174, loss=0.6763842701911926
I0130 18:13:49.609247 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:13:56.046040 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:14:04.878815 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:14:07.602813 140169137129280 submission_runner.py:408] Time since start: 59184.30s, 	Step: 170016, 	{'train/accuracy': 0.9563735723495483, 'train/loss': 0.16319619119167328, 'validation/accuracy': 0.7527999877929688, 'validation/loss': 1.0671993494033813, 'validation/num_examples': 50000, 'test/accuracy': 0.6259000301361084, 'test/loss': 1.8455358743667603, 'test/num_examples': 10000, 'score': 57159.68464636803, 'total_duration': 59184.30030846596, 'accumulated_submission_time': 57159.68464636803, 'accumulated_eval_time': 2013.9880871772766, 'accumulated_logging_time': 5.121249437332153}
I0130 18:14:07.651454 140005297075968 logging_writer.py:48] [170016] accumulated_eval_time=2013.988087, accumulated_logging_time=5.121249, accumulated_submission_time=57159.684646, global_step=170016, preemption_count=0, score=57159.684646, test/accuracy=0.625900, test/loss=1.845536, test/num_examples=10000, total_duration=59184.300308, train/accuracy=0.956374, train/loss=0.163196, validation/accuracy=0.752800, validation/loss=1.067199, validation/num_examples=50000
I0130 18:14:36.137568 140005330646784 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.277624130249023, loss=0.6537565588951111
I0130 18:15:09.709292 140005297075968 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.067749500274658, loss=0.6082261204719543
I0130 18:15:43.229590 140005330646784 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.56580924987793, loss=0.686894953250885
I0130 18:16:16.751634 140005297075968 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.935953140258789, loss=0.5923596024513245
I0130 18:16:50.304801 140005330646784 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.791982173919678, loss=0.7008244395256042
I0130 18:17:23.919285 140005297075968 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.775913238525391, loss=0.71439129114151
I0130 18:17:57.644574 140005330646784 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.376584529876709, loss=0.6243904829025269
I0130 18:18:31.299802 140005297075968 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.422289848327637, loss=0.7116967439651489
I0130 18:19:04.922744 140005330646784 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.765815258026123, loss=0.6286133527755737
I0130 18:19:38.544310 140005297075968 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.205011367797852, loss=0.67365962266922
I0130 18:20:12.156639 140005330646784 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.437823295593262, loss=0.6745172142982483
I0130 18:20:45.760271 140005297075968 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.288137912750244, loss=0.6652855277061462
I0130 18:21:19.414116 140005330646784 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.541739463806152, loss=0.5950273871421814
I0130 18:21:53.033187 140005297075968 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.748547554016113, loss=0.6201387643814087
I0130 18:22:26.655755 140005330646784 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.30181884765625, loss=0.612493097782135
I0130 18:22:37.907075 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:22:44.519603 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:22:52.969320 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:22:55.728948 140169137129280 submission_runner.py:408] Time since start: 59712.43s, 	Step: 171535, 	{'train/accuracy': 0.9563934803009033, 'train/loss': 0.1602298766374588, 'validation/accuracy': 0.7529199719429016, 'validation/loss': 1.065573811531067, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8335827589035034, 'test/num_examples': 10000, 'score': 57669.88044476509, 'total_duration': 59712.426441669464, 'accumulated_submission_time': 57669.88044476509, 'accumulated_eval_time': 2031.8099205493927, 'accumulated_logging_time': 5.179990291595459}
I0130 18:22:55.779016 140005288683264 logging_writer.py:48] [171535] accumulated_eval_time=2031.809921, accumulated_logging_time=5.179990, accumulated_submission_time=57669.880445, global_step=171535, preemption_count=0, score=57669.880445, test/accuracy=0.629000, test/loss=1.833583, test/num_examples=10000, total_duration=59712.426442, train/accuracy=0.956393, train/loss=0.160230, validation/accuracy=0.752920, validation/loss=1.065574, validation/num_examples=50000
I0130 18:23:17.909676 140005305468672 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.282181739807129, loss=0.577093243598938
I0130 18:23:51.482331 140005288683264 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.7609944343566895, loss=0.6690031886100769
I0130 18:24:25.107048 140005305468672 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.397058010101318, loss=0.6514528393745422
I0130 18:24:58.799611 140005288683264 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.596007823944092, loss=0.6870943903923035
I0130 18:25:32.335723 140005305468672 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.146679878234863, loss=0.588374137878418
I0130 18:26:05.883831 140005288683264 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.589588642120361, loss=0.6361099481582642
I0130 18:26:39.496695 140005305468672 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.58001184463501, loss=0.6439815163612366
I0130 18:27:13.133165 140005288683264 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.565357685089111, loss=0.7158262729644775
I0130 18:27:46.761589 140005305468672 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.881932735443115, loss=0.7033056616783142
I0130 18:28:20.397340 140005288683264 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.485896587371826, loss=0.6334294676780701
I0130 18:28:54.031440 140005305468672 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.484121799468994, loss=0.6531335711479187
I0130 18:29:27.666110 140005288683264 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.314696311950684, loss=0.6558062434196472
I0130 18:30:01.296437 140005305468672 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.18702507019043, loss=0.5714102983474731
I0130 18:30:34.931471 140005288683264 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.930544853210449, loss=0.595526397228241
I0130 18:31:08.633114 140005305468672 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.4371337890625, loss=0.6711649894714355
I0130 18:31:25.930809 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:31:32.311983 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:31:41.029971 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:31:43.753383 140169137129280 submission_runner.py:408] Time since start: 60240.45s, 	Step: 173053, 	{'train/accuracy': 0.9550382494926453, 'train/loss': 0.1676359325647354, 'validation/accuracy': 0.7534399628639221, 'validation/loss': 1.057241439819336, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8320350646972656, 'test/num_examples': 10000, 'score': 58179.970638751984, 'total_duration': 60240.450879096985, 'accumulated_submission_time': 58179.970638751984, 'accumulated_eval_time': 2049.6324560642242, 'accumulated_logging_time': 5.242520332336426}
I0130 18:31:43.803451 140004667934464 logging_writer.py:48] [173053] accumulated_eval_time=2049.632456, accumulated_logging_time=5.242520, accumulated_submission_time=58179.970639, global_step=173053, preemption_count=0, score=58179.970639, test/accuracy=0.627500, test/loss=1.832035, test/num_examples=10000, total_duration=60240.450879, train/accuracy=0.955038, train/loss=0.167636, validation/accuracy=0.753440, validation/loss=1.057241, validation/num_examples=50000
I0130 18:31:59.900016 140004676327168 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.805699348449707, loss=0.6641866564750671
I0130 18:32:33.450827 140004667934464 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.803510665893555, loss=0.6872681379318237
I0130 18:33:07.092502 140004676327168 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.562880039215088, loss=0.6478942632675171
I0130 18:33:40.717325 140004667934464 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.724402904510498, loss=0.7392303943634033
I0130 18:34:14.353628 140004676327168 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.2150373458862305, loss=0.631836473941803
I0130 18:34:47.975442 140004667934464 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.736137866973877, loss=0.6340216994285583
I0130 18:35:21.553634 140004676327168 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.431741237640381, loss=0.6604413986206055
I0130 18:35:55.106427 140004667934464 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.795619487762451, loss=0.6703165173530579
I0130 18:36:28.638994 140004676327168 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.749149799346924, loss=0.6349365711212158
I0130 18:37:02.192663 140004667934464 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.369054317474365, loss=0.5996113419532776
I0130 18:37:35.811828 140004676327168 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.241849899291992, loss=0.5643621683120728
I0130 18:38:09.442789 140004667934464 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.0673298835754395, loss=0.6274868249893188
I0130 18:38:42.972034 140004676327168 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.918501377105713, loss=0.6855537295341492
I0130 18:39:16.539639 140004667934464 logging_writer.py:48] [174400] global_step=174400, grad_norm=5.502566337585449, loss=0.691889226436615
I0130 18:39:50.159811 140004676327168 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.4965314865112305, loss=0.6489036679267883
I0130 18:40:13.842569 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:40:20.903598 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:40:29.385207 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:40:32.091702 140169137129280 submission_runner.py:408] Time since start: 60768.79s, 	Step: 174572, 	{'train/accuracy': 0.9543008208274841, 'train/loss': 0.1634673774242401, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0598605871200562, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.833614706993103, 'test/num_examples': 10000, 'score': 58689.94938135147, 'total_duration': 60768.789187669754, 'accumulated_submission_time': 58689.94938135147, 'accumulated_eval_time': 2067.8815484046936, 'accumulated_logging_time': 5.303300857543945}
I0130 18:40:32.141222 140004676327168 logging_writer.py:48] [174572] accumulated_eval_time=2067.881548, accumulated_logging_time=5.303301, accumulated_submission_time=58689.949381, global_step=174572, preemption_count=0, score=58689.949381, test/accuracy=0.630400, test/loss=1.833615, test/num_examples=10000, total_duration=60768.789188, train/accuracy=0.954301, train/loss=0.163467, validation/accuracy=0.754900, validation/loss=1.059861, validation/num_examples=50000
I0130 18:40:41.897284 140005313861376 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.7104339599609375, loss=0.6389762759208679
I0130 18:41:15.483999 140004676327168 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.201473712921143, loss=0.6571947336196899
I0130 18:41:49.125645 140005313861376 logging_writer.py:48] [174800] global_step=174800, grad_norm=4.294554233551025, loss=0.641258180141449
I0130 18:42:22.758257 140004676327168 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.393209934234619, loss=0.6487476825714111
I0130 18:42:56.370956 140005313861376 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.830477714538574, loss=0.7150998115539551
I0130 18:43:30.015447 140004676327168 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.536861896514893, loss=0.6402955651283264
I0130 18:44:03.647810 140005313861376 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.536063194274902, loss=0.6877639293670654
I0130 18:44:37.328593 140004676327168 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.555100917816162, loss=0.6368759870529175
I0130 18:45:10.871618 140005313861376 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.843366622924805, loss=0.6965122222900391
I0130 18:45:44.416537 140004676327168 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.578621864318848, loss=0.6078560948371887
I0130 18:46:18.006115 140005313861376 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.328394889831543, loss=0.6212615966796875
I0130 18:46:51.642934 140004676327168 logging_writer.py:48] [175700] global_step=175700, grad_norm=5.228964328765869, loss=0.7221722602844238
I0130 18:47:25.265295 140005313861376 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.874781131744385, loss=0.7375534772872925
I0130 18:47:58.894143 140004676327168 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.357348918914795, loss=0.7040444612503052
I0130 18:48:32.518993 140005313861376 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.35974645614624, loss=0.6099901795387268
I0130 18:49:02.224496 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:49:08.653548 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:49:17.129022 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:49:19.859853 140169137129280 submission_runner.py:408] Time since start: 61296.56s, 	Step: 176090, 	{'train/accuracy': 0.9560347199440002, 'train/loss': 0.16032513976097107, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0561996698379517, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.832032561302185, 'test/num_examples': 10000, 'score': 59199.97342252731, 'total_duration': 61296.55711436272, 'accumulated_submission_time': 59199.97342252731, 'accumulated_eval_time': 2085.516634464264, 'accumulated_logging_time': 5.3632285594940186}
I0130 18:49:19.910222 140005297075968 logging_writer.py:48] [176090] accumulated_eval_time=2085.516634, accumulated_logging_time=5.363229, accumulated_submission_time=59199.973423, global_step=176090, preemption_count=0, score=59199.973423, test/accuracy=0.630200, test/loss=1.832033, test/num_examples=10000, total_duration=61296.557114, train/accuracy=0.956035, train/loss=0.160325, validation/accuracy=0.756040, validation/loss=1.056200, validation/num_examples=50000
I0130 18:49:23.631064 140005305468672 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.711949825286865, loss=0.6474534273147583
I0130 18:49:57.162123 140005297075968 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.624049663543701, loss=0.6656644344329834
I0130 18:50:30.721551 140005305468672 logging_writer.py:48] [176300] global_step=176300, grad_norm=3.9061553478240967, loss=0.6095506548881531
I0130 18:51:04.401510 140005297075968 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.666757106781006, loss=0.5985058546066284
I0130 18:51:38.023856 140005305468672 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.549137115478516, loss=0.643271803855896
I0130 18:52:11.599854 140005297075968 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.372125148773193, loss=0.5985719561576843
I0130 18:52:45.245848 140005305468672 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.524633407592773, loss=0.631621778011322
I0130 18:53:18.882180 140005297075968 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.512958526611328, loss=0.6497771739959717
I0130 18:53:52.508081 140005305468672 logging_writer.py:48] [176900] global_step=176900, grad_norm=5.540443420410156, loss=0.6765971183776855
I0130 18:54:26.108250 140005297075968 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.328711986541748, loss=0.6656056642532349
I0130 18:54:59.756344 140005305468672 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.568686008453369, loss=0.6335165500640869
I0130 18:55:33.395793 140005297075968 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.496565818786621, loss=0.6832261085510254
I0130 18:56:07.015748 140005305468672 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.291744232177734, loss=0.6112087368965149
I0130 18:56:40.655744 140005297075968 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.525286674499512, loss=0.6487252116203308
I0130 18:57:14.287830 140005305468672 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.8333845138549805, loss=0.6322166919708252
I0130 18:57:47.952337 140005297075968 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.782212257385254, loss=0.6221290826797485
I0130 18:57:50.122563 140169137129280 spec.py:321] Evaluating on the training split.
I0130 18:57:56.551938 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 18:58:05.321831 140169137129280 spec.py:349] Evaluating on the test split.
I0130 18:58:08.004197 140169137129280 submission_runner.py:408] Time since start: 61824.70s, 	Step: 177608, 	{'train/accuracy': 0.9580675959587097, 'train/loss': 0.1550871878862381, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0575575828552246, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8350627422332764, 'test/num_examples': 10000, 'score': 59710.124698638916, 'total_duration': 61824.701684474945, 'accumulated_submission_time': 59710.124698638916, 'accumulated_eval_time': 2103.398220539093, 'accumulated_logging_time': 5.4248974323272705}
I0130 18:58:08.055428 140004676327168 logging_writer.py:48] [177608] accumulated_eval_time=2103.398221, accumulated_logging_time=5.424897, accumulated_submission_time=59710.124699, global_step=177608, preemption_count=0, score=59710.124699, test/accuracy=0.630400, test/loss=1.835063, test/num_examples=10000, total_duration=61824.701684, train/accuracy=0.958068, train/loss=0.155087, validation/accuracy=0.754840, validation/loss=1.057558, validation/num_examples=50000
I0130 18:58:39.238335 140005288683264 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.576390743255615, loss=0.6478479504585266
I0130 18:59:12.796911 140004676327168 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.336284160614014, loss=0.6731354594230652
I0130 18:59:46.402350 140005288683264 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.376255035400391, loss=0.6495756506919861
I0130 19:00:20.012292 140004676327168 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.849420547485352, loss=0.6715880632400513
I0130 19:00:53.654509 140005288683264 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.411768436431885, loss=0.6326177716255188
I0130 19:01:27.287647 140004676327168 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.780887603759766, loss=0.6718045473098755
I0130 19:02:00.890152 140005288683264 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.926755428314209, loss=0.6999313831329346
I0130 19:02:34.540123 140004676327168 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.198663711547852, loss=0.5886856317520142
I0130 19:03:08.164499 140005288683264 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.930350303649902, loss=0.6836279034614563
I0130 19:03:41.731483 140004676327168 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.721099376678467, loss=0.5772560238838196
I0130 19:04:15.365939 140005288683264 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.319908142089844, loss=0.6188924908638
I0130 19:04:48.895488 140004676327168 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.51975154876709, loss=0.5971605777740479
I0130 19:05:22.435497 140005288683264 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.450410842895508, loss=0.6238265037536621
I0130 19:05:56.026870 140004676327168 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.357520580291748, loss=0.6164605617523193
I0130 19:06:29.662291 140005288683264 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.641730785369873, loss=0.6554979681968689
I0130 19:06:38.214318 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:06:44.565373 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:06:53.051857 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:06:55.870639 140169137129280 submission_runner.py:408] Time since start: 62352.57s, 	Step: 179127, 	{'train/accuracy': 0.9602997303009033, 'train/loss': 0.14733432233333588, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.0583137273788452, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8351722955703735, 'test/num_examples': 10000, 'score': 60220.22158074379, 'total_duration': 62352.568135261536, 'accumulated_submission_time': 60220.22158074379, 'accumulated_eval_time': 2121.0545020103455, 'accumulated_logging_time': 5.489213466644287}
I0130 19:06:55.921646 140004676327168 logging_writer.py:48] [179127] accumulated_eval_time=2121.054502, accumulated_logging_time=5.489213, accumulated_submission_time=60220.221581, global_step=179127, preemption_count=0, score=60220.221581, test/accuracy=0.629800, test/loss=1.835172, test/num_examples=10000, total_duration=62352.568135, train/accuracy=0.960300, train/loss=0.147334, validation/accuracy=0.754280, validation/loss=1.058314, validation/num_examples=50000
I0130 19:07:20.739291 140005305468672 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.328329086303711, loss=0.7002049684524536
I0130 19:07:54.275237 140004676327168 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.5866169929504395, loss=0.584294855594635
I0130 19:08:27.861489 140005305468672 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.613016128540039, loss=0.6677722334861755
I0130 19:09:01.488055 140004676327168 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.34220552444458, loss=0.6244595050811768
I0130 19:09:35.110614 140005305468672 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.50850248336792, loss=0.6409069299697876
I0130 19:10:08.743970 140004676327168 logging_writer.py:48] [179700] global_step=179700, grad_norm=4.440589904785156, loss=0.5880321264266968
I0130 19:10:42.420777 140005305468672 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.516007423400879, loss=0.6058162450790405
I0130 19:11:15.955815 140004676327168 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.7325215339660645, loss=0.680316686630249
I0130 19:11:49.504257 140005305468672 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.438186168670654, loss=0.6099335551261902
I0130 19:12:23.129139 140004676327168 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.868601322174072, loss=0.6213160753250122
I0130 19:12:56.776009 140005305468672 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.589658737182617, loss=0.7150475382804871
I0130 19:13:30.428701 140004676327168 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.555829048156738, loss=0.6366838812828064
I0130 19:14:04.052306 140005305468672 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.537146091461182, loss=0.582980215549469
I0130 19:14:37.684396 140004676327168 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.677412509918213, loss=0.5795647501945496
I0130 19:15:11.323183 140005305468672 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.995387554168701, loss=0.6743619441986084
I0130 19:15:25.945965 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:15:32.345940 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:15:40.911130 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:15:43.645645 140169137129280 submission_runner.py:408] Time since start: 62880.34s, 	Step: 180645, 	{'train/accuracy': 0.9606983065605164, 'train/loss': 0.1453530639410019, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.055141806602478, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8299227952957153, 'test/num_examples': 10000, 'score': 60730.1836707592, 'total_duration': 62880.34313893318, 'accumulated_submission_time': 60730.1836707592, 'accumulated_eval_time': 2138.7541739940643, 'accumulated_logging_time': 5.5535314083099365}
I0130 19:15:43.694893 140005313861376 logging_writer.py:48] [180645] accumulated_eval_time=2138.754174, accumulated_logging_time=5.553531, accumulated_submission_time=60730.183671, global_step=180645, preemption_count=0, score=60730.183671, test/accuracy=0.630600, test/loss=1.829923, test/num_examples=10000, total_duration=62880.343139, train/accuracy=0.960698, train/loss=0.145353, validation/accuracy=0.755640, validation/loss=1.055142, validation/num_examples=50000
I0130 19:16:02.530216 140005330646784 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.6401686668396, loss=0.5775487422943115
I0130 19:16:36.150549 140005313861376 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.202494144439697, loss=0.6135435104370117
I0130 19:17:09.856383 140005330646784 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.568737506866455, loss=0.6677390933036804
I0130 19:17:43.378644 140005313861376 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.740600109100342, loss=0.6476151347160339
I0130 19:18:16.991625 140005330646784 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.324245452880859, loss=0.6349625587463379
I0130 19:18:50.582852 140005313861376 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.641188621520996, loss=0.6068830490112305
I0130 19:19:24.217156 140005330646784 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.554165840148926, loss=0.6559332609176636
I0130 19:19:57.863109 140005313861376 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.359283924102783, loss=0.5801315307617188
I0130 19:20:31.483670 140005330646784 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.389636039733887, loss=0.6631407737731934
I0130 19:21:05.106015 140005313861376 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.752460479736328, loss=0.5706309676170349
I0130 19:21:38.737538 140005330646784 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.194207668304443, loss=0.6292343139648438
I0130 19:22:12.367213 140005313861376 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.808812618255615, loss=0.6602153778076172
I0130 19:22:46.004860 140005330646784 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.216314792633057, loss=0.5803170204162598
I0130 19:23:19.648418 140005313861376 logging_writer.py:48] [182000] global_step=182000, grad_norm=5.129227161407471, loss=0.6756282448768616
I0130 19:23:53.324526 140005330646784 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.376479625701904, loss=0.6337171792984009
I0130 19:24:13.934639 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:24:20.392883 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:24:29.108133 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:24:31.822261 140169137129280 submission_runner.py:408] Time since start: 63408.52s, 	Step: 182163, 	{'train/accuracy': 0.959004282951355, 'train/loss': 0.14809420704841614, 'validation/accuracy': 0.7565400004386902, 'validation/loss': 1.0530595779418945, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.828680157661438, 'test/num_examples': 10000, 'score': 61240.363770484924, 'total_duration': 63408.5197532177, 'accumulated_submission_time': 61240.363770484924, 'accumulated_eval_time': 2156.6417529582977, 'accumulated_logging_time': 5.613237142562866}
I0130 19:24:31.873548 140005288683264 logging_writer.py:48] [182163] accumulated_eval_time=2156.641753, accumulated_logging_time=5.613237, accumulated_submission_time=61240.363770, global_step=182163, preemption_count=0, score=61240.363770, test/accuracy=0.630300, test/loss=1.828680, test/num_examples=10000, total_duration=63408.519753, train/accuracy=0.959004, train/loss=0.148094, validation/accuracy=0.756540, validation/loss=1.053060, validation/num_examples=50000
I0130 19:24:44.624078 140005297075968 logging_writer.py:48] [182200] global_step=182200, grad_norm=5.218476295471191, loss=0.6271939277648926
I0130 19:25:18.178964 140005288683264 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.586573123931885, loss=0.6414841413497925
I0130 19:25:51.819330 140005297075968 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.8625874519348145, loss=0.6117109060287476
I0130 19:26:25.437542 140005288683264 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.419511318206787, loss=0.6132530570030212
I0130 19:26:59.026446 140005297075968 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.785720348358154, loss=0.6271688342094421
I0130 19:27:32.655783 140005288683264 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.242558479309082, loss=0.6358813047409058
I0130 19:28:06.242563 140005297075968 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.938695907592773, loss=0.6382275819778442
I0130 19:28:39.862700 140005288683264 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.393326282501221, loss=0.6072759032249451
I0130 19:29:13.512605 140005297075968 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.0668044090271, loss=0.6107155084609985
I0130 19:29:47.133650 140005288683264 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.4194655418396, loss=0.57012939453125
I0130 19:30:20.872216 140005297075968 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.12516450881958, loss=0.5700762867927551
I0130 19:30:54.511077 140005288683264 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.780280590057373, loss=0.6480717062950134
I0130 19:31:28.145022 140005297075968 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.814105987548828, loss=0.6562469005584717
I0130 19:32:01.759757 140005288683264 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.346635341644287, loss=0.6071697473526001
I0130 19:32:35.402658 140005297075968 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.349888801574707, loss=0.5913713574409485
I0130 19:33:02.112776 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:33:08.510272 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:33:16.977099 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:33:19.742821 140169137129280 submission_runner.py:408] Time since start: 63936.44s, 	Step: 183681, 	{'train/accuracy': 0.9598413109779358, 'train/loss': 0.150454580783844, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0534089803695679, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8285574913024902, 'test/num_examples': 10000, 'score': 61750.54144191742, 'total_duration': 63936.44031405449, 'accumulated_submission_time': 61750.54144191742, 'accumulated_eval_time': 2174.271763563156, 'accumulated_logging_time': 5.677387714385986}
I0130 19:33:19.796764 140004676327168 logging_writer.py:48] [183681] accumulated_eval_time=2174.271764, accumulated_logging_time=5.677388, accumulated_submission_time=61750.541442, global_step=183681, preemption_count=0, score=61750.541442, test/accuracy=0.630800, test/loss=1.828557, test/num_examples=10000, total_duration=63936.440314, train/accuracy=0.959841, train/loss=0.150455, validation/accuracy=0.756000, validation/loss=1.053409, validation/num_examples=50000
I0130 19:33:26.511447 140005313861376 logging_writer.py:48] [183700] global_step=183700, grad_norm=3.9961345195770264, loss=0.49574363231658936
I0130 19:34:00.086092 140004676327168 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.045462131500244, loss=0.6283883452415466
I0130 19:34:33.692802 140005313861376 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.538809776306152, loss=0.6290708780288696
I0130 19:35:07.337198 140004676327168 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.800845146179199, loss=0.5832996368408203
I0130 19:35:40.969759 140005313861376 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.294180393218994, loss=0.7222405672073364
I0130 19:36:14.583043 140004676327168 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.569041728973389, loss=0.612565279006958
I0130 19:36:48.292745 140005313861376 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.599689960479736, loss=0.7547589540481567
I0130 19:37:21.926452 140004676327168 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.385499477386475, loss=0.6091923713684082
I0130 19:37:55.569053 140005313861376 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.544567108154297, loss=0.5734937191009521
I0130 19:38:29.192220 140004676327168 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.493948936462402, loss=0.6308574676513672
I0130 19:39:02.810007 140005313861376 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.646603107452393, loss=0.6145622730255127
I0130 19:39:36.456567 140004676327168 logging_writer.py:48] [184800] global_step=184800, grad_norm=4.542181968688965, loss=0.6083688735961914
I0130 19:40:10.092071 140005313861376 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.210984706878662, loss=0.6210393905639648
I0130 19:40:43.740479 140004676327168 logging_writer.py:48] [185000] global_step=185000, grad_norm=4.904720783233643, loss=0.6581998467445374
I0130 19:41:17.327575 140005313861376 logging_writer.py:48] [185100] global_step=185100, grad_norm=4.36657190322876, loss=0.6115645170211792
I0130 19:41:49.772677 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:41:56.264279 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:42:04.967912 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:42:07.683609 140169137129280 submission_runner.py:408] Time since start: 64464.38s, 	Step: 185198, 	{'train/accuracy': 0.9613759517669678, 'train/loss': 0.14299754798412323, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.053418755531311, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8302977085113525, 'test/num_examples': 10000, 'score': 62260.458035469055, 'total_duration': 64464.38110399246, 'accumulated_submission_time': 62260.458035469055, 'accumulated_eval_time': 2192.1826646327972, 'accumulated_logging_time': 5.741932153701782}
I0130 19:42:07.734265 140004667934464 logging_writer.py:48] [185198] accumulated_eval_time=2192.182665, accumulated_logging_time=5.741932, accumulated_submission_time=62260.458035, global_step=185198, preemption_count=0, score=62260.458035, test/accuracy=0.630500, test/loss=1.830298, test/num_examples=10000, total_duration=64464.381104, train/accuracy=0.961376, train/loss=0.142998, validation/accuracy=0.755540, validation/loss=1.053419, validation/num_examples=50000
I0130 19:42:08.787448 140004676327168 logging_writer.py:48] [185200] global_step=185200, grad_norm=4.334672451019287, loss=0.6466571092605591
I0130 19:42:42.367847 140004667934464 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.342020034790039, loss=0.5722514986991882
I0130 19:43:16.080494 140004676327168 logging_writer.py:48] [185400] global_step=185400, grad_norm=4.4765448570251465, loss=0.6108711361885071
I0130 19:43:49.621075 140004667934464 logging_writer.py:48] [185500] global_step=185500, grad_norm=4.978113651275635, loss=0.6401161551475525
I0130 19:44:23.185961 140004676327168 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.549374580383301, loss=0.6249332427978516
I0130 19:44:56.795164 140004667934464 logging_writer.py:48] [185700] global_step=185700, grad_norm=4.449805736541748, loss=0.6515098810195923
I0130 19:45:30.405702 140004676327168 logging_writer.py:48] [185800] global_step=185800, grad_norm=5.369877815246582, loss=0.6760565042495728
I0130 19:46:04.018818 140004667934464 logging_writer.py:48] [185900] global_step=185900, grad_norm=4.366780757904053, loss=0.6569586992263794
I0130 19:46:37.638314 140004676327168 logging_writer.py:48] [186000] global_step=186000, grad_norm=3.961629629135132, loss=0.6419590711593628
I0130 19:47:11.253789 140004667934464 logging_writer.py:48] [186100] global_step=186100, grad_norm=5.337819576263428, loss=0.7218009233474731
I0130 19:47:44.877615 140004676327168 logging_writer.py:48] [186200] global_step=186200, grad_norm=4.358819484710693, loss=0.5913061499595642
I0130 19:48:18.508538 140004667934464 logging_writer.py:48] [186300] global_step=186300, grad_norm=4.436020374298096, loss=0.6412943005561829
I0130 19:48:52.119107 140004676327168 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.632381439208984, loss=0.6356807351112366
I0130 19:49:25.731491 140004667934464 logging_writer.py:48] [186500] global_step=186500, grad_norm=4.879673957824707, loss=0.657075047492981
I0130 19:49:59.430408 140004676327168 logging_writer.py:48] [186600] global_step=186600, grad_norm=4.241906642913818, loss=0.5681109428405762
I0130 19:50:21.090767 140169137129280 spec.py:321] Evaluating on the training split.
I0130 19:50:27.571451 140169137129280 spec.py:333] Evaluating on the validation split.
I0130 19:50:36.260238 140169137129280 spec.py:349] Evaluating on the test split.
I0130 19:50:38.985828 140169137129280 submission_runner.py:408] Time since start: 64975.68s, 	Step: 186666, 	{'train/accuracy': 0.9604591727256775, 'train/loss': 0.14778749644756317, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0542693138122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.83073890209198, 'test/num_examples': 10000, 'score': 62753.75569033623, 'total_duration': 64975.68322634697, 'accumulated_submission_time': 62753.75569033623, 'accumulated_eval_time': 2210.0775923728943, 'accumulated_logging_time': 5.804227828979492}
I0130 19:50:39.039576 140004659541760 logging_writer.py:48] [186666] accumulated_eval_time=2210.077592, accumulated_logging_time=5.804228, accumulated_submission_time=62753.755690, global_step=186666, preemption_count=0, score=62753.755690, test/accuracy=0.630900, test/loss=1.830739, test/num_examples=10000, total_duration=64975.683226, train/accuracy=0.960459, train/loss=0.147787, validation/accuracy=0.756200, validation/loss=1.054269, validation/num_examples=50000
I0130 19:50:39.087422 140004667934464 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62753.755690
I0130 19:50:39.414694 140169137129280 checkpoints.py:490] Saving checkpoint at step: 186666
I0130 19:50:40.567111 140169137129280 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5/checkpoint_186666
I0130 19:50:40.593641 140169137129280 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_resnet_jax/trial_5/checkpoint_186666.
I0130 19:50:41.307343 140169137129280 submission_runner.py:583] Tuning trial 5/5
I0130 19:50:41.307569 140169137129280 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0130 19:50:41.316792 140169137129280 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007772640092298388, 'train/loss': 6.909999847412109, 'validation/accuracy': 0.0009599999757483602, 'validation/loss': 6.910243988037109, 'validation/num_examples': 50000, 'test/accuracy': 0.0006000000284984708, 'test/loss': 6.910250186920166, 'test/num_examples': 10000, 'score': 34.15979290008545, 'total_duration': 52.07652568817139, 'accumulated_submission_time': 34.15979290008545, 'accumulated_eval_time': 17.916585683822632, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1514, {'train/accuracy': 0.1859654039144516, 'train/loss': 4.123931407928467, 'validation/accuracy': 0.1693599969148636, 'validation/loss': 4.246126651763916, 'validation/num_examples': 50000, 'test/accuracy': 0.11990000307559967, 'test/loss': 4.764464855194092, 'test/num_examples': 10000, 'score': 544.2960352897644, 'total_duration': 579.9500658512115, 'accumulated_submission_time': 544.2960352897644, 'accumulated_eval_time': 35.58520984649658, 'accumulated_logging_time': 0.019943952560424805, 'global_step': 1514, 'preemption_count': 0}), (3027, {'train/accuracy': 0.3240991532802582, 'train/loss': 3.1349141597747803, 'validation/accuracy': 0.30215999484062195, 'validation/loss': 3.293057441711426, 'validation/num_examples': 50000, 'test/accuracy': 0.22670000791549683, 'test/loss': 3.9505279064178467, 'test/num_examples': 10000, 'score': 1054.3922047615051, 'total_duration': 1107.8278141021729, 'accumulated_submission_time': 1054.3922047615051, 'accumulated_eval_time': 53.290247440338135, 'accumulated_logging_time': 0.047429561614990234, 'global_step': 3027, 'preemption_count': 0}), (4541, {'train/accuracy': 0.5005580186843872, 'train/loss': 2.1658248901367188, 'validation/accuracy': 0.4260199964046478, 'validation/loss': 2.557955503463745, 'validation/num_examples': 50000, 'test/accuracy': 0.3175000250339508, 'test/loss': 3.3052544593811035, 'test/num_examples': 10000, 'score': 1564.3699560165405, 'total_duration': 1635.4321525096893, 'accumulated_submission_time': 1564.3699560165405, 'accumulated_eval_time': 70.83682298660278, 'accumulated_logging_time': 0.07883095741271973, 'global_step': 4541, 'preemption_count': 0}), (6057, {'train/accuracy': 0.554109513759613, 'train/loss': 1.864499568939209, 'validation/accuracy': 0.5052399635314941, 'validation/loss': 2.1542959213256836, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.921358346939087, 'test/num_examples': 10000, 'score': 2074.408198595047, 'total_duration': 2163.3676924705505, 'accumulated_submission_time': 2074.408198595047, 'accumulated_eval_time': 88.65340995788574, 'accumulated_logging_time': 0.11072349548339844, 'global_step': 6057, 'preemption_count': 0}), (7574, {'train/accuracy': 0.5734215378761292, 'train/loss': 1.780144214630127, 'validation/accuracy': 0.5266799926757812, 'validation/loss': 2.0440306663513184, 'validation/num_examples': 50000, 'test/accuracy': 0.4020000100135803, 'test/loss': 2.817870855331421, 'test/num_examples': 10000, 'score': 2584.638957977295, 'total_duration': 2691.767830848694, 'accumulated_submission_time': 2584.638957977295, 'accumulated_eval_time': 106.74323630332947, 'accumulated_logging_time': 0.14153456687927246, 'global_step': 7574, 'preemption_count': 0}), (9090, {'train/accuracy': 0.5981544852256775, 'train/loss': 1.6540687084197998, 'validation/accuracy': 0.5498999953269958, 'validation/loss': 1.9097604751586914, 'validation/num_examples': 50000, 'test/accuracy': 0.42760002613067627, 'test/loss': 2.6754543781280518, 'test/num_examples': 10000, 'score': 3094.6172511577606, 'total_duration': 3219.9098541736603, 'accumulated_submission_time': 3094.6172511577606, 'accumulated_eval_time': 124.82778286933899, 'accumulated_logging_time': 0.17210912704467773, 'global_step': 9090, 'preemption_count': 0}), (10607, {'train/accuracy': 0.6220703125, 'train/loss': 1.550428032875061, 'validation/accuracy': 0.5730400085449219, 'validation/loss': 1.805853247642517, 'validation/num_examples': 50000, 'test/accuracy': 0.44300001859664917, 'test/loss': 2.5722270011901855, 'test/num_examples': 10000, 'score': 3604.871292591095, 'total_duration': 3748.189495563507, 'accumulated_submission_time': 3604.871292591095, 'accumulated_eval_time': 142.7768428325653, 'accumulated_logging_time': 0.1999351978302002, 'global_step': 10607, 'preemption_count': 0}), (12124, {'train/accuracy': 0.6409637928009033, 'train/loss': 1.4560710191726685, 'validation/accuracy': 0.5910199880599976, 'validation/loss': 1.7010043859481812, 'validation/num_examples': 50000, 'test/accuracy': 0.4637000262737274, 'test/loss': 2.4511096477508545, 'test/num_examples': 10000, 'score': 4114.898060321808, 'total_duration': 4275.873243093491, 'accumulated_submission_time': 4114.898060321808, 'accumulated_eval_time': 160.3536171913147, 'accumulated_logging_time': 0.23119378089904785, 'global_step': 12124, 'preemption_count': 0}), (13641, {'train/accuracy': 0.684988796710968, 'train/loss': 1.2643846273422241, 'validation/accuracy': 0.5950599908828735, 'validation/loss': 1.6960663795471191, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.450937509536743, 'test/num_examples': 10000, 'score': 4624.97420334816, 'total_duration': 4803.666056632996, 'accumulated_submission_time': 4624.97420334816, 'accumulated_eval_time': 177.99181604385376, 'accumulated_logging_time': 0.2608301639556885, 'global_step': 13641, 'preemption_count': 0}), (15157, {'train/accuracy': 0.671894907951355, 'train/loss': 1.3063687086105347, 'validation/accuracy': 0.6003199815750122, 'validation/loss': 1.6621453762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.469400018453598, 'test/loss': 2.4176716804504395, 'test/num_examples': 10000, 'score': 5134.960388422012, 'total_duration': 5331.829082727432, 'accumulated_submission_time': 5134.960388422012, 'accumulated_eval_time': 196.0883026123047, 'accumulated_logging_time': 0.2926368713378906, 'global_step': 15157, 'preemption_count': 0}), (16674, {'train/accuracy': 0.6599569320678711, 'train/loss': 1.3636835813522339, 'validation/accuracy': 0.5981000065803528, 'validation/loss': 1.6931911706924438, 'validation/num_examples': 50000, 'test/accuracy': 0.4724000096321106, 'test/loss': 2.433629035949707, 'test/num_examples': 10000, 'score': 5644.989506959915, 'total_duration': 5859.929432630539, 'accumulated_submission_time': 5644.989506959915, 'accumulated_eval_time': 214.07593870162964, 'accumulated_logging_time': 0.32698750495910645, 'global_step': 16674, 'preemption_count': 0}), (18191, {'train/accuracy': 0.6560705900192261, 'train/loss': 1.3798298835754395, 'validation/accuracy': 0.5999000072479248, 'validation/loss': 1.6763943433761597, 'validation/num_examples': 50000, 'test/accuracy': 0.47350001335144043, 'test/loss': 2.4417972564697266, 'test/num_examples': 10000, 'score': 6154.933459997177, 'total_duration': 6387.788614749908, 'accumulated_submission_time': 6154.933459997177, 'accumulated_eval_time': 231.9118676185608, 'accumulated_logging_time': 0.35696887969970703, 'global_step': 18191, 'preemption_count': 0}), (19708, {'train/accuracy': 0.6623684763908386, 'train/loss': 1.3440089225769043, 'validation/accuracy': 0.6084399819374084, 'validation/loss': 1.626311182975769, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.366896867752075, 'test/num_examples': 10000, 'score': 6665.012505054474, 'total_duration': 6916.881340265274, 'accumulated_submission_time': 6665.012505054474, 'accumulated_eval_time': 250.8443946838379, 'accumulated_logging_time': 0.3887760639190674, 'global_step': 19708, 'preemption_count': 0}), (21225, {'train/accuracy': 0.6512874364852905, 'train/loss': 1.397564172744751, 'validation/accuracy': 0.6033200025558472, 'validation/loss': 1.6574549674987793, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.4436450004577637, 'test/num_examples': 10000, 'score': 7175.144921779633, 'total_duration': 7444.737069368362, 'accumulated_submission_time': 7175.144921779633, 'accumulated_eval_time': 268.487957239151, 'accumulated_logging_time': 0.41996049880981445, 'global_step': 21225, 'preemption_count': 0}), (22742, {'train/accuracy': 0.6950533986091614, 'train/loss': 1.1915576457977295, 'validation/accuracy': 0.6091399788856506, 'validation/loss': 1.6365070343017578, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.4041829109191895, 'test/num_examples': 10000, 'score': 7685.214646100998, 'total_duration': 7972.439017057419, 'accumulated_submission_time': 7685.214646100998, 'accumulated_eval_time': 286.0397162437439, 'accumulated_logging_time': 0.4515516757965088, 'global_step': 22742, 'preemption_count': 0}), (24260, {'train/accuracy': 0.6743263602256775, 'train/loss': 1.2909504175186157, 'validation/accuracy': 0.6072799563407898, 'validation/loss': 1.6269196271896362, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.375473737716675, 'test/num_examples': 10000, 'score': 8195.194686412811, 'total_duration': 8500.469371795654, 'accumulated_submission_time': 8195.194686412811, 'accumulated_eval_time': 304.0058841705322, 'accumulated_logging_time': 0.4865305423736572, 'global_step': 24260, 'preemption_count': 0}), (25777, {'train/accuracy': 0.6813815236091614, 'train/loss': 1.2523821592330933, 'validation/accuracy': 0.6219599843025208, 'validation/loss': 1.5588935613632202, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.291343927383423, 'test/num_examples': 10000, 'score': 8705.145479679108, 'total_duration': 9028.483579874039, 'accumulated_submission_time': 8705.145479679108, 'accumulated_eval_time': 321.99264454841614, 'accumulated_logging_time': 0.5148470401763916, 'global_step': 25777, 'preemption_count': 0}), (27294, {'train/accuracy': 0.6639030575752258, 'train/loss': 1.3441171646118164, 'validation/accuracy': 0.6081399917602539, 'validation/loss': 1.6323024034500122, 'validation/num_examples': 50000, 'test/accuracy': 0.4772000312805176, 'test/loss': 2.400562047958374, 'test/num_examples': 10000, 'score': 9215.205620288849, 'total_duration': 9556.281441688538, 'accumulated_submission_time': 9215.205620288849, 'accumulated_eval_time': 339.64798951148987, 'accumulated_logging_time': 0.5484886169433594, 'global_step': 27294, 'preemption_count': 0}), (28812, {'train/accuracy': 0.6672313213348389, 'train/loss': 1.3261438608169556, 'validation/accuracy': 0.6168999671936035, 'validation/loss': 1.6034150123596191, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.371846914291382, 'test/num_examples': 10000, 'score': 9725.125751495361, 'total_duration': 10084.276794195175, 'accumulated_submission_time': 9725.125751495361, 'accumulated_eval_time': 357.6402759552002, 'accumulated_logging_time': 0.5823171138763428, 'global_step': 28812, 'preemption_count': 0}), (30329, {'train/accuracy': 0.6734893321990967, 'train/loss': 1.2919286489486694, 'validation/accuracy': 0.610260009765625, 'validation/loss': 1.616947889328003, 'validation/num_examples': 50000, 'test/accuracy': 0.4878000319004059, 'test/loss': 2.3278603553771973, 'test/num_examples': 10000, 'score': 10235.195373535156, 'total_duration': 10612.255376338959, 'accumulated_submission_time': 10235.195373535156, 'accumulated_eval_time': 375.4659821987152, 'accumulated_logging_time': 0.6163196563720703, 'global_step': 30329, 'preemption_count': 0}), (31847, {'train/accuracy': 0.7060347199440002, 'train/loss': 1.1288166046142578, 'validation/accuracy': 0.6247999668121338, 'validation/loss': 1.5537689924240112, 'validation/num_examples': 50000, 'test/accuracy': 0.499500036239624, 'test/loss': 2.2625062465667725, 'test/num_examples': 10000, 'score': 10745.252250671387, 'total_duration': 11140.534215211868, 'accumulated_submission_time': 10745.252250671387, 'accumulated_eval_time': 393.60303115844727, 'accumulated_logging_time': 0.6519203186035156, 'global_step': 31847, 'preemption_count': 0}), (33365, {'train/accuracy': 0.6925222873687744, 'train/loss': 1.2043423652648926, 'validation/accuracy': 0.6267799735069275, 'validation/loss': 1.553866982460022, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.2885379791259766, 'test/num_examples': 10000, 'score': 11255.309514045715, 'total_duration': 11668.54907798767, 'accumulated_submission_time': 11255.309514045715, 'accumulated_eval_time': 411.4780659675598, 'accumulated_logging_time': 0.6850986480712891, 'global_step': 33365, 'preemption_count': 0}), (34883, {'train/accuracy': 0.6701012253761292, 'train/loss': 1.305039405822754, 'validation/accuracy': 0.6144799590110779, 'validation/loss': 1.6147181987762451, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.387573003768921, 'test/num_examples': 10000, 'score': 11765.304506778717, 'total_duration': 12196.398695707321, 'accumulated_submission_time': 11765.304506778717, 'accumulated_eval_time': 429.24707651138306, 'accumulated_logging_time': 0.7219088077545166, 'global_step': 34883, 'preemption_count': 0}), (36401, {'train/accuracy': 0.6705994606018066, 'train/loss': 1.3002781867980957, 'validation/accuracy': 0.6158999800682068, 'validation/loss': 1.5967798233032227, 'validation/num_examples': 50000, 'test/accuracy': 0.480400025844574, 'test/loss': 2.386909008026123, 'test/num_examples': 10000, 'score': 12275.594941139221, 'total_duration': 12724.276602745056, 'accumulated_submission_time': 12275.594941139221, 'accumulated_eval_time': 446.7454869747162, 'accumulated_logging_time': 0.7614481449127197, 'global_step': 36401, 'preemption_count': 0}), (37918, {'train/accuracy': 0.6964285373687744, 'train/loss': 1.1979633569717407, 'validation/accuracy': 0.6372399926185608, 'validation/loss': 1.4970052242279053, 'validation/num_examples': 50000, 'test/accuracy': 0.5107000470161438, 'test/loss': 2.2210662364959717, 'test/num_examples': 10000, 'score': 12785.589760780334, 'total_duration': 13252.253804206848, 'accumulated_submission_time': 12785.589760780334, 'accumulated_eval_time': 464.64219307899475, 'accumulated_logging_time': 0.7978343963623047, 'global_step': 37918, 'preemption_count': 0}), (39436, {'train/accuracy': 0.7492027878761292, 'train/loss': 0.9707934260368347, 'validation/accuracy': 0.644320011138916, 'validation/loss': 1.4667904376983643, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.183366060256958, 'test/num_examples': 10000, 'score': 13295.721585988998, 'total_duration': 13780.402867794037, 'accumulated_submission_time': 13295.721585988998, 'accumulated_eval_time': 482.57131838798523, 'accumulated_logging_time': 0.8367643356323242, 'global_step': 39436, 'preemption_count': 0}), (40954, {'train/accuracy': 0.711933970451355, 'train/loss': 1.1202871799468994, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.5269010066986084, 'validation/num_examples': 50000, 'test/accuracy': 0.5002000331878662, 'test/loss': 2.254570484161377, 'test/num_examples': 10000, 'score': 13805.760528564453, 'total_duration': 14308.188065290451, 'accumulated_submission_time': 13805.760528564453, 'accumulated_eval_time': 500.2351813316345, 'accumulated_logging_time': 0.8699560165405273, 'global_step': 40954, 'preemption_count': 0}), (42473, {'train/accuracy': 0.7022480964660645, 'train/loss': 1.17045259475708, 'validation/accuracy': 0.6351400017738342, 'validation/loss': 1.5160480737686157, 'validation/num_examples': 50000, 'test/accuracy': 0.5049000382423401, 'test/loss': 2.2341115474700928, 'test/num_examples': 10000, 'score': 14315.898855924606, 'total_duration': 14836.172902822495, 'accumulated_submission_time': 14315.898855924606, 'accumulated_eval_time': 517.9955246448517, 'accumulated_logging_time': 0.9070932865142822, 'global_step': 42473, 'preemption_count': 0}), (43991, {'train/accuracy': 0.6902702450752258, 'train/loss': 1.219701886177063, 'validation/accuracy': 0.6301400065422058, 'validation/loss': 1.5335793495178223, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.266105890274048, 'test/num_examples': 10000, 'score': 14826.120551109314, 'total_duration': 15364.315973997116, 'accumulated_submission_time': 14826.120551109314, 'accumulated_eval_time': 535.8300864696503, 'accumulated_logging_time': 0.9444942474365234, 'global_step': 43991, 'preemption_count': 0}), (45509, {'train/accuracy': 0.7076291441917419, 'train/loss': 1.1384071111679077, 'validation/accuracy': 0.6442599892616272, 'validation/loss': 1.483494520187378, 'validation/num_examples': 50000, 'test/accuracy': 0.5153000354766846, 'test/loss': 2.2475781440734863, 'test/num_examples': 10000, 'score': 15336.121745824814, 'total_duration': 15892.178544044495, 'accumulated_submission_time': 15336.121745824814, 'accumulated_eval_time': 553.6056270599365, 'accumulated_logging_time': 0.9802684783935547, 'global_step': 45509, 'preemption_count': 0}), (47027, {'train/accuracy': 0.6973453164100647, 'train/loss': 1.1850770711898804, 'validation/accuracy': 0.6376000046730042, 'validation/loss': 1.4864510297775269, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.221822500228882, 'test/num_examples': 10000, 'score': 15846.208704471588, 'total_duration': 16419.95223212242, 'accumulated_submission_time': 15846.208704471588, 'accumulated_eval_time': 571.2046520709991, 'accumulated_logging_time': 1.0184855461120605, 'global_step': 47027, 'preemption_count': 0}), (48545, {'train/accuracy': 0.7341358065605164, 'train/loss': 1.0244120359420776, 'validation/accuracy': 0.6324599981307983, 'validation/loss': 1.5245925188064575, 'validation/num_examples': 50000, 'test/accuracy': 0.5067000389099121, 'test/loss': 2.224776268005371, 'test/num_examples': 10000, 'score': 16356.312096595764, 'total_duration': 16947.796236276627, 'accumulated_submission_time': 16356.312096595764, 'accumulated_eval_time': 588.8579633235931, 'accumulated_logging_time': 1.0557844638824463, 'global_step': 48545, 'preemption_count': 0}), (50063, {'train/accuracy': 0.7353116869926453, 'train/loss': 1.0175830125808716, 'validation/accuracy': 0.6514399647712708, 'validation/loss': 1.4259246587753296, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.150914430618286, 'test/num_examples': 10000, 'score': 16866.38495707512, 'total_duration': 17475.897213935852, 'accumulated_submission_time': 16866.38495707512, 'accumulated_eval_time': 606.7944264411926, 'accumulated_logging_time': 1.0985126495361328, 'global_step': 50063, 'preemption_count': 0}), (51583, {'train/accuracy': 0.7122129797935486, 'train/loss': 1.1027354001998901, 'validation/accuracy': 0.6414600014686584, 'validation/loss': 1.4719308614730835, 'validation/num_examples': 50000, 'test/accuracy': 0.5144000053405762, 'test/loss': 2.2002127170562744, 'test/num_examples': 10000, 'score': 17376.622447252274, 'total_duration': 18004.093755483627, 'accumulated_submission_time': 17376.622447252274, 'accumulated_eval_time': 624.6651320457458, 'accumulated_logging_time': 1.1381235122680664, 'global_step': 51583, 'preemption_count': 0}), (53101, {'train/accuracy': 0.7161391973495483, 'train/loss': 1.1013281345367432, 'validation/accuracy': 0.6503399610519409, 'validation/loss': 1.445090889930725, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.178856134414673, 'test/num_examples': 10000, 'score': 17886.875101804733, 'total_duration': 18531.929088115692, 'accumulated_submission_time': 17886.875101804733, 'accumulated_eval_time': 642.1593663692474, 'accumulated_logging_time': 1.176835536956787, 'global_step': 53101, 'preemption_count': 0}), (54619, {'train/accuracy': 0.7102997303009033, 'train/loss': 1.1230549812316895, 'validation/accuracy': 0.6488800048828125, 'validation/loss': 1.4407885074615479, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.1671206951141357, 'test/num_examples': 10000, 'score': 18396.955702781677, 'total_duration': 19059.830214738846, 'accumulated_submission_time': 18396.955702781677, 'accumulated_eval_time': 659.8881301879883, 'accumulated_logging_time': 1.2191162109375, 'global_step': 54619, 'preemption_count': 0}), (56137, {'train/accuracy': 0.6968669891357422, 'train/loss': 1.1886688470840454, 'validation/accuracy': 0.6329799890518188, 'validation/loss': 1.519242763519287, 'validation/num_examples': 50000, 'test/accuracy': 0.5105000138282776, 'test/loss': 2.2319228649139404, 'test/num_examples': 10000, 'score': 18906.910097837448, 'total_duration': 19587.460858106613, 'accumulated_submission_time': 18906.910097837448, 'accumulated_eval_time': 677.467814207077, 'accumulated_logging_time': 1.266390085220337, 'global_step': 56137, 'preemption_count': 0}), (57656, {'train/accuracy': 0.7419084906578064, 'train/loss': 0.977180004119873, 'validation/accuracy': 0.6464999914169312, 'validation/loss': 1.4487738609313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2348201274871826, 'test/num_examples': 10000, 'score': 19417.059530496597, 'total_duration': 20115.970502853394, 'accumulated_submission_time': 19417.059530496597, 'accumulated_eval_time': 695.7363994121552, 'accumulated_logging_time': 1.3087108135223389, 'global_step': 57656, 'preemption_count': 0}), (59175, {'train/accuracy': 0.7238121628761292, 'train/loss': 1.0625795125961304, 'validation/accuracy': 0.6496399641036987, 'validation/loss': 1.435991883277893, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.1798503398895264, 'test/num_examples': 10000, 'score': 19927.296014785767, 'total_duration': 20643.969654798508, 'accumulated_submission_time': 19927.296014785767, 'accumulated_eval_time': 713.406112909317, 'accumulated_logging_time': 1.3524727821350098, 'global_step': 59175, 'preemption_count': 0}), (60694, {'train/accuracy': 0.7162587642669678, 'train/loss': 1.1006048917770386, 'validation/accuracy': 0.6487999558448792, 'validation/loss': 1.4462645053863525, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1441636085510254, 'test/num_examples': 10000, 'score': 20437.448969364166, 'total_duration': 21171.929075479507, 'accumulated_submission_time': 20437.448969364166, 'accumulated_eval_time': 731.1252071857452, 'accumulated_logging_time': 1.3902521133422852, 'global_step': 60694, 'preemption_count': 0}), (62213, {'train/accuracy': 0.7052375674247742, 'train/loss': 1.142183542251587, 'validation/accuracy': 0.6395800113677979, 'validation/loss': 1.4831955432891846, 'validation/num_examples': 50000, 'test/accuracy': 0.511900007724762, 'test/loss': 2.2172904014587402, 'test/num_examples': 10000, 'score': 20947.50677704811, 'total_duration': 21699.70861840248, 'accumulated_submission_time': 20947.50677704811, 'accumulated_eval_time': 748.7553527355194, 'accumulated_logging_time': 1.4329195022583008, 'global_step': 62213, 'preemption_count': 0}), (63731, {'train/accuracy': 0.7198262214660645, 'train/loss': 1.0845756530761719, 'validation/accuracy': 0.657039999961853, 'validation/loss': 1.4061697721481323, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.1699273586273193, 'test/num_examples': 10000, 'score': 21457.440640211105, 'total_duration': 22227.43039250374, 'accumulated_submission_time': 21457.440640211105, 'accumulated_eval_time': 766.4331395626068, 'accumulated_logging_time': 1.4937834739685059, 'global_step': 63731, 'preemption_count': 0}), (65249, {'train/accuracy': 0.732421875, 'train/loss': 1.0268968343734741, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.399552345275879, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.1229050159454346, 'test/num_examples': 10000, 'score': 21967.380613327026, 'total_duration': 22755.134654521942, 'accumulated_submission_time': 21967.380613327026, 'accumulated_eval_time': 784.1061565876007, 'accumulated_logging_time': 1.5358808040618896, 'global_step': 65249, 'preemption_count': 0}), (66768, {'train/accuracy': 0.7518733739852905, 'train/loss': 0.9370354413986206, 'validation/accuracy': 0.6610400080680847, 'validation/loss': 1.3839858770370483, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.1156187057495117, 'test/num_examples': 10000, 'score': 22477.35139322281, 'total_duration': 23283.116079568863, 'accumulated_submission_time': 22477.35139322281, 'accumulated_eval_time': 802.0245227813721, 'accumulated_logging_time': 1.5785470008850098, 'global_step': 66768, 'preemption_count': 0}), (68286, {'train/accuracy': 0.7399752736091614, 'train/loss': 0.9882227778434753, 'validation/accuracy': 0.6643999814987183, 'validation/loss': 1.3710088729858398, 'validation/num_examples': 50000, 'test/accuracy': 0.5396000146865845, 'test/loss': 2.0881545543670654, 'test/num_examples': 10000, 'score': 22987.300876379013, 'total_duration': 23810.711325645447, 'accumulated_submission_time': 22987.300876379013, 'accumulated_eval_time': 819.579030752182, 'accumulated_logging_time': 1.6207971572875977, 'global_step': 68286, 'preemption_count': 0}), (69804, {'train/accuracy': 0.736726701259613, 'train/loss': 1.0046941041946411, 'validation/accuracy': 0.661359965801239, 'validation/loss': 1.3887338638305664, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.137101888656616, 'test/num_examples': 10000, 'score': 23497.229088783264, 'total_duration': 24338.664605140686, 'accumulated_submission_time': 23497.229088783264, 'accumulated_eval_time': 837.5111167430878, 'accumulated_logging_time': 1.6648674011230469, 'global_step': 69804, 'preemption_count': 0}), (71322, {'train/accuracy': 0.7341557741165161, 'train/loss': 1.0238789319992065, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.3770734071731567, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.085169553756714, 'test/num_examples': 10000, 'score': 24007.27220749855, 'total_duration': 24866.694973945618, 'accumulated_submission_time': 24007.27220749855, 'accumulated_eval_time': 855.4055006504059, 'accumulated_logging_time': 1.7085967063903809, 'global_step': 71322, 'preemption_count': 0}), (72840, {'train/accuracy': 0.7183912396430969, 'train/loss': 1.0752381086349487, 'validation/accuracy': 0.6584999561309814, 'validation/loss': 1.404468059539795, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.1446402072906494, 'test/num_examples': 10000, 'score': 24517.43322777748, 'total_duration': 25394.934241056442, 'accumulated_submission_time': 24517.43322777748, 'accumulated_eval_time': 873.3924815654755, 'accumulated_logging_time': 1.7509582042694092, 'global_step': 72840, 'preemption_count': 0}), (74359, {'train/accuracy': 0.7762077450752258, 'train/loss': 0.8492001891136169, 'validation/accuracy': 0.660539984703064, 'validation/loss': 1.3998095989227295, 'validation/num_examples': 50000, 'test/accuracy': 0.531000018119812, 'test/loss': 2.1150431632995605, 'test/num_examples': 10000, 'score': 25027.502433538437, 'total_duration': 25922.918542146683, 'accumulated_submission_time': 25027.502433538437, 'accumulated_eval_time': 891.2145557403564, 'accumulated_logging_time': 1.7948503494262695, 'global_step': 74359, 'preemption_count': 0}), (75877, {'train/accuracy': 0.7547233700752258, 'train/loss': 0.9260692000389099, 'validation/accuracy': 0.668940007686615, 'validation/loss': 1.3541022539138794, 'validation/num_examples': 50000, 'test/accuracy': 0.5397000312805176, 'test/loss': 2.06687068939209, 'test/num_examples': 10000, 'score': 25537.48215198517, 'total_duration': 26450.67085123062, 'accumulated_submission_time': 25537.48215198517, 'accumulated_eval_time': 908.893723487854, 'accumulated_logging_time': 1.8392260074615479, 'global_step': 75877, 'preemption_count': 0}), (77396, {'train/accuracy': 0.7463727593421936, 'train/loss': 0.9656037092208862, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.3637856245040894, 'validation/num_examples': 50000, 'test/accuracy': 0.5337000489234924, 'test/loss': 2.0874693393707275, 'test/num_examples': 10000, 'score': 26047.611650943756, 'total_duration': 26978.572833299637, 'accumulated_submission_time': 26047.611650943756, 'accumulated_eval_time': 926.574179649353, 'accumulated_logging_time': 1.88204026222229, 'global_step': 77396, 'preemption_count': 0}), (78915, {'train/accuracy': 0.724609375, 'train/loss': 1.0671521425247192, 'validation/accuracy': 0.6492800116539001, 'validation/loss': 1.4398260116577148, 'validation/num_examples': 50000, 'test/accuracy': 0.525700032711029, 'test/loss': 2.150360107421875, 'test/num_examples': 10000, 'score': 26557.652361154556, 'total_duration': 27506.367225170135, 'accumulated_submission_time': 26557.652361154556, 'accumulated_eval_time': 944.2308526039124, 'accumulated_logging_time': 1.929426908493042, 'global_step': 78915, 'preemption_count': 0}), (80433, {'train/accuracy': 0.7413902878761292, 'train/loss': 0.9853048324584961, 'validation/accuracy': 0.6678000092506409, 'validation/loss': 1.3506805896759033, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.104771375656128, 'test/num_examples': 10000, 'score': 27067.61016345024, 'total_duration': 28034.159109592438, 'accumulated_submission_time': 27067.61016345024, 'accumulated_eval_time': 961.969643831253, 'accumulated_logging_time': 1.9752976894378662, 'global_step': 80433, 'preemption_count': 0}), (81951, {'train/accuracy': 0.7299306392669678, 'train/loss': 1.0427826642990112, 'validation/accuracy': 0.6603999733924866, 'validation/loss': 1.401990294456482, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.1657660007476807, 'test/num_examples': 10000, 'score': 27577.544088840485, 'total_duration': 28561.997648715973, 'accumulated_submission_time': 27577.544088840485, 'accumulated_eval_time': 979.7718670368195, 'accumulated_logging_time': 2.0286335945129395, 'global_step': 81951, 'preemption_count': 0}), (83470, {'train/accuracy': 0.7867506146430969, 'train/loss': 0.7846525311470032, 'validation/accuracy': 0.6734799742698669, 'validation/loss': 1.3287465572357178, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.0561273097991943, 'test/num_examples': 10000, 'score': 28087.777733802795, 'total_duration': 29090.011644124985, 'accumulated_submission_time': 28087.777733802795, 'accumulated_eval_time': 997.4572043418884, 'accumulated_logging_time': 2.0746822357177734, 'global_step': 83470, 'preemption_count': 0}), (84988, {'train/accuracy': 0.7635124325752258, 'train/loss': 0.8853896856307983, 'validation/accuracy': 0.6748999953269958, 'validation/loss': 1.3213706016540527, 'validation/num_examples': 50000, 'test/accuracy': 0.5521000027656555, 'test/loss': 2.043912649154663, 'test/num_examples': 10000, 'score': 28597.729063987732, 'total_duration': 29618.0389854908, 'accumulated_submission_time': 28597.729063987732, 'accumulated_eval_time': 1015.4390184879303, 'accumulated_logging_time': 2.119399070739746, 'global_step': 84988, 'preemption_count': 0}), (86506, {'train/accuracy': 0.7604631781578064, 'train/loss': 0.9026753902435303, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.3243170976638794, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.0270941257476807, 'test/num_examples': 10000, 'score': 29107.663843154907, 'total_duration': 30145.59640312195, 'accumulated_submission_time': 29107.663843154907, 'accumulated_eval_time': 1032.9682595729828, 'accumulated_logging_time': 2.1635024547576904, 'global_step': 86506, 'preemption_count': 0}), (88024, {'train/accuracy': 0.753926157951355, 'train/loss': 0.9288226366043091, 'validation/accuracy': 0.6761999726295471, 'validation/loss': 1.318540334701538, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.064603328704834, 'test/num_examples': 10000, 'score': 29617.69456934929, 'total_duration': 30673.468591213226, 'accumulated_submission_time': 29617.69456934929, 'accumulated_eval_time': 1050.7145743370056, 'accumulated_logging_time': 2.2094473838806152, 'global_step': 88024, 'preemption_count': 0}), (89542, {'train/accuracy': 0.7547233700752258, 'train/loss': 0.9210852384567261, 'validation/accuracy': 0.6802200078964233, 'validation/loss': 1.296499252319336, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.026613473892212, 'test/num_examples': 10000, 'score': 30127.660895824432, 'total_duration': 31201.483937740326, 'accumulated_submission_time': 30127.660895824432, 'accumulated_eval_time': 1068.669147491455, 'accumulated_logging_time': 2.254591703414917, 'global_step': 89542, 'preemption_count': 0}), (91061, {'train/accuracy': 0.7591477632522583, 'train/loss': 0.9124574065208435, 'validation/accuracy': 0.6842399835586548, 'validation/loss': 1.2905629873275757, 'validation/num_examples': 50000, 'test/accuracy': 0.5562000274658203, 'test/loss': 1.9942518472671509, 'test/num_examples': 10000, 'score': 30637.73820257187, 'total_duration': 31729.2611079216, 'accumulated_submission_time': 30637.73820257187, 'accumulated_eval_time': 1086.2701907157898, 'accumulated_logging_time': 2.303988218307495, 'global_step': 91061, 'preemption_count': 0}), (92579, {'train/accuracy': 0.7926697731018066, 'train/loss': 0.767687976360321, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.2874754667282104, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.0014238357543945, 'test/num_examples': 10000, 'score': 31147.860745429993, 'total_duration': 32257.387805223465, 'accumulated_submission_time': 31147.860745429993, 'accumulated_eval_time': 1104.1770284175873, 'accumulated_logging_time': 2.3518271446228027, 'global_step': 92579, 'preemption_count': 0}), (94096, {'train/accuracy': 0.7694913744926453, 'train/loss': 0.8633120656013489, 'validation/accuracy': 0.6793199777603149, 'validation/loss': 1.320208191871643, 'validation/num_examples': 50000, 'test/accuracy': 0.553600013256073, 'test/loss': 2.048421621322632, 'test/num_examples': 10000, 'score': 31657.77154326439, 'total_duration': 32785.05307340622, 'accumulated_submission_time': 31657.77154326439, 'accumulated_eval_time': 1121.8362724781036, 'accumulated_logging_time': 2.3979732990264893, 'global_step': 94096, 'preemption_count': 0}), (95614, {'train/accuracy': 0.7730388641357422, 'train/loss': 0.8482105135917664, 'validation/accuracy': 0.6853799819946289, 'validation/loss': 1.288509726524353, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.018871784210205, 'test/num_examples': 10000, 'score': 32167.696942329407, 'total_duration': 33312.856902360916, 'accumulated_submission_time': 32167.696942329407, 'accumulated_eval_time': 1139.6173412799835, 'accumulated_logging_time': 2.44614315032959, 'global_step': 95614, 'preemption_count': 0}), (97132, {'train/accuracy': 0.751375138759613, 'train/loss': 0.9182205200195312, 'validation/accuracy': 0.6756399869918823, 'validation/loss': 1.318800449371338, 'validation/num_examples': 50000, 'test/accuracy': 0.5425000190734863, 'test/loss': 2.0723519325256348, 'test/num_examples': 10000, 'score': 32677.73971581459, 'total_duration': 33841.79132437706, 'accumulated_submission_time': 32677.73971581459, 'accumulated_eval_time': 1158.4141011238098, 'accumulated_logging_time': 2.492197275161743, 'global_step': 97132, 'preemption_count': 0}), (98651, {'train/accuracy': 0.7703284025192261, 'train/loss': 0.8573641180992126, 'validation/accuracy': 0.6903600096702576, 'validation/loss': 1.2503340244293213, 'validation/num_examples': 50000, 'test/accuracy': 0.5623000264167786, 'test/loss': 1.983777403831482, 'test/num_examples': 10000, 'score': 33187.793897628784, 'total_duration': 34369.720563173294, 'accumulated_submission_time': 33187.793897628784, 'accumulated_eval_time': 1176.1916980743408, 'accumulated_logging_time': 2.540599822998047, 'global_step': 98651, 'preemption_count': 0}), (100169, {'train/accuracy': 0.7879264950752258, 'train/loss': 0.7837117314338684, 'validation/accuracy': 0.6886000037193298, 'validation/loss': 1.271813988685608, 'validation/num_examples': 50000, 'test/accuracy': 0.5621000528335571, 'test/loss': 1.9868594408035278, 'test/num_examples': 10000, 'score': 33697.73100566864, 'total_duration': 34897.45811223984, 'accumulated_submission_time': 33697.73100566864, 'accumulated_eval_time': 1193.8929476737976, 'accumulated_logging_time': 2.5904340744018555, 'global_step': 100169, 'preemption_count': 0}), (101687, {'train/accuracy': 0.7952805757522583, 'train/loss': 0.7596470713615417, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.2509621381759644, 'validation/num_examples': 50000, 'test/accuracy': 0.5557000041007996, 'test/loss': 1.9959415197372437, 'test/num_examples': 10000, 'score': 34207.68478536606, 'total_duration': 35425.44287323952, 'accumulated_submission_time': 34207.68478536606, 'accumulated_eval_time': 1211.8229558467865, 'accumulated_logging_time': 2.642286777496338, 'global_step': 101687, 'preemption_count': 0}), (103206, {'train/accuracy': 0.7831433415412903, 'train/loss': 0.8068510293960571, 'validation/accuracy': 0.6899600028991699, 'validation/loss': 1.2710587978363037, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 2.0179336071014404, 'test/num_examples': 10000, 'score': 34717.76279425621, 'total_duration': 35953.27993154526, 'accumulated_submission_time': 34717.76279425621, 'accumulated_eval_time': 1229.4830603599548, 'accumulated_logging_time': 2.692025899887085, 'global_step': 103206, 'preemption_count': 0}), (104726, {'train/accuracy': 0.7667809128761292, 'train/loss': 0.8686758279800415, 'validation/accuracy': 0.6767399907112122, 'validation/loss': 1.3189637660980225, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.056422472000122, 'test/num_examples': 10000, 'score': 35227.90310502052, 'total_duration': 36481.48370862007, 'accumulated_submission_time': 35227.90310502052, 'accumulated_eval_time': 1247.4493174552917, 'accumulated_logging_time': 2.7401788234710693, 'global_step': 104726, 'preemption_count': 0}), (106245, {'train/accuracy': 0.7792769074440002, 'train/loss': 0.8142668008804321, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.2566287517547607, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9923611879348755, 'test/num_examples': 10000, 'score': 35738.055804252625, 'total_duration': 37009.439423561096, 'accumulated_submission_time': 35738.055804252625, 'accumulated_eval_time': 1265.152621269226, 'accumulated_logging_time': 2.7908036708831787, 'global_step': 106245, 'preemption_count': 0}), (107764, {'train/accuracy': 0.7824258208274841, 'train/loss': 0.7984607219696045, 'validation/accuracy': 0.6949399709701538, 'validation/loss': 1.2417380809783936, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9687929153442383, 'test/num_examples': 10000, 'score': 36248.185584783554, 'total_duration': 37537.60767388344, 'accumulated_submission_time': 36248.185584783554, 'accumulated_eval_time': 1283.092089176178, 'accumulated_logging_time': 2.8411149978637695, 'global_step': 107764, 'preemption_count': 0}), (109283, {'train/accuracy': 0.8298588991165161, 'train/loss': 0.6174313426017761, 'validation/accuracy': 0.7004799842834473, 'validation/loss': 1.2328135967254639, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9671730995178223, 'test/num_examples': 10000, 'score': 36758.29444336891, 'total_duration': 38065.49232196808, 'accumulated_submission_time': 36758.29444336891, 'accumulated_eval_time': 1300.768966436386, 'accumulated_logging_time': 2.890819549560547, 'global_step': 109283, 'preemption_count': 0}), (110801, {'train/accuracy': 0.8005221486091614, 'train/loss': 0.7223421931266785, 'validation/accuracy': 0.695580005645752, 'validation/loss': 1.2395744323730469, 'validation/num_examples': 50000, 'test/accuracy': 0.5663000345230103, 'test/loss': 1.978838562965393, 'test/num_examples': 10000, 'score': 37268.264142751694, 'total_duration': 38593.53087067604, 'accumulated_submission_time': 37268.264142751694, 'accumulated_eval_time': 1318.7363233566284, 'accumulated_logging_time': 2.9423089027404785, 'global_step': 110801, 'preemption_count': 0}), (112320, {'train/accuracy': 0.800223171710968, 'train/loss': 0.7208084464073181, 'validation/accuracy': 0.6976799964904785, 'validation/loss': 1.2322026491165161, 'validation/num_examples': 50000, 'test/accuracy': 0.5700000524520874, 'test/loss': 1.9467543363571167, 'test/num_examples': 10000, 'score': 37778.35658097267, 'total_duration': 39121.58758187294, 'accumulated_submission_time': 37778.35658097267, 'accumulated_eval_time': 1336.601214170456, 'accumulated_logging_time': 2.9928505420684814, 'global_step': 112320, 'preemption_count': 0}), (113839, {'train/accuracy': 0.8005420565605164, 'train/loss': 0.7312487959861755, 'validation/accuracy': 0.7041599750518799, 'validation/loss': 1.2149962186813354, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.942642331123352, 'test/num_examples': 10000, 'score': 38288.51729607582, 'total_duration': 39649.69808101654, 'accumulated_submission_time': 38288.51729607582, 'accumulated_eval_time': 1354.4533264636993, 'accumulated_logging_time': 3.0412895679473877, 'global_step': 113839, 'preemption_count': 0}), (115357, {'train/accuracy': 0.7947823405265808, 'train/loss': 0.752173125743866, 'validation/accuracy': 0.6983799934387207, 'validation/loss': 1.2410300970077515, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 1.9666895866394043, 'test/num_examples': 10000, 'score': 38798.464625597, 'total_duration': 40177.886984825134, 'accumulated_submission_time': 38798.464625597, 'accumulated_eval_time': 1372.5982236862183, 'accumulated_logging_time': 3.089034080505371, 'global_step': 115357, 'preemption_count': 0}), (116875, {'train/accuracy': 0.7978315949440002, 'train/loss': 0.7409703135490417, 'validation/accuracy': 0.7010599970817566, 'validation/loss': 1.2206995487213135, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 1.9554057121276855, 'test/num_examples': 10000, 'score': 39308.584755182266, 'total_duration': 40705.944717884064, 'accumulated_submission_time': 39308.584755182266, 'accumulated_eval_time': 1390.4349954128265, 'accumulated_logging_time': 3.141059637069702, 'global_step': 116875, 'preemption_count': 0}), (118393, {'train/accuracy': 0.8346220850944519, 'train/loss': 0.5950406193733215, 'validation/accuracy': 0.7032999992370605, 'validation/loss': 1.2153681516647339, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9618397951126099, 'test/num_examples': 10000, 'score': 39818.51747059822, 'total_duration': 41233.966715574265, 'accumulated_submission_time': 39818.51747059822, 'accumulated_eval_time': 1408.42196559906, 'accumulated_logging_time': 3.193824052810669, 'global_step': 118393, 'preemption_count': 0}), (119912, {'train/accuracy': 0.8252750039100647, 'train/loss': 0.6352033615112305, 'validation/accuracy': 0.7068799734115601, 'validation/loss': 1.1854684352874756, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.8898450136184692, 'test/num_examples': 10000, 'score': 40328.73139810562, 'total_duration': 41762.01231408119, 'accumulated_submission_time': 40328.73139810562, 'accumulated_eval_time': 1426.150707244873, 'accumulated_logging_time': 3.247591495513916, 'global_step': 119912, 'preemption_count': 0}), (121431, {'train/accuracy': 0.8082947731018066, 'train/loss': 0.6803027391433716, 'validation/accuracy': 0.7044599652290344, 'validation/loss': 1.2126598358154297, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.9775440692901611, 'test/num_examples': 10000, 'score': 40838.95229148865, 'total_duration': 42290.05305337906, 'accumulated_submission_time': 40838.95229148865, 'accumulated_eval_time': 1443.8689014911652, 'accumulated_logging_time': 3.300241231918335, 'global_step': 121431, 'preemption_count': 0}), (122950, {'train/accuracy': 0.8224449753761292, 'train/loss': 0.6331303119659424, 'validation/accuracy': 0.7143599987030029, 'validation/loss': 1.1689563989639282, 'validation/num_examples': 50000, 'test/accuracy': 0.5859000086784363, 'test/loss': 1.9121652841567993, 'test/num_examples': 10000, 'score': 41349.047131061554, 'total_duration': 42818.18391633034, 'accumulated_submission_time': 41349.047131061554, 'accumulated_eval_time': 1461.8070714473724, 'accumulated_logging_time': 3.3489990234375, 'global_step': 122950, 'preemption_count': 0}), (124469, {'train/accuracy': 0.8160474896430969, 'train/loss': 0.6538271903991699, 'validation/accuracy': 0.7134999632835388, 'validation/loss': 1.174975872039795, 'validation/num_examples': 50000, 'test/accuracy': 0.5866000056266785, 'test/loss': 1.8961846828460693, 'test/num_examples': 10000, 'score': 41859.047733068466, 'total_duration': 43345.82070159912, 'accumulated_submission_time': 41859.047733068466, 'accumulated_eval_time': 1479.3437526226044, 'accumulated_logging_time': 3.399578094482422, 'global_step': 124469, 'preemption_count': 0}), (125988, {'train/accuracy': 0.8224050998687744, 'train/loss': 0.6366180181503296, 'validation/accuracy': 0.7099800109863281, 'validation/loss': 1.189315915107727, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.9083479642868042, 'test/num_examples': 10000, 'score': 42368.96591067314, 'total_duration': 43873.70950007439, 'accumulated_submission_time': 42368.96591067314, 'accumulated_eval_time': 1497.206482887268, 'accumulated_logging_time': 3.4583938121795654, 'global_step': 125988, 'preemption_count': 0}), (127506, {'train/accuracy': 0.8494299650192261, 'train/loss': 0.529978334903717, 'validation/accuracy': 0.7148199677467346, 'validation/loss': 1.1761623620986938, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.8952957391738892, 'test/num_examples': 10000, 'score': 42879.00650596619, 'total_duration': 44401.515615940094, 'accumulated_submission_time': 42879.00650596619, 'accumulated_eval_time': 1514.8715977668762, 'accumulated_logging_time': 3.5093042850494385, 'global_step': 127506, 'preemption_count': 0}), (129025, {'train/accuracy': 0.8404814600944519, 'train/loss': 0.564034104347229, 'validation/accuracy': 0.7139599919319153, 'validation/loss': 1.1836059093475342, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.9311414957046509, 'test/num_examples': 10000, 'score': 43389.02200245857, 'total_duration': 44929.42847776413, 'accumulated_submission_time': 43389.02200245857, 'accumulated_eval_time': 1532.6676306724548, 'accumulated_logging_time': 3.56146502494812, 'global_step': 129025, 'preemption_count': 0}), (130544, {'train/accuracy': 0.8401227593421936, 'train/loss': 0.5696372985839844, 'validation/accuracy': 0.7194199562072754, 'validation/loss': 1.1422617435455322, 'validation/num_examples': 50000, 'test/accuracy': 0.5923000574111938, 'test/loss': 1.8697551488876343, 'test/num_examples': 10000, 'score': 43899.07996249199, 'total_duration': 45457.17548465729, 'accumulated_submission_time': 43899.07996249199, 'accumulated_eval_time': 1550.256118774414, 'accumulated_logging_time': 3.6132612228393555, 'global_step': 130544, 'preemption_count': 0}), (132064, {'train/accuracy': 0.8375318646430969, 'train/loss': 0.5773704051971436, 'validation/accuracy': 0.7168599963188171, 'validation/loss': 1.1653879880905151, 'validation/num_examples': 50000, 'test/accuracy': 0.588200032711029, 'test/loss': 1.8888368606567383, 'test/num_examples': 10000, 'score': 44409.24393892288, 'total_duration': 45985.240468502045, 'accumulated_submission_time': 44409.24393892288, 'accumulated_eval_time': 1568.0413491725922, 'accumulated_logging_time': 3.679718255996704, 'global_step': 132064, 'preemption_count': 0}), (133582, {'train/accuracy': 0.8428332209587097, 'train/loss': 0.5535558462142944, 'validation/accuracy': 0.720579981803894, 'validation/loss': 1.1483376026153564, 'validation/num_examples': 50000, 'test/accuracy': 0.5958000421524048, 'test/loss': 1.894065499305725, 'test/num_examples': 10000, 'score': 44919.25254154205, 'total_duration': 46513.27612757683, 'accumulated_submission_time': 44919.25254154205, 'accumulated_eval_time': 1585.9611542224884, 'accumulated_logging_time': 3.7378854751586914, 'global_step': 133582, 'preemption_count': 0}), (135101, {'train/accuracy': 0.8835100531578064, 'train/loss': 0.4169529676437378, 'validation/accuracy': 0.72461998462677, 'validation/loss': 1.1391019821166992, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.867887020111084, 'test/num_examples': 10000, 'score': 45429.48771595955, 'total_duration': 47042.147736787796, 'accumulated_submission_time': 45429.48771595955, 'accumulated_eval_time': 1604.497786283493, 'accumulated_logging_time': 3.788525342941284, 'global_step': 135101, 'preemption_count': 0}), (136619, {'train/accuracy': 0.8668088316917419, 'train/loss': 0.46367859840393066, 'validation/accuracy': 0.724399983882904, 'validation/loss': 1.1427175998687744, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.8947100639343262, 'test/num_examples': 10000, 'score': 45939.46535348892, 'total_duration': 47570.19637298584, 'accumulated_submission_time': 45939.46535348892, 'accumulated_eval_time': 1622.4473087787628, 'accumulated_logging_time': 3.860964298248291, 'global_step': 136619, 'preemption_count': 0}), (138138, {'train/accuracy': 0.8622449040412903, 'train/loss': 0.4757066071033478, 'validation/accuracy': 0.7257999777793884, 'validation/loss': 1.1301318407058716, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.837876796722412, 'test/num_examples': 10000, 'score': 46449.43815970421, 'total_duration': 48098.23912549019, 'accumulated_submission_time': 46449.43815970421, 'accumulated_eval_time': 1640.4148676395416, 'accumulated_logging_time': 3.913733720779419, 'global_step': 138138, 'preemption_count': 0}), (139656, {'train/accuracy': 0.8647361397743225, 'train/loss': 0.466713011264801, 'validation/accuracy': 0.7309799790382385, 'validation/loss': 1.1106938123703003, 'validation/num_examples': 50000, 'test/accuracy': 0.6045000553131104, 'test/loss': 1.8522108793258667, 'test/num_examples': 10000, 'score': 46959.3910138607, 'total_duration': 48626.24474787712, 'accumulated_submission_time': 46959.3910138607, 'accumulated_eval_time': 1658.3606095314026, 'accumulated_logging_time': 3.971649646759033, 'global_step': 139656, 'preemption_count': 0}), (141173, {'train/accuracy': 0.8649353981018066, 'train/loss': 0.4697670340538025, 'validation/accuracy': 0.7278599739074707, 'validation/loss': 1.1257988214492798, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8480082750320435, 'test/num_examples': 10000, 'score': 47469.405690431595, 'total_duration': 49154.069472551346, 'accumulated_submission_time': 47469.405690431595, 'accumulated_eval_time': 1676.064817905426, 'accumulated_logging_time': 4.028349161148071, 'global_step': 141173, 'preemption_count': 0}), (142691, {'train/accuracy': 0.8581592440605164, 'train/loss': 0.48604610562324524, 'validation/accuracy': 0.7240399718284607, 'validation/loss': 1.1514825820922852, 'validation/num_examples': 50000, 'test/accuracy': 0.5978000164031982, 'test/loss': 1.939948320388794, 'test/num_examples': 10000, 'score': 47979.4405105114, 'total_duration': 49682.18212604523, 'accumulated_submission_time': 47979.4405105114, 'accumulated_eval_time': 1694.0392887592316, 'accumulated_logging_time': 4.082364082336426, 'global_step': 142691, 'preemption_count': 0}), (144208, {'train/accuracy': 0.8985769748687744, 'train/loss': 0.3564726412296295, 'validation/accuracy': 0.7285599708557129, 'validation/loss': 1.129584789276123, 'validation/num_examples': 50000, 'test/accuracy': 0.6085000038146973, 'test/loss': 1.8798846006393433, 'test/num_examples': 10000, 'score': 48489.41442799568, 'total_duration': 50209.914311885834, 'accumulated_submission_time': 48489.41442799568, 'accumulated_eval_time': 1711.6912882328033, 'accumulated_logging_time': 4.139669179916382, 'global_step': 144208, 'preemption_count': 0}), (145726, {'train/accuracy': 0.8902861475944519, 'train/loss': 0.38217586278915405, 'validation/accuracy': 0.7315999865531921, 'validation/loss': 1.1145838499069214, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.8463207483291626, 'test/num_examples': 10000, 'score': 48999.39666056633, 'total_duration': 50737.630281448364, 'accumulated_submission_time': 48999.39666056633, 'accumulated_eval_time': 1729.3212552070618, 'accumulated_logging_time': 4.194288969039917, 'global_step': 145726, 'preemption_count': 0}), (147244, {'train/accuracy': 0.8928371667861938, 'train/loss': 0.36683279275894165, 'validation/accuracy': 0.7371399998664856, 'validation/loss': 1.0903639793395996, 'validation/num_examples': 50000, 'test/accuracy': 0.6055000424385071, 'test/loss': 1.8396549224853516, 'test/num_examples': 10000, 'score': 49509.461168289185, 'total_duration': 51265.44833254814, 'accumulated_submission_time': 49509.461168289185, 'accumulated_eval_time': 1746.9731595516205, 'accumulated_logging_time': 4.246668100357056, 'global_step': 147244, 'preemption_count': 0}), (148762, {'train/accuracy': 0.8951091766357422, 'train/loss': 0.3621106743812561, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.090329647064209, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8255805969238281, 'test/num_examples': 10000, 'score': 50019.50054001808, 'total_duration': 51793.38860464096, 'accumulated_submission_time': 50019.50054001808, 'accumulated_eval_time': 1764.7672312259674, 'accumulated_logging_time': 4.303528308868408, 'global_step': 148762, 'preemption_count': 0}), (150280, {'train/accuracy': 0.8956672549247742, 'train/loss': 0.356880247592926, 'validation/accuracy': 0.7396799921989441, 'validation/loss': 1.1052157878875732, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.8790510892868042, 'test/num_examples': 10000, 'score': 50529.406017541885, 'total_duration': 52320.97790455818, 'accumulated_submission_time': 50529.406017541885, 'accumulated_eval_time': 1782.3478038311005, 'accumulated_logging_time': 4.357873916625977, 'global_step': 150280, 'preemption_count': 0}), (151799, {'train/accuracy': 0.904715359210968, 'train/loss': 0.33212369680404663, 'validation/accuracy': 0.7416599988937378, 'validation/loss': 1.0797576904296875, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.824073076248169, 'test/num_examples': 10000, 'score': 51039.55972290039, 'total_duration': 52849.2077562809, 'accumulated_submission_time': 51039.55972290039, 'accumulated_eval_time': 1800.3187172412872, 'accumulated_logging_time': 4.414421319961548, 'global_step': 151799, 'preemption_count': 0}), (153317, {'train/accuracy': 0.9268175959587097, 'train/loss': 0.26082730293273926, 'validation/accuracy': 0.7423999905586243, 'validation/loss': 1.080202579498291, 'validation/num_examples': 50000, 'test/accuracy': 0.6152999997138977, 'test/loss': 1.8277418613433838, 'test/num_examples': 10000, 'score': 51549.500520944595, 'total_duration': 53376.834444761276, 'accumulated_submission_time': 51549.500520944595, 'accumulated_eval_time': 1817.896065711975, 'accumulated_logging_time': 4.474432706832886, 'global_step': 153317, 'preemption_count': 0}), (154835, {'train/accuracy': 0.9178690910339355, 'train/loss': 0.28511112928390503, 'validation/accuracy': 0.7413199543952942, 'validation/loss': 1.083351969718933, 'validation/num_examples': 50000, 'test/accuracy': 0.6127000451087952, 'test/loss': 1.845734715461731, 'test/num_examples': 10000, 'score': 52059.46150302887, 'total_duration': 53904.571982860565, 'accumulated_submission_time': 52059.46150302887, 'accumulated_eval_time': 1835.5662310123444, 'accumulated_logging_time': 4.531611204147339, 'global_step': 154835, 'preemption_count': 0}), (156353, {'train/accuracy': 0.9176298975944519, 'train/loss': 0.2818046808242798, 'validation/accuracy': 0.7437999844551086, 'validation/loss': 1.084424614906311, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.8379580974578857, 'test/num_examples': 10000, 'score': 52569.59485697746, 'total_duration': 54432.38970851898, 'accumulated_submission_time': 52569.59485697746, 'accumulated_eval_time': 1853.1430249214172, 'accumulated_logging_time': 4.589997291564941, 'global_step': 156353, 'preemption_count': 0}), (157873, {'train/accuracy': 0.9197026491165161, 'train/loss': 0.27838096022605896, 'validation/accuracy': 0.7439000010490417, 'validation/loss': 1.0915087461471558, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8537700176239014, 'test/num_examples': 10000, 'score': 53079.72857952118, 'total_duration': 54960.33554697037, 'accumulated_submission_time': 53079.72857952118, 'accumulated_eval_time': 1870.8468675613403, 'accumulated_logging_time': 4.649376630783081, 'global_step': 157873, 'preemption_count': 0}), (159391, {'train/accuracy': 0.9197424650192261, 'train/loss': 0.27487990260124207, 'validation/accuracy': 0.7453799843788147, 'validation/loss': 1.0846729278564453, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8550739288330078, 'test/num_examples': 10000, 'score': 53589.63150548935, 'total_duration': 55488.387427806854, 'accumulated_submission_time': 53589.63150548935, 'accumulated_eval_time': 1888.8875906467438, 'accumulated_logging_time': 4.708525896072388, 'global_step': 159391, 'preemption_count': 0}), (160908, {'train/accuracy': 0.9289500713348389, 'train/loss': 0.24715971946716309, 'validation/accuracy': 0.7472599744796753, 'validation/loss': 1.0752619504928589, 'validation/num_examples': 50000, 'test/accuracy': 0.6208000183105469, 'test/loss': 1.851715087890625, 'test/num_examples': 10000, 'score': 54099.56414723396, 'total_duration': 56016.02576851845, 'accumulated_submission_time': 54099.56414723396, 'accumulated_eval_time': 1906.48606300354, 'accumulated_logging_time': 4.766141414642334, 'global_step': 160908, 'preemption_count': 0}), (162426, {'train/accuracy': 0.9414859414100647, 'train/loss': 0.20910905301570892, 'validation/accuracy': 0.7475999593734741, 'validation/loss': 1.0705816745758057, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.8417563438415527, 'test/num_examples': 10000, 'score': 54609.62043738365, 'total_duration': 56544.218824863434, 'accumulated_submission_time': 54609.62043738365, 'accumulated_eval_time': 1924.5159137248993, 'accumulated_logging_time': 4.82317328453064, 'global_step': 162426, 'preemption_count': 0}), (163945, {'train/accuracy': 0.9399114847183228, 'train/loss': 0.21189145743846893, 'validation/accuracy': 0.7474600076675415, 'validation/loss': 1.0750255584716797, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.8490190505981445, 'test/num_examples': 10000, 'score': 55119.633002758026, 'total_duration': 57072.43365240097, 'accumulated_submission_time': 55119.633002758026, 'accumulated_eval_time': 1942.612450838089, 'accumulated_logging_time': 4.879745721817017, 'global_step': 163945, 'preemption_count': 0}), (165463, {'train/accuracy': 0.9395328164100647, 'train/loss': 0.20963868498802185, 'validation/accuracy': 0.7483199834823608, 'validation/loss': 1.0747122764587402, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.847937822341919, 'test/num_examples': 10000, 'score': 55629.809804201126, 'total_duration': 57600.36788249016, 'accumulated_submission_time': 55629.809804201126, 'accumulated_eval_time': 1960.2603447437286, 'accumulated_logging_time': 4.9395411014556885, 'global_step': 165463, 'preemption_count': 0}), (166981, {'train/accuracy': 0.9429607391357422, 'train/loss': 0.20566825568675995, 'validation/accuracy': 0.750059962272644, 'validation/loss': 1.0746914148330688, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.840732455253601, 'test/num_examples': 10000, 'score': 56139.720806121826, 'total_duration': 58128.30862569809, 'accumulated_submission_time': 56139.720806121826, 'accumulated_eval_time': 1978.183670282364, 'accumulated_logging_time': 4.996540307998657, 'global_step': 166981, 'preemption_count': 0}), (168498, {'train/accuracy': 0.9451330900192261, 'train/loss': 0.19239141047000885, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.0681169033050537, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.8350611925125122, 'test/num_examples': 10000, 'score': 56649.69009900093, 'total_duration': 58656.19987845421, 'accumulated_submission_time': 56649.69009900093, 'accumulated_eval_time': 1995.9945611953735, 'accumulated_logging_time': 5.058220386505127, 'global_step': 168498, 'preemption_count': 0}), (170016, {'train/accuracy': 0.9563735723495483, 'train/loss': 0.16319619119167328, 'validation/accuracy': 0.7527999877929688, 'validation/loss': 1.0671993494033813, 'validation/num_examples': 50000, 'test/accuracy': 0.6259000301361084, 'test/loss': 1.8455358743667603, 'test/num_examples': 10000, 'score': 57159.68464636803, 'total_duration': 59184.30030846596, 'accumulated_submission_time': 57159.68464636803, 'accumulated_eval_time': 2013.9880871772766, 'accumulated_logging_time': 5.121249437332153, 'global_step': 170016, 'preemption_count': 0}), (171535, {'train/accuracy': 0.9563934803009033, 'train/loss': 0.1602298766374588, 'validation/accuracy': 0.7529199719429016, 'validation/loss': 1.065573811531067, 'validation/num_examples': 50000, 'test/accuracy': 0.6290000081062317, 'test/loss': 1.8335827589035034, 'test/num_examples': 10000, 'score': 57669.88044476509, 'total_duration': 59712.426441669464, 'accumulated_submission_time': 57669.88044476509, 'accumulated_eval_time': 2031.8099205493927, 'accumulated_logging_time': 5.179990291595459, 'global_step': 171535, 'preemption_count': 0}), (173053, {'train/accuracy': 0.9550382494926453, 'train/loss': 0.1676359325647354, 'validation/accuracy': 0.7534399628639221, 'validation/loss': 1.057241439819336, 'validation/num_examples': 50000, 'test/accuracy': 0.627500057220459, 'test/loss': 1.8320350646972656, 'test/num_examples': 10000, 'score': 58179.970638751984, 'total_duration': 60240.450879096985, 'accumulated_submission_time': 58179.970638751984, 'accumulated_eval_time': 2049.6324560642242, 'accumulated_logging_time': 5.242520332336426, 'global_step': 173053, 'preemption_count': 0}), (174572, {'train/accuracy': 0.9543008208274841, 'train/loss': 0.1634673774242401, 'validation/accuracy': 0.7548999786376953, 'validation/loss': 1.0598605871200562, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.833614706993103, 'test/num_examples': 10000, 'score': 58689.94938135147, 'total_duration': 60768.789187669754, 'accumulated_submission_time': 58689.94938135147, 'accumulated_eval_time': 2067.8815484046936, 'accumulated_logging_time': 5.303300857543945, 'global_step': 174572, 'preemption_count': 0}), (176090, {'train/accuracy': 0.9560347199440002, 'train/loss': 0.16032513976097107, 'validation/accuracy': 0.7560399770736694, 'validation/loss': 1.0561996698379517, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.832032561302185, 'test/num_examples': 10000, 'score': 59199.97342252731, 'total_duration': 61296.55711436272, 'accumulated_submission_time': 59199.97342252731, 'accumulated_eval_time': 2085.516634464264, 'accumulated_logging_time': 5.3632285594940186, 'global_step': 176090, 'preemption_count': 0}), (177608, {'train/accuracy': 0.9580675959587097, 'train/loss': 0.1550871878862381, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.0575575828552246, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.8350627422332764, 'test/num_examples': 10000, 'score': 59710.124698638916, 'total_duration': 61824.701684474945, 'accumulated_submission_time': 59710.124698638916, 'accumulated_eval_time': 2103.398220539093, 'accumulated_logging_time': 5.4248974323272705, 'global_step': 177608, 'preemption_count': 0}), (179127, {'train/accuracy': 0.9602997303009033, 'train/loss': 0.14733432233333588, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.0583137273788452, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8351722955703735, 'test/num_examples': 10000, 'score': 60220.22158074379, 'total_duration': 62352.568135261536, 'accumulated_submission_time': 60220.22158074379, 'accumulated_eval_time': 2121.0545020103455, 'accumulated_logging_time': 5.489213466644287, 'global_step': 179127, 'preemption_count': 0}), (180645, {'train/accuracy': 0.9606983065605164, 'train/loss': 0.1453530639410019, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 1.055141806602478, 'validation/num_examples': 50000, 'test/accuracy': 0.6306000351905823, 'test/loss': 1.8299227952957153, 'test/num_examples': 10000, 'score': 60730.1836707592, 'total_duration': 62880.34313893318, 'accumulated_submission_time': 60730.1836707592, 'accumulated_eval_time': 2138.7541739940643, 'accumulated_logging_time': 5.5535314083099365, 'global_step': 180645, 'preemption_count': 0}), (182163, {'train/accuracy': 0.959004282951355, 'train/loss': 0.14809420704841614, 'validation/accuracy': 0.7565400004386902, 'validation/loss': 1.0530595779418945, 'validation/num_examples': 50000, 'test/accuracy': 0.6303000450134277, 'test/loss': 1.828680157661438, 'test/num_examples': 10000, 'score': 61240.363770484924, 'total_duration': 63408.5197532177, 'accumulated_submission_time': 61240.363770484924, 'accumulated_eval_time': 2156.6417529582977, 'accumulated_logging_time': 5.613237142562866, 'global_step': 182163, 'preemption_count': 0}), (183681, {'train/accuracy': 0.9598413109779358, 'train/loss': 0.150454580783844, 'validation/accuracy': 0.7559999823570251, 'validation/loss': 1.0534089803695679, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.8285574913024902, 'test/num_examples': 10000, 'score': 61750.54144191742, 'total_duration': 63936.44031405449, 'accumulated_submission_time': 61750.54144191742, 'accumulated_eval_time': 2174.271763563156, 'accumulated_logging_time': 5.677387714385986, 'global_step': 183681, 'preemption_count': 0}), (185198, {'train/accuracy': 0.9613759517669678, 'train/loss': 0.14299754798412323, 'validation/accuracy': 0.7555399537086487, 'validation/loss': 1.053418755531311, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.8302977085113525, 'test/num_examples': 10000, 'score': 62260.458035469055, 'total_duration': 64464.38110399246, 'accumulated_submission_time': 62260.458035469055, 'accumulated_eval_time': 2192.1826646327972, 'accumulated_logging_time': 5.741932153701782, 'global_step': 185198, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9604591727256775, 'train/loss': 0.14778749644756317, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0542693138122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.83073890209198, 'test/num_examples': 10000, 'score': 62753.75569033623, 'total_duration': 64975.68322634697, 'accumulated_submission_time': 62753.75569033623, 'accumulated_eval_time': 2210.0775923728943, 'accumulated_logging_time': 5.804227828979492, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0130 19:50:41.317182 140169137129280 submission_runner.py:586] Timing: 62753.75569033623
I0130 19:50:41.317276 140169137129280 submission_runner.py:588] Total number of evals: 124
I0130 19:50:41.317320 140169137129280 submission_runner.py:589] ====================
I0130 19:50:41.318849 140169137129280 submission_runner.py:673] Final imagenet_resnet score: 62446.866736888885
