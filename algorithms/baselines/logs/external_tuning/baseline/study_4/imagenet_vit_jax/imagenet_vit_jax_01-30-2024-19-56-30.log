python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=1800903789 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_vit_jax_01-30-2024-19-56-30.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0130 19:56:51.169522 140085747812160 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax.
I0130 19:56:52.243488 140085747812160 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0130 19:56:52.244478 140085747812160 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0130 19:56:52.244707 140085747812160 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0130 19:56:52.246140 140085747812160 submission_runner.py:542] Using RNG seed 1800903789
I0130 19:56:57.616092 140085747812160 submission_runner.py:551] --- Tuning run 1/5 ---
I0130 19:56:57.616366 140085747812160 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1.
I0130 19:56:57.616580 140085747812160 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1/hparams.json.
I0130 19:56:57.804191 140085747812160 submission_runner.py:206] Initializing dataset.
I0130 19:56:57.820453 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:56:57.832784 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 19:56:58.208090 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:57:06.480126 140085747812160 submission_runner.py:213] Initializing model.
I0130 19:57:15.305140 140085747812160 submission_runner.py:255] Initializing optimizer.
I0130 19:57:16.280887 140085747812160 submission_runner.py:262] Initializing metrics bundle.
I0130 19:57:16.281073 140085747812160 submission_runner.py:280] Initializing checkpoint and logger.
I0130 19:57:16.282269 140085747812160 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0130 19:57:16.282414 140085747812160 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0130 19:57:16.705903 140085747812160 logger_utils.py:220] Unable to record git information. Continuing without it.
I0130 19:57:17.100446 140085747812160 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1/flags_0.json.
I0130 19:57:17.110400 140085747812160 submission_runner.py:314] Starting training loop.
2024-01-30 19:58:07.726802: E external/xla/xla/service/rendezvous.cc:31] This thread has been waiting for 10 seconds and may be stuck:
2024-01-30 19:58:11.531602: E external/xla/xla/service/rendezvous.cc:36] Thread is unstuck! Warning above was a false-positive. Perhaps the timeout is too short.
I0130 19:58:13.124081 139923323614976 logging_writer.py:48] [0] global_step=0, grad_norm=0.32910990715026855, loss=6.907756805419922
I0130 19:58:13.141949 140085747812160 spec.py:321] Evaluating on the training split.
I0130 19:58:13.150182 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:58:13.159276 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 19:58:13.244252 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:58:30.567951 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 19:58:30.576202 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:58:30.595969 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0130 19:58:30.677425 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0130 19:58:48.103980 140085747812160 spec.py:349] Evaluating on the test split.
I0130 19:58:48.113531 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 19:58:48.120181 140085747812160 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0130 19:58:48.174484 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0130 19:58:53.456827 140085747812160 submission_runner.py:408] Time since start: 96.35s, 	Step: 1, 	{'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 56.03143048286438, 'total_duration': 96.34635639190674, 'accumulated_submission_time': 56.03143048286438, 'accumulated_eval_time': 40.31481671333313, 'accumulated_logging_time': 0}
I0130 19:58:53.478810 139884777948928 logging_writer.py:48] [1] accumulated_eval_time=40.314817, accumulated_logging_time=0, accumulated_submission_time=56.031430, global_step=1, preemption_count=0, score=56.031430, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=96.346356, train/accuracy=0.000918, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0130 19:59:55.876197 139921452914432 logging_writer.py:48] [100] global_step=100, grad_norm=0.334309458732605, loss=6.906836986541748
I0130 20:00:40.695338 139921461307136 logging_writer.py:48] [200] global_step=200, grad_norm=0.41020020842552185, loss=6.893394947052002
I0130 20:01:26.781437 139921452914432 logging_writer.py:48] [300] global_step=300, grad_norm=0.5273445844650269, loss=6.857131481170654
I0130 20:02:12.666694 139921461307136 logging_writer.py:48] [400] global_step=400, grad_norm=0.6525280475616455, loss=6.807192325592041
I0130 20:02:58.752777 139921452914432 logging_writer.py:48] [500] global_step=500, grad_norm=0.7201225161552429, loss=6.785980224609375
I0130 20:03:44.431086 139921461307136 logging_writer.py:48] [600] global_step=600, grad_norm=1.1000946760177612, loss=6.741386890411377
I0130 20:04:30.249492 139921452914432 logging_writer.py:48] [700] global_step=700, grad_norm=0.9100100994110107, loss=6.810025691986084
I0130 20:05:15.966447 139921461307136 logging_writer.py:48] [800] global_step=800, grad_norm=1.192735195159912, loss=6.656243324279785
I0130 20:05:53.699468 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:06:05.775009 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:06:13.865769 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:06:15.677981 140085747812160 submission_runner.py:408] Time since start: 538.57s, 	Step: 884, 	{'train/accuracy': 0.015097656287252903, 'train/loss': 6.414700031280518, 'validation/accuracy': 0.014619999565184116, 'validation/loss': 6.424112796783447, 'validation/num_examples': 50000, 'test/accuracy': 0.011200000531971455, 'test/loss': 6.467188358306885, 'test/num_examples': 10000, 'score': 476.19164657592773, 'total_duration': 538.5675106048584, 'accumulated_submission_time': 476.19164657592773, 'accumulated_eval_time': 62.29332900047302, 'accumulated_logging_time': 0.0311734676361084}
I0130 20:06:15.695080 139884786341632 logging_writer.py:48] [884] accumulated_eval_time=62.293329, accumulated_logging_time=0.031173, accumulated_submission_time=476.191647, global_step=884, preemption_count=0, score=476.191647, test/accuracy=0.011200, test/loss=6.467188, test/num_examples=10000, total_duration=538.567511, train/accuracy=0.015098, train/loss=6.414700, validation/accuracy=0.014620, validation/loss=6.424113, validation/num_examples=50000
I0130 20:06:22.397252 139884794734336 logging_writer.py:48] [900] global_step=900, grad_norm=3.1867454051971436, loss=6.574435234069824
I0130 20:07:03.605993 139884786341632 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.533499002456665, loss=6.567874908447266
I0130 20:07:49.383312 139884794734336 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.086824655532837, loss=6.465560436248779
I0130 20:08:35.349672 139884786341632 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.6976290941238403, loss=6.436184883117676
I0130 20:09:21.225186 139884794734336 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8970184326171875, loss=6.45155143737793
I0130 20:10:06.855949 139884786341632 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.2821145057678223, loss=6.417068004608154
I0130 20:10:52.287611 139884794734336 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.5363595485687256, loss=6.437841415405273
I0130 20:11:37.838331 139884786341632 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7365093231201172, loss=6.283599853515625
I0130 20:12:23.558209 139884794734336 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.3382030725479126, loss=6.571628570556641
I0130 20:13:09.371489 139884786341632 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.44956636428833, loss=6.499438285827637
I0130 20:13:15.907781 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:13:27.860535 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:13:35.985969 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:13:37.645561 140085747812160 submission_runner.py:408] Time since start: 980.54s, 	Step: 1816, 	{'train/accuracy': 0.04582031071186066, 'train/loss': 5.830249309539795, 'validation/accuracy': 0.044039998203516006, 'validation/loss': 5.860832214355469, 'validation/num_examples': 50000, 'test/accuracy': 0.03880000114440918, 'test/loss': 5.978135585784912, 'test/num_examples': 10000, 'score': 896.3407437801361, 'total_duration': 980.5350985527039, 'accumulated_submission_time': 896.3407437801361, 'accumulated_eval_time': 84.03109979629517, 'accumulated_logging_time': 0.058878421783447266}
I0130 20:13:37.661880 139884794734336 logging_writer.py:48] [1816] accumulated_eval_time=84.031100, accumulated_logging_time=0.058878, accumulated_submission_time=896.340744, global_step=1816, preemption_count=0, score=896.340744, test/accuracy=0.038800, test/loss=5.978136, test/num_examples=10000, total_duration=980.535099, train/accuracy=0.045820, train/loss=5.830249, validation/accuracy=0.044040, validation/loss=5.860832, validation/num_examples=50000
I0130 20:14:11.202799 139884786341632 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.262904405593872, loss=6.695959091186523
I0130 20:14:56.625486 139884794734336 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.995912790298462, loss=6.1162495613098145
I0130 20:15:42.209331 139884786341632 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.6380743980407715, loss=6.136719703674316
I0130 20:16:27.948692 139884794734336 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.0715973377227783, loss=6.113777160644531
I0130 20:17:13.854548 139884786341632 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.3674516677856445, loss=6.072237491607666
I0130 20:17:59.490392 139884794734336 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.1478984355926514, loss=6.066251754760742
I0130 20:18:45.071586 139884786341632 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.5049723386764526, loss=6.028589248657227
I0130 20:19:30.774880 139884794734336 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.7792006731033325, loss=6.012537002563477
I0130 20:20:16.414747 139884786341632 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.4447225332260132, loss=6.491280555725098
I0130 20:20:37.935918 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:20:49.965449 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:20:58.158975 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:20:59.820342 140085747812160 submission_runner.py:408] Time since start: 1422.71s, 	Step: 2749, 	{'train/accuracy': 0.07023437321186066, 'train/loss': 5.432772636413574, 'validation/accuracy': 0.06391999870538712, 'validation/loss': 5.477494716644287, 'validation/num_examples': 50000, 'test/accuracy': 0.04910000413656235, 'test/loss': 5.647575378417969, 'test/num_examples': 10000, 'score': 1316.5516102313995, 'total_duration': 1422.709864616394, 'accumulated_submission_time': 1316.5516102313995, 'accumulated_eval_time': 105.91550326347351, 'accumulated_logging_time': 0.08542728424072266}
I0130 20:20:59.838043 139884794734336 logging_writer.py:48] [2749] accumulated_eval_time=105.915503, accumulated_logging_time=0.085427, accumulated_submission_time=1316.551610, global_step=2749, preemption_count=0, score=1316.551610, test/accuracy=0.049100, test/loss=5.647575, test/num_examples=10000, total_duration=1422.709865, train/accuracy=0.070234, train/loss=5.432773, validation/accuracy=0.063920, validation/loss=5.477495, validation/num_examples=50000
I0130 20:21:20.355023 139884786341632 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.777781367301941, loss=5.910261154174805
I0130 20:22:04.142121 139884794734336 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.011845588684082, loss=5.899960041046143
I0130 20:22:49.843495 139884786341632 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4685583114624023, loss=6.0078887939453125
I0130 20:23:35.505772 139884794734336 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.7601772546768188, loss=5.888437747955322
I0130 20:24:21.224414 139884786341632 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.326470136642456, loss=6.49884033203125
I0130 20:25:06.693746 139884794734336 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.6396442651748657, loss=5.819902420043945
I0130 20:25:52.354833 139884786341632 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.9351850748062134, loss=5.884766101837158
I0130 20:26:38.170699 139884794734336 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.4570884704589844, loss=6.048254013061523
I0130 20:27:23.984834 139884786341632 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.698074460029602, loss=6.408495903015137
I0130 20:28:00.176324 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:28:12.106464 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:28:21.990884 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:28:23.638471 140085747812160 submission_runner.py:408] Time since start: 1866.53s, 	Step: 3681, 	{'train/accuracy': 0.09486328065395355, 'train/loss': 5.102994441986084, 'validation/accuracy': 0.08893999457359314, 'validation/loss': 5.145900249481201, 'validation/num_examples': 50000, 'test/accuracy': 0.06800000369548798, 'test/loss': 5.371665954589844, 'test/num_examples': 10000, 'score': 1736.8272049427032, 'total_duration': 1866.5280022621155, 'accumulated_submission_time': 1736.8272049427032, 'accumulated_eval_time': 129.37765622138977, 'accumulated_logging_time': 0.11379456520080566}
I0130 20:28:23.655699 139884794734336 logging_writer.py:48] [3681] accumulated_eval_time=129.377656, accumulated_logging_time=0.113795, accumulated_submission_time=1736.827205, global_step=3681, preemption_count=0, score=1736.827205, test/accuracy=0.068000, test/loss=5.371666, test/num_examples=10000, total_duration=1866.528002, train/accuracy=0.094863, train/loss=5.102994, validation/accuracy=0.088940, validation/loss=5.145900, validation/num_examples=50000
I0130 20:28:31.558079 139884786341632 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.537024974822998, loss=5.739633560180664
I0130 20:29:13.681282 139884794734336 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.548625111579895, loss=5.765267372131348
I0130 20:29:59.302460 139884786341632 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.3460654020309448, loss=5.798430442810059
I0130 20:30:45.116384 139884794734336 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.9165420532226562, loss=5.734317779541016
I0130 20:31:30.912320 139884786341632 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.9846426248550415, loss=5.690876007080078
I0130 20:32:16.610844 139884794734336 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.5840680599212646, loss=5.63986349105835
I0130 20:33:02.623459 139884786341632 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.006465435028076, loss=5.615768909454346
I0130 20:33:48.300867 139884794734336 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.1998066902160645, loss=6.424536228179932
I0130 20:34:34.035027 139884786341632 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.9522899389266968, loss=5.549839019775391
I0130 20:35:19.702022 139884794734336 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.745065689086914, loss=5.520509243011475
I0130 20:35:23.931856 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:35:36.101136 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:35:44.492070 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:35:46.145678 140085747812160 submission_runner.py:408] Time since start: 2309.04s, 	Step: 4611, 	{'train/accuracy': 0.14060546457767487, 'train/loss': 4.725737571716309, 'validation/accuracy': 0.13011999428272247, 'validation/loss': 4.775650501251221, 'validation/num_examples': 50000, 'test/accuracy': 0.09880000352859497, 'test/loss': 5.062837600708008, 'test/num_examples': 10000, 'score': 2157.039966583252, 'total_duration': 2309.0352096557617, 'accumulated_submission_time': 2157.039966583252, 'accumulated_eval_time': 151.5914740562439, 'accumulated_logging_time': 0.14225101470947266}
I0130 20:35:46.162606 139884786341632 logging_writer.py:48] [4611] accumulated_eval_time=151.591474, accumulated_logging_time=0.142251, accumulated_submission_time=2157.039967, global_step=4611, preemption_count=0, score=2157.039967, test/accuracy=0.098800, test/loss=5.062838, test/num_examples=10000, total_duration=2309.035210, train/accuracy=0.140605, train/loss=4.725738, validation/accuracy=0.130120, validation/loss=4.775651, validation/num_examples=50000
I0130 20:36:22.106615 139884794734336 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.7527719736099243, loss=5.5340657234191895
I0130 20:37:07.951454 139884786341632 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.3503634929656982, loss=6.449416160583496
I0130 20:37:53.859384 139884794734336 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.8452932834625244, loss=5.408026218414307
I0130 20:38:40.027152 139884786341632 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5928312540054321, loss=5.586423873901367
I0130 20:39:25.721481 139884794734336 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.765557885169983, loss=5.456533432006836
I0130 20:40:11.604431 139884786341632 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.864950180053711, loss=5.372303009033203
I0130 20:40:57.681306 139884794734336 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.3584589958190918, loss=6.354146480560303
I0130 20:41:43.355611 139884786341632 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.5860062837600708, loss=6.494044780731201
I0130 20:42:29.286376 139884794734336 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.522041916847229, loss=5.700617790222168
I0130 20:42:46.342606 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:42:58.439985 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:43:06.661814 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:43:08.321310 140085747812160 submission_runner.py:408] Time since start: 2751.21s, 	Step: 5539, 	{'train/accuracy': 0.18742187321186066, 'train/loss': 4.307502269744873, 'validation/accuracy': 0.16944000124931335, 'validation/loss': 4.408010959625244, 'validation/num_examples': 50000, 'test/accuracy': 0.12810000777244568, 'test/loss': 4.742315292358398, 'test/num_examples': 10000, 'score': 2577.1580624580383, 'total_duration': 2751.210842370987, 'accumulated_submission_time': 2577.1580624580383, 'accumulated_eval_time': 173.5701711177826, 'accumulated_logging_time': 0.1694018840789795}
I0130 20:43:08.338865 139884786341632 logging_writer.py:48] [5539] accumulated_eval_time=173.570171, accumulated_logging_time=0.169402, accumulated_submission_time=2577.158062, global_step=5539, preemption_count=0, score=2577.158062, test/accuracy=0.128100, test/loss=4.742315, test/num_examples=10000, total_duration=2751.210842, train/accuracy=0.187422, train/loss=4.307502, validation/accuracy=0.169440, validation/loss=4.408011, validation/num_examples=50000
I0130 20:43:32.718132 139884794734336 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.6466540098190308, loss=5.2836151123046875
I0130 20:44:17.352089 139884786341632 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.6471450328826904, loss=5.567917346954346
I0130 20:45:03.184332 139884794734336 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.3429315090179443, loss=5.8169050216674805
I0130 20:45:48.952379 139884786341632 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.5587778091430664, loss=5.2847418785095215
I0130 20:46:34.864707 139884794734336 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.9667061567306519, loss=5.573970794677734
I0130 20:47:20.692319 139884786341632 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.7129865884780884, loss=5.0972795486450195
I0130 20:48:07.034807 139884794734336 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.4702239036560059, loss=5.626331806182861
I0130 20:48:53.393335 139884786341632 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.6484748125076294, loss=5.0726141929626465
I0130 20:49:39.755020 139884794734336 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.6401852369308472, loss=5.2614054679870605
I0130 20:50:08.363408 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:50:20.398272 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:50:31.026239 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:50:32.684708 140085747812160 submission_runner.py:408] Time since start: 3195.57s, 	Step: 6464, 	{'train/accuracy': 0.22345702350139618, 'train/loss': 4.075367450714111, 'validation/accuracy': 0.20885999500751495, 'validation/loss': 4.152421951293945, 'validation/num_examples': 50000, 'test/accuracy': 0.15880000591278076, 'test/loss': 4.514123916625977, 'test/num_examples': 10000, 'score': 2997.1211307048798, 'total_duration': 3195.5742268562317, 'accumulated_submission_time': 2997.1211307048798, 'accumulated_eval_time': 197.8914442062378, 'accumulated_logging_time': 0.19727015495300293}
I0130 20:50:32.709480 139884786341632 logging_writer.py:48] [6464] accumulated_eval_time=197.891444, accumulated_logging_time=0.197270, accumulated_submission_time=2997.121131, global_step=6464, preemption_count=0, score=2997.121131, test/accuracy=0.158800, test/loss=4.514124, test/num_examples=10000, total_duration=3195.574227, train/accuracy=0.223457, train/loss=4.075367, validation/accuracy=0.208860, validation/loss=4.152422, validation/num_examples=50000
I0130 20:50:47.291872 139884794734336 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.5626813173294067, loss=5.306756019592285
I0130 20:51:30.724423 139884786341632 logging_writer.py:48] [6600] global_step=6600, grad_norm=1.5431538820266724, loss=5.578542709350586
I0130 20:52:16.553227 139884794734336 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.0858131647109985, loss=6.381525993347168
I0130 20:53:02.752264 139884786341632 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.6122162342071533, loss=5.297370433807373
I0130 20:53:48.775817 139884794734336 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8433221578598022, loss=4.941377639770508
I0130 20:54:34.505751 139884786341632 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.4324809312820435, loss=6.397069931030273
I0130 20:55:20.543487 139884794734336 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.426645278930664, loss=6.339537143707275
I0130 20:56:06.480138 139884786341632 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.4725290536880493, loss=6.29453182220459
I0130 20:56:52.649093 139884794734336 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.2776930332183838, loss=6.226548194885254
I0130 20:57:32.919136 140085747812160 spec.py:321] Evaluating on the training split.
I0130 20:57:45.052218 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 20:57:57.430959 140085747812160 spec.py:349] Evaluating on the test split.
I0130 20:57:59.132443 140085747812160 submission_runner.py:408] Time since start: 3642.02s, 	Step: 7389, 	{'train/accuracy': 0.26749998331069946, 'train/loss': 3.717461109161377, 'validation/accuracy': 0.24699999392032623, 'validation/loss': 3.8280117511749268, 'validation/num_examples': 50000, 'test/accuracy': 0.1915000081062317, 'test/loss': 4.239701271057129, 'test/num_examples': 10000, 'score': 3417.268706560135, 'total_duration': 3642.021924495697, 'accumulated_submission_time': 3417.268706560135, 'accumulated_eval_time': 224.10468411445618, 'accumulated_logging_time': 0.23234915733337402}
I0130 20:57:59.164177 139884786341632 logging_writer.py:48] [7389] accumulated_eval_time=224.104684, accumulated_logging_time=0.232349, accumulated_submission_time=3417.268707, global_step=7389, preemption_count=0, score=3417.268707, test/accuracy=0.191500, test/loss=4.239701, test/num_examples=10000, total_duration=3642.021924, train/accuracy=0.267500, train/loss=3.717461, validation/accuracy=0.247000, validation/loss=3.828012, validation/num_examples=50000
I0130 20:58:03.907938 139884794734336 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.6891117095947266, loss=4.762199878692627
I0130 20:58:45.076619 139884786341632 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.4720782041549683, loss=5.4630608558654785
I0130 20:59:30.565694 139884794734336 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.5113028287887573, loss=4.820557594299316
I0130 21:00:16.305080 139884786341632 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.476000428199768, loss=4.812648296356201
I0130 21:01:02.005379 139884794734336 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.8742754459381104, loss=4.840090274810791
I0130 21:01:48.229719 139884786341632 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.2618273496627808, loss=5.61167049407959
I0130 21:02:33.963248 139884794734336 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.6610467433929443, loss=4.667851448059082
I0130 21:03:19.784258 139884786341632 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.5596470832824707, loss=4.754899024963379
I0130 21:04:05.678336 139884794734336 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.1524535417556763, loss=6.229020118713379
I0130 21:04:51.357917 139884786341632 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.6814334392547607, loss=4.8059258460998535
I0130 21:04:59.343264 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:05:11.500486 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:05:27.591231 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:05:29.240008 140085747812160 submission_runner.py:408] Time since start: 4092.13s, 	Step: 8319, 	{'train/accuracy': 0.30726560950279236, 'train/loss': 3.447201728820801, 'validation/accuracy': 0.2823199927806854, 'validation/loss': 3.5882530212402344, 'validation/num_examples': 50000, 'test/accuracy': 0.21450001001358032, 'test/loss': 4.04166316986084, 'test/num_examples': 10000, 'score': 3837.3803448677063, 'total_duration': 4092.129539489746, 'accumulated_submission_time': 3837.3803448677063, 'accumulated_eval_time': 254.00139904022217, 'accumulated_logging_time': 0.27797937393188477}
I0130 21:05:29.260495 139884794734336 logging_writer.py:48] [8319] accumulated_eval_time=254.001399, accumulated_logging_time=0.277979, accumulated_submission_time=3837.380345, global_step=8319, preemption_count=0, score=3837.380345, test/accuracy=0.214500, test/loss=4.041663, test/num_examples=10000, total_duration=4092.129539, train/accuracy=0.307266, train/loss=3.447202, validation/accuracy=0.282320, validation/loss=3.588253, validation/num_examples=50000
I0130 21:06:01.672216 139884786341632 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.588668942451477, loss=4.629473686218262
I0130 21:06:47.494239 139884794734336 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.6573487520217896, loss=4.632051944732666
I0130 21:07:33.434453 139884786341632 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.6787688732147217, loss=4.746321678161621
I0130 21:08:19.502690 139884794734336 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.4559719562530518, loss=5.1351704597473145
I0130 21:09:05.657332 139884786341632 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.9948889017105103, loss=4.558259963989258
I0130 21:09:51.411707 139884794734336 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.6879791021347046, loss=5.659008502960205
I0130 21:10:37.262997 139884786341632 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.6319952011108398, loss=4.593474864959717
I0130 21:11:23.168628 139884794734336 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.1331517696380615, loss=6.202422618865967
I0130 21:12:09.110821 139884786341632 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.132729172706604, loss=6.141654968261719
I0130 21:12:29.657398 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:12:42.871447 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:13:01.700238 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:13:03.402623 140085747812160 submission_runner.py:408] Time since start: 4546.29s, 	Step: 9246, 	{'train/accuracy': 0.34611326456069946, 'train/loss': 3.243919849395752, 'validation/accuracy': 0.3188599944114685, 'validation/loss': 3.357321262359619, 'validation/num_examples': 50000, 'test/accuracy': 0.24560001492500305, 'test/loss': 3.852262020111084, 'test/num_examples': 10000, 'score': 4257.711834192276, 'total_duration': 4546.292115211487, 'accumulated_submission_time': 4257.711834192276, 'accumulated_eval_time': 287.7465693950653, 'accumulated_logging_time': 0.3111603260040283}
I0130 21:13:03.430892 139884794734336 logging_writer.py:48] [9246] accumulated_eval_time=287.746569, accumulated_logging_time=0.311160, accumulated_submission_time=4257.711834, global_step=9246, preemption_count=0, score=4257.711834, test/accuracy=0.245600, test/loss=3.852262, test/num_examples=10000, total_duration=4546.292115, train/accuracy=0.346113, train/loss=3.243920, validation/accuracy=0.318860, validation/loss=3.357321, validation/num_examples=50000
I0130 21:13:25.040200 139884786341632 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.18086838722229, loss=5.932537078857422
I0130 21:14:08.727919 139884794734336 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.1117712259292603, loss=6.158563613891602
I0130 21:14:54.556882 139884786341632 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.0278671979904175, loss=6.219107151031494
I0130 21:15:40.300035 139884794734336 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.6878737211227417, loss=4.620962142944336
I0130 21:16:25.992654 139884786341632 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.66873300075531, loss=4.558520317077637
I0130 21:17:11.899382 139884794734336 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.7196667194366455, loss=4.497627258300781
I0130 21:17:57.356319 139884786341632 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.601945161819458, loss=4.456141471862793
I0130 21:18:43.417211 139884794734336 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4438128471374512, loss=4.527024269104004
I0130 21:19:29.512000 139884786341632 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.524749755859375, loss=5.283027648925781
I0130 21:20:03.592620 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:20:15.901407 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:20:30.802234 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:20:32.444628 140085747812160 submission_runner.py:408] Time since start: 4995.33s, 	Step: 10176, 	{'train/accuracy': 0.36494138836860657, 'train/loss': 3.0633111000061035, 'validation/accuracy': 0.3436799943447113, 'validation/loss': 3.1808507442474365, 'validation/num_examples': 50000, 'test/accuracy': 0.26250001788139343, 'test/loss': 3.7037575244903564, 'test/num_examples': 10000, 'score': 4677.799368858337, 'total_duration': 4995.334156990051, 'accumulated_submission_time': 4677.799368858337, 'accumulated_eval_time': 316.598552942276, 'accumulated_logging_time': 0.35977983474731445}
I0130 21:20:32.466249 139884794734336 logging_writer.py:48] [10176] accumulated_eval_time=316.598553, accumulated_logging_time=0.359780, accumulated_submission_time=4677.799369, global_step=10176, preemption_count=0, score=4677.799369, test/accuracy=0.262500, test/loss=3.703758, test/num_examples=10000, total_duration=4995.334157, train/accuracy=0.364941, train/loss=3.063311, validation/accuracy=0.343680, validation/loss=3.180851, validation/num_examples=50000
I0130 21:20:42.349401 139884786341632 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0803195238113403, loss=5.622827529907227
I0130 21:21:24.653720 139884794734336 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.623188853263855, loss=4.4333086013793945
I0130 21:22:10.763481 139884786341632 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.9277244806289673, loss=4.664090633392334
I0130 21:22:56.954889 139884794734336 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.5543205738067627, loss=4.338735580444336
I0130 21:23:42.720862 139884786341632 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.4223601818084717, loss=4.688762187957764
I0130 21:24:28.651856 139884794734336 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.6198804378509521, loss=4.918020725250244
I0130 21:25:14.545737 139884786341632 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.0385596752166748, loss=6.127367973327637
I0130 21:26:00.494984 139884794734336 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5829188823699951, loss=4.393806457519531
I0130 21:26:46.619380 139884786341632 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.3569800853729248, loss=4.618592739105225
I0130 21:27:32.935416 139884794734336 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.6510075330734253, loss=4.356064319610596
I0130 21:27:32.951754 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:27:45.713162 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:28:01.678366 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:28:03.340055 140085747812160 submission_runner.py:408] Time since start: 5446.23s, 	Step: 11101, 	{'train/accuracy': 0.3976367115974426, 'train/loss': 2.9350926876068115, 'validation/accuracy': 0.3646000027656555, 'validation/loss': 3.088977336883545, 'validation/num_examples': 50000, 'test/accuracy': 0.2785000205039978, 'test/loss': 3.6206798553466797, 'test/num_examples': 10000, 'score': 5098.2224934101105, 'total_duration': 5446.229580163956, 'accumulated_submission_time': 5098.2224934101105, 'accumulated_eval_time': 346.9868311882019, 'accumulated_logging_time': 0.3919997215270996}
I0130 21:28:03.359188 139884786341632 logging_writer.py:48] [11101] accumulated_eval_time=346.986831, accumulated_logging_time=0.392000, accumulated_submission_time=5098.222493, global_step=11101, preemption_count=0, score=5098.222493, test/accuracy=0.278500, test/loss=3.620680, test/num_examples=10000, total_duration=5446.229580, train/accuracy=0.397637, train/loss=2.935093, validation/accuracy=0.364600, validation/loss=3.088977, validation/num_examples=50000
I0130 21:28:44.529616 139884794734336 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.9642390608787537, loss=5.881104946136475
I0130 21:29:30.550638 139884786341632 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.8593858480453491, loss=4.512618541717529
I0130 21:30:16.343205 139884794734336 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.3111152648925781, loss=5.876743316650391
I0130 21:31:02.024700 139884786341632 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.5711606740951538, loss=4.228761196136475
I0130 21:31:47.984103 139884794734336 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.685486912727356, loss=4.263760566711426
I0130 21:32:34.123525 139884786341632 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.5589702129364014, loss=4.311927318572998
I0130 21:33:20.034815 139884794734336 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.4923192262649536, loss=4.34010124206543
I0130 21:34:06.474105 139884786341632 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.6231695413589478, loss=4.139243125915527
I0130 21:34:52.965812 139884794734336 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4598290920257568, loss=4.210677146911621
I0130 21:35:03.811680 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:35:16.089587 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:35:27.964993 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:35:29.626819 140085747812160 submission_runner.py:408] Time since start: 5892.52s, 	Step: 12025, 	{'train/accuracy': 0.4351562261581421, 'train/loss': 2.699234962463379, 'validation/accuracy': 0.3865399956703186, 'validation/loss': 2.927830457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.2924000024795532, 'test/loss': 3.4869821071624756, 'test/num_examples': 10000, 'score': 5518.611259937286, 'total_duration': 5892.516352653503, 'accumulated_submission_time': 5518.611259937286, 'accumulated_eval_time': 372.8019599914551, 'accumulated_logging_time': 0.4224827289581299}
I0130 21:35:29.647168 139884786341632 logging_writer.py:48] [12025] accumulated_eval_time=372.801960, accumulated_logging_time=0.422483, accumulated_submission_time=5518.611260, global_step=12025, preemption_count=0, score=5518.611260, test/accuracy=0.292400, test/loss=3.486982, test/num_examples=10000, total_duration=5892.516353, train/accuracy=0.435156, train/loss=2.699235, validation/accuracy=0.386540, validation/loss=2.927830, validation/num_examples=50000
I0130 21:35:59.511602 139884794734336 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.6456257104873657, loss=4.169549465179443
I0130 21:36:45.213652 139884786341632 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.2103641033172607, loss=5.913884162902832
I0130 21:37:31.069037 139884794734336 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.1635106801986694, loss=5.297341346740723
I0130 21:38:17.038672 139884786341632 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1336276531219482, loss=5.969995021820068
I0130 21:39:03.133613 139884794734336 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.6688778400421143, loss=4.217840194702148
I0130 21:39:49.150500 139884786341632 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.1335307359695435, loss=6.034396171569824
I0130 21:40:35.248415 139884794734336 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4787582159042358, loss=4.23126745223999
I0130 21:41:21.572593 139884786341632 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.356152057647705, loss=4.230616569519043
I0130 21:42:08.367094 139884794734336 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6841148138046265, loss=4.092414379119873
I0130 21:42:29.672440 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:42:42.152358 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:42:55.486048 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:42:57.190424 140085747812160 submission_runner.py:408] Time since start: 6340.08s, 	Step: 12947, 	{'train/accuracy': 0.42998045682907104, 'train/loss': 2.715768575668335, 'validation/accuracy': 0.3989799916744232, 'validation/loss': 2.8612937927246094, 'validation/num_examples': 50000, 'test/accuracy': 0.31060001254081726, 'test/loss': 3.41860294342041, 'test/num_examples': 10000, 'score': 5938.5728850364685, 'total_duration': 6340.0799124240875, 'accumulated_submission_time': 5938.5728850364685, 'accumulated_eval_time': 400.3198812007904, 'accumulated_logging_time': 0.4549741744995117}
I0130 21:42:57.220416 139884786341632 logging_writer.py:48] [12947] accumulated_eval_time=400.319881, accumulated_logging_time=0.454974, accumulated_submission_time=5938.572885, global_step=12947, preemption_count=0, score=5938.572885, test/accuracy=0.310600, test/loss=3.418603, test/num_examples=10000, total_duration=6340.079912, train/accuracy=0.429980, train/loss=2.715769, validation/accuracy=0.398980, validation/loss=2.861294, validation/num_examples=50000
I0130 21:43:18.457848 139884794734336 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9236606955528259, loss=6.050308704376221
I0130 21:44:02.004108 139884786341632 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.3215205669403076, loss=4.304862022399902
I0130 21:44:48.318693 139884794734336 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4519805908203125, loss=4.190070152282715
I0130 21:45:34.555213 139884786341632 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.3251724243164062, loss=4.083017349243164
I0130 21:46:20.699198 139884794734336 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.7520818710327148, loss=4.284337997436523
I0130 21:47:06.584497 139884786341632 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.260362148284912, loss=5.2106451988220215
I0130 21:47:52.506788 139884794734336 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.5705852508544922, loss=4.071067810058594
I0130 21:48:38.782592 139884786341632 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.3978352546691895, loss=4.053632736206055
I0130 21:49:25.371405 139884794734336 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.5643031597137451, loss=4.074389934539795
I0130 21:49:57.530957 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:50:10.843185 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:50:29.704052 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:50:31.344976 140085747812160 submission_runner.py:408] Time since start: 6794.23s, 	Step: 13871, 	{'train/accuracy': 0.44755858182907104, 'train/loss': 2.601588010787964, 'validation/accuracy': 0.41808000206947327, 'validation/loss': 2.7594058513641357, 'validation/num_examples': 50000, 'test/accuracy': 0.3206000030040741, 'test/loss': 3.336005210876465, 'test/num_examples': 10000, 'score': 6358.816624403, 'total_duration': 6794.234518766403, 'accumulated_submission_time': 6358.816624403, 'accumulated_eval_time': 434.1339132785797, 'accumulated_logging_time': 0.499969482421875}
I0130 21:50:31.361258 139884786341632 logging_writer.py:48] [13871] accumulated_eval_time=434.133913, accumulated_logging_time=0.499969, accumulated_submission_time=6358.816624, global_step=13871, preemption_count=0, score=6358.816624, test/accuracy=0.320600, test/loss=3.336005, test/num_examples=10000, total_duration=6794.234519, train/accuracy=0.447559, train/loss=2.601588, validation/accuracy=0.418080, validation/loss=2.759406, validation/num_examples=50000
I0130 21:50:43.121055 139884794734336 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.1864664554595947, loss=4.874412536621094
I0130 21:51:24.318815 139884786341632 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.5946540832519531, loss=4.374196529388428
I0130 21:52:10.716901 139884794734336 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.5286966562271118, loss=4.075164318084717
I0130 21:52:57.414407 139884786341632 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.646716833114624, loss=3.997131109237671
I0130 21:53:43.865213 139884794734336 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.4210927486419678, loss=4.14578914642334
I0130 21:54:30.272077 139884786341632 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.8995557427406311, loss=5.568204879760742
I0130 21:55:16.642508 139884794734336 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.5422927141189575, loss=4.009584426879883
I0130 21:56:02.550455 139884786341632 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.4897006750106812, loss=3.998483180999756
I0130 21:56:48.550168 139884794734336 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9604812264442444, loss=5.752114295959473
I0130 21:57:31.472090 140085747812160 spec.py:321] Evaluating on the training split.
I0130 21:57:45.847605 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 21:58:06.436095 140085747812160 spec.py:349] Evaluating on the test split.
I0130 21:58:08.069007 140085747812160 submission_runner.py:408] Time since start: 7250.96s, 	Step: 14794, 	{'train/accuracy': 0.4675585925579071, 'train/loss': 2.4731810092926025, 'validation/accuracy': 0.4262399971485138, 'validation/loss': 2.686066150665283, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.2613182067871094, 'test/num_examples': 10000, 'score': 6778.866580486298, 'total_duration': 7250.958543777466, 'accumulated_submission_time': 6778.866580486298, 'accumulated_eval_time': 470.73082447052, 'accumulated_logging_time': 0.5254685878753662}
I0130 21:58:08.085581 139884786341632 logging_writer.py:48] [14794] accumulated_eval_time=470.730824, accumulated_logging_time=0.525469, accumulated_submission_time=6778.866580, global_step=14794, preemption_count=0, score=6778.866580, test/accuracy=0.336800, test/loss=3.261318, test/num_examples=10000, total_duration=7250.958544, train/accuracy=0.467559, train/loss=2.473181, validation/accuracy=0.426240, validation/loss=2.686066, validation/num_examples=50000
I0130 21:58:10.832910 139884794734336 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.5328525304794312, loss=3.9756200313568115
I0130 21:58:50.870781 139884786341632 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.9846097826957703, loss=5.6829094886779785
I0130 21:59:37.354328 139884794734336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9518222808837891, loss=5.925132751464844
I0130 22:00:23.815831 139884786341632 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.3863554000854492, loss=3.9816911220550537
I0130 22:01:10.118342 139884794734336 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.4890785217285156, loss=3.8780736923217773
I0130 22:01:56.449187 139884786341632 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.2588679790496826, loss=4.232624530792236
I0130 22:02:42.777081 139884794734336 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.4512782096862793, loss=4.048507213592529
I0130 22:03:29.131769 139884786341632 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.217690110206604, loss=4.342951774597168
I0130 22:04:15.460611 139884794734336 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.5127547979354858, loss=3.9181149005889893
I0130 22:05:01.645513 139884786341632 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.3477425575256348, loss=4.19345235824585
I0130 22:05:08.282745 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:05:23.813476 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:05:44.873894 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:05:46.513025 140085747812160 submission_runner.py:408] Time since start: 7709.40s, 	Step: 15716, 	{'train/accuracy': 0.4583398401737213, 'train/loss': 2.5445070266723633, 'validation/accuracy': 0.43347999453544617, 'validation/loss': 2.6762075424194336, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.258542060852051, 'test/num_examples': 10000, 'score': 7199.002569198608, 'total_duration': 7709.4025728702545, 'accumulated_submission_time': 7199.002569198608, 'accumulated_eval_time': 508.96109414100647, 'accumulated_logging_time': 0.5512490272521973}
I0130 22:05:46.533749 139884794734336 logging_writer.py:48] [15716] accumulated_eval_time=508.961094, accumulated_logging_time=0.551249, accumulated_submission_time=7199.002569, global_step=15716, preemption_count=0, score=7199.002569, test/accuracy=0.331500, test/loss=3.258542, test/num_examples=10000, total_duration=7709.402573, train/accuracy=0.458340, train/loss=2.544507, validation/accuracy=0.433480, validation/loss=2.676208, validation/num_examples=50000
I0130 22:06:20.354258 139884786341632 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.9804586172103882, loss=5.252469062805176
I0130 22:07:06.136008 139884794734336 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5217483043670654, loss=3.947904348373413
I0130 22:07:51.888861 139884786341632 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.321217656135559, loss=3.9140615463256836
I0130 22:08:38.269151 139884794734336 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.35141921043396, loss=4.013822555541992
I0130 22:09:25.097571 139884786341632 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.2376333475112915, loss=4.442386627197266
I0130 22:10:11.761711 139884794734336 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4876841306686401, loss=4.004084587097168
I0130 22:10:58.241095 139884786341632 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4812544584274292, loss=4.152889728546143
I0130 22:11:44.424629 139884794734336 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.2455931901931763, loss=4.3258161544799805
I0130 22:12:30.860488 139884786341632 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4370739459991455, loss=3.88482403755188
I0130 22:12:46.758781 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:13:02.484272 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:13:22.946805 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:13:24.584387 140085747812160 submission_runner.py:408] Time since start: 8167.47s, 	Step: 16636, 	{'train/accuracy': 0.4844140410423279, 'train/loss': 2.364227533340454, 'validation/accuracy': 0.4491399824619293, 'validation/loss': 2.542757511138916, 'validation/num_examples': 50000, 'test/accuracy': 0.34850001335144043, 'test/loss': 3.135037422180176, 'test/num_examples': 10000, 'score': 7619.164701223373, 'total_duration': 8167.473934412003, 'accumulated_submission_time': 7619.164701223373, 'accumulated_eval_time': 546.786703824997, 'accumulated_logging_time': 0.5834970474243164}
I0130 22:13:24.601031 139884794734336 logging_writer.py:48] [16636] accumulated_eval_time=546.786704, accumulated_logging_time=0.583497, accumulated_submission_time=7619.164701, global_step=16636, preemption_count=0, score=7619.164701, test/accuracy=0.348500, test/loss=3.135037, test/num_examples=10000, total_duration=8167.473934, train/accuracy=0.484414, train/loss=2.364228, validation/accuracy=0.449140, validation/loss=2.542758, validation/num_examples=50000
I0130 22:13:50.093871 139884786341632 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.5926384925842285, loss=4.295949935913086
I0130 22:14:35.450041 139884794734336 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.383143424987793, loss=4.465250015258789
I0130 22:15:22.009964 139884786341632 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.49144446849823, loss=4.013867378234863
I0130 22:16:08.542198 139884794734336 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1082401275634766, loss=4.68424654006958
I0130 22:16:55.185415 139884786341632 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.6043628454208374, loss=3.871999979019165
I0130 22:17:41.610100 139884794734336 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4651881456375122, loss=3.9099411964416504
I0130 22:18:28.139119 139884786341632 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.436232566833496, loss=3.95086669921875
I0130 22:19:14.722669 139884794734336 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.2810240983963013, loss=3.9638123512268066
I0130 22:20:01.455502 139884786341632 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0740163326263428, loss=4.727111339569092
I0130 22:20:24.985798 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:20:40.293462 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:20:59.514636 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:21:01.160621 140085747812160 submission_runner.py:408] Time since start: 8624.05s, 	Step: 17552, 	{'train/accuracy': 0.48808592557907104, 'train/loss': 2.3479232788085938, 'validation/accuracy': 0.4491199851036072, 'validation/loss': 2.5377163887023926, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.126204013824463, 'test/num_examples': 10000, 'score': 8039.487378358841, 'total_duration': 8624.050165176392, 'accumulated_submission_time': 8039.487378358841, 'accumulated_eval_time': 582.9615287780762, 'accumulated_logging_time': 0.6108376979827881}
I0130 22:21:01.180160 139884794734336 logging_writer.py:48] [17552] accumulated_eval_time=582.961529, accumulated_logging_time=0.610838, accumulated_submission_time=8039.487378, global_step=17552, preemption_count=0, score=8039.487378, test/accuracy=0.350500, test/loss=3.126204, test/num_examples=10000, total_duration=8624.050165, train/accuracy=0.488086, train/loss=2.347923, validation/accuracy=0.449120, validation/loss=2.537716, validation/num_examples=50000
I0130 22:21:20.393973 139884786341632 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0354174375534058, loss=5.420445919036865
I0130 22:22:04.438343 139884794734336 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5003479719161987, loss=3.9735050201416016
I0130 22:22:51.111413 139884786341632 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.4032788276672363, loss=3.9308900833129883
I0130 22:23:37.904367 139884794734336 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.8548170924186707, loss=5.908079147338867
I0130 22:24:24.253615 139884786341632 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.3372358083724976, loss=4.007137298583984
I0130 22:25:10.748183 139884794734336 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.4698307514190674, loss=3.8919241428375244
I0130 22:25:57.134211 139884786341632 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.5414420366287231, loss=3.86503005027771
I0130 22:26:43.516719 139884794734336 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6699435710906982, loss=3.9934186935424805
I0130 22:27:29.861039 139884786341632 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.541576862335205, loss=3.833982467651367
I0130 22:28:01.467936 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:28:16.775420 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:28:38.491164 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:28:40.131412 140085747812160 submission_runner.py:408] Time since start: 9083.02s, 	Step: 18470, 	{'train/accuracy': 0.494140625, 'train/loss': 2.3677914142608643, 'validation/accuracy': 0.4590199887752533, 'validation/loss': 2.532339334487915, 'validation/num_examples': 50000, 'test/accuracy': 0.3570000231266022, 'test/loss': 3.1239099502563477, 'test/num_examples': 10000, 'score': 8459.713441371918, 'total_duration': 9083.020942211151, 'accumulated_submission_time': 8459.713441371918, 'accumulated_eval_time': 621.6249876022339, 'accumulated_logging_time': 0.6407480239868164}
I0130 22:28:40.149050 139884794734336 logging_writer.py:48] [18470] accumulated_eval_time=621.624988, accumulated_logging_time=0.640748, accumulated_submission_time=8459.713441, global_step=18470, preemption_count=0, score=8459.713441, test/accuracy=0.357000, test/loss=3.123910, test/num_examples=10000, total_duration=9083.020942, train/accuracy=0.494141, train/loss=2.367791, validation/accuracy=0.459020, validation/loss=2.532339, validation/num_examples=50000
I0130 22:28:52.298028 139884786341632 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1713141202926636, loss=3.993100881576538
I0130 22:29:35.228098 139884794734336 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.3021504878997803, loss=4.648151397705078
I0130 22:30:21.300919 139884786341632 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9785704612731934, loss=5.830051422119141
I0130 22:31:08.105666 139884794734336 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.6943790912628174, loss=3.889596700668335
I0130 22:31:54.460540 139884786341632 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2155276536941528, loss=4.189431190490723
I0130 22:32:41.067085 139884794734336 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.3895806074142456, loss=3.8477587699890137
I0130 22:33:27.384297 139884786341632 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5886379480361938, loss=3.8842740058898926
I0130 22:34:13.558548 139884794734336 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.610491156578064, loss=4.5736613273620605
I0130 22:34:59.924277 139884786341632 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.3938943147659302, loss=4.164162635803223
I0130 22:35:40.262715 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:35:54.627859 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:36:18.079494 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:36:19.712500 140085747812160 submission_runner.py:408] Time since start: 9542.60s, 	Step: 19389, 	{'train/accuracy': 0.500292956829071, 'train/loss': 2.304877996444702, 'validation/accuracy': 0.4635799825191498, 'validation/loss': 2.468919515609741, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.0912277698516846, 'test/num_examples': 10000, 'score': 8879.765430927277, 'total_duration': 9542.602039337158, 'accumulated_submission_time': 8879.765430927277, 'accumulated_eval_time': 661.0747628211975, 'accumulated_logging_time': 0.6683251857757568}
I0130 22:36:20.546804 139884794734336 logging_writer.py:48] [19389] accumulated_eval_time=661.074763, accumulated_logging_time=0.668325, accumulated_submission_time=8879.765431, global_step=19389, preemption_count=0, score=8879.765431, test/accuracy=0.357900, test/loss=3.091228, test/num_examples=10000, total_duration=9542.602039, train/accuracy=0.500293, train/loss=2.304878, validation/accuracy=0.463580, validation/loss=2.468920, validation/num_examples=50000
I0130 22:36:25.251386 139884786341632 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.2309150695800781, loss=4.11074161529541
I0130 22:37:05.969291 139884794734336 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.3228763341903687, loss=3.9737894535064697
I0130 22:37:51.750653 139884786341632 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.2795075178146362, loss=3.8661603927612305
I0130 22:38:37.740671 139884794734336 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0593756437301636, loss=4.532344818115234
I0130 22:39:23.871064 139884786341632 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.3464409112930298, loss=3.76658034324646
I0130 22:40:10.741491 139884794734336 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.3504639863967896, loss=3.7825427055358887
I0130 22:40:57.425176 139884786341632 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9313416481018066, loss=5.582919597625732
I0130 22:41:44.260614 139884794734336 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.4095168113708496, loss=3.9532551765441895
I0130 22:42:30.859364 139884786341632 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.9752209782600403, loss=5.178431510925293
I0130 22:43:17.445395 139884794734336 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.453650712966919, loss=3.8991987705230713
I0130 22:43:19.827696 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:43:34.602035 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:43:55.747964 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:43:57.398711 140085747812160 submission_runner.py:408] Time since start: 10000.29s, 	Step: 20307, 	{'train/accuracy': 0.5157226324081421, 'train/loss': 2.2452750205993652, 'validation/accuracy': 0.471780002117157, 'validation/loss': 2.446197986602783, 'validation/num_examples': 50000, 'test/accuracy': 0.3741000294685364, 'test/loss': 3.045219898223877, 'test/num_examples': 10000, 'score': 9298.976362466812, 'total_duration': 10000.288239002228, 'accumulated_submission_time': 9298.976362466812, 'accumulated_eval_time': 698.6457741260529, 'accumulated_logging_time': 1.5207412242889404}
I0130 22:43:57.416236 139884786341632 logging_writer.py:48] [20307] accumulated_eval_time=698.645774, accumulated_logging_time=1.520741, accumulated_submission_time=9298.976362, global_step=20307, preemption_count=0, score=9298.976362, test/accuracy=0.374100, test/loss=3.045220, test/num_examples=10000, total_duration=10000.288239, train/accuracy=0.515723, train/loss=2.245275, validation/accuracy=0.471780, validation/loss=2.446198, validation/num_examples=50000
I0130 22:44:35.654440 139884794734336 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.357591152191162, loss=3.9521212577819824
I0130 22:45:21.726107 139884786341632 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.257974624633789, loss=3.741992235183716
I0130 22:46:08.167207 139884794734336 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.2402129173278809, loss=3.981931686401367
I0130 22:46:54.726422 139884786341632 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8609640598297119, loss=5.770843505859375
I0130 22:47:40.607095 139884794734336 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8296219706535339, loss=5.704002857208252
I0130 22:48:27.258062 139884786341632 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1395689249038696, loss=5.690472602844238
I0130 22:49:13.823605 139884794734336 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2990304231643677, loss=4.778151512145996
I0130 22:50:00.202044 139884786341632 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0826696157455444, loss=5.650552272796631
I0130 22:50:46.552066 139884794734336 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.3302496671676636, loss=3.972583770751953
I0130 22:50:57.734129 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:51:12.533463 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:51:30.538642 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:51:32.195940 140085747812160 submission_runner.py:408] Time since start: 10455.09s, 	Step: 21226, 	{'train/accuracy': 0.5420507788658142, 'train/loss': 2.0856730937957764, 'validation/accuracy': 0.4818599820137024, 'validation/loss': 2.3691818714141846, 'validation/num_examples': 50000, 'test/accuracy': 0.3777000308036804, 'test/loss': 2.9841349124908447, 'test/num_examples': 10000, 'score': 9719.232017755508, 'total_duration': 10455.08547091484, 'accumulated_submission_time': 9719.232017755508, 'accumulated_eval_time': 733.1075923442841, 'accumulated_logging_time': 1.548471212387085}
I0130 22:51:32.221512 139884786341632 logging_writer.py:48] [21226] accumulated_eval_time=733.107592, accumulated_logging_time=1.548471, accumulated_submission_time=9719.232018, global_step=21226, preemption_count=0, score=9719.232018, test/accuracy=0.377700, test/loss=2.984135, test/num_examples=10000, total_duration=10455.085471, train/accuracy=0.542051, train/loss=2.085673, validation/accuracy=0.481860, validation/loss=2.369182, validation/num_examples=50000
I0130 22:52:02.793904 139884794734336 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.3740553855895996, loss=3.8578360080718994
I0130 22:52:49.247502 139884786341632 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.0680952072143555, loss=5.198733329772949
I0130 22:53:35.622414 139884794734336 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.8044488430023193, loss=3.7078287601470947
I0130 22:54:21.917620 139884786341632 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.8537771105766296, loss=5.624101638793945
I0130 22:55:08.256003 139884794734336 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.1113475561141968, loss=4.660679817199707
I0130 22:55:54.415753 139884786341632 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.2205443382263184, loss=4.024322986602783
I0130 22:56:40.959442 139884794734336 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.3377238512039185, loss=3.7676985263824463
I0130 22:57:27.507945 139884786341632 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0081714391708374, loss=4.874507904052734
I0130 22:58:13.824114 139884794734336 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.3025511503219604, loss=3.7123939990997314
I0130 22:58:32.575529 140085747812160 spec.py:321] Evaluating on the training split.
I0130 22:58:43.472135 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 22:59:04.583205 140085747812160 spec.py:349] Evaluating on the test split.
I0130 22:59:06.223949 140085747812160 submission_runner.py:408] Time since start: 10909.11s, 	Step: 22142, 	{'train/accuracy': 0.5222460627555847, 'train/loss': 2.2017922401428223, 'validation/accuracy': 0.48715999722480774, 'validation/loss': 2.363506555557251, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.985949754714966, 'test/num_examples': 10000, 'score': 10139.521540164948, 'total_duration': 10909.113496303558, 'accumulated_submission_time': 10139.521540164948, 'accumulated_eval_time': 766.7560062408447, 'accumulated_logging_time': 1.5872962474822998}
I0130 22:59:06.243988 139884786341632 logging_writer.py:48] [22142] accumulated_eval_time=766.756006, accumulated_logging_time=1.587296, accumulated_submission_time=10139.521540, global_step=22142, preemption_count=0, score=10139.521540, test/accuracy=0.380700, test/loss=2.985950, test/num_examples=10000, total_duration=10909.113496, train/accuracy=0.522246, train/loss=2.201792, validation/accuracy=0.487160, validation/loss=2.363507, validation/num_examples=50000
I0130 22:59:29.382840 139884794734336 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.4561176300048828, loss=3.8140907287597656
I0130 23:00:13.672328 139884786341632 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.2450239658355713, loss=3.7186059951782227
I0130 23:01:00.122012 139884794734336 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9567053318023682, loss=5.659094333648682
I0130 23:01:46.224931 139884786341632 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.9109190106391907, loss=5.727263927459717
I0130 23:02:32.796436 139884794734336 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.3285200595855713, loss=4.366584777832031
I0130 23:03:19.076501 139884786341632 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.3661344051361084, loss=3.952103853225708
I0130 23:04:05.219731 139884794734336 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1857447624206543, loss=4.028304100036621
I0130 23:04:51.318869 139884786341632 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.320840835571289, loss=3.7644853591918945
I0130 23:05:37.543446 139884794734336 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8921905755996704, loss=5.771596908569336
I0130 23:06:06.409410 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:06:16.513939 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:06:37.896149 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:06:39.540635 140085747812160 submission_runner.py:408] Time since start: 11362.43s, 	Step: 23064, 	{'train/accuracy': 0.5399999618530273, 'train/loss': 2.108938455581665, 'validation/accuracy': 0.499099999666214, 'validation/loss': 2.298219919204712, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.9014742374420166, 'test/num_examples': 10000, 'score': 10559.62688589096, 'total_duration': 11362.430181026459, 'accumulated_submission_time': 10559.62688589096, 'accumulated_eval_time': 799.8872475624084, 'accumulated_logging_time': 1.6160616874694824}
I0130 23:06:39.560678 139884786341632 logging_writer.py:48] [23064] accumulated_eval_time=799.887248, accumulated_logging_time=1.616062, accumulated_submission_time=10559.626886, global_step=23064, preemption_count=0, score=10559.626886, test/accuracy=0.389900, test/loss=2.901474, test/num_examples=10000, total_duration=11362.430181, train/accuracy=0.540000, train/loss=2.108938, validation/accuracy=0.499100, validation/loss=2.298220, validation/num_examples=50000
I0130 23:06:54.082442 139884794734336 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.424643874168396, loss=3.7365736961364746
I0130 23:07:37.194540 139884786341632 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0457932949066162, loss=5.377885818481445
I0130 23:08:23.301089 139884794734336 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.676532506942749, loss=3.7834036350250244
I0130 23:09:09.815437 139884786341632 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.3494542837142944, loss=4.2397685050964355
I0130 23:09:56.061500 139884794734336 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.52182137966156, loss=3.7060818672180176
I0130 23:10:42.339465 139884786341632 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.508665680885315, loss=3.655724048614502
I0130 23:11:28.782011 139884794734336 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.1809566020965576, loss=4.140776634216309
I0130 23:12:15.185697 139884786341632 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1213423013687134, loss=4.168757915496826
I0130 23:13:02.031805 139884794734336 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.9071064591407776, loss=5.708869934082031
I0130 23:13:39.959649 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:13:50.310686 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:14:11.758835 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:14:13.402713 140085747812160 submission_runner.py:408] Time since start: 11816.29s, 	Step: 23984, 	{'train/accuracy': 0.5517382621765137, 'train/loss': 2.0394484996795654, 'validation/accuracy': 0.5036199688911438, 'validation/loss': 2.2866790294647217, 'validation/num_examples': 50000, 'test/accuracy': 0.39010003209114075, 'test/loss': 2.90531587600708, 'test/num_examples': 10000, 'score': 10979.964556217194, 'total_duration': 11816.292253255844, 'accumulated_submission_time': 10979.964556217194, 'accumulated_eval_time': 833.3303182125092, 'accumulated_logging_time': 1.6461646556854248}
I0130 23:14:13.426275 139884786341632 logging_writer.py:48] [23984] accumulated_eval_time=833.330318, accumulated_logging_time=1.646165, accumulated_submission_time=10979.964556, global_step=23984, preemption_count=0, score=10979.964556, test/accuracy=0.390100, test/loss=2.905316, test/num_examples=10000, total_duration=11816.292253, train/accuracy=0.551738, train/loss=2.039448, validation/accuracy=0.503620, validation/loss=2.286679, validation/num_examples=50000
I0130 23:14:20.103443 139884794734336 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1446731090545654, loss=4.319785118103027
I0130 23:15:02.261988 139884786341632 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.5763640403747559, loss=3.6969871520996094
I0130 23:15:48.507715 139884794734336 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0352113246917725, loss=5.712244987487793
I0130 23:16:35.226902 139884786341632 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0702307224273682, loss=4.541460037231445
I0130 23:17:21.629886 139884794734336 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0796602964401245, loss=4.835906028747559
I0130 23:18:08.027406 139884786341632 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.3475223779678345, loss=3.7413296699523926
I0130 23:18:53.984238 139884794734336 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3509624004364014, loss=3.759366273880005
I0130 23:19:40.321053 139884786341632 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.119957447052002, loss=5.7508344650268555
I0130 23:20:26.565614 139884794734336 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.2994294166564941, loss=3.542072057723999
I0130 23:21:12.748364 139884786341632 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.4912039041519165, loss=3.7775230407714844
I0130 23:21:13.824688 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:21:23.823353 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:21:45.581181 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:21:47.219057 140085747812160 submission_runner.py:408] Time since start: 12270.11s, 	Step: 24904, 	{'train/accuracy': 0.541796863079071, 'train/loss': 2.0832176208496094, 'validation/accuracy': 0.5054000020027161, 'validation/loss': 2.2420337200164795, 'validation/num_examples': 50000, 'test/accuracy': 0.3970000147819519, 'test/loss': 2.8567094802856445, 'test/num_examples': 10000, 'score': 11400.30337524414, 'total_duration': 12270.108603954315, 'accumulated_submission_time': 11400.30337524414, 'accumulated_eval_time': 866.7247035503387, 'accumulated_logging_time': 1.6784873008728027}
I0130 23:21:47.242759 139884794734336 logging_writer.py:48] [24904] accumulated_eval_time=866.724704, accumulated_logging_time=1.678487, accumulated_submission_time=11400.303375, global_step=24904, preemption_count=0, score=11400.303375, test/accuracy=0.397000, test/loss=2.856709, test/num_examples=10000, total_duration=12270.108604, train/accuracy=0.541797, train/loss=2.083218, validation/accuracy=0.505400, validation/loss=2.242034, validation/num_examples=50000
I0130 23:22:26.963144 139884786341632 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.9513381719589233, loss=5.307951927185059
I0130 23:23:13.482521 139884794734336 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.9223494529724121, loss=5.002499580383301
I0130 23:24:00.137548 139884786341632 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.3481005430221558, loss=3.7473433017730713
I0130 23:24:46.577103 139884794734336 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.4711946249008179, loss=3.64152193069458
I0130 23:25:34.886614 139884786341632 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.386527419090271, loss=3.7607734203338623
I0130 23:26:23.051142 139884794734336 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.3930277824401855, loss=4.375123023986816
I0130 23:27:09.597814 139884786341632 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0428212881088257, loss=5.738567352294922
I0130 23:27:56.224432 139884794734336 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.8290998935699463, loss=3.701347589492798
I0130 23:28:43.247448 139884786341632 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.497255563735962, loss=3.673466920852661
I0130 23:28:47.603196 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:28:57.997166 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:29:19.466636 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:29:21.110530 140085747812160 submission_runner.py:408] Time since start: 12724.00s, 	Step: 25811, 	{'train/accuracy': 0.5504101514816284, 'train/loss': 2.0340700149536133, 'validation/accuracy': 0.5138599872589111, 'validation/loss': 2.201979875564575, 'validation/num_examples': 50000, 'test/accuracy': 0.40130001306533813, 'test/loss': 2.8194544315338135, 'test/num_examples': 10000, 'score': 11820.599084615707, 'total_duration': 12724.000076532364, 'accumulated_submission_time': 11820.599084615707, 'accumulated_eval_time': 900.2320485115051, 'accumulated_logging_time': 1.71818208694458}
I0130 23:29:21.132479 139884794734336 logging_writer.py:48] [25811] accumulated_eval_time=900.232049, accumulated_logging_time=1.718182, accumulated_submission_time=11820.599085, global_step=25811, preemption_count=0, score=11820.599085, test/accuracy=0.401300, test/loss=2.819454, test/num_examples=10000, total_duration=12724.000077, train/accuracy=0.550410, train/loss=2.034070, validation/accuracy=0.513860, validation/loss=2.201980, validation/num_examples=50000
I0130 23:29:57.730992 139884786341632 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.3727567195892334, loss=3.666177988052368
I0130 23:30:44.146120 139884794734336 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0701199769973755, loss=4.6434221267700195
I0130 23:31:31.282418 139884786341632 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.5500178337097168, loss=3.6275734901428223
I0130 23:32:17.836168 139884794734336 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.5036799907684326, loss=3.6958651542663574
I0130 23:33:04.695506 139884786341632 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.237276554107666, loss=4.761890888214111
I0130 23:33:51.032536 139884794734336 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.1872265338897705, loss=4.593261241912842
I0130 23:34:37.593464 139884786341632 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.3204227685928345, loss=3.725980758666992
I0130 23:35:24.090072 139884794734336 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.2213748693466187, loss=3.7330026626586914
I0130 23:36:10.300436 139884786341632 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.1156631708145142, loss=4.372858047485352
I0130 23:36:21.117920 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:36:31.127979 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:36:53.239717 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:36:54.884108 140085747812160 submission_runner.py:408] Time since start: 13177.77s, 	Step: 26725, 	{'train/accuracy': 0.566210925579071, 'train/loss': 2.00087571144104, 'validation/accuracy': 0.5141199827194214, 'validation/loss': 2.229660987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.832810878753662, 'test/num_examples': 10000, 'score': 12240.525072574615, 'total_duration': 13177.773655176163, 'accumulated_submission_time': 12240.525072574615, 'accumulated_eval_time': 933.998238325119, 'accumulated_logging_time': 1.7490291595458984}
I0130 23:36:54.905019 139884794734336 logging_writer.py:48] [26725] accumulated_eval_time=933.998238, accumulated_logging_time=1.749029, accumulated_submission_time=12240.525073, global_step=26725, preemption_count=0, score=12240.525073, test/accuracy=0.407000, test/loss=2.832811, test/num_examples=10000, total_duration=13177.773655, train/accuracy=0.566211, train/loss=2.000876, validation/accuracy=0.514120, validation/loss=2.229661, validation/num_examples=50000
I0130 23:37:24.959225 139884786341632 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.1974323987960815, loss=3.965045690536499
I0130 23:38:10.786455 139884794734336 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.0564069747924805, loss=3.632399320602417
I0130 23:38:57.455033 139884786341632 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1331530809402466, loss=5.1867265701293945
I0130 23:39:43.959307 139884794734336 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.1577647924423218, loss=4.960824966430664
I0130 23:40:30.163257 139884786341632 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.244970679283142, loss=4.982001304626465
I0130 23:41:16.675906 139884794734336 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.336591124534607, loss=3.5217881202697754
I0130 23:42:03.038573 139884786341632 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0242273807525635, loss=5.238153457641602
I0130 23:42:49.338973 139884794734336 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2648428678512573, loss=3.9948158264160156
I0130 23:43:35.936254 139884786341632 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.2259060144424438, loss=3.8986566066741943
I0130 23:43:55.012142 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:44:05.652730 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:44:26.088047 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:44:27.726580 140085747812160 submission_runner.py:408] Time since start: 13630.62s, 	Step: 27643, 	{'train/accuracy': 0.5484570264816284, 'train/loss': 2.040403366088867, 'validation/accuracy': 0.519760012626648, 'validation/loss': 2.1909496784210205, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.8294668197631836, 'test/num_examples': 10000, 'score': 12660.572672367096, 'total_duration': 13630.616124868393, 'accumulated_submission_time': 12660.572672367096, 'accumulated_eval_time': 966.7126722335815, 'accumulated_logging_time': 1.7793781757354736}
I0130 23:44:27.750306 139884794734336 logging_writer.py:48] [27643] accumulated_eval_time=966.712672, accumulated_logging_time=1.779378, accumulated_submission_time=12660.572672, global_step=27643, preemption_count=0, score=12660.572672, test/accuracy=0.404300, test/loss=2.829467, test/num_examples=10000, total_duration=13630.616125, train/accuracy=0.548457, train/loss=2.040403, validation/accuracy=0.519760, validation/loss=2.190950, validation/num_examples=50000
I0130 23:44:50.497742 139884786341632 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.3406552076339722, loss=3.595386505126953
I0130 23:45:35.562288 139884794734336 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.4391554594039917, loss=3.5451254844665527
I0130 23:46:22.224583 139884786341632 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.382919192314148, loss=3.64570689201355
I0130 23:47:08.791497 139884794734336 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1764309406280518, loss=5.344028949737549
I0130 23:47:55.168541 139884786341632 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.473625659942627, loss=3.5032639503479004
I0130 23:48:41.828675 139884794734336 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.3952380418777466, loss=3.6596477031707764
I0130 23:49:28.391806 139884786341632 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.4951646327972412, loss=3.5234317779541016
I0130 23:50:14.401796 139884794734336 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.4902390241622925, loss=3.936548948287964
I0130 23:51:00.879564 139884786341632 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1397563219070435, loss=5.22020959854126
I0130 23:51:27.888958 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:51:38.335637 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:51:58.007474 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:51:59.674496 140085747812160 submission_runner.py:408] Time since start: 14082.56s, 	Step: 28560, 	{'train/accuracy': 0.560546875, 'train/loss': 1.999878168106079, 'validation/accuracy': 0.5216999650001526, 'validation/loss': 2.1805710792541504, 'validation/num_examples': 50000, 'test/accuracy': 0.4077000319957733, 'test/loss': 2.8053174018859863, 'test/num_examples': 10000, 'score': 13080.651557922363, 'total_duration': 14082.56403541565, 'accumulated_submission_time': 13080.651557922363, 'accumulated_eval_time': 998.4982385635376, 'accumulated_logging_time': 1.811950922012329}
I0130 23:51:59.695866 139884794734336 logging_writer.py:48] [28560] accumulated_eval_time=998.498239, accumulated_logging_time=1.811951, accumulated_submission_time=13080.651558, global_step=28560, preemption_count=0, score=13080.651558, test/accuracy=0.407700, test/loss=2.805317, test/num_examples=10000, total_duration=14082.564035, train/accuracy=0.560547, train/loss=1.999878, validation/accuracy=0.521700, validation/loss=2.180571, validation/num_examples=50000
I0130 23:52:15.791459 139884786341632 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.3663690090179443, loss=3.462423324584961
I0130 23:53:00.011163 139884794734336 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.5302265882492065, loss=3.654175043106079
I0130 23:53:46.736577 139884786341632 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.759865403175354, loss=3.778299570083618
I0130 23:54:33.044985 139884794734336 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.3285479545593262, loss=3.696286916732788
I0130 23:55:19.460648 139884786341632 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.0499931573867798, loss=4.556058883666992
I0130 23:56:05.906841 139884794734336 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.1171150207519531, loss=5.752851486206055
I0130 23:56:52.105618 139884786341632 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.5785695314407349, loss=3.5062265396118164
I0130 23:57:38.555110 139884794734336 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.9641351699829102, loss=5.806744575500488
I0130 23:58:24.987797 139884786341632 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.177526593208313, loss=4.379580020904541
I0130 23:58:59.892440 140085747812160 spec.py:321] Evaluating on the training split.
I0130 23:59:10.421533 140085747812160 spec.py:333] Evaluating on the validation split.
I0130 23:59:29.883978 140085747812160 spec.py:349] Evaluating on the test split.
I0130 23:59:31.530261 140085747812160 submission_runner.py:408] Time since start: 14534.42s, 	Step: 29477, 	{'train/accuracy': 0.5726562142372131, 'train/loss': 1.928755760192871, 'validation/accuracy': 0.5296599864959717, 'validation/loss': 2.13201904296875, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.758517026901245, 'test/num_examples': 10000, 'score': 13500.786858081818, 'total_duration': 14534.419796228409, 'accumulated_submission_time': 13500.786858081818, 'accumulated_eval_time': 1030.136039018631, 'accumulated_logging_time': 1.8442060947418213}
I0130 23:59:31.551807 139884794734336 logging_writer.py:48] [29477] accumulated_eval_time=1030.136039, accumulated_logging_time=1.844206, accumulated_submission_time=13500.786858, global_step=29477, preemption_count=0, score=13500.786858, test/accuracy=0.416600, test/loss=2.758517, test/num_examples=10000, total_duration=14534.419796, train/accuracy=0.572656, train/loss=1.928756, validation/accuracy=0.529660, validation/loss=2.132019, validation/num_examples=50000
I0130 23:59:40.966711 139884786341632 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.3200682401657104, loss=4.22260856628418
I0131 00:00:24.098151 139884794734336 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.014831304550171, loss=5.006468296051025
I0131 00:01:10.244436 139884786341632 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.9999203085899353, loss=5.688665390014648
I0131 00:01:56.885090 139884794734336 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.3581281900405884, loss=3.5497658252716064
I0131 00:02:43.279577 139884786341632 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.3372606039047241, loss=3.6749820709228516
I0131 00:03:29.587289 139884794734336 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.127437949180603, loss=5.716855049133301
I0131 00:04:16.301919 139884786341632 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5506757497787476, loss=3.6002652645111084
I0131 00:05:02.629100 139884794734336 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2707953453063965, loss=4.112058162689209
I0131 00:05:49.085280 139884786341632 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.4008746147155762, loss=3.574974298477173
I0131 00:06:31.558606 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:06:42.125513 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:07:02.982888 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:07:04.621714 140085747812160 submission_runner.py:408] Time since start: 14987.51s, 	Step: 30393, 	{'train/accuracy': 0.5676171779632568, 'train/loss': 1.9483190774917603, 'validation/accuracy': 0.5301600098609924, 'validation/loss': 2.114281415939331, 'validation/num_examples': 50000, 'test/accuracy': 0.4150000214576721, 'test/loss': 2.7555928230285645, 'test/num_examples': 10000, 'score': 13920.732541561127, 'total_duration': 14987.511257886887, 'accumulated_submission_time': 13920.732541561127, 'accumulated_eval_time': 1063.1991493701935, 'accumulated_logging_time': 1.8761727809906006}
I0131 00:07:04.640951 139884794734336 logging_writer.py:48] [30393] accumulated_eval_time=1063.199149, accumulated_logging_time=1.876173, accumulated_submission_time=13920.732542, global_step=30393, preemption_count=0, score=13920.732542, test/accuracy=0.415000, test/loss=2.755593, test/num_examples=10000, total_duration=14987.511258, train/accuracy=0.567617, train/loss=1.948319, validation/accuracy=0.530160, validation/loss=2.114281, validation/num_examples=50000
I0131 00:07:07.782991 139884786341632 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4005920886993408, loss=3.6290171146392822
I0131 00:07:49.312288 139884794734336 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3922792673110962, loss=3.4909636974334717
I0131 00:08:35.730894 139884786341632 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9752746820449829, loss=5.692786693572998
I0131 00:09:22.136542 139884794734336 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.3935742378234863, loss=3.4090468883514404
I0131 00:10:08.588494 139884786341632 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.4203462600708008, loss=3.5381951332092285
I0131 00:10:54.983638 139884794734336 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.4020886421203613, loss=3.527884006500244
I0131 00:11:41.460930 139884786341632 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.4327757358551025, loss=3.54286527633667
I0131 00:12:27.837339 139884794734336 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.4422366619110107, loss=3.508392333984375
I0131 00:13:14.512051 139884786341632 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.3385076522827148, loss=3.859494686126709
I0131 00:14:01.198428 139884794734336 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.2566639184951782, loss=3.8417022228240967
I0131 00:14:04.628926 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:14:14.934975 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:14:36.282696 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:14:37.922011 140085747812160 submission_runner.py:408] Time since start: 15440.81s, 	Step: 31309, 	{'train/accuracy': 0.5728124976158142, 'train/loss': 1.927043080329895, 'validation/accuracy': 0.537559986114502, 'validation/loss': 2.104546546936035, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7284553050994873, 'test/num_examples': 10000, 'score': 14340.659535884857, 'total_duration': 15440.811559438705, 'accumulated_submission_time': 14340.659535884857, 'accumulated_eval_time': 1096.492238998413, 'accumulated_logging_time': 1.905264139175415}
I0131 00:14:37.944096 139884786341632 logging_writer.py:48] [31309] accumulated_eval_time=1096.492239, accumulated_logging_time=1.905264, accumulated_submission_time=14340.659536, global_step=31309, preemption_count=0, score=14340.659536, test/accuracy=0.422800, test/loss=2.728455, test/num_examples=10000, total_duration=15440.811559, train/accuracy=0.572812, train/loss=1.927043, validation/accuracy=0.537560, validation/loss=2.104547, validation/num_examples=50000
I0131 00:15:15.302852 139884794734336 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.31333589553833, loss=4.330877304077148
I0131 00:16:01.362158 139884786341632 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.548439621925354, loss=3.4388904571533203
I0131 00:16:47.898854 139884794734336 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.6738314628601074, loss=3.5187156200408936
I0131 00:17:34.143014 139884786341632 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.3389418125152588, loss=3.4994874000549316
I0131 00:18:20.388788 139884794734336 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.54684317111969, loss=3.5975699424743652
I0131 00:19:06.900033 139884786341632 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.515579342842102, loss=3.4795150756835938
I0131 00:19:53.569316 139884794734336 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.2783794403076172, loss=4.518467903137207
I0131 00:20:39.792942 139884786341632 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.6957769393920898, loss=3.495502471923828
I0131 00:21:26.309275 139884794734336 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.622565746307373, loss=3.4220147132873535
I0131 00:21:37.963073 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:21:48.354044 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:22:08.598820 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:22:10.238803 140085747812160 submission_runner.py:408] Time since start: 15893.13s, 	Step: 32227, 	{'train/accuracy': 0.5775781273841858, 'train/loss': 1.9075417518615723, 'validation/accuracy': 0.5331599712371826, 'validation/loss': 2.1145474910736084, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.728424072265625, 'test/num_examples': 10000, 'score': 14760.618643760681, 'total_duration': 15893.128350257874, 'accumulated_submission_time': 14760.618643760681, 'accumulated_eval_time': 1128.7679710388184, 'accumulated_logging_time': 1.9361648559570312}
I0131 00:22:10.258137 139884786341632 logging_writer.py:48] [32227] accumulated_eval_time=1128.767971, accumulated_logging_time=1.936165, accumulated_submission_time=14760.618644, global_step=32227, preemption_count=0, score=14760.618644, test/accuracy=0.424400, test/loss=2.728424, test/num_examples=10000, total_duration=15893.128350, train/accuracy=0.577578, train/loss=1.907542, validation/accuracy=0.533160, validation/loss=2.114547, validation/num_examples=50000
I0131 00:22:39.808673 139884794734336 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.355210542678833, loss=3.531332492828369
I0131 00:23:26.224759 139884786341632 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.3515076637268066, loss=4.007159233093262
I0131 00:24:12.553666 139884794734336 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.462868332862854, loss=3.521390438079834
I0131 00:24:59.118165 139884786341632 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.323778748512268, loss=3.901521682739258
I0131 00:25:45.506640 139884794734336 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1848126649856567, loss=3.964428424835205
I0131 00:26:32.345675 139884786341632 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.2855122089385986, loss=3.6656429767608643
I0131 00:27:18.847869 139884794734336 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.6598111391067505, loss=3.5450711250305176
I0131 00:28:05.303354 139884786341632 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.5732356309890747, loss=3.542543888092041
I0131 00:28:51.678614 139884794734336 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.4175320863723755, loss=3.5631866455078125
I0131 00:29:10.554694 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:29:20.795398 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:29:41.860540 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:29:43.499459 140085747812160 submission_runner.py:408] Time since start: 16346.39s, 	Step: 33142, 	{'train/accuracy': 0.6056835651397705, 'train/loss': 1.7929567098617554, 'validation/accuracy': 0.5388399958610535, 'validation/loss': 2.0905473232269287, 'validation/num_examples': 50000, 'test/accuracy': 0.4270000159740448, 'test/loss': 2.7143259048461914, 'test/num_examples': 10000, 'score': 15180.854577302933, 'total_duration': 16346.38900589943, 'accumulated_submission_time': 15180.854577302933, 'accumulated_eval_time': 1161.712742805481, 'accumulated_logging_time': 1.9648699760437012}
I0131 00:29:43.520419 139884786341632 logging_writer.py:48] [33142] accumulated_eval_time=1161.712743, accumulated_logging_time=1.964870, accumulated_submission_time=15180.854577, global_step=33142, preemption_count=0, score=15180.854577, test/accuracy=0.427000, test/loss=2.714326, test/num_examples=10000, total_duration=16346.389006, train/accuracy=0.605684, train/loss=1.792957, validation/accuracy=0.538840, validation/loss=2.090547, validation/num_examples=50000
I0131 00:30:06.665765 139884794734336 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2306991815567017, loss=4.337687015533447
I0131 00:30:51.636042 139884786341632 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.591588020324707, loss=3.5262789726257324
I0131 00:31:38.127790 139884794734336 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1359341144561768, loss=4.341958522796631
I0131 00:32:24.704029 139884786341632 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.5568761825561523, loss=3.607748508453369
I0131 00:33:10.999747 139884794734336 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2773572206497192, loss=3.985971450805664
I0131 00:33:57.609560 139884786341632 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.468854546546936, loss=3.605889320373535
I0131 00:34:44.372312 139884794734336 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.4206653833389282, loss=3.517484426498413
I0131 00:35:30.853337 139884786341632 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1559008359909058, loss=5.049483299255371
I0131 00:36:17.350093 139884794734336 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.3346902132034302, loss=3.4607510566711426
I0131 00:36:43.861674 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:36:53.887796 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:37:14.252826 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:37:15.896197 140085747812160 submission_runner.py:408] Time since start: 16798.79s, 	Step: 34059, 	{'train/accuracy': 0.577441394329071, 'train/loss': 1.9018783569335938, 'validation/accuracy': 0.5426999926567078, 'validation/loss': 2.0791501998901367, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.717620372772217, 'test/num_examples': 10000, 'score': 15601.136335372925, 'total_duration': 16798.78573513031, 'accumulated_submission_time': 15601.136335372925, 'accumulated_eval_time': 1193.7472488880157, 'accumulated_logging_time': 1.9952008724212646}
I0131 00:37:15.919089 139884786341632 logging_writer.py:48] [34059] accumulated_eval_time=1193.747249, accumulated_logging_time=1.995201, accumulated_submission_time=15601.136335, global_step=34059, preemption_count=0, score=15601.136335, test/accuracy=0.422700, test/loss=2.717620, test/num_examples=10000, total_duration=16798.785735, train/accuracy=0.577441, train/loss=1.901878, validation/accuracy=0.542700, validation/loss=2.079150, validation/num_examples=50000
I0131 00:37:32.405718 139884794734336 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1991157531738281, loss=5.381577968597412
I0131 00:38:16.579541 139884786341632 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.9982545971870422, loss=5.674258708953857
I0131 00:39:03.051973 139884794734336 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.5884286165237427, loss=3.448368549346924
I0131 00:39:49.259661 139884786341632 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.2656832933425903, loss=4.077171325683594
I0131 00:40:35.428720 139884794734336 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.239812970161438, loss=3.961979866027832
I0131 00:41:21.935952 139884786341632 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.3786324262619019, loss=3.705836057662964
I0131 00:42:08.271476 139884794734336 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.4012644290924072, loss=3.532360553741455
I0131 00:42:54.991552 139884786341632 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.4380276203155518, loss=3.3493213653564453
I0131 00:43:41.269292 139884794734336 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.216012954711914, loss=5.448099136352539
I0131 00:44:16.210431 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:44:26.377927 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:44:45.983813 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:44:47.634085 140085747812160 submission_runner.py:408] Time since start: 17250.52s, 	Step: 34977, 	{'train/accuracy': 0.5922070145606995, 'train/loss': 1.842816710472107, 'validation/accuracy': 0.5476399660110474, 'validation/loss': 2.05157208442688, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.695765256881714, 'test/num_examples': 10000, 'score': 16021.365535497665, 'total_duration': 17250.523627996445, 'accumulated_submission_time': 16021.365535497665, 'accumulated_eval_time': 1225.1709115505219, 'accumulated_logging_time': 2.0287718772888184}
I0131 00:44:47.653990 139884786341632 logging_writer.py:48] [34977] accumulated_eval_time=1225.170912, accumulated_logging_time=2.028772, accumulated_submission_time=16021.365535, global_step=34977, preemption_count=0, score=16021.365535, test/accuracy=0.429500, test/loss=2.695765, test/num_examples=10000, total_duration=17250.523628, train/accuracy=0.592207, train/loss=1.842817, validation/accuracy=0.547640, validation/loss=2.051572, validation/num_examples=50000
I0131 00:44:57.081887 139884794734336 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.417318344116211, loss=3.441189765930176
I0131 00:45:40.007933 139884786341632 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.531023383140564, loss=3.486084461212158
I0131 00:46:26.185901 139884794734336 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.053112268447876, loss=5.6133575439453125
I0131 00:47:12.837086 139884786341632 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.9168505072593689, loss=5.399340629577637
I0131 00:47:59.136464 139884794734336 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.36154043674469, loss=3.9912784099578857
I0131 00:48:45.454387 139884786341632 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.474064588546753, loss=3.4400134086608887
I0131 00:49:31.775283 139884794734336 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.8937164545059204, loss=3.6068663597106934
I0131 00:50:18.271931 139884786341632 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.550119400024414, loss=3.435272216796875
I0131 00:51:04.392610 139884794734336 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.2622724771499634, loss=4.709062576293945
I0131 00:51:48.053326 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:51:57.953557 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:52:18.527267 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:52:20.198056 140085747812160 submission_runner.py:408] Time since start: 17703.09s, 	Step: 35896, 	{'train/accuracy': 0.6042382717132568, 'train/loss': 1.7828890085220337, 'validation/accuracy': 0.5482999682426453, 'validation/loss': 2.0370826721191406, 'validation/num_examples': 50000, 'test/accuracy': 0.4358000159263611, 'test/loss': 2.6567018032073975, 'test/num_examples': 10000, 'score': 16441.70537519455, 'total_duration': 17703.087596178055, 'accumulated_submission_time': 16441.70537519455, 'accumulated_eval_time': 1257.3156542778015, 'accumulated_logging_time': 2.0581812858581543}
I0131 00:52:20.218863 139884786341632 logging_writer.py:48] [35896] accumulated_eval_time=1257.315654, accumulated_logging_time=2.058181, accumulated_submission_time=16441.705375, global_step=35896, preemption_count=0, score=16441.705375, test/accuracy=0.435800, test/loss=2.656702, test/num_examples=10000, total_duration=17703.087596, train/accuracy=0.604238, train/loss=1.782889, validation/accuracy=0.548300, validation/loss=2.037083, validation/num_examples=50000
I0131 00:52:22.183033 139884794734336 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.5348474979400635, loss=3.7812411785125732
I0131 00:53:03.779534 139884786341632 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.2773990631103516, loss=4.230031490325928
I0131 00:53:50.564994 139884794734336 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.4860131740570068, loss=3.413454055786133
I0131 00:54:36.739704 139884786341632 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.825499415397644, loss=3.5282516479492188
I0131 00:55:23.289541 139884794734336 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9871756434440613, loss=4.886425018310547
I0131 00:56:09.765552 139884786341632 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.8363765478134155, loss=3.4763901233673096
I0131 00:56:56.635821 139884794734336 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.5348131656646729, loss=3.621915340423584
I0131 00:57:42.537853 139884786341632 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.502273678779602, loss=3.458062171936035
I0131 00:58:28.924317 139884794734336 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.7807161808013916, loss=3.435675859451294
I0131 00:59:15.319356 139884786341632 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.456240177154541, loss=3.528104305267334
I0131 00:59:20.598446 140085747812160 spec.py:321] Evaluating on the training split.
I0131 00:59:31.187627 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 00:59:50.379637 140085747812160 spec.py:349] Evaluating on the test split.
I0131 00:59:52.048595 140085747812160 submission_runner.py:408] Time since start: 18154.94s, 	Step: 36813, 	{'train/accuracy': 0.5854101181030273, 'train/loss': 1.868322730064392, 'validation/accuracy': 0.5462200045585632, 'validation/loss': 2.047626256942749, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.669766902923584, 'test/num_examples': 10000, 'score': 16862.025440216064, 'total_duration': 18154.93812942505, 'accumulated_submission_time': 16862.025440216064, 'accumulated_eval_time': 1288.7657787799835, 'accumulated_logging_time': 2.0886483192443848}
I0131 00:59:52.072528 139884794734336 logging_writer.py:48] [36813] accumulated_eval_time=1288.765779, accumulated_logging_time=2.088648, accumulated_submission_time=16862.025440, global_step=36813, preemption_count=0, score=16862.025440, test/accuracy=0.431400, test/loss=2.669767, test/num_examples=10000, total_duration=18154.938129, train/accuracy=0.585410, train/loss=1.868323, validation/accuracy=0.546220, validation/loss=2.047626, validation/num_examples=50000
I0131 01:00:27.552916 139884786341632 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.3961795568466187, loss=3.5975422859191895
I0131 01:01:13.816647 139884794734336 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.1212623119354248, loss=5.122786045074463
I0131 01:02:00.142075 139884786341632 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3764402866363525, loss=3.5622808933258057
I0131 01:02:46.735257 139884794734336 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.352274775505066, loss=5.022589206695557
I0131 01:03:33.199632 139884786341632 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.50163733959198, loss=3.648341178894043
I0131 01:04:19.724686 139884794734336 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5205670595169067, loss=3.3646304607391357
I0131 01:05:05.871789 139884786341632 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4059182405471802, loss=3.433093309402466
I0131 01:05:52.074990 139884794734336 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.2291767597198486, loss=4.747646331787109
I0131 01:06:38.278307 139884786341632 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.3865091800689697, loss=3.3620240688323975
I0131 01:06:52.345895 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:07:02.935110 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:07:22.697652 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:07:24.347857 140085747812160 submission_runner.py:408] Time since start: 18607.24s, 	Step: 37732, 	{'train/accuracy': 0.5966015458106995, 'train/loss': 1.8163317441940308, 'validation/accuracy': 0.5523599982261658, 'validation/loss': 2.01404070854187, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.634584426879883, 'test/num_examples': 10000, 'score': 17282.239025354385, 'total_duration': 18607.237397432327, 'accumulated_submission_time': 17282.239025354385, 'accumulated_eval_time': 1320.7677319049835, 'accumulated_logging_time': 2.1223950386047363}
I0131 01:07:24.370599 139884794734336 logging_writer.py:48] [37732] accumulated_eval_time=1320.767732, accumulated_logging_time=2.122395, accumulated_submission_time=17282.239025, global_step=37732, preemption_count=0, score=17282.239025, test/accuracy=0.440100, test/loss=2.634584, test/num_examples=10000, total_duration=18607.237397, train/accuracy=0.596602, train/loss=1.816332, validation/accuracy=0.552360, validation/loss=2.014041, validation/num_examples=50000
I0131 01:07:51.437591 139884786341632 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5826996564865112, loss=3.5295426845550537
I0131 01:08:37.053101 139884794734336 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.0322818756103516, loss=5.408206462860107
I0131 01:09:23.446995 139884786341632 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.2350311279296875, loss=3.846501350402832
I0131 01:10:09.698373 139884794734336 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.5029643774032593, loss=3.5068905353546143
I0131 01:10:55.960503 139884786341632 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.6685118675231934, loss=3.529114246368408
I0131 01:11:42.401219 139884794734336 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.4358901977539062, loss=5.6753621101379395
I0131 01:12:28.783970 139884786341632 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.4617551565170288, loss=3.5254673957824707
I0131 01:13:15.117746 139884794734336 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.5066112279891968, loss=3.5609800815582275
I0131 01:14:01.760707 139884786341632 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.038050889968872, loss=5.250055313110352
I0131 01:14:24.525067 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:14:34.475117 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:14:55.558420 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:14:57.204442 140085747812160 submission_runner.py:408] Time since start: 19060.09s, 	Step: 38651, 	{'train/accuracy': 0.60693359375, 'train/loss': 1.7717552185058594, 'validation/accuracy': 0.5542600154876709, 'validation/loss': 2.0164029598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.6386351585388184, 'test/num_examples': 10000, 'score': 17702.3341217041, 'total_duration': 19060.093980789185, 'accumulated_submission_time': 17702.3341217041, 'accumulated_eval_time': 1353.4470887184143, 'accumulated_logging_time': 2.154141902923584}
I0131 01:14:57.224436 139884794734336 logging_writer.py:48] [38651] accumulated_eval_time=1353.447089, accumulated_logging_time=2.154142, accumulated_submission_time=17702.334122, global_step=38651, preemption_count=0, score=17702.334122, test/accuracy=0.439400, test/loss=2.638635, test/num_examples=10000, total_duration=19060.093981, train/accuracy=0.606934, train/loss=1.771755, validation/accuracy=0.554260, validation/loss=2.016403, validation/num_examples=50000
I0131 01:15:16.857008 139884786341632 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.4960352182388306, loss=3.7929182052612305
I0131 01:16:00.915505 139884794734336 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.0874872207641602, loss=4.762180328369141
I0131 01:16:47.261516 139884786341632 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2653889656066895, loss=4.9719085693359375
I0131 01:17:34.058720 139884794734336 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.7187342643737793, loss=3.3934695720672607
I0131 01:18:20.408565 139884786341632 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1785223484039307, loss=4.180703163146973
I0131 01:19:06.703228 139884794734336 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.1751474142074585, loss=5.647995948791504
I0131 01:19:52.993787 139884786341632 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.4534735679626465, loss=3.6206302642822266
I0131 01:20:39.242021 139884794734336 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.3279576301574707, loss=5.573660850524902
I0131 01:21:25.309736 139884786341632 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0608360767364502, loss=5.690296173095703
I0131 01:21:57.313043 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:22:07.503499 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:22:29.464618 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:22:31.110003 140085747812160 submission_runner.py:408] Time since start: 19514.00s, 	Step: 39571, 	{'train/accuracy': 0.590136706829071, 'train/loss': 1.8692361116409302, 'validation/accuracy': 0.5558800101280212, 'validation/loss': 2.0332841873168945, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.6673877239227295, 'test/num_examples': 10000, 'score': 18122.364139556885, 'total_duration': 19513.99955034256, 'accumulated_submission_time': 18122.364139556885, 'accumulated_eval_time': 1387.2440507411957, 'accumulated_logging_time': 2.182854175567627}
I0131 01:22:31.133570 139884794734336 logging_writer.py:48] [39571] accumulated_eval_time=1387.244051, accumulated_logging_time=2.182854, accumulated_submission_time=18122.364140, global_step=39571, preemption_count=0, score=18122.364140, test/accuracy=0.434100, test/loss=2.667388, test/num_examples=10000, total_duration=19513.999550, train/accuracy=0.590137, train/loss=1.869236, validation/accuracy=0.555880, validation/loss=2.033284, validation/num_examples=50000
I0131 01:22:42.905352 139884786341632 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.4871227741241455, loss=3.4918031692504883
I0131 01:23:25.532458 139884794734336 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.3630452156066895, loss=3.480262041091919
I0131 01:24:12.070503 139884786341632 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.178164005279541, loss=4.872716903686523
I0131 01:24:58.905178 139884794734336 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.222861647605896, loss=4.147446155548096
I0131 01:25:45.073307 139884786341632 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.4743465185165405, loss=3.6319308280944824
I0131 01:26:31.527346 139884794734336 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5026671886444092, loss=3.5653977394104004
I0131 01:27:17.781808 139884786341632 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0641049146652222, loss=5.593484878540039
I0131 01:28:03.997329 139884794734336 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.7059624195098877, loss=3.4562952518463135
I0131 01:28:50.386128 139884786341632 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.3918805122375488, loss=3.4529869556427
I0131 01:29:31.189057 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:29:41.516761 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:30:00.735755 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:30:02.391222 140085747812160 submission_runner.py:408] Time since start: 19965.28s, 	Step: 40490, 	{'train/accuracy': 0.5978320240974426, 'train/loss': 1.8688842058181763, 'validation/accuracy': 0.5544999837875366, 'validation/loss': 2.060164213180542, 'validation/num_examples': 50000, 'test/accuracy': 0.43870002031326294, 'test/loss': 2.6926157474517822, 'test/num_examples': 10000, 'score': 18542.358523845673, 'total_duration': 19965.28075647354, 'accumulated_submission_time': 18542.358523845673, 'accumulated_eval_time': 1418.446210384369, 'accumulated_logging_time': 2.216303586959839}
I0131 01:30:02.415965 139884794734336 logging_writer.py:48] [40490] accumulated_eval_time=1418.446210, accumulated_logging_time=2.216304, accumulated_submission_time=18542.358524, global_step=40490, preemption_count=0, score=18542.358524, test/accuracy=0.438700, test/loss=2.692616, test/num_examples=10000, total_duration=19965.280756, train/accuracy=0.597832, train/loss=1.868884, validation/accuracy=0.554500, validation/loss=2.060164, validation/num_examples=50000
I0131 01:30:06.742509 139884786341632 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.4024516344070435, loss=3.7348830699920654
I0131 01:30:48.899931 139884794734336 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.5418285131454468, loss=3.458258628845215
I0131 01:31:35.176993 139884786341632 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.3558136224746704, loss=4.21209716796875
I0131 01:32:21.713658 139884794734336 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.4523106813430786, loss=3.357863664627075
I0131 01:33:08.250265 139884786341632 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.6070835590362549, loss=3.5673489570617676
I0131 01:33:54.572077 139884794734336 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.7209181785583496, loss=3.424661636352539
I0131 01:34:41.033794 139884786341632 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.4459725618362427, loss=3.4891631603240967
I0131 01:35:27.308699 139884794734336 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5629796981811523, loss=3.740427017211914
I0131 01:36:14.023342 139884786341632 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.052951455116272, loss=5.443426132202148
I0131 01:37:00.277519 139884794734336 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.3600373268127441, loss=3.327075481414795
I0131 01:37:02.770150 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:37:13.055730 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:37:32.375440 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:37:34.032929 140085747812160 submission_runner.py:408] Time since start: 20416.92s, 	Step: 41407, 	{'train/accuracy': 0.6039062142372131, 'train/loss': 1.8160500526428223, 'validation/accuracy': 0.5564199686050415, 'validation/loss': 2.0289831161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.4399000108242035, 'test/loss': 2.6659891605377197, 'test/num_examples': 10000, 'score': 18962.650825738907, 'total_duration': 20416.922464370728, 'accumulated_submission_time': 18962.650825738907, 'accumulated_eval_time': 1449.7089776992798, 'accumulated_logging_time': 2.2516136169433594}
I0131 01:37:34.062001 139884786341632 logging_writer.py:48] [41407] accumulated_eval_time=1449.708978, accumulated_logging_time=2.251614, accumulated_submission_time=18962.650826, global_step=41407, preemption_count=0, score=18962.650826, test/accuracy=0.439900, test/loss=2.665989, test/num_examples=10000, total_duration=20416.922464, train/accuracy=0.603906, train/loss=1.816050, validation/accuracy=0.556420, validation/loss=2.028983, validation/num_examples=50000
I0131 01:38:12.409821 139884794734336 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.267308235168457, loss=5.030193328857422
I0131 01:38:58.703665 139884786341632 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.5225310325622559, loss=3.532855749130249
I0131 01:39:44.866670 139884794734336 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.133344292640686, loss=4.630496501922607
I0131 01:40:31.226577 139884786341632 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6963878870010376, loss=3.5100557804107666
I0131 01:41:17.466567 139884794734336 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.561091423034668, loss=3.6794872283935547
I0131 01:42:03.820221 139884786341632 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.4280792474746704, loss=3.378798246383667
I0131 01:42:50.110464 139884794734336 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.4695992469787598, loss=3.6069984436035156
I0131 01:43:36.486620 139884786341632 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.5438976287841797, loss=3.5179834365844727
I0131 01:44:23.049945 139884794734336 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.255976676940918, loss=5.166144371032715
I0131 01:44:34.199309 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:44:44.489011 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:45:05.077645 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:45:06.716764 140085747812160 submission_runner.py:408] Time since start: 20869.61s, 	Step: 42326, 	{'train/accuracy': 0.5984765291213989, 'train/loss': 1.8087574243545532, 'validation/accuracy': 0.5619800090789795, 'validation/loss': 1.9925109148025513, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.6232457160949707, 'test/num_examples': 10000, 'score': 19382.726880311966, 'total_duration': 20869.60631251335, 'accumulated_submission_time': 19382.726880311966, 'accumulated_eval_time': 1482.2264337539673, 'accumulated_logging_time': 2.291311264038086}
I0131 01:45:06.739736 139884786341632 logging_writer.py:48] [42326] accumulated_eval_time=1482.226434, accumulated_logging_time=2.291311, accumulated_submission_time=19382.726880, global_step=42326, preemption_count=0, score=19382.726880, test/accuracy=0.442900, test/loss=2.623246, test/num_examples=10000, total_duration=20869.606313, train/accuracy=0.598477, train/loss=1.808757, validation/accuracy=0.561980, validation/loss=1.992511, validation/num_examples=50000
I0131 01:45:36.158613 139884794734336 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.24385666847229, loss=4.1104631423950195
I0131 01:46:22.222757 139884786341632 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.4777872562408447, loss=3.414576292037964
I0131 01:47:08.767104 139884794734336 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6823768615722656, loss=3.440415859222412
I0131 01:47:55.442744 139884786341632 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2284334897994995, loss=4.676366806030273
I0131 01:48:41.619278 139884794734336 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2133771181106567, loss=4.860232353210449
I0131 01:49:28.000100 139884786341632 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.5348483324050903, loss=3.4870128631591797
I0131 01:50:14.416967 139884794734336 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.576365351676941, loss=3.426832675933838
I0131 01:51:00.971135 139884786341632 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.6281253099441528, loss=3.4135310649871826
I0131 01:51:47.104170 139884794734336 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.234294056892395, loss=3.8567140102386475
I0131 01:52:06.827482 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:52:16.915153 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 01:52:37.929251 140085747812160 spec.py:349] Evaluating on the test split.
I0131 01:52:39.567823 140085747812160 submission_runner.py:408] Time since start: 21322.46s, 	Step: 43244, 	{'train/accuracy': 0.6040819883346558, 'train/loss': 1.7556627988815308, 'validation/accuracy': 0.5645599961280823, 'validation/loss': 1.9334279298782349, 'validation/num_examples': 50000, 'test/accuracy': 0.4507000148296356, 'test/loss': 2.5570688247680664, 'test/num_examples': 10000, 'score': 19802.75421524048, 'total_duration': 21322.457344055176, 'accumulated_submission_time': 19802.75421524048, 'accumulated_eval_time': 1514.9667398929596, 'accumulated_logging_time': 2.3243231773376465}
I0131 01:52:39.592223 139884786341632 logging_writer.py:48] [43244] accumulated_eval_time=1514.966740, accumulated_logging_time=2.324323, accumulated_submission_time=19802.754215, global_step=43244, preemption_count=0, score=19802.754215, test/accuracy=0.450700, test/loss=2.557069, test/num_examples=10000, total_duration=21322.457344, train/accuracy=0.604082, train/loss=1.755663, validation/accuracy=0.564560, validation/loss=1.933428, validation/num_examples=50000
I0131 01:53:01.947232 139884794734336 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.5204715728759766, loss=3.4968183040618896
I0131 01:53:46.856199 139884786341632 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5030232667922974, loss=3.3681046962738037
I0131 01:54:33.341058 139884794734336 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.6076911687850952, loss=3.4890618324279785
I0131 01:55:19.897935 139884786341632 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.4495607614517212, loss=3.5060014724731445
I0131 01:56:06.271352 139884794734336 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.126484751701355, loss=5.049402236938477
I0131 01:56:52.882772 139884786341632 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.473060965538025, loss=3.5109291076660156
I0131 01:57:39.250117 139884794734336 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.4663076400756836, loss=3.4332940578460693
I0131 01:58:25.648784 139884786341632 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.3679975271224976, loss=3.473499298095703
I0131 01:59:11.918477 139884794734336 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.4132238626480103, loss=4.918460845947266
I0131 01:59:39.914167 140085747812160 spec.py:321] Evaluating on the training split.
I0131 01:59:50.190596 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:00:11.270366 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:00:12.908228 140085747812160 submission_runner.py:408] Time since start: 21775.80s, 	Step: 44162, 	{'train/accuracy': 0.6100195050239563, 'train/loss': 1.7446229457855225, 'validation/accuracy': 0.5627399682998657, 'validation/loss': 1.9554500579833984, 'validation/num_examples': 50000, 'test/accuracy': 0.45010003447532654, 'test/loss': 2.5805513858795166, 'test/num_examples': 10000, 'score': 20223.016721487045, 'total_duration': 21775.797772169113, 'accumulated_submission_time': 20223.016721487045, 'accumulated_eval_time': 1547.9607965946198, 'accumulated_logging_time': 2.3581182956695557}
I0131 02:00:12.932760 139884786341632 logging_writer.py:48] [44162] accumulated_eval_time=1547.960797, accumulated_logging_time=2.358118, accumulated_submission_time=20223.016721, global_step=44162, preemption_count=0, score=20223.016721, test/accuracy=0.450100, test/loss=2.580551, test/num_examples=10000, total_duration=21775.797772, train/accuracy=0.610020, train/loss=1.744623, validation/accuracy=0.562740, validation/loss=1.955450, validation/num_examples=50000
I0131 02:00:28.231623 139884794734336 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.4566072225570679, loss=3.410656690597534
I0131 02:01:11.667817 139884786341632 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.534855604171753, loss=3.467229127883911
I0131 02:01:58.430804 139884794734336 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.2774115800857544, loss=4.61861515045166
I0131 02:02:45.164112 139884786341632 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.506606936454773, loss=3.3904037475585938
I0131 02:03:31.260585 139884794734336 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7019215822219849, loss=3.439922332763672
I0131 02:04:17.823152 139884786341632 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.473765254020691, loss=3.9550349712371826
I0131 02:05:04.144668 139884794734336 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.321194052696228, loss=4.8641486167907715
I0131 02:05:50.967746 139884786341632 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.5227940082550049, loss=3.3822948932647705
I0131 02:06:37.219696 139884794734336 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1824315786361694, loss=5.149305820465088
I0131 02:07:13.212518 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:07:23.758063 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:07:45.195856 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:07:46.836471 140085747812160 submission_runner.py:408] Time since start: 22229.73s, 	Step: 45079, 	{'train/accuracy': 0.6299804449081421, 'train/loss': 1.65792977809906, 'validation/accuracy': 0.564079999923706, 'validation/loss': 1.9666937589645386, 'validation/num_examples': 50000, 'test/accuracy': 0.4431000351905823, 'test/loss': 2.618972063064575, 'test/num_examples': 10000, 'score': 20643.23784804344, 'total_duration': 22229.7259953022, 'accumulated_submission_time': 20643.23784804344, 'accumulated_eval_time': 1581.5847356319427, 'accumulated_logging_time': 2.3912997245788574}
I0131 02:07:46.860566 139884786341632 logging_writer.py:48] [45079] accumulated_eval_time=1581.584736, accumulated_logging_time=2.391300, accumulated_submission_time=20643.237848, global_step=45079, preemption_count=0, score=20643.237848, test/accuracy=0.443100, test/loss=2.618972, test/num_examples=10000, total_duration=22229.725995, train/accuracy=0.629980, train/loss=1.657930, validation/accuracy=0.564080, validation/loss=1.966694, validation/num_examples=50000
I0131 02:07:55.711349 139884794734336 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.3552954196929932, loss=4.905580520629883
I0131 02:08:37.631895 139884786341632 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.1647859811782837, loss=5.609240531921387
I0131 02:09:23.889542 139884794734336 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5067318677902222, loss=3.484145164489746
I0131 02:10:10.657073 139884786341632 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.4448246955871582, loss=3.3893940448760986
I0131 02:10:56.969090 139884794734336 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7063441276550293, loss=3.420042037963867
I0131 02:11:43.412142 139884786341632 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.7096314430236816, loss=3.3872368335723877
I0131 02:12:29.909675 139884794734336 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1291896104812622, loss=5.431621551513672
I0131 02:13:16.109370 139884786341632 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1045488119125366, loss=4.9971513748168945
I0131 02:14:02.502329 139884794734336 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.6523164510726929, loss=3.4666876792907715
I0131 02:14:47.199503 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:14:57.625091 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:15:15.742470 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:15:17.388368 140085747812160 submission_runner.py:408] Time since start: 22680.28s, 	Step: 45998, 	{'train/accuracy': 0.6064062118530273, 'train/loss': 1.7620118856430054, 'validation/accuracy': 0.5678600072860718, 'validation/loss': 1.9412288665771484, 'validation/num_examples': 50000, 'test/accuracy': 0.45600003004074097, 'test/loss': 2.5845894813537598, 'test/num_examples': 10000, 'score': 21063.517508268356, 'total_duration': 22680.277902126312, 'accumulated_submission_time': 21063.517508268356, 'accumulated_eval_time': 1611.773603439331, 'accumulated_logging_time': 2.4242405891418457}
I0131 02:15:17.416970 139884786341632 logging_writer.py:48] [45998] accumulated_eval_time=1611.773603, accumulated_logging_time=2.424241, accumulated_submission_time=21063.517508, global_step=45998, preemption_count=0, score=21063.517508, test/accuracy=0.456000, test/loss=2.584589, test/num_examples=10000, total_duration=22680.277902, train/accuracy=0.606406, train/loss=1.762012, validation/accuracy=0.567860, validation/loss=1.941229, validation/num_examples=50000
I0131 02:15:18.641354 139884794734336 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.3469254970550537, loss=5.214666366577148
I0131 02:16:01.006554 139884786341632 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.5328435897827148, loss=3.429579973220825
I0131 02:16:47.209444 139884794734336 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7341324090957642, loss=3.444774866104126
I0131 02:17:34.125892 139884786341632 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.4892640113830566, loss=4.09694242477417
I0131 02:18:20.822650 139884794734336 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.4901268482208252, loss=3.341719388961792
I0131 02:19:07.234494 139884786341632 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.434654951095581, loss=3.5169496536254883
I0131 02:19:53.698906 139884794734336 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6742198467254639, loss=3.3171494007110596
I0131 02:20:39.857480 139884786341632 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.5576189756393433, loss=3.4668843746185303
I0131 02:21:26.393888 139884794734336 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1305867433547974, loss=5.40720272064209
I0131 02:22:12.959140 139884786341632 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.4981145858764648, loss=4.696863174438477
I0131 02:22:17.712141 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:22:28.534900 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:22:46.612300 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:22:48.263207 140085747812160 submission_runner.py:408] Time since start: 23131.15s, 	Step: 46912, 	{'train/accuracy': 0.6177538633346558, 'train/loss': 1.7487009763717651, 'validation/accuracy': 0.5697599649429321, 'validation/loss': 1.956942081451416, 'validation/num_examples': 50000, 'test/accuracy': 0.45440003275871277, 'test/loss': 2.5762476921081543, 'test/num_examples': 10000, 'score': 21483.748101711273, 'total_duration': 23131.15274167061, 'accumulated_submission_time': 21483.748101711273, 'accumulated_eval_time': 1642.3246562480927, 'accumulated_logging_time': 2.4671669006347656}
I0131 02:22:48.288866 139884794734336 logging_writer.py:48] [46912] accumulated_eval_time=1642.324656, accumulated_logging_time=2.467167, accumulated_submission_time=21483.748102, global_step=46912, preemption_count=0, score=21483.748102, test/accuracy=0.454400, test/loss=2.576248, test/num_examples=10000, total_duration=23131.152742, train/accuracy=0.617754, train/loss=1.748701, validation/accuracy=0.569760, validation/loss=1.956942, validation/num_examples=50000
I0131 02:23:24.501014 139884786341632 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.3198413848876953, loss=4.612857818603516
I0131 02:24:10.803470 139884794734336 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.6414389610290527, loss=3.434574604034424
I0131 02:24:57.495874 139884786341632 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.50700843334198, loss=3.299598455429077
I0131 02:25:44.136029 139884794734336 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.167638897895813, loss=4.954885959625244
I0131 02:26:30.608372 139884786341632 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.99538654088974, loss=5.5584259033203125
I0131 02:27:17.143955 139884794734336 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.5721839666366577, loss=3.594804525375366
I0131 02:28:03.652429 139884786341632 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.4827805757522583, loss=3.4662675857543945
I0131 02:28:50.133595 139884794734336 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.3819321393966675, loss=4.333733081817627
I0131 02:29:36.644537 139884786341632 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.3357760906219482, loss=4.141058444976807
I0131 02:29:48.501640 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:29:58.806043 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:30:17.822531 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:30:19.469891 140085747812160 submission_runner.py:408] Time since start: 23582.36s, 	Step: 47827, 	{'train/accuracy': 0.6209570169448853, 'train/loss': 1.705472469329834, 'validation/accuracy': 0.5658599734306335, 'validation/loss': 1.9722998142242432, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5957021713256836, 'test/num_examples': 10000, 'score': 21903.900598526, 'total_duration': 23582.35942387581, 'accumulated_submission_time': 21903.900598526, 'accumulated_eval_time': 1673.292890548706, 'accumulated_logging_time': 2.5029873847961426}
I0131 02:30:19.496073 139884794734336 logging_writer.py:48] [47827] accumulated_eval_time=1673.292891, accumulated_logging_time=2.502987, accumulated_submission_time=21903.900599, global_step=47827, preemption_count=0, score=21903.900599, test/accuracy=0.446200, test/loss=2.595702, test/num_examples=10000, total_duration=23582.359424, train/accuracy=0.620957, train/loss=1.705472, validation/accuracy=0.565860, validation/loss=1.972300, validation/num_examples=50000
I0131 02:30:48.596890 139884786341632 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.6017422676086426, loss=3.4019389152526855
I0131 02:31:34.774247 139884794734336 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4979171752929688, loss=4.198518753051758
I0131 02:32:21.285835 139884786341632 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.4862152338027954, loss=3.444150924682617
I0131 02:33:07.920802 139884794734336 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.5541459321975708, loss=3.5740253925323486
I0131 02:33:54.437729 139884786341632 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.4973286390304565, loss=3.4670286178588867
I0131 02:34:41.023856 139884794734336 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.5707868337631226, loss=3.3863096237182617
I0131 02:35:27.614296 139884786341632 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.5858210325241089, loss=3.290919542312622
I0131 02:36:14.038902 139884794734336 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.459334373474121, loss=3.3759846687316895
I0131 02:37:00.317140 139884786341632 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.4996819496154785, loss=3.356189250946045
I0131 02:37:19.524129 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:37:29.629683 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:37:50.122950 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:37:51.766769 140085747812160 submission_runner.py:408] Time since start: 24034.66s, 	Step: 48743, 	{'train/accuracy': 0.604785144329071, 'train/loss': 1.7722070217132568, 'validation/accuracy': 0.568120002746582, 'validation/loss': 1.9483237266540527, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.602745771408081, 'test/num_examples': 10000, 'score': 22323.86534023285, 'total_duration': 24034.65631222725, 'accumulated_submission_time': 22323.86534023285, 'accumulated_eval_time': 1705.5355398654938, 'accumulated_logging_time': 2.5407254695892334}
I0131 02:37:51.789227 139884794734336 logging_writer.py:48] [48743] accumulated_eval_time=1705.535540, accumulated_logging_time=2.540725, accumulated_submission_time=22323.865340, global_step=48743, preemption_count=0, score=22323.865340, test/accuracy=0.446300, test/loss=2.602746, test/num_examples=10000, total_duration=24034.656312, train/accuracy=0.604785, train/loss=1.772207, validation/accuracy=0.568120, validation/loss=1.948324, validation/num_examples=50000
I0131 02:38:14.557715 139884786341632 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.573317050933838, loss=3.3880059719085693
I0131 02:38:59.506266 139884794734336 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.4965611696243286, loss=4.090238571166992
I0131 02:39:45.967870 139884786341632 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.5461962223052979, loss=4.584913730621338
I0131 02:40:32.432090 139884794734336 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.6819953918457031, loss=3.2970869541168213
I0131 02:41:18.580863 139884786341632 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.5698277950286865, loss=3.417998790740967
I0131 02:42:04.830400 139884794734336 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.3285198211669922, loss=4.569746017456055
I0131 02:42:51.300052 139884786341632 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.5678784847259521, loss=3.420243263244629
I0131 02:43:37.573273 139884794734336 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7696354389190674, loss=3.2264585494995117
I0131 02:44:23.808326 139884786341632 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.3850915431976318, loss=5.49875020980835
I0131 02:44:51.801439 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:45:02.374278 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:45:23.608406 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:45:25.245407 140085747812160 submission_runner.py:408] Time since start: 24488.13s, 	Step: 49662, 	{'train/accuracy': 0.6148828268051147, 'train/loss': 1.721850037574768, 'validation/accuracy': 0.5727399587631226, 'validation/loss': 1.9160726070404053, 'validation/num_examples': 50000, 'test/accuracy': 0.4552000164985657, 'test/loss': 2.5426297187805176, 'test/num_examples': 10000, 'score': 22743.817096471786, 'total_duration': 24488.13495373726, 'accumulated_submission_time': 22743.817096471786, 'accumulated_eval_time': 1738.979534626007, 'accumulated_logging_time': 2.573563814163208}
I0131 02:45:25.267370 139884794734336 logging_writer.py:48] [49662] accumulated_eval_time=1738.979535, accumulated_logging_time=2.573564, accumulated_submission_time=22743.817096, global_step=49662, preemption_count=0, score=22743.817096, test/accuracy=0.455200, test/loss=2.542630, test/num_examples=10000, total_duration=24488.134954, train/accuracy=0.614883, train/loss=1.721850, validation/accuracy=0.572740, validation/loss=1.916073, validation/num_examples=50000
I0131 02:45:40.567240 139884786341632 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7171229124069214, loss=3.380336284637451
I0131 02:46:24.021142 139884794734336 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.4569742679595947, loss=3.2708334922790527
I0131 02:47:10.338701 139884786341632 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.4875255823135376, loss=3.483466148376465
I0131 02:47:57.000624 139884794734336 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.5554850101470947, loss=3.354287624359131
I0131 02:48:43.348397 139884786341632 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.0631858110427856, loss=5.4601263999938965
I0131 02:49:29.487181 139884794734336 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.426790714263916, loss=3.336881160736084
I0131 02:50:15.967540 139884786341632 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.7090909481048584, loss=3.69511079788208
I0131 02:51:02.519350 139884794734336 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2624861001968384, loss=5.568553924560547
I0131 02:51:48.966783 139884786341632 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.5569889545440674, loss=3.47054123878479
I0131 02:52:25.429724 140085747812160 spec.py:321] Evaluating on the training split.
I0131 02:52:35.759695 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 02:52:54.913214 140085747812160 spec.py:349] Evaluating on the test split.
I0131 02:52:56.562716 140085747812160 submission_runner.py:408] Time since start: 24939.45s, 	Step: 50580, 	{'train/accuracy': 0.61865234375, 'train/loss': 1.7359861135482788, 'validation/accuracy': 0.5676199793815613, 'validation/loss': 1.9560025930404663, 'validation/num_examples': 50000, 'test/accuracy': 0.45170003175735474, 'test/loss': 2.5810353755950928, 'test/num_examples': 10000, 'score': 23163.91581749916, 'total_duration': 24939.452256917953, 'accumulated_submission_time': 23163.91581749916, 'accumulated_eval_time': 1770.112517118454, 'accumulated_logging_time': 2.60744047164917}
I0131 02:52:56.588216 139884794734336 logging_writer.py:48] [50580] accumulated_eval_time=1770.112517, accumulated_logging_time=2.607440, accumulated_submission_time=23163.915817, global_step=50580, preemption_count=0, score=23163.915817, test/accuracy=0.451700, test/loss=2.581035, test/num_examples=10000, total_duration=24939.452257, train/accuracy=0.618652, train/loss=1.735986, validation/accuracy=0.567620, validation/loss=1.956003, validation/num_examples=50000
I0131 02:53:04.838428 139884786341632 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.5419163703918457, loss=3.4415533542633057
I0131 02:53:47.342254 139884794734336 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.4793765544891357, loss=4.0796074867248535
I0131 02:54:33.734115 139884786341632 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.764465093612671, loss=3.2841227054595947
I0131 02:55:20.550826 139884794734336 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.5893219709396362, loss=3.31864333152771
I0131 02:56:06.894480 139884786341632 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.590083360671997, loss=3.3651905059814453
I0131 02:56:53.351261 139884794734336 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.6353259086608887, loss=3.3604040145874023
I0131 02:57:39.925189 139884786341632 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.4390593767166138, loss=5.020747661590576
I0131 02:58:26.415973 139884794734336 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.5175154209136963, loss=3.332178831100464
I0131 02:59:12.509209 139884786341632 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.4703750610351562, loss=3.4237844944000244
I0131 02:59:56.976236 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:00:07.303595 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:00:27.711165 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:00:29.352210 140085747812160 submission_runner.py:408] Time since start: 25392.24s, 	Step: 51498, 	{'train/accuracy': 0.6126366853713989, 'train/loss': 1.7147948741912842, 'validation/accuracy': 0.5749599933624268, 'validation/loss': 1.904335618019104, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.5383946895599365, 'test/num_examples': 10000, 'score': 23584.244931459427, 'total_duration': 25392.24175477028, 'accumulated_submission_time': 23584.244931459427, 'accumulated_eval_time': 1802.4884850978851, 'accumulated_logging_time': 2.6418302059173584}
I0131 03:00:29.376729 139884794734336 logging_writer.py:48] [51498] accumulated_eval_time=1802.488485, accumulated_logging_time=2.641830, accumulated_submission_time=23584.244931, global_step=51498, preemption_count=0, score=23584.244931, test/accuracy=0.458300, test/loss=2.538395, test/num_examples=10000, total_duration=25392.241755, train/accuracy=0.612637, train/loss=1.714795, validation/accuracy=0.574960, validation/loss=1.904336, validation/num_examples=50000
I0131 03:00:30.563051 139884786341632 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.4742121696472168, loss=3.314661979675293
I0131 03:01:11.840070 139884794734336 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2159932851791382, loss=5.568646430969238
I0131 03:01:58.072499 139884786341632 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.4709166288375854, loss=3.999727249145508
I0131 03:02:45.073129 139884794734336 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.232089638710022, loss=5.1593732833862305
I0131 03:03:31.547745 139884786341632 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.6626241207122803, loss=3.2454159259796143
I0131 03:04:17.934846 139884794734336 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.4654316902160645, loss=3.7583189010620117
I0131 03:05:04.362110 139884786341632 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.0653740167617798, loss=5.002357482910156
I0131 03:05:50.893815 139884794734336 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.582553505897522, loss=3.3184993267059326
I0131 03:06:37.328554 139884786341632 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.5623379945755005, loss=3.290513038635254
I0131 03:07:23.972564 139884794734336 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.2020496129989624, loss=4.22065544128418
I0131 03:07:29.557006 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:07:39.822727 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:07:59.366594 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:08:01.008775 140085747812160 submission_runner.py:408] Time since start: 25843.90s, 	Step: 52414, 	{'train/accuracy': 0.6123437285423279, 'train/loss': 1.7380605936050415, 'validation/accuracy': 0.5724999904632568, 'validation/loss': 1.917758822441101, 'validation/num_examples': 50000, 'test/accuracy': 0.45730000734329224, 'test/loss': 2.529676914215088, 'test/num_examples': 10000, 'score': 24004.364980459213, 'total_duration': 25843.898319244385, 'accumulated_submission_time': 24004.364980459213, 'accumulated_eval_time': 1833.9402480125427, 'accumulated_logging_time': 2.6762707233428955}
I0131 03:08:01.034983 139884786341632 logging_writer.py:48] [52414] accumulated_eval_time=1833.940248, accumulated_logging_time=2.676271, accumulated_submission_time=24004.364980, global_step=52414, preemption_count=0, score=24004.364980, test/accuracy=0.457300, test/loss=2.529677, test/num_examples=10000, total_duration=25843.898319, train/accuracy=0.612344, train/loss=1.738061, validation/accuracy=0.572500, validation/loss=1.917759, validation/num_examples=50000
I0131 03:08:35.490337 139884794734336 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.5263350009918213, loss=3.7606019973754883
I0131 03:09:21.904228 139884786341632 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.4710317850112915, loss=5.187854766845703
I0131 03:10:08.652965 139884794734336 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.1174604892730713, loss=5.451117992401123
I0131 03:10:55.569371 139884786341632 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.633807897567749, loss=3.664349317550659
I0131 03:11:41.864950 139884794734336 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.3553071022033691, loss=4.244450569152832
I0131 03:12:28.650936 139884786341632 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.4719884395599365, loss=3.4658865928649902
I0131 03:13:15.377443 139884794734336 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.731284499168396, loss=3.4841673374176025
I0131 03:14:01.801151 139884786341632 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.4284486770629883, loss=3.463229179382324
I0131 03:14:48.208721 139884794734336 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.3911796808242798, loss=3.780595302581787
I0131 03:15:01.336954 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:15:11.604992 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:15:31.292227 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:15:32.942458 140085747812160 submission_runner.py:408] Time since start: 26295.83s, 	Step: 53330, 	{'train/accuracy': 0.6242578029632568, 'train/loss': 1.6958975791931152, 'validation/accuracy': 0.5759199857711792, 'validation/loss': 1.9176926612854004, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.568796396255493, 'test/num_examples': 10000, 'score': 24424.607334136963, 'total_duration': 26295.831995487213, 'accumulated_submission_time': 24424.607334136963, 'accumulated_eval_time': 1865.5457472801208, 'accumulated_logging_time': 2.711615800857544}
I0131 03:15:32.978283 139884786341632 logging_writer.py:48] [53330] accumulated_eval_time=1865.545747, accumulated_logging_time=2.711616, accumulated_submission_time=24424.607334, global_step=53330, preemption_count=0, score=24424.607334, test/accuracy=0.452100, test/loss=2.568796, test/num_examples=10000, total_duration=26295.831995, train/accuracy=0.624258, train/loss=1.695898, validation/accuracy=0.575920, validation/loss=1.917693, validation/num_examples=50000
I0131 03:16:00.884291 139884794734336 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.5007131099700928, loss=3.6788783073425293
I0131 03:16:47.224786 139884786341632 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2996525764465332, loss=5.583730220794678
I0131 03:17:33.904640 139884794734336 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.227676510810852, loss=4.534554481506348
I0131 03:18:20.351879 139884786341632 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.526002049446106, loss=3.3672921657562256
I0131 03:19:06.879345 139884794734336 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2683411836624146, loss=5.295647621154785
I0131 03:19:53.485793 139884786341632 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.1839135885238647, loss=5.009337425231934
I0131 03:20:40.167834 139884794734336 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.3787964582443237, loss=4.229809284210205
I0131 03:21:26.778836 139884786341632 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.5190911293029785, loss=3.298074722290039
I0131 03:22:13.152592 139884794734336 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.6727360486984253, loss=3.3744029998779297
I0131 03:22:33.219609 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:22:43.413206 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:23:04.028704 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:23:05.668304 140085747812160 submission_runner.py:408] Time since start: 26748.56s, 	Step: 54244, 	{'train/accuracy': 0.6257421970367432, 'train/loss': 1.7023617029190063, 'validation/accuracy': 0.5797600150108337, 'validation/loss': 1.9058101177215576, 'validation/num_examples': 50000, 'test/accuracy': 0.4621000289916992, 'test/loss': 2.5444371700286865, 'test/num_examples': 10000, 'score': 24844.78872013092, 'total_duration': 26748.557849645615, 'accumulated_submission_time': 24844.78872013092, 'accumulated_eval_time': 1897.9944295883179, 'accumulated_logging_time': 2.7577033042907715}
I0131 03:23:05.691420 139884786341632 logging_writer.py:48] [54244] accumulated_eval_time=1897.994430, accumulated_logging_time=2.757703, accumulated_submission_time=24844.788720, global_step=54244, preemption_count=0, score=24844.788720, test/accuracy=0.462100, test/loss=2.544437, test/num_examples=10000, total_duration=26748.557850, train/accuracy=0.625742, train/loss=1.702362, validation/accuracy=0.579760, validation/loss=1.905810, validation/num_examples=50000
I0131 03:23:28.064938 139884794734336 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.427464485168457, loss=3.712324619293213
I0131 03:24:13.270921 139884786341632 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8508880138397217, loss=3.5134565830230713
I0131 03:24:59.515058 139884794734336 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.6566177606582642, loss=3.1497437953948975
I0131 03:25:45.884684 139884786341632 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.575782299041748, loss=3.3277454376220703
I0131 03:26:32.215852 139884794734336 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.4213130474090576, loss=3.6207265853881836
I0131 03:27:18.469165 139884786341632 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.4645401239395142, loss=4.010673999786377
I0131 03:28:04.835736 139884794734336 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.4798222780227661, loss=3.396331548690796
I0131 03:28:51.241353 139884786341632 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.514829397201538, loss=3.2474911212921143
I0131 03:29:37.998490 139884794734336 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.4701323509216309, loss=4.220254898071289
I0131 03:30:05.928102 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:30:16.266574 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:30:36.078724 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:30:37.727641 140085747812160 submission_runner.py:408] Time since start: 27200.62s, 	Step: 55162, 	{'train/accuracy': 0.6188085675239563, 'train/loss': 1.706288456916809, 'validation/accuracy': 0.5832399725914001, 'validation/loss': 1.87911856174469, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.5232367515563965, 'test/num_examples': 10000, 'score': 25264.96487569809, 'total_duration': 27200.61718583107, 'accumulated_submission_time': 25264.96487569809, 'accumulated_eval_time': 1929.7939734458923, 'accumulated_logging_time': 2.7902755737304688}
I0131 03:30:37.750931 139884786341632 logging_writer.py:48] [55162] accumulated_eval_time=1929.793973, accumulated_logging_time=2.790276, accumulated_submission_time=25264.964876, global_step=55162, preemption_count=0, score=25264.964876, test/accuracy=0.459900, test/loss=2.523237, test/num_examples=10000, total_duration=27200.617186, train/accuracy=0.618809, train/loss=1.706288, validation/accuracy=0.583240, validation/loss=1.879119, validation/num_examples=50000
I0131 03:30:53.058579 139884794734336 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.5917720794677734, loss=3.3795275688171387
I0131 03:31:36.925745 139884786341632 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.5512058734893799, loss=3.380852699279785
I0131 03:32:23.229621 139884794734336 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.6579902172088623, loss=3.3329572677612305
I0131 03:33:10.368295 139884786341632 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1479219198226929, loss=5.5067620277404785
I0131 03:33:56.531993 139884794734336 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.4859410524368286, loss=3.440207004547119
I0131 03:34:43.147431 139884786341632 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.4439579248428345, loss=4.506374359130859
I0131 03:35:29.528622 139884794734336 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.398916482925415, loss=4.576169490814209
I0131 03:36:15.830065 139884786341632 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.5483121871948242, loss=3.2861368656158447
I0131 03:37:02.431070 139884794734336 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1715781688690186, loss=5.327244758605957
I0131 03:37:37.849271 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:37:48.194403 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:38:08.170821 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:38:09.811726 140085747812160 submission_runner.py:408] Time since start: 27652.70s, 	Step: 56078, 	{'train/accuracy': 0.6207226514816284, 'train/loss': 1.7265022993087769, 'validation/accuracy': 0.5751399993896484, 'validation/loss': 1.9327647686004639, 'validation/num_examples': 50000, 'test/accuracy': 0.4586000144481659, 'test/loss': 2.5697994232177734, 'test/num_examples': 10000, 'score': 25685.004106283188, 'total_duration': 27652.701270341873, 'accumulated_submission_time': 25685.004106283188, 'accumulated_eval_time': 1961.75643324852, 'accumulated_logging_time': 2.8226845264434814}
I0131 03:38:09.836772 139884786341632 logging_writer.py:48] [56078] accumulated_eval_time=1961.756433, accumulated_logging_time=2.822685, accumulated_submission_time=25685.004106, global_step=56078, preemption_count=0, score=25685.004106, test/accuracy=0.458600, test/loss=2.569799, test/num_examples=10000, total_duration=27652.701270, train/accuracy=0.620723, train/loss=1.726502, validation/accuracy=0.575140, validation/loss=1.932765, validation/num_examples=50000
I0131 03:38:18.866084 139884794734336 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.4635913372039795, loss=3.583136558532715
I0131 03:39:01.178772 139884786341632 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.5100672245025635, loss=3.42946195602417
I0131 03:39:47.942871 139884794734336 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.3808672428131104, loss=4.354650497436523
I0131 03:40:34.712512 139884786341632 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.6637407541275024, loss=3.398728132247925
I0131 03:41:21.034251 139884794734336 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.4056546688079834, loss=3.796323537826538
I0131 03:42:07.512899 139884786341632 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.4944885969161987, loss=3.2371199131011963
I0131 03:42:53.966448 139884794734336 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.551016926765442, loss=3.2810420989990234
I0131 03:43:40.378612 139884786341632 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.680858850479126, loss=3.389770746231079
I0131 03:44:26.966639 139884794734336 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.6987568140029907, loss=3.320020914077759
I0131 03:45:09.912092 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:45:20.054412 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:45:40.839610 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:45:42.476897 140085747812160 submission_runner.py:408] Time since start: 28105.37s, 	Step: 56994, 	{'train/accuracy': 0.6523046493530273, 'train/loss': 1.600400686264038, 'validation/accuracy': 0.580299973487854, 'validation/loss': 1.9175043106079102, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.537362575531006, 'test/num_examples': 10000, 'score': 26105.01860499382, 'total_duration': 28105.36644411087, 'accumulated_submission_time': 26105.01860499382, 'accumulated_eval_time': 1994.321251630783, 'accumulated_logging_time': 2.8584837913513184}
I0131 03:45:42.499709 139884786341632 logging_writer.py:48] [56994] accumulated_eval_time=1994.321252, accumulated_logging_time=2.858484, accumulated_submission_time=26105.018605, global_step=56994, preemption_count=0, score=26105.018605, test/accuracy=0.468800, test/loss=2.537363, test/num_examples=10000, total_duration=28105.366444, train/accuracy=0.652305, train/loss=1.600401, validation/accuracy=0.580300, validation/loss=1.917504, validation/num_examples=50000
I0131 03:45:45.250432 139884794734336 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.647929072380066, loss=3.279477119445801
I0131 03:46:26.864448 139884786341632 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.6774227619171143, loss=3.3341050148010254
I0131 03:47:13.548136 139884794734336 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.3409693241119385, loss=3.716226100921631
I0131 03:48:00.016398 139884786341632 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.489928126335144, loss=3.4566521644592285
I0131 03:48:46.203682 139884794734336 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.5076426267623901, loss=3.6265461444854736
I0131 03:49:32.673109 139884786341632 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.6138575077056885, loss=3.2815051078796387
I0131 03:50:19.225291 139884794734336 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.239668607711792, loss=4.590326309204102
I0131 03:51:05.517076 139884786341632 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.3647745847702026, loss=3.523108720779419
I0131 03:51:51.776551 139884794734336 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.5098994970321655, loss=3.4562230110168457
I0131 03:52:38.600607 139884786341632 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.5525598526000977, loss=3.290011405944824
I0131 03:52:42.901877 140085747812160 spec.py:321] Evaluating on the training split.
I0131 03:52:52.865840 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 03:53:12.968709 140085747812160 spec.py:349] Evaluating on the test split.
I0131 03:53:14.617062 140085747812160 submission_runner.py:408] Time since start: 28557.51s, 	Step: 57911, 	{'train/accuracy': 0.6321093440055847, 'train/loss': 1.6738526821136475, 'validation/accuracy': 0.5862999558448792, 'validation/loss': 1.8718279600143433, 'validation/num_examples': 50000, 'test/accuracy': 0.4709000289440155, 'test/loss': 2.4993984699249268, 'test/num_examples': 10000, 'score': 26525.360887289047, 'total_duration': 28557.5066075325, 'accumulated_submission_time': 26525.360887289047, 'accumulated_eval_time': 2026.0364346504211, 'accumulated_logging_time': 2.8909971714019775}
I0131 03:53:14.640873 139884794734336 logging_writer.py:48] [57911] accumulated_eval_time=2026.036435, accumulated_logging_time=2.890997, accumulated_submission_time=26525.360887, global_step=57911, preemption_count=0, score=26525.360887, test/accuracy=0.470900, test/loss=2.499398, test/num_examples=10000, total_duration=28557.506608, train/accuracy=0.632109, train/loss=1.673853, validation/accuracy=0.586300, validation/loss=1.871828, validation/num_examples=50000
I0131 03:53:51.036378 139884786341632 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2573082447052002, loss=5.469857215881348
I0131 03:54:37.152092 139884794734336 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2713494300842285, loss=5.471532821655273
I0131 03:55:23.964148 139884786341632 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.5827016830444336, loss=3.285719394683838
I0131 03:56:10.388038 139884794734336 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.676835060119629, loss=3.373065710067749
I0131 03:56:56.689873 139884786341632 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.183582067489624, loss=4.849215030670166
I0131 03:57:43.181112 139884794734336 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.5015109777450562, loss=3.454235076904297
I0131 03:58:29.560660 139884786341632 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.24030601978302, loss=5.145272254943848
I0131 03:59:15.841502 139884794734336 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9301594495773315, loss=3.3488495349884033
I0131 04:00:02.118590 139884786341632 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1533949375152588, loss=5.090336799621582
I0131 04:00:14.791581 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:00:25.145902 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:00:46.021290 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:00:47.668127 140085747812160 submission_runner.py:408] Time since start: 29010.56s, 	Step: 58829, 	{'train/accuracy': 0.6249608993530273, 'train/loss': 1.6657127141952515, 'validation/accuracy': 0.5823599696159363, 'validation/loss': 1.8642138242721558, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5110361576080322, 'test/num_examples': 10000, 'score': 26945.452782392502, 'total_duration': 29010.55766916275, 'accumulated_submission_time': 26945.452782392502, 'accumulated_eval_time': 2058.912977695465, 'accumulated_logging_time': 2.924208402633667}
I0131 04:00:47.696404 139884794734336 logging_writer.py:48] [58829] accumulated_eval_time=2058.912978, accumulated_logging_time=2.924208, accumulated_submission_time=26945.452782, global_step=58829, preemption_count=0, score=26945.452782, test/accuracy=0.459300, test/loss=2.511036, test/num_examples=10000, total_duration=29010.557669, train/accuracy=0.624961, train/loss=1.665713, validation/accuracy=0.582360, validation/loss=1.864214, validation/num_examples=50000
I0131 04:01:15.957091 139884786341632 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1379531621932983, loss=5.305087566375732
I0131 04:02:01.592439 139884794734336 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.6353639364242554, loss=3.3065125942230225
I0131 04:02:48.172502 139884786341632 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.777321219444275, loss=3.2477574348449707
I0131 04:03:34.647058 139884794734336 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9620661735534668, loss=3.393839120864868
I0131 04:04:20.899850 139884786341632 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1813406944274902, loss=4.836458206176758
I0131 04:05:07.342001 139884794734336 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.5299909114837646, loss=3.33314847946167
I0131 04:05:53.824236 139884786341632 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.287635326385498, loss=3.8396663665771484
I0131 04:06:40.202182 139884794734336 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.7249727249145508, loss=3.340471029281616
I0131 04:07:26.777708 139884786341632 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.678060531616211, loss=3.4979653358459473
I0131 04:07:47.806439 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:07:58.082380 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:08:19.677607 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:08:21.314339 140085747812160 submission_runner.py:408] Time since start: 29464.20s, 	Step: 59747, 	{'train/accuracy': 0.643359363079071, 'train/loss': 1.587910771369934, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.8554558753967285, 'validation/num_examples': 50000, 'test/accuracy': 0.4684000313282013, 'test/loss': 2.4895777702331543, 'test/num_examples': 10000, 'score': 27365.503889799118, 'total_duration': 29464.2038834095, 'accumulated_submission_time': 27365.503889799118, 'accumulated_eval_time': 2092.420877933502, 'accumulated_logging_time': 2.9613430500030518}
I0131 04:08:21.339808 139884794734336 logging_writer.py:48] [59747] accumulated_eval_time=2092.420878, accumulated_logging_time=2.961343, accumulated_submission_time=27365.503890, global_step=59747, preemption_count=0, score=27365.503890, test/accuracy=0.468400, test/loss=2.489578, test/num_examples=10000, total_duration=29464.203883, train/accuracy=0.643359, train/loss=1.587911, validation/accuracy=0.587040, validation/loss=1.855456, validation/num_examples=50000
I0131 04:08:42.533089 139884786341632 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.2237308025360107, loss=5.287974834442139
I0131 04:09:26.863632 139884794734336 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.762996792793274, loss=3.385669708251953
I0131 04:10:13.259885 139884786341632 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.3262680768966675, loss=4.701233863830566
I0131 04:11:00.045767 139884794734336 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.561570167541504, loss=3.355131149291992
I0131 04:11:46.564288 139884786341632 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.534222960472107, loss=3.519514322280884
I0131 04:12:33.286754 139884794734336 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.6304824352264404, loss=3.240612745285034
I0131 04:13:19.859221 139884786341632 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.5648690462112427, loss=3.46597957611084
I0131 04:14:06.223404 139884794734336 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.4199875593185425, loss=3.977808713912964
I0131 04:14:52.793040 139884786341632 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7126866579055786, loss=3.3537280559539795
I0131 04:15:21.616241 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:15:32.173513 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:15:52.543291 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:15:54.188892 140085747812160 submission_runner.py:408] Time since start: 29917.08s, 	Step: 60664, 	{'train/accuracy': 0.6283788681030273, 'train/loss': 1.6621125936508179, 'validation/accuracy': 0.585319995880127, 'validation/loss': 1.851845622062683, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.470869541168213, 'test/num_examples': 10000, 'score': 27785.721799850464, 'total_duration': 29917.078412532806, 'accumulated_submission_time': 27785.721799850464, 'accumulated_eval_time': 2124.993516921997, 'accumulated_logging_time': 2.995525598526001}
I0131 04:15:54.214793 139884794734336 logging_writer.py:48] [60664] accumulated_eval_time=2124.993517, accumulated_logging_time=2.995526, accumulated_submission_time=27785.721800, global_step=60664, preemption_count=0, score=27785.721800, test/accuracy=0.467200, test/loss=2.470870, test/num_examples=10000, total_duration=29917.078413, train/accuracy=0.628379, train/loss=1.662113, validation/accuracy=0.585320, validation/loss=1.851846, validation/num_examples=50000
I0131 04:16:08.734313 139884786341632 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2651597261428833, loss=4.278779983520508
I0131 04:16:52.249522 139884794734336 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.6744623184204102, loss=3.299243927001953
I0131 04:17:38.662267 139884786341632 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.3661795854568481, loss=5.391216278076172
I0131 04:18:25.164493 139884794734336 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3674311637878418, loss=3.8534722328186035
I0131 04:19:11.416687 139884786341632 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.5394219160079956, loss=3.1948094367980957
I0131 04:19:57.870030 139884794734336 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.7162749767303467, loss=3.2888972759246826
I0131 04:20:44.415186 139884786341632 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.3270946741104126, loss=5.04471492767334
I0131 04:21:30.677334 139884794734336 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.6206990480422974, loss=3.3547983169555664
I0131 04:22:17.305858 139884786341632 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.5795940160751343, loss=3.4058687686920166
I0131 04:22:54.527569 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:23:05.277693 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:23:26.478907 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:23:28.117522 140085747812160 submission_runner.py:408] Time since start: 30371.01s, 	Step: 61582, 	{'train/accuracy': 0.6338085532188416, 'train/loss': 1.6124237775802612, 'validation/accuracy': 0.591219961643219, 'validation/loss': 1.8236290216445923, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.4673588275909424, 'test/num_examples': 10000, 'score': 28205.97324538231, 'total_duration': 30371.007069826126, 'accumulated_submission_time': 28205.97324538231, 'accumulated_eval_time': 2158.5834772586823, 'accumulated_logging_time': 3.031461477279663}
I0131 04:23:28.145038 139884794734336 logging_writer.py:48] [61582] accumulated_eval_time=2158.583477, accumulated_logging_time=3.031461, accumulated_submission_time=28205.973245, global_step=61582, preemption_count=0, score=28205.973245, test/accuracy=0.475600, test/loss=2.467359, test/num_examples=10000, total_duration=30371.007070, train/accuracy=0.633809, train/loss=1.612424, validation/accuracy=0.591220, validation/loss=1.823629, validation/num_examples=50000
I0131 04:23:35.611266 139884786341632 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.3188480138778687, loss=4.710681915283203
I0131 04:24:17.843765 139884794734336 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.4615494012832642, loss=3.289896011352539
I0131 04:25:04.045825 139884786341632 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.7577073574066162, loss=3.3448684215545654
I0131 04:25:50.573917 139884794734336 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.6087992191314697, loss=3.5951414108276367
I0131 04:26:37.036929 139884786341632 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2588181495666504, loss=4.770879745483398
I0131 04:27:23.261501 139884794734336 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.6307672262191772, loss=3.328014850616455
I0131 04:28:09.570424 139884786341632 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.268337607383728, loss=4.971268653869629
I0131 04:28:55.927269 139884794734336 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.358816385269165, loss=3.588304042816162
I0131 04:29:42.547012 139884786341632 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.5837841033935547, loss=3.5283942222595215
I0131 04:30:28.839873 139884794734336 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.451005220413208, loss=4.018672466278076
I0131 04:30:28.854228 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:30:39.354485 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:30:58.211739 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:30:59.847268 140085747812160 submission_runner.py:408] Time since start: 30822.74s, 	Step: 62501, 	{'train/accuracy': 0.6354882717132568, 'train/loss': 1.626826286315918, 'validation/accuracy': 0.5827599763870239, 'validation/loss': 1.8661490678787231, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.5128331184387207, 'test/num_examples': 10000, 'score': 28626.62084031105, 'total_duration': 30822.73681116104, 'accumulated_submission_time': 28626.62084031105, 'accumulated_eval_time': 2189.576506137848, 'accumulated_logging_time': 3.0699973106384277}
I0131 04:30:59.872122 139884786341632 logging_writer.py:48] [62501] accumulated_eval_time=2189.576506, accumulated_logging_time=3.069997, accumulated_submission_time=28626.620840, global_step=62501, preemption_count=0, score=28626.620840, test/accuracy=0.469700, test/loss=2.512833, test/num_examples=10000, total_duration=30822.736811, train/accuracy=0.635488, train/loss=1.626826, validation/accuracy=0.582760, validation/loss=1.866149, validation/num_examples=50000
I0131 04:31:40.768263 139884794734336 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.494681715965271, loss=4.709476947784424
I0131 04:32:27.167739 139884786341632 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.5174055099487305, loss=3.476294994354248
I0131 04:33:13.891985 139884794734336 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3736506700515747, loss=5.429423809051514
I0131 04:34:00.383703 139884786341632 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.525050163269043, loss=3.2226223945617676
I0131 04:34:46.939042 139884794734336 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.588046669960022, loss=3.303138256072998
I0131 04:35:33.415397 139884786341632 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.6276112794876099, loss=3.191624402999878
I0131 04:36:19.511404 139884794734336 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.2047137022018433, loss=4.880866050720215
I0131 04:37:05.929484 139884786341632 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.3018368482589722, loss=5.32749080657959
I0131 04:37:52.130563 139884794734336 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2481849193572998, loss=5.348417282104492
I0131 04:37:59.988903 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:38:10.378382 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:38:31.528913 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:38:33.178684 140085747812160 submission_runner.py:408] Time since start: 31276.07s, 	Step: 63419, 	{'train/accuracy': 0.6289843320846558, 'train/loss': 1.6643264293670654, 'validation/accuracy': 0.583899974822998, 'validation/loss': 1.8471851348876953, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.485158920288086, 'test/num_examples': 10000, 'score': 29046.67929458618, 'total_duration': 31276.068233013153, 'accumulated_submission_time': 29046.67929458618, 'accumulated_eval_time': 2222.7663156986237, 'accumulated_logging_time': 3.1039247512817383}
I0131 04:38:33.204697 139884786341632 logging_writer.py:48] [63419] accumulated_eval_time=2222.766316, accumulated_logging_time=3.103925, accumulated_submission_time=29046.679295, global_step=63419, preemption_count=0, score=29046.679295, test/accuracy=0.467400, test/loss=2.485159, test/num_examples=10000, total_duration=31276.068233, train/accuracy=0.628984, train/loss=1.664326, validation/accuracy=0.583900, validation/loss=1.847185, validation/num_examples=50000
I0131 04:39:06.139321 139884794734336 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.5480659008026123, loss=3.191133737564087
I0131 04:39:52.259045 139884786341632 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.4060488939285278, loss=4.8682661056518555
I0131 04:40:38.791764 139884794734336 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.6969918012619019, loss=3.38777494430542
I0131 04:41:25.427194 139884786341632 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.6677218675613403, loss=3.2921745777130127
I0131 04:42:11.661077 139884794734336 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.3822404146194458, loss=4.012582778930664
I0131 04:42:58.352114 139884786341632 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.7348183393478394, loss=3.310741662979126
I0131 04:43:44.747604 139884794734336 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.657597303390503, loss=3.3037831783294678
I0131 04:44:31.203919 139884786341632 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.477708339691162, loss=4.307192325592041
I0131 04:45:17.761198 139884794734336 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.5163477659225464, loss=3.128939151763916
I0131 04:45:33.280981 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:45:43.797008 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:46:04.150696 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:46:05.788096 140085747812160 submission_runner.py:408] Time since start: 31728.68s, 	Step: 64335, 	{'train/accuracy': 0.6406640410423279, 'train/loss': 1.5750188827514648, 'validation/accuracy': 0.5931800007820129, 'validation/loss': 1.7844353914260864, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.420977830886841, 'test/num_examples': 10000, 'score': 29466.694981575012, 'total_duration': 31728.67764186859, 'accumulated_submission_time': 29466.694981575012, 'accumulated_eval_time': 2255.2734336853027, 'accumulated_logging_time': 3.1402459144592285}
I0131 04:46:05.814369 139884786341632 logging_writer.py:48] [64335] accumulated_eval_time=2255.273434, accumulated_logging_time=3.140246, accumulated_submission_time=29466.694982, global_step=64335, preemption_count=0, score=29466.694982, test/accuracy=0.473100, test/loss=2.420978, test/num_examples=10000, total_duration=31728.677642, train/accuracy=0.640664, train/loss=1.575019, validation/accuracy=0.593180, validation/loss=1.784435, validation/num_examples=50000
I0131 04:46:31.714605 139884794734336 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.9681137800216675, loss=3.396564245223999
I0131 04:47:17.244718 139884786341632 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.8716022968292236, loss=3.1748013496398926
I0131 04:48:03.774308 139884794734336 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.850300908088684, loss=3.418348789215088
I0131 04:48:50.101884 139884786341632 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.3597657680511475, loss=5.198418617248535
I0131 04:49:36.727190 139884794734336 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.6754881143569946, loss=3.3230714797973633
I0131 04:50:23.203876 139884786341632 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.4370019435882568, loss=3.6512131690979004
I0131 04:51:09.839727 139884794734336 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.633225440979004, loss=3.3660078048706055
I0131 04:51:56.409259 139884786341632 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.7520053386688232, loss=3.225637197494507
I0131 04:52:43.099927 139884794734336 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.7442238330841064, loss=3.2622570991516113
I0131 04:53:06.087943 140085747812160 spec.py:321] Evaluating on the training split.
I0131 04:53:16.383647 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 04:53:37.399169 140085747812160 spec.py:349] Evaluating on the test split.
I0131 04:53:39.038198 140085747812160 submission_runner.py:408] Time since start: 32181.93s, 	Step: 65251, 	{'train/accuracy': 0.6419140696525574, 'train/loss': 1.613297700881958, 'validation/accuracy': 0.5929799675941467, 'validation/loss': 1.8457796573638916, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.480360984802246, 'test/num_examples': 10000, 'score': 29886.907014369965, 'total_duration': 32181.92774629593, 'accumulated_submission_time': 29886.907014369965, 'accumulated_eval_time': 2288.2237129211426, 'accumulated_logging_time': 3.1775012016296387}
I0131 04:53:39.063164 139884786341632 logging_writer.py:48] [65251] accumulated_eval_time=2288.223713, accumulated_logging_time=3.177501, accumulated_submission_time=29886.907014, global_step=65251, preemption_count=0, score=29886.907014, test/accuracy=0.478100, test/loss=2.480361, test/num_examples=10000, total_duration=32181.927746, train/accuracy=0.641914, train/loss=1.613298, validation/accuracy=0.592980, validation/loss=1.845780, validation/num_examples=50000
I0131 04:53:58.696613 139884794734336 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.683349847793579, loss=3.1952173709869385
I0131 04:54:43.030881 139884786341632 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9311983585357666, loss=3.2977821826934814
I0131 04:55:29.718054 139884794734336 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8352354764938354, loss=3.425309658050537
I0131 04:56:16.353173 139884786341632 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.5387063026428223, loss=4.179469108581543
I0131 04:57:02.573433 139884794734336 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.630856990814209, loss=3.5839076042175293
I0131 04:57:49.113307 139884786341632 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.7110553979873657, loss=3.186305522918701
I0131 04:58:35.632164 139884794734336 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.3162574768066406, loss=5.4183349609375
I0131 04:59:22.322839 139884786341632 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.379066824913025, loss=3.6491527557373047
I0131 05:00:08.812073 139884794734336 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4806805849075317, loss=5.12601375579834
I0131 05:00:39.263747 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:00:49.334430 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:01:09.934432 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:01:11.579947 140085747812160 submission_runner.py:408] Time since start: 32634.47s, 	Step: 66167, 	{'train/accuracy': 0.6465820074081421, 'train/loss': 1.5871703624725342, 'validation/accuracy': 0.596560001373291, 'validation/loss': 1.818373203277588, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.46311092376709, 'test/num_examples': 10000, 'score': 30307.044764518738, 'total_duration': 32634.469495773315, 'accumulated_submission_time': 30307.044764518738, 'accumulated_eval_time': 2320.53994345665, 'accumulated_logging_time': 3.215022325515747}
I0131 05:01:11.607062 139884786341632 logging_writer.py:48] [66167] accumulated_eval_time=2320.539943, accumulated_logging_time=3.215022, accumulated_submission_time=30307.044765, global_step=66167, preemption_count=0, score=30307.044765, test/accuracy=0.472000, test/loss=2.463111, test/num_examples=10000, total_duration=32634.469496, train/accuracy=0.646582, train/loss=1.587170, validation/accuracy=0.596560, validation/loss=1.818373, validation/num_examples=50000
I0131 05:01:24.959599 139884794734336 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.3427718877792358, loss=4.746370315551758
I0131 05:02:08.542817 139884786341632 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.4437739849090576, loss=3.985840082168579
I0131 05:02:54.828741 139884794734336 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.3070178031921387, loss=4.822782039642334
I0131 05:03:41.377292 139884786341632 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.448915958404541, loss=5.38073205947876
I0131 05:04:27.900652 139884794734336 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.5246483087539673, loss=3.2284398078918457
I0131 05:05:14.372638 139884786341632 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.73518967628479, loss=3.277763605117798
I0131 05:06:00.908766 139884794734336 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.8433688879013062, loss=3.306790590286255
I0131 05:06:47.250118 139884786341632 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.6966869831085205, loss=3.2452232837677
I0131 05:07:33.686927 139884794734336 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.5979865789413452, loss=3.3177762031555176
I0131 05:08:11.722236 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:08:22.118899 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:08:41.783175 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:08:43.424701 140085747812160 submission_runner.py:408] Time since start: 33086.31s, 	Step: 67084, 	{'train/accuracy': 0.6468554735183716, 'train/loss': 1.5900592803955078, 'validation/accuracy': 0.6029599905014038, 'validation/loss': 1.7880793809890747, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.422799825668335, 'test/num_examples': 10000, 'score': 30727.09972333908, 'total_duration': 33086.314239263535, 'accumulated_submission_time': 30727.09972333908, 'accumulated_eval_time': 2352.2424013614655, 'accumulated_logging_time': 3.251006841659546}
I0131 05:08:43.453919 139884786341632 logging_writer.py:48] [67084] accumulated_eval_time=2352.242401, accumulated_logging_time=3.251007, accumulated_submission_time=30727.099723, global_step=67084, preemption_count=0, score=30727.099723, test/accuracy=0.483000, test/loss=2.422800, test/num_examples=10000, total_duration=33086.314239, train/accuracy=0.646855, train/loss=1.590059, validation/accuracy=0.602960, validation/loss=1.788079, validation/num_examples=50000
I0131 05:08:50.129323 139884794734336 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.5354952812194824, loss=3.584395408630371
I0131 05:09:32.373856 139884786341632 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.5264250040054321, loss=3.440453052520752
I0131 05:10:18.893999 139884794734336 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.6701990365982056, loss=3.3618545532226562
I0131 05:11:05.415209 139884786341632 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3139894008636475, loss=4.8527116775512695
I0131 05:11:51.702372 139884794734336 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.7535243034362793, loss=3.3013861179351807
I0131 05:12:38.251202 139884786341632 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.2982579469680786, loss=4.333009719848633
I0131 05:13:24.644750 139884794734336 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.5317270755767822, loss=3.4187185764312744
I0131 05:14:10.967282 139884786341632 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.700497031211853, loss=3.263037919998169
I0131 05:14:57.569536 139884794734336 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.391801357269287, loss=4.236578941345215
I0131 05:15:43.777812 139884786341632 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.5477855205535889, loss=3.3963825702667236
I0131 05:15:43.792658 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:15:54.178176 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:16:14.896174 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:16:16.538097 140085747812160 submission_runner.py:408] Time since start: 33539.43s, 	Step: 68001, 	{'train/accuracy': 0.6465234160423279, 'train/loss': 1.5876259803771973, 'validation/accuracy': 0.5993599891662598, 'validation/loss': 1.8079662322998047, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.4284214973449707, 'test/num_examples': 10000, 'score': 31147.37824487686, 'total_duration': 33539.42764592171, 'accumulated_submission_time': 31147.37824487686, 'accumulated_eval_time': 2384.98783993721, 'accumulated_logging_time': 3.2904880046844482}
I0131 05:16:16.561815 139884794734336 logging_writer.py:48] [68001] accumulated_eval_time=2384.987840, accumulated_logging_time=3.290488, accumulated_submission_time=31147.378245, global_step=68001, preemption_count=0, score=31147.378245, test/accuracy=0.483200, test/loss=2.428421, test/num_examples=10000, total_duration=33539.427646, train/accuracy=0.646523, train/loss=1.587626, validation/accuracy=0.599360, validation/loss=1.807966, validation/num_examples=50000
I0131 05:16:57.125054 139884786341632 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.6550958156585693, loss=3.2521562576293945
I0131 05:17:43.430446 139884794734336 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.6089231967926025, loss=3.247098922729492
I0131 05:18:29.947540 139884786341632 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.41868257522583, loss=4.490780830383301
I0131 05:19:16.278943 139884794734336 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.7976001501083374, loss=3.256016731262207
I0131 05:20:02.750410 139884786341632 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2962101697921753, loss=4.7075653076171875
I0131 05:20:50.088170 139884794734336 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.3680483102798462, loss=5.401349067687988
I0131 05:21:36.435324 139884786341632 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.4856740236282349, loss=3.8374526500701904
I0131 05:22:23.291525 139884794734336 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.6974443197250366, loss=3.247788667678833
I0131 05:23:09.962009 139884786341632 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.921634554862976, loss=3.351114273071289
I0131 05:23:16.583041 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:23:26.994953 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:23:46.733179 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:23:48.384149 140085747812160 submission_runner.py:408] Time since start: 33991.27s, 	Step: 68916, 	{'train/accuracy': 0.6715624928474426, 'train/loss': 1.442800521850586, 'validation/accuracy': 0.6010000109672546, 'validation/loss': 1.7613905668258667, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.398958206176758, 'test/num_examples': 10000, 'score': 31567.33975481987, 'total_duration': 33991.27369570732, 'accumulated_submission_time': 31567.33975481987, 'accumulated_eval_time': 2416.7889444828033, 'accumulated_logging_time': 3.324157476425171}
I0131 05:23:48.412103 139884794734336 logging_writer.py:48] [68916] accumulated_eval_time=2416.788944, accumulated_logging_time=3.324157, accumulated_submission_time=31567.339755, global_step=68916, preemption_count=0, score=31567.339755, test/accuracy=0.484300, test/loss=2.398958, test/num_examples=10000, total_duration=33991.273696, train/accuracy=0.671562, train/loss=1.442801, validation/accuracy=0.601000, validation/loss=1.761391, validation/num_examples=50000
I0131 05:24:22.625229 139884786341632 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3112518787384033, loss=4.032983779907227
I0131 05:25:08.909972 139884794734336 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.2795673608779907, loss=4.870262145996094
I0131 05:25:55.610081 139884786341632 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3879379034042358, loss=4.435140132904053
I0131 05:26:42.142647 139884794734336 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.6171016693115234, loss=3.5372281074523926
I0131 05:27:28.678744 139884786341632 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.8166559934616089, loss=4.827388763427734
I0131 05:28:15.249353 139884794734336 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.5313608646392822, loss=3.519710063934326
I0131 05:29:01.497660 139884786341632 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.4693666696548462, loss=3.889072895050049
I0131 05:29:48.242301 139884794734336 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.7106397151947021, loss=3.352832317352295
I0131 05:30:35.071120 139884786341632 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.0579116344451904, loss=3.2023935317993164
I0131 05:30:48.824955 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:30:59.222171 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:31:19.244956 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:31:20.889857 140085747812160 submission_runner.py:408] Time since start: 34443.78s, 	Step: 69831, 	{'train/accuracy': 0.6374413967132568, 'train/loss': 1.655277967453003, 'validation/accuracy': 0.5964199900627136, 'validation/loss': 1.8324724435806274, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.4573731422424316, 'test/num_examples': 10000, 'score': 31987.69392681122, 'total_duration': 34443.77940249443, 'accumulated_submission_time': 31987.69392681122, 'accumulated_eval_time': 2448.8538324832916, 'accumulated_logging_time': 3.3609249591827393}
I0131 05:31:20.916567 139884794734336 logging_writer.py:48] [69831] accumulated_eval_time=2448.853832, accumulated_logging_time=3.360925, accumulated_submission_time=31987.693927, global_step=69831, preemption_count=0, score=31987.693927, test/accuracy=0.482600, test/loss=2.457373, test/num_examples=10000, total_duration=34443.779402, train/accuracy=0.637441, train/loss=1.655278, validation/accuracy=0.596420, validation/loss=1.832472, validation/num_examples=50000
I0131 05:31:48.380060 139884786341632 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.6538374423980713, loss=3.124630928039551
I0131 05:32:34.627901 139884794734336 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.5361758470535278, loss=4.00020170211792
I0131 05:33:21.247175 139884786341632 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.81163489818573, loss=3.2063567638397217
I0131 05:34:07.824681 139884794734336 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4100574254989624, loss=3.7773194313049316
I0131 05:34:53.978202 139884786341632 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.5995380878448486, loss=3.1858208179473877
I0131 05:35:40.547297 139884794734336 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.4924530982971191, loss=4.508002758026123
I0131 05:36:26.905568 139884786341632 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.4441769123077393, loss=5.287622451782227
I0131 05:37:13.230567 139884794734336 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.608221173286438, loss=3.186387538909912
I0131 05:38:00.019549 139884786341632 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.75173819065094, loss=3.291862726211548
I0131 05:38:20.957957 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:38:31.184984 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:38:49.093433 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:38:50.745118 140085747812160 submission_runner.py:408] Time since start: 34893.63s, 	Step: 70747, 	{'train/accuracy': 0.6484960913658142, 'train/loss': 1.5486077070236206, 'validation/accuracy': 0.6013599634170532, 'validation/loss': 1.7578142881393433, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.390446662902832, 'test/num_examples': 10000, 'score': 32407.676120996475, 'total_duration': 34893.63464021683, 'accumulated_submission_time': 32407.676120996475, 'accumulated_eval_time': 2478.6409730911255, 'accumulated_logging_time': 3.3965108394622803}
I0131 05:38:50.779177 139884794734336 logging_writer.py:48] [70747] accumulated_eval_time=2478.640973, accumulated_logging_time=3.396511, accumulated_submission_time=32407.676121, global_step=70747, preemption_count=0, score=32407.676121, test/accuracy=0.476300, test/loss=2.390447, test/num_examples=10000, total_duration=34893.634640, train/accuracy=0.648496, train/loss=1.548608, validation/accuracy=0.601360, validation/loss=1.757814, validation/num_examples=50000
I0131 05:39:11.996520 139884786341632 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.804142713546753, loss=3.185845136642456
I0131 05:39:57.060848 139884794734336 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.3972445726394653, loss=4.493012428283691
I0131 05:40:43.592474 139884786341632 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.5615800619125366, loss=3.427313804626465
I0131 05:41:30.333489 139884794734336 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.66091787815094, loss=5.30116081237793
I0131 05:42:16.486489 139884786341632 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.6653382778167725, loss=3.2459640502929688
I0131 05:43:03.192407 139884794734336 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.333044409751892, loss=4.427462577819824
I0131 05:43:49.735233 139884786341632 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.8310331106185913, loss=3.234802007675171
I0131 05:44:36.069875 139884794734336 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.6213105916976929, loss=3.2367725372314453
I0131 05:45:22.736148 139884786341632 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4131039381027222, loss=4.158437728881836
I0131 05:45:50.990234 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:46:01.592662 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:46:21.221070 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:46:22.867933 140085747812160 submission_runner.py:408] Time since start: 35345.76s, 	Step: 71662, 	{'train/accuracy': 0.6622851490974426, 'train/loss': 1.4937940835952759, 'validation/accuracy': 0.6013199687004089, 'validation/loss': 1.7728713750839233, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.4019968509674072, 'test/num_examples': 10000, 'score': 32827.82526350021, 'total_duration': 35345.7574763298, 'accumulated_submission_time': 32827.82526350021, 'accumulated_eval_time': 2510.5186746120453, 'accumulated_logging_time': 3.442301034927368}
I0131 05:46:22.893474 139884794734336 logging_writer.py:48] [71662] accumulated_eval_time=2510.518675, accumulated_logging_time=3.442301, accumulated_submission_time=32827.825264, global_step=71662, preemption_count=0, score=32827.825264, test/accuracy=0.479800, test/loss=2.401997, test/num_examples=10000, total_duration=35345.757476, train/accuracy=0.662285, train/loss=1.493794, validation/accuracy=0.601320, validation/loss=1.772871, validation/num_examples=50000
I0131 05:46:38.201692 139884786341632 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.717366099357605, loss=3.159691333770752
I0131 05:47:21.798059 139884794734336 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.4064030647277832, loss=4.023471832275391
I0131 05:48:08.357008 139884786341632 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.4953664541244507, loss=3.533447742462158
I0131 05:48:55.132173 139884794734336 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.5282951593399048, loss=3.842487096786499
I0131 05:49:41.429558 139884786341632 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.6956303119659424, loss=3.6954548358917236
I0131 05:50:27.803104 139884794734336 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.0902509689331055, loss=3.375943899154663
I0131 05:51:14.395096 139884786341632 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.8121652603149414, loss=3.2271854877471924
I0131 05:52:00.757256 139884794734336 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.6822222471237183, loss=3.270115375518799
I0131 05:52:47.277861 139884786341632 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3544328212738037, loss=5.13554048538208
I0131 05:53:22.980768 140085747812160 spec.py:321] Evaluating on the training split.
I0131 05:53:32.927211 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 05:53:53.595144 140085747812160 spec.py:349] Evaluating on the test split.
I0131 05:53:55.246185 140085747812160 submission_runner.py:408] Time since start: 35798.14s, 	Step: 72578, 	{'train/accuracy': 0.6486914157867432, 'train/loss': 1.5760599374771118, 'validation/accuracy': 0.6078199744224548, 'validation/loss': 1.7598813772201538, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.406587600708008, 'test/num_examples': 10000, 'score': 33247.85134124756, 'total_duration': 35798.13573217392, 'accumulated_submission_time': 33247.85134124756, 'accumulated_eval_time': 2542.7840864658356, 'accumulated_logging_time': 3.47953462600708}
I0131 05:53:55.274091 139884794734336 logging_writer.py:48] [72578] accumulated_eval_time=2542.784086, accumulated_logging_time=3.479535, accumulated_submission_time=33247.851341, global_step=72578, preemption_count=0, score=33247.851341, test/accuracy=0.482000, test/loss=2.406588, test/num_examples=10000, total_duration=35798.135732, train/accuracy=0.648691, train/loss=1.576060, validation/accuracy=0.607820, validation/loss=1.759881, validation/num_examples=50000
I0131 05:54:04.316071 139884786341632 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.6515390872955322, loss=3.192976474761963
I0131 05:54:46.289703 139884794734336 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.3360024690628052, loss=5.125785827636719
I0131 05:55:32.548781 139884786341632 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9727094173431396, loss=3.250293493270874
I0131 05:56:19.383338 139884794734336 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.5173566341400146, loss=4.865423202514648
I0131 05:57:05.697857 139884786341632 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.4769864082336426, loss=3.8511550426483154
I0131 05:57:52.156043 139884794734336 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.6295667886734009, loss=3.1050829887390137
I0131 05:58:38.318979 139884786341632 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.6356998682022095, loss=3.1686651706695557
I0131 05:59:24.489870 139884794734336 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.5309441089630127, loss=3.0880050659179688
I0131 06:00:10.793923 139884786341632 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.6025049686431885, loss=3.141059398651123
I0131 06:00:55.641814 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:01:05.791180 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:01:25.671183 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:01:27.327667 140085747812160 submission_runner.py:408] Time since start: 36250.22s, 	Step: 73498, 	{'train/accuracy': 0.650683581829071, 'train/loss': 1.5488523244857788, 'validation/accuracy': 0.602840006351471, 'validation/loss': 1.7498250007629395, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.388460159301758, 'test/num_examples': 10000, 'score': 33668.151161670685, 'total_duration': 36250.21720933914, 'accumulated_submission_time': 33668.151161670685, 'accumulated_eval_time': 2574.4699428081512, 'accumulated_logging_time': 3.524752616882324}
I0131 06:01:27.354994 139884794734336 logging_writer.py:48] [73498] accumulated_eval_time=2574.469943, accumulated_logging_time=3.524753, accumulated_submission_time=33668.151162, global_step=73498, preemption_count=0, score=33668.151162, test/accuracy=0.481200, test/loss=2.388460, test/num_examples=10000, total_duration=36250.217209, train/accuracy=0.650684, train/loss=1.548852, validation/accuracy=0.602840, validation/loss=1.749825, validation/num_examples=50000
I0131 06:01:28.534790 139884786341632 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.501312255859375, loss=3.49017071723938
I0131 06:02:09.859162 139884794734336 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.6857681274414062, loss=3.1776223182678223
I0131 06:02:56.346217 139884786341632 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.7782313823699951, loss=3.2293288707733154
I0131 06:03:42.727449 139884794734336 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.6757787466049194, loss=3.1962831020355225
I0131 06:04:29.267202 139884786341632 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.5576494932174683, loss=4.830780029296875
I0131 06:05:15.602255 139884794734336 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.5554282665252686, loss=3.9100277423858643
I0131 06:06:01.953046 139884786341632 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.610766053199768, loss=3.3653078079223633
I0131 06:06:48.347901 139884794734336 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3812904357910156, loss=5.302803039550781
I0131 06:07:34.813843 139884786341632 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.8349674940109253, loss=3.1484055519104004
I0131 06:08:21.250988 139884794734336 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.834850788116455, loss=3.3047773838043213
I0131 06:08:27.420641 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:08:37.576356 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:08:57.316292 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:08:58.951961 140085747812160 submission_runner.py:408] Time since start: 36701.84s, 	Step: 74415, 	{'train/accuracy': 0.6654492020606995, 'train/loss': 1.506258487701416, 'validation/accuracy': 0.6062999963760376, 'validation/loss': 1.7637112140655518, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.3955132961273193, 'test/num_examples': 10000, 'score': 34088.156512498856, 'total_duration': 36701.84150767326, 'accumulated_submission_time': 34088.156512498856, 'accumulated_eval_time': 2606.0012538433075, 'accumulated_logging_time': 3.561843156814575}
I0131 06:08:58.981870 139884786341632 logging_writer.py:48] [74415] accumulated_eval_time=2606.001254, accumulated_logging_time=3.561843, accumulated_submission_time=34088.156512, global_step=74415, preemption_count=0, score=34088.156512, test/accuracy=0.489100, test/loss=2.395513, test/num_examples=10000, total_duration=36701.841508, train/accuracy=0.665449, train/loss=1.506258, validation/accuracy=0.606300, validation/loss=1.763711, validation/num_examples=50000
I0131 06:09:33.046580 139884794734336 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.8066291809082031, loss=3.327334403991699
I0131 06:10:19.257277 139884786341632 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.9016177654266357, loss=3.3327155113220215
I0131 06:11:05.945208 139884794734336 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.7958194017410278, loss=3.2642765045166016
I0131 06:11:52.430063 139884786341632 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.78684663772583, loss=3.0291712284088135
I0131 06:12:39.158275 139884794734336 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.713628888130188, loss=3.91247296333313
I0131 06:13:25.533993 139884786341632 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.6645925045013428, loss=3.111607313156128
I0131 06:14:11.898293 139884794734336 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.6822423934936523, loss=3.1841022968292236
I0131 06:14:58.409997 139884786341632 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.6365032196044922, loss=3.465343952178955
I0131 06:15:44.870155 139884794734336 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4783135652542114, loss=4.958222389221191
I0131 06:15:59.309991 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:16:09.425570 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:16:31.712548 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:16:33.351533 140085747812160 submission_runner.py:408] Time since start: 37156.24s, 	Step: 75333, 	{'train/accuracy': 0.6486523151397705, 'train/loss': 1.547737956047058, 'validation/accuracy': 0.612500011920929, 'validation/loss': 1.729038119316101, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3694193363189697, 'test/num_examples': 10000, 'score': 34508.42599821091, 'total_duration': 37156.241079092026, 'accumulated_submission_time': 34508.42599821091, 'accumulated_eval_time': 2640.042801618576, 'accumulated_logging_time': 3.6005239486694336}
I0131 06:16:33.380043 139884786341632 logging_writer.py:48] [75333] accumulated_eval_time=2640.042802, accumulated_logging_time=3.600524, accumulated_submission_time=34508.425998, global_step=75333, preemption_count=0, score=34508.425998, test/accuracy=0.485600, test/loss=2.369419, test/num_examples=10000, total_duration=37156.241079, train/accuracy=0.648652, train/loss=1.547738, validation/accuracy=0.612500, validation/loss=1.729038, validation/num_examples=50000
I0131 06:17:00.068040 139884794734336 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.7463104724884033, loss=3.4336447715759277
I0131 06:17:45.417004 139884786341632 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.5652116537094116, loss=3.0963082313537598
I0131 06:18:32.162177 139884794734336 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.579047441482544, loss=3.2267398834228516
I0131 06:19:18.826407 139884786341632 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.5934079885482788, loss=5.371561527252197
I0131 06:20:05.132863 139884794734336 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.6040980815887451, loss=3.385611057281494
I0131 06:20:51.275148 139884786341632 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3925561904907227, loss=4.828777313232422
I0131 06:21:37.539506 139884794734336 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.6520144939422607, loss=3.167238235473633
I0131 06:22:24.345244 139884786341632 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.7808804512023926, loss=3.2726759910583496
I0131 06:23:10.894469 139884794734336 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.784331202507019, loss=3.0984179973602295
I0131 06:23:33.737037 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:23:43.887954 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:24:05.338485 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:24:06.982447 140085747812160 submission_runner.py:408] Time since start: 37609.87s, 	Step: 76251, 	{'train/accuracy': 0.6528710722923279, 'train/loss': 1.5526872873306274, 'validation/accuracy': 0.6100199818611145, 'validation/loss': 1.7567421197891235, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.3885414600372314, 'test/num_examples': 10000, 'score': 34928.72370290756, 'total_duration': 37609.87198114395, 'accumulated_submission_time': 34928.72370290756, 'accumulated_eval_time': 2673.2882075309753, 'accumulated_logging_time': 3.637963056564331}
I0131 06:24:07.008256 139884786341632 logging_writer.py:48] [76251] accumulated_eval_time=2673.288208, accumulated_logging_time=3.637963, accumulated_submission_time=34928.723703, global_step=76251, preemption_count=0, score=34928.723703, test/accuracy=0.486100, test/loss=2.388541, test/num_examples=10000, total_duration=37609.871981, train/accuracy=0.652871, train/loss=1.552687, validation/accuracy=0.610020, validation/loss=1.756742, validation/num_examples=50000
I0131 06:24:26.632239 139884794734336 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.6771531105041504, loss=3.2957186698913574
I0131 06:25:11.488712 139884786341632 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.5408660173416138, loss=3.072430372238159
I0131 06:25:57.782694 139884794734336 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.7954368591308594, loss=3.1683766841888428
I0131 06:26:44.418972 139884786341632 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.808925986289978, loss=3.2934956550598145
I0131 06:27:30.810842 139884794734336 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.761252999305725, loss=3.1325526237487793
I0131 06:28:17.579405 139884786341632 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.636809229850769, loss=5.303438663482666
I0131 06:29:03.882221 139884794734336 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.757095456123352, loss=3.1017563343048096
I0131 06:29:50.152493 139884786341632 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4927104711532593, loss=5.364895820617676
I0131 06:30:36.770892 139884794734336 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.4475417137145996, loss=4.737903118133545
I0131 06:31:07.164512 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:31:17.862308 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:31:40.342422 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:31:41.985973 140085747812160 submission_runner.py:408] Time since start: 38064.88s, 	Step: 77167, 	{'train/accuracy': 0.6625585556030273, 'train/loss': 1.513722538948059, 'validation/accuracy': 0.6113399863243103, 'validation/loss': 1.7449710369110107, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.388901948928833, 'test/num_examples': 10000, 'score': 35348.81794667244, 'total_duration': 38064.8754966259, 'accumulated_submission_time': 35348.81794667244, 'accumulated_eval_time': 2708.1096754074097, 'accumulated_logging_time': 3.672785997390747}
I0131 06:31:42.015547 139884786341632 logging_writer.py:48] [77167] accumulated_eval_time=2708.109675, accumulated_logging_time=3.672786, accumulated_submission_time=35348.817947, global_step=77167, preemption_count=0, score=35348.817947, test/accuracy=0.490300, test/loss=2.388902, test/num_examples=10000, total_duration=38064.875497, train/accuracy=0.662559, train/loss=1.513723, validation/accuracy=0.611340, validation/loss=1.744971, validation/num_examples=50000
I0131 06:31:55.351794 139884794734336 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.3846580982208252, loss=5.161131381988525
I0131 06:32:38.841039 139884786341632 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4909251928329468, loss=4.557971954345703
I0131 06:33:25.596865 139884794734336 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.7104437351226807, loss=3.151353597640991
I0131 06:34:12.329321 139884786341632 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.4061776399612427, loss=4.838713645935059
I0131 06:34:58.581788 139884794734336 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.6678411960601807, loss=3.83695125579834
I0131 06:35:45.159757 139884786341632 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.6322591304779053, loss=3.1935362815856934
I0131 06:36:31.694541 139884794734336 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.7802988290786743, loss=4.223829746246338
I0131 06:37:18.087077 139884786341632 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.6367863416671753, loss=3.230103015899658
I0131 06:38:04.360673 139884794734336 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.8328434228897095, loss=3.1595683097839355
I0131 06:38:42.251498 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:38:52.524998 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:39:13.376240 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:39:15.012417 140085747812160 submission_runner.py:408] Time since start: 38517.90s, 	Step: 78083, 	{'train/accuracy': 0.6606640219688416, 'train/loss': 1.493207335472107, 'validation/accuracy': 0.6092599630355835, 'validation/loss': 1.7219427824020386, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.3637871742248535, 'test/num_examples': 10000, 'score': 35768.99364566803, 'total_duration': 38517.90196752548, 'accumulated_submission_time': 35768.99364566803, 'accumulated_eval_time': 2740.87061214447, 'accumulated_logging_time': 3.7120413780212402}
I0131 06:39:15.042517 139884786341632 logging_writer.py:48] [78083] accumulated_eval_time=2740.870612, accumulated_logging_time=3.712041, accumulated_submission_time=35768.993646, global_step=78083, preemption_count=0, score=35768.993646, test/accuracy=0.486500, test/loss=2.363787, test/num_examples=10000, total_duration=38517.901968, train/accuracy=0.660664, train/loss=1.493207, validation/accuracy=0.609260, validation/loss=1.721943, validation/num_examples=50000
I0131 06:39:22.117720 139884794734336 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.700851321220398, loss=3.1086320877075195
I0131 06:40:04.508973 139884786341632 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.53628408908844, loss=3.2880263328552246
I0131 06:40:51.106159 139884794734336 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.4002304077148438, loss=4.867584228515625
I0131 06:41:37.798681 139884786341632 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.8607971668243408, loss=3.138721466064453
I0131 06:42:24.141452 139884794734336 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4380218982696533, loss=5.106081485748291
I0131 06:43:10.736205 139884786341632 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.8783494234085083, loss=3.192944049835205
I0131 06:43:57.410580 139884794734336 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.653240442276001, loss=3.397406578063965
I0131 06:44:43.966188 139884786341632 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.7567468881607056, loss=3.132499933242798
I0131 06:45:30.678727 139884794734336 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.6619434356689453, loss=3.235640287399292
I0131 06:46:15.406023 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:46:25.951818 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:46:47.135428 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:46:48.784472 140085747812160 submission_runner.py:408] Time since start: 38971.67s, 	Step: 78998, 	{'train/accuracy': 0.659375011920929, 'train/loss': 1.5171035528182983, 'validation/accuracy': 0.6125400066375732, 'validation/loss': 1.7171244621276855, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.359149217605591, 'test/num_examples': 10000, 'score': 36189.29687142372, 'total_duration': 38971.674016952515, 'accumulated_submission_time': 36189.29687142372, 'accumulated_eval_time': 2774.2490651607513, 'accumulated_logging_time': 3.7525453567504883}
I0131 06:46:48.814007 139884786341632 logging_writer.py:48] [78998] accumulated_eval_time=2774.249065, accumulated_logging_time=3.752545, accumulated_submission_time=36189.296871, global_step=78998, preemption_count=0, score=36189.296871, test/accuracy=0.488900, test/loss=2.359149, test/num_examples=10000, total_duration=38971.674017, train/accuracy=0.659375, train/loss=1.517104, validation/accuracy=0.612540, validation/loss=1.717124, validation/num_examples=50000
I0131 06:46:49.992457 139884794734336 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.7125496864318848, loss=3.488921642303467
I0131 06:47:30.953776 139884786341632 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.734893798828125, loss=3.1753320693969727
I0131 06:48:17.430567 139884794734336 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.799132227897644, loss=3.3263654708862305
I0131 06:49:03.887655 139884786341632 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.5703436136245728, loss=4.238257884979248
I0131 06:49:50.327244 139884794734336 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.7587876319885254, loss=3.4126391410827637
I0131 06:50:36.819591 139884786341632 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.5176748037338257, loss=3.780956745147705
I0131 06:51:23.311971 139884794734336 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.613631248474121, loss=3.4804491996765137
I0131 06:52:09.586361 139884786341632 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.7107481956481934, loss=3.1413915157318115
I0131 06:52:56.248400 139884794734336 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.944054126739502, loss=3.161996841430664
I0131 06:53:42.831754 139884786341632 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.9154183864593506, loss=3.223726749420166
I0131 06:53:49.031352 140085747812160 spec.py:321] Evaluating on the training split.
I0131 06:53:59.029798 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 06:54:20.520481 140085747812160 spec.py:349] Evaluating on the test split.
I0131 06:54:22.157994 140085747812160 submission_runner.py:408] Time since start: 39425.05s, 	Step: 79915, 	{'train/accuracy': 0.66845703125, 'train/loss': 1.4566603899002075, 'validation/accuracy': 0.6201399564743042, 'validation/loss': 1.6755112409591675, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.326002836227417, 'test/num_examples': 10000, 'score': 36609.45451402664, 'total_duration': 39425.04754161835, 'accumulated_submission_time': 36609.45451402664, 'accumulated_eval_time': 2807.375717639923, 'accumulated_logging_time': 3.791879415512085}
I0131 06:54:22.191152 139884794734336 logging_writer.py:48] [79915] accumulated_eval_time=2807.375718, accumulated_logging_time=3.791879, accumulated_submission_time=36609.454514, global_step=79915, preemption_count=0, score=36609.454514, test/accuracy=0.494300, test/loss=2.326003, test/num_examples=10000, total_duration=39425.047542, train/accuracy=0.668457, train/loss=1.456660, validation/accuracy=0.620140, validation/loss=1.675511, validation/num_examples=50000
I0131 06:54:56.696620 139884786341632 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.5263772010803223, loss=4.1394877433776855
I0131 06:55:43.061474 139884794734336 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.7164486646652222, loss=3.2786498069763184
I0131 06:56:29.589268 139884786341632 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.4235658645629883, loss=4.869535446166992
I0131 06:57:16.056003 139884794734336 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.8184258937835693, loss=3.3340988159179688
I0131 06:58:02.543277 139884786341632 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.8950831890106201, loss=3.300172805786133
I0131 06:58:49.182592 139884794734336 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.642552137374878, loss=3.1222846508026123
I0131 06:59:35.365107 139884786341632 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.7065796852111816, loss=3.1176843643188477
I0131 07:00:21.767108 139884794734336 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.5987555980682373, loss=3.297983169555664
I0131 07:01:08.436917 139884786341632 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.7291710376739502, loss=3.252502679824829
I0131 07:01:22.584410 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:01:32.827650 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:01:51.393298 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:01:53.045178 140085747812160 submission_runner.py:408] Time since start: 39875.93s, 	Step: 80832, 	{'train/accuracy': 0.6868945360183716, 'train/loss': 1.4382601976394653, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.7528892755508423, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.4042420387268066, 'test/num_examples': 10000, 'score': 37029.78786754608, 'total_duration': 39875.934716939926, 'accumulated_submission_time': 37029.78786754608, 'accumulated_eval_time': 2837.8364627361298, 'accumulated_logging_time': 3.83516001701355}
I0131 07:01:53.078564 139884794734336 logging_writer.py:48] [80832] accumulated_eval_time=2837.836463, accumulated_logging_time=3.835160, accumulated_submission_time=37029.787868, global_step=80832, preemption_count=0, score=37029.787868, test/accuracy=0.491000, test/loss=2.404242, test/num_examples=10000, total_duration=39875.934717, train/accuracy=0.686895, train/loss=1.438260, validation/accuracy=0.614820, validation/loss=1.752889, validation/num_examples=50000
I0131 07:02:20.461202 139884786341632 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.7866976261138916, loss=3.170940637588501
I0131 07:03:06.945021 139884794734336 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.930071473121643, loss=3.141092300415039
I0131 07:03:53.672209 139884786341632 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.009627103805542, loss=3.684114456176758
I0131 07:04:40.212409 139884794734336 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.8738380670547485, loss=3.1413586139678955
I0131 07:05:26.608191 139884786341632 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.9000416994094849, loss=3.1305739879608154
I0131 07:06:13.168764 139884794734336 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.6821969747543335, loss=3.183062791824341
I0131 07:06:59.173145 139884786341632 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.656618356704712, loss=3.1163806915283203
I0131 07:07:45.521424 139884794734336 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.871901273727417, loss=3.5020155906677246
I0131 07:08:32.079501 139884786341632 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.4329118728637695, loss=5.254579544067383
I0131 07:08:53.121521 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:09:03.285908 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:09:25.186747 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:09:26.836442 140085747812160 submission_runner.py:408] Time since start: 40329.73s, 	Step: 81747, 	{'train/accuracy': 0.6599804759025574, 'train/loss': 1.4933381080627441, 'validation/accuracy': 0.6163600087165833, 'validation/loss': 1.6874927282333374, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3316457271575928, 'test/num_examples': 10000, 'score': 37449.77145719528, 'total_duration': 40329.72599077225, 'accumulated_submission_time': 37449.77145719528, 'accumulated_eval_time': 2871.551378250122, 'accumulated_logging_time': 3.878218173980713}
I0131 07:09:26.863437 139884794734336 logging_writer.py:48] [81747] accumulated_eval_time=2871.551378, accumulated_logging_time=3.878218, accumulated_submission_time=37449.771457, global_step=81747, preemption_count=0, score=37449.771457, test/accuracy=0.495000, test/loss=2.331646, test/num_examples=10000, total_duration=40329.725991, train/accuracy=0.659980, train/loss=1.493338, validation/accuracy=0.616360, validation/loss=1.687493, validation/num_examples=50000
I0131 07:09:48.213630 139884786341632 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.6999661922454834, loss=3.0709598064422607
I0131 07:10:32.533049 139884794734336 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.793813705444336, loss=3.1134986877441406
I0131 07:11:19.086861 139884786341632 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.5911617279052734, loss=3.8260655403137207
I0131 07:12:05.480005 139884794734336 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.8832688331604004, loss=3.1328911781311035
I0131 07:12:51.915406 139884786341632 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.4259541034698486, loss=4.5190558433532715
I0131 07:13:38.554547 139884794734336 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.7714415788650513, loss=3.1027350425720215
I0131 07:14:24.858815 139884786341632 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.9398216009140015, loss=3.1059679985046387
I0131 07:15:11.493431 139884794734336 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.7497256994247437, loss=5.197691440582275
I0131 07:15:57.980277 139884786341632 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.7057831287384033, loss=3.2352521419525146
I0131 07:16:26.886816 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:16:37.148638 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:16:57.378453 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:16:59.032566 140085747812160 submission_runner.py:408] Time since start: 40781.92s, 	Step: 82663, 	{'train/accuracy': 0.6682031154632568, 'train/loss': 1.4543018341064453, 'validation/accuracy': 0.6208800077438354, 'validation/loss': 1.668615698814392, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.3003392219543457, 'test/num_examples': 10000, 'score': 37869.73485660553, 'total_duration': 40781.922105789185, 'accumulated_submission_time': 37869.73485660553, 'accumulated_eval_time': 2903.6971168518066, 'accumulated_logging_time': 3.9156272411346436}
I0131 07:16:59.061942 139884794734336 logging_writer.py:48] [82663] accumulated_eval_time=2903.697117, accumulated_logging_time=3.915627, accumulated_submission_time=37869.734857, global_step=82663, preemption_count=0, score=37869.734857, test/accuracy=0.493400, test/loss=2.300339, test/num_examples=10000, total_duration=40781.922106, train/accuracy=0.668203, train/loss=1.454302, validation/accuracy=0.620880, validation/loss=1.668616, validation/num_examples=50000
I0131 07:17:13.981726 139884786341632 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.9541172981262207, loss=3.014241933822632
I0131 07:17:56.832421 139884794734336 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.8958436250686646, loss=3.034257650375366
I0131 07:18:43.474311 139884786341632 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.625697135925293, loss=3.934797525405884
I0131 07:19:30.190938 139884794734336 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.7737449407577515, loss=3.1484627723693848
I0131 07:20:16.498279 139884786341632 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.5523107051849365, loss=4.885709285736084
I0131 07:21:02.882452 139884794734336 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.489171028137207, loss=4.502242565155029
I0131 07:21:49.339282 139884786341632 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.6247007846832275, loss=4.780937194824219
I0131 07:22:35.885312 139884794734336 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.6618233919143677, loss=3.4225077629089355
I0131 07:23:22.476203 139884786341632 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.5515345335006714, loss=3.9468469619750977
I0131 07:23:59.190760 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:24:09.845499 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:24:29.627413 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:24:31.276929 140085747812160 submission_runner.py:408] Time since start: 41234.17s, 	Step: 83581, 	{'train/accuracy': 0.6833788752555847, 'train/loss': 1.3950210809707642, 'validation/accuracy': 0.6195200085639954, 'validation/loss': 1.6812617778778076, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.320722818374634, 'test/num_examples': 10000, 'score': 38289.803233623505, 'total_duration': 41234.16646409035, 'accumulated_submission_time': 38289.803233623505, 'accumulated_eval_time': 2935.783272266388, 'accumulated_logging_time': 3.9549033641815186}
I0131 07:24:31.305251 139884794734336 logging_writer.py:48] [83581] accumulated_eval_time=2935.783272, accumulated_logging_time=3.954903, accumulated_submission_time=38289.803234, global_step=83581, preemption_count=0, score=38289.803234, test/accuracy=0.494600, test/loss=2.320723, test/num_examples=10000, total_duration=41234.166464, train/accuracy=0.683379, train/loss=1.395021, validation/accuracy=0.619520, validation/loss=1.681262, validation/num_examples=50000
I0131 07:24:39.158796 139884786341632 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.4751853942871094, loss=4.5644755363464355
I0131 07:25:21.873985 139884794734336 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.6661077737808228, loss=3.1365699768066406
I0131 07:26:08.206597 139884786341632 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.7694424390792847, loss=3.1006641387939453
I0131 07:26:54.863917 139884794734336 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.6326899528503418, loss=4.4649810791015625
I0131 07:27:41.264386 139884786341632 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.8181610107421875, loss=3.0963776111602783
I0131 07:28:28.040825 139884794734336 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.6487902402877808, loss=3.673760414123535
I0131 07:29:14.934223 139884786341632 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.623199224472046, loss=3.6641688346862793
I0131 07:30:01.231213 139884794734336 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.7712626457214355, loss=3.122544765472412
I0131 07:30:47.676253 139884786341632 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.8578253984451294, loss=3.0541021823883057
I0131 07:31:31.501080 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:31:42.014264 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:32:03.568502 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:32:05.213958 140085747812160 submission_runner.py:408] Time since start: 41688.10s, 	Step: 84496, 	{'train/accuracy': 0.6632617115974426, 'train/loss': 1.50346839427948, 'validation/accuracy': 0.6191200017929077, 'validation/loss': 1.6938670873641968, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.347602367401123, 'test/num_examples': 10000, 'score': 38709.94033956528, 'total_duration': 41688.10350394249, 'accumulated_submission_time': 38709.94033956528, 'accumulated_eval_time': 2969.496173620224, 'accumulated_logging_time': 3.991729259490967}
I0131 07:32:05.245914 139884794734336 logging_writer.py:48] [84496] accumulated_eval_time=2969.496174, accumulated_logging_time=3.991729, accumulated_submission_time=38709.940340, global_step=84496, preemption_count=0, score=38709.940340, test/accuracy=0.493400, test/loss=2.347602, test/num_examples=10000, total_duration=41688.103504, train/accuracy=0.663262, train/loss=1.503468, validation/accuracy=0.619120, validation/loss=1.693867, validation/num_examples=50000
I0131 07:32:07.216994 139884786341632 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.8573960065841675, loss=3.414830207824707
I0131 07:32:48.411154 139884794734336 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.5686442852020264, loss=4.669667720794678
I0131 07:33:34.624394 139884786341632 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.5794880390167236, loss=4.523641586303711
I0131 07:34:21.255715 139884794734336 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.8137279748916626, loss=3.253267526626587
I0131 07:35:07.751466 139884786341632 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.7431366443634033, loss=3.0899367332458496
I0131 07:35:54.257829 139884794734336 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.4472825527191162, loss=4.5135369300842285
I0131 07:36:40.954748 139884786341632 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.5714025497436523, loss=4.009481430053711
I0131 07:37:27.273998 139884794734336 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.4606313705444336, loss=5.177403926849365
I0131 07:38:13.904429 139884786341632 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.6681443452835083, loss=3.4199373722076416
I0131 07:39:00.567605 139884794734336 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.0904364585876465, loss=3.181889057159424
I0131 07:39:05.359406 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:39:16.008177 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:39:34.837954 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:39:36.508671 140085747812160 submission_runner.py:408] Time since start: 42139.40s, 	Step: 85412, 	{'train/accuracy': 0.6689453125, 'train/loss': 1.475509762763977, 'validation/accuracy': 0.6204000115394592, 'validation/loss': 1.6864943504333496, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3224990367889404, 'test/num_examples': 10000, 'score': 39129.99418258667, 'total_duration': 42139.39819288254, 'accumulated_submission_time': 39129.99418258667, 'accumulated_eval_time': 3000.645429611206, 'accumulated_logging_time': 4.0334930419921875}
I0131 07:39:36.544905 139884786341632 logging_writer.py:48] [85412] accumulated_eval_time=3000.645430, accumulated_logging_time=4.033493, accumulated_submission_time=39129.994183, global_step=85412, preemption_count=0, score=39129.994183, test/accuracy=0.497100, test/loss=2.322499, test/num_examples=10000, total_duration=42139.398193, train/accuracy=0.668945, train/loss=1.475510, validation/accuracy=0.620400, validation/loss=1.686494, validation/num_examples=50000
I0131 07:40:13.005288 139884794734336 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.0733730792999268, loss=3.44929838180542
I0131 07:40:59.244391 139884786341632 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.0453104972839355, loss=3.151337146759033
I0131 07:41:45.670552 139884794734336 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.775495171546936, loss=3.3113880157470703
I0131 07:42:32.378273 139884786341632 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.6200517416000366, loss=3.731203317642212
I0131 07:43:18.885232 139884794734336 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.685807228088379, loss=3.315979480743408
I0131 07:44:05.383452 139884786341632 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.4455511569976807, loss=4.89789342880249
I0131 07:44:52.123343 139884794734336 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.781428575515747, loss=3.1481809616088867
I0131 07:45:38.714112 139884786341632 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.6640599966049194, loss=3.576942205429077
I0131 07:46:25.080754 139884794734336 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.9575464725494385, loss=3.200603723526001
I0131 07:46:36.633768 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:46:47.698686 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:47:09.087820 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:47:10.723693 140085747812160 submission_runner.py:408] Time since start: 42593.61s, 	Step: 86326, 	{'train/accuracy': 0.679492175579071, 'train/loss': 1.427093505859375, 'validation/accuracy': 0.627299964427948, 'validation/loss': 1.6685994863510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.304348945617676, 'test/num_examples': 10000, 'score': 39550.02034115791, 'total_duration': 42593.61324310303, 'accumulated_submission_time': 39550.02034115791, 'accumulated_eval_time': 3034.7353508472443, 'accumulated_logging_time': 4.0823657512664795}
I0131 07:47:10.753509 139884786341632 logging_writer.py:48] [86326] accumulated_eval_time=3034.735351, accumulated_logging_time=4.082366, accumulated_submission_time=39550.020341, global_step=86326, preemption_count=0, score=39550.020341, test/accuracy=0.501000, test/loss=2.304349, test/num_examples=10000, total_duration=42593.613243, train/accuracy=0.679492, train/loss=1.427094, validation/accuracy=0.627300, validation/loss=1.668599, validation/num_examples=50000
I0131 07:47:40.171026 139884794734336 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.5268237590789795, loss=3.9260382652282715
I0131 07:48:25.873029 139884786341632 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.6930631399154663, loss=3.6267213821411133
I0131 07:49:12.509662 139884794734336 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.8843715190887451, loss=3.0944631099700928
I0131 07:49:58.897702 139884786341632 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.5738860368728638, loss=3.897555351257324
I0131 07:50:45.497069 139884794734336 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.845309853553772, loss=3.197972297668457
I0131 07:51:31.849229 139884786341632 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.6792601346969604, loss=5.331300258636475
I0131 07:52:18.118965 139884794734336 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.70891273021698, loss=3.0176234245300293
I0131 07:53:04.735992 139884786341632 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.9477415084838867, loss=5.243827819824219
I0131 07:53:51.111038 139884794734336 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.088669776916504, loss=3.2122108936309814
I0131 07:54:10.738995 140085747812160 spec.py:321] Evaluating on the training split.
I0131 07:54:21.033205 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 07:54:42.326038 140085747812160 spec.py:349] Evaluating on the test split.
I0131 07:54:43.976267 140085747812160 submission_runner.py:408] Time since start: 43046.87s, 	Step: 87244, 	{'train/accuracy': 0.6720312237739563, 'train/loss': 1.4575451612472534, 'validation/accuracy': 0.6271199584007263, 'validation/loss': 1.6576414108276367, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.290591239929199, 'test/num_examples': 10000, 'score': 39969.945786714554, 'total_duration': 43046.86581158638, 'accumulated_submission_time': 39969.945786714554, 'accumulated_eval_time': 3067.9726133346558, 'accumulated_logging_time': 4.12157940864563}
I0131 07:54:44.006576 139884786341632 logging_writer.py:48] [87244] accumulated_eval_time=3067.972613, accumulated_logging_time=4.121579, accumulated_submission_time=39969.945787, global_step=87244, preemption_count=0, score=39969.945787, test/accuracy=0.506300, test/loss=2.290591, test/num_examples=10000, total_duration=43046.865812, train/accuracy=0.672031, train/loss=1.457545, validation/accuracy=0.627120, validation/loss=1.657641, validation/num_examples=50000
I0131 07:55:06.382198 139884794734336 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5044844150543213, loss=4.8740692138671875
I0131 07:55:51.148515 139884786341632 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.7673475742340088, loss=3.1313371658325195
I0131 07:56:37.558294 139884794734336 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.7032718658447266, loss=3.538778781890869
I0131 07:57:24.623442 139884786341632 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.7423088550567627, loss=3.4730889797210693
I0131 07:58:10.764981 139884794734336 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.8202571868896484, loss=3.031968116760254
I0131 07:58:57.143219 139884786341632 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.0540924072265625, loss=3.121814012527466
I0131 07:59:43.357349 139884794734336 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.999953269958496, loss=3.0470192432403564
I0131 08:00:29.824542 139884786341632 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.0466268062591553, loss=3.059262275695801
I0131 08:01:16.225175 139884794734336 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.5849838256835938, loss=3.916674852371216
I0131 08:01:44.108347 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:01:54.750326 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:02:16.083811 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:02:17.721056 140085747812160 submission_runner.py:408] Time since start: 43500.61s, 	Step: 88162, 	{'train/accuracy': 0.6704687476158142, 'train/loss': 1.4411784410476685, 'validation/accuracy': 0.622439980506897, 'validation/loss': 1.6661144495010376, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.302229642868042, 'test/num_examples': 10000, 'score': 40389.9857711792, 'total_duration': 43500.610604286194, 'accumulated_submission_time': 40389.9857711792, 'accumulated_eval_time': 3101.585325717926, 'accumulated_logging_time': 4.163185358047485}
I0131 08:02:17.749525 139884786341632 logging_writer.py:48] [88162] accumulated_eval_time=3101.585326, accumulated_logging_time=4.163185, accumulated_submission_time=40389.985771, global_step=88162, preemption_count=0, score=40389.985771, test/accuracy=0.499700, test/loss=2.302230, test/num_examples=10000, total_duration=43500.610604, train/accuracy=0.670469, train/loss=1.441178, validation/accuracy=0.622440, validation/loss=1.666114, validation/num_examples=50000
I0131 08:02:33.061183 139884794734336 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.7275317907333374, loss=3.502258777618408
I0131 08:03:16.705083 139884786341632 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.7023571729660034, loss=3.6133816242218018
I0131 08:04:02.925157 139884794734336 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.6385464668273926, loss=4.068959712982178
I0131 08:04:49.865039 139884786341632 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.8807175159454346, loss=3.0619616508483887
I0131 08:05:36.192110 139884794734336 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.9351335763931274, loss=3.128247022628784
I0131 08:06:22.562453 139884786341632 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.9307435750961304, loss=4.6901936531066895
I0131 08:07:08.869734 139884794734336 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.6834778785705566, loss=2.982090950012207
I0131 08:07:55.427286 139884786341632 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.8792859315872192, loss=3.1628808975219727
I0131 08:08:41.749430 139884794734336 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.670045018196106, loss=3.7497851848602295
I0131 08:09:18.064118 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:09:28.354613 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:09:46.414277 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:09:48.066205 140085747812160 submission_runner.py:408] Time since start: 43950.96s, 	Step: 89080, 	{'train/accuracy': 0.6823828220367432, 'train/loss': 1.4180799722671509, 'validation/accuracy': 0.6261999607086182, 'validation/loss': 1.6651922464370728, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2891640663146973, 'test/num_examples': 10000, 'score': 40810.23924612999, 'total_duration': 43950.955739974976, 'accumulated_submission_time': 40810.23924612999, 'accumulated_eval_time': 3131.58740067482, 'accumulated_logging_time': 4.202484369277954}
I0131 08:09:48.100086 139884786341632 logging_writer.py:48] [89080] accumulated_eval_time=3131.587401, accumulated_logging_time=4.202484, accumulated_submission_time=40810.239246, global_step=89080, preemption_count=0, score=40810.239246, test/accuracy=0.508500, test/loss=2.289164, test/num_examples=10000, total_duration=43950.955740, train/accuracy=0.682383, train/loss=1.418080, validation/accuracy=0.626200, validation/loss=1.665192, validation/num_examples=50000
I0131 08:09:56.398172 139884794734336 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.5898082256317139, loss=3.7972018718719482
I0131 08:10:39.788777 139884786341632 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.7784650325775146, loss=3.089601516723633
I0131 08:11:26.177709 139884794734336 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.9327985048294067, loss=3.235459804534912
I0131 08:12:12.787784 139884786341632 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.4522066116333008, loss=4.276397705078125
I0131 08:12:59.368155 139884794734336 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.975709319114685, loss=3.100463628768921
I0131 08:13:45.890552 139884786341632 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.8276464939117432, loss=5.2205023765563965
I0131 08:14:32.188399 139884794734336 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.6149659156799316, loss=3.8614718914031982
I0131 08:15:18.470345 139884786341632 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.591981053352356, loss=3.4207382202148438
I0131 08:16:05.119619 139884794734336 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.8032732009887695, loss=3.1241683959960938
I0131 08:16:48.356703 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:16:58.456981 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:17:19.387686 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:17:21.033582 140085747812160 submission_runner.py:408] Time since start: 44403.92s, 	Step: 89995, 	{'train/accuracy': 0.6862109303474426, 'train/loss': 1.4118703603744507, 'validation/accuracy': 0.6287199854850769, 'validation/loss': 1.6493552923202515, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2911794185638428, 'test/num_examples': 10000, 'score': 41230.43558573723, 'total_duration': 44403.9231262207, 'accumulated_submission_time': 41230.43558573723, 'accumulated_eval_time': 3164.264275074005, 'accumulated_logging_time': 4.245952367782593}
I0131 08:17:21.065077 139884786341632 logging_writer.py:48] [89995] accumulated_eval_time=3164.264275, accumulated_logging_time=4.245952, accumulated_submission_time=41230.435586, global_step=89995, preemption_count=0, score=41230.435586, test/accuracy=0.505500, test/loss=2.291179, test/num_examples=10000, total_duration=44403.923126, train/accuracy=0.686211, train/loss=1.411870, validation/accuracy=0.628720, validation/loss=1.649355, validation/num_examples=50000
I0131 08:17:23.423310 139884794734336 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.923789381980896, loss=3.144911050796509
I0131 08:18:04.943693 139884786341632 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.671104907989502, loss=3.4559125900268555
I0131 08:18:51.082307 139884794734336 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.6886496543884277, loss=3.376392126083374
I0131 08:19:37.737248 139884786341632 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.1422412395477295, loss=3.2108399868011475
I0131 08:20:24.335173 139884794734336 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5709433555603027, loss=4.111515998840332
I0131 08:21:10.701331 139884786341632 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.9964609146118164, loss=2.9857239723205566
I0131 08:21:57.209792 139884794734336 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.4250097274780273, loss=4.332645893096924
I0131 08:22:43.848945 139884786341632 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.469754934310913, loss=4.484935760498047
I0131 08:23:30.435954 139884794734336 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.8195878267288208, loss=3.039357900619507
I0131 08:24:16.687139 139884786341632 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.7521703243255615, loss=3.040236234664917
I0131 08:24:21.514409 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:24:31.623441 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:24:51.240663 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:24:52.887016 140085747812160 submission_runner.py:408] Time since start: 44855.78s, 	Step: 90912, 	{'train/accuracy': 0.6795117259025574, 'train/loss': 1.4182102680206299, 'validation/accuracy': 0.6332799792289734, 'validation/loss': 1.625242829322815, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.259521722793579, 'test/num_examples': 10000, 'score': 41650.82543206215, 'total_duration': 44855.7765583992, 'accumulated_submission_time': 41650.82543206215, 'accumulated_eval_time': 3195.6368894577026, 'accumulated_logging_time': 4.286664247512817}
I0131 08:24:52.919045 139884794734336 logging_writer.py:48] [90912] accumulated_eval_time=3195.636889, accumulated_logging_time=4.286664, accumulated_submission_time=41650.825432, global_step=90912, preemption_count=0, score=41650.825432, test/accuracy=0.512400, test/loss=2.259522, test/num_examples=10000, total_duration=44855.776558, train/accuracy=0.679512, train/loss=1.418210, validation/accuracy=0.633280, validation/loss=1.625243, validation/num_examples=50000
I0131 08:25:28.876447 139884786341632 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.825283169746399, loss=3.849757671356201
I0131 08:26:15.239666 139884794734336 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.4967741966247559, loss=4.016722679138184
I0131 08:27:01.711101 139884786341632 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.6543773412704468, loss=4.668436050415039
I0131 08:27:48.090868 139884794734336 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.9072092771530151, loss=3.2689480781555176
I0131 08:28:34.613384 139884786341632 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.582829236984253, loss=4.812987327575684
I0131 08:29:21.105354 139884794734336 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.478189468383789, loss=4.585326194763184
I0131 08:30:07.730945 139884786341632 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.8268624544143677, loss=3.1910338401794434
I0131 08:30:54.055519 139884794734336 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.8607349395751953, loss=3.170119524002075
I0131 08:31:40.644480 139884786341632 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.94594407081604, loss=3.03804874420166
I0131 08:31:52.994006 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:32:03.329353 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:32:24.330466 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:32:25.973812 140085747812160 submission_runner.py:408] Time since start: 45308.86s, 	Step: 91828, 	{'train/accuracy': 0.6826757788658142, 'train/loss': 1.4430053234100342, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.6762853860855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.3019185066223145, 'test/num_examples': 10000, 'score': 42070.84104323387, 'total_duration': 45308.86335706711, 'accumulated_submission_time': 42070.84104323387, 'accumulated_eval_time': 3228.6166894435883, 'accumulated_logging_time': 4.32750678062439}
I0131 08:32:26.002919 139884794734336 logging_writer.py:48] [91828] accumulated_eval_time=3228.616689, accumulated_logging_time=4.327507, accumulated_submission_time=42070.841043, global_step=91828, preemption_count=0, score=42070.841043, test/accuracy=0.508100, test/loss=2.301919, test/num_examples=10000, total_duration=45308.863357, train/accuracy=0.682676, train/loss=1.443005, validation/accuracy=0.631740, validation/loss=1.676285, validation/num_examples=50000
I0131 08:32:54.646816 139884786341632 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.581088900566101, loss=4.329268932342529
I0131 08:33:40.295904 139884794734336 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.8541641235351562, loss=3.034269332885742
I0131 08:34:26.851165 139884786341632 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.7673450708389282, loss=3.2764782905578613
I0131 08:35:13.361883 139884794734336 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.5162558555603027, loss=5.006114959716797
I0131 08:35:59.654761 139884786341632 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.7266311645507812, loss=2.9446656703948975
I0131 08:36:46.093850 139884794734336 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.0502922534942627, loss=3.305175304412842
I0131 08:37:32.761308 139884786341632 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.913824439048767, loss=2.9250340461730957
I0131 08:38:19.404941 139884794734336 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.7468055486679077, loss=3.0518088340759277
I0131 08:39:05.969568 139884786341632 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.6373064517974854, loss=4.9482879638671875
I0131 08:39:26.020521 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:39:36.459075 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:39:54.936403 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:39:56.605197 140085747812160 submission_runner.py:408] Time since start: 45759.49s, 	Step: 92745, 	{'train/accuracy': 0.7028710842132568, 'train/loss': 1.3684577941894531, 'validation/accuracy': 0.6319199800491333, 'validation/loss': 1.6790177822113037, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.3166470527648926, 'test/num_examples': 10000, 'score': 42490.799355745316, 'total_duration': 45759.4947450161, 'accumulated_submission_time': 42490.799355745316, 'accumulated_eval_time': 3259.2013654708862, 'accumulated_logging_time': 4.36621356010437}
I0131 08:39:56.638216 139884794734336 logging_writer.py:48] [92745] accumulated_eval_time=3259.201365, accumulated_logging_time=4.366214, accumulated_submission_time=42490.799356, global_step=92745, preemption_count=0, score=42490.799356, test/accuracy=0.510000, test/loss=2.316647, test/num_examples=10000, total_duration=45759.494745, train/accuracy=0.702871, train/loss=1.368458, validation/accuracy=0.631920, validation/loss=1.679018, validation/num_examples=50000
I0131 08:40:18.631606 139884786341632 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.6779122352600098, loss=3.317133903503418
I0131 08:41:03.799448 139884794734336 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.9269094467163086, loss=3.1403446197509766
I0131 08:41:50.687930 139884786341632 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.6535066366195679, loss=5.225213050842285
I0131 08:42:37.958569 139884794734336 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.9810171127319336, loss=3.0418601036071777
I0131 08:43:24.449676 139884786341632 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.159574508666992, loss=3.3198602199554443
I0131 08:44:11.277742 139884794734336 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.6219512224197388, loss=4.695732593536377
I0131 08:44:57.910692 139884786341632 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.8712302446365356, loss=3.0251944065093994
I0131 08:45:44.392736 139884794734336 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.8650786876678467, loss=3.0395236015319824
I0131 08:46:31.361573 139884786341632 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.6212376356124878, loss=3.71856427192688
I0131 08:46:56.801043 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:47:07.571174 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:47:28.598294 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:47:30.232090 140085747812160 submission_runner.py:408] Time since start: 46213.12s, 	Step: 93656, 	{'train/accuracy': 0.6768945455551147, 'train/loss': 1.43509840965271, 'validation/accuracy': 0.6339799761772156, 'validation/loss': 1.6315512657165527, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2607595920562744, 'test/num_examples': 10000, 'score': 42910.90014410019, 'total_duration': 46213.121638059616, 'accumulated_submission_time': 42910.90014410019, 'accumulated_eval_time': 3292.6324059963226, 'accumulated_logging_time': 4.411803960800171}
I0131 08:47:30.262541 139884794734336 logging_writer.py:48] [93656] accumulated_eval_time=3292.632406, accumulated_logging_time=4.411804, accumulated_submission_time=42910.900144, global_step=93656, preemption_count=0, score=42910.900144, test/accuracy=0.513400, test/loss=2.260760, test/num_examples=10000, total_duration=46213.121638, train/accuracy=0.676895, train/loss=1.435098, validation/accuracy=0.633980, validation/loss=1.631551, validation/num_examples=50000
I0131 08:47:48.115732 139884786341632 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.8323808908462524, loss=3.09063458442688
I0131 08:48:31.968649 139884794734336 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.584382176399231, loss=4.783647537231445
I0131 08:49:18.542171 139884786341632 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.8235154151916504, loss=4.109469890594482
I0131 08:50:05.176433 139884794734336 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.8370656967163086, loss=3.032273054122925
I0131 08:50:51.709386 139884786341632 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.74490225315094, loss=3.0711662769317627
I0131 08:51:38.363416 139884794734336 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.9021008014678955, loss=3.0541281700134277
I0131 08:52:25.360542 139884786341632 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.9575793743133545, loss=3.0287976264953613
I0131 08:53:12.113805 139884794734336 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.7159188985824585, loss=3.521660327911377
I0131 08:53:58.521395 139884786341632 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.159990072250366, loss=3.1138880252838135
I0131 08:54:30.570845 140085747812160 spec.py:321] Evaluating on the training split.
I0131 08:54:41.493585 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 08:55:01.609733 140085747812160 spec.py:349] Evaluating on the test split.
I0131 08:55:03.264976 140085747812160 submission_runner.py:408] Time since start: 46666.15s, 	Step: 94571, 	{'train/accuracy': 0.6860546469688416, 'train/loss': 1.4129973649978638, 'validation/accuracy': 0.6372999548912048, 'validation/loss': 1.6310029029846191, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.2514212131500244, 'test/num_examples': 10000, 'score': 43331.148206710815, 'total_duration': 46666.15451836586, 'accumulated_submission_time': 43331.148206710815, 'accumulated_eval_time': 3325.3265323638916, 'accumulated_logging_time': 4.453327894210815}
I0131 08:55:03.296489 139884794734336 logging_writer.py:48] [94571] accumulated_eval_time=3325.326532, accumulated_logging_time=4.453328, accumulated_submission_time=43331.148207, global_step=94571, preemption_count=0, score=43331.148207, test/accuracy=0.518400, test/loss=2.251421, test/num_examples=10000, total_duration=46666.154518, train/accuracy=0.686055, train/loss=1.412997, validation/accuracy=0.637300, validation/loss=1.631003, validation/num_examples=50000
I0131 08:55:15.090083 139884786341632 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.8417446613311768, loss=3.0690252780914307
I0131 08:55:58.178712 139884794734336 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5541819334030151, loss=5.039957046508789
I0131 08:56:44.584160 139884786341632 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.8007588386535645, loss=3.0764853954315186
I0131 08:57:31.597222 139884794734336 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.908128261566162, loss=3.1252100467681885
I0131 08:58:17.841419 139884786341632 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.8608348369598389, loss=3.1032803058624268
I0131 08:59:04.445298 139884794734336 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.8925775289535522, loss=2.9299087524414062
I0131 08:59:50.693891 139884786341632 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.8904430866241455, loss=3.1656243801116943
I0131 09:00:36.965095 139884794734336 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.587716817855835, loss=5.182808876037598
I0131 09:01:23.381093 139884786341632 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.6068148612976074, loss=4.542932510375977
I0131 09:02:03.488385 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:02:13.758097 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:02:33.448314 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:02:35.101079 140085747812160 submission_runner.py:408] Time since start: 47117.99s, 	Step: 95488, 	{'train/accuracy': 0.6981640458106995, 'train/loss': 1.355225682258606, 'validation/accuracy': 0.6358199715614319, 'validation/loss': 1.629407286643982, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.261467218399048, 'test/num_examples': 10000, 'score': 43751.27952218056, 'total_duration': 47117.990614652634, 'accumulated_submission_time': 43751.27952218056, 'accumulated_eval_time': 3356.9392170906067, 'accumulated_logging_time': 4.495721101760864}
I0131 09:02:35.136631 139884794734336 logging_writer.py:48] [95488] accumulated_eval_time=3356.939217, accumulated_logging_time=4.495721, accumulated_submission_time=43751.279522, global_step=95488, preemption_count=0, score=43751.279522, test/accuracy=0.515200, test/loss=2.261467, test/num_examples=10000, total_duration=47117.990615, train/accuracy=0.698164, train/loss=1.355226, validation/accuracy=0.635820, validation/loss=1.629407, validation/num_examples=50000
I0131 09:02:40.260383 139884786341632 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.9986920356750488, loss=3.7730791568756104
I0131 09:03:22.523739 139884794734336 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.9583299160003662, loss=2.9481396675109863
I0131 09:04:08.786244 139884786341632 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.9999796152114868, loss=2.9554007053375244
I0131 09:04:55.512736 139884794734336 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.060576915740967, loss=3.2475476264953613
I0131 09:05:41.908274 139884786341632 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.7211480140686035, loss=5.113945484161377
I0131 09:06:28.295562 139884794734336 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.527785301208496, loss=4.525172233581543
I0131 09:07:14.834704 139884786341632 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.8595949411392212, loss=2.986272096633911
I0131 09:08:01.166350 139884794734336 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7249630689620972, loss=3.268681287765503
I0131 09:08:47.736905 139884786341632 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.1690824031829834, loss=3.1786556243896484
I0131 09:09:34.252576 139884794734336 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.762147307395935, loss=4.996710300445557
I0131 09:09:35.342570 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:09:45.567872 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:10:07.875106 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:10:09.518657 140085747812160 submission_runner.py:408] Time since start: 47572.41s, 	Step: 96404, 	{'train/accuracy': 0.6825000047683716, 'train/loss': 1.4293886423110962, 'validation/accuracy': 0.6386799812316895, 'validation/loss': 1.6273837089538574, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2728638648986816, 'test/num_examples': 10000, 'score': 44171.423545598984, 'total_duration': 47572.40820598602, 'accumulated_submission_time': 44171.423545598984, 'accumulated_eval_time': 3391.115294933319, 'accumulated_logging_time': 4.54344367980957}
I0131 09:10:09.550283 139884786341632 logging_writer.py:48] [96404] accumulated_eval_time=3391.115295, accumulated_logging_time=4.543444, accumulated_submission_time=44171.423546, global_step=96404, preemption_count=0, score=44171.423546, test/accuracy=0.516300, test/loss=2.272864, test/num_examples=10000, total_duration=47572.408206, train/accuracy=0.682500, train/loss=1.429389, validation/accuracy=0.638680, validation/loss=1.627384, validation/num_examples=50000
I0131 09:10:48.517934 139884794734336 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.8054039478302002, loss=2.9585442543029785
I0131 09:11:34.994793 139884786341632 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.82345712184906, loss=4.3638200759887695
I0131 09:12:21.517290 139884794734336 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.027158498764038, loss=3.0424485206604004
I0131 09:13:08.149315 139884786341632 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.7485065460205078, loss=4.14119291305542
I0131 09:13:54.684730 139884794734336 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.0270137786865234, loss=3.027524709701538
I0131 09:14:41.076932 139884786341632 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.903042197227478, loss=3.1117100715637207
I0131 09:15:27.363609 139884794734336 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.6372302770614624, loss=3.923340082168579
I0131 09:16:14.025491 139884786341632 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.8048861026763916, loss=4.363289833068848
I0131 09:17:00.327380 139884794734336 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.9445271492004395, loss=2.993650436401367
I0131 09:17:09.823445 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:17:20.016633 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:17:40.793180 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:17:42.432476 140085747812160 submission_runner.py:408] Time since start: 48025.32s, 	Step: 97322, 	{'train/accuracy': 0.6897070407867432, 'train/loss': 1.3623371124267578, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.586683988571167, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2348742485046387, 'test/num_examples': 10000, 'score': 44591.63816213608, 'total_duration': 48025.32202601433, 'accumulated_submission_time': 44591.63816213608, 'accumulated_eval_time': 3423.7243151664734, 'accumulated_logging_time': 4.584112644195557}
I0131 09:17:42.463933 139884786341632 logging_writer.py:48] [97322] accumulated_eval_time=3423.724315, accumulated_logging_time=4.584113, accumulated_submission_time=44591.638162, global_step=97322, preemption_count=0, score=44591.638162, test/accuracy=0.515500, test/loss=2.234874, test/num_examples=10000, total_duration=48025.322026, train/accuracy=0.689707, train/loss=1.362337, validation/accuracy=0.640620, validation/loss=1.586684, validation/num_examples=50000
I0131 09:18:14.053419 139884794734336 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.9519928693771362, loss=2.9969913959503174
I0131 09:19:00.396521 139884786341632 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.0344884395599365, loss=3.066664934158325
I0131 09:19:47.138848 139884794734336 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.7859890460968018, loss=3.0924174785614014
I0131 09:20:33.668908 139884786341632 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.0306856632232666, loss=2.9960715770721436
I0131 09:21:19.980801 139884794734336 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.8773460388183594, loss=3.0262486934661865
I0131 09:22:06.579997 139884786341632 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.001105785369873, loss=3.0414865016937256
I0131 09:22:52.926123 139884794734336 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.972406029701233, loss=2.9145407676696777
I0131 09:23:39.311340 139884786341632 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.8661084175109863, loss=3.039564847946167
I0131 09:24:25.844332 139884794734336 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.7759201526641846, loss=4.609671115875244
I0131 09:24:42.546157 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:24:52.816132 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:25:14.531633 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:25:16.180660 140085747812160 submission_runner.py:408] Time since start: 48479.07s, 	Step: 98238, 	{'train/accuracy': 0.7014843821525574, 'train/loss': 1.3189449310302734, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.5679948329925537, 'validation/num_examples': 50000, 'test/accuracy': 0.5222000479698181, 'test/loss': 2.2077906131744385, 'test/num_examples': 10000, 'score': 45011.66071987152, 'total_duration': 48479.070209264755, 'accumulated_submission_time': 45011.66071987152, 'accumulated_eval_time': 3457.3588194847107, 'accumulated_logging_time': 4.624876022338867}
I0131 09:25:16.211002 139884786341632 logging_writer.py:48] [98238] accumulated_eval_time=3457.358819, accumulated_logging_time=4.624876, accumulated_submission_time=45011.660720, global_step=98238, preemption_count=0, score=45011.660720, test/accuracy=0.522200, test/loss=2.207791, test/num_examples=10000, total_duration=48479.070209, train/accuracy=0.701484, train/loss=1.318945, validation/accuracy=0.646400, validation/loss=1.567995, validation/num_examples=50000
I0131 09:25:40.930233 139884794734336 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.8170393705368042, loss=3.2707529067993164
I0131 09:26:26.189360 139884786341632 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.086900472640991, loss=3.074389934539795
I0131 09:27:12.823466 139884794734336 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.055217981338501, loss=2.858199119567871
I0131 09:27:59.389165 139884786341632 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.546919822692871, loss=5.085049629211426
I0131 09:28:45.798603 139884794734336 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.948410987854004, loss=2.926560640335083
I0131 09:29:32.146528 139884786341632 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.9302581548690796, loss=3.0180814266204834
I0131 09:30:18.425879 139884794734336 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.0676193237304688, loss=2.9755027294158936
I0131 09:31:04.689077 139884786341632 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.1714775562286377, loss=3.0792646408081055
I0131 09:31:51.245227 139884794734336 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.7312098741531372, loss=4.298185348510742
I0131 09:32:16.496018 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:32:26.721913 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:32:48.394943 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:32:50.030873 140085747812160 submission_runner.py:408] Time since start: 48932.92s, 	Step: 99156, 	{'train/accuracy': 0.69189453125, 'train/loss': 1.3856916427612305, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.6010757684707642, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.2255241870880127, 'test/num_examples': 10000, 'score': 45431.88610672951, 'total_duration': 48932.92041492462, 'accumulated_submission_time': 45431.88610672951, 'accumulated_eval_time': 3490.8936812877655, 'accumulated_logging_time': 4.664891719818115}
I0131 09:32:50.062808 139884786341632 logging_writer.py:48] [99156] accumulated_eval_time=3490.893681, accumulated_logging_time=4.664892, accumulated_submission_time=45431.886107, global_step=99156, preemption_count=0, score=45431.886107, test/accuracy=0.519700, test/loss=2.225524, test/num_examples=10000, total_duration=48932.920415, train/accuracy=0.691895, train/loss=1.385692, validation/accuracy=0.642140, validation/loss=1.601076, validation/num_examples=50000
I0131 09:33:07.738702 139884794734336 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.6811771392822266, loss=3.9744174480438232
I0131 09:33:51.578204 139884786341632 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.9194514751434326, loss=2.992668628692627
I0131 09:34:37.885406 139884794734336 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.7766178846359253, loss=3.6786317825317383
I0131 09:35:24.849761 139884786341632 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.758500099182129, loss=5.219749450683594
I0131 09:36:11.513281 139884794734336 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.012561559677124, loss=3.085057497024536
I0131 09:36:57.848482 139884786341632 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.9610108137130737, loss=3.3520100116729736
I0131 09:37:44.186018 139884794734336 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.9186999797821045, loss=3.142421245574951
I0131 09:38:31.011035 139884786341632 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.8717906475067139, loss=3.0248637199401855
I0131 09:39:17.625805 139884794734336 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.9749019145965576, loss=2.9936530590057373
I0131 09:39:50.457368 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:40:01.032537 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:40:20.582732 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:40:22.238447 140085747812160 submission_runner.py:408] Time since start: 49385.13s, 	Step: 100073, 	{'train/accuracy': 0.6983593702316284, 'train/loss': 1.3234851360321045, 'validation/accuracy': 0.646619975566864, 'validation/loss': 1.549912452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.527999997138977, 'test/loss': 2.1694071292877197, 'test/num_examples': 10000, 'score': 45852.21845984459, 'total_duration': 49385.12797021866, 'accumulated_submission_time': 45852.21845984459, 'accumulated_eval_time': 3522.6747205257416, 'accumulated_logging_time': 4.70860743522644}
I0131 09:40:22.276730 139884786341632 logging_writer.py:48] [100073] accumulated_eval_time=3522.674721, accumulated_logging_time=4.708607, accumulated_submission_time=45852.218460, global_step=100073, preemption_count=0, score=45852.218460, test/accuracy=0.528000, test/loss=2.169407, test/num_examples=10000, total_duration=49385.127970, train/accuracy=0.698359, train/loss=1.323485, validation/accuracy=0.646620, validation/loss=1.549912, validation/num_examples=50000
I0131 09:40:33.288804 139884794734336 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.362531900405884, loss=3.074611186981201
I0131 09:41:17.011420 139884786341632 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.655552864074707, loss=3.9543707370758057
I0131 09:42:03.700539 139884794734336 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.0812859535217285, loss=3.025646686553955
I0131 09:42:50.921093 139884786341632 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.2976036071777344, loss=3.004814386367798
I0131 09:43:37.343008 139884794734336 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.6932650804519653, loss=4.422881603240967
I0131 09:44:24.027112 139884786341632 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.7250722646713257, loss=3.803948402404785
I0131 09:45:10.713503 139884794734336 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.8400392532348633, loss=3.854417085647583
I0131 09:45:57.162533 139884786341632 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.8101884126663208, loss=3.914985418319702
I0131 09:46:43.940025 139884794734336 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.8307299613952637, loss=4.230287075042725
I0131 09:47:22.640014 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:47:33.294687 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:47:51.133258 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:47:52.784990 140085747812160 submission_runner.py:408] Time since start: 49835.67s, 	Step: 100985, 	{'train/accuracy': 0.6984570026397705, 'train/loss': 1.3088330030441284, 'validation/accuracy': 0.6455999612808228, 'validation/loss': 1.5476953983306885, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.1762855052948, 'test/num_examples': 10000, 'score': 46272.5213842392, 'total_duration': 49835.674525260925, 'accumulated_submission_time': 46272.5213842392, 'accumulated_eval_time': 3552.8196897506714, 'accumulated_logging_time': 4.7579333782196045}
I0131 09:47:52.824138 139884786341632 logging_writer.py:48] [100985] accumulated_eval_time=3552.819690, accumulated_logging_time=4.757933, accumulated_submission_time=46272.521384, global_step=100985, preemption_count=0, score=46272.521384, test/accuracy=0.523200, test/loss=2.176286, test/num_examples=10000, total_duration=49835.674525, train/accuracy=0.698457, train/loss=1.308833, validation/accuracy=0.645600, validation/loss=1.547695, validation/num_examples=50000
I0131 09:47:59.168632 139884794734336 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.076791763305664, loss=2.971705436706543
I0131 09:48:41.804922 139884786341632 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.047081708908081, loss=2.990819215774536
I0131 09:49:28.442625 139884794734336 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.2245006561279297, loss=2.907214641571045
I0131 09:50:15.131426 139884786341632 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.8961659669876099, loss=3.2783825397491455
I0131 09:51:01.659637 139884794734336 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.0966765880584717, loss=2.909472942352295
I0131 09:51:48.093944 139884786341632 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.972093939781189, loss=3.2010409832000732
I0131 09:52:34.601420 139884794734336 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.263226270675659, loss=3.2558236122131348
I0131 09:53:21.141880 139884786341632 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.0615596771240234, loss=3.558459997177124
I0131 09:54:07.544268 139884794734336 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.9388481378555298, loss=5.189230918884277
I0131 09:54:53.074306 140085747812160 spec.py:321] Evaluating on the training split.
I0131 09:55:03.931916 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 09:55:22.972566 140085747812160 spec.py:349] Evaluating on the test split.
I0131 09:55:24.627822 140085747812160 submission_runner.py:408] Time since start: 50287.52s, 	Step: 101900, 	{'train/accuracy': 0.7024999856948853, 'train/loss': 1.3207366466522217, 'validation/accuracy': 0.6457799673080444, 'validation/loss': 1.5591189861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.5252000093460083, 'test/loss': 2.188363552093506, 'test/num_examples': 10000, 'score': 46692.711477041245, 'total_duration': 50287.51734471321, 'accumulated_submission_time': 46692.711477041245, 'accumulated_eval_time': 3584.373185634613, 'accumulated_logging_time': 4.80739164352417}
I0131 09:55:24.666008 139884786341632 logging_writer.py:48] [101900] accumulated_eval_time=3584.373186, accumulated_logging_time=4.807392, accumulated_submission_time=46692.711477, global_step=101900, preemption_count=0, score=46692.711477, test/accuracy=0.525200, test/loss=2.188364, test/num_examples=10000, total_duration=50287.517345, train/accuracy=0.702500, train/loss=1.320737, validation/accuracy=0.645780, validation/loss=1.559119, validation/num_examples=50000
I0131 09:55:25.081261 139884794734336 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.6751112937927246, loss=4.834204196929932
I0131 09:56:06.134118 139884786341632 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.7967345714569092, loss=3.15816330909729
I0131 09:56:52.387181 139884794734336 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.997041940689087, loss=3.0159101486206055
I0131 09:57:38.704581 139884786341632 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.096005916595459, loss=3.092787027359009
I0131 09:58:25.274161 139884794734336 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.1053898334503174, loss=3.0729384422302246
I0131 09:59:11.651801 139884786341632 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.2001147270202637, loss=2.8733670711517334
I0131 09:59:58.190499 139884794734336 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.900564193725586, loss=4.967715740203857
I0131 10:00:44.769064 139884786341632 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.107999324798584, loss=2.9626197814941406
I0131 10:01:31.078476 139884794734336 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.630943775177002, loss=4.297976970672607
I0131 10:02:17.452594 139884786341632 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.6221444606781006, loss=5.048338413238525
I0131 10:02:25.035506 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:02:35.424298 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:02:55.960394 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:02:57.602884 140085747812160 submission_runner.py:408] Time since start: 50740.49s, 	Step: 102818, 	{'train/accuracy': 0.6942577958106995, 'train/loss': 1.4096049070358276, 'validation/accuracy': 0.644599974155426, 'validation/loss': 1.627078652381897, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.2475533485412598, 'test/num_examples': 10000, 'score': 47113.01589202881, 'total_duration': 50740.492428302765, 'accumulated_submission_time': 47113.01589202881, 'accumulated_eval_time': 3616.9405534267426, 'accumulated_logging_time': 4.861057758331299}
I0131 10:02:57.634628 139884794734336 logging_writer.py:48] [102818] accumulated_eval_time=3616.940553, accumulated_logging_time=4.861058, accumulated_submission_time=47113.015892, global_step=102818, preemption_count=0, score=47113.015892, test/accuracy=0.524700, test/loss=2.247553, test/num_examples=10000, total_duration=50740.492428, train/accuracy=0.694258, train/loss=1.409605, validation/accuracy=0.644600, validation/loss=1.627079, validation/num_examples=50000
I0131 10:03:30.276769 139884786341632 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.6630250215530396, loss=3.7120156288146973
I0131 10:04:16.827803 139884794734336 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.8683300018310547, loss=5.082226276397705
I0131 10:05:03.544929 139884786341632 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.938393473625183, loss=3.0041146278381348
I0131 10:05:49.749303 139884794734336 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.6865897178649902, loss=3.7613584995269775
I0131 10:06:36.383358 139884786341632 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.9084559679031372, loss=2.849496841430664
I0131 10:07:22.983558 139884794734336 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.2871177196502686, loss=2.898345947265625
I0131 10:08:09.468980 139884786341632 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.190131902694702, loss=3.017556667327881
I0131 10:08:55.881958 139884794734336 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.7339725494384766, loss=3.996422529220581
I0131 10:09:42.152687 139884786341632 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.972417950630188, loss=2.9104888439178467
I0131 10:09:57.712762 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:10:08.146728 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:10:29.331377 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:10:30.970301 140085747812160 submission_runner.py:408] Time since start: 51193.86s, 	Step: 103735, 	{'train/accuracy': 0.7060937285423279, 'train/loss': 1.303206443786621, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5560694932937622, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.18750262260437, 'test/num_examples': 10000, 'score': 47533.03374886513, 'total_duration': 51193.859845638275, 'accumulated_submission_time': 47533.03374886513, 'accumulated_eval_time': 3650.1980924606323, 'accumulated_logging_time': 4.902195453643799}
I0131 10:10:31.003723 139884794734336 logging_writer.py:48] [103735] accumulated_eval_time=3650.198092, accumulated_logging_time=4.902195, accumulated_submission_time=47533.033749, global_step=103735, preemption_count=0, score=47533.033749, test/accuracy=0.529600, test/loss=2.187503, test/num_examples=10000, total_duration=51193.859846, train/accuracy=0.706094, train/loss=1.303206, validation/accuracy=0.648180, validation/loss=1.556069, validation/num_examples=50000
I0131 10:10:56.913773 139884786341632 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.0900635719299316, loss=2.959176778793335
I0131 10:11:42.046468 139884794734336 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.2538421154022217, loss=3.0173885822296143
I0131 10:12:28.713432 139884786341632 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.7368334531784058, loss=4.687133312225342
I0131 10:13:15.552937 139884794734336 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.3101704120635986, loss=3.000988006591797
I0131 10:14:02.067761 139884786341632 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.9050711393356323, loss=3.1550538539886475
I0131 10:14:48.634141 139884794734336 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.251330614089966, loss=2.9437553882598877
I0131 10:15:34.922920 139884786341632 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.8698698282241821, loss=4.692953109741211
I0131 10:16:21.392040 139884794734336 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.1991803646087646, loss=3.033694267272949
I0131 10:17:07.874898 139884786341632 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.9195679426193237, loss=3.16054105758667
I0131 10:17:31.130727 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:17:41.520211 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:18:02.351941 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:18:04.003641 140085747812160 submission_runner.py:408] Time since start: 51646.89s, 	Step: 104652, 	{'train/accuracy': 0.728710949420929, 'train/loss': 1.2095915079116821, 'validation/accuracy': 0.655519962310791, 'validation/loss': 1.5235939025878906, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1618435382843018, 'test/num_examples': 10000, 'score': 47953.09908533096, 'total_duration': 51646.89317679405, 'accumulated_submission_time': 47953.09908533096, 'accumulated_eval_time': 3683.071009159088, 'accumulated_logging_time': 4.946327209472656}
I0131 10:18:04.042660 139884794734336 logging_writer.py:48] [104652] accumulated_eval_time=3683.071009, accumulated_logging_time=4.946327, accumulated_submission_time=47953.099085, global_step=104652, preemption_count=0, score=47953.099085, test/accuracy=0.522900, test/loss=2.161844, test/num_examples=10000, total_duration=51646.893177, train/accuracy=0.728711, train/loss=1.209592, validation/accuracy=0.655520, validation/loss=1.523594, validation/num_examples=50000
I0131 10:18:23.277480 139884786341632 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.187931537628174, loss=2.930310010910034
I0131 10:19:07.943892 139884794734336 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.9927905797958374, loss=3.5691661834716797
I0131 10:19:54.445963 139884786341632 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.3323514461517334, loss=2.971721887588501
I0131 10:20:41.406996 139884794734336 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.1553633213043213, loss=2.929377555847168
I0131 10:21:27.854356 139884786341632 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.9875802993774414, loss=3.0078999996185303
I0131 10:22:14.363132 139884794734336 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.7439221143722534, loss=4.728068828582764
I0131 10:23:00.791558 139884786341632 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.9703729152679443, loss=3.27846360206604
I0131 10:23:46.855412 139884794734336 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.9635084867477417, loss=3.3984529972076416
I0131 10:24:33.292229 139884786341632 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.9142813682556152, loss=2.9143102169036865
I0131 10:25:04.379232 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:25:14.602451 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:25:35.364862 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:25:36.999913 140085747812160 submission_runner.py:408] Time since start: 52099.89s, 	Step: 105569, 	{'train/accuracy': 0.7003515362739563, 'train/loss': 1.328979730606079, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.5396244525909424, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.1648764610290527, 'test/num_examples': 10000, 'score': 48373.37495970726, 'total_duration': 52099.889456510544, 'accumulated_submission_time': 48373.37495970726, 'accumulated_eval_time': 3715.691724061966, 'accumulated_logging_time': 4.996261835098267}
I0131 10:25:37.032229 139884794734336 logging_writer.py:48] [105569] accumulated_eval_time=3715.691724, accumulated_logging_time=4.996262, accumulated_submission_time=48373.374960, global_step=105569, preemption_count=0, score=48373.374960, test/accuracy=0.535800, test/loss=2.164876, test/num_examples=10000, total_duration=52099.889457, train/accuracy=0.700352, train/loss=1.328980, validation/accuracy=0.655080, validation/loss=1.539624, validation/num_examples=50000
I0131 10:25:49.603919 139884786341632 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.1901566982269287, loss=2.952439069747925
I0131 10:26:32.930965 139884794734336 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.6395128965377808, loss=3.945556163787842
I0131 10:27:19.451537 139884786341632 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.144050359725952, loss=3.074467897415161
I0131 10:28:06.037399 139884794734336 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.7045063972473145, loss=4.429016590118408
I0131 10:28:52.088048 139884786341632 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.787519097328186, loss=4.53663444519043
I0131 10:29:38.426319 139884794734336 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.081542491912842, loss=3.0358476638793945
I0131 10:30:25.088793 139884786341632 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.8777717351913452, loss=3.1037757396698
I0131 10:31:11.637843 139884794734336 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.071390390396118, loss=5.1195597648620605
I0131 10:31:58.241825 139884786341632 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.0473058223724365, loss=3.0201308727264404
I0131 10:32:37.173179 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:32:47.584434 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:33:09.267480 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:33:10.906438 140085747812160 submission_runner.py:408] Time since start: 52553.80s, 	Step: 106485, 	{'train/accuracy': 0.706250011920929, 'train/loss': 1.3207530975341797, 'validation/accuracy': 0.6521199941635132, 'validation/loss': 1.5590405464172363, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.192826986312866, 'test/num_examples': 10000, 'score': 48793.45449113846, 'total_duration': 52553.79598546028, 'accumulated_submission_time': 48793.45449113846, 'accumulated_eval_time': 3749.4249980449677, 'accumulated_logging_time': 5.040008306503296}
I0131 10:33:10.938749 139884794734336 logging_writer.py:48] [106485] accumulated_eval_time=3749.424998, accumulated_logging_time=5.040008, accumulated_submission_time=48793.454491, global_step=106485, preemption_count=0, score=48793.454491, test/accuracy=0.531400, test/loss=2.192827, test/num_examples=10000, total_duration=52553.795985, train/accuracy=0.706250, train/loss=1.320753, validation/accuracy=0.652120, validation/loss=1.559041, validation/num_examples=50000
I0131 10:33:17.228017 139884786341632 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.053234100341797, loss=3.3443069458007812
I0131 10:33:59.220607 139884794734336 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.0658395290374756, loss=2.9999475479125977
I0131 10:34:45.589941 139884786341632 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.136718273162842, loss=2.8902859687805176
I0131 10:35:32.477358 139884794734336 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.2208573818206787, loss=3.104583263397217
I0131 10:36:18.865353 139884786341632 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.9528918266296387, loss=3.772900104522705
I0131 10:37:05.724753 139884794734336 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.870642900466919, loss=3.2965030670166016
I0131 10:37:51.996559 139884786341632 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.2028424739837646, loss=3.0041615962982178
I0131 10:38:38.501474 139884794734336 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.1066577434539795, loss=2.9137825965881348
I0131 10:39:24.999650 139884786341632 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.033423662185669, loss=3.0280935764312744
I0131 10:40:11.536850 139884794734336 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.0043084621429443, loss=3.2338500022888184
I0131 10:40:11.550595 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:40:22.039836 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:40:43.020832 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:40:44.669123 140085747812160 submission_runner.py:408] Time since start: 53007.56s, 	Step: 107401, 	{'train/accuracy': 0.7234960794448853, 'train/loss': 1.20840322971344, 'validation/accuracy': 0.6581199765205383, 'validation/loss': 1.5009328126907349, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.1324949264526367, 'test/num_examples': 10000, 'score': 49214.00630617142, 'total_duration': 53007.55867099762, 'accumulated_submission_time': 49214.00630617142, 'accumulated_eval_time': 3782.5435230731964, 'accumulated_logging_time': 5.082547903060913}
I0131 10:40:44.699794 139884786341632 logging_writer.py:48] [107401] accumulated_eval_time=3782.543523, accumulated_logging_time=5.082548, accumulated_submission_time=49214.006306, global_step=107401, preemption_count=0, score=49214.006306, test/accuracy=0.539200, test/loss=2.132495, test/num_examples=10000, total_duration=53007.558671, train/accuracy=0.723496, train/loss=1.208403, validation/accuracy=0.658120, validation/loss=1.500933, validation/num_examples=50000
I0131 10:41:25.624503 139884794734336 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.080207347869873, loss=2.9620778560638428
I0131 10:42:12.024670 139884786341632 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.0751559734344482, loss=2.8674917221069336
I0131 10:42:58.963673 139884794734336 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.071573495864868, loss=3.089616298675537
I0131 10:43:45.423977 139884786341632 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.3301026821136475, loss=2.9648919105529785
I0131 10:44:31.942064 139884794734336 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.8586771488189697, loss=3.949773073196411
I0131 10:45:18.549712 139884786341632 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.1983437538146973, loss=2.9673235416412354
I0131 10:46:05.078066 139884794734336 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.9227442741394043, loss=3.387037754058838
I0131 10:46:51.778100 139884786341632 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.121032238006592, loss=5.1079936027526855
I0131 10:47:38.378935 139884794734336 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.29241681098938, loss=2.916151762008667
I0131 10:47:45.004355 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:47:55.570904 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:48:14.857406 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:48:16.512059 140085747812160 submission_runner.py:408] Time since start: 53459.40s, 	Step: 108316, 	{'train/accuracy': 0.7032812237739563, 'train/loss': 1.2869420051574707, 'validation/accuracy': 0.6589800119400024, 'validation/loss': 1.5009398460388184, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.155637502670288, 'test/num_examples': 10000, 'score': 49634.25281214714, 'total_duration': 53459.401591300964, 'accumulated_submission_time': 49634.25281214714, 'accumulated_eval_time': 3814.0512182712555, 'accumulated_logging_time': 5.121634244918823}
I0131 10:48:16.549965 139884786341632 logging_writer.py:48] [108316] accumulated_eval_time=3814.051218, accumulated_logging_time=5.121634, accumulated_submission_time=49634.252812, global_step=108316, preemption_count=0, score=49634.252812, test/accuracy=0.530300, test/loss=2.155638, test/num_examples=10000, total_duration=53459.401591, train/accuracy=0.703281, train/loss=1.286942, validation/accuracy=0.658980, validation/loss=1.500940, validation/num_examples=50000
I0131 10:48:51.323084 139884794734336 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.061030387878418, loss=2.912060260772705
I0131 10:49:37.411131 139884786341632 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.9721816778182983, loss=4.874876022338867
I0131 10:50:24.168991 139884794734336 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.8673982620239258, loss=3.978224277496338
I0131 10:51:10.709149 139884786341632 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.0431973934173584, loss=2.9024245738983154
I0131 10:51:57.266202 139884794734336 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.2224016189575195, loss=2.927854061126709
I0131 10:52:43.997838 139884786341632 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.193699359893799, loss=2.9998040199279785
I0131 10:53:30.394211 139884794734336 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.9904273748397827, loss=3.0862786769866943
I0131 10:54:16.669263 139884786341632 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.110548734664917, loss=3.025109052658081
I0131 10:55:02.899562 139884794734336 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.1295359134674072, loss=3.2262072563171387
I0131 10:55:16.889615 140085747812160 spec.py:321] Evaluating on the training split.
I0131 10:55:27.065334 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 10:55:46.460518 140085747812160 spec.py:349] Evaluating on the test split.
I0131 10:55:48.117162 140085747812160 submission_runner.py:408] Time since start: 53911.01s, 	Step: 109232, 	{'train/accuracy': 0.7122656106948853, 'train/loss': 1.288287878036499, 'validation/accuracy': 0.657759964466095, 'validation/loss': 1.5199744701385498, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.154524326324463, 'test/num_examples': 10000, 'score': 50054.5297896862, 'total_duration': 53911.006695985794, 'accumulated_submission_time': 50054.5297896862, 'accumulated_eval_time': 3845.278760910034, 'accumulated_logging_time': 5.172155141830444}
I0131 10:55:48.157064 139884786341632 logging_writer.py:48] [109232] accumulated_eval_time=3845.278761, accumulated_logging_time=5.172155, accumulated_submission_time=50054.529790, global_step=109232, preemption_count=0, score=50054.529790, test/accuracy=0.532900, test/loss=2.154524, test/num_examples=10000, total_duration=53911.006696, train/accuracy=0.712266, train/loss=1.288288, validation/accuracy=0.657760, validation/loss=1.519974, validation/num_examples=50000
I0131 10:56:15.259111 139884794734336 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.9289228916168213, loss=3.793527603149414
I0131 10:57:01.315649 139884786341632 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.4889156818389893, loss=2.8948309421539307
I0131 10:57:47.874537 139884794734336 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.006197690963745, loss=4.011873245239258
I0131 10:58:34.575762 139884786341632 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.0361788272857666, loss=2.9438376426696777
I0131 10:59:20.774003 139884794734336 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.8903506994247437, loss=3.605900526046753
I0131 11:00:07.315867 139884786341632 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.1723639965057373, loss=2.8680896759033203
I0131 11:00:53.572500 139884794734336 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.1708855628967285, loss=2.947707176208496
I0131 11:01:40.277929 139884786341632 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.88075852394104, loss=3.471553087234497
I0131 11:02:26.733451 139884794734336 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.0700490474700928, loss=4.610359191894531
I0131 11:02:48.200333 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:02:58.707167 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:03:19.552229 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:03:21.199196 140085747812160 submission_runner.py:408] Time since start: 54364.09s, 	Step: 110148, 	{'train/accuracy': 0.7259570360183716, 'train/loss': 1.2455788850784302, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.5217573642730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5366000533103943, 'test/loss': 2.149949550628662, 'test/num_examples': 10000, 'score': 50474.51218295097, 'total_duration': 54364.08873963356, 'accumulated_submission_time': 50474.51218295097, 'accumulated_eval_time': 3878.277625799179, 'accumulated_logging_time': 5.222049713134766}
I0131 11:03:21.230803 139884786341632 logging_writer.py:48] [110148] accumulated_eval_time=3878.277626, accumulated_logging_time=5.222050, accumulated_submission_time=50474.512183, global_step=110148, preemption_count=0, score=50474.512183, test/accuracy=0.536600, test/loss=2.149950, test/num_examples=10000, total_duration=54364.088740, train/accuracy=0.725957, train/loss=1.245579, validation/accuracy=0.663520, validation/loss=1.521757, validation/num_examples=50000
I0131 11:03:42.031602 139884794734336 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.106966495513916, loss=2.9128165245056152
I0131 11:04:26.521235 139884786341632 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.8469425439834595, loss=4.674434185028076
I0131 11:05:13.041204 139884794734336 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.9595946073532104, loss=4.697834014892578
I0131 11:05:59.540575 139884786341632 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.8046205043792725, loss=4.3608317375183105
I0131 11:06:45.877559 139884794734336 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.2280516624450684, loss=2.7874372005462646
I0131 11:07:32.310935 139884786341632 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.143301248550415, loss=2.88688063621521
I0131 11:08:18.726663 139884794734336 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.992239236831665, loss=2.721101999282837
I0131 11:09:05.202902 139884786341632 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.005216360092163, loss=3.1248552799224854
I0131 11:09:51.834451 139884794734336 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.062770128250122, loss=2.972360610961914
I0131 11:10:21.284756 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:10:32.070995 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:10:52.635332 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:10:54.313717 140085747812160 submission_runner.py:408] Time since start: 54817.20s, 	Step: 111065, 	{'train/accuracy': 0.71337890625, 'train/loss': 1.262039303779602, 'validation/accuracy': 0.6626600027084351, 'validation/loss': 1.4916067123413086, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.113887310028076, 'test/num_examples': 10000, 'score': 50894.50727891922, 'total_duration': 54817.203261852264, 'accumulated_submission_time': 50894.50727891922, 'accumulated_eval_time': 3911.3065781593323, 'accumulated_logging_time': 5.262262582778931}
I0131 11:10:54.347648 139884786341632 logging_writer.py:48] [111065] accumulated_eval_time=3911.306578, accumulated_logging_time=5.262263, accumulated_submission_time=50894.507279, global_step=111065, preemption_count=0, score=50894.507279, test/accuracy=0.542100, test/loss=2.113887, test/num_examples=10000, total_duration=54817.203262, train/accuracy=0.713379, train/loss=1.262039, validation/accuracy=0.662660, validation/loss=1.491607, validation/num_examples=50000
I0131 11:11:08.475799 139884794734336 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.1299142837524414, loss=5.071566104888916
I0131 11:11:51.772218 139884786341632 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.9167181253433228, loss=4.982994556427002
I0131 11:12:38.535168 139884794734336 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.1851108074188232, loss=3.321030616760254
I0131 11:13:25.600421 139884786341632 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.9349355697631836, loss=3.6304500102996826
I0131 11:14:11.815188 139884794734336 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.125357151031494, loss=2.858867645263672
I0131 11:14:58.324013 139884786341632 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.0781376361846924, loss=3.937958002090454
I0131 11:15:45.584160 139884794734336 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.07275652885437, loss=3.1051201820373535
I0131 11:16:31.803657 139884786341632 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.8935729265213013, loss=5.081393241882324
I0131 11:17:18.382516 139884794734336 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.8481709957122803, loss=3.898322343826294
I0131 11:17:54.540943 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:18:05.109993 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:18:25.535900 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:18:27.168185 140085747812160 submission_runner.py:408] Time since start: 55270.06s, 	Step: 111980, 	{'train/accuracy': 0.7187108993530273, 'train/loss': 1.2647498846054077, 'validation/accuracy': 0.6654999852180481, 'validation/loss': 1.5069676637649536, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.1428349018096924, 'test/num_examples': 10000, 'score': 51314.64157676697, 'total_duration': 55270.057732343674, 'accumulated_submission_time': 51314.64157676697, 'accumulated_eval_time': 3943.933856487274, 'accumulated_logging_time': 5.305820465087891}
I0131 11:18:27.202848 139884786341632 logging_writer.py:48] [111980] accumulated_eval_time=3943.933856, accumulated_logging_time=5.305820, accumulated_submission_time=51314.641577, global_step=111980, preemption_count=0, score=51314.641577, test/accuracy=0.540900, test/loss=2.142835, test/num_examples=10000, total_duration=55270.057732, train/accuracy=0.718711, train/loss=1.264750, validation/accuracy=0.665500, validation/loss=1.506968, validation/num_examples=50000
I0131 11:18:35.448637 139884794734336 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.234992504119873, loss=2.9530112743377686
I0131 11:19:17.987893 139884786341632 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.061910629272461, loss=4.083521842956543
I0131 11:20:04.514165 139884794734336 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.9183807373046875, loss=4.9369425773620605
I0131 11:20:50.752883 139884786341632 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.9748597145080566, loss=3.519972801208496
I0131 11:21:36.938680 139884794734336 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.271259069442749, loss=3.077512264251709
I0131 11:22:23.462337 139884786341632 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.079606771469116, loss=2.9087724685668945
I0131 11:23:10.351444 139884794734336 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.9809869527816772, loss=4.009690284729004
I0131 11:23:56.802482 139884786341632 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.142714500427246, loss=2.8842170238494873
I0131 11:24:43.505579 139884794734336 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.8095569610595703, loss=4.223574638366699
I0131 11:25:27.206163 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:25:37.379040 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:25:57.914200 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:25:59.548368 140085747812160 submission_runner.py:408] Time since start: 55722.44s, 	Step: 112896, 	{'train/accuracy': 0.7238085865974426, 'train/loss': 1.2229537963867188, 'validation/accuracy': 0.6610999703407288, 'validation/loss': 1.4882365465164185, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1151599884033203, 'test/num_examples': 10000, 'score': 51734.58570885658, 'total_duration': 55722.437901735306, 'accumulated_submission_time': 51734.58570885658, 'accumulated_eval_time': 3976.276055574417, 'accumulated_logging_time': 5.350057363510132}
I0131 11:25:59.581999 139884786341632 logging_writer.py:48] [112896] accumulated_eval_time=3976.276056, accumulated_logging_time=5.350057, accumulated_submission_time=51734.585709, global_step=112896, preemption_count=0, score=51734.585709, test/accuracy=0.539800, test/loss=2.115160, test/num_examples=10000, total_duration=55722.437902, train/accuracy=0.723809, train/loss=1.222954, validation/accuracy=0.661100, validation/loss=1.488237, validation/num_examples=50000
I0131 11:26:01.554561 139884794734336 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.256399393081665, loss=2.9623563289642334
I0131 11:26:42.563075 139884786341632 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.882777452468872, loss=4.601624488830566
I0131 11:27:29.144167 139884794734336 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.264082670211792, loss=2.9430856704711914
I0131 11:28:16.012462 139884786341632 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.077359914779663, loss=3.9865801334381104
I0131 11:29:02.309592 139884794734336 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.3478925228118896, loss=2.882762908935547
I0131 11:29:48.974743 139884786341632 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.054931163787842, loss=2.843644618988037
I0131 11:30:35.472601 139884794734336 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.097264289855957, loss=3.0547330379486084
I0131 11:31:21.737576 139884786341632 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.3660924434661865, loss=2.8628416061401367
I0131 11:32:08.606603 139884794734336 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.1504290103912354, loss=2.849453926086426
I0131 11:32:55.307783 139884786341632 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.36240553855896, loss=2.90397310256958
I0131 11:32:59.718764 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:33:10.381768 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:33:28.384660 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:33:30.033612 140085747812160 submission_runner.py:408] Time since start: 56172.92s, 	Step: 113811, 	{'train/accuracy': 0.7215625047683716, 'train/loss': 1.2210685014724731, 'validation/accuracy': 0.6649199724197388, 'validation/loss': 1.4647303819656372, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.0943243503570557, 'test/num_examples': 10000, 'score': 52154.663420677185, 'total_duration': 56172.92315030098, 'accumulated_submission_time': 52154.663420677185, 'accumulated_eval_time': 4006.590903520584, 'accumulated_logging_time': 5.393388271331787}
I0131 11:33:30.073740 139884794734336 logging_writer.py:48] [113811] accumulated_eval_time=4006.590904, accumulated_logging_time=5.393388, accumulated_submission_time=52154.663421, global_step=113811, preemption_count=0, score=52154.663421, test/accuracy=0.545100, test/loss=2.094324, test/num_examples=10000, total_duration=56172.923150, train/accuracy=0.721563, train/loss=1.221069, validation/accuracy=0.664920, validation/loss=1.464730, validation/num_examples=50000
I0131 11:34:07.123344 139884786341632 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.0478568077087402, loss=4.534925937652588
I0131 11:34:53.507432 139884794734336 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.0164589881896973, loss=4.673222064971924
I0131 11:35:40.155326 139884786341632 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.1895322799682617, loss=2.9139108657836914
I0131 11:36:26.757005 139884794734336 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.1348490715026855, loss=3.576498031616211
I0131 11:37:13.167027 139884786341632 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.1590824127197266, loss=4.287754058837891
I0131 11:37:59.746102 139884794734336 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.298328399658203, loss=2.943358898162842
I0131 11:38:46.096038 139884786341632 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.1373589038848877, loss=3.409449338912964
I0131 11:39:32.814064 139884794734336 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.277879238128662, loss=2.9109673500061035
I0131 11:40:19.278247 139884786341632 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.423879384994507, loss=2.9370310306549072
I0131 11:40:30.482530 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:40:41.188105 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:41:00.513341 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:41:02.164790 140085747812160 submission_runner.py:408] Time since start: 56625.05s, 	Step: 114726, 	{'train/accuracy': 0.7206835746765137, 'train/loss': 1.2099860906600952, 'validation/accuracy': 0.67249995470047, 'validation/loss': 1.443393349647522, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.0834412574768066, 'test/num_examples': 10000, 'score': 52575.012565374374, 'total_duration': 56625.05432701111, 'accumulated_submission_time': 52575.012565374374, 'accumulated_eval_time': 4038.273143053055, 'accumulated_logging_time': 5.442919015884399}
I0131 11:41:02.200778 139884794734336 logging_writer.py:48] [114726] accumulated_eval_time=4038.273143, accumulated_logging_time=5.442919, accumulated_submission_time=52575.012565, global_step=114726, preemption_count=0, score=52575.012565, test/accuracy=0.545000, test/loss=2.083441, test/num_examples=10000, total_duration=56625.054327, train/accuracy=0.720684, train/loss=1.209986, validation/accuracy=0.672500, validation/loss=1.443393, validation/num_examples=50000
I0131 11:41:31.643254 139884786341632 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.0930631160736084, loss=4.1191840171813965
I0131 11:42:18.022974 139884794734336 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.1958649158477783, loss=2.812077283859253
I0131 11:43:05.063048 139884786341632 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.0710346698760986, loss=4.765214443206787
I0131 11:43:51.806855 139884794734336 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.1757872104644775, loss=2.868863344192505
I0131 11:44:38.305325 139884786341632 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.2525548934936523, loss=2.948430061340332
I0131 11:45:24.863953 139884794734336 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.3679990768432617, loss=2.7923624515533447
I0131 11:46:11.342962 139884786341632 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.2700326442718506, loss=3.0599188804626465
I0131 11:46:57.996901 139884794734336 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.04642653465271, loss=3.493018865585327
I0131 11:47:44.508121 139884786341632 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.064846992492676, loss=4.390418529510498
I0131 11:48:02.294068 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:48:12.476480 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:48:35.228419 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:48:36.876933 140085747812160 submission_runner.py:408] Time since start: 57079.77s, 	Step: 115640, 	{'train/accuracy': 0.7305468320846558, 'train/loss': 1.191508173942566, 'validation/accuracy': 0.67249995470047, 'validation/loss': 1.4450451135635376, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.064040422439575, 'test/num_examples': 10000, 'score': 52995.04573082924, 'total_duration': 57079.766466617584, 'accumulated_submission_time': 52995.04573082924, 'accumulated_eval_time': 4072.8560173511505, 'accumulated_logging_time': 5.4888222217559814}
I0131 11:48:36.912083 139884794734336 logging_writer.py:48] [115640] accumulated_eval_time=4072.856017, accumulated_logging_time=5.488822, accumulated_submission_time=52995.045731, global_step=115640, preemption_count=0, score=52995.045731, test/accuracy=0.549600, test/loss=2.064040, test/num_examples=10000, total_duration=57079.766467, train/accuracy=0.730547, train/loss=1.191508, validation/accuracy=0.672500, validation/loss=1.445045, validation/num_examples=50000
I0131 11:49:00.856049 139884786341632 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.42452073097229, loss=2.8504951000213623
I0131 11:49:46.068030 139884794734336 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.9932692050933838, loss=3.2648675441741943
I0131 11:50:32.810773 139884786341632 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.6393725872039795, loss=2.77323055267334
I0131 11:51:19.525071 139884794734336 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.1175785064697266, loss=2.781050443649292
I0131 11:52:05.800485 139884786341632 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.176388740539551, loss=2.8192992210388184
I0131 11:52:52.789354 139884794734336 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.1472508907318115, loss=2.982880115509033
I0131 11:53:38.925007 139884786341632 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.1989283561706543, loss=2.7547435760498047
I0131 11:54:25.712460 139884794734336 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.2859909534454346, loss=2.8198630809783936
I0131 11:55:12.090552 139884786341632 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.3276865482330322, loss=3.0411739349365234
I0131 11:55:37.285241 140085747812160 spec.py:321] Evaluating on the training split.
I0131 11:55:47.733568 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 11:56:09.133810 140085747812160 spec.py:349] Evaluating on the test split.
I0131 11:56:10.761884 140085747812160 submission_runner.py:408] Time since start: 57533.65s, 	Step: 116556, 	{'train/accuracy': 0.7452148199081421, 'train/loss': 1.1201274394989014, 'validation/accuracy': 0.6720199584960938, 'validation/loss': 1.4388738870620728, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 2.0630745887756348, 'test/num_examples': 10000, 'score': 53415.35868215561, 'total_duration': 57533.651420116425, 'accumulated_submission_time': 53415.35868215561, 'accumulated_eval_time': 4106.33264541626, 'accumulated_logging_time': 5.534053087234497}
I0131 11:56:10.795588 139884794734336 logging_writer.py:48] [116556] accumulated_eval_time=4106.332645, accumulated_logging_time=5.534053, accumulated_submission_time=53415.358682, global_step=116556, preemption_count=0, score=53415.358682, test/accuracy=0.549800, test/loss=2.063075, test/num_examples=10000, total_duration=57533.651420, train/accuracy=0.745215, train/loss=1.120127, validation/accuracy=0.672020, validation/loss=1.438874, validation/num_examples=50000
I0131 11:56:28.458548 139884786341632 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.057551383972168, loss=2.964064359664917
I0131 11:57:12.628317 139884794734336 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.1191534996032715, loss=3.2174713611602783
I0131 11:57:59.136126 139884786341632 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.410900592803955, loss=2.889019012451172
I0131 11:58:45.750479 139884794734336 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.238450527191162, loss=2.953441619873047
I0131 11:59:32.386185 139884786341632 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.0479416847229004, loss=4.464362144470215
I0131 12:00:18.849657 139884794734336 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.085236072540283, loss=2.794640064239502
I0131 12:01:05.242275 139884786341632 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.3040075302124023, loss=3.0136892795562744
I0131 12:01:51.657637 139884794734336 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.207096576690674, loss=2.836437702178955
I0131 12:02:38.340958 139884786341632 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.9881364107131958, loss=3.4483392238616943
I0131 12:03:10.907719 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:03:21.158800 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:03:42.611019 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:03:44.249720 140085747812160 submission_runner.py:408] Time since start: 57987.14s, 	Step: 117472, 	{'train/accuracy': 0.7250390648841858, 'train/loss': 1.2660032510757446, 'validation/accuracy': 0.667199969291687, 'validation/loss': 1.5061935186386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.128594160079956, 'test/num_examples': 10000, 'score': 53835.411581754684, 'total_duration': 57987.13926744461, 'accumulated_submission_time': 53835.411581754684, 'accumulated_eval_time': 4139.674651861191, 'accumulated_logging_time': 5.577041864395142}
I0131 12:03:44.283147 139884794734336 logging_writer.py:48] [117472] accumulated_eval_time=4139.674652, accumulated_logging_time=5.577042, accumulated_submission_time=53835.411582, global_step=117472, preemption_count=0, score=53835.411582, test/accuracy=0.549600, test/loss=2.128594, test/num_examples=10000, total_duration=57987.139267, train/accuracy=0.725039, train/loss=1.266003, validation/accuracy=0.667200, validation/loss=1.506194, validation/num_examples=50000
I0131 12:03:55.668996 139884786341632 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.234914779663086, loss=4.597685813903809
I0131 12:04:38.602805 139884794734336 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.342607021331787, loss=2.8930768966674805
I0131 12:05:25.052904 139884786341632 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.3029792308807373, loss=4.899945259094238
I0131 12:06:11.967405 139884794734336 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.6155924797058105, loss=2.7843117713928223
I0131 12:06:58.521141 139884786341632 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.4724948406219482, loss=3.1154556274414062
I0131 12:07:45.025586 139884794734336 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.3836703300476074, loss=2.8482818603515625
I0131 12:08:31.757280 139884786341632 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.4261484146118164, loss=4.9447021484375
I0131 12:09:18.403057 139884794734336 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.107433557510376, loss=4.285711765289307
I0131 12:10:05.272558 139884786341632 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.0506021976470947, loss=3.637355089187622
I0131 12:10:44.511380 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:10:54.928487 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:11:15.869566 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:11:17.511419 140085747812160 submission_runner.py:408] Time since start: 58440.40s, 	Step: 118386, 	{'train/accuracy': 0.73046875, 'train/loss': 1.1939589977264404, 'validation/accuracy': 0.6730200052261353, 'validation/loss': 1.4451491832733154, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.0854063034057617, 'test/num_examples': 10000, 'score': 54255.57951760292, 'total_duration': 58440.40096735954, 'accumulated_submission_time': 54255.57951760292, 'accumulated_eval_time': 4172.6747174263, 'accumulated_logging_time': 5.619581699371338}
I0131 12:11:17.544140 139884794734336 logging_writer.py:48] [118386] accumulated_eval_time=4172.674717, accumulated_logging_time=5.619582, accumulated_submission_time=54255.579518, global_step=118386, preemption_count=0, score=54255.579518, test/accuracy=0.550200, test/loss=2.085406, test/num_examples=10000, total_duration=58440.400967, train/accuracy=0.730469, train/loss=1.193959, validation/accuracy=0.673020, validation/loss=1.445149, validation/num_examples=50000
I0131 12:11:23.444656 139884786341632 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.4538004398345947, loss=2.8516077995300293
I0131 12:12:05.828243 139884794734336 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.183112144470215, loss=4.450527667999268
I0131 12:12:52.561984 139884786341632 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.1161060333251953, loss=3.491339683532715
I0131 12:13:39.548234 139884794734336 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.17911434173584, loss=2.892303228378296
I0131 12:14:25.838070 139884786341632 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.0467467308044434, loss=3.775935411453247
I0131 12:15:12.662279 139884794734336 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.294928789138794, loss=2.8532731533050537
I0131 12:15:59.296196 139884786341632 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.1883866786956787, loss=2.82114839553833
I0131 12:16:45.784088 139884794734336 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.4106621742248535, loss=5.010567665100098
I0131 12:17:32.440498 139884786341632 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.5686967372894287, loss=2.807900905609131
I0131 12:18:17.867177 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:18:28.220359 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:18:49.619062 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:18:51.255865 140085747812160 submission_runner.py:408] Time since start: 58894.15s, 	Step: 119299, 	{'train/accuracy': 0.7431640625, 'train/loss': 1.1251846551895142, 'validation/accuracy': 0.6783999800682068, 'validation/loss': 1.4041777849197388, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.039374589920044, 'test/num_examples': 10000, 'score': 54675.842202186584, 'total_duration': 58894.145411252975, 'accumulated_submission_time': 54675.842202186584, 'accumulated_eval_time': 4206.0634133815765, 'accumulated_logging_time': 5.661855459213257}
I0131 12:18:51.289707 139884794734336 logging_writer.py:48] [119299] accumulated_eval_time=4206.063413, accumulated_logging_time=5.661855, accumulated_submission_time=54675.842202, global_step=119299, preemption_count=0, score=54675.842202, test/accuracy=0.551600, test/loss=2.039375, test/num_examples=10000, total_duration=58894.145411, train/accuracy=0.743164, train/loss=1.125185, validation/accuracy=0.678400, validation/loss=1.404178, validation/num_examples=50000
I0131 12:18:52.079657 139884786341632 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.246784210205078, loss=3.2629199028015137
I0131 12:19:33.001899 139884794734336 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.2527010440826416, loss=3.0991787910461426
I0131 12:20:19.244692 139884786341632 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.5635428428649902, loss=4.789781093597412
I0131 12:21:05.988045 139884794734336 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.2366318702697754, loss=2.7990193367004395
I0131 12:21:52.265607 139884786341632 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.49031662940979, loss=2.8018672466278076
I0131 12:22:38.882103 139884794734336 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.3513050079345703, loss=2.713773488998413
I0131 12:23:25.330526 139884786341632 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.4700779914855957, loss=2.7529234886169434
I0131 12:24:11.777440 139884794734336 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.471431255340576, loss=2.7988741397857666
I0131 12:24:58.168472 139884786341632 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.3012025356292725, loss=3.013448715209961
I0131 12:25:44.628025 139884794734336 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.298177480697632, loss=2.768754482269287
I0131 12:25:51.271411 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:26:01.488398 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:26:23.447818 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:26:25.082575 140085747812160 submission_runner.py:408] Time since start: 59347.97s, 	Step: 120216, 	{'train/accuracy': 0.7290429472923279, 'train/loss': 1.183713674545288, 'validation/accuracy': 0.6782199740409851, 'validation/loss': 1.4144235849380493, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0480198860168457, 'test/num_examples': 10000, 'score': 55095.76208996773, 'total_duration': 59347.97211909294, 'accumulated_submission_time': 55095.76208996773, 'accumulated_eval_time': 4239.874573707581, 'accumulated_logging_time': 5.706495046615601}
I0131 12:26:25.117270 139884786341632 logging_writer.py:48] [120216] accumulated_eval_time=4239.874574, accumulated_logging_time=5.706495, accumulated_submission_time=55095.762090, global_step=120216, preemption_count=0, score=55095.762090, test/accuracy=0.553500, test/loss=2.048020, test/num_examples=10000, total_duration=59347.972119, train/accuracy=0.729043, train/loss=1.183714, validation/accuracy=0.678220, validation/loss=1.414424, validation/num_examples=50000
I0131 12:26:59.339522 139884794734336 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.4463934898376465, loss=2.82869815826416
I0131 12:27:45.683106 139884786341632 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.260556936264038, loss=3.052999258041382
I0131 12:28:32.203694 139884794734336 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.578364372253418, loss=2.964198589324951
I0131 12:29:18.920219 139884786341632 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.133845329284668, loss=3.4038033485412598
I0131 12:30:05.314851 139884794734336 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.0761260986328125, loss=3.962242603302002
I0131 12:30:51.809793 139884786341632 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.2198946475982666, loss=2.912383794784546
I0131 12:31:38.196869 139884794734336 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.1249663829803467, loss=3.268167018890381
I0131 12:32:24.553247 139884786341632 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.8016345500946045, loss=4.898216724395752
I0131 12:33:11.284544 139884794734336 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.1271250247955322, loss=3.813664436340332
I0131 12:33:25.416951 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:33:35.754217 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:33:56.269805 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:33:57.904148 140085747812160 submission_runner.py:408] Time since start: 59800.79s, 	Step: 121132, 	{'train/accuracy': 0.7370507717132568, 'train/loss': 1.166053056716919, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.4264823198318481, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.0637381076812744, 'test/num_examples': 10000, 'score': 55516.00203704834, 'total_duration': 59800.793695926666, 'accumulated_submission_time': 55516.00203704834, 'accumulated_eval_time': 4272.361785888672, 'accumulated_logging_time': 5.750799179077148}
I0131 12:33:57.939287 139884786341632 logging_writer.py:48] [121132] accumulated_eval_time=4272.361786, accumulated_logging_time=5.750799, accumulated_submission_time=55516.002037, global_step=121132, preemption_count=0, score=55516.002037, test/accuracy=0.555600, test/loss=2.063738, test/num_examples=10000, total_duration=59800.793696, train/accuracy=0.737051, train/loss=1.166053, validation/accuracy=0.676780, validation/loss=1.426482, validation/num_examples=50000
I0131 12:34:25.034958 139884794734336 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.4837777614593506, loss=4.831723690032959
I0131 12:35:10.801187 139884786341632 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.5923190116882324, loss=5.087405204772949
I0131 12:35:57.371019 139884794734336 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.1966004371643066, loss=4.79518461227417
I0131 12:36:44.219019 139884786341632 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.1895883083343506, loss=3.687049627304077
I0131 12:37:30.434722 139884794734336 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.042492151260376, loss=4.893917083740234
I0131 12:38:17.063318 139884786341632 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.428617238998413, loss=2.7656219005584717
I0131 12:39:03.415860 139884794734336 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.0312740802764893, loss=4.192540645599365
I0131 12:39:49.794805 139884786341632 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.7880189418792725, loss=2.657317876815796
I0131 12:40:36.355549 139884794734336 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.3650074005126953, loss=3.0075087547302246
I0131 12:40:58.135640 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:41:08.331472 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:41:29.723353 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:41:31.365410 140085747812160 submission_runner.py:408] Time since start: 60254.25s, 	Step: 122049, 	{'train/accuracy': 0.7417968511581421, 'train/loss': 1.1477159261703491, 'validation/accuracy': 0.6811599731445312, 'validation/loss': 1.4139902591705322, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.025280237197876, 'test/num_examples': 10000, 'score': 55936.13686299324, 'total_duration': 60254.25495505333, 'accumulated_submission_time': 55936.13686299324, 'accumulated_eval_time': 4305.591577529907, 'accumulated_logging_time': 5.79656982421875}
I0131 12:41:31.400450 139884786341632 logging_writer.py:48] [122049] accumulated_eval_time=4305.591578, accumulated_logging_time=5.796570, accumulated_submission_time=55936.136863, global_step=122049, preemption_count=0, score=55936.136863, test/accuracy=0.557900, test/loss=2.025280, test/num_examples=10000, total_duration=60254.254955, train/accuracy=0.741797, train/loss=1.147716, validation/accuracy=0.681160, validation/loss=1.413990, validation/num_examples=50000
I0131 12:41:51.831746 139884794734336 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.1519622802734375, loss=3.8011484146118164
I0131 12:42:36.486978 139884786341632 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.393056631088257, loss=2.785871744155884
I0131 12:43:23.066400 139884794734336 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.3836488723754883, loss=4.943681716918945
I0131 12:44:09.884122 139884786341632 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.4204864501953125, loss=2.9747354984283447
I0131 12:44:56.514429 139884794734336 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.4233405590057373, loss=3.6813440322875977
I0131 12:45:43.367842 139884786341632 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.409517765045166, loss=2.8056583404541016
I0131 12:46:29.824149 139884794734336 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.450467109680176, loss=3.5590972900390625
I0131 12:47:16.492136 139884786341632 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.2050621509552, loss=4.14882755279541
I0131 12:48:03.240781 139884794734336 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.3916382789611816, loss=2.889953136444092
I0131 12:48:31.581345 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:48:42.075423 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:49:01.554995 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:49:03.213600 140085747812160 submission_runner.py:408] Time since start: 60706.10s, 	Step: 122963, 	{'train/accuracy': 0.7372655868530273, 'train/loss': 1.1513622999191284, 'validation/accuracy': 0.688979983329773, 'validation/loss': 1.3695464134216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 2.017380475997925, 'test/num_examples': 10000, 'score': 56356.257717609406, 'total_duration': 60706.10312247276, 'accumulated_submission_time': 56356.257717609406, 'accumulated_eval_time': 4337.223828554153, 'accumulated_logging_time': 5.842200517654419}
I0131 12:49:03.256199 139884786341632 logging_writer.py:48] [122963] accumulated_eval_time=4337.223829, accumulated_logging_time=5.842201, accumulated_submission_time=56356.257718, global_step=122963, preemption_count=0, score=56356.257718, test/accuracy=0.561400, test/loss=2.017380, test/num_examples=10000, total_duration=60706.103122, train/accuracy=0.737266, train/loss=1.151362, validation/accuracy=0.688980, validation/loss=1.369546, validation/num_examples=50000
I0131 12:49:18.213155 139884794734336 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.5626723766326904, loss=2.830000877380371
I0131 12:50:02.203297 139884786341632 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.421839952468872, loss=4.4722185134887695
I0131 12:50:48.669826 139884794734336 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.5522067546844482, loss=2.6996750831604004
I0131 12:51:35.318676 139884786341632 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.187587261199951, loss=3.2943673133850098
I0131 12:52:21.472470 139884794734336 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.4784388542175293, loss=2.643688917160034
I0131 12:53:08.483455 139884786341632 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.2961835861206055, loss=2.6779797077178955
I0131 12:53:54.688320 139884794734336 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.129758358001709, loss=4.176271438598633
I0131 12:54:41.484480 139884786341632 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.4139206409454346, loss=2.787788152694702
I0131 12:55:27.993966 139884794734336 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.366093635559082, loss=2.810378074645996
I0131 12:56:03.582759 140085747812160 spec.py:321] Evaluating on the training split.
I0131 12:56:14.108438 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 12:56:34.709739 140085747812160 spec.py:349] Evaluating on the test split.
I0131 12:56:36.368068 140085747812160 submission_runner.py:408] Time since start: 61159.26s, 	Step: 123878, 	{'train/accuracy': 0.7439843416213989, 'train/loss': 1.1398800611495972, 'validation/accuracy': 0.6844399571418762, 'validation/loss': 1.3942458629608154, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 2.015331745147705, 'test/num_examples': 10000, 'score': 56776.523052453995, 'total_duration': 61159.25761413574, 'accumulated_submission_time': 56776.523052453995, 'accumulated_eval_time': 4370.009130716324, 'accumulated_logging_time': 5.895697832107544}
I0131 12:56:36.408145 139884786341632 logging_writer.py:48] [123878] accumulated_eval_time=4370.009131, accumulated_logging_time=5.895698, accumulated_submission_time=56776.523052, global_step=123878, preemption_count=0, score=56776.523052, test/accuracy=0.564900, test/loss=2.015332, test/num_examples=10000, total_duration=61159.257614, train/accuracy=0.743984, train/loss=1.139880, validation/accuracy=0.684440, validation/loss=1.394246, validation/num_examples=50000
I0131 12:56:45.445077 139884794734336 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.695108652114868, loss=2.8350090980529785
I0131 12:57:28.276428 139884786341632 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.3181240558624268, loss=3.6651172637939453
I0131 12:58:14.751176 139884794734336 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.269925117492676, loss=3.6966018676757812
I0131 12:59:01.368949 139884786341632 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.5937652587890625, loss=2.917731761932373
I0131 12:59:47.721246 139884794734336 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.4615392684936523, loss=2.825169801712036
I0131 13:00:34.281649 139884786341632 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5560736656188965, loss=2.760472297668457
I0131 13:01:20.770160 139884794734336 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.1558659076690674, loss=4.0658440589904785
I0131 13:02:07.474863 139884786341632 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.376530408859253, loss=3.097827911376953
I0131 13:02:53.849102 139884794734336 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.386899471282959, loss=4.9282355308532715
I0131 13:03:36.812379 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:03:47.197044 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:04:08.908526 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:04:10.544739 140085747812160 submission_runner.py:408] Time since start: 61613.43s, 	Step: 124794, 	{'train/accuracy': 0.74818354845047, 'train/loss': 1.1148760318756104, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.3866878747940063, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 2.016331672668457, 'test/num_examples': 10000, 'score': 57196.867312431335, 'total_duration': 61613.43428826332, 'accumulated_submission_time': 57196.867312431335, 'accumulated_eval_time': 4403.7414972782135, 'accumulated_logging_time': 5.944585561752319}
I0131 13:04:10.581596 139884786341632 logging_writer.py:48] [124794] accumulated_eval_time=4403.741497, accumulated_logging_time=5.944586, accumulated_submission_time=57196.867312, global_step=124794, preemption_count=0, score=57196.867312, test/accuracy=0.562000, test/loss=2.016332, test/num_examples=10000, total_duration=61613.434288, train/accuracy=0.748184, train/loss=1.114876, validation/accuracy=0.686020, validation/loss=1.386688, validation/num_examples=50000
I0131 13:04:13.331324 139884794734336 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.504567861557007, loss=2.8614609241485596
I0131 13:04:54.505621 139884786341632 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.8269155025482178, loss=2.8688278198242188
I0131 13:05:40.904299 139884794734336 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.1950430870056152, loss=4.203542709350586
I0131 13:06:27.833750 139884786341632 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.3192789554595947, loss=3.3831143379211426
I0131 13:07:14.189818 139884794734336 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.3604304790496826, loss=2.72414231300354
I0131 13:08:00.695549 139884786341632 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.404693365097046, loss=2.7074198722839355
I0131 13:08:47.206484 139884794734336 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.5141704082489014, loss=4.143985271453857
I0131 13:09:33.868722 139884786341632 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.6714072227478027, loss=2.6777167320251465
I0131 13:10:20.738753 139884794734336 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.548962116241455, loss=2.794891595840454
I0131 13:11:07.252352 139884786341632 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.650285482406616, loss=2.775935411453247
I0131 13:11:10.719371 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:11:20.851370 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:11:39.947147 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:11:41.606944 140085747812160 submission_runner.py:408] Time since start: 62064.50s, 	Step: 125709, 	{'train/accuracy': 0.7470703125, 'train/loss': 1.1385987997055054, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.4028888940811157, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.021228790283203, 'test/num_examples': 10000, 'score': 57616.94558739662, 'total_duration': 62064.4964826107, 'accumulated_submission_time': 57616.94558739662, 'accumulated_eval_time': 4434.629065275192, 'accumulated_logging_time': 5.990480661392212}
I0131 13:11:41.647617 139884794734336 logging_writer.py:48] [125709] accumulated_eval_time=4434.629065, accumulated_logging_time=5.990481, accumulated_submission_time=57616.945587, global_step=125709, preemption_count=0, score=57616.945587, test/accuracy=0.562600, test/loss=2.021229, test/num_examples=10000, total_duration=62064.496483, train/accuracy=0.747070, train/loss=1.138599, validation/accuracy=0.683920, validation/loss=1.402889, validation/num_examples=50000
I0131 13:12:19.365421 139884786341632 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.4307892322540283, loss=2.95404052734375
I0131 13:13:06.084942 139884794734336 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.546689987182617, loss=2.714881181716919
I0131 13:13:52.826025 139884786341632 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.4232232570648193, loss=2.6265995502471924
I0131 13:14:39.389970 139884794734336 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.2129647731781006, loss=3.431905508041382
I0131 13:15:26.144312 139884786341632 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.4617936611175537, loss=4.505873680114746
I0131 13:16:13.098129 139884794734336 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.538952589035034, loss=2.7491455078125
I0131 13:16:59.833208 139884786341632 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.316448211669922, loss=4.457447052001953
I0131 13:17:46.451233 139884794734336 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.4562416076660156, loss=2.847421407699585
I0131 13:18:33.387063 139884786341632 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.647430419921875, loss=2.8254618644714355
I0131 13:18:41.825301 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:18:52.232105 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:19:11.637081 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:19:13.291372 140085747812160 submission_runner.py:408] Time since start: 62516.18s, 	Step: 126620, 	{'train/accuracy': 0.7459765672683716, 'train/loss': 1.1232553720474243, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.3786057233810425, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 1.9932000637054443, 'test/num_examples': 10000, 'score': 58037.063480854034, 'total_duration': 62516.18090867996, 'accumulated_submission_time': 58037.063480854034, 'accumulated_eval_time': 4466.095130681992, 'accumulated_logging_time': 6.0411882400512695}
I0131 13:19:13.334654 139884794734336 logging_writer.py:48] [126620] accumulated_eval_time=4466.095131, accumulated_logging_time=6.041188, accumulated_submission_time=58037.063481, global_step=126620, preemption_count=0, score=58037.063481, test/accuracy=0.568400, test/loss=1.993200, test/num_examples=10000, total_duration=62516.180909, train/accuracy=0.745977, train/loss=1.123255, validation/accuracy=0.688820, validation/loss=1.378606, validation/num_examples=50000
I0131 13:19:45.965504 139884786341632 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.839123487472534, loss=4.941127300262451
I0131 13:20:32.308737 139884794734336 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.8468515872955322, loss=2.9378089904785156
I0131 13:21:18.999088 139884786341632 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.771958112716675, loss=2.6923961639404297
I0131 13:22:05.524506 139884794734336 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.5901899337768555, loss=4.741483688354492
I0131 13:22:51.997604 139884786341632 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.9116742610931396, loss=2.8467376232147217
I0131 13:23:38.614151 139884794734336 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.491136074066162, loss=2.8129241466522217
I0131 13:24:25.175244 139884786341632 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.741584300994873, loss=2.733508825302124
I0131 13:25:11.645739 139884794734336 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.522580623626709, loss=2.6434824466705322
I0131 13:25:58.010456 139884786341632 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.496262788772583, loss=4.8936333656311035
I0131 13:26:13.456264 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:26:23.658197 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:26:45.401270 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:26:47.040469 140085747812160 submission_runner.py:408] Time since start: 62969.93s, 	Step: 127535, 	{'train/accuracy': 0.7511913776397705, 'train/loss': 1.0778286457061768, 'validation/accuracy': 0.6904999613761902, 'validation/loss': 1.3422291278839111, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.9629532098770142, 'test/num_examples': 10000, 'score': 58457.123943567276, 'total_duration': 62969.93001675606, 'accumulated_submission_time': 58457.123943567276, 'accumulated_eval_time': 4499.6793439388275, 'accumulated_logging_time': 6.095107316970825}
I0131 13:26:47.076548 139884794734336 logging_writer.py:48] [127535] accumulated_eval_time=4499.679344, accumulated_logging_time=6.095107, accumulated_submission_time=58457.123944, global_step=127535, preemption_count=0, score=58457.123944, test/accuracy=0.565900, test/loss=1.962953, test/num_examples=10000, total_duration=62969.930017, train/accuracy=0.751191, train/loss=1.077829, validation/accuracy=0.690500, validation/loss=1.342229, validation/num_examples=50000
I0131 13:27:12.963235 139884786341632 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.3420822620391846, loss=2.691701650619507
I0131 13:27:58.389868 139884794734336 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.5112266540527344, loss=4.194923400878906
I0131 13:28:44.867806 139884786341632 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.497013807296753, loss=4.394904613494873
I0131 13:29:31.454667 139884794734336 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.409749984741211, loss=4.227606773376465
I0131 13:30:18.045137 139884786341632 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.63726806640625, loss=2.785062789916992
I0131 13:31:04.456533 139884794734336 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.4586074352264404, loss=4.123349666595459
I0131 13:31:50.858903 139884786341632 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.6218221187591553, loss=4.401820659637451
I0131 13:32:37.621535 139884794734336 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.7896950244903564, loss=4.392690658569336
I0131 13:33:24.114454 139884786341632 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.717600107192993, loss=2.817192792892456
I0131 13:33:47.366763 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:33:57.443775 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:34:17.386169 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:34:19.035246 140085747812160 submission_runner.py:408] Time since start: 63421.92s, 	Step: 128452, 	{'train/accuracy': 0.7696874737739563, 'train/loss': 1.0241750478744507, 'validation/accuracy': 0.6924600005149841, 'validation/loss': 1.3428523540496826, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.9711560010910034, 'test/num_examples': 10000, 'score': 58877.354343652725, 'total_duration': 63421.92477989197, 'accumulated_submission_time': 58877.354343652725, 'accumulated_eval_time': 4531.347820997238, 'accumulated_logging_time': 6.140000343322754}
I0131 13:34:19.079107 139884794734336 logging_writer.py:48] [128452] accumulated_eval_time=4531.347821, accumulated_logging_time=6.140000, accumulated_submission_time=58877.354344, global_step=128452, preemption_count=0, score=58877.354344, test/accuracy=0.569000, test/loss=1.971156, test/num_examples=10000, total_duration=63421.924780, train/accuracy=0.769687, train/loss=1.024175, validation/accuracy=0.692460, validation/loss=1.342852, validation/num_examples=50000
I0131 13:34:38.313158 139884786341632 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.4377291202545166, loss=2.9533092975616455
I0131 13:35:23.296028 139884794734336 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.862443208694458, loss=2.782850503921509
I0131 13:36:10.058614 139884786341632 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.7156379222869873, loss=2.7247869968414307
I0131 13:36:56.547728 139884794734336 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.7598485946655273, loss=2.7359414100646973
I0131 13:37:43.066540 139884786341632 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.3824446201324463, loss=3.8222622871398926
I0131 13:38:29.325229 139884794734336 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.703843832015991, loss=3.0288140773773193
I0131 13:39:15.836486 139884786341632 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.657383680343628, loss=2.646620512008667
I0131 13:40:02.098947 139884794734336 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.321810483932495, loss=3.6304943561553955
I0131 13:40:48.591292 139884786341632 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.6169536113739014, loss=2.6795294284820557
I0131 13:41:19.092454 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:41:29.189674 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:41:50.818450 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:41:52.460258 140085747812160 submission_runner.py:408] Time since start: 63875.35s, 	Step: 129367, 	{'train/accuracy': 0.7485156059265137, 'train/loss': 1.1012896299362183, 'validation/accuracy': 0.6915199756622314, 'validation/loss': 1.3491532802581787, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 1.9738880395889282, 'test/num_examples': 10000, 'score': 59297.305203437805, 'total_duration': 63875.34980750084, 'accumulated_submission_time': 59297.305203437805, 'accumulated_eval_time': 4564.715627908707, 'accumulated_logging_time': 6.195974826812744}
I0131 13:41:52.496902 139884794734336 logging_writer.py:48] [129367] accumulated_eval_time=4564.715628, accumulated_logging_time=6.195975, accumulated_submission_time=59297.305203, global_step=129367, preemption_count=0, score=59297.305203, test/accuracy=0.562900, test/loss=1.973888, test/num_examples=10000, total_duration=63875.349808, train/accuracy=0.748516, train/loss=1.101290, validation/accuracy=0.691520, validation/loss=1.349153, validation/num_examples=50000
I0131 13:42:05.846628 139884786341632 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.5404131412506104, loss=4.464404582977295
I0131 13:42:49.347496 139884794734336 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.3862321376800537, loss=4.167593002319336
I0131 13:43:35.854572 139884786341632 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.652082681655884, loss=4.727997779846191
I0131 13:44:22.484157 139884794734336 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.018756866455078, loss=2.7088980674743652
I0131 13:45:08.697286 139884786341632 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.6579408645629883, loss=4.837001800537109
I0131 13:45:55.297559 139884794734336 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.6239688396453857, loss=2.7321419715881348
I0131 13:46:41.732758 139884786341632 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.6130995750427246, loss=3.0150673389434814
I0131 13:47:28.299891 139884794734336 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.8636486530303955, loss=2.762737512588501
I0131 13:48:15.237455 139884786341632 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.6259920597076416, loss=2.6961216926574707
I0131 13:48:52.882539 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:49:03.243987 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:49:23.618085 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:49:25.277618 140085747812160 submission_runner.py:408] Time since start: 64328.17s, 	Step: 130283, 	{'train/accuracy': 0.7594531178474426, 'train/loss': 1.0408467054367065, 'validation/accuracy': 0.6985399723052979, 'validation/loss': 1.3164602518081665, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.9376977682113647, 'test/num_examples': 10000, 'score': 59717.62939977646, 'total_duration': 64328.16715955734, 'accumulated_submission_time': 59717.62939977646, 'accumulated_eval_time': 4597.110694646835, 'accumulated_logging_time': 6.243996620178223}
I0131 13:49:25.313579 139884794734336 logging_writer.py:48] [130283] accumulated_eval_time=4597.110695, accumulated_logging_time=6.243997, accumulated_submission_time=59717.629400, global_step=130283, preemption_count=0, score=59717.629400, test/accuracy=0.576300, test/loss=1.937698, test/num_examples=10000, total_duration=64328.167160, train/accuracy=0.759453, train/loss=1.040847, validation/accuracy=0.698540, validation/loss=1.316460, validation/num_examples=50000
I0131 13:49:32.379369 139884786341632 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.3137080669403076, loss=3.4919822216033936
I0131 13:50:14.799756 139884794734336 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.3652899265289307, loss=3.19791841506958
I0131 13:51:01.083891 139884786341632 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.6803064346313477, loss=2.7823801040649414
I0131 13:51:47.679806 139884794734336 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.599130868911743, loss=2.671091318130493
I0131 13:52:34.226516 139884786341632 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.63258695602417, loss=2.980522394180298
I0131 13:53:20.807664 139884794734336 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.207404136657715, loss=4.875814437866211
I0131 13:54:07.173589 139884786341632 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.3000471591949463, loss=4.000304222106934
I0131 13:54:53.669615 139884794734336 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.6645877361297607, loss=2.936323881149292
I0131 13:55:40.014431 139884786341632 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.8687782287597656, loss=2.827012300491333
I0131 13:56:25.753566 140085747812160 spec.py:321] Evaluating on the training split.
I0131 13:56:36.476274 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 13:56:57.067810 140085747812160 spec.py:349] Evaluating on the test split.
I0131 13:56:58.712084 140085747812160 submission_runner.py:408] Time since start: 64781.60s, 	Step: 131200, 	{'train/accuracy': 0.7681640386581421, 'train/loss': 1.0163657665252686, 'validation/accuracy': 0.6958799958229065, 'validation/loss': 1.330001711845398, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.946197748184204, 'test/num_examples': 10000, 'score': 60138.00821852684, 'total_duration': 64781.60162329674, 'accumulated_submission_time': 60138.00821852684, 'accumulated_eval_time': 4630.069223642349, 'accumulated_logging_time': 6.288846492767334}
I0131 13:56:58.748478 139884794734336 logging_writer.py:48] [131200] accumulated_eval_time=4630.069224, accumulated_logging_time=6.288846, accumulated_submission_time=60138.008219, global_step=131200, preemption_count=0, score=60138.008219, test/accuracy=0.575000, test/loss=1.946198, test/num_examples=10000, total_duration=64781.601623, train/accuracy=0.768164, train/loss=1.016366, validation/accuracy=0.695880, validation/loss=1.330002, validation/num_examples=50000
I0131 13:56:59.144721 139884786341632 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.4259121417999268, loss=4.221765995025635
I0131 13:57:40.121381 139884794734336 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.5886847972869873, loss=2.6254451274871826
I0131 13:58:26.692120 139884786341632 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.7834835052490234, loss=3.18813419342041
I0131 13:59:13.187260 139884794734336 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.616457939147949, loss=2.680715560913086
I0131 13:59:59.755293 139884786341632 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.821366548538208, loss=2.7645606994628906
I0131 14:00:46.440418 139884794734336 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.7796618938446045, loss=2.739075183868408
I0131 14:01:33.109580 139884786341632 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.608333110809326, loss=2.81502628326416
I0131 14:02:19.420388 139884794734336 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.572025775909424, loss=2.8097140789031982
I0131 14:03:05.949295 139884786341632 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.6341240406036377, loss=2.7251579761505127
I0131 14:03:52.297941 139884794734336 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.9804763793945312, loss=4.492136001586914
I0131 14:03:58.875439 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:04:09.362191 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:04:29.351161 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:04:30.999404 140085747812160 submission_runner.py:408] Time since start: 65233.89s, 	Step: 132116, 	{'train/accuracy': 0.7521288990974426, 'train/loss': 1.0870712995529175, 'validation/accuracy': 0.6966399550437927, 'validation/loss': 1.3381699323654175, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 1.9606136083602905, 'test/num_examples': 10000, 'score': 60558.07671093941, 'total_duration': 65233.88895082474, 'accumulated_submission_time': 60558.07671093941, 'accumulated_eval_time': 4662.193197011948, 'accumulated_logging_time': 6.333850622177124}
I0131 14:04:31.033863 139884786341632 logging_writer.py:48] [132116] accumulated_eval_time=4662.193197, accumulated_logging_time=6.333851, accumulated_submission_time=60558.076711, global_step=132116, preemption_count=0, score=60558.076711, test/accuracy=0.571700, test/loss=1.960614, test/num_examples=10000, total_duration=65233.888951, train/accuracy=0.752129, train/loss=1.087071, validation/accuracy=0.696640, validation/loss=1.338170, validation/num_examples=50000
I0131 14:05:05.354061 139884794734336 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.7536003589630127, loss=2.6880526542663574
I0131 14:05:51.474281 139884786341632 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.5650830268859863, loss=4.4106950759887695
I0131 14:06:37.871097 139884794734336 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.4615931510925293, loss=2.541579008102417
I0131 14:07:24.379373 139884786341632 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.6765940189361572, loss=4.477272033691406
I0131 14:08:10.847953 139884794734336 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.017831563949585, loss=2.6821751594543457
I0131 14:08:57.569800 139884786341632 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.7022461891174316, loss=2.7630646228790283
I0131 14:09:44.103898 139884794734336 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.844640016555786, loss=2.690312385559082
I0131 14:10:30.808771 139884786341632 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.784263849258423, loss=2.7415547370910645
I0131 14:11:17.447818 139884794734336 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.486616611480713, loss=3.5703952312469482
I0131 14:11:31.036790 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:11:41.467234 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:12:01.778613 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:12:03.429194 140085747812160 submission_runner.py:408] Time since start: 65686.32s, 	Step: 133031, 	{'train/accuracy': 0.7635741829872131, 'train/loss': 1.053499698638916, 'validation/accuracy': 0.6963199973106384, 'validation/loss': 1.3278359174728394, 'validation/num_examples': 50000, 'test/accuracy': 0.5728000402450562, 'test/loss': 1.9535140991210938, 'test/num_examples': 10000, 'score': 60978.0214304924, 'total_duration': 65686.31872987747, 'accumulated_submission_time': 60978.0214304924, 'accumulated_eval_time': 4694.585594415665, 'accumulated_logging_time': 6.377013444900513}
I0131 14:12:03.467161 139884786341632 logging_writer.py:48] [133031] accumulated_eval_time=4694.585594, accumulated_logging_time=6.377013, accumulated_submission_time=60978.021430, global_step=133031, preemption_count=0, score=60978.021430, test/accuracy=0.572800, test/loss=1.953514, test/num_examples=10000, total_duration=65686.318730, train/accuracy=0.763574, train/loss=1.053500, validation/accuracy=0.696320, validation/loss=1.327836, validation/num_examples=50000
I0131 14:12:30.936895 139884794734336 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.8821628093719482, loss=2.8103628158569336
I0131 14:13:17.357720 139884786341632 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.514310836791992, loss=4.775954246520996
I0131 14:14:04.022683 139884794734336 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.9112913608551025, loss=2.6367580890655518
I0131 14:14:50.544190 139884786341632 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.764324426651001, loss=4.1555328369140625
I0131 14:15:37.207896 139884794734336 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.860283851623535, loss=2.7690043449401855
I0131 14:16:23.733752 139884786341632 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.8819024562835693, loss=2.650099039077759
I0131 14:17:10.378296 139884794734336 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.493846893310547, loss=4.17091178894043
I0131 14:17:56.793371 139884786341632 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.5466182231903076, loss=2.6282577514648438
I0131 14:18:43.278976 139884794734336 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.2894318103790283, loss=2.725825786590576
I0131 14:19:03.745577 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:19:13.977302 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:19:35.427134 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:19:37.062462 140085747812160 submission_runner.py:408] Time since start: 66139.95s, 	Step: 133946, 	{'train/accuracy': 0.7712304592132568, 'train/loss': 0.9981160759925842, 'validation/accuracy': 0.7023999691009521, 'validation/loss': 1.3043373823165894, 'validation/num_examples': 50000, 'test/accuracy': 0.5840000510215759, 'test/loss': 1.9154057502746582, 'test/num_examples': 10000, 'score': 61398.24069237709, 'total_duration': 66139.95198273659, 'accumulated_submission_time': 61398.24069237709, 'accumulated_eval_time': 4727.902453184128, 'accumulated_logging_time': 6.423879384994507}
I0131 14:19:37.100031 139884786341632 logging_writer.py:48] [133946] accumulated_eval_time=4727.902453, accumulated_logging_time=6.423879, accumulated_submission_time=61398.240692, global_step=133946, preemption_count=0, score=61398.240692, test/accuracy=0.584000, test/loss=1.915406, test/num_examples=10000, total_duration=66139.951983, train/accuracy=0.771230, train/loss=0.998116, validation/accuracy=0.702400, validation/loss=1.304337, validation/num_examples=50000
I0131 14:19:58.701807 139884794734336 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.8940088748931885, loss=2.751194477081299
I0131 14:20:43.403240 139884786341632 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.888324022293091, loss=2.7540807723999023
I0131 14:21:29.890045 139884794734336 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.6641571521759033, loss=2.5793328285217285
I0131 14:22:16.713640 139884786341632 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.9078006744384766, loss=2.714857578277588
I0131 14:23:03.179611 139884794734336 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.692582845687866, loss=3.001661539077759
I0131 14:23:49.644690 139884786341632 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.910876512527466, loss=2.634312868118286
I0131 14:24:35.945219 139884794734336 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.1372268199920654, loss=4.7179741859436035
I0131 14:25:22.604661 139884786341632 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.972414255142212, loss=2.817532777786255
I0131 14:26:08.962419 139884794734336 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.7058913707733154, loss=3.4395618438720703
I0131 14:26:37.097609 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:26:48.124639 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:27:09.378797 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:27:11.015684 140085747812160 submission_runner.py:408] Time since start: 66593.91s, 	Step: 134862, 	{'train/accuracy': 0.7629492282867432, 'train/loss': 1.0372766256332397, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.304007887840271, 'validation/num_examples': 50000, 'test/accuracy': 0.5812000036239624, 'test/loss': 1.9075292348861694, 'test/num_examples': 10000, 'score': 61818.17729949951, 'total_duration': 66593.90523028374, 'accumulated_submission_time': 61818.17729949951, 'accumulated_eval_time': 4761.82052397728, 'accumulated_logging_time': 6.472502708435059}
I0131 14:27:11.054644 139884786341632 logging_writer.py:48] [134862] accumulated_eval_time=4761.820524, accumulated_logging_time=6.472503, accumulated_submission_time=61818.177299, global_step=134862, preemption_count=0, score=61818.177299, test/accuracy=0.581200, test/loss=1.907529, test/num_examples=10000, total_duration=66593.905230, train/accuracy=0.762949, train/loss=1.037277, validation/accuracy=0.702740, validation/loss=1.304008, validation/num_examples=50000
I0131 14:27:26.352398 139884794734336 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.8523752689361572, loss=2.631911277770996
I0131 14:28:09.891816 139884786341632 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7662723064422607, loss=2.8642358779907227
I0131 14:28:56.431571 139884794734336 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.495910406112671, loss=4.413506507873535
I0131 14:29:43.474672 139884786341632 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.638418674468994, loss=3.597269296646118
I0131 14:30:29.817255 139884794734336 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.1609439849853516, loss=4.663546085357666
I0131 14:31:16.308362 139884786341632 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.5288875102996826, loss=3.0679054260253906
I0131 14:32:02.917296 139884794734336 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.8906617164611816, loss=2.6767706871032715
I0131 14:32:49.458803 139884786341632 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.9688267707824707, loss=2.5925796031951904
I0131 14:33:36.074050 139884794734336 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.2421627044677734, loss=2.6346189975738525
I0131 14:34:11.193456 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:34:21.313093 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:34:41.962993 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:34:43.609686 140085747812160 submission_runner.py:408] Time since start: 67046.50s, 	Step: 135777, 	{'train/accuracy': 0.7660546898841858, 'train/loss': 1.0418373346328735, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.3056179285049438, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.9201245307922363, 'test/num_examples': 10000, 'score': 62238.25712633133, 'total_duration': 67046.49923276901, 'accumulated_submission_time': 62238.25712633133, 'accumulated_eval_time': 4794.236758947372, 'accumulated_logging_time': 6.520557165145874}
I0131 14:34:43.647633 139884786341632 logging_writer.py:48] [135777] accumulated_eval_time=4794.236759, accumulated_logging_time=6.520557, accumulated_submission_time=62238.257126, global_step=135777, preemption_count=0, score=62238.257126, test/accuracy=0.579800, test/loss=1.920125, test/num_examples=10000, total_duration=67046.499233, train/accuracy=0.766055, train/loss=1.041837, validation/accuracy=0.704140, validation/loss=1.305618, validation/num_examples=50000
I0131 14:34:53.341069 139884794734336 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.6766679286956787, loss=3.837979316711426
I0131 14:35:35.986423 139884786341632 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.6930418014526367, loss=3.0288615226745605
I0131 14:36:22.344473 139884794734336 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.5153934955596924, loss=3.912414073944092
I0131 14:37:08.990413 139884786341632 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.9383339881896973, loss=3.0574886798858643
I0131 14:37:55.526094 139884794734336 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.8399834632873535, loss=2.6248888969421387
I0131 14:38:42.195637 139884786341632 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.0841763019561768, loss=2.722904920578003
I0131 14:39:28.840389 139884794734336 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.3281912803649902, loss=2.6974709033966064
I0131 14:40:15.275376 139884786341632 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.4933547973632812, loss=2.6283438205718994
I0131 14:41:01.578547 139884794734336 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.5152363777160645, loss=4.165153503417969
I0131 14:41:44.134584 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:41:54.566820 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:42:15.098252 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:42:16.744279 140085747812160 submission_runner.py:408] Time since start: 67499.63s, 	Step: 136693, 	{'train/accuracy': 0.7722460627555847, 'train/loss': 1.0183627605438232, 'validation/accuracy': 0.7065199613571167, 'validation/loss': 1.3071863651275635, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9220960140228271, 'test/num_examples': 10000, 'score': 62658.68343901634, 'total_duration': 67499.63382172585, 'accumulated_submission_time': 62658.68343901634, 'accumulated_eval_time': 4826.84645652771, 'accumulated_logging_time': 6.569271087646484}
I0131 14:42:16.779760 139884786341632 logging_writer.py:48] [136693] accumulated_eval_time=4826.846457, accumulated_logging_time=6.569271, accumulated_submission_time=62658.683439, global_step=136693, preemption_count=0, score=62658.683439, test/accuracy=0.582600, test/loss=1.922096, test/num_examples=10000, total_duration=67499.633822, train/accuracy=0.772246, train/loss=1.018363, validation/accuracy=0.706520, validation/loss=1.307186, validation/num_examples=50000
I0131 14:42:19.923895 139884794734336 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.0343382358551025, loss=2.558619260787964
I0131 14:43:01.825252 139884786341632 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.758528232574463, loss=2.7714028358459473
I0131 14:43:48.299667 139884794734336 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.1358749866485596, loss=2.942786455154419
I0131 14:44:35.026382 139884786341632 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.9365696907043457, loss=2.6122806072235107
I0131 14:45:21.472537 139884794734336 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.582630157470703, loss=3.8169853687286377
I0131 14:46:07.772559 139884786341632 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.1630985736846924, loss=2.6330533027648926
I0131 14:46:54.465833 139884794734336 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.9039041996002197, loss=3.5604515075683594
I0131 14:47:40.799958 139884786341632 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.182873010635376, loss=2.6244232654571533
I0131 14:48:27.472056 139884794734336 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.3038811683654785, loss=2.61568546295166
I0131 14:49:13.949967 139884786341632 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.864309787750244, loss=2.691685438156128
I0131 14:49:16.883107 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:49:26.948846 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:49:48.811756 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:49:50.452051 140085747812160 submission_runner.py:408] Time since start: 67953.34s, 	Step: 137608, 	{'train/accuracy': 0.7710937261581421, 'train/loss': 1.0141106843948364, 'validation/accuracy': 0.7095400094985962, 'validation/loss': 1.2850812673568726, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.897950291633606, 'test/num_examples': 10000, 'score': 63078.72762656212, 'total_duration': 67953.34159827232, 'accumulated_submission_time': 63078.72762656212, 'accumulated_eval_time': 4860.415402173996, 'accumulated_logging_time': 6.61378026008606}
I0131 14:49:50.495457 139884794734336 logging_writer.py:48] [137608] accumulated_eval_time=4860.415402, accumulated_logging_time=6.613780, accumulated_submission_time=63078.727627, global_step=137608, preemption_count=0, score=63078.727627, test/accuracy=0.586900, test/loss=1.897950, test/num_examples=10000, total_duration=67953.341598, train/accuracy=0.771094, train/loss=1.014111, validation/accuracy=0.709540, validation/loss=1.285081, validation/num_examples=50000
I0131 14:50:27.862625 139884786341632 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.817639112472534, loss=4.261946678161621
I0131 14:51:14.315629 139884794734336 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.9641244411468506, loss=2.7633936405181885
I0131 14:52:01.150686 139884786341632 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.7255520820617676, loss=3.7001092433929443
I0131 14:52:47.930759 139884794734336 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.8866260051727295, loss=2.9362525939941406
I0131 14:53:34.514635 139884786341632 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.1140143871307373, loss=3.425527572631836
I0131 14:54:21.100263 139884794734336 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.7393558025360107, loss=4.304073810577393
I0131 14:55:07.664567 139884786341632 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.0118939876556396, loss=2.611117362976074
I0131 14:55:54.459382 139884794734336 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.7607407569885254, loss=4.3927717208862305
I0131 14:56:40.853824 139884786341632 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.331298589706421, loss=4.701221466064453
I0131 14:56:50.698836 140085747812160 spec.py:321] Evaluating on the training split.
I0131 14:57:01.238762 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 14:57:21.831746 140085747812160 spec.py:349] Evaluating on the test split.
I0131 14:57:23.483361 140085747812160 submission_runner.py:408] Time since start: 68406.37s, 	Step: 138523, 	{'train/accuracy': 0.7734375, 'train/loss': 1.0056006908416748, 'validation/accuracy': 0.7101199626922607, 'validation/loss': 1.2799828052520752, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8924295902252197, 'test/num_examples': 10000, 'score': 63498.872061014175, 'total_duration': 68406.37291073799, 'accumulated_submission_time': 63498.872061014175, 'accumulated_eval_time': 4893.19992518425, 'accumulated_logging_time': 6.666823387145996}
I0131 14:57:23.518897 139884794734336 logging_writer.py:48] [138523] accumulated_eval_time=4893.199925, accumulated_logging_time=6.666823, accumulated_submission_time=63498.872061, global_step=138523, preemption_count=0, score=63498.872061, test/accuracy=0.589600, test/loss=1.892430, test/num_examples=10000, total_duration=68406.372911, train/accuracy=0.773438, train/loss=1.005601, validation/accuracy=0.710120, validation/loss=1.279983, validation/num_examples=50000
I0131 14:57:54.565818 139884786341632 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.6335644721984863, loss=2.985880136489868
I0131 14:58:40.934837 139884794734336 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.930567979812622, loss=3.091848134994507
I0131 14:59:27.839984 139884786341632 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.08736252784729, loss=3.055345058441162
I0131 15:00:14.743764 139884794734336 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.99415922164917, loss=2.7005362510681152
I0131 15:01:00.756452 139884786341632 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.2139241695404053, loss=4.112372398376465
I0131 15:01:47.247243 139884794734336 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.029996156692505, loss=2.5975899696350098
I0131 15:02:33.886922 139884786341632 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.070277214050293, loss=2.7032063007354736
I0131 15:03:20.445756 139884794734336 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.2198336124420166, loss=2.822550058364868
I0131 15:04:06.892382 139884786341632 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.9311556816101074, loss=4.4664764404296875
I0131 15:04:23.744318 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:04:34.210586 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:04:54.922009 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:04:56.573590 140085747812160 submission_runner.py:408] Time since start: 68859.46s, 	Step: 139438, 	{'train/accuracy': 0.7773827910423279, 'train/loss': 0.9855165481567383, 'validation/accuracy': 0.7126399874687195, 'validation/loss': 1.2628765106201172, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.8855514526367188, 'test/num_examples': 10000, 'score': 63919.03820538521, 'total_duration': 68859.4631357193, 'accumulated_submission_time': 63919.03820538521, 'accumulated_eval_time': 4926.0291867256165, 'accumulated_logging_time': 6.712033033370972}
I0131 15:04:56.608884 139884794734336 logging_writer.py:48] [139438] accumulated_eval_time=4926.029187, accumulated_logging_time=6.712033, accumulated_submission_time=63919.038205, global_step=139438, preemption_count=0, score=63919.038205, test/accuracy=0.587300, test/loss=1.885551, test/num_examples=10000, total_duration=68859.463136, train/accuracy=0.777383, train/loss=0.985517, validation/accuracy=0.712640, validation/loss=1.262877, validation/num_examples=50000
I0131 15:05:21.329113 139884786341632 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.0155975818634033, loss=4.005043983459473
I0131 15:06:06.880368 139884794734336 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.308058023452759, loss=2.5696446895599365
I0131 15:06:53.323334 139884786341632 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.9181435108184814, loss=2.7036094665527344
I0131 15:07:39.802377 139884794734336 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.255202531814575, loss=2.618499517440796
I0131 15:08:26.016979 139884786341632 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.203509569168091, loss=2.51278018951416
I0131 15:09:12.719309 139884794734336 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.9958107471466064, loss=2.524637460708618
I0131 15:09:59.009352 139884786341632 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.853803873062134, loss=2.833422899246216
I0131 15:10:45.800698 139884794734336 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.0317604541778564, loss=3.2154488563537598
I0131 15:11:32.488506 139884786341632 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.035257577896118, loss=2.6490166187286377
I0131 15:11:56.818028 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:12:07.461991 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:12:26.705910 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:12:28.361145 140085747812160 submission_runner.py:408] Time since start: 69311.25s, 	Step: 140354, 	{'train/accuracy': 0.78968745470047, 'train/loss': 0.938185453414917, 'validation/accuracy': 0.7109599709510803, 'validation/loss': 1.2699692249298096, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.891028642654419, 'test/num_examples': 10000, 'score': 64339.187994003296, 'total_duration': 69311.2506814003, 'accumulated_submission_time': 64339.187994003296, 'accumulated_eval_time': 4957.572335958481, 'accumulated_logging_time': 6.756109237670898}
I0131 15:12:28.404928 139884794734336 logging_writer.py:48] [140354] accumulated_eval_time=4957.572336, accumulated_logging_time=6.756109, accumulated_submission_time=64339.187994, global_step=140354, preemption_count=0, score=64339.187994, test/accuracy=0.587100, test/loss=1.891029, test/num_examples=10000, total_duration=69311.250681, train/accuracy=0.789687, train/loss=0.938185, validation/accuracy=0.710960, validation/loss=1.269969, validation/num_examples=50000
I0131 15:12:46.896353 139884786341632 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.853083848953247, loss=2.995840549468994
I0131 15:13:31.460321 139884794734336 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.662364959716797, loss=3.8302972316741943
I0131 15:14:17.969003 139884786341632 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.341676950454712, loss=2.5378103256225586
I0131 15:15:04.913311 139884794734336 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.0411651134490967, loss=2.5146894454956055
I0131 15:15:51.276725 139884786341632 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.8681740760803223, loss=2.5786123275756836
I0131 15:16:37.939240 139884794734336 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.9718551635742188, loss=2.553070068359375
I0131 15:17:24.333717 139884786341632 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.0470235347747803, loss=2.850027084350586
I0131 15:18:10.932827 139884794734336 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.1162917613983154, loss=4.48477029800415
I0131 15:18:57.552390 139884786341632 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.1631107330322266, loss=3.5682590007781982
I0131 15:19:28.460649 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:19:39.135228 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:20:00.012694 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:20:01.667128 140085747812160 submission_runner.py:408] Time since start: 69764.56s, 	Step: 141268, 	{'train/accuracy': 0.775585949420929, 'train/loss': 0.9803733825683594, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.2429838180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.8562695980072021, 'test/num_examples': 10000, 'score': 64759.182983875275, 'total_duration': 69764.55665397644, 'accumulated_submission_time': 64759.182983875275, 'accumulated_eval_time': 4990.778796195984, 'accumulated_logging_time': 6.810421466827393}
I0131 15:20:01.703723 139884794734336 logging_writer.py:48] [141268] accumulated_eval_time=4990.778796, accumulated_logging_time=6.810421, accumulated_submission_time=64759.182984, global_step=141268, preemption_count=0, score=64759.182984, test/accuracy=0.590800, test/loss=1.856270, test/num_examples=10000, total_duration=69764.556654, train/accuracy=0.775586, train/loss=0.980373, validation/accuracy=0.714340, validation/loss=1.242984, validation/num_examples=50000
I0131 15:20:14.643720 139884786341632 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.2377469539642334, loss=4.315854072570801
I0131 15:20:57.969665 139884794734336 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.9230868816375732, loss=3.1287786960601807
I0131 15:21:44.470669 139884786341632 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.8405942916870117, loss=2.8371615409851074
I0131 15:22:31.200295 139884794734336 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.1980855464935303, loss=4.13490104675293
I0131 15:23:17.764927 139884786341632 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.261082887649536, loss=4.191377639770508
I0131 15:24:04.144227 139884794734336 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4603614807128906, loss=2.4748103618621826
I0131 15:24:50.367663 139884786341632 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.899503469467163, loss=4.750997543334961
I0131 15:25:36.903572 139884794734336 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.2251358032226562, loss=2.6022720336914062
I0131 15:26:23.506311 139884786341632 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.1204042434692383, loss=3.4128856658935547
I0131 15:27:02.042293 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:27:12.390004 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:27:31.739189 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:27:33.400752 140085747812160 submission_runner.py:408] Time since start: 70216.29s, 	Step: 142185, 	{'train/accuracy': 0.7842773199081421, 'train/loss': 0.9769614338874817, 'validation/accuracy': 0.7149400115013123, 'validation/loss': 1.2617579698562622, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.858317494392395, 'test/num_examples': 10000, 'score': 65179.461914777756, 'total_duration': 70216.29029083252, 'accumulated_submission_time': 65179.461914777756, 'accumulated_eval_time': 5022.137254714966, 'accumulated_logging_time': 6.856017589569092}
I0131 15:27:33.444483 139884794734336 logging_writer.py:48] [142185] accumulated_eval_time=5022.137255, accumulated_logging_time=6.856018, accumulated_submission_time=65179.461915, global_step=142185, preemption_count=0, score=65179.461915, test/accuracy=0.596500, test/loss=1.858317, test/num_examples=10000, total_duration=70216.290291, train/accuracy=0.784277, train/loss=0.976961, validation/accuracy=0.714940, validation/loss=1.261758, validation/num_examples=50000
I0131 15:27:39.739324 139884786341632 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.3047127723693848, loss=2.575784683227539
I0131 15:28:22.001280 139884794734336 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.8881659507751465, loss=3.6296684741973877
I0131 15:29:08.371658 139884786341632 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.058370590209961, loss=2.6739773750305176
I0131 15:29:55.033547 139884794734336 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.1100900173187256, loss=2.4617085456848145
I0131 15:30:41.446206 139884786341632 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.0058412551879883, loss=2.541301965713501
I0131 15:31:28.007432 139884794734336 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.026625633239746, loss=2.9091854095458984
I0131 15:32:14.327238 139884786341632 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.0199594497680664, loss=3.362419605255127
I0131 15:33:01.176850 139884794734336 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.500218629837036, loss=2.6160686016082764
I0131 15:33:47.717034 139884786341632 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.529768943786621, loss=4.386165142059326
I0131 15:34:34.134335 139884794734336 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.151345729827881, loss=2.4926021099090576
I0131 15:34:34.148919 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:34:44.419762 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:35:03.929026 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:35:05.575892 140085747812160 submission_runner.py:408] Time since start: 70668.47s, 	Step: 143101, 	{'train/accuracy': 0.7922265529632568, 'train/loss': 0.911641538143158, 'validation/accuracy': 0.7174999713897705, 'validation/loss': 1.237351417541504, 'validation/num_examples': 50000, 'test/accuracy': 0.5972000360488892, 'test/loss': 1.844268798828125, 'test/num_examples': 10000, 'score': 65600.1053712368, 'total_duration': 70668.46542716026, 'accumulated_submission_time': 65600.1053712368, 'accumulated_eval_time': 5053.564244508743, 'accumulated_logging_time': 6.910325288772583}
I0131 15:35:05.613993 139884786341632 logging_writer.py:48] [143101] accumulated_eval_time=5053.564245, accumulated_logging_time=6.910325, accumulated_submission_time=65600.105371, global_step=143101, preemption_count=0, score=65600.105371, test/accuracy=0.597200, test/loss=1.844269, test/num_examples=10000, total_duration=70668.465427, train/accuracy=0.792227, train/loss=0.911642, validation/accuracy=0.717500, validation/loss=1.237351, validation/num_examples=50000
I0131 15:35:46.755258 139884794734336 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.4002208709716797, loss=2.5978503227233887
I0131 15:36:33.059398 139884786341632 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.877208709716797, loss=4.663038730621338
I0131 15:37:19.725711 139884794734336 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.430577039718628, loss=2.6503591537475586
I0131 15:38:06.017918 139884786341632 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.1612131595611572, loss=2.6204309463500977
I0131 15:38:52.557743 139884794734336 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.0814239978790283, loss=3.0458874702453613
I0131 15:39:39.063243 139884786341632 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.944878101348877, loss=3.4110710620880127
I0131 15:40:25.558406 139884794734336 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.259803056716919, loss=2.556140899658203
I0131 15:41:12.179736 139884786341632 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.8536176681518555, loss=3.490365505218506
I0131 15:41:58.856960 139884794734336 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.5132365226745605, loss=2.5581562519073486
I0131 15:42:05.586755 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:42:15.712570 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:42:37.085772 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:42:38.722845 140085747812160 submission_runner.py:408] Time since start: 71121.61s, 	Step: 144016, 	{'train/accuracy': 0.7851757407188416, 'train/loss': 0.9561517238616943, 'validation/accuracy': 0.7189399600028992, 'validation/loss': 1.2434120178222656, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8443188667297363, 'test/num_examples': 10000, 'score': 66020.01851248741, 'total_duration': 71121.61238765717, 'accumulated_submission_time': 66020.01851248741, 'accumulated_eval_time': 5086.700350761414, 'accumulated_logging_time': 6.957584381103516}
I0131 15:42:38.760623 139884786341632 logging_writer.py:48] [144016] accumulated_eval_time=5086.700351, accumulated_logging_time=6.957584, accumulated_submission_time=66020.018512, global_step=144016, preemption_count=0, score=66020.018512, test/accuracy=0.600300, test/loss=1.844319, test/num_examples=10000, total_duration=71121.612388, train/accuracy=0.785176, train/loss=0.956152, validation/accuracy=0.718940, validation/loss=1.243412, validation/num_examples=50000
I0131 15:43:12.592566 139884794734336 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.282604455947876, loss=4.346324443817139
I0131 15:43:58.880599 139884786341632 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.304030418395996, loss=2.575666904449463
I0131 15:44:45.789003 139884794734336 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.182070255279541, loss=2.487117052078247
I0131 15:45:32.264621 139884786341632 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.4259345531463623, loss=2.6131410598754883
I0131 15:46:18.907578 139884794734336 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.9328246116638184, loss=2.605367422103882
I0131 15:47:05.509449 139884786341632 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.6103389263153076, loss=2.534113883972168
I0131 15:47:52.093698 139884794734336 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.601348638534546, loss=2.5328304767608643
I0131 15:48:38.697587 139884786341632 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.1668715476989746, loss=2.919062376022339
I0131 15:49:25.257476 139884794734336 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.2965574264526367, loss=2.5122478008270264
I0131 15:49:38.914628 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:49:49.422639 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:50:10.584218 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:50:12.224718 140085747812160 submission_runner.py:408] Time since start: 71575.11s, 	Step: 144931, 	{'train/accuracy': 0.7807421684265137, 'train/loss': 0.9476374387741089, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.2305222749710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.8433793783187866, 'test/num_examples': 10000, 'score': 66440.11183166504, 'total_duration': 71575.11425447464, 'accumulated_submission_time': 66440.11183166504, 'accumulated_eval_time': 5120.0104303359985, 'accumulated_logging_time': 7.005602598190308}
I0131 15:50:12.262020 139884786341632 logging_writer.py:48] [144931] accumulated_eval_time=5120.010430, accumulated_logging_time=7.005603, accumulated_submission_time=66440.111832, global_step=144931, preemption_count=0, score=66440.111832, test/accuracy=0.593700, test/loss=1.843379, test/num_examples=10000, total_duration=71575.114254, train/accuracy=0.780742, train/loss=0.947637, validation/accuracy=0.718200, validation/loss=1.230522, validation/num_examples=50000
I0131 15:50:39.734080 139884794734336 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.0996010303497314, loss=3.268132448196411
I0131 15:51:25.632739 139884786341632 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.423969268798828, loss=2.643036365509033
I0131 15:52:12.229289 139884794734336 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.2854549884796143, loss=2.5891880989074707
I0131 15:52:59.038175 139884786341632 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.3995776176452637, loss=2.468956708908081
I0131 15:53:45.299750 139884794734336 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.329829216003418, loss=2.560939311981201
I0131 15:54:32.123490 139884786341632 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.3919413089752197, loss=2.4964497089385986
I0131 15:55:18.713165 139884794734336 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.4217376708984375, loss=4.250113487243652
I0131 15:56:05.233316 139884786341632 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.2895286083221436, loss=2.4408462047576904
I0131 15:56:51.836663 139884794734336 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.223227024078369, loss=3.5483145713806152
I0131 15:57:12.429434 140085747812160 spec.py:321] Evaluating on the training split.
I0131 15:57:23.052600 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 15:57:41.784762 140085747812160 spec.py:349] Evaluating on the test split.
I0131 15:57:43.438434 140085747812160 submission_runner.py:408] Time since start: 72026.33s, 	Step: 145846, 	{'train/accuracy': 0.7958202958106995, 'train/loss': 0.9202547073364258, 'validation/accuracy': 0.7206000089645386, 'validation/loss': 1.2377933263778687, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.8521920442581177, 'test/num_examples': 10000, 'score': 66860.22039985657, 'total_duration': 72026.32795882225, 'accumulated_submission_time': 66860.22039985657, 'accumulated_eval_time': 5151.019411563873, 'accumulated_logging_time': 7.051945686340332}
I0131 15:57:43.483974 139884786341632 logging_writer.py:48] [145846] accumulated_eval_time=5151.019412, accumulated_logging_time=7.051946, accumulated_submission_time=66860.220400, global_step=145846, preemption_count=0, score=66860.220400, test/accuracy=0.601400, test/loss=1.852192, test/num_examples=10000, total_duration=72026.327959, train/accuracy=0.795820, train/loss=0.920255, validation/accuracy=0.720600, validation/loss=1.237793, validation/num_examples=50000
I0131 15:58:05.104057 139884794734336 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.122467041015625, loss=3.957512855529785
I0131 15:58:50.534751 139884786341632 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.5176920890808105, loss=2.6621925830841064
I0131 15:59:37.095965 139884794734336 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.2208313941955566, loss=2.49948787689209
I0131 16:00:23.876031 139884786341632 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.438694953918457, loss=2.5000228881835938
I0131 16:01:10.248879 139884794734336 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.4637279510498047, loss=2.745532274246216
I0131 16:01:56.826688 139884786341632 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.3901255130767822, loss=3.100229501724243
I0131 16:02:43.674211 139884794734336 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.924067974090576, loss=4.627799987792969
I0131 16:03:30.106035 139884786341632 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.410281181335449, loss=2.545536756515503
I0131 16:04:16.940973 139884794734336 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.484548807144165, loss=2.5112359523773193
I0131 16:04:43.643947 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:04:53.888231 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:05:15.420699 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:05:17.058988 140085747812160 submission_runner.py:408] Time since start: 72479.95s, 	Step: 146759, 	{'train/accuracy': 0.7870507836341858, 'train/loss': 0.943701982498169, 'validation/accuracy': 0.7204200029373169, 'validation/loss': 1.2290470600128174, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8332223892211914, 'test/num_examples': 10000, 'score': 67280.31985378265, 'total_duration': 72479.94853115082, 'accumulated_submission_time': 67280.31985378265, 'accumulated_eval_time': 5184.43443775177, 'accumulated_logging_time': 7.107837200164795}
I0131 16:05:17.098430 139884786341632 logging_writer.py:48] [146759] accumulated_eval_time=5184.434438, accumulated_logging_time=7.107837, accumulated_submission_time=67280.319854, global_step=146759, preemption_count=0, score=67280.319854, test/accuracy=0.601200, test/loss=1.833222, test/num_examples=10000, total_duration=72479.948531, train/accuracy=0.787051, train/loss=0.943702, validation/accuracy=0.720420, validation/loss=1.229047, validation/num_examples=50000
I0131 16:05:33.581533 139884794734336 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.776768684387207, loss=4.644071102142334
I0131 16:06:17.860059 139884786341632 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.5371596813201904, loss=2.8305773735046387
I0131 16:07:04.639545 139884794734336 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.5213449001312256, loss=2.5266337394714355
I0131 16:07:51.371857 139884786341632 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.215096950531006, loss=2.611788034439087
I0131 16:08:37.757379 139884794734336 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.205012798309326, loss=3.7965354919433594
I0131 16:09:24.361099 139884786341632 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.9705817699432373, loss=2.4835903644561768
I0131 16:10:11.184887 139884794734336 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.400017023086548, loss=3.612539529800415
I0131 16:10:57.797786 139884786341632 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.413062810897827, loss=3.210092544555664
I0131 16:11:44.225050 139884794734336 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.878513813018799, loss=2.4879376888275146
I0131 16:12:17.374376 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:12:27.620400 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:12:49.731774 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:12:51.372362 140085747812160 submission_runner.py:408] Time since start: 72934.26s, 	Step: 147673, 	{'train/accuracy': 0.79359370470047, 'train/loss': 0.9170867204666138, 'validation/accuracy': 0.7231999635696411, 'validation/loss': 1.2131633758544922, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.8122233152389526, 'test/num_examples': 10000, 'score': 67700.53570318222, 'total_duration': 72934.26190567017, 'accumulated_submission_time': 67700.53570318222, 'accumulated_eval_time': 5218.43242764473, 'accumulated_logging_time': 7.157444715499878}
I0131 16:12:51.411505 139884786341632 logging_writer.py:48] [147673] accumulated_eval_time=5218.432428, accumulated_logging_time=7.157445, accumulated_submission_time=67700.535703, global_step=147673, preemption_count=0, score=67700.535703, test/accuracy=0.598700, test/loss=1.812223, test/num_examples=10000, total_duration=72934.261906, train/accuracy=0.793594, train/loss=0.917087, validation/accuracy=0.723200, validation/loss=1.213163, validation/num_examples=50000
I0131 16:13:02.504998 139884794734336 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.3168814182281494, loss=2.7829904556274414
I0131 16:13:44.592760 139884786341632 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.4489519596099854, loss=2.5072832107543945
I0131 16:14:30.823110 139884794734336 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.3148584365844727, loss=2.4490277767181396
I0131 16:15:17.500022 139884786341632 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.552905321121216, loss=2.5146474838256836
I0131 16:16:03.722349 139884794734336 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.5951693058013916, loss=2.7716047763824463
I0131 16:16:50.139811 139884786341632 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.7980544567108154, loss=2.647860050201416
I0131 16:17:36.608316 139884794734336 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.197744369506836, loss=2.7433595657348633
I0131 16:18:22.974269 139884786341632 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.5231337547302246, loss=3.638941764831543
I0131 16:19:09.336056 139884794734336 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.358323097229004, loss=4.0122809410095215
I0131 16:19:51.419354 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:20:01.519664 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:20:23.682691 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:20:25.314278 140085747812160 submission_runner.py:408] Time since start: 73388.20s, 	Step: 148592, 	{'train/accuracy': 0.7974804639816284, 'train/loss': 0.8924198150634766, 'validation/accuracy': 0.7242599725723267, 'validation/loss': 1.2011919021606445, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.801270842552185, 'test/num_examples': 10000, 'score': 68120.48303842545, 'total_duration': 73388.2038257122, 'accumulated_submission_time': 68120.48303842545, 'accumulated_eval_time': 5252.327347993851, 'accumulated_logging_time': 7.205964088439941}
I0131 16:20:25.350714 139884786341632 logging_writer.py:48] [148592] accumulated_eval_time=5252.327348, accumulated_logging_time=7.205964, accumulated_submission_time=68120.483038, global_step=148592, preemption_count=0, score=68120.483038, test/accuracy=0.602800, test/loss=1.801271, test/num_examples=10000, total_duration=73388.203826, train/accuracy=0.797480, train/loss=0.892420, validation/accuracy=0.724260, validation/loss=1.201192, validation/num_examples=50000
I0131 16:20:28.886735 139884794734336 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.426173686981201, loss=2.4505879878997803
I0131 16:21:10.449497 139884786341632 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.430215358734131, loss=2.386007785797119
I0131 16:21:56.615054 139884794734336 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.8124048709869385, loss=4.4380340576171875
I0131 16:22:43.415519 139884786341632 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.528651714324951, loss=2.679478168487549
I0131 16:23:30.114282 139884794734336 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8713607788085938, loss=2.4261422157287598
I0131 16:24:16.589756 139884786341632 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.467604160308838, loss=2.5031018257141113
I0131 16:25:03.179924 139884794734336 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.7423698902130127, loss=2.508608102798462
I0131 16:25:49.462906 139884786341632 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.934072256088257, loss=2.8971800804138184
I0131 16:26:36.288673 139884794734336 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.011508941650391, loss=4.480707168579102
I0131 16:27:22.595966 139884786341632 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.074211597442627, loss=2.3802566528320312
I0131 16:27:25.499454 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:27:35.687542 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:27:57.472099 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:27:59.122231 140085747812160 submission_runner.py:408] Time since start: 73842.01s, 	Step: 149508, 	{'train/accuracy': 0.7968554496765137, 'train/loss': 0.8914695382118225, 'validation/accuracy': 0.7272399663925171, 'validation/loss': 1.1844379901885986, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.780668020248413, 'test/num_examples': 10000, 'score': 68540.57233738899, 'total_duration': 73842.01174998283, 'accumulated_submission_time': 68540.57233738899, 'accumulated_eval_time': 5285.950101613998, 'accumulated_logging_time': 7.251769781112671}
I0131 16:27:59.160591 139884794734336 logging_writer.py:48] [149508] accumulated_eval_time=5285.950102, accumulated_logging_time=7.251770, accumulated_submission_time=68540.572337, global_step=149508, preemption_count=0, score=68540.572337, test/accuracy=0.607600, test/loss=1.780668, test/num_examples=10000, total_duration=73842.011750, train/accuracy=0.796855, train/loss=0.891470, validation/accuracy=0.727240, validation/loss=1.184438, validation/num_examples=50000
I0131 16:28:36.835128 139884786341632 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.3558037281036377, loss=3.112781047821045
I0131 16:29:23.294554 139884794734336 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.2990403175354004, loss=3.4601070880889893
I0131 16:30:09.920454 139884786341632 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.7768754959106445, loss=2.559271812438965
I0131 16:30:56.948638 139884794734336 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.875802993774414, loss=2.5185325145721436
I0131 16:31:42.907726 139884786341632 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.810128927230835, loss=2.5577030181884766
I0131 16:32:29.684913 139884794734336 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.5664303302764893, loss=3.425159215927124
I0131 16:33:16.160029 139884786341632 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.8722283840179443, loss=2.498318672180176
I0131 16:34:02.588763 139884794734336 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.064248085021973, loss=4.133264064788818
I0131 16:34:49.145475 139884786341632 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.355546474456787, loss=3.6547775268554688
I0131 16:34:59.450781 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:35:09.701834 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:35:29.386519 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:35:31.039967 140085747812160 submission_runner.py:408] Time since start: 74293.93s, 	Step: 150424, 	{'train/accuracy': 0.7983788847923279, 'train/loss': 0.8947476744651794, 'validation/accuracy': 0.7258999943733215, 'validation/loss': 1.197722315788269, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7993606328964233, 'test/num_examples': 10000, 'score': 68960.80321502686, 'total_duration': 74293.92950153351, 'accumulated_submission_time': 68960.80321502686, 'accumulated_eval_time': 5317.539261579514, 'accumulated_logging_time': 7.299367904663086}
I0131 16:35:31.089601 139884794734336 logging_writer.py:48] [150424] accumulated_eval_time=5317.539262, accumulated_logging_time=7.299368, accumulated_submission_time=68960.803215, global_step=150424, preemption_count=0, score=68960.803215, test/accuracy=0.610100, test/loss=1.799361, test/num_examples=10000, total_duration=74293.929502, train/accuracy=0.798379, train/loss=0.894748, validation/accuracy=0.725900, validation/loss=1.197722, validation/num_examples=50000
I0131 16:36:01.765805 139884786341632 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.4944841861724854, loss=2.5083205699920654
I0131 16:36:47.926681 139884794734336 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.499340295791626, loss=2.5216808319091797
I0131 16:37:34.391568 139884786341632 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.8060219287872314, loss=4.269171237945557
I0131 16:38:21.029328 139884794734336 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.4954299926757812, loss=3.1247718334198
I0131 16:39:07.387763 139884786341632 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.7024104595184326, loss=2.392547369003296
I0131 16:39:53.957072 139884794734336 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.825634241104126, loss=2.388857364654541
I0131 16:40:40.392888 139884786341632 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.77034854888916, loss=2.5156450271606445
I0131 16:41:26.968470 139884794734336 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.0099077224731445, loss=2.464045286178589
I0131 16:42:13.635768 139884786341632 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.7498176097869873, loss=4.207904815673828
I0131 16:42:31.110279 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:42:41.290467 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:43:01.166397 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:43:02.810130 140085747812160 submission_runner.py:408] Time since start: 74745.70s, 	Step: 151339, 	{'train/accuracy': 0.8019726276397705, 'train/loss': 0.8627512454986572, 'validation/accuracy': 0.7283200025558472, 'validation/loss': 1.176479458808899, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7731417417526245, 'test/num_examples': 10000, 'score': 69380.76213383675, 'total_duration': 74745.69967579842, 'accumulated_submission_time': 69380.76213383675, 'accumulated_eval_time': 5349.239114522934, 'accumulated_logging_time': 7.360641241073608}
I0131 16:43:02.849685 139884794734336 logging_writer.py:48] [151339] accumulated_eval_time=5349.239115, accumulated_logging_time=7.360641, accumulated_submission_time=69380.762134, global_step=151339, preemption_count=0, score=69380.762134, test/accuracy=0.613500, test/loss=1.773142, test/num_examples=10000, total_duration=74745.699676, train/accuracy=0.801973, train/loss=0.862751, validation/accuracy=0.728320, validation/loss=1.176479, validation/num_examples=50000
I0131 16:43:27.184746 139884786341632 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.7749147415161133, loss=3.8934736251831055
I0131 16:44:12.494769 139884794734336 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.3028242588043213, loss=3.3151919841766357
I0131 16:44:59.180740 139884786341632 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.0335259437561035, loss=2.5111241340637207
I0131 16:45:46.104522 139884794734336 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.80733060836792, loss=2.516864061355591
I0131 16:46:32.690647 139884786341632 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.6970443725585938, loss=2.8812341690063477
I0131 16:47:19.454749 139884794734336 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.129729270935059, loss=2.4045419692993164
I0131 16:48:05.932836 139884786341632 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.467815399169922, loss=2.468035936355591
I0131 16:48:52.541718 139884794734336 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.4129109382629395, loss=2.5396852493286133
I0131 16:49:39.132702 139884786341632 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.5823726654052734, loss=2.445544958114624
I0131 16:50:02.917286 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:50:13.138115 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:50:34.765622 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:50:36.407718 140085747812160 submission_runner.py:408] Time since start: 75199.30s, 	Step: 152253, 	{'train/accuracy': 0.8101562261581421, 'train/loss': 0.8360294103622437, 'validation/accuracy': 0.7314800024032593, 'validation/loss': 1.171595811843872, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.764483094215393, 'test/num_examples': 10000, 'score': 69800.7703230381, 'total_duration': 75199.2972612381, 'accumulated_submission_time': 69800.7703230381, 'accumulated_eval_time': 5382.729545354843, 'accumulated_logging_time': 7.409675359725952}
I0131 16:50:36.445740 139884794734336 logging_writer.py:48] [152253] accumulated_eval_time=5382.729545, accumulated_logging_time=7.409675, accumulated_submission_time=69800.770323, global_step=152253, preemption_count=0, score=69800.770323, test/accuracy=0.611200, test/loss=1.764483, test/num_examples=10000, total_duration=75199.297261, train/accuracy=0.810156, train/loss=0.836029, validation/accuracy=0.731480, validation/loss=1.171596, validation/num_examples=50000
I0131 16:50:55.298307 139884786341632 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7502357959747314, loss=3.6669862270355225
I0131 16:51:39.419960 139884794734336 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.8568079471588135, loss=2.6171813011169434
I0131 16:52:26.003843 139884786341632 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.7116143703460693, loss=2.643658399581909
I0131 16:53:12.664709 139884794734336 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.6543796062469482, loss=2.8001182079315186
I0131 16:53:59.433533 139884786341632 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.6835899353027344, loss=2.3768768310546875
I0131 16:54:45.927922 139884794734336 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.125522613525391, loss=2.47393536567688
I0131 16:55:32.555942 139884786341632 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.571382999420166, loss=4.131556987762451
I0131 16:56:19.073374 139884794734336 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.6961538791656494, loss=3.451667547225952
I0131 16:57:05.846967 139884786341632 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.211040019989014, loss=3.9976394176483154
I0131 16:57:36.569425 140085747812160 spec.py:321] Evaluating on the training split.
I0131 16:57:47.113603 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 16:58:06.930850 140085747812160 spec.py:349] Evaluating on the test split.
I0131 16:58:08.579117 140085747812160 submission_runner.py:408] Time since start: 75651.47s, 	Step: 153168, 	{'train/accuracy': 0.8065429329872131, 'train/loss': 0.850128710269928, 'validation/accuracy': 0.7335399985313416, 'validation/loss': 1.1555325984954834, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.7503547668457031, 'test/num_examples': 10000, 'score': 70220.83098077774, 'total_duration': 75651.4686627388, 'accumulated_submission_time': 70220.83098077774, 'accumulated_eval_time': 5414.739239692688, 'accumulated_logging_time': 7.460782766342163}
I0131 16:58:08.619681 139884794734336 logging_writer.py:48] [153168] accumulated_eval_time=5414.739240, accumulated_logging_time=7.460783, accumulated_submission_time=70220.830981, global_step=153168, preemption_count=0, score=70220.830981, test/accuracy=0.614000, test/loss=1.750355, test/num_examples=10000, total_duration=75651.468663, train/accuracy=0.806543, train/loss=0.850129, validation/accuracy=0.733540, validation/loss=1.155533, validation/num_examples=50000
I0131 16:58:21.572036 139884786341632 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.9453468322753906, loss=2.575665235519409
I0131 16:59:05.307645 139884794734336 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.210642337799072, loss=2.899343967437744
I0131 16:59:51.804291 139884786341632 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.9992074966430664, loss=2.350252866744995
I0131 17:00:38.331556 139884794734336 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.816382646560669, loss=3.258009433746338
I0131 17:01:24.760732 139884786341632 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.213654041290283, loss=2.49800181388855
I0131 17:02:11.364344 139884794734336 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.8123700618743896, loss=2.487009048461914
I0131 17:02:58.315662 139884786341632 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.63698410987854, loss=3.1388769149780273
I0131 17:03:44.754629 139884794734336 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.821056604385376, loss=2.364720582962036
I0131 17:04:31.083313 139884786341632 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.2610249519348145, loss=2.3900763988494873
I0131 17:05:08.781059 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:05:18.927902 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:05:41.134159 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:05:42.770571 140085747812160 submission_runner.py:408] Time since start: 76105.66s, 	Step: 154083, 	{'train/accuracy': 0.8090429306030273, 'train/loss': 0.8482322692871094, 'validation/accuracy': 0.7348799705505371, 'validation/loss': 1.1653943061828613, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.7686083316802979, 'test/num_examples': 10000, 'score': 70640.93357086182, 'total_duration': 76105.66011571884, 'accumulated_submission_time': 70640.93357086182, 'accumulated_eval_time': 5448.728743553162, 'accumulated_logging_time': 7.51036524772644}
I0131 17:05:42.817537 139884794734336 logging_writer.py:48] [154083] accumulated_eval_time=5448.728744, accumulated_logging_time=7.510365, accumulated_submission_time=70640.933571, global_step=154083, preemption_count=0, score=70640.933571, test/accuracy=0.615900, test/loss=1.768608, test/num_examples=10000, total_duration=76105.660116, train/accuracy=0.809043, train/loss=0.848232, validation/accuracy=0.734880, validation/loss=1.165394, validation/num_examples=50000
I0131 17:05:49.896897 139884786341632 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.698037147521973, loss=4.430337905883789
I0131 17:06:32.324275 139884794734336 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.210200309753418, loss=2.5734689235687256
I0131 17:07:18.992771 139884786341632 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.166494846343994, loss=2.715941905975342
I0131 17:08:06.061741 139884794734336 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.3524861335754395, loss=2.3586819171905518
I0131 17:08:52.347474 139884786341632 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.8528013229370117, loss=3.029242515563965
I0131 17:09:38.861977 139884794734336 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.046011447906494, loss=2.4038076400756836
I0131 17:10:25.428488 139884786341632 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.215269088745117, loss=2.425694465637207
I0131 17:11:11.897574 139884794734336 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.27777624130249, loss=2.582160472869873
I0131 17:11:59.181652 139884786341632 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.2051496505737305, loss=2.414612054824829
I0131 17:12:43.169874 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:12:53.395857 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:13:14.081329 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:13:15.725373 140085747812160 submission_runner.py:408] Time since start: 76558.61s, 	Step: 154996, 	{'train/accuracy': 0.8161718845367432, 'train/loss': 0.8130109310150146, 'validation/accuracy': 0.7342199683189392, 'validation/loss': 1.1588497161865234, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7606123685836792, 'test/num_examples': 10000, 'score': 71061.22500658035, 'total_duration': 76558.61492061615, 'accumulated_submission_time': 71061.22500658035, 'accumulated_eval_time': 5481.284249305725, 'accumulated_logging_time': 7.567243576049805}
I0131 17:13:15.763734 139884794734336 logging_writer.py:48] [154996] accumulated_eval_time=5481.284249, accumulated_logging_time=7.567244, accumulated_submission_time=71061.225007, global_step=154996, preemption_count=0, score=71061.225007, test/accuracy=0.614300, test/loss=1.760612, test/num_examples=10000, total_duration=76558.614921, train/accuracy=0.816172, train/loss=0.813011, validation/accuracy=0.734220, validation/loss=1.158850, validation/num_examples=50000
I0131 17:13:17.726814 139884786341632 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.828829050064087, loss=2.9394142627716064
I0131 17:13:59.162725 139884794734336 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.99835467338562, loss=2.5353121757507324
I0131 17:14:45.661629 139884786341632 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.210378170013428, loss=2.4285452365875244
I0131 17:15:32.324062 139884794734336 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.787182331085205, loss=2.843395233154297
I0131 17:16:19.036481 139884786341632 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.093016624450684, loss=2.436319589614868
I0131 17:17:05.505838 139884794734336 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.531345844268799, loss=4.448711395263672
I0131 17:17:52.207041 139884786341632 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.717799663543701, loss=2.4211201667785645
I0131 17:18:38.635568 139884794734336 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.006226539611816, loss=3.015172004699707
I0131 17:19:25.203254 139884786341632 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.765045166015625, loss=2.401275396347046
I0131 17:20:11.398706 139884794734336 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.8369245529174805, loss=2.790356159210205
I0131 17:20:16.083808 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:20:26.217724 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:20:47.144513 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:20:48.782772 140085747812160 submission_runner.py:408] Time since start: 77011.67s, 	Step: 155912, 	{'train/accuracy': 0.8117773532867432, 'train/loss': 0.8497369289398193, 'validation/accuracy': 0.7390799522399902, 'validation/loss': 1.1490964889526367, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.7688266038894653, 'test/num_examples': 10000, 'score': 71481.48496103287, 'total_duration': 77011.67231607437, 'accumulated_submission_time': 71481.48496103287, 'accumulated_eval_time': 5513.983215093613, 'accumulated_logging_time': 7.616268157958984}
I0131 17:20:48.821164 139884786341632 logging_writer.py:48] [155912] accumulated_eval_time=5513.983215, accumulated_logging_time=7.616268, accumulated_submission_time=71481.484961, global_step=155912, preemption_count=0, score=71481.484961, test/accuracy=0.616400, test/loss=1.768827, test/num_examples=10000, total_duration=77011.672316, train/accuracy=0.811777, train/loss=0.849737, validation/accuracy=0.739080, validation/loss=1.149096, validation/num_examples=50000
I0131 17:21:24.718541 139884794734336 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.1174397468566895, loss=2.474411964416504
I0131 17:22:11.254150 139884786341632 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.502464294433594, loss=2.4924733638763428
I0131 17:22:58.389639 139884794734336 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.2247209548950195, loss=2.4220728874206543
I0131 17:23:45.138563 139884786341632 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.149582862854004, loss=3.843905448913574
I0131 17:24:31.808136 139884794734336 logging_writer.py:48] [156400] global_step=156400, grad_norm=5.561456203460693, loss=4.486193656921387
I0131 17:25:18.170239 139884786341632 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.057732582092285, loss=2.394543409347534
I0131 17:26:04.814013 139884794734336 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.132514953613281, loss=2.5373239517211914
I0131 17:26:51.309810 139884786341632 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.48155403137207, loss=2.416386842727661
I0131 17:27:37.908690 139884794734336 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.227214813232422, loss=2.3761861324310303
I0131 17:27:48.796535 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:27:58.954286 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:28:19.367783 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:28:21.012972 140085747812160 submission_runner.py:408] Time since start: 77463.90s, 	Step: 156825, 	{'train/accuracy': 0.8138867020606995, 'train/loss': 0.8235838413238525, 'validation/accuracy': 0.7387599945068359, 'validation/loss': 1.1445491313934326, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.7345569133758545, 'test/num_examples': 10000, 'score': 71901.40081095695, 'total_duration': 77463.90251994133, 'accumulated_submission_time': 71901.40081095695, 'accumulated_eval_time': 5546.199652433395, 'accumulated_logging_time': 7.663997650146484}
I0131 17:28:21.052283 139884786341632 logging_writer.py:48] [156825] accumulated_eval_time=5546.199652, accumulated_logging_time=7.663998, accumulated_submission_time=71901.400811, global_step=156825, preemption_count=0, score=71901.400811, test/accuracy=0.617900, test/loss=1.734557, test/num_examples=10000, total_duration=77463.902520, train/accuracy=0.813887, train/loss=0.823584, validation/accuracy=0.738760, validation/loss=1.144549, validation/num_examples=50000
I0131 17:28:50.958275 139884794734336 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.378652572631836, loss=2.4892070293426514
I0131 17:29:37.145548 139884786341632 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.6743903160095215, loss=2.296552896499634
I0131 17:30:23.856434 139884794734336 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.108972549438477, loss=2.3834121227264404
I0131 17:31:10.443228 139884786341632 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.056256294250488, loss=3.5382888317108154
I0131 17:31:56.909451 139884794734336 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.5435380935668945, loss=2.4285244941711426
I0131 17:32:43.892938 139884786341632 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.913051128387451, loss=2.36327862739563
I0131 17:33:30.053159 139884794734336 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.924886703491211, loss=2.329599618911743
I0131 17:34:16.804479 139884786341632 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.321388244628906, loss=2.371366262435913
I0131 17:35:03.386001 139884794734336 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.261392593383789, loss=2.34419322013855
I0131 17:35:21.089166 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:35:31.323322 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:35:53.041151 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:35:54.682627 140085747812160 submission_runner.py:408] Time since start: 77917.57s, 	Step: 157740, 	{'train/accuracy': 0.8207616806030273, 'train/loss': 0.800974428653717, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.1447432041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.7404789924621582, 'test/num_examples': 10000, 'score': 72321.37640357018, 'total_duration': 77917.57216620445, 'accumulated_submission_time': 72321.37640357018, 'accumulated_eval_time': 5579.793093681335, 'accumulated_logging_time': 7.714296340942383}
I0131 17:35:54.724320 139884786341632 logging_writer.py:48] [157740] accumulated_eval_time=5579.793094, accumulated_logging_time=7.714296, accumulated_submission_time=72321.376404, global_step=157740, preemption_count=0, score=72321.376404, test/accuracy=0.621200, test/loss=1.740479, test/num_examples=10000, total_duration=77917.572166, train/accuracy=0.820762, train/loss=0.800974, validation/accuracy=0.739720, validation/loss=1.144743, validation/num_examples=50000
I0131 17:36:18.666479 139884794734336 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.223545551300049, loss=2.553450584411621
I0131 17:37:03.344031 139884786341632 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.212522506713867, loss=2.453709125518799
I0131 17:37:50.077741 139884794734336 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.233813285827637, loss=2.389967441558838
I0131 17:38:36.994955 139884786341632 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.268279552459717, loss=2.3182373046875
I0131 17:39:23.583286 139884794734336 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.9887561798095703, loss=3.6059134006500244
I0131 17:40:10.640153 139884786341632 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.333523273468018, loss=2.5270297527313232
I0131 17:40:57.026027 139884794734336 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.360614776611328, loss=2.424269199371338
I0131 17:41:43.546351 139884786341632 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.555657386779785, loss=2.595992088317871
I0131 17:42:30.247275 139884794734336 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.909698486328125, loss=4.095012664794922
I0131 17:42:54.700397 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:43:05.088136 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:43:26.805595 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:43:28.459439 140085747812160 submission_runner.py:408] Time since start: 78371.35s, 	Step: 158654, 	{'train/accuracy': 0.81507807970047, 'train/loss': 0.812286913394928, 'validation/accuracy': 0.7413399815559387, 'validation/loss': 1.1248809099197388, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.7191606760025024, 'test/num_examples': 10000, 'score': 72741.29397368431, 'total_duration': 78371.34893417358, 'accumulated_submission_time': 72741.29397368431, 'accumulated_eval_time': 5613.552079439163, 'accumulated_logging_time': 7.765320539474487}
I0131 17:43:28.512989 139884786341632 logging_writer.py:48] [158654] accumulated_eval_time=5613.552079, accumulated_logging_time=7.765321, accumulated_submission_time=72741.293974, global_step=158654, preemption_count=0, score=72741.293974, test/accuracy=0.625700, test/loss=1.719161, test/num_examples=10000, total_duration=78371.348934, train/accuracy=0.815078, train/loss=0.812287, validation/accuracy=0.741340, validation/loss=1.124881, validation/num_examples=50000
I0131 17:43:46.957916 139884794734336 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.32756233215332, loss=3.326171398162842
I0131 17:44:31.202552 139884786341632 logging_writer.py:48] [158800] global_step=158800, grad_norm=5.182661056518555, loss=4.010507583618164
I0131 17:45:18.189784 139884794734336 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.250816822052002, loss=3.06392502784729
I0131 17:46:05.104084 139884786341632 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.480186939239502, loss=2.39011549949646
I0131 17:46:51.264357 139884794734336 logging_writer.py:48] [159100] global_step=159100, grad_norm=3.938549757003784, loss=2.7232894897460938
I0131 17:47:38.022714 139884786341632 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.400202751159668, loss=3.1143529415130615
I0131 17:48:24.535411 139884794734336 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.376198768615723, loss=2.4618301391601562
I0131 17:49:11.032929 139884786341632 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.694644451141357, loss=2.431065559387207
I0131 17:49:57.534976 139884794734336 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.411555290222168, loss=2.319305896759033
I0131 17:50:28.877638 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:50:39.172471 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:50:59.318829 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:51:00.964648 140085747812160 submission_runner.py:408] Time since start: 78823.85s, 	Step: 159569, 	{'train/accuracy': 0.8191210627555847, 'train/loss': 0.8072444200515747, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.1267979145050049, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.720393419265747, 'test/num_examples': 10000, 'score': 73161.59970808029, 'total_duration': 78823.85418653488, 'accumulated_submission_time': 73161.59970808029, 'accumulated_eval_time': 5645.6390788555145, 'accumulated_logging_time': 7.827660083770752}
I0131 17:51:01.004174 139884786341632 logging_writer.py:48] [159569] accumulated_eval_time=5645.639079, accumulated_logging_time=7.827660, accumulated_submission_time=73161.599708, global_step=159569, preemption_count=0, score=73161.599708, test/accuracy=0.626000, test/loss=1.720393, test/num_examples=10000, total_duration=78823.854187, train/accuracy=0.819121, train/loss=0.807244, validation/accuracy=0.743280, validation/loss=1.126798, validation/num_examples=50000
I0131 17:51:13.596415 139884794734336 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.120426654815674, loss=3.612583875656128
I0131 17:51:56.742152 139884786341632 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.634108066558838, loss=3.7441699504852295
I0131 17:52:43.578859 139884794734336 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.380425453186035, loss=2.321308135986328
I0131 17:53:30.701938 139884786341632 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.627361297607422, loss=3.331549644470215
I0131 17:54:17.231667 139884794734336 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.902771472930908, loss=4.087365627288818
I0131 17:55:03.881834 139884786341632 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.2303547859191895, loss=3.5898499488830566
I0131 17:56:06.591383 139884794734336 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.949488639831543, loss=3.75502347946167
I0131 17:56:55.279727 139884786341632 logging_writer.py:48] [160300] global_step=160300, grad_norm=5.220884323120117, loss=4.226953983306885
I0131 17:57:41.980529 139884794734336 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.442016124725342, loss=2.449759006500244
I0131 17:58:00.999091 140085747812160 spec.py:321] Evaluating on the training split.
I0131 17:58:11.484579 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 17:58:34.082427 140085747812160 spec.py:349] Evaluating on the test split.
I0131 17:58:35.723175 140085747812160 submission_runner.py:408] Time since start: 79278.61s, 	Step: 160442, 	{'train/accuracy': 0.8229101300239563, 'train/loss': 0.7834881544113159, 'validation/accuracy': 0.744659960269928, 'validation/loss': 1.1210800409317017, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.7195051908493042, 'test/num_examples': 10000, 'score': 73581.53504562378, 'total_duration': 79278.61271739006, 'accumulated_submission_time': 73581.53504562378, 'accumulated_eval_time': 5680.3631727695465, 'accumulated_logging_time': 7.879194021224976}
I0131 17:58:35.764352 139884786341632 logging_writer.py:48] [160442] accumulated_eval_time=5680.363173, accumulated_logging_time=7.879194, accumulated_submission_time=73581.535046, global_step=160442, preemption_count=0, score=73581.535046, test/accuracy=0.627100, test/loss=1.719505, test/num_examples=10000, total_duration=79278.612717, train/accuracy=0.822910, train/loss=0.783488, validation/accuracy=0.744660, validation/loss=1.121080, validation/num_examples=50000
I0131 17:58:58.907321 139884794734336 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.391048908233643, loss=2.230522632598877
I0131 17:59:44.395971 139884786341632 logging_writer.py:48] [160600] global_step=160600, grad_norm=5.195230960845947, loss=4.134737014770508
I0131 18:00:30.874291 139884794734336 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.8753838539123535, loss=2.451442003250122
I0131 18:01:17.587836 139884786341632 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.7346649169921875, loss=3.7968995571136475
I0131 18:02:04.046670 139884794734336 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.770632743835449, loss=2.584559679031372
I0131 18:02:50.680553 139884786341632 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.768653392791748, loss=2.7071452140808105
I0131 18:03:37.280402 139884794734336 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.53788948059082, loss=2.243035316467285
I0131 18:04:23.880385 139884786341632 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.479496479034424, loss=2.3434834480285645
I0131 18:05:10.144264 139884794734336 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.768067836761475, loss=2.3828368186950684
I0131 18:05:36.105346 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:05:46.516292 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:06:05.854441 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:06:07.506300 140085747812160 submission_runner.py:408] Time since start: 79730.40s, 	Step: 161358, 	{'train/accuracy': 0.82289057970047, 'train/loss': 0.7732518315315247, 'validation/accuracy': 0.7462199926376343, 'validation/loss': 1.0985074043273926, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.683404803276062, 'test/num_examples': 10000, 'score': 74001.81647443771, 'total_duration': 79730.39583301544, 'accumulated_submission_time': 74001.81647443771, 'accumulated_eval_time': 5711.764150619507, 'accumulated_logging_time': 7.929988384246826}
I0131 18:06:07.554995 139884786341632 logging_writer.py:48] [161358] accumulated_eval_time=5711.764151, accumulated_logging_time=7.929988, accumulated_submission_time=74001.816474, global_step=161358, preemption_count=0, score=74001.816474, test/accuracy=0.632600, test/loss=1.683405, test/num_examples=10000, total_duration=79730.395833, train/accuracy=0.822891, train/loss=0.773252, validation/accuracy=0.746220, validation/loss=1.098507, validation/num_examples=50000
I0131 18:06:24.438812 139884794734336 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.196041107177734, loss=2.8996143341064453
I0131 18:07:08.912197 139884786341632 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.24624490737915, loss=3.3354320526123047
I0131 18:07:55.935058 139884794734336 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.53772497177124, loss=2.385517120361328
I0131 18:08:42.721239 139884786341632 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.784184455871582, loss=2.3898205757141113
I0131 18:09:29.137871 139884794734336 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.480059623718262, loss=2.262418270111084
I0131 18:10:15.767251 139884786341632 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.231372356414795, loss=4.058050155639648
I0131 18:11:02.207271 139884794734336 logging_writer.py:48] [162000] global_step=162000, grad_norm=5.040839672088623, loss=2.428605794906616
I0131 18:11:48.658273 139884786341632 logging_writer.py:48] [162100] global_step=162100, grad_norm=6.156623840332031, loss=4.3218536376953125
I0131 18:12:35.462855 139884794734336 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.320743083953857, loss=2.389997959136963
I0131 18:13:07.787756 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:13:18.410828 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:13:38.282536 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:13:39.934684 140085747812160 submission_runner.py:408] Time since start: 80182.82s, 	Step: 162271, 	{'train/accuracy': 0.8252733945846558, 'train/loss': 0.7839264273643494, 'validation/accuracy': 0.7461000084877014, 'validation/loss': 1.112027645111084, 'validation/num_examples': 50000, 'test/accuracy': 0.632900059223175, 'test/loss': 1.7059128284454346, 'test/num_examples': 10000, 'score': 74421.98902630806, 'total_duration': 80182.82422280312, 'accumulated_submission_time': 74421.98902630806, 'accumulated_eval_time': 5743.91107749939, 'accumulated_logging_time': 7.989051818847656}
I0131 18:13:39.975384 139884786341632 logging_writer.py:48] [162271] accumulated_eval_time=5743.911077, accumulated_logging_time=7.989052, accumulated_submission_time=74421.989026, global_step=162271, preemption_count=0, score=74421.989026, test/accuracy=0.632900, test/loss=1.705913, test/num_examples=10000, total_duration=80182.824223, train/accuracy=0.825273, train/loss=0.783926, validation/accuracy=0.746100, validation/loss=1.112028, validation/num_examples=50000
I0131 18:13:51.744384 139884794734336 logging_writer.py:48] [162300] global_step=162300, grad_norm=5.341925621032715, loss=2.339716911315918
I0131 18:14:34.939866 139884786341632 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.635260581970215, loss=2.323840379714966
I0131 18:15:21.202016 139884794734336 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.869153022766113, loss=2.3675875663757324
I0131 18:16:07.729922 139884786341632 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.6907243728637695, loss=2.323892116546631
I0131 18:16:54.262941 139884794734336 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.924404621124268, loss=2.312532424926758
I0131 18:17:40.802969 139884786341632 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.845522403717041, loss=4.054104328155518
I0131 18:18:27.571763 139884794734336 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.956879615783691, loss=3.6295080184936523
I0131 18:19:14.117509 139884786341632 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.859817981719971, loss=2.338843822479248
I0131 18:20:00.768068 139884794734336 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.318820953369141, loss=3.072408437728882
I0131 18:20:40.165882 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:20:50.708574 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:21:11.631755 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:21:13.271724 140085747812160 submission_runner.py:408] Time since start: 80636.16s, 	Step: 163186, 	{'train/accuracy': 0.8279882669448853, 'train/loss': 0.7568796277046204, 'validation/accuracy': 0.7485599517822266, 'validation/loss': 1.0927319526672363, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.6792042255401611, 'test/num_examples': 10000, 'score': 74842.11705088615, 'total_duration': 80636.16126894951, 'accumulated_submission_time': 74842.11705088615, 'accumulated_eval_time': 5777.016916275024, 'accumulated_logging_time': 8.03922438621521}
I0131 18:21:13.310665 139884786341632 logging_writer.py:48] [163186] accumulated_eval_time=5777.016916, accumulated_logging_time=8.039224, accumulated_submission_time=74842.117051, global_step=163186, preemption_count=0, score=74842.117051, test/accuracy=0.631500, test/loss=1.679204, test/num_examples=10000, total_duration=80636.161269, train/accuracy=0.827988, train/loss=0.756880, validation/accuracy=0.748560, validation/loss=1.092732, validation/num_examples=50000
I0131 18:21:19.192105 139884794734336 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.730288028717041, loss=2.29304575920105
I0131 18:22:01.360072 139884786341632 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.158687591552734, loss=2.4242024421691895
I0131 18:22:48.256410 139884794734336 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.868239402770996, loss=2.3573927879333496
I0131 18:23:34.781840 139884786341632 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.072216033935547, loss=3.5809245109558105
I0131 18:24:21.424125 139884794734336 logging_writer.py:48] [163600] global_step=163600, grad_norm=5.106939792633057, loss=3.9465320110321045
I0131 18:25:07.913799 139884786341632 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.745593547821045, loss=3.1041829586029053
I0131 18:25:54.471284 139884794734336 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.227296352386475, loss=2.3490753173828125
I0131 18:26:41.103242 139884786341632 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.920544624328613, loss=2.3849010467529297
I0131 18:27:27.599060 139884794734336 logging_writer.py:48] [164000] global_step=164000, grad_norm=5.08156681060791, loss=3.559532403945923
I0131 18:28:13.925355 139884786341632 logging_writer.py:48] [164100] global_step=164100, grad_norm=6.056404113769531, loss=4.372023582458496
I0131 18:28:13.939688 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:28:24.010874 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:28:44.551021 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:28:46.197612 140085747812160 submission_runner.py:408] Time since start: 81089.09s, 	Step: 164101, 	{'train/accuracy': 0.8231250047683716, 'train/loss': 0.784327507019043, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 1.100296974182129, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.6938539743423462, 'test/num_examples': 10000, 'score': 75262.68575310707, 'total_duration': 81089.08715343475, 'accumulated_submission_time': 75262.68575310707, 'accumulated_eval_time': 5809.2748148441315, 'accumulated_logging_time': 8.088002920150757}
I0131 18:28:46.240210 139884794734336 logging_writer.py:48] [164101] accumulated_eval_time=5809.274815, accumulated_logging_time=8.088003, accumulated_submission_time=75262.685753, global_step=164101, preemption_count=0, score=75262.685753, test/accuracy=0.635000, test/loss=1.693854, test/num_examples=10000, total_duration=81089.087153, train/accuracy=0.823125, train/loss=0.784328, validation/accuracy=0.751120, validation/loss=1.100297, validation/num_examples=50000
I0131 18:29:27.052345 139884786341632 logging_writer.py:48] [164200] global_step=164200, grad_norm=5.639889240264893, loss=4.287580490112305
I0131 18:30:13.607733 139884794734336 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.2638773918151855, loss=4.013200283050537
I0131 18:31:00.330544 139884786341632 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.870556831359863, loss=2.417146682739258
I0131 18:31:46.704661 139884794734336 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.613019943237305, loss=2.243004560470581
I0131 18:32:33.335026 139884786341632 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.368901252746582, loss=2.312512159347534
I0131 18:33:19.912700 139884794734336 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.918354511260986, loss=3.7023496627807617
I0131 18:34:06.356692 139884786341632 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.5376482009887695, loss=2.7750885486602783
I0131 18:34:52.782275 139884794734336 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.236910343170166, loss=2.519700288772583
I0131 18:35:39.517937 139884786341632 logging_writer.py:48] [165000] global_step=165000, grad_norm=5.042342185974121, loss=2.373418092727661
I0131 18:35:46.602910 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:35:57.493521 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:36:17.482812 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:36:19.140319 140085747812160 submission_runner.py:408] Time since start: 81542.03s, 	Step: 165017, 	{'train/accuracy': 0.8282421827316284, 'train/loss': 0.7752244472503662, 'validation/accuracy': 0.7502599954605103, 'validation/loss': 1.0974425077438354, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.6791881322860718, 'test/num_examples': 10000, 'score': 75682.98915481567, 'total_duration': 81542.0298383236, 'accumulated_submission_time': 75682.98915481567, 'accumulated_eval_time': 5841.812193155289, 'accumulated_logging_time': 8.13952898979187}
I0131 18:36:19.189097 139884794734336 logging_writer.py:48] [165017] accumulated_eval_time=5841.812193, accumulated_logging_time=8.139529, accumulated_submission_time=75682.989155, global_step=165017, preemption_count=0, score=75682.989155, test/accuracy=0.629200, test/loss=1.679188, test/num_examples=10000, total_duration=81542.029838, train/accuracy=0.828242, train/loss=0.775224, validation/accuracy=0.750260, validation/loss=1.097443, validation/num_examples=50000
I0131 18:36:53.163507 139884786341632 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.770753860473633, loss=3.5231504440307617
I0131 18:37:39.311935 139884794734336 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.759608268737793, loss=2.4380221366882324
I0131 18:38:25.815321 139884786341632 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.7342848777771, loss=2.524930953979492
I0131 18:39:12.297463 139884794734336 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.860020160675049, loss=2.3368587493896484
I0131 18:39:58.593371 139884786341632 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.747579574584961, loss=3.2437222003936768
I0131 18:40:44.964335 139884794734336 logging_writer.py:48] [165600] global_step=165600, grad_norm=5.655955791473389, loss=4.256263732910156
I0131 18:41:31.363035 139884786341632 logging_writer.py:48] [165700] global_step=165700, grad_norm=5.577881336212158, loss=2.319122552871704
I0131 18:42:17.812157 139884794734336 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.2219319343566895, loss=2.273172378540039
I0131 18:43:04.489874 139884786341632 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.970115661621094, loss=2.6735434532165527
I0131 18:43:19.504780 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:43:29.878030 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:43:51.608074 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:43:53.246011 140085747812160 submission_runner.py:408] Time since start: 81996.14s, 	Step: 165934, 	{'train/accuracy': 0.8337695002555847, 'train/loss': 0.7408263087272644, 'validation/accuracy': 0.7530800104141235, 'validation/loss': 1.0839965343475342, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.672433853149414, 'test/num_examples': 10000, 'score': 76103.24325037003, 'total_duration': 81996.13554024696, 'accumulated_submission_time': 76103.24325037003, 'accumulated_eval_time': 5875.553389310837, 'accumulated_logging_time': 8.199171781539917}
I0131 18:43:53.290711 139884794734336 logging_writer.py:48] [165934] accumulated_eval_time=5875.553389, accumulated_logging_time=8.199172, accumulated_submission_time=76103.243250, global_step=165934, preemption_count=0, score=76103.243250, test/accuracy=0.633500, test/loss=1.672434, test/num_examples=10000, total_duration=81996.135540, train/accuracy=0.833770, train/loss=0.740826, validation/accuracy=0.753080, validation/loss=1.083997, validation/num_examples=50000
I0131 18:44:19.562918 139884786341632 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.9227447509765625, loss=2.473745346069336
I0131 18:45:04.890437 139884794734336 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.098374366760254, loss=2.737607002258301
I0131 18:45:51.596223 139884786341632 logging_writer.py:48] [166200] global_step=166200, grad_norm=5.122195720672607, loss=2.510587215423584
I0131 18:46:38.151816 139884794734336 logging_writer.py:48] [166300] global_step=166300, grad_norm=6.687984943389893, loss=2.460575819015503
I0131 18:47:24.661825 139884786341632 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.021049976348877, loss=2.7627530097961426
I0131 18:48:10.995687 139884794734336 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.982980251312256, loss=2.2688186168670654
I0131 18:48:57.613216 139884786341632 logging_writer.py:48] [166600] global_step=166600, grad_norm=5.181909084320068, loss=3.432798147201538
I0131 18:49:44.100033 139884794734336 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.507928371429443, loss=2.6068201065063477
I0131 18:50:30.879265 139884786341632 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.809514045715332, loss=2.345822334289551
I0131 18:50:53.350807 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:51:04.375970 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:51:26.415112 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:51:28.054805 140085747812160 submission_runner.py:408] Time since start: 82450.94s, 	Step: 166850, 	{'train/accuracy': 0.8430468440055847, 'train/loss': 0.7069917917251587, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0686745643615723, 'validation/num_examples': 50000, 'test/accuracy': 0.6446000337600708, 'test/loss': 1.6342931985855103, 'test/num_examples': 10000, 'score': 76523.2434270382, 'total_duration': 82450.94435310364, 'accumulated_submission_time': 76523.2434270382, 'accumulated_eval_time': 5910.257388830185, 'accumulated_logging_time': 8.253775358200073}
I0131 18:51:28.094091 139884794734336 logging_writer.py:48] [166850] accumulated_eval_time=5910.257389, accumulated_logging_time=8.253775, accumulated_submission_time=76523.243427, global_step=166850, preemption_count=0, score=76523.243427, test/accuracy=0.644600, test/loss=1.634293, test/num_examples=10000, total_duration=82450.944353, train/accuracy=0.843047, train/loss=0.706992, validation/accuracy=0.755260, validation/loss=1.068675, validation/num_examples=50000
I0131 18:51:48.105079 139884786341632 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.388858795166016, loss=3.9067606925964355
I0131 18:52:32.555271 139884794734336 logging_writer.py:48] [167000] global_step=167000, grad_norm=5.083310604095459, loss=3.7707791328430176
I0131 18:53:19.240707 139884786341632 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.173586845397949, loss=2.3447606563568115
I0131 18:54:06.306196 139884794734336 logging_writer.py:48] [167200] global_step=167200, grad_norm=5.0717620849609375, loss=2.824500799179077
I0131 18:54:52.671163 139884786341632 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.595765113830566, loss=2.4052374362945557
I0131 18:55:39.182039 139884794734336 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.802096366882324, loss=3.7182650566101074
I0131 18:56:25.698744 139884786341632 logging_writer.py:48] [167500] global_step=167500, grad_norm=5.49710750579834, loss=2.2624316215515137
I0131 18:57:12.434047 139884794734336 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.28504753112793, loss=2.2743709087371826
I0131 18:57:59.002635 139884786341632 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.511831760406494, loss=3.7317371368408203
I0131 18:58:28.332743 140085747812160 spec.py:321] Evaluating on the training split.
I0131 18:58:38.640628 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 18:58:59.901899 140085747812160 spec.py:349] Evaluating on the test split.
I0131 18:59:01.554873 140085747812160 submission_runner.py:408] Time since start: 82904.44s, 	Step: 167765, 	{'train/accuracy': 0.8338086009025574, 'train/loss': 0.7452972531318665, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.074892282485962, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.656589150428772, 'test/num_examples': 10000, 'score': 76943.4230298996, 'total_duration': 82904.4444053173, 'accumulated_submission_time': 76943.4230298996, 'accumulated_eval_time': 5943.479510307312, 'accumulated_logging_time': 8.302427291870117}
I0131 18:59:01.603287 139884794734336 logging_writer.py:48] [167765] accumulated_eval_time=5943.479510, accumulated_logging_time=8.302427, accumulated_submission_time=76943.423030, global_step=167765, preemption_count=0, score=76943.423030, test/accuracy=0.640600, test/loss=1.656589, test/num_examples=10000, total_duration=82904.444405, train/accuracy=0.833809, train/loss=0.745297, validation/accuracy=0.754840, validation/loss=1.074892, validation/num_examples=50000
I0131 18:59:15.734749 139884786341632 logging_writer.py:48] [167800] global_step=167800, grad_norm=5.021325588226318, loss=3.5008137226104736
I0131 18:59:59.111703 139884794734336 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.580679416656494, loss=3.0366408824920654
I0131 19:00:45.692111 139884786341632 logging_writer.py:48] [168000] global_step=168000, grad_norm=5.791859149932861, loss=2.2566311359405518
I0131 19:01:32.564326 139884794734336 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.455316066741943, loss=3.121443510055542
I0131 19:02:19.086185 139884786341632 logging_writer.py:48] [168200] global_step=168200, grad_norm=5.024442195892334, loss=2.814004898071289
I0131 19:03:06.008694 139884794734336 logging_writer.py:48] [168300] global_step=168300, grad_norm=5.1516337394714355, loss=3.008328914642334
I0131 19:03:52.528939 139884786341632 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.848824977874756, loss=3.1664340496063232
I0131 19:04:39.052648 139884794734336 logging_writer.py:48] [168500] global_step=168500, grad_norm=5.477470397949219, loss=2.9541168212890625
I0131 19:05:25.762508 139884786341632 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.0680999755859375, loss=2.3949291706085205
I0131 19:06:01.802555 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:06:12.207874 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:06:33.886930 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:06:35.539849 140085747812160 submission_runner.py:408] Time since start: 83358.43s, 	Step: 168679, 	{'train/accuracy': 0.83363276720047, 'train/loss': 0.7375044822692871, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0720763206481934, 'validation/num_examples': 50000, 'test/accuracy': 0.6388000249862671, 'test/loss': 1.652498722076416, 'test/num_examples': 10000, 'score': 77363.56269574165, 'total_duration': 83358.4293923378, 'accumulated_submission_time': 77363.56269574165, 'accumulated_eval_time': 5977.2169399261475, 'accumulated_logging_time': 8.360501766204834}
I0131 19:06:35.581720 139884794734336 logging_writer.py:48] [168679] accumulated_eval_time=5977.216940, accumulated_logging_time=8.360502, accumulated_submission_time=77363.562696, global_step=168679, preemption_count=0, score=77363.562696, test/accuracy=0.638800, test/loss=1.652499, test/num_examples=10000, total_duration=83358.429392, train/accuracy=0.833633, train/loss=0.737504, validation/accuracy=0.756200, validation/loss=1.072076, validation/num_examples=50000
I0131 19:06:44.207914 139884786341632 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.996603965759277, loss=4.1054558753967285
I0131 19:07:26.593808 139884794734336 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.888433456420898, loss=2.345025062561035
I0131 19:08:13.229159 139884786341632 logging_writer.py:48] [168900] global_step=168900, grad_norm=5.3833513259887695, loss=2.859145164489746
I0131 19:08:59.584973 139884794734336 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.973914623260498, loss=2.2644686698913574
I0131 19:09:12.438484 139884786341632 logging_writer.py:48] [169029] global_step=169029, preemption_count=0, score=77520.330180
I0131 19:09:13.122118 140085747812160 checkpoints.py:490] Saving checkpoint at step: 169029
I0131 19:09:14.329305 140085747812160 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1/checkpoint_169029
I0131 19:09:14.349542 140085747812160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_1/checkpoint_169029.
I0131 19:09:15.103320 140085747812160 submission_runner.py:583] Tuning trial 1/5
I0131 19:09:15.103596 140085747812160 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0131 19:09:15.113470 140085747812160 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009179687476716936, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 56.03143048286438, 'total_duration': 96.34635639190674, 'accumulated_submission_time': 56.03143048286438, 'accumulated_eval_time': 40.31481671333313, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (884, {'train/accuracy': 0.015097656287252903, 'train/loss': 6.414700031280518, 'validation/accuracy': 0.014619999565184116, 'validation/loss': 6.424112796783447, 'validation/num_examples': 50000, 'test/accuracy': 0.011200000531971455, 'test/loss': 6.467188358306885, 'test/num_examples': 10000, 'score': 476.19164657592773, 'total_duration': 538.5675106048584, 'accumulated_submission_time': 476.19164657592773, 'accumulated_eval_time': 62.29332900047302, 'accumulated_logging_time': 0.0311734676361084, 'global_step': 884, 'preemption_count': 0}), (1816, {'train/accuracy': 0.04582031071186066, 'train/loss': 5.830249309539795, 'validation/accuracy': 0.044039998203516006, 'validation/loss': 5.860832214355469, 'validation/num_examples': 50000, 'test/accuracy': 0.03880000114440918, 'test/loss': 5.978135585784912, 'test/num_examples': 10000, 'score': 896.3407437801361, 'total_duration': 980.5350985527039, 'accumulated_submission_time': 896.3407437801361, 'accumulated_eval_time': 84.03109979629517, 'accumulated_logging_time': 0.058878421783447266, 'global_step': 1816, 'preemption_count': 0}), (2749, {'train/accuracy': 0.07023437321186066, 'train/loss': 5.432772636413574, 'validation/accuracy': 0.06391999870538712, 'validation/loss': 5.477494716644287, 'validation/num_examples': 50000, 'test/accuracy': 0.04910000413656235, 'test/loss': 5.647575378417969, 'test/num_examples': 10000, 'score': 1316.5516102313995, 'total_duration': 1422.709864616394, 'accumulated_submission_time': 1316.5516102313995, 'accumulated_eval_time': 105.91550326347351, 'accumulated_logging_time': 0.08542728424072266, 'global_step': 2749, 'preemption_count': 0}), (3681, {'train/accuracy': 0.09486328065395355, 'train/loss': 5.102994441986084, 'validation/accuracy': 0.08893999457359314, 'validation/loss': 5.145900249481201, 'validation/num_examples': 50000, 'test/accuracy': 0.06800000369548798, 'test/loss': 5.371665954589844, 'test/num_examples': 10000, 'score': 1736.8272049427032, 'total_duration': 1866.5280022621155, 'accumulated_submission_time': 1736.8272049427032, 'accumulated_eval_time': 129.37765622138977, 'accumulated_logging_time': 0.11379456520080566, 'global_step': 3681, 'preemption_count': 0}), (4611, {'train/accuracy': 0.14060546457767487, 'train/loss': 4.725737571716309, 'validation/accuracy': 0.13011999428272247, 'validation/loss': 4.775650501251221, 'validation/num_examples': 50000, 'test/accuracy': 0.09880000352859497, 'test/loss': 5.062837600708008, 'test/num_examples': 10000, 'score': 2157.039966583252, 'total_duration': 2309.0352096557617, 'accumulated_submission_time': 2157.039966583252, 'accumulated_eval_time': 151.5914740562439, 'accumulated_logging_time': 0.14225101470947266, 'global_step': 4611, 'preemption_count': 0}), (5539, {'train/accuracy': 0.18742187321186066, 'train/loss': 4.307502269744873, 'validation/accuracy': 0.16944000124931335, 'validation/loss': 4.408010959625244, 'validation/num_examples': 50000, 'test/accuracy': 0.12810000777244568, 'test/loss': 4.742315292358398, 'test/num_examples': 10000, 'score': 2577.1580624580383, 'total_duration': 2751.210842370987, 'accumulated_submission_time': 2577.1580624580383, 'accumulated_eval_time': 173.5701711177826, 'accumulated_logging_time': 0.1694018840789795, 'global_step': 5539, 'preemption_count': 0}), (6464, {'train/accuracy': 0.22345702350139618, 'train/loss': 4.075367450714111, 'validation/accuracy': 0.20885999500751495, 'validation/loss': 4.152421951293945, 'validation/num_examples': 50000, 'test/accuracy': 0.15880000591278076, 'test/loss': 4.514123916625977, 'test/num_examples': 10000, 'score': 2997.1211307048798, 'total_duration': 3195.5742268562317, 'accumulated_submission_time': 2997.1211307048798, 'accumulated_eval_time': 197.8914442062378, 'accumulated_logging_time': 0.19727015495300293, 'global_step': 6464, 'preemption_count': 0}), (7389, {'train/accuracy': 0.26749998331069946, 'train/loss': 3.717461109161377, 'validation/accuracy': 0.24699999392032623, 'validation/loss': 3.8280117511749268, 'validation/num_examples': 50000, 'test/accuracy': 0.1915000081062317, 'test/loss': 4.239701271057129, 'test/num_examples': 10000, 'score': 3417.268706560135, 'total_duration': 3642.021924495697, 'accumulated_submission_time': 3417.268706560135, 'accumulated_eval_time': 224.10468411445618, 'accumulated_logging_time': 0.23234915733337402, 'global_step': 7389, 'preemption_count': 0}), (8319, {'train/accuracy': 0.30726560950279236, 'train/loss': 3.447201728820801, 'validation/accuracy': 0.2823199927806854, 'validation/loss': 3.5882530212402344, 'validation/num_examples': 50000, 'test/accuracy': 0.21450001001358032, 'test/loss': 4.04166316986084, 'test/num_examples': 10000, 'score': 3837.3803448677063, 'total_duration': 4092.129539489746, 'accumulated_submission_time': 3837.3803448677063, 'accumulated_eval_time': 254.00139904022217, 'accumulated_logging_time': 0.27797937393188477, 'global_step': 8319, 'preemption_count': 0}), (9246, {'train/accuracy': 0.34611326456069946, 'train/loss': 3.243919849395752, 'validation/accuracy': 0.3188599944114685, 'validation/loss': 3.357321262359619, 'validation/num_examples': 50000, 'test/accuracy': 0.24560001492500305, 'test/loss': 3.852262020111084, 'test/num_examples': 10000, 'score': 4257.711834192276, 'total_duration': 4546.292115211487, 'accumulated_submission_time': 4257.711834192276, 'accumulated_eval_time': 287.7465693950653, 'accumulated_logging_time': 0.3111603260040283, 'global_step': 9246, 'preemption_count': 0}), (10176, {'train/accuracy': 0.36494138836860657, 'train/loss': 3.0633111000061035, 'validation/accuracy': 0.3436799943447113, 'validation/loss': 3.1808507442474365, 'validation/num_examples': 50000, 'test/accuracy': 0.26250001788139343, 'test/loss': 3.7037575244903564, 'test/num_examples': 10000, 'score': 4677.799368858337, 'total_duration': 4995.334156990051, 'accumulated_submission_time': 4677.799368858337, 'accumulated_eval_time': 316.598552942276, 'accumulated_logging_time': 0.35977983474731445, 'global_step': 10176, 'preemption_count': 0}), (11101, {'train/accuracy': 0.3976367115974426, 'train/loss': 2.9350926876068115, 'validation/accuracy': 0.3646000027656555, 'validation/loss': 3.088977336883545, 'validation/num_examples': 50000, 'test/accuracy': 0.2785000205039978, 'test/loss': 3.6206798553466797, 'test/num_examples': 10000, 'score': 5098.2224934101105, 'total_duration': 5446.229580163956, 'accumulated_submission_time': 5098.2224934101105, 'accumulated_eval_time': 346.9868311882019, 'accumulated_logging_time': 0.3919997215270996, 'global_step': 11101, 'preemption_count': 0}), (12025, {'train/accuracy': 0.4351562261581421, 'train/loss': 2.699234962463379, 'validation/accuracy': 0.3865399956703186, 'validation/loss': 2.927830457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.2924000024795532, 'test/loss': 3.4869821071624756, 'test/num_examples': 10000, 'score': 5518.611259937286, 'total_duration': 5892.516352653503, 'accumulated_submission_time': 5518.611259937286, 'accumulated_eval_time': 372.8019599914551, 'accumulated_logging_time': 0.4224827289581299, 'global_step': 12025, 'preemption_count': 0}), (12947, {'train/accuracy': 0.42998045682907104, 'train/loss': 2.715768575668335, 'validation/accuracy': 0.3989799916744232, 'validation/loss': 2.8612937927246094, 'validation/num_examples': 50000, 'test/accuracy': 0.31060001254081726, 'test/loss': 3.41860294342041, 'test/num_examples': 10000, 'score': 5938.5728850364685, 'total_duration': 6340.0799124240875, 'accumulated_submission_time': 5938.5728850364685, 'accumulated_eval_time': 400.3198812007904, 'accumulated_logging_time': 0.4549741744995117, 'global_step': 12947, 'preemption_count': 0}), (13871, {'train/accuracy': 0.44755858182907104, 'train/loss': 2.601588010787964, 'validation/accuracy': 0.41808000206947327, 'validation/loss': 2.7594058513641357, 'validation/num_examples': 50000, 'test/accuracy': 0.3206000030040741, 'test/loss': 3.336005210876465, 'test/num_examples': 10000, 'score': 6358.816624403, 'total_duration': 6794.234518766403, 'accumulated_submission_time': 6358.816624403, 'accumulated_eval_time': 434.1339132785797, 'accumulated_logging_time': 0.499969482421875, 'global_step': 13871, 'preemption_count': 0}), (14794, {'train/accuracy': 0.4675585925579071, 'train/loss': 2.4731810092926025, 'validation/accuracy': 0.4262399971485138, 'validation/loss': 2.686066150665283, 'validation/num_examples': 50000, 'test/accuracy': 0.3368000090122223, 'test/loss': 3.2613182067871094, 'test/num_examples': 10000, 'score': 6778.866580486298, 'total_duration': 7250.958543777466, 'accumulated_submission_time': 6778.866580486298, 'accumulated_eval_time': 470.73082447052, 'accumulated_logging_time': 0.5254685878753662, 'global_step': 14794, 'preemption_count': 0}), (15716, {'train/accuracy': 0.4583398401737213, 'train/loss': 2.5445070266723633, 'validation/accuracy': 0.43347999453544617, 'validation/loss': 2.6762075424194336, 'validation/num_examples': 50000, 'test/accuracy': 0.33150002360343933, 'test/loss': 3.258542060852051, 'test/num_examples': 10000, 'score': 7199.002569198608, 'total_duration': 7709.4025728702545, 'accumulated_submission_time': 7199.002569198608, 'accumulated_eval_time': 508.96109414100647, 'accumulated_logging_time': 0.5512490272521973, 'global_step': 15716, 'preemption_count': 0}), (16636, {'train/accuracy': 0.4844140410423279, 'train/loss': 2.364227533340454, 'validation/accuracy': 0.4491399824619293, 'validation/loss': 2.542757511138916, 'validation/num_examples': 50000, 'test/accuracy': 0.34850001335144043, 'test/loss': 3.135037422180176, 'test/num_examples': 10000, 'score': 7619.164701223373, 'total_duration': 8167.473934412003, 'accumulated_submission_time': 7619.164701223373, 'accumulated_eval_time': 546.786703824997, 'accumulated_logging_time': 0.5834970474243164, 'global_step': 16636, 'preemption_count': 0}), (17552, {'train/accuracy': 0.48808592557907104, 'train/loss': 2.3479232788085938, 'validation/accuracy': 0.4491199851036072, 'validation/loss': 2.5377163887023926, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.126204013824463, 'test/num_examples': 10000, 'score': 8039.487378358841, 'total_duration': 8624.050165176392, 'accumulated_submission_time': 8039.487378358841, 'accumulated_eval_time': 582.9615287780762, 'accumulated_logging_time': 0.6108376979827881, 'global_step': 17552, 'preemption_count': 0}), (18470, {'train/accuracy': 0.494140625, 'train/loss': 2.3677914142608643, 'validation/accuracy': 0.4590199887752533, 'validation/loss': 2.532339334487915, 'validation/num_examples': 50000, 'test/accuracy': 0.3570000231266022, 'test/loss': 3.1239099502563477, 'test/num_examples': 10000, 'score': 8459.713441371918, 'total_duration': 9083.020942211151, 'accumulated_submission_time': 8459.713441371918, 'accumulated_eval_time': 621.6249876022339, 'accumulated_logging_time': 0.6407480239868164, 'global_step': 18470, 'preemption_count': 0}), (19389, {'train/accuracy': 0.500292956829071, 'train/loss': 2.304877996444702, 'validation/accuracy': 0.4635799825191498, 'validation/loss': 2.468919515609741, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.0912277698516846, 'test/num_examples': 10000, 'score': 8879.765430927277, 'total_duration': 9542.602039337158, 'accumulated_submission_time': 8879.765430927277, 'accumulated_eval_time': 661.0747628211975, 'accumulated_logging_time': 0.6683251857757568, 'global_step': 19389, 'preemption_count': 0}), (20307, {'train/accuracy': 0.5157226324081421, 'train/loss': 2.2452750205993652, 'validation/accuracy': 0.471780002117157, 'validation/loss': 2.446197986602783, 'validation/num_examples': 50000, 'test/accuracy': 0.3741000294685364, 'test/loss': 3.045219898223877, 'test/num_examples': 10000, 'score': 9298.976362466812, 'total_duration': 10000.288239002228, 'accumulated_submission_time': 9298.976362466812, 'accumulated_eval_time': 698.6457741260529, 'accumulated_logging_time': 1.5207412242889404, 'global_step': 20307, 'preemption_count': 0}), (21226, {'train/accuracy': 0.5420507788658142, 'train/loss': 2.0856730937957764, 'validation/accuracy': 0.4818599820137024, 'validation/loss': 2.3691818714141846, 'validation/num_examples': 50000, 'test/accuracy': 0.3777000308036804, 'test/loss': 2.9841349124908447, 'test/num_examples': 10000, 'score': 9719.232017755508, 'total_duration': 10455.08547091484, 'accumulated_submission_time': 9719.232017755508, 'accumulated_eval_time': 733.1075923442841, 'accumulated_logging_time': 1.548471212387085, 'global_step': 21226, 'preemption_count': 0}), (22142, {'train/accuracy': 0.5222460627555847, 'train/loss': 2.2017922401428223, 'validation/accuracy': 0.48715999722480774, 'validation/loss': 2.363506555557251, 'validation/num_examples': 50000, 'test/accuracy': 0.380700021982193, 'test/loss': 2.985949754714966, 'test/num_examples': 10000, 'score': 10139.521540164948, 'total_duration': 10909.113496303558, 'accumulated_submission_time': 10139.521540164948, 'accumulated_eval_time': 766.7560062408447, 'accumulated_logging_time': 1.5872962474822998, 'global_step': 22142, 'preemption_count': 0}), (23064, {'train/accuracy': 0.5399999618530273, 'train/loss': 2.108938455581665, 'validation/accuracy': 0.499099999666214, 'validation/loss': 2.298219919204712, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.9014742374420166, 'test/num_examples': 10000, 'score': 10559.62688589096, 'total_duration': 11362.430181026459, 'accumulated_submission_time': 10559.62688589096, 'accumulated_eval_time': 799.8872475624084, 'accumulated_logging_time': 1.6160616874694824, 'global_step': 23064, 'preemption_count': 0}), (23984, {'train/accuracy': 0.5517382621765137, 'train/loss': 2.0394484996795654, 'validation/accuracy': 0.5036199688911438, 'validation/loss': 2.2866790294647217, 'validation/num_examples': 50000, 'test/accuracy': 0.39010003209114075, 'test/loss': 2.90531587600708, 'test/num_examples': 10000, 'score': 10979.964556217194, 'total_duration': 11816.292253255844, 'accumulated_submission_time': 10979.964556217194, 'accumulated_eval_time': 833.3303182125092, 'accumulated_logging_time': 1.6461646556854248, 'global_step': 23984, 'preemption_count': 0}), (24904, {'train/accuracy': 0.541796863079071, 'train/loss': 2.0832176208496094, 'validation/accuracy': 0.5054000020027161, 'validation/loss': 2.2420337200164795, 'validation/num_examples': 50000, 'test/accuracy': 0.3970000147819519, 'test/loss': 2.8567094802856445, 'test/num_examples': 10000, 'score': 11400.30337524414, 'total_duration': 12270.108603954315, 'accumulated_submission_time': 11400.30337524414, 'accumulated_eval_time': 866.7247035503387, 'accumulated_logging_time': 1.6784873008728027, 'global_step': 24904, 'preemption_count': 0}), (25811, {'train/accuracy': 0.5504101514816284, 'train/loss': 2.0340700149536133, 'validation/accuracy': 0.5138599872589111, 'validation/loss': 2.201979875564575, 'validation/num_examples': 50000, 'test/accuracy': 0.40130001306533813, 'test/loss': 2.8194544315338135, 'test/num_examples': 10000, 'score': 11820.599084615707, 'total_duration': 12724.000076532364, 'accumulated_submission_time': 11820.599084615707, 'accumulated_eval_time': 900.2320485115051, 'accumulated_logging_time': 1.71818208694458, 'global_step': 25811, 'preemption_count': 0}), (26725, {'train/accuracy': 0.566210925579071, 'train/loss': 2.00087571144104, 'validation/accuracy': 0.5141199827194214, 'validation/loss': 2.229660987854004, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.832810878753662, 'test/num_examples': 10000, 'score': 12240.525072574615, 'total_duration': 13177.773655176163, 'accumulated_submission_time': 12240.525072574615, 'accumulated_eval_time': 933.998238325119, 'accumulated_logging_time': 1.7490291595458984, 'global_step': 26725, 'preemption_count': 0}), (27643, {'train/accuracy': 0.5484570264816284, 'train/loss': 2.040403366088867, 'validation/accuracy': 0.519760012626648, 'validation/loss': 2.1909496784210205, 'validation/num_examples': 50000, 'test/accuracy': 0.4043000340461731, 'test/loss': 2.8294668197631836, 'test/num_examples': 10000, 'score': 12660.572672367096, 'total_duration': 13630.616124868393, 'accumulated_submission_time': 12660.572672367096, 'accumulated_eval_time': 966.7126722335815, 'accumulated_logging_time': 1.7793781757354736, 'global_step': 27643, 'preemption_count': 0}), (28560, {'train/accuracy': 0.560546875, 'train/loss': 1.999878168106079, 'validation/accuracy': 0.5216999650001526, 'validation/loss': 2.1805710792541504, 'validation/num_examples': 50000, 'test/accuracy': 0.4077000319957733, 'test/loss': 2.8053174018859863, 'test/num_examples': 10000, 'score': 13080.651557922363, 'total_duration': 14082.56403541565, 'accumulated_submission_time': 13080.651557922363, 'accumulated_eval_time': 998.4982385635376, 'accumulated_logging_time': 1.811950922012329, 'global_step': 28560, 'preemption_count': 0}), (29477, {'train/accuracy': 0.5726562142372131, 'train/loss': 1.928755760192871, 'validation/accuracy': 0.5296599864959717, 'validation/loss': 2.13201904296875, 'validation/num_examples': 50000, 'test/accuracy': 0.4166000187397003, 'test/loss': 2.758517026901245, 'test/num_examples': 10000, 'score': 13500.786858081818, 'total_duration': 14534.419796228409, 'accumulated_submission_time': 13500.786858081818, 'accumulated_eval_time': 1030.136039018631, 'accumulated_logging_time': 1.8442060947418213, 'global_step': 29477, 'preemption_count': 0}), (30393, {'train/accuracy': 0.5676171779632568, 'train/loss': 1.9483190774917603, 'validation/accuracy': 0.5301600098609924, 'validation/loss': 2.114281415939331, 'validation/num_examples': 50000, 'test/accuracy': 0.4150000214576721, 'test/loss': 2.7555928230285645, 'test/num_examples': 10000, 'score': 13920.732541561127, 'total_duration': 14987.511257886887, 'accumulated_submission_time': 13920.732541561127, 'accumulated_eval_time': 1063.1991493701935, 'accumulated_logging_time': 1.8761727809906006, 'global_step': 30393, 'preemption_count': 0}), (31309, {'train/accuracy': 0.5728124976158142, 'train/loss': 1.927043080329895, 'validation/accuracy': 0.537559986114502, 'validation/loss': 2.104546546936035, 'validation/num_examples': 50000, 'test/accuracy': 0.4228000342845917, 'test/loss': 2.7284553050994873, 'test/num_examples': 10000, 'score': 14340.659535884857, 'total_duration': 15440.811559438705, 'accumulated_submission_time': 14340.659535884857, 'accumulated_eval_time': 1096.492238998413, 'accumulated_logging_time': 1.905264139175415, 'global_step': 31309, 'preemption_count': 0}), (32227, {'train/accuracy': 0.5775781273841858, 'train/loss': 1.9075417518615723, 'validation/accuracy': 0.5331599712371826, 'validation/loss': 2.1145474910736084, 'validation/num_examples': 50000, 'test/accuracy': 0.4244000315666199, 'test/loss': 2.728424072265625, 'test/num_examples': 10000, 'score': 14760.618643760681, 'total_duration': 15893.128350257874, 'accumulated_submission_time': 14760.618643760681, 'accumulated_eval_time': 1128.7679710388184, 'accumulated_logging_time': 1.9361648559570312, 'global_step': 32227, 'preemption_count': 0}), (33142, {'train/accuracy': 0.6056835651397705, 'train/loss': 1.7929567098617554, 'validation/accuracy': 0.5388399958610535, 'validation/loss': 2.0905473232269287, 'validation/num_examples': 50000, 'test/accuracy': 0.4270000159740448, 'test/loss': 2.7143259048461914, 'test/num_examples': 10000, 'score': 15180.854577302933, 'total_duration': 16346.38900589943, 'accumulated_submission_time': 15180.854577302933, 'accumulated_eval_time': 1161.712742805481, 'accumulated_logging_time': 1.9648699760437012, 'global_step': 33142, 'preemption_count': 0}), (34059, {'train/accuracy': 0.577441394329071, 'train/loss': 1.9018783569335938, 'validation/accuracy': 0.5426999926567078, 'validation/loss': 2.0791501998901367, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.717620372772217, 'test/num_examples': 10000, 'score': 15601.136335372925, 'total_duration': 16798.78573513031, 'accumulated_submission_time': 15601.136335372925, 'accumulated_eval_time': 1193.7472488880157, 'accumulated_logging_time': 1.9952008724212646, 'global_step': 34059, 'preemption_count': 0}), (34977, {'train/accuracy': 0.5922070145606995, 'train/loss': 1.842816710472107, 'validation/accuracy': 0.5476399660110474, 'validation/loss': 2.05157208442688, 'validation/num_examples': 50000, 'test/accuracy': 0.429500013589859, 'test/loss': 2.695765256881714, 'test/num_examples': 10000, 'score': 16021.365535497665, 'total_duration': 17250.523627996445, 'accumulated_submission_time': 16021.365535497665, 'accumulated_eval_time': 1225.1709115505219, 'accumulated_logging_time': 2.0287718772888184, 'global_step': 34977, 'preemption_count': 0}), (35896, {'train/accuracy': 0.6042382717132568, 'train/loss': 1.7828890085220337, 'validation/accuracy': 0.5482999682426453, 'validation/loss': 2.0370826721191406, 'validation/num_examples': 50000, 'test/accuracy': 0.4358000159263611, 'test/loss': 2.6567018032073975, 'test/num_examples': 10000, 'score': 16441.70537519455, 'total_duration': 17703.087596178055, 'accumulated_submission_time': 16441.70537519455, 'accumulated_eval_time': 1257.3156542778015, 'accumulated_logging_time': 2.0581812858581543, 'global_step': 35896, 'preemption_count': 0}), (36813, {'train/accuracy': 0.5854101181030273, 'train/loss': 1.868322730064392, 'validation/accuracy': 0.5462200045585632, 'validation/loss': 2.047626256942749, 'validation/num_examples': 50000, 'test/accuracy': 0.43140003085136414, 'test/loss': 2.669766902923584, 'test/num_examples': 10000, 'score': 16862.025440216064, 'total_duration': 18154.93812942505, 'accumulated_submission_time': 16862.025440216064, 'accumulated_eval_time': 1288.7657787799835, 'accumulated_logging_time': 2.0886483192443848, 'global_step': 36813, 'preemption_count': 0}), (37732, {'train/accuracy': 0.5966015458106995, 'train/loss': 1.8163317441940308, 'validation/accuracy': 0.5523599982261658, 'validation/loss': 2.01404070854187, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.634584426879883, 'test/num_examples': 10000, 'score': 17282.239025354385, 'total_duration': 18607.237397432327, 'accumulated_submission_time': 17282.239025354385, 'accumulated_eval_time': 1320.7677319049835, 'accumulated_logging_time': 2.1223950386047363, 'global_step': 37732, 'preemption_count': 0}), (38651, {'train/accuracy': 0.60693359375, 'train/loss': 1.7717552185058594, 'validation/accuracy': 0.5542600154876709, 'validation/loss': 2.0164029598236084, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.6386351585388184, 'test/num_examples': 10000, 'score': 17702.3341217041, 'total_duration': 19060.093980789185, 'accumulated_submission_time': 17702.3341217041, 'accumulated_eval_time': 1353.4470887184143, 'accumulated_logging_time': 2.154141902923584, 'global_step': 38651, 'preemption_count': 0}), (39571, {'train/accuracy': 0.590136706829071, 'train/loss': 1.8692361116409302, 'validation/accuracy': 0.5558800101280212, 'validation/loss': 2.0332841873168945, 'validation/num_examples': 50000, 'test/accuracy': 0.43410003185272217, 'test/loss': 2.6673877239227295, 'test/num_examples': 10000, 'score': 18122.364139556885, 'total_duration': 19513.99955034256, 'accumulated_submission_time': 18122.364139556885, 'accumulated_eval_time': 1387.2440507411957, 'accumulated_logging_time': 2.182854175567627, 'global_step': 39571, 'preemption_count': 0}), (40490, {'train/accuracy': 0.5978320240974426, 'train/loss': 1.8688842058181763, 'validation/accuracy': 0.5544999837875366, 'validation/loss': 2.060164213180542, 'validation/num_examples': 50000, 'test/accuracy': 0.43870002031326294, 'test/loss': 2.6926157474517822, 'test/num_examples': 10000, 'score': 18542.358523845673, 'total_duration': 19965.28075647354, 'accumulated_submission_time': 18542.358523845673, 'accumulated_eval_time': 1418.446210384369, 'accumulated_logging_time': 2.216303586959839, 'global_step': 40490, 'preemption_count': 0}), (41407, {'train/accuracy': 0.6039062142372131, 'train/loss': 1.8160500526428223, 'validation/accuracy': 0.5564199686050415, 'validation/loss': 2.0289831161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.4399000108242035, 'test/loss': 2.6659891605377197, 'test/num_examples': 10000, 'score': 18962.650825738907, 'total_duration': 20416.922464370728, 'accumulated_submission_time': 18962.650825738907, 'accumulated_eval_time': 1449.7089776992798, 'accumulated_logging_time': 2.2516136169433594, 'global_step': 41407, 'preemption_count': 0}), (42326, {'train/accuracy': 0.5984765291213989, 'train/loss': 1.8087574243545532, 'validation/accuracy': 0.5619800090789795, 'validation/loss': 1.9925109148025513, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.6232457160949707, 'test/num_examples': 10000, 'score': 19382.726880311966, 'total_duration': 20869.60631251335, 'accumulated_submission_time': 19382.726880311966, 'accumulated_eval_time': 1482.2264337539673, 'accumulated_logging_time': 2.291311264038086, 'global_step': 42326, 'preemption_count': 0}), (43244, {'train/accuracy': 0.6040819883346558, 'train/loss': 1.7556627988815308, 'validation/accuracy': 0.5645599961280823, 'validation/loss': 1.9334279298782349, 'validation/num_examples': 50000, 'test/accuracy': 0.4507000148296356, 'test/loss': 2.5570688247680664, 'test/num_examples': 10000, 'score': 19802.75421524048, 'total_duration': 21322.457344055176, 'accumulated_submission_time': 19802.75421524048, 'accumulated_eval_time': 1514.9667398929596, 'accumulated_logging_time': 2.3243231773376465, 'global_step': 43244, 'preemption_count': 0}), (44162, {'train/accuracy': 0.6100195050239563, 'train/loss': 1.7446229457855225, 'validation/accuracy': 0.5627399682998657, 'validation/loss': 1.9554500579833984, 'validation/num_examples': 50000, 'test/accuracy': 0.45010003447532654, 'test/loss': 2.5805513858795166, 'test/num_examples': 10000, 'score': 20223.016721487045, 'total_duration': 21775.797772169113, 'accumulated_submission_time': 20223.016721487045, 'accumulated_eval_time': 1547.9607965946198, 'accumulated_logging_time': 2.3581182956695557, 'global_step': 44162, 'preemption_count': 0}), (45079, {'train/accuracy': 0.6299804449081421, 'train/loss': 1.65792977809906, 'validation/accuracy': 0.564079999923706, 'validation/loss': 1.9666937589645386, 'validation/num_examples': 50000, 'test/accuracy': 0.4431000351905823, 'test/loss': 2.618972063064575, 'test/num_examples': 10000, 'score': 20643.23784804344, 'total_duration': 22229.7259953022, 'accumulated_submission_time': 20643.23784804344, 'accumulated_eval_time': 1581.5847356319427, 'accumulated_logging_time': 2.3912997245788574, 'global_step': 45079, 'preemption_count': 0}), (45998, {'train/accuracy': 0.6064062118530273, 'train/loss': 1.7620118856430054, 'validation/accuracy': 0.5678600072860718, 'validation/loss': 1.9412288665771484, 'validation/num_examples': 50000, 'test/accuracy': 0.45600003004074097, 'test/loss': 2.5845894813537598, 'test/num_examples': 10000, 'score': 21063.517508268356, 'total_duration': 22680.277902126312, 'accumulated_submission_time': 21063.517508268356, 'accumulated_eval_time': 1611.773603439331, 'accumulated_logging_time': 2.4242405891418457, 'global_step': 45998, 'preemption_count': 0}), (46912, {'train/accuracy': 0.6177538633346558, 'train/loss': 1.7487009763717651, 'validation/accuracy': 0.5697599649429321, 'validation/loss': 1.956942081451416, 'validation/num_examples': 50000, 'test/accuracy': 0.45440003275871277, 'test/loss': 2.5762476921081543, 'test/num_examples': 10000, 'score': 21483.748101711273, 'total_duration': 23131.15274167061, 'accumulated_submission_time': 21483.748101711273, 'accumulated_eval_time': 1642.3246562480927, 'accumulated_logging_time': 2.4671669006347656, 'global_step': 46912, 'preemption_count': 0}), (47827, {'train/accuracy': 0.6209570169448853, 'train/loss': 1.705472469329834, 'validation/accuracy': 0.5658599734306335, 'validation/loss': 1.9722998142242432, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5957021713256836, 'test/num_examples': 10000, 'score': 21903.900598526, 'total_duration': 23582.35942387581, 'accumulated_submission_time': 21903.900598526, 'accumulated_eval_time': 1673.292890548706, 'accumulated_logging_time': 2.5029873847961426, 'global_step': 47827, 'preemption_count': 0}), (48743, {'train/accuracy': 0.604785144329071, 'train/loss': 1.7722070217132568, 'validation/accuracy': 0.568120002746582, 'validation/loss': 1.9483237266540527, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.602745771408081, 'test/num_examples': 10000, 'score': 22323.86534023285, 'total_duration': 24034.65631222725, 'accumulated_submission_time': 22323.86534023285, 'accumulated_eval_time': 1705.5355398654938, 'accumulated_logging_time': 2.5407254695892334, 'global_step': 48743, 'preemption_count': 0}), (49662, {'train/accuracy': 0.6148828268051147, 'train/loss': 1.721850037574768, 'validation/accuracy': 0.5727399587631226, 'validation/loss': 1.9160726070404053, 'validation/num_examples': 50000, 'test/accuracy': 0.4552000164985657, 'test/loss': 2.5426297187805176, 'test/num_examples': 10000, 'score': 22743.817096471786, 'total_duration': 24488.13495373726, 'accumulated_submission_time': 22743.817096471786, 'accumulated_eval_time': 1738.979534626007, 'accumulated_logging_time': 2.573563814163208, 'global_step': 49662, 'preemption_count': 0}), (50580, {'train/accuracy': 0.61865234375, 'train/loss': 1.7359861135482788, 'validation/accuracy': 0.5676199793815613, 'validation/loss': 1.9560025930404663, 'validation/num_examples': 50000, 'test/accuracy': 0.45170003175735474, 'test/loss': 2.5810353755950928, 'test/num_examples': 10000, 'score': 23163.91581749916, 'total_duration': 24939.452256917953, 'accumulated_submission_time': 23163.91581749916, 'accumulated_eval_time': 1770.112517118454, 'accumulated_logging_time': 2.60744047164917, 'global_step': 50580, 'preemption_count': 0}), (51498, {'train/accuracy': 0.6126366853713989, 'train/loss': 1.7147948741912842, 'validation/accuracy': 0.5749599933624268, 'validation/loss': 1.904335618019104, 'validation/num_examples': 50000, 'test/accuracy': 0.45830002427101135, 'test/loss': 2.5383946895599365, 'test/num_examples': 10000, 'score': 23584.244931459427, 'total_duration': 25392.24175477028, 'accumulated_submission_time': 23584.244931459427, 'accumulated_eval_time': 1802.4884850978851, 'accumulated_logging_time': 2.6418302059173584, 'global_step': 51498, 'preemption_count': 0}), (52414, {'train/accuracy': 0.6123437285423279, 'train/loss': 1.7380605936050415, 'validation/accuracy': 0.5724999904632568, 'validation/loss': 1.917758822441101, 'validation/num_examples': 50000, 'test/accuracy': 0.45730000734329224, 'test/loss': 2.529676914215088, 'test/num_examples': 10000, 'score': 24004.364980459213, 'total_duration': 25843.898319244385, 'accumulated_submission_time': 24004.364980459213, 'accumulated_eval_time': 1833.9402480125427, 'accumulated_logging_time': 2.6762707233428955, 'global_step': 52414, 'preemption_count': 0}), (53330, {'train/accuracy': 0.6242578029632568, 'train/loss': 1.6958975791931152, 'validation/accuracy': 0.5759199857711792, 'validation/loss': 1.9176926612854004, 'validation/num_examples': 50000, 'test/accuracy': 0.45210000872612, 'test/loss': 2.568796396255493, 'test/num_examples': 10000, 'score': 24424.607334136963, 'total_duration': 26295.831995487213, 'accumulated_submission_time': 24424.607334136963, 'accumulated_eval_time': 1865.5457472801208, 'accumulated_logging_time': 2.711615800857544, 'global_step': 53330, 'preemption_count': 0}), (54244, {'train/accuracy': 0.6257421970367432, 'train/loss': 1.7023617029190063, 'validation/accuracy': 0.5797600150108337, 'validation/loss': 1.9058101177215576, 'validation/num_examples': 50000, 'test/accuracy': 0.4621000289916992, 'test/loss': 2.5444371700286865, 'test/num_examples': 10000, 'score': 24844.78872013092, 'total_duration': 26748.557849645615, 'accumulated_submission_time': 24844.78872013092, 'accumulated_eval_time': 1897.9944295883179, 'accumulated_logging_time': 2.7577033042907715, 'global_step': 54244, 'preemption_count': 0}), (55162, {'train/accuracy': 0.6188085675239563, 'train/loss': 1.706288456916809, 'validation/accuracy': 0.5832399725914001, 'validation/loss': 1.87911856174469, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.5232367515563965, 'test/num_examples': 10000, 'score': 25264.96487569809, 'total_duration': 27200.61718583107, 'accumulated_submission_time': 25264.96487569809, 'accumulated_eval_time': 1929.7939734458923, 'accumulated_logging_time': 2.7902755737304688, 'global_step': 55162, 'preemption_count': 0}), (56078, {'train/accuracy': 0.6207226514816284, 'train/loss': 1.7265022993087769, 'validation/accuracy': 0.5751399993896484, 'validation/loss': 1.9327647686004639, 'validation/num_examples': 50000, 'test/accuracy': 0.4586000144481659, 'test/loss': 2.5697994232177734, 'test/num_examples': 10000, 'score': 25685.004106283188, 'total_duration': 27652.701270341873, 'accumulated_submission_time': 25685.004106283188, 'accumulated_eval_time': 1961.75643324852, 'accumulated_logging_time': 2.8226845264434814, 'global_step': 56078, 'preemption_count': 0}), (56994, {'train/accuracy': 0.6523046493530273, 'train/loss': 1.600400686264038, 'validation/accuracy': 0.580299973487854, 'validation/loss': 1.9175043106079102, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.537362575531006, 'test/num_examples': 10000, 'score': 26105.01860499382, 'total_duration': 28105.36644411087, 'accumulated_submission_time': 26105.01860499382, 'accumulated_eval_time': 1994.321251630783, 'accumulated_logging_time': 2.8584837913513184, 'global_step': 56994, 'preemption_count': 0}), (57911, {'train/accuracy': 0.6321093440055847, 'train/loss': 1.6738526821136475, 'validation/accuracy': 0.5862999558448792, 'validation/loss': 1.8718279600143433, 'validation/num_examples': 50000, 'test/accuracy': 0.4709000289440155, 'test/loss': 2.4993984699249268, 'test/num_examples': 10000, 'score': 26525.360887289047, 'total_duration': 28557.5066075325, 'accumulated_submission_time': 26525.360887289047, 'accumulated_eval_time': 2026.0364346504211, 'accumulated_logging_time': 2.8909971714019775, 'global_step': 57911, 'preemption_count': 0}), (58829, {'train/accuracy': 0.6249608993530273, 'train/loss': 1.6657127141952515, 'validation/accuracy': 0.5823599696159363, 'validation/loss': 1.8642138242721558, 'validation/num_examples': 50000, 'test/accuracy': 0.4593000113964081, 'test/loss': 2.5110361576080322, 'test/num_examples': 10000, 'score': 26945.452782392502, 'total_duration': 29010.55766916275, 'accumulated_submission_time': 26945.452782392502, 'accumulated_eval_time': 2058.912977695465, 'accumulated_logging_time': 2.924208402633667, 'global_step': 58829, 'preemption_count': 0}), (59747, {'train/accuracy': 0.643359363079071, 'train/loss': 1.587910771369934, 'validation/accuracy': 0.5870400071144104, 'validation/loss': 1.8554558753967285, 'validation/num_examples': 50000, 'test/accuracy': 0.4684000313282013, 'test/loss': 2.4895777702331543, 'test/num_examples': 10000, 'score': 27365.503889799118, 'total_duration': 29464.2038834095, 'accumulated_submission_time': 27365.503889799118, 'accumulated_eval_time': 2092.420877933502, 'accumulated_logging_time': 2.9613430500030518, 'global_step': 59747, 'preemption_count': 0}), (60664, {'train/accuracy': 0.6283788681030273, 'train/loss': 1.6621125936508179, 'validation/accuracy': 0.585319995880127, 'validation/loss': 1.851845622062683, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.470869541168213, 'test/num_examples': 10000, 'score': 27785.721799850464, 'total_duration': 29917.078412532806, 'accumulated_submission_time': 27785.721799850464, 'accumulated_eval_time': 2124.993516921997, 'accumulated_logging_time': 2.995525598526001, 'global_step': 60664, 'preemption_count': 0}), (61582, {'train/accuracy': 0.6338085532188416, 'train/loss': 1.6124237775802612, 'validation/accuracy': 0.591219961643219, 'validation/loss': 1.8236290216445923, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.4673588275909424, 'test/num_examples': 10000, 'score': 28205.97324538231, 'total_duration': 30371.007069826126, 'accumulated_submission_time': 28205.97324538231, 'accumulated_eval_time': 2158.5834772586823, 'accumulated_logging_time': 3.031461477279663, 'global_step': 61582, 'preemption_count': 0}), (62501, {'train/accuracy': 0.6354882717132568, 'train/loss': 1.626826286315918, 'validation/accuracy': 0.5827599763870239, 'validation/loss': 1.8661490678787231, 'validation/num_examples': 50000, 'test/accuracy': 0.46970000863075256, 'test/loss': 2.5128331184387207, 'test/num_examples': 10000, 'score': 28626.62084031105, 'total_duration': 30822.73681116104, 'accumulated_submission_time': 28626.62084031105, 'accumulated_eval_time': 2189.576506137848, 'accumulated_logging_time': 3.0699973106384277, 'global_step': 62501, 'preemption_count': 0}), (63419, {'train/accuracy': 0.6289843320846558, 'train/loss': 1.6643264293670654, 'validation/accuracy': 0.583899974822998, 'validation/loss': 1.8471851348876953, 'validation/num_examples': 50000, 'test/accuracy': 0.4674000144004822, 'test/loss': 2.485158920288086, 'test/num_examples': 10000, 'score': 29046.67929458618, 'total_duration': 31276.068233013153, 'accumulated_submission_time': 29046.67929458618, 'accumulated_eval_time': 2222.7663156986237, 'accumulated_logging_time': 3.1039247512817383, 'global_step': 63419, 'preemption_count': 0}), (64335, {'train/accuracy': 0.6406640410423279, 'train/loss': 1.5750188827514648, 'validation/accuracy': 0.5931800007820129, 'validation/loss': 1.7844353914260864, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.420977830886841, 'test/num_examples': 10000, 'score': 29466.694981575012, 'total_duration': 31728.67764186859, 'accumulated_submission_time': 29466.694981575012, 'accumulated_eval_time': 2255.2734336853027, 'accumulated_logging_time': 3.1402459144592285, 'global_step': 64335, 'preemption_count': 0}), (65251, {'train/accuracy': 0.6419140696525574, 'train/loss': 1.613297700881958, 'validation/accuracy': 0.5929799675941467, 'validation/loss': 1.8457796573638916, 'validation/num_examples': 50000, 'test/accuracy': 0.4781000316143036, 'test/loss': 2.480360984802246, 'test/num_examples': 10000, 'score': 29886.907014369965, 'total_duration': 32181.92774629593, 'accumulated_submission_time': 29886.907014369965, 'accumulated_eval_time': 2288.2237129211426, 'accumulated_logging_time': 3.1775012016296387, 'global_step': 65251, 'preemption_count': 0}), (66167, {'train/accuracy': 0.6465820074081421, 'train/loss': 1.5871703624725342, 'validation/accuracy': 0.596560001373291, 'validation/loss': 1.818373203277588, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.46311092376709, 'test/num_examples': 10000, 'score': 30307.044764518738, 'total_duration': 32634.469495773315, 'accumulated_submission_time': 30307.044764518738, 'accumulated_eval_time': 2320.53994345665, 'accumulated_logging_time': 3.215022325515747, 'global_step': 66167, 'preemption_count': 0}), (67084, {'train/accuracy': 0.6468554735183716, 'train/loss': 1.5900592803955078, 'validation/accuracy': 0.6029599905014038, 'validation/loss': 1.7880793809890747, 'validation/num_examples': 50000, 'test/accuracy': 0.4830000102519989, 'test/loss': 2.422799825668335, 'test/num_examples': 10000, 'score': 30727.09972333908, 'total_duration': 33086.314239263535, 'accumulated_submission_time': 30727.09972333908, 'accumulated_eval_time': 2352.2424013614655, 'accumulated_logging_time': 3.251006841659546, 'global_step': 67084, 'preemption_count': 0}), (68001, {'train/accuracy': 0.6465234160423279, 'train/loss': 1.5876259803771973, 'validation/accuracy': 0.5993599891662598, 'validation/loss': 1.8079662322998047, 'validation/num_examples': 50000, 'test/accuracy': 0.4832000136375427, 'test/loss': 2.4284214973449707, 'test/num_examples': 10000, 'score': 31147.37824487686, 'total_duration': 33539.42764592171, 'accumulated_submission_time': 31147.37824487686, 'accumulated_eval_time': 2384.98783993721, 'accumulated_logging_time': 3.2904880046844482, 'global_step': 68001, 'preemption_count': 0}), (68916, {'train/accuracy': 0.6715624928474426, 'train/loss': 1.442800521850586, 'validation/accuracy': 0.6010000109672546, 'validation/loss': 1.7613905668258667, 'validation/num_examples': 50000, 'test/accuracy': 0.48430001735687256, 'test/loss': 2.398958206176758, 'test/num_examples': 10000, 'score': 31567.33975481987, 'total_duration': 33991.27369570732, 'accumulated_submission_time': 31567.33975481987, 'accumulated_eval_time': 2416.7889444828033, 'accumulated_logging_time': 3.324157476425171, 'global_step': 68916, 'preemption_count': 0}), (69831, {'train/accuracy': 0.6374413967132568, 'train/loss': 1.655277967453003, 'validation/accuracy': 0.5964199900627136, 'validation/loss': 1.8324724435806274, 'validation/num_examples': 50000, 'test/accuracy': 0.48260003328323364, 'test/loss': 2.4573731422424316, 'test/num_examples': 10000, 'score': 31987.69392681122, 'total_duration': 34443.77940249443, 'accumulated_submission_time': 31987.69392681122, 'accumulated_eval_time': 2448.8538324832916, 'accumulated_logging_time': 3.3609249591827393, 'global_step': 69831, 'preemption_count': 0}), (70747, {'train/accuracy': 0.6484960913658142, 'train/loss': 1.5486077070236206, 'validation/accuracy': 0.6013599634170532, 'validation/loss': 1.7578142881393433, 'validation/num_examples': 50000, 'test/accuracy': 0.47630003094673157, 'test/loss': 2.390446662902832, 'test/num_examples': 10000, 'score': 32407.676120996475, 'total_duration': 34893.63464021683, 'accumulated_submission_time': 32407.676120996475, 'accumulated_eval_time': 2478.6409730911255, 'accumulated_logging_time': 3.3965108394622803, 'global_step': 70747, 'preemption_count': 0}), (71662, {'train/accuracy': 0.6622851490974426, 'train/loss': 1.4937940835952759, 'validation/accuracy': 0.6013199687004089, 'validation/loss': 1.7728713750839233, 'validation/num_examples': 50000, 'test/accuracy': 0.4798000156879425, 'test/loss': 2.4019968509674072, 'test/num_examples': 10000, 'score': 32827.82526350021, 'total_duration': 35345.7574763298, 'accumulated_submission_time': 32827.82526350021, 'accumulated_eval_time': 2510.5186746120453, 'accumulated_logging_time': 3.442301034927368, 'global_step': 71662, 'preemption_count': 0}), (72578, {'train/accuracy': 0.6486914157867432, 'train/loss': 1.5760599374771118, 'validation/accuracy': 0.6078199744224548, 'validation/loss': 1.7598813772201538, 'validation/num_examples': 50000, 'test/accuracy': 0.4820000231266022, 'test/loss': 2.406587600708008, 'test/num_examples': 10000, 'score': 33247.85134124756, 'total_duration': 35798.13573217392, 'accumulated_submission_time': 33247.85134124756, 'accumulated_eval_time': 2542.7840864658356, 'accumulated_logging_time': 3.47953462600708, 'global_step': 72578, 'preemption_count': 0}), (73498, {'train/accuracy': 0.650683581829071, 'train/loss': 1.5488523244857788, 'validation/accuracy': 0.602840006351471, 'validation/loss': 1.7498250007629395, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.388460159301758, 'test/num_examples': 10000, 'score': 33668.151161670685, 'total_duration': 36250.21720933914, 'accumulated_submission_time': 33668.151161670685, 'accumulated_eval_time': 2574.4699428081512, 'accumulated_logging_time': 3.524752616882324, 'global_step': 73498, 'preemption_count': 0}), (74415, {'train/accuracy': 0.6654492020606995, 'train/loss': 1.506258487701416, 'validation/accuracy': 0.6062999963760376, 'validation/loss': 1.7637112140655518, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.3955132961273193, 'test/num_examples': 10000, 'score': 34088.156512498856, 'total_duration': 36701.84150767326, 'accumulated_submission_time': 34088.156512498856, 'accumulated_eval_time': 2606.0012538433075, 'accumulated_logging_time': 3.561843156814575, 'global_step': 74415, 'preemption_count': 0}), (75333, {'train/accuracy': 0.6486523151397705, 'train/loss': 1.547737956047058, 'validation/accuracy': 0.612500011920929, 'validation/loss': 1.729038119316101, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3694193363189697, 'test/num_examples': 10000, 'score': 34508.42599821091, 'total_duration': 37156.241079092026, 'accumulated_submission_time': 34508.42599821091, 'accumulated_eval_time': 2640.042801618576, 'accumulated_logging_time': 3.6005239486694336, 'global_step': 75333, 'preemption_count': 0}), (76251, {'train/accuracy': 0.6528710722923279, 'train/loss': 1.5526872873306274, 'validation/accuracy': 0.6100199818611145, 'validation/loss': 1.7567421197891235, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.3885414600372314, 'test/num_examples': 10000, 'score': 34928.72370290756, 'total_duration': 37609.87198114395, 'accumulated_submission_time': 34928.72370290756, 'accumulated_eval_time': 2673.2882075309753, 'accumulated_logging_time': 3.637963056564331, 'global_step': 76251, 'preemption_count': 0}), (77167, {'train/accuracy': 0.6625585556030273, 'train/loss': 1.513722538948059, 'validation/accuracy': 0.6113399863243103, 'validation/loss': 1.7449710369110107, 'validation/num_examples': 50000, 'test/accuracy': 0.4903000295162201, 'test/loss': 2.388901948928833, 'test/num_examples': 10000, 'score': 35348.81794667244, 'total_duration': 38064.8754966259, 'accumulated_submission_time': 35348.81794667244, 'accumulated_eval_time': 2708.1096754074097, 'accumulated_logging_time': 3.672785997390747, 'global_step': 77167, 'preemption_count': 0}), (78083, {'train/accuracy': 0.6606640219688416, 'train/loss': 1.493207335472107, 'validation/accuracy': 0.6092599630355835, 'validation/loss': 1.7219427824020386, 'validation/num_examples': 50000, 'test/accuracy': 0.4865000247955322, 'test/loss': 2.3637871742248535, 'test/num_examples': 10000, 'score': 35768.99364566803, 'total_duration': 38517.90196752548, 'accumulated_submission_time': 35768.99364566803, 'accumulated_eval_time': 2740.87061214447, 'accumulated_logging_time': 3.7120413780212402, 'global_step': 78083, 'preemption_count': 0}), (78998, {'train/accuracy': 0.659375011920929, 'train/loss': 1.5171035528182983, 'validation/accuracy': 0.6125400066375732, 'validation/loss': 1.7171244621276855, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.359149217605591, 'test/num_examples': 10000, 'score': 36189.29687142372, 'total_duration': 38971.674016952515, 'accumulated_submission_time': 36189.29687142372, 'accumulated_eval_time': 2774.2490651607513, 'accumulated_logging_time': 3.7525453567504883, 'global_step': 78998, 'preemption_count': 0}), (79915, {'train/accuracy': 0.66845703125, 'train/loss': 1.4566603899002075, 'validation/accuracy': 0.6201399564743042, 'validation/loss': 1.6755112409591675, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.326002836227417, 'test/num_examples': 10000, 'score': 36609.45451402664, 'total_duration': 39425.04754161835, 'accumulated_submission_time': 36609.45451402664, 'accumulated_eval_time': 2807.375717639923, 'accumulated_logging_time': 3.791879415512085, 'global_step': 79915, 'preemption_count': 0}), (80832, {'train/accuracy': 0.6868945360183716, 'train/loss': 1.4382601976394653, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.7528892755508423, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.4042420387268066, 'test/num_examples': 10000, 'score': 37029.78786754608, 'total_duration': 39875.934716939926, 'accumulated_submission_time': 37029.78786754608, 'accumulated_eval_time': 2837.8364627361298, 'accumulated_logging_time': 3.83516001701355, 'global_step': 80832, 'preemption_count': 0}), (81747, {'train/accuracy': 0.6599804759025574, 'train/loss': 1.4933381080627441, 'validation/accuracy': 0.6163600087165833, 'validation/loss': 1.6874927282333374, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3316457271575928, 'test/num_examples': 10000, 'score': 37449.77145719528, 'total_duration': 40329.72599077225, 'accumulated_submission_time': 37449.77145719528, 'accumulated_eval_time': 2871.551378250122, 'accumulated_logging_time': 3.878218173980713, 'global_step': 81747, 'preemption_count': 0}), (82663, {'train/accuracy': 0.6682031154632568, 'train/loss': 1.4543018341064453, 'validation/accuracy': 0.6208800077438354, 'validation/loss': 1.668615698814392, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.3003392219543457, 'test/num_examples': 10000, 'score': 37869.73485660553, 'total_duration': 40781.922105789185, 'accumulated_submission_time': 37869.73485660553, 'accumulated_eval_time': 2903.6971168518066, 'accumulated_logging_time': 3.9156272411346436, 'global_step': 82663, 'preemption_count': 0}), (83581, {'train/accuracy': 0.6833788752555847, 'train/loss': 1.3950210809707642, 'validation/accuracy': 0.6195200085639954, 'validation/loss': 1.6812617778778076, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.320722818374634, 'test/num_examples': 10000, 'score': 38289.803233623505, 'total_duration': 41234.16646409035, 'accumulated_submission_time': 38289.803233623505, 'accumulated_eval_time': 2935.783272266388, 'accumulated_logging_time': 3.9549033641815186, 'global_step': 83581, 'preemption_count': 0}), (84496, {'train/accuracy': 0.6632617115974426, 'train/loss': 1.50346839427948, 'validation/accuracy': 0.6191200017929077, 'validation/loss': 1.6938670873641968, 'validation/num_examples': 50000, 'test/accuracy': 0.49340003728866577, 'test/loss': 2.347602367401123, 'test/num_examples': 10000, 'score': 38709.94033956528, 'total_duration': 41688.10350394249, 'accumulated_submission_time': 38709.94033956528, 'accumulated_eval_time': 2969.496173620224, 'accumulated_logging_time': 3.991729259490967, 'global_step': 84496, 'preemption_count': 0}), (85412, {'train/accuracy': 0.6689453125, 'train/loss': 1.475509762763977, 'validation/accuracy': 0.6204000115394592, 'validation/loss': 1.6864943504333496, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3224990367889404, 'test/num_examples': 10000, 'score': 39129.99418258667, 'total_duration': 42139.39819288254, 'accumulated_submission_time': 39129.99418258667, 'accumulated_eval_time': 3000.645429611206, 'accumulated_logging_time': 4.0334930419921875, 'global_step': 85412, 'preemption_count': 0}), (86326, {'train/accuracy': 0.679492175579071, 'train/loss': 1.427093505859375, 'validation/accuracy': 0.627299964427948, 'validation/loss': 1.6685994863510132, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.304348945617676, 'test/num_examples': 10000, 'score': 39550.02034115791, 'total_duration': 42593.61324310303, 'accumulated_submission_time': 39550.02034115791, 'accumulated_eval_time': 3034.7353508472443, 'accumulated_logging_time': 4.0823657512664795, 'global_step': 86326, 'preemption_count': 0}), (87244, {'train/accuracy': 0.6720312237739563, 'train/loss': 1.4575451612472534, 'validation/accuracy': 0.6271199584007263, 'validation/loss': 1.6576414108276367, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.290591239929199, 'test/num_examples': 10000, 'score': 39969.945786714554, 'total_duration': 43046.86581158638, 'accumulated_submission_time': 39969.945786714554, 'accumulated_eval_time': 3067.9726133346558, 'accumulated_logging_time': 4.12157940864563, 'global_step': 87244, 'preemption_count': 0}), (88162, {'train/accuracy': 0.6704687476158142, 'train/loss': 1.4411784410476685, 'validation/accuracy': 0.622439980506897, 'validation/loss': 1.6661144495010376, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.302229642868042, 'test/num_examples': 10000, 'score': 40389.9857711792, 'total_duration': 43500.610604286194, 'accumulated_submission_time': 40389.9857711792, 'accumulated_eval_time': 3101.585325717926, 'accumulated_logging_time': 4.163185358047485, 'global_step': 88162, 'preemption_count': 0}), (89080, {'train/accuracy': 0.6823828220367432, 'train/loss': 1.4180799722671509, 'validation/accuracy': 0.6261999607086182, 'validation/loss': 1.6651922464370728, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2891640663146973, 'test/num_examples': 10000, 'score': 40810.23924612999, 'total_duration': 43950.955739974976, 'accumulated_submission_time': 40810.23924612999, 'accumulated_eval_time': 3131.58740067482, 'accumulated_logging_time': 4.202484369277954, 'global_step': 89080, 'preemption_count': 0}), (89995, {'train/accuracy': 0.6862109303474426, 'train/loss': 1.4118703603744507, 'validation/accuracy': 0.6287199854850769, 'validation/loss': 1.6493552923202515, 'validation/num_examples': 50000, 'test/accuracy': 0.5055000185966492, 'test/loss': 2.2911794185638428, 'test/num_examples': 10000, 'score': 41230.43558573723, 'total_duration': 44403.9231262207, 'accumulated_submission_time': 41230.43558573723, 'accumulated_eval_time': 3164.264275074005, 'accumulated_logging_time': 4.245952367782593, 'global_step': 89995, 'preemption_count': 0}), (90912, {'train/accuracy': 0.6795117259025574, 'train/loss': 1.4182102680206299, 'validation/accuracy': 0.6332799792289734, 'validation/loss': 1.625242829322815, 'validation/num_examples': 50000, 'test/accuracy': 0.5124000310897827, 'test/loss': 2.259521722793579, 'test/num_examples': 10000, 'score': 41650.82543206215, 'total_duration': 44855.7765583992, 'accumulated_submission_time': 41650.82543206215, 'accumulated_eval_time': 3195.6368894577026, 'accumulated_logging_time': 4.286664247512817, 'global_step': 90912, 'preemption_count': 0}), (91828, {'train/accuracy': 0.6826757788658142, 'train/loss': 1.4430053234100342, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.6762853860855103, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.3019185066223145, 'test/num_examples': 10000, 'score': 42070.84104323387, 'total_duration': 45308.86335706711, 'accumulated_submission_time': 42070.84104323387, 'accumulated_eval_time': 3228.6166894435883, 'accumulated_logging_time': 4.32750678062439, 'global_step': 91828, 'preemption_count': 0}), (92745, {'train/accuracy': 0.7028710842132568, 'train/loss': 1.3684577941894531, 'validation/accuracy': 0.6319199800491333, 'validation/loss': 1.6790177822113037, 'validation/num_examples': 50000, 'test/accuracy': 0.5100000500679016, 'test/loss': 2.3166470527648926, 'test/num_examples': 10000, 'score': 42490.799355745316, 'total_duration': 45759.4947450161, 'accumulated_submission_time': 42490.799355745316, 'accumulated_eval_time': 3259.2013654708862, 'accumulated_logging_time': 4.36621356010437, 'global_step': 92745, 'preemption_count': 0}), (93656, {'train/accuracy': 0.6768945455551147, 'train/loss': 1.43509840965271, 'validation/accuracy': 0.6339799761772156, 'validation/loss': 1.6315512657165527, 'validation/num_examples': 50000, 'test/accuracy': 0.5134000182151794, 'test/loss': 2.2607595920562744, 'test/num_examples': 10000, 'score': 42910.90014410019, 'total_duration': 46213.121638059616, 'accumulated_submission_time': 42910.90014410019, 'accumulated_eval_time': 3292.6324059963226, 'accumulated_logging_time': 4.411803960800171, 'global_step': 93656, 'preemption_count': 0}), (94571, {'train/accuracy': 0.6860546469688416, 'train/loss': 1.4129973649978638, 'validation/accuracy': 0.6372999548912048, 'validation/loss': 1.6310029029846191, 'validation/num_examples': 50000, 'test/accuracy': 0.5184000134468079, 'test/loss': 2.2514212131500244, 'test/num_examples': 10000, 'score': 43331.148206710815, 'total_duration': 46666.15451836586, 'accumulated_submission_time': 43331.148206710815, 'accumulated_eval_time': 3325.3265323638916, 'accumulated_logging_time': 4.453327894210815, 'global_step': 94571, 'preemption_count': 0}), (95488, {'train/accuracy': 0.6981640458106995, 'train/loss': 1.355225682258606, 'validation/accuracy': 0.6358199715614319, 'validation/loss': 1.629407286643982, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.261467218399048, 'test/num_examples': 10000, 'score': 43751.27952218056, 'total_duration': 47117.990614652634, 'accumulated_submission_time': 43751.27952218056, 'accumulated_eval_time': 3356.9392170906067, 'accumulated_logging_time': 4.495721101760864, 'global_step': 95488, 'preemption_count': 0}), (96404, {'train/accuracy': 0.6825000047683716, 'train/loss': 1.4293886423110962, 'validation/accuracy': 0.6386799812316895, 'validation/loss': 1.6273837089538574, 'validation/num_examples': 50000, 'test/accuracy': 0.5163000226020813, 'test/loss': 2.2728638648986816, 'test/num_examples': 10000, 'score': 44171.423545598984, 'total_duration': 47572.40820598602, 'accumulated_submission_time': 44171.423545598984, 'accumulated_eval_time': 3391.115294933319, 'accumulated_logging_time': 4.54344367980957, 'global_step': 96404, 'preemption_count': 0}), (97322, {'train/accuracy': 0.6897070407867432, 'train/loss': 1.3623371124267578, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.586683988571167, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2348742485046387, 'test/num_examples': 10000, 'score': 44591.63816213608, 'total_duration': 48025.32202601433, 'accumulated_submission_time': 44591.63816213608, 'accumulated_eval_time': 3423.7243151664734, 'accumulated_logging_time': 4.584112644195557, 'global_step': 97322, 'preemption_count': 0}), (98238, {'train/accuracy': 0.7014843821525574, 'train/loss': 1.3189449310302734, 'validation/accuracy': 0.646399974822998, 'validation/loss': 1.5679948329925537, 'validation/num_examples': 50000, 'test/accuracy': 0.5222000479698181, 'test/loss': 2.2077906131744385, 'test/num_examples': 10000, 'score': 45011.66071987152, 'total_duration': 48479.070209264755, 'accumulated_submission_time': 45011.66071987152, 'accumulated_eval_time': 3457.3588194847107, 'accumulated_logging_time': 4.624876022338867, 'global_step': 98238, 'preemption_count': 0}), (99156, {'train/accuracy': 0.69189453125, 'train/loss': 1.3856916427612305, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.6010757684707642, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.2255241870880127, 'test/num_examples': 10000, 'score': 45431.88610672951, 'total_duration': 48932.92041492462, 'accumulated_submission_time': 45431.88610672951, 'accumulated_eval_time': 3490.8936812877655, 'accumulated_logging_time': 4.664891719818115, 'global_step': 99156, 'preemption_count': 0}), (100073, {'train/accuracy': 0.6983593702316284, 'train/loss': 1.3234851360321045, 'validation/accuracy': 0.646619975566864, 'validation/loss': 1.549912452697754, 'validation/num_examples': 50000, 'test/accuracy': 0.527999997138977, 'test/loss': 2.1694071292877197, 'test/num_examples': 10000, 'score': 45852.21845984459, 'total_duration': 49385.12797021866, 'accumulated_submission_time': 45852.21845984459, 'accumulated_eval_time': 3522.6747205257416, 'accumulated_logging_time': 4.70860743522644, 'global_step': 100073, 'preemption_count': 0}), (100985, {'train/accuracy': 0.6984570026397705, 'train/loss': 1.3088330030441284, 'validation/accuracy': 0.6455999612808228, 'validation/loss': 1.5476953983306885, 'validation/num_examples': 50000, 'test/accuracy': 0.5232000350952148, 'test/loss': 2.1762855052948, 'test/num_examples': 10000, 'score': 46272.5213842392, 'total_duration': 49835.674525260925, 'accumulated_submission_time': 46272.5213842392, 'accumulated_eval_time': 3552.8196897506714, 'accumulated_logging_time': 4.7579333782196045, 'global_step': 100985, 'preemption_count': 0}), (101900, {'train/accuracy': 0.7024999856948853, 'train/loss': 1.3207366466522217, 'validation/accuracy': 0.6457799673080444, 'validation/loss': 1.5591189861297607, 'validation/num_examples': 50000, 'test/accuracy': 0.5252000093460083, 'test/loss': 2.188363552093506, 'test/num_examples': 10000, 'score': 46692.711477041245, 'total_duration': 50287.51734471321, 'accumulated_submission_time': 46692.711477041245, 'accumulated_eval_time': 3584.373185634613, 'accumulated_logging_time': 4.80739164352417, 'global_step': 101900, 'preemption_count': 0}), (102818, {'train/accuracy': 0.6942577958106995, 'train/loss': 1.4096049070358276, 'validation/accuracy': 0.644599974155426, 'validation/loss': 1.627078652381897, 'validation/num_examples': 50000, 'test/accuracy': 0.5247000455856323, 'test/loss': 2.2475533485412598, 'test/num_examples': 10000, 'score': 47113.01589202881, 'total_duration': 50740.492428302765, 'accumulated_submission_time': 47113.01589202881, 'accumulated_eval_time': 3616.9405534267426, 'accumulated_logging_time': 4.861057758331299, 'global_step': 102818, 'preemption_count': 0}), (103735, {'train/accuracy': 0.7060937285423279, 'train/loss': 1.303206443786621, 'validation/accuracy': 0.6481800079345703, 'validation/loss': 1.5560694932937622, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.18750262260437, 'test/num_examples': 10000, 'score': 47533.03374886513, 'total_duration': 51193.859845638275, 'accumulated_submission_time': 47533.03374886513, 'accumulated_eval_time': 3650.1980924606323, 'accumulated_logging_time': 4.902195453643799, 'global_step': 103735, 'preemption_count': 0}), (104652, {'train/accuracy': 0.728710949420929, 'train/loss': 1.2095915079116821, 'validation/accuracy': 0.655519962310791, 'validation/loss': 1.5235939025878906, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.1618435382843018, 'test/num_examples': 10000, 'score': 47953.09908533096, 'total_duration': 51646.89317679405, 'accumulated_submission_time': 47953.09908533096, 'accumulated_eval_time': 3683.071009159088, 'accumulated_logging_time': 4.946327209472656, 'global_step': 104652, 'preemption_count': 0}), (105569, {'train/accuracy': 0.7003515362739563, 'train/loss': 1.328979730606079, 'validation/accuracy': 0.6550799608230591, 'validation/loss': 1.5396244525909424, 'validation/num_examples': 50000, 'test/accuracy': 0.535800039768219, 'test/loss': 2.1648764610290527, 'test/num_examples': 10000, 'score': 48373.37495970726, 'total_duration': 52099.889456510544, 'accumulated_submission_time': 48373.37495970726, 'accumulated_eval_time': 3715.691724061966, 'accumulated_logging_time': 4.996261835098267, 'global_step': 105569, 'preemption_count': 0}), (106485, {'train/accuracy': 0.706250011920929, 'train/loss': 1.3207530975341797, 'validation/accuracy': 0.6521199941635132, 'validation/loss': 1.5590405464172363, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.192826986312866, 'test/num_examples': 10000, 'score': 48793.45449113846, 'total_duration': 52553.79598546028, 'accumulated_submission_time': 48793.45449113846, 'accumulated_eval_time': 3749.4249980449677, 'accumulated_logging_time': 5.040008306503296, 'global_step': 106485, 'preemption_count': 0}), (107401, {'train/accuracy': 0.7234960794448853, 'train/loss': 1.20840322971344, 'validation/accuracy': 0.6581199765205383, 'validation/loss': 1.5009328126907349, 'validation/num_examples': 50000, 'test/accuracy': 0.5392000079154968, 'test/loss': 2.1324949264526367, 'test/num_examples': 10000, 'score': 49214.00630617142, 'total_duration': 53007.55867099762, 'accumulated_submission_time': 49214.00630617142, 'accumulated_eval_time': 3782.5435230731964, 'accumulated_logging_time': 5.082547903060913, 'global_step': 107401, 'preemption_count': 0}), (108316, {'train/accuracy': 0.7032812237739563, 'train/loss': 1.2869420051574707, 'validation/accuracy': 0.6589800119400024, 'validation/loss': 1.5009398460388184, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.155637502670288, 'test/num_examples': 10000, 'score': 49634.25281214714, 'total_duration': 53459.401591300964, 'accumulated_submission_time': 49634.25281214714, 'accumulated_eval_time': 3814.0512182712555, 'accumulated_logging_time': 5.121634244918823, 'global_step': 108316, 'preemption_count': 0}), (109232, {'train/accuracy': 0.7122656106948853, 'train/loss': 1.288287878036499, 'validation/accuracy': 0.657759964466095, 'validation/loss': 1.5199744701385498, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.154524326324463, 'test/num_examples': 10000, 'score': 50054.5297896862, 'total_duration': 53911.006695985794, 'accumulated_submission_time': 50054.5297896862, 'accumulated_eval_time': 3845.278760910034, 'accumulated_logging_time': 5.172155141830444, 'global_step': 109232, 'preemption_count': 0}), (110148, {'train/accuracy': 0.7259570360183716, 'train/loss': 1.2455788850784302, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.5217573642730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5366000533103943, 'test/loss': 2.149949550628662, 'test/num_examples': 10000, 'score': 50474.51218295097, 'total_duration': 54364.08873963356, 'accumulated_submission_time': 50474.51218295097, 'accumulated_eval_time': 3878.277625799179, 'accumulated_logging_time': 5.222049713134766, 'global_step': 110148, 'preemption_count': 0}), (111065, {'train/accuracy': 0.71337890625, 'train/loss': 1.262039303779602, 'validation/accuracy': 0.6626600027084351, 'validation/loss': 1.4916067123413086, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.113887310028076, 'test/num_examples': 10000, 'score': 50894.50727891922, 'total_duration': 54817.203261852264, 'accumulated_submission_time': 50894.50727891922, 'accumulated_eval_time': 3911.3065781593323, 'accumulated_logging_time': 5.262262582778931, 'global_step': 111065, 'preemption_count': 0}), (111980, {'train/accuracy': 0.7187108993530273, 'train/loss': 1.2647498846054077, 'validation/accuracy': 0.6654999852180481, 'validation/loss': 1.5069676637649536, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.1428349018096924, 'test/num_examples': 10000, 'score': 51314.64157676697, 'total_duration': 55270.057732343674, 'accumulated_submission_time': 51314.64157676697, 'accumulated_eval_time': 3943.933856487274, 'accumulated_logging_time': 5.305820465087891, 'global_step': 111980, 'preemption_count': 0}), (112896, {'train/accuracy': 0.7238085865974426, 'train/loss': 1.2229537963867188, 'validation/accuracy': 0.6610999703407288, 'validation/loss': 1.4882365465164185, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1151599884033203, 'test/num_examples': 10000, 'score': 51734.58570885658, 'total_duration': 55722.437901735306, 'accumulated_submission_time': 51734.58570885658, 'accumulated_eval_time': 3976.276055574417, 'accumulated_logging_time': 5.350057363510132, 'global_step': 112896, 'preemption_count': 0}), (113811, {'train/accuracy': 0.7215625047683716, 'train/loss': 1.2210685014724731, 'validation/accuracy': 0.6649199724197388, 'validation/loss': 1.4647303819656372, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.0943243503570557, 'test/num_examples': 10000, 'score': 52154.663420677185, 'total_duration': 56172.92315030098, 'accumulated_submission_time': 52154.663420677185, 'accumulated_eval_time': 4006.590903520584, 'accumulated_logging_time': 5.393388271331787, 'global_step': 113811, 'preemption_count': 0}), (114726, {'train/accuracy': 0.7206835746765137, 'train/loss': 1.2099860906600952, 'validation/accuracy': 0.67249995470047, 'validation/loss': 1.443393349647522, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.0834412574768066, 'test/num_examples': 10000, 'score': 52575.012565374374, 'total_duration': 56625.05432701111, 'accumulated_submission_time': 52575.012565374374, 'accumulated_eval_time': 4038.273143053055, 'accumulated_logging_time': 5.442919015884399, 'global_step': 114726, 'preemption_count': 0}), (115640, {'train/accuracy': 0.7305468320846558, 'train/loss': 1.191508173942566, 'validation/accuracy': 0.67249995470047, 'validation/loss': 1.4450451135635376, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.064040422439575, 'test/num_examples': 10000, 'score': 52995.04573082924, 'total_duration': 57079.766466617584, 'accumulated_submission_time': 52995.04573082924, 'accumulated_eval_time': 4072.8560173511505, 'accumulated_logging_time': 5.4888222217559814, 'global_step': 115640, 'preemption_count': 0}), (116556, {'train/accuracy': 0.7452148199081421, 'train/loss': 1.1201274394989014, 'validation/accuracy': 0.6720199584960938, 'validation/loss': 1.4388738870620728, 'validation/num_examples': 50000, 'test/accuracy': 0.5498000383377075, 'test/loss': 2.0630745887756348, 'test/num_examples': 10000, 'score': 53415.35868215561, 'total_duration': 57533.651420116425, 'accumulated_submission_time': 53415.35868215561, 'accumulated_eval_time': 4106.33264541626, 'accumulated_logging_time': 5.534053087234497, 'global_step': 116556, 'preemption_count': 0}), (117472, {'train/accuracy': 0.7250390648841858, 'train/loss': 1.2660032510757446, 'validation/accuracy': 0.667199969291687, 'validation/loss': 1.5061935186386108, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 2.128594160079956, 'test/num_examples': 10000, 'score': 53835.411581754684, 'total_duration': 57987.13926744461, 'accumulated_submission_time': 53835.411581754684, 'accumulated_eval_time': 4139.674651861191, 'accumulated_logging_time': 5.577041864395142, 'global_step': 117472, 'preemption_count': 0}), (118386, {'train/accuracy': 0.73046875, 'train/loss': 1.1939589977264404, 'validation/accuracy': 0.6730200052261353, 'validation/loss': 1.4451491832733154, 'validation/num_examples': 50000, 'test/accuracy': 0.5502000451087952, 'test/loss': 2.0854063034057617, 'test/num_examples': 10000, 'score': 54255.57951760292, 'total_duration': 58440.40096735954, 'accumulated_submission_time': 54255.57951760292, 'accumulated_eval_time': 4172.6747174263, 'accumulated_logging_time': 5.619581699371338, 'global_step': 118386, 'preemption_count': 0}), (119299, {'train/accuracy': 0.7431640625, 'train/loss': 1.1251846551895142, 'validation/accuracy': 0.6783999800682068, 'validation/loss': 1.4041777849197388, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.039374589920044, 'test/num_examples': 10000, 'score': 54675.842202186584, 'total_duration': 58894.145411252975, 'accumulated_submission_time': 54675.842202186584, 'accumulated_eval_time': 4206.0634133815765, 'accumulated_logging_time': 5.661855459213257, 'global_step': 119299, 'preemption_count': 0}), (120216, {'train/accuracy': 0.7290429472923279, 'train/loss': 1.183713674545288, 'validation/accuracy': 0.6782199740409851, 'validation/loss': 1.4144235849380493, 'validation/num_examples': 50000, 'test/accuracy': 0.5534999966621399, 'test/loss': 2.0480198860168457, 'test/num_examples': 10000, 'score': 55095.76208996773, 'total_duration': 59347.97211909294, 'accumulated_submission_time': 55095.76208996773, 'accumulated_eval_time': 4239.874573707581, 'accumulated_logging_time': 5.706495046615601, 'global_step': 120216, 'preemption_count': 0}), (121132, {'train/accuracy': 0.7370507717132568, 'train/loss': 1.166053056716919, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.4264823198318481, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.0637381076812744, 'test/num_examples': 10000, 'score': 55516.00203704834, 'total_duration': 59800.793695926666, 'accumulated_submission_time': 55516.00203704834, 'accumulated_eval_time': 4272.361785888672, 'accumulated_logging_time': 5.750799179077148, 'global_step': 121132, 'preemption_count': 0}), (122049, {'train/accuracy': 0.7417968511581421, 'train/loss': 1.1477159261703491, 'validation/accuracy': 0.6811599731445312, 'validation/loss': 1.4139902591705322, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.025280237197876, 'test/num_examples': 10000, 'score': 55936.13686299324, 'total_duration': 60254.25495505333, 'accumulated_submission_time': 55936.13686299324, 'accumulated_eval_time': 4305.591577529907, 'accumulated_logging_time': 5.79656982421875, 'global_step': 122049, 'preemption_count': 0}), (122963, {'train/accuracy': 0.7372655868530273, 'train/loss': 1.1513622999191284, 'validation/accuracy': 0.688979983329773, 'validation/loss': 1.3695464134216309, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 2.017380475997925, 'test/num_examples': 10000, 'score': 56356.257717609406, 'total_duration': 60706.10312247276, 'accumulated_submission_time': 56356.257717609406, 'accumulated_eval_time': 4337.223828554153, 'accumulated_logging_time': 5.842200517654419, 'global_step': 122963, 'preemption_count': 0}), (123878, {'train/accuracy': 0.7439843416213989, 'train/loss': 1.1398800611495972, 'validation/accuracy': 0.6844399571418762, 'validation/loss': 1.3942458629608154, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 2.015331745147705, 'test/num_examples': 10000, 'score': 56776.523052453995, 'total_duration': 61159.25761413574, 'accumulated_submission_time': 56776.523052453995, 'accumulated_eval_time': 4370.009130716324, 'accumulated_logging_time': 5.895697832107544, 'global_step': 123878, 'preemption_count': 0}), (124794, {'train/accuracy': 0.74818354845047, 'train/loss': 1.1148760318756104, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.3866878747940063, 'validation/num_examples': 50000, 'test/accuracy': 0.562000036239624, 'test/loss': 2.016331672668457, 'test/num_examples': 10000, 'score': 57196.867312431335, 'total_duration': 61613.43428826332, 'accumulated_submission_time': 57196.867312431335, 'accumulated_eval_time': 4403.7414972782135, 'accumulated_logging_time': 5.944585561752319, 'global_step': 124794, 'preemption_count': 0}), (125709, {'train/accuracy': 0.7470703125, 'train/loss': 1.1385987997055054, 'validation/accuracy': 0.6839199662208557, 'validation/loss': 1.4028888940811157, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.021228790283203, 'test/num_examples': 10000, 'score': 57616.94558739662, 'total_duration': 62064.4964826107, 'accumulated_submission_time': 57616.94558739662, 'accumulated_eval_time': 4434.629065275192, 'accumulated_logging_time': 5.990480661392212, 'global_step': 125709, 'preemption_count': 0}), (126620, {'train/accuracy': 0.7459765672683716, 'train/loss': 1.1232553720474243, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.3786057233810425, 'validation/num_examples': 50000, 'test/accuracy': 0.5684000253677368, 'test/loss': 1.9932000637054443, 'test/num_examples': 10000, 'score': 58037.063480854034, 'total_duration': 62516.18090867996, 'accumulated_submission_time': 58037.063480854034, 'accumulated_eval_time': 4466.095130681992, 'accumulated_logging_time': 6.0411882400512695, 'global_step': 126620, 'preemption_count': 0}), (127535, {'train/accuracy': 0.7511913776397705, 'train/loss': 1.0778286457061768, 'validation/accuracy': 0.6904999613761902, 'validation/loss': 1.3422291278839111, 'validation/num_examples': 50000, 'test/accuracy': 0.5659000277519226, 'test/loss': 1.9629532098770142, 'test/num_examples': 10000, 'score': 58457.123943567276, 'total_duration': 62969.93001675606, 'accumulated_submission_time': 58457.123943567276, 'accumulated_eval_time': 4499.6793439388275, 'accumulated_logging_time': 6.095107316970825, 'global_step': 127535, 'preemption_count': 0}), (128452, {'train/accuracy': 0.7696874737739563, 'train/loss': 1.0241750478744507, 'validation/accuracy': 0.6924600005149841, 'validation/loss': 1.3428523540496826, 'validation/num_examples': 50000, 'test/accuracy': 0.5690000057220459, 'test/loss': 1.9711560010910034, 'test/num_examples': 10000, 'score': 58877.354343652725, 'total_duration': 63421.92477989197, 'accumulated_submission_time': 58877.354343652725, 'accumulated_eval_time': 4531.347820997238, 'accumulated_logging_time': 6.140000343322754, 'global_step': 128452, 'preemption_count': 0}), (129367, {'train/accuracy': 0.7485156059265137, 'train/loss': 1.1012896299362183, 'validation/accuracy': 0.6915199756622314, 'validation/loss': 1.3491532802581787, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 1.9738880395889282, 'test/num_examples': 10000, 'score': 59297.305203437805, 'total_duration': 63875.34980750084, 'accumulated_submission_time': 59297.305203437805, 'accumulated_eval_time': 4564.715627908707, 'accumulated_logging_time': 6.195974826812744, 'global_step': 129367, 'preemption_count': 0}), (130283, {'train/accuracy': 0.7594531178474426, 'train/loss': 1.0408467054367065, 'validation/accuracy': 0.6985399723052979, 'validation/loss': 1.3164602518081665, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.9376977682113647, 'test/num_examples': 10000, 'score': 59717.62939977646, 'total_duration': 64328.16715955734, 'accumulated_submission_time': 59717.62939977646, 'accumulated_eval_time': 4597.110694646835, 'accumulated_logging_time': 6.243996620178223, 'global_step': 130283, 'preemption_count': 0}), (131200, {'train/accuracy': 0.7681640386581421, 'train/loss': 1.0163657665252686, 'validation/accuracy': 0.6958799958229065, 'validation/loss': 1.330001711845398, 'validation/num_examples': 50000, 'test/accuracy': 0.5750000476837158, 'test/loss': 1.946197748184204, 'test/num_examples': 10000, 'score': 60138.00821852684, 'total_duration': 64781.60162329674, 'accumulated_submission_time': 60138.00821852684, 'accumulated_eval_time': 4630.069223642349, 'accumulated_logging_time': 6.288846492767334, 'global_step': 131200, 'preemption_count': 0}), (132116, {'train/accuracy': 0.7521288990974426, 'train/loss': 1.0870712995529175, 'validation/accuracy': 0.6966399550437927, 'validation/loss': 1.3381699323654175, 'validation/num_examples': 50000, 'test/accuracy': 0.5717000365257263, 'test/loss': 1.9606136083602905, 'test/num_examples': 10000, 'score': 60558.07671093941, 'total_duration': 65233.88895082474, 'accumulated_submission_time': 60558.07671093941, 'accumulated_eval_time': 4662.193197011948, 'accumulated_logging_time': 6.333850622177124, 'global_step': 132116, 'preemption_count': 0}), (133031, {'train/accuracy': 0.7635741829872131, 'train/loss': 1.053499698638916, 'validation/accuracy': 0.6963199973106384, 'validation/loss': 1.3278359174728394, 'validation/num_examples': 50000, 'test/accuracy': 0.5728000402450562, 'test/loss': 1.9535140991210938, 'test/num_examples': 10000, 'score': 60978.0214304924, 'total_duration': 65686.31872987747, 'accumulated_submission_time': 60978.0214304924, 'accumulated_eval_time': 4694.585594415665, 'accumulated_logging_time': 6.377013444900513, 'global_step': 133031, 'preemption_count': 0}), (133946, {'train/accuracy': 0.7712304592132568, 'train/loss': 0.9981160759925842, 'validation/accuracy': 0.7023999691009521, 'validation/loss': 1.3043373823165894, 'validation/num_examples': 50000, 'test/accuracy': 0.5840000510215759, 'test/loss': 1.9154057502746582, 'test/num_examples': 10000, 'score': 61398.24069237709, 'total_duration': 66139.95198273659, 'accumulated_submission_time': 61398.24069237709, 'accumulated_eval_time': 4727.902453184128, 'accumulated_logging_time': 6.423879384994507, 'global_step': 133946, 'preemption_count': 0}), (134862, {'train/accuracy': 0.7629492282867432, 'train/loss': 1.0372766256332397, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.304007887840271, 'validation/num_examples': 50000, 'test/accuracy': 0.5812000036239624, 'test/loss': 1.9075292348861694, 'test/num_examples': 10000, 'score': 61818.17729949951, 'total_duration': 66593.90523028374, 'accumulated_submission_time': 61818.17729949951, 'accumulated_eval_time': 4761.82052397728, 'accumulated_logging_time': 6.472502708435059, 'global_step': 134862, 'preemption_count': 0}), (135777, {'train/accuracy': 0.7660546898841858, 'train/loss': 1.0418373346328735, 'validation/accuracy': 0.7041400074958801, 'validation/loss': 1.3056179285049438, 'validation/num_examples': 50000, 'test/accuracy': 0.579800009727478, 'test/loss': 1.9201245307922363, 'test/num_examples': 10000, 'score': 62238.25712633133, 'total_duration': 67046.49923276901, 'accumulated_submission_time': 62238.25712633133, 'accumulated_eval_time': 4794.236758947372, 'accumulated_logging_time': 6.520557165145874, 'global_step': 135777, 'preemption_count': 0}), (136693, {'train/accuracy': 0.7722460627555847, 'train/loss': 1.0183627605438232, 'validation/accuracy': 0.7065199613571167, 'validation/loss': 1.3071863651275635, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9220960140228271, 'test/num_examples': 10000, 'score': 62658.68343901634, 'total_duration': 67499.63382172585, 'accumulated_submission_time': 62658.68343901634, 'accumulated_eval_time': 4826.84645652771, 'accumulated_logging_time': 6.569271087646484, 'global_step': 136693, 'preemption_count': 0}), (137608, {'train/accuracy': 0.7710937261581421, 'train/loss': 1.0141106843948364, 'validation/accuracy': 0.7095400094985962, 'validation/loss': 1.2850812673568726, 'validation/num_examples': 50000, 'test/accuracy': 0.5869000554084778, 'test/loss': 1.897950291633606, 'test/num_examples': 10000, 'score': 63078.72762656212, 'total_duration': 67953.34159827232, 'accumulated_submission_time': 63078.72762656212, 'accumulated_eval_time': 4860.415402173996, 'accumulated_logging_time': 6.61378026008606, 'global_step': 137608, 'preemption_count': 0}), (138523, {'train/accuracy': 0.7734375, 'train/loss': 1.0056006908416748, 'validation/accuracy': 0.7101199626922607, 'validation/loss': 1.2799828052520752, 'validation/num_examples': 50000, 'test/accuracy': 0.5896000266075134, 'test/loss': 1.8924295902252197, 'test/num_examples': 10000, 'score': 63498.872061014175, 'total_duration': 68406.37291073799, 'accumulated_submission_time': 63498.872061014175, 'accumulated_eval_time': 4893.19992518425, 'accumulated_logging_time': 6.666823387145996, 'global_step': 138523, 'preemption_count': 0}), (139438, {'train/accuracy': 0.7773827910423279, 'train/loss': 0.9855165481567383, 'validation/accuracy': 0.7126399874687195, 'validation/loss': 1.2628765106201172, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.8855514526367188, 'test/num_examples': 10000, 'score': 63919.03820538521, 'total_duration': 68859.4631357193, 'accumulated_submission_time': 63919.03820538521, 'accumulated_eval_time': 4926.0291867256165, 'accumulated_logging_time': 6.712033033370972, 'global_step': 139438, 'preemption_count': 0}), (140354, {'train/accuracy': 0.78968745470047, 'train/loss': 0.938185453414917, 'validation/accuracy': 0.7109599709510803, 'validation/loss': 1.2699692249298096, 'validation/num_examples': 50000, 'test/accuracy': 0.5871000289916992, 'test/loss': 1.891028642654419, 'test/num_examples': 10000, 'score': 64339.187994003296, 'total_duration': 69311.2506814003, 'accumulated_submission_time': 64339.187994003296, 'accumulated_eval_time': 4957.572335958481, 'accumulated_logging_time': 6.756109237670898, 'global_step': 140354, 'preemption_count': 0}), (141268, {'train/accuracy': 0.775585949420929, 'train/loss': 0.9803733825683594, 'validation/accuracy': 0.7143399715423584, 'validation/loss': 1.2429838180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.5908000469207764, 'test/loss': 1.8562695980072021, 'test/num_examples': 10000, 'score': 64759.182983875275, 'total_duration': 69764.55665397644, 'accumulated_submission_time': 64759.182983875275, 'accumulated_eval_time': 4990.778796195984, 'accumulated_logging_time': 6.810421466827393, 'global_step': 141268, 'preemption_count': 0}), (142185, {'train/accuracy': 0.7842773199081421, 'train/loss': 0.9769614338874817, 'validation/accuracy': 0.7149400115013123, 'validation/loss': 1.2617579698562622, 'validation/num_examples': 50000, 'test/accuracy': 0.596500039100647, 'test/loss': 1.858317494392395, 'test/num_examples': 10000, 'score': 65179.461914777756, 'total_duration': 70216.29029083252, 'accumulated_submission_time': 65179.461914777756, 'accumulated_eval_time': 5022.137254714966, 'accumulated_logging_time': 6.856017589569092, 'global_step': 142185, 'preemption_count': 0}), (143101, {'train/accuracy': 0.7922265529632568, 'train/loss': 0.911641538143158, 'validation/accuracy': 0.7174999713897705, 'validation/loss': 1.237351417541504, 'validation/num_examples': 50000, 'test/accuracy': 0.5972000360488892, 'test/loss': 1.844268798828125, 'test/num_examples': 10000, 'score': 65600.1053712368, 'total_duration': 70668.46542716026, 'accumulated_submission_time': 65600.1053712368, 'accumulated_eval_time': 5053.564244508743, 'accumulated_logging_time': 6.910325288772583, 'global_step': 143101, 'preemption_count': 0}), (144016, {'train/accuracy': 0.7851757407188416, 'train/loss': 0.9561517238616943, 'validation/accuracy': 0.7189399600028992, 'validation/loss': 1.2434120178222656, 'validation/num_examples': 50000, 'test/accuracy': 0.6003000140190125, 'test/loss': 1.8443188667297363, 'test/num_examples': 10000, 'score': 66020.01851248741, 'total_duration': 71121.61238765717, 'accumulated_submission_time': 66020.01851248741, 'accumulated_eval_time': 5086.700350761414, 'accumulated_logging_time': 6.957584381103516, 'global_step': 144016, 'preemption_count': 0}), (144931, {'train/accuracy': 0.7807421684265137, 'train/loss': 0.9476374387741089, 'validation/accuracy': 0.7181999683380127, 'validation/loss': 1.2305222749710083, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.8433793783187866, 'test/num_examples': 10000, 'score': 66440.11183166504, 'total_duration': 71575.11425447464, 'accumulated_submission_time': 66440.11183166504, 'accumulated_eval_time': 5120.0104303359985, 'accumulated_logging_time': 7.005602598190308, 'global_step': 144931, 'preemption_count': 0}), (145846, {'train/accuracy': 0.7958202958106995, 'train/loss': 0.9202547073364258, 'validation/accuracy': 0.7206000089645386, 'validation/loss': 1.2377933263778687, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.8521920442581177, 'test/num_examples': 10000, 'score': 66860.22039985657, 'total_duration': 72026.32795882225, 'accumulated_submission_time': 66860.22039985657, 'accumulated_eval_time': 5151.019411563873, 'accumulated_logging_time': 7.051945686340332, 'global_step': 145846, 'preemption_count': 0}), (146759, {'train/accuracy': 0.7870507836341858, 'train/loss': 0.943701982498169, 'validation/accuracy': 0.7204200029373169, 'validation/loss': 1.2290470600128174, 'validation/num_examples': 50000, 'test/accuracy': 0.6012000441551208, 'test/loss': 1.8332223892211914, 'test/num_examples': 10000, 'score': 67280.31985378265, 'total_duration': 72479.94853115082, 'accumulated_submission_time': 67280.31985378265, 'accumulated_eval_time': 5184.43443775177, 'accumulated_logging_time': 7.107837200164795, 'global_step': 146759, 'preemption_count': 0}), (147673, {'train/accuracy': 0.79359370470047, 'train/loss': 0.9170867204666138, 'validation/accuracy': 0.7231999635696411, 'validation/loss': 1.2131633758544922, 'validation/num_examples': 50000, 'test/accuracy': 0.5987000465393066, 'test/loss': 1.8122233152389526, 'test/num_examples': 10000, 'score': 67700.53570318222, 'total_duration': 72934.26190567017, 'accumulated_submission_time': 67700.53570318222, 'accumulated_eval_time': 5218.43242764473, 'accumulated_logging_time': 7.157444715499878, 'global_step': 147673, 'preemption_count': 0}), (148592, {'train/accuracy': 0.7974804639816284, 'train/loss': 0.8924198150634766, 'validation/accuracy': 0.7242599725723267, 'validation/loss': 1.2011919021606445, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.801270842552185, 'test/num_examples': 10000, 'score': 68120.48303842545, 'total_duration': 73388.2038257122, 'accumulated_submission_time': 68120.48303842545, 'accumulated_eval_time': 5252.327347993851, 'accumulated_logging_time': 7.205964088439941, 'global_step': 148592, 'preemption_count': 0}), (149508, {'train/accuracy': 0.7968554496765137, 'train/loss': 0.8914695382118225, 'validation/accuracy': 0.7272399663925171, 'validation/loss': 1.1844379901885986, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.780668020248413, 'test/num_examples': 10000, 'score': 68540.57233738899, 'total_duration': 73842.01174998283, 'accumulated_submission_time': 68540.57233738899, 'accumulated_eval_time': 5285.950101613998, 'accumulated_logging_time': 7.251769781112671, 'global_step': 149508, 'preemption_count': 0}), (150424, {'train/accuracy': 0.7983788847923279, 'train/loss': 0.8947476744651794, 'validation/accuracy': 0.7258999943733215, 'validation/loss': 1.197722315788269, 'validation/num_examples': 50000, 'test/accuracy': 0.6101000308990479, 'test/loss': 1.7993606328964233, 'test/num_examples': 10000, 'score': 68960.80321502686, 'total_duration': 74293.92950153351, 'accumulated_submission_time': 68960.80321502686, 'accumulated_eval_time': 5317.539261579514, 'accumulated_logging_time': 7.299367904663086, 'global_step': 150424, 'preemption_count': 0}), (151339, {'train/accuracy': 0.8019726276397705, 'train/loss': 0.8627512454986572, 'validation/accuracy': 0.7283200025558472, 'validation/loss': 1.176479458808899, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.7731417417526245, 'test/num_examples': 10000, 'score': 69380.76213383675, 'total_duration': 74745.69967579842, 'accumulated_submission_time': 69380.76213383675, 'accumulated_eval_time': 5349.239114522934, 'accumulated_logging_time': 7.360641241073608, 'global_step': 151339, 'preemption_count': 0}), (152253, {'train/accuracy': 0.8101562261581421, 'train/loss': 0.8360294103622437, 'validation/accuracy': 0.7314800024032593, 'validation/loss': 1.171595811843872, 'validation/num_examples': 50000, 'test/accuracy': 0.6112000346183777, 'test/loss': 1.764483094215393, 'test/num_examples': 10000, 'score': 69800.7703230381, 'total_duration': 75199.2972612381, 'accumulated_submission_time': 69800.7703230381, 'accumulated_eval_time': 5382.729545354843, 'accumulated_logging_time': 7.409675359725952, 'global_step': 152253, 'preemption_count': 0}), (153168, {'train/accuracy': 0.8065429329872131, 'train/loss': 0.850128710269928, 'validation/accuracy': 0.7335399985313416, 'validation/loss': 1.1555325984954834, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.7503547668457031, 'test/num_examples': 10000, 'score': 70220.83098077774, 'total_duration': 75651.4686627388, 'accumulated_submission_time': 70220.83098077774, 'accumulated_eval_time': 5414.739239692688, 'accumulated_logging_time': 7.460782766342163, 'global_step': 153168, 'preemption_count': 0}), (154083, {'train/accuracy': 0.8090429306030273, 'train/loss': 0.8482322692871094, 'validation/accuracy': 0.7348799705505371, 'validation/loss': 1.1653943061828613, 'validation/num_examples': 50000, 'test/accuracy': 0.6159000396728516, 'test/loss': 1.7686083316802979, 'test/num_examples': 10000, 'score': 70640.93357086182, 'total_duration': 76105.66011571884, 'accumulated_submission_time': 70640.93357086182, 'accumulated_eval_time': 5448.728743553162, 'accumulated_logging_time': 7.51036524772644, 'global_step': 154083, 'preemption_count': 0}), (154996, {'train/accuracy': 0.8161718845367432, 'train/loss': 0.8130109310150146, 'validation/accuracy': 0.7342199683189392, 'validation/loss': 1.1588497161865234, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.7606123685836792, 'test/num_examples': 10000, 'score': 71061.22500658035, 'total_duration': 76558.61492061615, 'accumulated_submission_time': 71061.22500658035, 'accumulated_eval_time': 5481.284249305725, 'accumulated_logging_time': 7.567243576049805, 'global_step': 154996, 'preemption_count': 0}), (155912, {'train/accuracy': 0.8117773532867432, 'train/loss': 0.8497369289398193, 'validation/accuracy': 0.7390799522399902, 'validation/loss': 1.1490964889526367, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.7688266038894653, 'test/num_examples': 10000, 'score': 71481.48496103287, 'total_duration': 77011.67231607437, 'accumulated_submission_time': 71481.48496103287, 'accumulated_eval_time': 5513.983215093613, 'accumulated_logging_time': 7.616268157958984, 'global_step': 155912, 'preemption_count': 0}), (156825, {'train/accuracy': 0.8138867020606995, 'train/loss': 0.8235838413238525, 'validation/accuracy': 0.7387599945068359, 'validation/loss': 1.1445491313934326, 'validation/num_examples': 50000, 'test/accuracy': 0.617900013923645, 'test/loss': 1.7345569133758545, 'test/num_examples': 10000, 'score': 71901.40081095695, 'total_duration': 77463.90251994133, 'accumulated_submission_time': 71901.40081095695, 'accumulated_eval_time': 5546.199652433395, 'accumulated_logging_time': 7.663997650146484, 'global_step': 156825, 'preemption_count': 0}), (157740, {'train/accuracy': 0.8207616806030273, 'train/loss': 0.800974428653717, 'validation/accuracy': 0.7397199869155884, 'validation/loss': 1.1447432041168213, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.7404789924621582, 'test/num_examples': 10000, 'score': 72321.37640357018, 'total_duration': 77917.57216620445, 'accumulated_submission_time': 72321.37640357018, 'accumulated_eval_time': 5579.793093681335, 'accumulated_logging_time': 7.714296340942383, 'global_step': 157740, 'preemption_count': 0}), (158654, {'train/accuracy': 0.81507807970047, 'train/loss': 0.812286913394928, 'validation/accuracy': 0.7413399815559387, 'validation/loss': 1.1248809099197388, 'validation/num_examples': 50000, 'test/accuracy': 0.625700056552887, 'test/loss': 1.7191606760025024, 'test/num_examples': 10000, 'score': 72741.29397368431, 'total_duration': 78371.34893417358, 'accumulated_submission_time': 72741.29397368431, 'accumulated_eval_time': 5613.552079439163, 'accumulated_logging_time': 7.765320539474487, 'global_step': 158654, 'preemption_count': 0}), (159569, {'train/accuracy': 0.8191210627555847, 'train/loss': 0.8072444200515747, 'validation/accuracy': 0.7432799935340881, 'validation/loss': 1.1267979145050049, 'validation/num_examples': 50000, 'test/accuracy': 0.6260000467300415, 'test/loss': 1.720393419265747, 'test/num_examples': 10000, 'score': 73161.59970808029, 'total_duration': 78823.85418653488, 'accumulated_submission_time': 73161.59970808029, 'accumulated_eval_time': 5645.6390788555145, 'accumulated_logging_time': 7.827660083770752, 'global_step': 159569, 'preemption_count': 0}), (160442, {'train/accuracy': 0.8229101300239563, 'train/loss': 0.7834881544113159, 'validation/accuracy': 0.744659960269928, 'validation/loss': 1.1210800409317017, 'validation/num_examples': 50000, 'test/accuracy': 0.6271000504493713, 'test/loss': 1.7195051908493042, 'test/num_examples': 10000, 'score': 73581.53504562378, 'total_duration': 79278.61271739006, 'accumulated_submission_time': 73581.53504562378, 'accumulated_eval_time': 5680.3631727695465, 'accumulated_logging_time': 7.879194021224976, 'global_step': 160442, 'preemption_count': 0}), (161358, {'train/accuracy': 0.82289057970047, 'train/loss': 0.7732518315315247, 'validation/accuracy': 0.7462199926376343, 'validation/loss': 1.0985074043273926, 'validation/num_examples': 50000, 'test/accuracy': 0.6326000094413757, 'test/loss': 1.683404803276062, 'test/num_examples': 10000, 'score': 74001.81647443771, 'total_duration': 79730.39583301544, 'accumulated_submission_time': 74001.81647443771, 'accumulated_eval_time': 5711.764150619507, 'accumulated_logging_time': 7.929988384246826, 'global_step': 161358, 'preemption_count': 0}), (162271, {'train/accuracy': 0.8252733945846558, 'train/loss': 0.7839264273643494, 'validation/accuracy': 0.7461000084877014, 'validation/loss': 1.112027645111084, 'validation/num_examples': 50000, 'test/accuracy': 0.632900059223175, 'test/loss': 1.7059128284454346, 'test/num_examples': 10000, 'score': 74421.98902630806, 'total_duration': 80182.82422280312, 'accumulated_submission_time': 74421.98902630806, 'accumulated_eval_time': 5743.91107749939, 'accumulated_logging_time': 7.989051818847656, 'global_step': 162271, 'preemption_count': 0}), (163186, {'train/accuracy': 0.8279882669448853, 'train/loss': 0.7568796277046204, 'validation/accuracy': 0.7485599517822266, 'validation/loss': 1.0927319526672363, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.6792042255401611, 'test/num_examples': 10000, 'score': 74842.11705088615, 'total_duration': 80636.16126894951, 'accumulated_submission_time': 74842.11705088615, 'accumulated_eval_time': 5777.016916275024, 'accumulated_logging_time': 8.03922438621521, 'global_step': 163186, 'preemption_count': 0}), (164101, {'train/accuracy': 0.8231250047683716, 'train/loss': 0.784327507019043, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 1.100296974182129, 'validation/num_examples': 50000, 'test/accuracy': 0.6350000500679016, 'test/loss': 1.6938539743423462, 'test/num_examples': 10000, 'score': 75262.68575310707, 'total_duration': 81089.08715343475, 'accumulated_submission_time': 75262.68575310707, 'accumulated_eval_time': 5809.2748148441315, 'accumulated_logging_time': 8.088002920150757, 'global_step': 164101, 'preemption_count': 0}), (165017, {'train/accuracy': 0.8282421827316284, 'train/loss': 0.7752244472503662, 'validation/accuracy': 0.7502599954605103, 'validation/loss': 1.0974425077438354, 'validation/num_examples': 50000, 'test/accuracy': 0.6292000412940979, 'test/loss': 1.6791881322860718, 'test/num_examples': 10000, 'score': 75682.98915481567, 'total_duration': 81542.0298383236, 'accumulated_submission_time': 75682.98915481567, 'accumulated_eval_time': 5841.812193155289, 'accumulated_logging_time': 8.13952898979187, 'global_step': 165017, 'preemption_count': 0}), (165934, {'train/accuracy': 0.8337695002555847, 'train/loss': 0.7408263087272644, 'validation/accuracy': 0.7530800104141235, 'validation/loss': 1.0839965343475342, 'validation/num_examples': 50000, 'test/accuracy': 0.6335000395774841, 'test/loss': 1.672433853149414, 'test/num_examples': 10000, 'score': 76103.24325037003, 'total_duration': 81996.13554024696, 'accumulated_submission_time': 76103.24325037003, 'accumulated_eval_time': 5875.553389310837, 'accumulated_logging_time': 8.199171781539917, 'global_step': 165934, 'preemption_count': 0}), (166850, {'train/accuracy': 0.8430468440055847, 'train/loss': 0.7069917917251587, 'validation/accuracy': 0.7552599906921387, 'validation/loss': 1.0686745643615723, 'validation/num_examples': 50000, 'test/accuracy': 0.6446000337600708, 'test/loss': 1.6342931985855103, 'test/num_examples': 10000, 'score': 76523.2434270382, 'total_duration': 82450.94435310364, 'accumulated_submission_time': 76523.2434270382, 'accumulated_eval_time': 5910.257388830185, 'accumulated_logging_time': 8.253775358200073, 'global_step': 166850, 'preemption_count': 0}), (167765, {'train/accuracy': 0.8338086009025574, 'train/loss': 0.7452972531318665, 'validation/accuracy': 0.7548399567604065, 'validation/loss': 1.074892282485962, 'validation/num_examples': 50000, 'test/accuracy': 0.6406000256538391, 'test/loss': 1.656589150428772, 'test/num_examples': 10000, 'score': 76943.4230298996, 'total_duration': 82904.4444053173, 'accumulated_submission_time': 76943.4230298996, 'accumulated_eval_time': 5943.479510307312, 'accumulated_logging_time': 8.302427291870117, 'global_step': 167765, 'preemption_count': 0}), (168679, {'train/accuracy': 0.83363276720047, 'train/loss': 0.7375044822692871, 'validation/accuracy': 0.7561999559402466, 'validation/loss': 1.0720763206481934, 'validation/num_examples': 50000, 'test/accuracy': 0.6388000249862671, 'test/loss': 1.652498722076416, 'test/num_examples': 10000, 'score': 77363.56269574165, 'total_duration': 83358.4293923378, 'accumulated_submission_time': 77363.56269574165, 'accumulated_eval_time': 5977.2169399261475, 'accumulated_logging_time': 8.360501766204834, 'global_step': 168679, 'preemption_count': 0})], 'global_step': 169029}
I0131 19:09:15.114240 140085747812160 submission_runner.py:586] Timing: 77520.33018016815
I0131 19:09:15.114312 140085747812160 submission_runner.py:588] Total number of evals: 185
I0131 19:09:15.114356 140085747812160 submission_runner.py:589] ====================
I0131 19:09:15.114401 140085747812160 submission_runner.py:542] Using RNG seed 1800903789
I0131 19:09:15.115905 140085747812160 submission_runner.py:551] --- Tuning run 2/5 ---
I0131 19:09:15.116006 140085747812160 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2.
I0131 19:09:15.120216 140085747812160 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2/hparams.json.
I0131 19:09:15.121865 140085747812160 submission_runner.py:206] Initializing dataset.
I0131 19:09:15.132918 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0131 19:09:15.143666 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0131 19:09:15.337200 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0131 19:09:19.884519 140085747812160 submission_runner.py:213] Initializing model.
I0131 19:09:26.139986 140085747812160 submission_runner.py:255] Initializing optimizer.
I0131 19:09:26.633060 140085747812160 submission_runner.py:262] Initializing metrics bundle.
I0131 19:09:26.633213 140085747812160 submission_runner.py:280] Initializing checkpoint and logger.
I0131 19:09:26.722744 140085747812160 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2 with prefix checkpoint_
I0131 19:09:26.722876 140085747812160 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0131 19:09:42.881652 140085747812160 logger_utils.py:220] Unable to record git information. Continuing without it.
I0131 19:09:58.761509 140085747812160 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2/flags_0.json.
I0131 19:09:58.773383 140085747812160 submission_runner.py:314] Starting training loop.
I0131 19:10:32.401321 139923826849536 logging_writer.py:48] [0] global_step=0, grad_norm=0.2937803566455841, loss=6.907753944396973
I0131 19:10:32.412199 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:10:40.740796 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:10:58.998044 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:11:00.659571 140085747812160 submission_runner.py:408] Time since start: 61.89s, 	Step: 1, 	{'train/accuracy': 0.0008007812430150807, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 33.638116121292114, 'total_duration': 61.886141538619995, 'accumulated_submission_time': 33.638116121292114, 'accumulated_eval_time': 28.24734091758728, 'accumulated_logging_time': 0}
I0131 19:11:00.668051 139923835242240 logging_writer.py:48] [1] accumulated_eval_time=28.247341, accumulated_logging_time=0, accumulated_submission_time=33.638116, global_step=1, preemption_count=0, score=33.638116, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=61.886142, train/accuracy=0.000801, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0131 19:12:06.001974 139923868813056 logging_writer.py:48] [100] global_step=100, grad_norm=0.32093313336372375, loss=6.9054646492004395
I0131 19:12:52.514282 139923852027648 logging_writer.py:48] [200] global_step=200, grad_norm=0.43192222714424133, loss=6.882070541381836
I0131 19:13:39.020681 139923868813056 logging_writer.py:48] [300] global_step=300, grad_norm=0.4908100664615631, loss=6.842380523681641
I0131 19:14:25.948636 139923852027648 logging_writer.py:48] [400] global_step=400, grad_norm=0.5400204062461853, loss=6.806601524353027
I0131 19:15:12.831408 139923868813056 logging_writer.py:48] [500] global_step=500, grad_norm=0.5087023973464966, loss=6.787075042724609
I0131 19:15:59.711649 139923852027648 logging_writer.py:48] [600] global_step=600, grad_norm=0.633621871471405, loss=6.735358715057373
I0131 19:16:46.301902 139923868813056 logging_writer.py:48] [700] global_step=700, grad_norm=0.6481859087944031, loss=6.815908432006836
I0131 19:17:32.993807 139923852027648 logging_writer.py:48] [800] global_step=800, grad_norm=0.8692016005516052, loss=6.6820573806762695
I0131 19:18:00.763541 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:18:11.907176 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:18:34.312165 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:18:35.950286 140085747812160 submission_runner.py:408] Time since start: 517.18s, 	Step: 861, 	{'train/accuracy': 0.01630859263241291, 'train/loss': 6.390534400939941, 'validation/accuracy': 0.016039999201893806, 'validation/loss': 6.40871000289917, 'validation/num_examples': 50000, 'test/accuracy': 0.01380000077188015, 'test/loss': 6.452201843261719, 'test/num_examples': 10000, 'score': 453.676509141922, 'total_duration': 517.1768567562103, 'accumulated_submission_time': 453.676509141922, 'accumulated_eval_time': 63.434080839157104, 'accumulated_logging_time': 0.017125606536865234}
I0131 19:18:35.970998 139923868813056 logging_writer.py:48] [861] accumulated_eval_time=63.434081, accumulated_logging_time=0.017126, accumulated_submission_time=453.676509, global_step=861, preemption_count=0, score=453.676509, test/accuracy=0.013800, test/loss=6.452202, test/num_examples=10000, total_duration=517.176857, train/accuracy=0.016309, train/loss=6.390534, validation/accuracy=0.016040, validation/loss=6.408710, validation/num_examples=50000
I0131 19:18:51.641776 139923852027648 logging_writer.py:48] [900] global_step=900, grad_norm=1.3987122774124146, loss=6.598748207092285
I0131 19:19:35.657463 139923868813056 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3596608638763428, loss=6.599713325500488
I0131 19:20:22.432107 139923852027648 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.327256441116333, loss=6.52070426940918
I0131 19:21:09.253049 139923868813056 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0169678926467896, loss=6.4813761711120605
I0131 19:21:56.060870 139923852027648 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.1300431489944458, loss=6.505059719085693
I0131 19:22:43.337650 139923868813056 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.0779510736465454, loss=6.466197490692139
I0131 19:23:29.881413 139923852027648 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.3657479286193848, loss=6.4889140129089355
I0131 19:24:16.763672 139923868813056 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.139743447303772, loss=6.359528064727783
I0131 19:25:03.466529 139923852027648 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.8195744156837463, loss=6.62559700012207
I0131 19:25:36.246908 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:25:47.022387 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:26:09.136239 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:26:10.775017 140085747812160 submission_runner.py:408] Time since start: 972.00s, 	Step: 1772, 	{'train/accuracy': 0.04798828065395355, 'train/loss': 5.8042778968811035, 'validation/accuracy': 0.04601999744772911, 'validation/loss': 5.834495544433594, 'validation/num_examples': 50000, 'test/accuracy': 0.03670000284910202, 'test/loss': 5.948112487792969, 'test/num_examples': 10000, 'score': 873.8910715579987, 'total_duration': 972.0015864372253, 'accumulated_submission_time': 873.8910715579987, 'accumulated_eval_time': 97.96224021911621, 'accumulated_logging_time': 0.04842209815979004}
I0131 19:26:10.792058 139923868813056 logging_writer.py:48] [1772] accumulated_eval_time=97.962240, accumulated_logging_time=0.048422, accumulated_submission_time=873.891072, global_step=1772, preemption_count=0, score=873.891072, test/accuracy=0.036700, test/loss=5.948112, test/num_examples=10000, total_duration=972.001586, train/accuracy=0.047988, train/loss=5.804278, validation/accuracy=0.046020, validation/loss=5.834496, validation/num_examples=50000
I0131 19:26:22.159891 139923852027648 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.0069795846939087, loss=6.569231986999512
I0131 19:27:05.588004 139923868813056 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.9823947548866272, loss=6.725350379943848
I0131 19:27:52.590530 139923852027648 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.0651391744613647, loss=6.222043514251709
I0131 19:28:39.297225 139923868813056 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.2516337633132935, loss=6.252001762390137
I0131 19:29:26.096115 139923852027648 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0460928678512573, loss=6.228236675262451
I0131 19:30:12.538389 139923868813056 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3127250671386719, loss=6.203062534332275
I0131 19:30:59.201533 139923852027648 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.1925632953643799, loss=6.197404861450195
I0131 19:31:45.894073 139923868813056 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2375587224960327, loss=6.1788649559021
I0131 19:32:32.540580 139923852027648 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.9493906497955322, loss=6.131653308868408
I0131 19:33:11.114223 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:33:21.505356 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:33:46.389511 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:33:48.029994 140085747812160 submission_runner.py:408] Time since start: 1429.26s, 	Step: 2684, 	{'train/accuracy': 0.07507812231779099, 'train/loss': 5.424489498138428, 'validation/accuracy': 0.06735999882221222, 'validation/loss': 5.472591876983643, 'validation/num_examples': 50000, 'test/accuracy': 0.051600001752376556, 'test/loss': 5.641368389129639, 'test/num_examples': 10000, 'score': 1294.1512134075165, 'total_duration': 1429.2565631866455, 'accumulated_submission_time': 1294.1512134075165, 'accumulated_eval_time': 134.87805843353271, 'accumulated_logging_time': 0.07643413543701172}
I0131 19:33:48.047703 139923868813056 logging_writer.py:48] [2684] accumulated_eval_time=134.878058, accumulated_logging_time=0.076434, accumulated_submission_time=1294.151213, global_step=2684, preemption_count=0, score=1294.151213, test/accuracy=0.051600, test/loss=5.641368, test/num_examples=10000, total_duration=1429.256563, train/accuracy=0.075078, train/loss=5.424489, validation/accuracy=0.067360, validation/loss=5.472592, validation/num_examples=50000
I0131 19:33:54.724345 139923852027648 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.995685875415802, loss=6.568632125854492
I0131 19:34:36.950595 139923868813056 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.3829551935195923, loss=6.113896369934082
I0131 19:35:23.727728 139923852027648 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.2814574241638184, loss=6.084150314331055
I0131 19:36:10.568759 139923868813056 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9298248887062073, loss=6.143847465515137
I0131 19:36:56.982772 139923852027648 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9341716170310974, loss=6.045720100402832
I0131 19:37:43.364873 139923868813056 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.2432295083999634, loss=6.597198009490967
I0131 19:38:29.820900 139923852027648 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.058996319770813, loss=5.973631858825684
I0131 19:39:16.626067 139923868813056 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.8107166290283203, loss=6.041940689086914
I0131 19:40:03.322881 139923852027648 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.387426495552063, loss=6.208271026611328
I0131 19:40:48.373009 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:40:58.872866 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:41:20.230019 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:41:21.864287 140085747812160 submission_runner.py:408] Time since start: 1883.09s, 	Step: 3598, 	{'train/accuracy': 0.10935546457767487, 'train/loss': 5.114729881286621, 'validation/accuracy': 0.10179999470710754, 'validation/loss': 5.160167217254639, 'validation/num_examples': 50000, 'test/accuracy': 0.07700000703334808, 'test/loss': 5.386642932891846, 'test/num_examples': 10000, 'score': 1714.4163200855255, 'total_duration': 1883.0908553600311, 'accumulated_submission_time': 1714.4163200855255, 'accumulated_eval_time': 168.3693425655365, 'accumulated_logging_time': 0.10368227958679199}
I0131 19:41:21.884403 139923868813056 logging_writer.py:48] [3598] accumulated_eval_time=168.369343, accumulated_logging_time=0.103682, accumulated_submission_time=1714.416320, global_step=3598, preemption_count=0, score=1714.416320, test/accuracy=0.077000, test/loss=5.386643, test/num_examples=10000, total_duration=1883.090855, train/accuracy=0.109355, train/loss=5.114730, validation/accuracy=0.101800, validation/loss=5.160167, validation/num_examples=50000
I0131 19:41:23.064974 139923852027648 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.030896544456482, loss=6.502150535583496
I0131 19:42:04.437060 139923868813056 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.2052927017211914, loss=5.88953971862793
I0131 19:42:51.150424 139923852027648 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.6923450231552124, loss=5.952023506164551
I0131 19:43:37.811061 139923868813056 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.013046383857727, loss=5.936764717102051
I0131 19:44:24.598301 139923852027648 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9911085963249207, loss=5.8610076904296875
I0131 19:45:11.322533 139923868813056 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.0323452949523926, loss=5.818922996520996
I0131 19:45:57.876474 139923852027648 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.051315188407898, loss=5.774353981018066
I0131 19:46:44.725570 139923868813056 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.2098572254180908, loss=5.765161514282227
I0131 19:47:31.033122 139923852027648 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8186907768249512, loss=6.474823951721191
I0131 19:48:17.545971 139923868813056 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.184922456741333, loss=5.733221054077148
I0131 19:48:21.871596 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:48:32.507156 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:48:56.127804 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:48:57.775580 140085747812160 submission_runner.py:408] Time since start: 2339.00s, 	Step: 4511, 	{'train/accuracy': 0.16072265803813934, 'train/loss': 4.637515068054199, 'validation/accuracy': 0.1462000012397766, 'validation/loss': 4.709775924682617, 'validation/num_examples': 50000, 'test/accuracy': 0.11030000448226929, 'test/loss': 4.995343208312988, 'test/num_examples': 10000, 'score': 2134.3432302474976, 'total_duration': 2339.002152442932, 'accumulated_submission_time': 2134.3432302474976, 'accumulated_eval_time': 204.27332472801208, 'accumulated_logging_time': 0.13352179527282715}
I0131 19:48:57.791177 139923852027648 logging_writer.py:48] [4511] accumulated_eval_time=204.273325, accumulated_logging_time=0.133522, accumulated_submission_time=2134.343230, global_step=4511, preemption_count=0, score=2134.343230, test/accuracy=0.110300, test/loss=4.995343, test/num_examples=10000, total_duration=2339.002152, train/accuracy=0.160723, train/loss=4.637515, validation/accuracy=0.146200, validation/loss=4.709776, validation/num_examples=50000
I0131 19:49:34.488681 139923868813056 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.2857487201690674, loss=5.710391521453857
I0131 19:50:21.058252 139923852027648 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.2512940168380737, loss=5.701634407043457
I0131 19:51:07.798006 139923868813056 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.9432154893875122, loss=6.55458402633667
I0131 19:51:54.287884 139923852027648 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1558504104614258, loss=5.639343738555908
I0131 19:52:41.313100 139923868813056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.9934292435646057, loss=5.748692512512207
I0131 19:53:27.669681 139923852027648 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.5304261445999146, loss=5.701103210449219
I0131 19:54:14.561720 139923868813056 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.3217180967330933, loss=5.552513122558594
I0131 19:55:01.000851 139923852027648 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.862229585647583, loss=6.424939155578613
I0131 19:55:47.425439 139923868813056 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.1694786548614502, loss=6.584023475646973
I0131 19:55:57.848611 140085747812160 spec.py:321] Evaluating on the training split.
I0131 19:56:08.319409 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 19:56:31.235320 140085747812160 spec.py:349] Evaluating on the test split.
I0131 19:56:32.876264 140085747812160 submission_runner.py:408] Time since start: 2794.10s, 	Step: 5424, 	{'train/accuracy': 0.20499999821186066, 'train/loss': 4.282425880432129, 'validation/accuracy': 0.18549999594688416, 'validation/loss': 4.378244400024414, 'validation/num_examples': 50000, 'test/accuracy': 0.14100000262260437, 'test/loss': 4.718472480773926, 'test/num_examples': 10000, 'score': 2554.3411922454834, 'total_duration': 2794.102832555771, 'accumulated_submission_time': 2554.3411922454834, 'accumulated_eval_time': 239.3009958267212, 'accumulated_logging_time': 0.15807175636291504}
I0131 19:56:32.895804 139923852027648 logging_writer.py:48] [5424] accumulated_eval_time=239.300996, accumulated_logging_time=0.158072, accumulated_submission_time=2554.341192, global_step=5424, preemption_count=0, score=2554.341192, test/accuracy=0.141000, test/loss=4.718472, test/num_examples=10000, total_duration=2794.102833, train/accuracy=0.205000, train/loss=4.282426, validation/accuracy=0.185500, validation/loss=4.378244, validation/num_examples=50000
I0131 19:57:03.745288 139923868813056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9021114706993103, loss=5.820135593414307
I0131 19:57:50.021020 139923852027648 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.2970492839813232, loss=5.477948188781738
I0131 19:58:36.807200 139923868813056 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.9790322780609131, loss=5.731114864349365
I0131 19:59:23.231910 139923852027648 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.8329662084579468, loss=5.938535213470459
I0131 20:00:09.960214 139923868813056 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9916728138923645, loss=5.491119861602783
I0131 20:00:56.379581 139923852027648 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.158085584640503, loss=5.753063201904297
I0131 20:01:43.184955 139923868813056 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.1538004875183105, loss=5.320934295654297
I0131 20:02:30.037525 139923852027648 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.1247161626815796, loss=5.803545951843262
I0131 20:03:16.804890 139923868813056 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0695916414260864, loss=5.315440654754639
I0131 20:03:33.099299 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:03:43.899451 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:04:06.601191 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:04:08.237872 140085747812160 submission_runner.py:408] Time since start: 3249.46s, 	Step: 6337, 	{'train/accuracy': 0.24888671934604645, 'train/loss': 3.9398257732391357, 'validation/accuracy': 0.23002000153064728, 'validation/loss': 4.036154747009277, 'validation/num_examples': 50000, 'test/accuracy': 0.17490001022815704, 'test/loss': 4.422224998474121, 'test/num_examples': 10000, 'score': 2974.4840099811554, 'total_duration': 3249.46443939209, 'accumulated_submission_time': 2974.4840099811554, 'accumulated_eval_time': 274.43955540657043, 'accumulated_logging_time': 0.18840432167053223}
I0131 20:04:08.257019 139923852027648 logging_writer.py:48] [6337] accumulated_eval_time=274.439555, accumulated_logging_time=0.188404, accumulated_submission_time=2974.484010, global_step=6337, preemption_count=0, score=2974.484010, test/accuracy=0.174900, test/loss=4.422225, test/num_examples=10000, total_duration=3249.464439, train/accuracy=0.248887, train/loss=3.939826, validation/accuracy=0.230020, validation/loss=4.036155, validation/num_examples=50000
I0131 20:04:33.366694 139923868813056 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.0529956817626953, loss=5.45836877822876
I0131 20:05:18.755633 139923852027648 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.253912329673767, loss=5.552886962890625
I0131 20:06:05.757979 139923868813056 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9256160259246826, loss=5.720831871032715
I0131 20:06:52.269972 139923852027648 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.7664813995361328, loss=6.4468183517456055
I0131 20:07:38.985818 139923868813056 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.992965042591095, loss=5.532188892364502
I0131 20:08:25.557086 139923852027648 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.2132259607315063, loss=5.2165093421936035
I0131 20:09:12.238122 139923868813056 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9429501891136169, loss=6.482198238372803
I0131 20:09:58.874485 139923852027648 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.1178780794143677, loss=6.45244026184082
I0131 20:10:45.371022 139923868813056 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9315861463546753, loss=6.342278003692627
I0131 20:11:08.412620 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:11:18.834966 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:11:40.471658 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:11:42.114313 140085747812160 submission_runner.py:408] Time since start: 3703.34s, 	Step: 7251, 	{'train/accuracy': 0.28951171040534973, 'train/loss': 3.659607172012329, 'validation/accuracy': 0.25793999433517456, 'validation/loss': 3.8160948753356934, 'validation/num_examples': 50000, 'test/accuracy': 0.20260000228881836, 'test/loss': 4.218814849853516, 'test/num_examples': 10000, 'score': 3394.577829360962, 'total_duration': 3703.3408839702606, 'accumulated_submission_time': 3394.577829360962, 'accumulated_eval_time': 308.14124870300293, 'accumulated_logging_time': 0.21875262260437012}
I0131 20:11:42.132002 139923852027648 logging_writer.py:48] [7251] accumulated_eval_time=308.141249, accumulated_logging_time=0.218753, accumulated_submission_time=3394.577829, global_step=7251, preemption_count=0, score=3394.577829, test/accuracy=0.202600, test/loss=4.218815, test/num_examples=10000, total_duration=3703.340884, train/accuracy=0.289512, train/loss=3.659607, validation/accuracy=0.257940, validation/loss=3.816095, validation/num_examples=50000
I0131 20:12:01.781296 139923868813056 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.7776291370391846, loss=6.315944194793701
I0131 20:12:46.920866 139923852027648 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.2180384397506714, loss=5.108914375305176
I0131 20:13:33.886712 139923868813056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.889784574508667, loss=5.650798320770264
I0131 20:14:20.373544 139923852027648 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.9568221569061279, loss=5.10003662109375
I0131 20:15:06.769925 139923868813056 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.0242582559585571, loss=5.116305351257324
I0131 20:15:53.392124 139923852027648 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.2472110986709595, loss=5.1612467765808105
I0131 20:16:40.122293 139923868813056 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.8376461267471313, loss=5.781950950622559
I0131 20:17:26.821214 139923852027648 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.428916096687317, loss=5.042536735534668
I0131 20:18:13.605317 139923868813056 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0636062622070312, loss=5.030937671661377
I0131 20:18:42.165865 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:18:52.862037 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:19:16.508131 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:19:18.150797 140085747812160 submission_runner.py:408] Time since start: 4159.38s, 	Step: 8163, 	{'train/accuracy': 0.3244921863079071, 'train/loss': 3.4557206630706787, 'validation/accuracy': 0.30140000581741333, 'validation/loss': 3.572089910507202, 'validation/num_examples': 50000, 'test/accuracy': 0.2290000170469284, 'test/loss': 4.028704643249512, 'test/num_examples': 10000, 'score': 3814.550952911377, 'total_duration': 4159.377365589142, 'accumulated_submission_time': 3814.550952911377, 'accumulated_eval_time': 344.1261923313141, 'accumulated_logging_time': 0.24541378021240234}
I0131 20:19:18.168853 139923852027648 logging_writer.py:48] [8163] accumulated_eval_time=344.126192, accumulated_logging_time=0.245414, accumulated_submission_time=3814.550953, global_step=8163, preemption_count=0, score=3814.550953, test/accuracy=0.229000, test/loss=4.028705, test/num_examples=10000, total_duration=4159.377366, train/accuracy=0.324492, train/loss=3.455721, validation/accuracy=0.301400, validation/loss=3.572090, validation/num_examples=50000
I0131 20:19:33.101455 139923868813056 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.6923195719718933, loss=6.301678657531738
I0131 20:20:16.948130 139923852027648 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.9680847525596619, loss=5.061337471008301
I0131 20:21:03.747287 139923868813056 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.196899175643921, loss=4.937653541564941
I0131 20:21:50.388048 139923852027648 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.400599718093872, loss=4.993380546569824
I0131 20:22:37.496619 139923868813056 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.0790908336639404, loss=5.091286659240723
I0131 20:23:24.130985 139923852027648 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8800076842308044, loss=5.3406572341918945
I0131 20:24:11.080449 139923868813056 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0271210670471191, loss=4.905641555786133
I0131 20:24:57.690256 139923852027648 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9069840908050537, loss=5.858943939208984
I0131 20:25:44.505701 139923868813056 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.095106601715088, loss=4.896968364715576
I0131 20:26:18.160545 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:26:28.925200 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:26:50.372337 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:26:52.007495 140085747812160 submission_runner.py:408] Time since start: 4613.23s, 	Step: 9074, 	{'train/accuracy': 0.35783201456069946, 'train/loss': 3.234337329864502, 'validation/accuracy': 0.32916000485420227, 'validation/loss': 3.3736839294433594, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 3.8573837280273438, 'test/num_examples': 10000, 'score': 4234.478483200073, 'total_duration': 4613.234064817429, 'accumulated_submission_time': 4234.478483200073, 'accumulated_eval_time': 377.9731593132019, 'accumulated_logging_time': 0.2774481773376465}
I0131 20:26:52.027867 139923852027648 logging_writer.py:48] [9074] accumulated_eval_time=377.973159, accumulated_logging_time=0.277448, accumulated_submission_time=4234.478483, global_step=9074, preemption_count=0, score=4234.478483, test/accuracy=0.249900, test/loss=3.857384, test/num_examples=10000, total_duration=4613.234065, train/accuracy=0.357832, train/loss=3.234337, validation/accuracy=0.329160, validation/loss=3.373684, validation/num_examples=50000
I0131 20:27:02.630367 139923868813056 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.6813340187072754, loss=6.292608737945557
I0131 20:27:45.707080 139923852027648 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7076879739761353, loss=6.2649383544921875
I0131 20:28:32.681850 139923868813056 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6810168027877808, loss=6.061827659606934
I0131 20:29:19.175012 139923852027648 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8330122828483582, loss=6.315250396728516
I0131 20:30:05.971215 139923868813056 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7226740121841431, loss=6.292728424072266
I0131 20:30:52.576859 139923852027648 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.9734194874763489, loss=4.9723591804504395
I0131 20:31:39.367796 139923868813056 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.31590735912323, loss=4.926929950714111
I0131 20:32:25.942787 139923852027648 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.040900468826294, loss=4.828653335571289
I0131 20:33:12.954251 139923868813056 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9504470825195312, loss=4.776092529296875
I0131 20:33:52.357424 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:34:02.758357 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:34:23.951669 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:34:25.592589 140085747812160 submission_runner.py:408] Time since start: 5066.82s, 	Step: 9986, 	{'train/accuracy': 0.3981249928474426, 'train/loss': 2.985623359680176, 'validation/accuracy': 0.3610599935054779, 'validation/loss': 3.1759495735168457, 'validation/num_examples': 50000, 'test/accuracy': 0.27790001034736633, 'test/loss': 3.6841766834259033, 'test/num_examples': 10000, 'score': 4654.747854232788, 'total_duration': 5066.819159269333, 'accumulated_submission_time': 4654.747854232788, 'accumulated_eval_time': 411.2083342075348, 'accumulated_logging_time': 0.30759716033935547}
I0131 20:34:25.612292 139923852027648 logging_writer.py:48] [9986] accumulated_eval_time=411.208334, accumulated_logging_time=0.307597, accumulated_submission_time=4654.747854, global_step=9986, preemption_count=0, score=4654.747854, test/accuracy=0.277900, test/loss=3.684177, test/num_examples=10000, total_duration=5066.819159, train/accuracy=0.398125, train/loss=2.985623, validation/accuracy=0.361060, validation/loss=3.175950, validation/num_examples=50000
I0131 20:34:31.504331 139923868813056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9623015522956848, loss=4.800956726074219
I0131 20:35:14.049651 139923852027648 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.863802969455719, loss=5.490021705627441
I0131 20:36:00.372674 139923868813056 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.7995179295539856, loss=5.77272891998291
I0131 20:36:47.205290 139923852027648 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.979743242263794, loss=4.709177017211914
I0131 20:37:33.859967 139923868813056 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9397064447402954, loss=4.933310508728027
I0131 20:38:20.692337 139923852027648 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.0012775659561157, loss=4.64830207824707
I0131 20:39:07.153093 139923868813056 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9537059664726257, loss=4.997157096862793
I0131 20:39:53.827750 139923852027648 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9006133079528809, loss=5.16381311416626
I0131 20:40:40.403889 139923868813056 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.849835216999054, loss=6.249496936798096
I0131 20:41:25.916984 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:41:36.895054 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:41:58.079372 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:41:59.729256 140085747812160 submission_runner.py:408] Time since start: 5520.96s, 	Step: 10899, 	{'train/accuracy': 0.42244139313697815, 'train/loss': 2.8559553623199463, 'validation/accuracy': 0.391759991645813, 'validation/loss': 2.990187644958496, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.5289344787597656, 'test/num_examples': 10000, 'score': 5074.991655111313, 'total_duration': 5520.955825805664, 'accumulated_submission_time': 5074.991655111313, 'accumulated_eval_time': 445.02062249183655, 'accumulated_logging_time': 0.33704257011413574}
I0131 20:41:59.748339 139923852027648 logging_writer.py:48] [10899] accumulated_eval_time=445.020622, accumulated_logging_time=0.337043, accumulated_submission_time=5074.991655, global_step=10899, preemption_count=0, score=5074.991655, test/accuracy=0.305600, test/loss=3.528934, test/num_examples=10000, total_duration=5520.955826, train/accuracy=0.422441, train/loss=2.855955, validation/accuracy=0.391760, validation/loss=2.990188, validation/num_examples=50000
I0131 20:42:00.540436 139923868813056 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9081750512123108, loss=4.721222877502441
I0131 20:42:42.284972 139923852027648 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8591126799583435, loss=4.9043660163879395
I0131 20:43:29.057967 139923868813056 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9895047545433044, loss=4.646023750305176
I0131 20:44:15.870871 139923852027648 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6480377912521362, loss=5.975602626800537
I0131 20:45:02.625370 139923868813056 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0499123334884644, loss=4.790731430053711
I0131 20:45:49.268371 139923852027648 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6655778884887695, loss=5.9381585121154785
I0131 20:46:35.613183 139923868813056 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.154200792312622, loss=4.545001029968262
I0131 20:47:22.508617 139923852027648 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8541804552078247, loss=4.530193328857422
I0131 20:48:08.999021 139923868813056 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9246803522109985, loss=4.605657577514648
I0131 20:48:55.505065 139923852027648 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9520747661590576, loss=4.649122714996338
I0131 20:48:59.817722 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:49:10.341471 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:49:32.508998 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:49:34.153450 140085747812160 submission_runner.py:408] Time since start: 5975.38s, 	Step: 11811, 	{'train/accuracy': 0.4569726586341858, 'train/loss': 2.680199146270752, 'validation/accuracy': 0.42282000184059143, 'validation/loss': 2.834578275680542, 'validation/num_examples': 50000, 'test/accuracy': 0.3346000015735626, 'test/loss': 3.379513740539551, 'test/num_examples': 10000, 'score': 5494.998826980591, 'total_duration': 5975.380021810532, 'accumulated_submission_time': 5494.998826980591, 'accumulated_eval_time': 479.35635805130005, 'accumulated_logging_time': 0.36748218536376953}
I0131 20:49:34.173000 139923868813056 logging_writer.py:48] [11811] accumulated_eval_time=479.356358, accumulated_logging_time=0.367482, accumulated_submission_time=5494.998827, global_step=11811, preemption_count=0, score=5494.998827, test/accuracy=0.334600, test/loss=3.379514, test/num_examples=10000, total_duration=5975.380022, train/accuracy=0.456973, train/loss=2.680199, validation/accuracy=0.422820, validation/loss=2.834578, validation/num_examples=50000
I0131 20:50:11.089666 139923852027648 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9532514810562134, loss=4.454133033752441
I0131 20:50:57.506454 139923868813056 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9593403935432434, loss=4.470907688140869
I0131 20:51:44.388720 139923852027648 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1640480756759644, loss=4.518026828765869
I0131 20:52:31.140441 139923868813056 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.6647207736968994, loss=5.984919548034668
I0131 20:53:17.974328 139923852027648 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.7031977772712708, loss=5.44704532623291
I0131 20:54:04.456609 139923868813056 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.6744127869606018, loss=6.064074516296387
I0131 20:54:51.156378 139923852027648 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9529068470001221, loss=4.498589992523193
I0131 20:55:37.778402 139923868813056 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7212450504302979, loss=6.11984395980835
I0131 20:56:24.534138 139923852027648 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.014951229095459, loss=4.504994869232178
I0131 20:56:34.188774 140085747812160 spec.py:321] Evaluating on the training split.
I0131 20:56:45.007517 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 20:57:06.603946 140085747812160 spec.py:349] Evaluating on the test split.
I0131 20:57:08.245573 140085747812160 submission_runner.py:408] Time since start: 6429.47s, 	Step: 12722, 	{'train/accuracy': 0.4777539074420929, 'train/loss': 2.5539870262145996, 'validation/accuracy': 0.4357999861240387, 'validation/loss': 2.754807949066162, 'validation/num_examples': 50000, 'test/accuracy': 0.34210002422332764, 'test/loss': 3.313959836959839, 'test/num_examples': 10000, 'score': 5914.95384311676, 'total_duration': 6429.472145318985, 'accumulated_submission_time': 5914.95384311676, 'accumulated_eval_time': 513.4131627082825, 'accumulated_logging_time': 0.39693355560302734}
I0131 20:57:08.264140 139923868813056 logging_writer.py:48] [12722] accumulated_eval_time=513.413163, accumulated_logging_time=0.396934, accumulated_submission_time=5914.953843, global_step=12722, preemption_count=0, score=5914.953843, test/accuracy=0.342100, test/loss=3.313960, test/num_examples=10000, total_duration=6429.472145, train/accuracy=0.477754, train/loss=2.553987, validation/accuracy=0.435800, validation/loss=2.754808, validation/num_examples=50000
I0131 20:57:39.942006 139923852027648 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9211232662200928, loss=4.512723922729492
I0131 20:58:26.448384 139923868813056 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9361648559570312, loss=4.365292549133301
I0131 20:59:13.454435 139923852027648 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6465939283370972, loss=6.119731903076172
I0131 20:59:59.774112 139923868813056 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9424948692321777, loss=4.5583014488220215
I0131 21:00:46.616229 139923852027648 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.925764262676239, loss=4.456535816192627
I0131 21:01:33.214156 139923868813056 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9342561960220337, loss=4.405351638793945
I0131 21:02:19.868948 139923852027648 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8967751860618591, loss=4.574337482452393
I0131 21:03:06.617884 139923868813056 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7396695017814636, loss=5.334022045135498
I0131 21:03:53.391151 139923852027648 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9428250193595886, loss=4.349713325500488
I0131 21:04:08.448826 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:04:19.061749 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:04:41.768284 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:04:43.409049 140085747812160 submission_runner.py:408] Time since start: 6884.64s, 	Step: 13634, 	{'train/accuracy': 0.4913281202316284, 'train/loss': 2.4966847896575928, 'validation/accuracy': 0.45917999744415283, 'validation/loss': 2.6533918380737305, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.21649432182312, 'test/num_examples': 10000, 'score': 6335.079059839249, 'total_duration': 6884.6356201171875, 'accumulated_submission_time': 6335.079059839249, 'accumulated_eval_time': 548.3734202384949, 'accumulated_logging_time': 0.42502331733703613}
I0131 21:04:43.429072 139923868813056 logging_writer.py:48] [13634] accumulated_eval_time=548.373420, accumulated_logging_time=0.425023, accumulated_submission_time=6335.079060, global_step=13634, preemption_count=0, score=6335.079060, test/accuracy=0.361300, test/loss=3.216494, test/num_examples=10000, total_duration=6884.635620, train/accuracy=0.491328, train/loss=2.496685, validation/accuracy=0.459180, validation/loss=2.653392, validation/num_examples=50000
I0131 21:05:09.724932 139923852027648 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9731511473655701, loss=4.391657829284668
I0131 21:05:55.850140 139923868813056 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0868237018585205, loss=4.420297145843506
I0131 21:06:42.258788 139923852027648 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7744132876396179, loss=5.0388288497924805
I0131 21:07:28.810040 139923868813056 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8400456309318542, loss=4.620397567749023
I0131 21:08:15.409149 139923852027648 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9247510433197021, loss=4.303752422332764
I0131 21:09:01.789608 139923868813056 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0509282350540161, loss=4.319062232971191
I0131 21:09:48.377501 139923852027648 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.9897816777229309, loss=4.428794860839844
I0131 21:10:34.857090 139923868813056 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6051523089408875, loss=5.667149066925049
I0131 21:11:21.260450 139923852027648 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8552441000938416, loss=4.25669002532959
I0131 21:11:43.766063 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:11:54.226250 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:12:16.717055 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:12:18.358942 140085747812160 submission_runner.py:408] Time since start: 7339.59s, 	Step: 14550, 	{'train/accuracy': 0.514355480670929, 'train/loss': 2.332359790802002, 'validation/accuracy': 0.4775199890136719, 'validation/loss': 2.504133939743042, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.088716983795166, 'test/num_examples': 10000, 'score': 6755.355276584625, 'total_duration': 7339.585505962372, 'accumulated_submission_time': 6755.355276584625, 'accumulated_eval_time': 582.966285943985, 'accumulated_logging_time': 0.45445990562438965}
I0131 21:12:18.375980 139923868813056 logging_writer.py:48] [14550] accumulated_eval_time=582.966286, accumulated_logging_time=0.454460, accumulated_submission_time=6755.355277, global_step=14550, preemption_count=0, score=6755.355277, test/accuracy=0.367100, test/loss=3.088717, test/num_examples=10000, total_duration=7339.585506, train/accuracy=0.514355, train/loss=2.332360, validation/accuracy=0.477520, validation/loss=2.504134, validation/num_examples=50000
I0131 21:12:38.398884 139923852027648 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.8779923915863037, loss=4.217715263366699
I0131 21:13:23.418660 139923868813056 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.7497478723526001, loss=5.810919761657715
I0131 21:14:10.259606 139923852027648 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9369478821754456, loss=4.269523620605469
I0131 21:14:56.802700 139923868813056 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6469346880912781, loss=5.745247840881348
I0131 21:15:43.392550 139923852027648 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7072550654411316, loss=6.003190994262695
I0131 21:16:29.775344 139923868813056 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.8517228960990906, loss=4.212862491607666
I0131 21:17:16.518603 139923852027648 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.8995087146759033, loss=4.170623779296875
I0131 21:18:03.173196 139923868813056 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9242904186248779, loss=4.456053733825684
I0131 21:18:49.713948 139923852027648 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.9376159310340881, loss=4.250751972198486
I0131 21:19:18.758791 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:19:29.171226 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:19:50.708636 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:19:52.359016 140085747812160 submission_runner.py:408] Time since start: 7793.59s, 	Step: 15464, 	{'train/accuracy': 0.5285546779632568, 'train/loss': 2.330787420272827, 'validation/accuracy': 0.48503997921943665, 'validation/loss': 2.5376617908477783, 'validation/num_examples': 50000, 'test/accuracy': 0.38200002908706665, 'test/loss': 3.105388879776001, 'test/num_examples': 10000, 'score': 7175.678120136261, 'total_duration': 7793.58558678627, 'accumulated_submission_time': 7175.678120136261, 'accumulated_eval_time': 616.5665156841278, 'accumulated_logging_time': 0.4815404415130615}
I0131 21:19:52.377346 139923868813056 logging_writer.py:48] [15464] accumulated_eval_time=616.566516, accumulated_logging_time=0.481540, accumulated_submission_time=7175.678120, global_step=15464, preemption_count=0, score=7175.678120, test/accuracy=0.382000, test/loss=3.105389, test/num_examples=10000, total_duration=7793.585587, train/accuracy=0.528555, train/loss=2.330787, validation/accuracy=0.485040, validation/loss=2.537662, validation/num_examples=50000
I0131 21:20:06.921518 139923852027648 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8600568771362305, loss=4.520481109619141
I0131 21:20:50.718872 139923868813056 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9078674912452698, loss=4.206699371337891
I0131 21:21:37.321480 139923852027648 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.8904006481170654, loss=4.483831405639648
I0131 21:22:23.794402 139923868813056 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7251039743423462, loss=5.352649688720703
I0131 21:23:10.513802 139923852027648 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.8761096000671387, loss=4.184959411621094
I0131 21:23:57.129658 139923868813056 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9208192825317383, loss=4.146548748016357
I0131 21:24:43.763807 139923852027648 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.9053376913070679, loss=4.253267765045166
I0131 21:25:30.447260 139923868813056 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.838100790977478, loss=4.653372287750244
I0131 21:26:17.094163 139923852027648 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9085047841072083, loss=4.264008522033691
I0131 21:26:52.774527 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:27:03.670202 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:27:25.580347 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:27:27.231384 140085747812160 submission_runner.py:408] Time since start: 8248.46s, 	Step: 16379, 	{'train/accuracy': 0.5498241782188416, 'train/loss': 2.2195632457733154, 'validation/accuracy': 0.5060399770736694, 'validation/loss': 2.409464120864868, 'validation/num_examples': 50000, 'test/accuracy': 0.38840001821517944, 'test/loss': 3.026716470718384, 'test/num_examples': 10000, 'score': 7596.0130705833435, 'total_duration': 8248.45794916153, 'accumulated_submission_time': 7596.0130705833435, 'accumulated_eval_time': 651.0233614444733, 'accumulated_logging_time': 0.5114836692810059}
I0131 21:27:27.249845 139923868813056 logging_writer.py:48] [16379] accumulated_eval_time=651.023361, accumulated_logging_time=0.511484, accumulated_submission_time=7596.013071, global_step=16379, preemption_count=0, score=7596.013071, test/accuracy=0.388400, test/loss=3.026716, test/num_examples=10000, total_duration=8248.457949, train/accuracy=0.549824, train/loss=2.219563, validation/accuracy=0.506040, validation/loss=2.409464, validation/num_examples=50000
I0131 21:27:35.891997 139923852027648 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9267844557762146, loss=4.361484050750732
I0131 21:28:18.570999 139923868813056 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.8465248942375183, loss=4.524188995361328
I0131 21:29:05.047567 139923852027648 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0277636051177979, loss=4.148208141326904
I0131 21:29:51.737556 139923868813056 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9911066889762878, loss=4.462747573852539
I0131 21:30:38.347387 139923852027648 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.032828688621521, loss=4.658525466918945
I0131 21:31:25.026003 139923868813056 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.94972163438797, loss=4.190399646759033
I0131 21:32:11.534938 139923852027648 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7361723780632019, loss=4.8102898597717285
I0131 21:32:58.336044 139923868813056 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9329328536987305, loss=4.096543788909912
I0131 21:33:44.834931 139923852027648 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0747103691101074, loss=4.145259857177734
I0131 21:34:27.422303 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:34:38.014174 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:34:58.094200 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:34:59.752376 140085747812160 submission_runner.py:408] Time since start: 8700.98s, 	Step: 17293, 	{'train/accuracy': 0.551562488079071, 'train/loss': 2.1806676387786865, 'validation/accuracy': 0.5150399804115295, 'validation/loss': 2.3470511436462402, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.956143617630005, 'test/num_examples': 10000, 'score': 8016.122918605804, 'total_duration': 8700.978935956955, 'accumulated_submission_time': 8016.122918605804, 'accumulated_eval_time': 683.3534235954285, 'accumulated_logging_time': 0.5411381721496582}
I0131 21:34:59.771070 139923868813056 logging_writer.py:48] [17293] accumulated_eval_time=683.353424, accumulated_logging_time=0.541138, accumulated_submission_time=8016.122919, global_step=17293, preemption_count=0, score=8016.122919, test/accuracy=0.409600, test/loss=2.956144, test/num_examples=10000, total_duration=8700.978936, train/accuracy=0.551562, train/loss=2.180668, validation/accuracy=0.515040, validation/loss=2.347051, validation/num_examples=50000
I0131 21:35:02.915338 139923852027648 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9485875964164734, loss=4.188478469848633
I0131 21:35:44.656124 139923868813056 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9583888053894043, loss=4.196120262145996
I0131 21:36:31.442097 139923852027648 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7941145896911621, loss=4.857909202575684
I0131 21:37:18.359701 139923868813056 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7925789952278137, loss=5.438067436218262
I0131 21:38:04.869887 139923852027648 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.919969916343689, loss=4.1604108810424805
I0131 21:38:51.573705 139923868813056 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0176832675933838, loss=4.160239219665527
I0131 21:39:38.024478 139923852027648 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.6956530213356018, loss=5.9279704093933105
I0131 21:40:24.833541 139923868813056 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1068285703659058, loss=4.269909381866455
I0131 21:41:11.377711 139923852027648 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9085715413093567, loss=4.057609558105469
I0131 21:41:58.052689 139923868813056 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9356694221496582, loss=4.099750518798828
I0131 21:42:00.053727 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:42:10.736979 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:42:32.868583 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:42:34.510302 140085747812160 submission_runner.py:408] Time since start: 9155.74s, 	Step: 18206, 	{'train/accuracy': 0.5733984112739563, 'train/loss': 2.0888290405273438, 'validation/accuracy': 0.5263000130653381, 'validation/loss': 2.2876055240631104, 'validation/num_examples': 50000, 'test/accuracy': 0.4150000214576721, 'test/loss': 2.885838031768799, 'test/num_examples': 10000, 'score': 8436.345466375351, 'total_duration': 9155.736872911453, 'accumulated_submission_time': 8436.345466375351, 'accumulated_eval_time': 717.8099949359894, 'accumulated_logging_time': 0.5694966316223145}
I0131 21:42:34.528598 139923852027648 logging_writer.py:48] [18206] accumulated_eval_time=717.809995, accumulated_logging_time=0.569497, accumulated_submission_time=8436.345466, global_step=18206, preemption_count=0, score=8436.345466, test/accuracy=0.415000, test/loss=2.885838, test/num_examples=10000, total_duration=9155.736873, train/accuracy=0.573398, train/loss=2.088829, validation/accuracy=0.526300, validation/loss=2.287606, validation/num_examples=50000
I0131 21:43:13.628172 139923868813056 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9463433623313904, loss=4.202337265014648
I0131 21:43:59.934606 139923852027648 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.065579891204834, loss=4.025015354156494
I0131 21:44:46.873062 139923868813056 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.923768162727356, loss=4.228206157684326
I0131 21:45:33.377405 139923852027648 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8567138314247131, loss=4.792289733886719
I0131 21:46:19.957583 139923868813056 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6920000314712524, loss=5.7817535400390625
I0131 21:47:06.574891 139923852027648 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.9651516675949097, loss=4.053778648376465
I0131 21:47:53.136067 139923868813056 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.9261792302131653, loss=4.398778915405273
I0131 21:48:39.805885 139923852027648 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9232325553894043, loss=4.041466236114502
I0131 21:49:26.480634 139923868813056 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.9030584692955017, loss=4.0990800857543945
I0131 21:49:34.588847 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:49:45.153797 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:50:07.058030 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:50:08.699875 140085747812160 submission_runner.py:408] Time since start: 9609.93s, 	Step: 19119, 	{'train/accuracy': 0.6010546684265137, 'train/loss': 1.9360231161117554, 'validation/accuracy': 0.5326799750328064, 'validation/loss': 2.2370855808258057, 'validation/num_examples': 50000, 'test/accuracy': 0.42100003361701965, 'test/loss': 2.853330612182617, 'test/num_examples': 10000, 'score': 8856.345707178116, 'total_duration': 9609.926443576813, 'accumulated_submission_time': 8856.345707178116, 'accumulated_eval_time': 751.9210438728333, 'accumulated_logging_time': 0.5966720581054688}
I0131 21:50:08.717939 139923852027648 logging_writer.py:48] [19119] accumulated_eval_time=751.921044, accumulated_logging_time=0.596672, accumulated_submission_time=8856.345707, global_step=19119, preemption_count=0, score=8856.345707, test/accuracy=0.421000, test/loss=2.853331, test/num_examples=10000, total_duration=9609.926444, train/accuracy=0.601055, train/loss=1.936023, validation/accuracy=0.532680, validation/loss=2.237086, validation/num_examples=50000
I0131 21:50:41.578198 139923868813056 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.8058081269264221, loss=4.688014507293701
I0131 21:51:28.256899 139923852027648 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8844313621520996, loss=4.327603816986084
I0131 21:52:15.117016 139923868813056 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9123367667198181, loss=4.2482428550720215
I0131 21:53:01.938192 139923852027648 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0255277156829834, loss=4.152106285095215
I0131 21:53:48.707579 139923868813056 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.8950973749160767, loss=4.054733753204346
I0131 21:54:35.146464 139923852027648 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7866347432136536, loss=4.646947860717773
I0131 21:55:24.083820 139923868813056 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.9517503976821899, loss=3.995471239089966
I0131 21:56:11.378899 139923852027648 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9242042303085327, loss=4.016351222991943
I0131 21:56:58.184234 139923868813056 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6610921025276184, loss=5.566089153289795
I0131 21:57:08.717286 140085747812160 spec.py:321] Evaluating on the training split.
I0131 21:57:19.182168 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 21:57:41.298362 140085747812160 spec.py:349] Evaluating on the test split.
I0131 21:57:42.938569 140085747812160 submission_runner.py:408] Time since start: 10064.17s, 	Step: 20024, 	{'train/accuracy': 0.581347644329071, 'train/loss': 2.0001742839813232, 'validation/accuracy': 0.541920006275177, 'validation/loss': 2.18528151512146, 'validation/num_examples': 50000, 'test/accuracy': 0.4229000210762024, 'test/loss': 2.806061267852783, 'test/num_examples': 10000, 'score': 9276.2863240242, 'total_duration': 10064.165142297745, 'accumulated_submission_time': 9276.2863240242, 'accumulated_eval_time': 786.1423320770264, 'accumulated_logging_time': 0.6236920356750488}
I0131 21:57:42.959922 139923852027648 logging_writer.py:48] [20024] accumulated_eval_time=786.142332, accumulated_logging_time=0.623692, accumulated_submission_time=9276.286324, global_step=20024, preemption_count=0, score=9276.286324, test/accuracy=0.422900, test/loss=2.806061, test/num_examples=10000, total_duration=10064.165142, train/accuracy=0.581348, train/loss=2.000174, validation/accuracy=0.541920, validation/loss=2.185282, validation/num_examples=50000
I0131 21:58:13.629623 139923868813056 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9010285139083862, loss=4.120832443237305
I0131 21:59:00.113631 139923852027648 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7461007833480835, loss=5.188658714294434
I0131 21:59:46.973510 139923868813056 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.936936616897583, loss=4.0841193199157715
I0131 22:00:33.916879 139923852027648 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.9276465177536011, loss=4.180865287780762
I0131 22:01:20.632229 139923868813056 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9748713374137878, loss=3.9524648189544678
I0131 22:02:07.501968 139923852027648 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.9178256988525391, loss=4.228893280029297
I0131 22:02:54.329771 139923868813056 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.695152997970581, loss=5.750393867492676
I0131 22:03:40.953411 139923852027648 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6716325283050537, loss=5.677337169647217
I0131 22:04:27.758442 139923868813056 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.7423197627067566, loss=5.655248641967773
I0131 22:04:43.273065 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:04:53.845532 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:05:16.601289 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:05:18.241941 140085747812160 submission_runner.py:408] Time since start: 10519.47s, 	Step: 20935, 	{'train/accuracy': 0.5981249809265137, 'train/loss': 1.9761009216308594, 'validation/accuracy': 0.5450199842453003, 'validation/loss': 2.1948776245117188, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.791691541671753, 'test/num_examples': 10000, 'score': 9696.536899328232, 'total_duration': 10519.46850657463, 'accumulated_submission_time': 9696.536899328232, 'accumulated_eval_time': 821.1112020015717, 'accumulated_logging_time': 0.6576018333435059}
I0131 22:05:18.260601 139923852027648 logging_writer.py:48] [20935] accumulated_eval_time=821.111202, accumulated_logging_time=0.657602, accumulated_submission_time=9696.536899, global_step=20935, preemption_count=0, score=9696.536899, test/accuracy=0.441300, test/loss=2.791692, test/num_examples=10000, total_duration=10519.468507, train/accuracy=0.598125, train/loss=1.976101, validation/accuracy=0.545020, validation/loss=2.194878, validation/num_examples=50000
I0131 22:05:44.181848 139923868813056 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8259479999542236, loss=4.807064056396484
I0131 22:06:30.302599 139923852027648 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8115496635437012, loss=5.63321590423584
I0131 22:07:17.421870 139923868813056 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9036138653755188, loss=4.179669380187988
I0131 22:08:04.278881 139923852027648 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.9554570913314819, loss=4.080936431884766
I0131 22:08:50.564453 139923868813056 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.7530162930488586, loss=5.164236545562744
I0131 22:09:37.609159 139923852027648 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.019598364830017, loss=3.94948148727417
I0131 22:10:24.389207 139923868813056 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.6737349629402161, loss=5.624689102172852
I0131 22:11:10.763862 139923852027648 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.738979697227478, loss=4.7725419998168945
I0131 22:11:57.543284 139923868813056 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8482269644737244, loss=4.18384313583374
I0131 22:12:18.778751 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:12:29.314060 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:12:51.941077 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:12:53.587490 140085747812160 submission_runner.py:408] Time since start: 10974.81s, 	Step: 21847, 	{'train/accuracy': 0.6225780844688416, 'train/loss': 1.8207433223724365, 'validation/accuracy': 0.5593400001525879, 'validation/loss': 2.095564365386963, 'validation/num_examples': 50000, 'test/accuracy': 0.4457000195980072, 'test/loss': 2.695769786834717, 'test/num_examples': 10000, 'score': 10116.994480848312, 'total_duration': 10974.814060688019, 'accumulated_submission_time': 10116.994480848312, 'accumulated_eval_time': 855.9199452400208, 'accumulated_logging_time': 0.6856215000152588}
I0131 22:12:53.609247 139923852027648 logging_writer.py:48] [21847] accumulated_eval_time=855.919945, accumulated_logging_time=0.685622, accumulated_submission_time=10116.994481, global_step=21847, preemption_count=0, score=10116.994481, test/accuracy=0.445700, test/loss=2.695770, test/num_examples=10000, total_duration=10974.814061, train/accuracy=0.622578, train/loss=1.820743, validation/accuracy=0.559340, validation/loss=2.095564, validation/num_examples=50000
I0131 22:13:14.815313 139923868813056 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9192051291465759, loss=3.968357563018799
I0131 22:14:00.424706 139923852027648 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.7268207669258118, loss=4.921134948730469
I0131 22:14:47.055484 139923868813056 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0391148328781128, loss=3.916207790374756
I0131 22:15:33.668670 139923852027648 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.9770050644874573, loss=4.008939743041992
I0131 22:16:20.444858 139923868813056 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9719506502151489, loss=3.9500198364257812
I0131 22:17:06.932783 139923852027648 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.7960354089736938, loss=5.614950656890869
I0131 22:17:53.520019 139923868813056 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.645937442779541, loss=5.6579790115356445
I0131 22:18:39.938631 139923852027648 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9295452237129211, loss=4.553070545196533
I0131 22:19:26.392088 139923868813056 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9074857234954834, loss=4.151124000549316
I0131 22:19:54.069970 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:20:04.438795 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:20:27.005817 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:20:28.655371 140085747812160 submission_runner.py:408] Time since start: 11429.88s, 	Step: 22761, 	{'train/accuracy': 0.6095117330551147, 'train/loss': 1.9055193662643433, 'validation/accuracy': 0.5639399886131287, 'validation/loss': 2.1037421226501465, 'validation/num_examples': 50000, 'test/accuracy': 0.44860002398490906, 'test/loss': 2.720872640609741, 'test/num_examples': 10000, 'score': 10537.393913984299, 'total_duration': 11429.881940364838, 'accumulated_submission_time': 10537.393913984299, 'accumulated_eval_time': 890.505363702774, 'accumulated_logging_time': 0.7179117202758789}
I0131 22:20:28.678523 139923852027648 logging_writer.py:48] [22761] accumulated_eval_time=890.505364, accumulated_logging_time=0.717912, accumulated_submission_time=10537.393914, global_step=22761, preemption_count=0, score=10537.393914, test/accuracy=0.448600, test/loss=2.720873, test/num_examples=10000, total_duration=11429.881940, train/accuracy=0.609512, train/loss=1.905519, validation/accuracy=0.563940, validation/loss=2.103742, validation/num_examples=50000
I0131 22:20:44.383707 139923868813056 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0147501230239868, loss=4.208881378173828
I0131 22:21:28.575748 139923852027648 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8850199580192566, loss=3.968064785003662
I0131 22:22:15.521828 139923868813056 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7233520150184631, loss=5.732799530029297
I0131 22:23:02.347003 139923852027648 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.9048831462860107, loss=3.8730764389038086
I0131 22:23:49.297533 139923868813056 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8075465559959412, loss=5.341396808624268
I0131 22:24:35.748653 139923852027648 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.9889530539512634, loss=3.986551523208618
I0131 22:25:22.515145 139923868813056 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.8944820761680603, loss=4.346748352050781
I0131 22:26:09.285283 139923852027648 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0195292234420776, loss=3.9574642181396484
I0131 22:26:55.847137 139923868813056 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.010605812072754, loss=3.8812477588653564
I0131 22:27:28.732675 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:27:39.037507 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:28:00.113644 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:28:01.746837 140085747812160 submission_runner.py:408] Time since start: 11882.97s, 	Step: 23672, 	{'train/accuracy': 0.6233788728713989, 'train/loss': 1.8191262483596802, 'validation/accuracy': 0.5717200040817261, 'validation/loss': 2.032801628112793, 'validation/num_examples': 50000, 'test/accuracy': 0.4562000334262848, 'test/loss': 2.6657521724700928, 'test/num_examples': 10000, 'score': 10957.388006210327, 'total_duration': 11882.973408699036, 'accumulated_submission_time': 10957.388006210327, 'accumulated_eval_time': 923.5195500850677, 'accumulated_logging_time': 0.7508177757263184}
I0131 22:28:01.767500 139923852027648 logging_writer.py:48] [23672] accumulated_eval_time=923.519550, accumulated_logging_time=0.750818, accumulated_submission_time=10957.388006, global_step=23672, preemption_count=0, score=10957.388006, test/accuracy=0.456200, test/loss=2.665752, test/num_examples=10000, total_duration=11882.973409, train/accuracy=0.623379, train/loss=1.819126, validation/accuracy=0.571720, validation/loss=2.032802, validation/num_examples=50000
I0131 22:28:13.165338 139923868813056 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9555287957191467, loss=4.30046272277832
I0131 22:28:56.640466 139923852027648 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.8845392465591431, loss=4.3227081298828125
I0131 22:29:42.955877 139923868813056 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.6951374411582947, loss=5.67024040222168
I0131 22:30:30.082815 139923852027648 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7930748462677002, loss=4.43758487701416
I0131 22:31:16.695974 139923868813056 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0884376764297485, loss=3.979104518890381
I0131 22:32:03.234456 139923852027648 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.880778431892395, loss=5.678123950958252
I0131 22:32:50.082179 139923868813056 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8063163161277771, loss=4.656919002532959
I0131 22:33:36.652433 139923852027648 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.7276219129562378, loss=4.874117374420166
I0131 22:34:23.462267 139923868813056 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.94800865650177, loss=4.004020690917969
I0131 22:35:02.085951 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:35:12.669750 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:35:34.561169 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:35:36.201411 140085747812160 submission_runner.py:408] Time since start: 12337.43s, 	Step: 24585, 	{'train/accuracy': 0.6335155963897705, 'train/loss': 1.7896169424057007, 'validation/accuracy': 0.5715799927711487, 'validation/loss': 2.054870367050171, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.6737120151519775, 'test/num_examples': 10000, 'score': 11377.642796039581, 'total_duration': 12337.427982330322, 'accumulated_submission_time': 11377.642796039581, 'accumulated_eval_time': 957.6350147724152, 'accumulated_logging_time': 0.784376859664917}
I0131 22:35:36.220976 139923852027648 logging_writer.py:48] [24585] accumulated_eval_time=957.635015, accumulated_logging_time=0.784377, accumulated_submission_time=11377.642796, global_step=24585, preemption_count=0, score=11377.642796, test/accuracy=0.456900, test/loss=2.673712, test/num_examples=10000, total_duration=12337.427982, train/accuracy=0.633516, train/loss=1.789617, validation/accuracy=0.571580, validation/loss=2.054870, validation/num_examples=50000
I0131 22:35:42.515213 139923868813056 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.957872748374939, loss=3.946978807449341
I0131 22:36:25.047240 139923852027648 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7162851095199585, loss=5.7086687088012695
I0131 22:37:11.907824 139923868813056 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0306378602981567, loss=3.8243088722229004
I0131 22:37:58.565209 139923852027648 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0842366218566895, loss=3.9471023082733154
I0131 22:38:45.467535 139923868813056 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7361924648284912, loss=5.3067145347595215
I0131 22:39:32.041141 139923852027648 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.775731086730957, loss=5.0722737312316895
I0131 22:40:18.686625 139923868813056 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.967490553855896, loss=3.9594814777374268
I0131 22:41:05.503767 139923852027648 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.0279955863952637, loss=3.858611583709717
I0131 22:41:52.226991 139923868813056 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.9477840065956116, loss=4.009763717651367
I0131 22:42:36.618912 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:42:46.988214 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:43:09.074185 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:43:10.713433 140085747812160 submission_runner.py:408] Time since start: 12791.94s, 	Step: 25496, 	{'train/accuracy': 0.6327929496765137, 'train/loss': 1.8049424886703491, 'validation/accuracy': 0.5851399898529053, 'validation/loss': 2.0230395793914795, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.616346597671509, 'test/num_examples': 10000, 'score': 11797.979766845703, 'total_duration': 12791.94000005722, 'accumulated_submission_time': 11797.979766845703, 'accumulated_eval_time': 991.7295281887054, 'accumulated_logging_time': 0.8135378360748291}
I0131 22:43:10.737204 139923852027648 logging_writer.py:48] [25496] accumulated_eval_time=991.729528, accumulated_logging_time=0.813538, accumulated_submission_time=11797.979767, global_step=25496, preemption_count=0, score=11797.979767, test/accuracy=0.470800, test/loss=2.616347, test/num_examples=10000, total_duration=12791.940000, train/accuracy=0.632793, train/loss=1.804942, validation/accuracy=0.585140, validation/loss=2.023040, validation/num_examples=50000
I0131 22:43:12.707715 139923868813056 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8804691433906555, loss=4.472235679626465
I0131 22:43:54.623137 139923852027648 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.7507359981536865, loss=5.685877323150635
I0131 22:44:41.309380 139923868813056 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.0771359205245972, loss=3.9751076698303223
I0131 22:45:28.196894 139923852027648 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.0007553100585938, loss=3.91229248046875
I0131 22:46:14.666972 139923868813056 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9658045768737793, loss=3.906466007232666
I0131 22:47:01.463314 139923852027648 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7550488710403442, loss=4.721418857574463
I0131 22:47:48.355243 139923868813056 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9562447667121887, loss=3.8307104110717773
I0131 22:48:35.293889 139923852027648 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.9367856383323669, loss=3.8482110500335693
I0131 22:49:22.192595 139923868813056 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8124623894691467, loss=4.791730880737305
I0131 22:50:09.002562 139923852027648 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9300761222839355, loss=4.689517498016357
I0131 22:50:10.955109 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:50:21.546810 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:50:43.404486 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:50:45.044362 140085747812160 submission_runner.py:408] Time since start: 13246.27s, 	Step: 26406, 	{'train/accuracy': 0.6333593726158142, 'train/loss': 1.802080750465393, 'validation/accuracy': 0.5868799686431885, 'validation/loss': 2.013918876647949, 'validation/num_examples': 50000, 'test/accuracy': 0.47290003299713135, 'test/loss': 2.6314289569854736, 'test/num_examples': 10000, 'score': 12218.137630224228, 'total_duration': 13246.270922422409, 'accumulated_submission_time': 12218.137630224228, 'accumulated_eval_time': 1025.8188734054565, 'accumulated_logging_time': 0.8469099998474121}
I0131 22:50:45.064800 139923868813056 logging_writer.py:48] [26406] accumulated_eval_time=1025.818873, accumulated_logging_time=0.846910, accumulated_submission_time=12218.137630, global_step=26406, preemption_count=0, score=12218.137630, test/accuracy=0.472900, test/loss=2.631429, test/num_examples=10000, total_duration=13246.270922, train/accuracy=0.633359, train/loss=1.802081, validation/accuracy=0.586880, validation/loss=2.013919, validation/num_examples=50000
I0131 22:51:24.038811 139923852027648 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.9563251733779907, loss=3.9078845977783203
I0131 22:52:10.687532 139923868813056 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.9313796758651733, loss=3.9816551208496094
I0131 22:52:57.852266 139923852027648 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.8309204578399658, loss=4.489902496337891
I0131 22:53:44.533079 139923868813056 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.8723821043968201, loss=4.1340837478637695
I0131 22:54:31.356727 139923852027648 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9997832775115967, loss=3.803577423095703
I0131 22:55:17.886885 139923868813056 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8458136320114136, loss=5.151858329772949
I0131 22:56:04.511876 139923852027648 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7940672039985657, loss=4.955905914306641
I0131 22:56:51.094747 139923868813056 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.8090546727180481, loss=5.024173259735107
I0131 22:57:37.881194 139923852027648 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9998272657394409, loss=3.77512264251709
I0131 22:57:45.144529 140085747812160 spec.py:321] Evaluating on the training split.
I0131 22:57:55.759445 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 22:58:17.936026 140085747812160 spec.py:349] Evaluating on the test split.
I0131 22:58:19.575102 140085747812160 submission_runner.py:408] Time since start: 13700.80s, 	Step: 27317, 	{'train/accuracy': 0.6479687094688416, 'train/loss': 1.6960623264312744, 'validation/accuracy': 0.5909599661827087, 'validation/loss': 1.9587397575378418, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.574725866317749, 'test/num_examples': 10000, 'score': 12638.156475067139, 'total_duration': 13700.801671981812, 'accumulated_submission_time': 12638.156475067139, 'accumulated_eval_time': 1060.2494506835938, 'accumulated_logging_time': 0.8771259784698486}
I0131 22:58:19.594442 139923868813056 logging_writer.py:48] [27317] accumulated_eval_time=1060.249451, accumulated_logging_time=0.877126, accumulated_submission_time=12638.156475, global_step=27317, preemption_count=0, score=12638.156475, test/accuracy=0.472700, test/loss=2.574726, test/num_examples=10000, total_duration=13700.801672, train/accuracy=0.647969, train/loss=1.696062, validation/accuracy=0.590960, validation/loss=1.958740, validation/num_examples=50000
I0131 22:58:53.324936 139923852027648 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.8145633935928345, loss=5.234450340270996
I0131 22:59:39.867632 139923868813056 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8134555220603943, loss=4.187135219573975
I0131 23:00:26.754830 139923852027648 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9021867513656616, loss=4.0553812980651855
I0131 23:01:13.357693 139923868813056 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9942046403884888, loss=3.8368771076202393
I0131 23:02:00.276790 139923852027648 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.0217036008834839, loss=3.811781883239746
I0131 23:02:47.269605 139923868813056 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9466676712036133, loss=3.8295176029205322
I0131 23:03:33.902254 139923852027648 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.8148980140686035, loss=5.291424751281738
I0131 23:04:20.402910 139923868813056 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.9948305487632751, loss=3.7642295360565186
I0131 23:05:07.148640 139923852027648 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.9781772494316101, loss=3.795567035675049
I0131 23:05:19.809874 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:05:30.243929 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:05:51.868938 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:05:53.505180 140085747812160 submission_runner.py:408] Time since start: 14154.73s, 	Step: 28229, 	{'train/accuracy': 0.6434375047683716, 'train/loss': 1.7335150241851807, 'validation/accuracy': 0.5952199697494507, 'validation/loss': 1.9401875734329224, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.569722890853882, 'test/num_examples': 10000, 'score': 13058.312211036682, 'total_duration': 14154.731746673584, 'accumulated_submission_time': 13058.312211036682, 'accumulated_eval_time': 1093.944759130478, 'accumulated_logging_time': 0.9052963256835938}
I0131 23:05:53.527462 139923868813056 logging_writer.py:48] [28229] accumulated_eval_time=1093.944759, accumulated_logging_time=0.905296, accumulated_submission_time=13058.312211, global_step=28229, preemption_count=0, score=13058.312211, test/accuracy=0.475300, test/loss=2.569723, test/num_examples=10000, total_duration=14154.731747, train/accuracy=0.643438, train/loss=1.733515, validation/accuracy=0.595220, validation/loss=1.940188, validation/num_examples=50000
I0131 23:06:22.084616 139923852027648 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.9418431520462036, loss=3.7853283882141113
I0131 23:07:08.479511 139923868813056 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.9629390239715576, loss=4.130245208740234
I0131 23:07:55.358015 139923852027648 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8201980590820312, loss=5.182727336883545
I0131 23:08:42.207077 139923868813056 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.066757082939148, loss=3.7311513423919678
I0131 23:09:29.124043 139923852027648 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9742273688316345, loss=3.8274877071380615
I0131 23:10:15.953715 139923868813056 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0152117013931274, loss=3.9377481937408447
I0131 23:11:02.488641 139923852027648 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9685814380645752, loss=3.96063232421875
I0131 23:11:49.177685 139923868813056 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7571353912353516, loss=4.6563897132873535
I0131 23:12:36.309114 139923852027648 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8038995862007141, loss=5.6893439292907715
I0131 23:12:53.692521 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:13:04.233911 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:13:26.000642 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:13:27.639947 140085747812160 submission_runner.py:408] Time since start: 14608.87s, 	Step: 29139, 	{'train/accuracy': 0.6492577791213989, 'train/loss': 1.7050681114196777, 'validation/accuracy': 0.601639986038208, 'validation/loss': 1.9144740104675293, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.5420379638671875, 'test/num_examples': 10000, 'score': 13478.41717338562, 'total_duration': 14608.866518974304, 'accumulated_submission_time': 13478.41717338562, 'accumulated_eval_time': 1127.892193555832, 'accumulated_logging_time': 0.9366669654846191}
I0131 23:13:27.663563 139923868813056 logging_writer.py:48] [29139] accumulated_eval_time=1127.892194, accumulated_logging_time=0.936667, accumulated_submission_time=13478.417173, global_step=29139, preemption_count=0, score=13478.417173, test/accuracy=0.475800, test/loss=2.542038, test/num_examples=10000, total_duration=14608.866519, train/accuracy=0.649258, train/loss=1.705068, validation/accuracy=0.601640, validation/loss=1.914474, validation/num_examples=50000
I0131 23:13:52.011326 139923852027648 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.9983079433441162, loss=3.723626136779785
I0131 23:14:37.785017 139923868813056 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6838886737823486, loss=5.694881439208984
I0131 23:15:24.512243 139923852027648 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.8228846192359924, loss=4.548160552978516
I0131 23:16:10.950298 139923868813056 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8983951210975647, loss=4.3377790451049805
I0131 23:16:57.583583 139923852027648 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.8015921711921692, loss=5.0138678550720215
I0131 23:17:44.192585 139923868813056 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.8356678485870361, loss=5.643850326538086
I0131 23:18:31.003733 139923852027648 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.9776448011398315, loss=3.736417770385742
I0131 23:19:17.525914 139923868813056 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1933945417404175, loss=3.8384721279144287
I0131 23:20:04.295084 139923852027648 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.8403255343437195, loss=5.621847152709961
I0131 23:20:27.963578 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:20:38.527346 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:21:00.213510 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:21:01.877514 140085747812160 submission_runner.py:408] Time since start: 15063.10s, 	Step: 30052, 	{'train/accuracy': 0.6590429544448853, 'train/loss': 1.6291348934173584, 'validation/accuracy': 0.6089199781417847, 'validation/loss': 1.8620059490203857, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.49440336227417, 'test/num_examples': 10000, 'score': 13898.656394004822, 'total_duration': 15063.104083538055, 'accumulated_submission_time': 13898.656394004822, 'accumulated_eval_time': 1161.8061337471008, 'accumulated_logging_time': 0.9697074890136719}
I0131 23:21:01.898150 139923868813056 logging_writer.py:48] [30052] accumulated_eval_time=1161.806134, accumulated_logging_time=0.969707, accumulated_submission_time=13898.656394, global_step=30052, preemption_count=0, score=13898.656394, test/accuracy=0.487900, test/loss=2.494403, test/num_examples=10000, total_duration=15063.104084, train/accuracy=0.659043, train/loss=1.629135, validation/accuracy=0.608920, validation/loss=1.862006, validation/num_examples=50000
I0131 23:21:21.347507 139923852027648 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0157580375671387, loss=3.827199697494507
I0131 23:22:06.097802 139923868813056 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.9407461285591125, loss=4.274619102478027
I0131 23:22:52.998910 139923852027648 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0956380367279053, loss=3.8430161476135254
I0131 23:23:39.730964 139923868813056 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.9830837249755859, loss=3.8393056392669678
I0131 23:24:26.649161 139923852027648 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.1437867879867554, loss=3.791187047958374
I0131 23:25:13.344096 139923868813056 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.8207674622535706, loss=5.633499622344971
I0131 23:26:00.120557 139923852027648 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0766568183898926, loss=3.6940624713897705
I0131 23:26:46.813397 139923868813056 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.0706430673599243, loss=3.7713263034820557
I0131 23:27:33.305592 139923852027648 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.0017337799072266, loss=3.7382164001464844
I0131 23:28:01.985477 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:28:12.346279 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:28:33.598632 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:28:35.249577 140085747812160 submission_runner.py:408] Time since start: 15516.48s, 	Step: 30963, 	{'train/accuracy': 0.6642773151397705, 'train/loss': 1.6285508871078491, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.841789960861206, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.465266227722168, 'test/num_examples': 10000, 'score': 14318.683321475983, 'total_duration': 15516.47614622116, 'accumulated_submission_time': 14318.683321475983, 'accumulated_eval_time': 1195.0702483654022, 'accumulated_logging_time': 1.0004019737243652}
I0131 23:28:35.273041 139923868813056 logging_writer.py:48] [30963] accumulated_eval_time=1195.070248, accumulated_logging_time=1.000402, accumulated_submission_time=14318.683321, global_step=30963, preemption_count=0, score=14318.683321, test/accuracy=0.496300, test/loss=2.465266, test/num_examples=10000, total_duration=15516.476146, train/accuracy=0.664277, train/loss=1.628551, validation/accuracy=0.613120, validation/loss=1.841790, validation/num_examples=50000
I0131 23:28:50.199532 139923852027648 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0162925720214844, loss=3.814380645751953
I0131 23:29:34.333372 139923868813056 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.9977527260780334, loss=3.7574362754821777
I0131 23:30:21.207944 139923852027648 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.8641432523727417, loss=4.036622047424316
I0131 23:31:08.100035 139923868813056 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.9337176084518433, loss=4.025537490844727
I0131 23:31:54.532288 139923852027648 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.926552951335907, loss=4.452145576477051
I0131 23:32:41.335250 139923868813056 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.9871528148651123, loss=3.665644645690918
I0131 23:33:27.915290 139923852027648 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0531467199325562, loss=3.7441534996032715
I0131 23:34:14.929773 139923868813056 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9997962713241577, loss=3.7213430404663086
I0131 23:35:01.499053 139923852027648 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0321394205093384, loss=3.8289642333984375
I0131 23:35:35.667902 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:35:46.248285 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:36:07.789742 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:36:09.431061 140085747812160 submission_runner.py:408] Time since start: 15970.66s, 	Step: 31875, 	{'train/accuracy': 0.6638085842132568, 'train/loss': 1.6437461376190186, 'validation/accuracy': 0.6118999719619751, 'validation/loss': 1.870315432548523, 'validation/num_examples': 50000, 'test/accuracy': 0.4912000298500061, 'test/loss': 2.4833028316497803, 'test/num_examples': 10000, 'score': 14739.014737844467, 'total_duration': 15970.657630205154, 'accumulated_submission_time': 14739.014737844467, 'accumulated_eval_time': 1228.8334305286407, 'accumulated_logging_time': 1.0361483097076416}
I0131 23:36:09.455029 139923868813056 logging_writer.py:48] [31875] accumulated_eval_time=1228.833431, accumulated_logging_time=1.036148, accumulated_submission_time=14739.014738, global_step=31875, preemption_count=0, score=14739.014738, test/accuracy=0.491200, test/loss=2.483303, test/num_examples=10000, total_duration=15970.657630, train/accuracy=0.663809, train/loss=1.643746, validation/accuracy=0.611900, validation/loss=1.870315, validation/num_examples=50000
I0131 23:36:19.676266 139923852027648 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.0295974016189575, loss=3.681894063949585
I0131 23:37:02.705846 139923868813056 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.835105836391449, loss=4.540277004241943
I0131 23:37:49.331660 139923852027648 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.2044717073440552, loss=3.728865146636963
I0131 23:38:36.034698 139923868813056 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0276271104812622, loss=3.693051815032959
I0131 23:39:22.543641 139923852027648 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.9361099600791931, loss=3.7173523902893066
I0131 23:40:09.222115 139923868813056 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9013006687164307, loss=4.1400299072265625
I0131 23:40:55.835695 139923852027648 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.0770939588546753, loss=3.740078926086426
I0131 23:41:42.358757 139923868813056 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0647400617599487, loss=4.058119297027588
I0131 23:42:28.833662 139923852027648 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.0777428150177002, loss=4.157517433166504
I0131 23:43:09.794772 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:43:20.484318 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:43:42.374867 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:43:44.022446 140085747812160 submission_runner.py:408] Time since start: 16425.25s, 	Step: 32789, 	{'train/accuracy': 0.6706835627555847, 'train/loss': 1.6040401458740234, 'validation/accuracy': 0.614139974117279, 'validation/loss': 1.8436777591705322, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.4702181816101074, 'test/num_examples': 10000, 'score': 15159.29290342331, 'total_duration': 16425.24866938591, 'accumulated_submission_time': 15159.29290342331, 'accumulated_eval_time': 1263.0607657432556, 'accumulated_logging_time': 1.0695884227752686}
I0131 23:43:44.049592 139923868813056 logging_writer.py:48] [32789] accumulated_eval_time=1263.060766, accumulated_logging_time=1.069588, accumulated_submission_time=15159.292903, global_step=32789, preemption_count=0, score=15159.292903, test/accuracy=0.496900, test/loss=2.470218, test/num_examples=10000, total_duration=16425.248669, train/accuracy=0.670684, train/loss=1.604040, validation/accuracy=0.614140, validation/loss=1.843678, validation/num_examples=50000
I0131 23:43:48.759104 139923852027648 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0140907764434814, loss=3.8576107025146484
I0131 23:44:30.936658 139923868813056 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.0660218000411987, loss=3.716413974761963
I0131 23:45:17.595618 139923852027648 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.985721230506897, loss=3.7504498958587646
I0131 23:46:04.606276 139923868813056 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.9378371834754944, loss=3.7998664379119873
I0131 23:46:51.247949 139923852027648 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.8470329642295837, loss=4.437535762786865
I0131 23:47:37.911865 139923868813056 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.9665396213531494, loss=3.7268710136413574
I0131 23:48:24.507273 139923852027648 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8628668189048767, loss=4.418551921844482
I0131 23:49:11.026626 139923868813056 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.993502676486969, loss=3.812138080596924
I0131 23:49:57.695160 139923852027648 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.8923848271369934, loss=4.141434669494629
I0131 23:50:44.293566 139923868813056 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.9762396812438965, loss=3.8137526512145996
I0131 23:50:44.311270 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:50:54.876099 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:51:18.034208 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:51:19.676546 140085747812160 submission_runner.py:408] Time since start: 16880.90s, 	Step: 33701, 	{'train/accuracy': 0.6911327838897705, 'train/loss': 1.4933984279632568, 'validation/accuracy': 0.6180599927902222, 'validation/loss': 1.8183931112289429, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.447704315185547, 'test/num_examples': 10000, 'score': 15579.493244409561, 'total_duration': 16880.903116226196, 'accumulated_submission_time': 15579.493244409561, 'accumulated_eval_time': 1298.4260349273682, 'accumulated_logging_time': 1.1072144508361816}
I0131 23:51:19.700226 139923852027648 logging_writer.py:48] [33701] accumulated_eval_time=1298.426035, accumulated_logging_time=1.107214, accumulated_submission_time=15579.493244, global_step=33701, preemption_count=0, score=15579.493244, test/accuracy=0.495800, test/loss=2.447704, test/num_examples=10000, total_duration=16880.903116, train/accuracy=0.691133, train/loss=1.493398, validation/accuracy=0.618060, validation/loss=1.818393, validation/num_examples=50000
I0131 23:52:01.075649 139923868813056 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.0758538246154785, loss=3.7319653034210205
I0131 23:52:47.429622 139923852027648 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9235854744911194, loss=5.036478042602539
I0131 23:53:34.265355 139923868813056 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.0246673822402954, loss=3.687303304672241
I0131 23:54:21.074473 139923852027648 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9593179225921631, loss=5.291744232177734
I0131 23:55:07.875913 139923868813056 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.7627885341644287, loss=5.556616306304932
I0131 23:55:54.459982 139923852027648 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.9958556890487671, loss=3.661280393600464
I0131 23:56:41.099738 139923868813056 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.8609826564788818, loss=4.188328266143799
I0131 23:57:27.665183 139923852027648 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.8877755403518677, loss=4.165369510650635
I0131 23:58:14.154460 139923868813056 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.9594668745994568, loss=3.8510055541992188
I0131 23:58:19.836890 140085747812160 spec.py:321] Evaluating on the training split.
I0131 23:58:30.137908 140085747812160 spec.py:333] Evaluating on the validation split.
I0131 23:58:51.471274 140085747812160 spec.py:349] Evaluating on the test split.
I0131 23:58:53.116653 140085747812160 submission_runner.py:408] Time since start: 17334.34s, 	Step: 34614, 	{'train/accuracy': 0.6721093654632568, 'train/loss': 1.5771911144256592, 'validation/accuracy': 0.619219958782196, 'validation/loss': 1.7937535047531128, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.422393560409546, 'test/num_examples': 10000, 'score': 15999.567883491516, 'total_duration': 17334.343224287033, 'accumulated_submission_time': 15999.567883491516, 'accumulated_eval_time': 1331.7058236598969, 'accumulated_logging_time': 1.141645908355713}
I0131 23:58:53.140068 139923852027648 logging_writer.py:48] [34614] accumulated_eval_time=1331.705824, accumulated_logging_time=1.141646, accumulated_submission_time=15999.567883, global_step=34614, preemption_count=0, score=15999.567883, test/accuracy=0.502000, test/loss=2.422394, test/num_examples=10000, total_duration=17334.343224, train/accuracy=0.672109, train/loss=1.577191, validation/accuracy=0.619220, validation/loss=1.793754, validation/num_examples=50000
I0131 23:59:28.457826 139923868813056 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.07610023021698, loss=3.7504193782806396
I0201 00:00:15.309839 139923852027648 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9341452717781067, loss=3.5889906883239746
I0201 00:01:02.366739 139923868813056 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8938735127449036, loss=5.386559963226318
I0201 00:01:48.839184 139923852027648 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.0596811771392822, loss=3.707366466522217
I0201 00:02:35.816342 139923868813056 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.9929145574569702, loss=3.6550452709198
I0201 00:03:22.484855 139923852027648 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.9308388233184814, loss=5.534399509429932
I0201 00:04:09.137448 139923868813056 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7548452615737915, loss=5.325717449188232
I0201 00:04:55.858906 139923852027648 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.8808533549308777, loss=4.097283840179443
I0201 00:05:42.595159 139923868813056 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.9268743395805359, loss=3.6551785469055176
I0201 00:05:53.505511 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:06:05.220444 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:06:25.697774 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:06:27.345717 140085747812160 submission_runner.py:408] Time since start: 17788.57s, 	Step: 35525, 	{'train/accuracy': 0.6826562285423279, 'train/loss': 1.5192320346832275, 'validation/accuracy': 0.6253799796104431, 'validation/loss': 1.7672829627990723, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3795220851898193, 'test/num_examples': 10000, 'score': 16419.87308859825, 'total_duration': 17788.572286605835, 'accumulated_submission_time': 16419.87308859825, 'accumulated_eval_time': 1365.5460460186005, 'accumulated_logging_time': 1.1748707294464111}
I0201 00:06:27.369597 139923852027648 logging_writer.py:48] [35525] accumulated_eval_time=1365.546046, accumulated_logging_time=1.174871, accumulated_submission_time=16419.873089, global_step=35525, preemption_count=0, score=16419.873089, test/accuracy=0.506600, test/loss=2.379522, test/num_examples=10000, total_duration=17788.572287, train/accuracy=0.682656, train/loss=1.519232, validation/accuracy=0.625380, validation/loss=1.767283, validation/num_examples=50000
I0201 00:06:57.370259 139923868813056 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.0123594999313354, loss=3.75484299659729
I0201 00:07:43.886583 139923852027648 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0096056461334229, loss=3.6472506523132324
I0201 00:08:30.528428 139923868813056 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.8430052995681763, loss=4.688070774078369
I0201 00:09:17.020576 139923852027648 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.8970450162887573, loss=3.9575605392456055
I0201 00:10:03.708175 139923868813056 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9465888142585754, loss=4.36347770690918
I0201 00:10:50.481954 139923852027648 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.05048668384552, loss=3.6839234828948975
I0201 00:11:37.022963 139923868813056 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9573183655738831, loss=3.700464963912964
I0201 00:12:23.837244 139923852027648 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9030246734619141, loss=4.913323402404785
I0201 00:13:10.700007 139923868813056 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.065118670463562, loss=3.6768627166748047
I0201 00:13:27.515556 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:13:37.897502 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:13:59.157818 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:14:00.798888 140085747812160 submission_runner.py:408] Time since start: 18242.03s, 	Step: 36438, 	{'train/accuracy': 0.6946093440055847, 'train/loss': 1.4783991575241089, 'validation/accuracy': 0.6278199553489685, 'validation/loss': 1.7693872451782227, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.390291929244995, 'test/num_examples': 10000, 'score': 16839.959003686905, 'total_duration': 18242.025453329086, 'accumulated_submission_time': 16839.959003686905, 'accumulated_eval_time': 1398.829402923584, 'accumulated_logging_time': 1.20875883102417}
I0201 00:14:00.819937 139923852027648 logging_writer.py:48] [36438] accumulated_eval_time=1398.829403, accumulated_logging_time=1.208759, accumulated_submission_time=16839.959004, global_step=36438, preemption_count=0, score=16839.959004, test/accuracy=0.505100, test/loss=2.390292, test/num_examples=10000, total_duration=18242.025453, train/accuracy=0.694609, train/loss=1.478399, validation/accuracy=0.627820, validation/loss=1.769387, validation/num_examples=50000
I0201 00:14:25.554083 139923868813056 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0388891696929932, loss=3.7558417320251465
I0201 00:15:11.374999 139923852027648 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.9899624586105347, loss=3.692206382751465
I0201 00:15:58.212672 139923868813056 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.1093130111694336, loss=3.6278810501098633
I0201 00:16:44.669541 139923852027648 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1273611783981323, loss=3.7496774196624756
I0201 00:17:31.534754 139923868813056 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.0080536603927612, loss=3.7489171028137207
I0201 00:18:18.102261 139923852027648 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8785480260848999, loss=5.070827007293701
I0201 00:19:04.663883 139923868813056 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.053398847579956, loss=3.754713773727417
I0201 00:19:51.497652 139923852027648 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.9010430574417114, loss=4.963247776031494
I0201 00:20:37.916553 139923868813056 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.0133222341537476, loss=3.860938549041748
I0201 00:21:00.966074 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:21:11.237772 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:21:32.649492 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:21:34.290451 140085747812160 submission_runner.py:408] Time since start: 18695.52s, 	Step: 37351, 	{'train/accuracy': 0.6821093559265137, 'train/loss': 1.5482853651046753, 'validation/accuracy': 0.6320799589157104, 'validation/loss': 1.7631151676177979, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.3797619342803955, 'test/num_examples': 10000, 'score': 17260.044524669647, 'total_duration': 18695.517024755478, 'accumulated_submission_time': 17260.044524669647, 'accumulated_eval_time': 1432.1537964344025, 'accumulated_logging_time': 1.2387840747833252}
I0201 00:21:34.315121 139923852027648 logging_writer.py:48] [37351] accumulated_eval_time=1432.153796, accumulated_logging_time=1.238784, accumulated_submission_time=17260.044525, global_step=37351, preemption_count=0, score=17260.044525, test/accuracy=0.512200, test/loss=2.379762, test/num_examples=10000, total_duration=18695.517025, train/accuracy=0.682109, train/loss=1.548285, validation/accuracy=0.632080, validation/loss=1.763115, validation/num_examples=50000
I0201 00:21:53.948539 139923868813056 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.9475668668746948, loss=3.5933964252471924
I0201 00:22:39.010096 139923852027648 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9909564852714539, loss=3.6750688552856445
I0201 00:23:25.752754 139923868813056 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.9341703057289124, loss=4.786301612854004
I0201 00:24:12.408632 139923852027648 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1346734762191772, loss=3.605875253677368
I0201 00:24:58.735041 139923868813056 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.0069202184677124, loss=3.6857571601867676
I0201 00:25:45.402037 139923852027648 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.8315594792366028, loss=5.299810886383057
I0201 00:26:32.201676 139923868813056 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9230268597602844, loss=4.0162858963012695
I0201 00:27:18.924571 139923852027648 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0525450706481934, loss=3.6508917808532715
I0201 00:28:05.564642 139923868813056 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.0846260786056519, loss=3.750171422958374
I0201 00:28:34.614153 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:28:45.131393 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:29:05.981785 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:29:07.621333 140085747812160 submission_runner.py:408] Time since start: 19148.85s, 	Step: 38264, 	{'train/accuracy': 0.6850000023841858, 'train/loss': 1.5315663814544678, 'validation/accuracy': 0.6313199996948242, 'validation/loss': 1.7667924165725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.3829877376556396, 'test/num_examples': 10000, 'score': 17680.28241252899, 'total_duration': 19148.84790277481, 'accumulated_submission_time': 17680.28241252899, 'accumulated_eval_time': 1465.1609783172607, 'accumulated_logging_time': 1.273481845855713}
I0201 00:29:07.646770 139923852027648 logging_writer.py:48] [38264] accumulated_eval_time=1465.160978, accumulated_logging_time=1.273482, accumulated_submission_time=17680.282413, global_step=38264, preemption_count=0, score=17680.282413, test/accuracy=0.514100, test/loss=2.382988, test/num_examples=10000, total_duration=19148.847903, train/accuracy=0.685000, train/loss=1.531566, validation/accuracy=0.631320, validation/loss=1.766792, validation/num_examples=50000
I0201 00:29:22.181538 139923868813056 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.0115418434143066, loss=5.517640113830566
I0201 00:30:06.211986 139923852027648 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.0805675983428955, loss=3.683488130569458
I0201 00:30:52.934755 139923868813056 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.1932398080825806, loss=3.727485418319702
I0201 00:31:39.760265 139923852027648 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.7911324501037598, loss=5.13927698135376
I0201 00:32:26.784449 139923868813056 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.9760189056396484, loss=3.953960418701172
I0201 00:33:13.661001 139923852027648 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.8657336831092834, loss=4.747757434844971
I0201 00:33:59.962557 139923868813056 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.8513439893722534, loss=4.925381183624268
I0201 00:34:46.705811 139923852027648 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.2731341123580933, loss=3.6379547119140625
I0201 00:35:33.058160 139923868813056 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.9459152817726135, loss=4.3488383293151855
I0201 00:36:07.835216 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:36:18.100546 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:36:39.458951 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:36:41.108015 140085747812160 submission_runner.py:408] Time since start: 19602.33s, 	Step: 39176, 	{'train/accuracy': 0.6991210579872131, 'train/loss': 1.461413025856018, 'validation/accuracy': 0.6354999542236328, 'validation/loss': 1.740901231765747, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.35638165473938, 'test/num_examples': 10000, 'score': 18100.411046266556, 'total_duration': 19602.334558963776, 'accumulated_submission_time': 18100.411046266556, 'accumulated_eval_time': 1498.4337601661682, 'accumulated_logging_time': 1.3083534240722656}
I0201 00:36:41.131634 139923852027648 logging_writer.py:48] [39176] accumulated_eval_time=1498.433760, accumulated_logging_time=1.308353, accumulated_submission_time=18100.411046, global_step=39176, preemption_count=0, score=18100.411046, test/accuracy=0.521300, test/loss=2.356382, test/num_examples=10000, total_duration=19602.334559, train/accuracy=0.699121, train/loss=1.461413, validation/accuracy=0.635500, validation/loss=1.740901, validation/num_examples=50000
I0201 00:36:50.955943 139923868813056 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.8900293707847595, loss=5.489908695220947
I0201 00:37:34.182732 139923852027648 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.0357648134231567, loss=3.8032538890838623
I0201 00:38:20.753334 139923868813056 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.9436845183372498, loss=5.435915946960449
I0201 00:39:07.388207 139923852027648 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8819463849067688, loss=5.5546674728393555
I0201 00:39:54.082382 139923868813056 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.0008479356765747, loss=3.7093634605407715
I0201 00:40:40.725773 139923852027648 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.0692578554153442, loss=3.665668487548828
I0201 00:41:27.499407 139923868813056 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.8959252238273621, loss=4.8361358642578125
I0201 00:42:14.122586 139923852027648 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9240336418151855, loss=4.259947299957275
I0201 00:43:00.762711 139923868813056 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.0965062379837036, loss=3.843132495880127
I0201 00:43:41.113461 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:43:51.436982 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:44:12.604775 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:44:14.244949 140085747812160 submission_runner.py:408] Time since start: 20055.47s, 	Step: 40088, 	{'train/accuracy': 0.6839648485183716, 'train/loss': 1.4925146102905273, 'validation/accuracy': 0.6367599964141846, 'validation/loss': 1.7041206359863281, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.318955183029175, 'test/num_examples': 10000, 'score': 18520.329745292664, 'total_duration': 20055.471518993378, 'accumulated_submission_time': 18520.329745292664, 'accumulated_eval_time': 1531.565257549286, 'accumulated_logging_time': 1.3446745872497559}
I0201 00:44:14.267450 139923852027648 logging_writer.py:48] [40088] accumulated_eval_time=1531.565258, accumulated_logging_time=1.344675, accumulated_submission_time=18520.329745, global_step=40088, preemption_count=0, score=18520.329745, test/accuracy=0.517000, test/loss=2.318955, test/num_examples=10000, total_duration=20055.471519, train/accuracy=0.683965, train/loss=1.492515, validation/accuracy=0.636760, validation/loss=1.704121, validation/num_examples=50000
I0201 00:44:19.491204 139923868813056 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0197945833206177, loss=3.745506763458252
I0201 00:45:01.493289 139923852027648 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.8550381660461426, loss=5.4571146965026855
I0201 00:45:48.024589 139923868813056 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.0177406072616577, loss=3.6488800048828125
I0201 00:46:34.715167 139923852027648 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.0387980937957764, loss=3.6002182960510254
I0201 00:47:21.287385 139923868813056 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0293076038360596, loss=3.9433600902557373
I0201 00:48:08.033431 139923852027648 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.085112452507019, loss=3.6704094409942627
I0201 00:48:54.559248 139923868813056 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.9360510110855103, loss=4.339493751525879
I0201 00:49:41.050717 139923852027648 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.130588173866272, loss=3.5601067543029785
I0201 00:50:27.791241 139923868813056 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.089051604270935, loss=3.7843563556671143
I0201 00:51:14.703250 139923852027648 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0390896797180176, loss=3.675615072250366
I0201 00:51:14.717331 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:51:25.193906 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:51:46.600521 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:51:48.240830 140085747812160 submission_runner.py:408] Time since start: 20509.47s, 	Step: 41001, 	{'train/accuracy': 0.6875976324081421, 'train/loss': 1.500531792640686, 'validation/accuracy': 0.6361799836158752, 'validation/loss': 1.7311961650848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.3340766429901123, 'test/num_examples': 10000, 'score': 18940.719674110413, 'total_duration': 20509.467386484146, 'accumulated_submission_time': 18940.719674110413, 'accumulated_eval_time': 1565.088744878769, 'accumulated_logging_time': 1.376600980758667}
I0201 00:51:48.265815 139923868813056 logging_writer.py:48] [41001] accumulated_eval_time=1565.088745, accumulated_logging_time=1.376601, accumulated_submission_time=18940.719674, global_step=41001, preemption_count=0, score=18940.719674, test/accuracy=0.517400, test/loss=2.334077, test/num_examples=10000, total_duration=20509.467386, train/accuracy=0.687598, train/loss=1.500532, validation/accuracy=0.636180, validation/loss=1.731196, validation/num_examples=50000
I0201 00:52:29.722242 139923852027648 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.004337191581726, loss=3.696960210800171
I0201 00:53:16.405663 139923868813056 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.9475316405296326, loss=3.840610980987549
I0201 00:54:03.430524 139923852027648 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.8298230767250061, loss=5.301342010498047
I0201 00:54:49.655758 139923868813056 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.0010569095611572, loss=3.55549693107605
I0201 00:55:36.574516 139923852027648 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9042817950248718, loss=4.972782135009766
I0201 00:56:23.291733 139923868813056 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0331782102584839, loss=3.7320899963378906
I0201 00:57:10.356164 139923852027648 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9311221837997437, loss=4.668421268463135
I0201 00:57:57.165508 139923868813056 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1184709072113037, loss=3.7079784870147705
I0201 00:58:43.785174 139923852027648 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9763787388801575, loss=3.913113594055176
I0201 00:58:48.579916 140085747812160 spec.py:321] Evaluating on the training split.
I0201 00:58:58.999777 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 00:59:19.233884 140085747812160 spec.py:349] Evaluating on the test split.
I0201 00:59:20.882657 140085747812160 submission_runner.py:408] Time since start: 20962.11s, 	Step: 41912, 	{'train/accuracy': 0.7040234208106995, 'train/loss': 1.4264371395111084, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.6851632595062256, 'validation/num_examples': 50000, 'test/accuracy': 0.5248000025749207, 'test/loss': 2.2989399433135986, 'test/num_examples': 10000, 'score': 19360.97429251671, 'total_duration': 20962.109224796295, 'accumulated_submission_time': 19360.97429251671, 'accumulated_eval_time': 1597.391491651535, 'accumulated_logging_time': 1.4102954864501953}
I0201 00:59:20.908287 139923868813056 logging_writer.py:48] [41912] accumulated_eval_time=1597.391492, accumulated_logging_time=1.410295, accumulated_submission_time=19360.974293, global_step=41912, preemption_count=0, score=19360.974293, test/accuracy=0.524800, test/loss=2.298940, test/num_examples=10000, total_duration=20962.109225, train/accuracy=0.704023, train/loss=1.426437, validation/accuracy=0.645840, validation/loss=1.685163, validation/num_examples=50000
I0201 00:59:57.506240 139923852027648 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0116541385650635, loss=3.5914993286132812
I0201 01:00:44.175108 139923868813056 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.0913081169128418, loss=3.7327170372009277
I0201 01:01:31.052314 139923852027648 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1088534593582153, loss=3.6779768466949463
I0201 01:02:17.850739 139923868813056 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.9255049228668213, loss=5.063106060028076
I0201 01:03:05.036856 139923852027648 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.9333391189575195, loss=4.186742782592773
I0201 01:03:51.898408 139923868813056 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9924650192260742, loss=3.592437744140625
I0201 01:04:38.883803 139923852027648 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0687919855117798, loss=3.5833919048309326
I0201 01:05:25.410818 139923868813056 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.9538909792900085, loss=4.681044101715088
I0201 01:06:12.307043 139923852027648 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.9038945436477661, loss=4.829797267913818
I0201 01:06:21.333931 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:06:31.478190 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:06:53.710560 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:06:55.350277 140085747812160 submission_runner.py:408] Time since start: 21416.58s, 	Step: 42821, 	{'train/accuracy': 0.6966992020606995, 'train/loss': 1.4827358722686768, 'validation/accuracy': 0.6440399885177612, 'validation/loss': 1.7102241516113281, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.3438947200775146, 'test/num_examples': 10000, 'score': 19781.339330911636, 'total_duration': 21416.576837062836, 'accumulated_submission_time': 19781.339330911636, 'accumulated_eval_time': 1631.4078161716461, 'accumulated_logging_time': 1.445805549621582}
I0201 01:06:55.375730 139923868813056 logging_writer.py:48] [42821] accumulated_eval_time=1631.407816, accumulated_logging_time=1.445806, accumulated_submission_time=19781.339331, global_step=42821, preemption_count=0, score=19781.339331, test/accuracy=0.521600, test/loss=2.343895, test/num_examples=10000, total_duration=21416.576837, train/accuracy=0.696699, train/loss=1.482736, validation/accuracy=0.644040, validation/loss=1.710224, validation/num_examples=50000
I0201 01:07:27.272688 139923852027648 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.0605546236038208, loss=3.6928930282592773
I0201 01:08:13.796348 139923868813056 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0262672901153564, loss=3.5667641162872314
I0201 01:09:00.653454 139923852027648 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.9829999804496765, loss=3.568571090698242
I0201 01:09:47.191318 139923868813056 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9332577586174011, loss=3.992379903793335
I0201 01:10:33.851803 139923852027648 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.9969459772109985, loss=3.6540732383728027
I0201 01:11:20.529413 139923868813056 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0405815839767456, loss=3.5828518867492676
I0201 01:12:07.312714 139923852027648 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.089954137802124, loss=3.658329963684082
I0201 01:12:54.048220 139923868813056 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.9865573644638062, loss=3.6448652744293213
I0201 01:13:40.708962 139923852027648 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.8667638897895813, loss=4.963787078857422
I0201 01:13:55.381619 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:14:05.749468 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:14:27.415690 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:14:29.056572 140085747812160 submission_runner.py:408] Time since start: 21870.28s, 	Step: 43733, 	{'train/accuracy': 0.6943163871765137, 'train/loss': 1.5012829303741455, 'validation/accuracy': 0.6460599899291992, 'validation/loss': 1.7175486087799072, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.349139451980591, 'test/num_examples': 10000, 'score': 20201.28215122223, 'total_duration': 21870.283143281937, 'accumulated_submission_time': 20201.28215122223, 'accumulated_eval_time': 1665.082776069641, 'accumulated_logging_time': 1.4831054210662842}
I0201 01:14:29.084086 139923868813056 logging_writer.py:48] [43733] accumulated_eval_time=1665.082776, accumulated_logging_time=1.483105, accumulated_submission_time=20201.282151, global_step=43733, preemption_count=0, score=20201.282151, test/accuracy=0.519100, test/loss=2.349139, test/num_examples=10000, total_duration=21870.283143, train/accuracy=0.694316, train/loss=1.501283, validation/accuracy=0.646060, validation/loss=1.717549, validation/num_examples=50000
I0201 01:14:55.792494 139923852027648 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0099414587020874, loss=3.661553382873535
I0201 01:15:41.759194 139923868813056 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0399079322814941, loss=3.6536002159118652
I0201 01:16:28.443889 139923852027648 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.013124704360962, loss=3.648080348968506
I0201 01:17:15.357046 139923868813056 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.9442151784896851, loss=4.861200332641602
I0201 01:18:02.116087 139923852027648 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.043667197227478, loss=3.6021127700805664
I0201 01:18:48.770375 139923868813056 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1916285753250122, loss=3.634911060333252
I0201 01:19:35.405708 139923852027648 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.9490848183631897, loss=4.584207057952881
I0201 01:20:22.050066 139923868813056 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.139620065689087, loss=3.5731594562530518
I0201 01:21:08.777291 139923852027648 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1477675437927246, loss=3.6694693565368652
I0201 01:21:29.467941 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:21:39.897683 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:21:59.351552 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:22:00.996209 140085747812160 submission_runner.py:408] Time since start: 22322.22s, 	Step: 44646, 	{'train/accuracy': 0.70361328125, 'train/loss': 1.4847627878189087, 'validation/accuracy': 0.643559992313385, 'validation/loss': 1.7350969314575195, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.3520002365112305, 'test/num_examples': 10000, 'score': 20621.605316638947, 'total_duration': 22322.22277355194, 'accumulated_submission_time': 20621.605316638947, 'accumulated_eval_time': 1696.6110389232635, 'accumulated_logging_time': 1.5203256607055664}
I0201 01:22:01.022080 139923868813056 logging_writer.py:48] [44646] accumulated_eval_time=1696.611039, accumulated_logging_time=1.520326, accumulated_submission_time=20621.605317, global_step=44646, preemption_count=0, score=20621.605317, test/accuracy=0.524200, test/loss=2.352000, test/num_examples=10000, total_duration=22322.222774, train/accuracy=0.703613, train/loss=1.484763, validation/accuracy=0.643560, validation/loss=1.735097, validation/num_examples=50000
I0201 01:22:22.626259 139923852027648 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0323984622955322, loss=4.0912699699401855
I0201 01:23:08.290354 139923868813056 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.9900128245353699, loss=4.814452648162842
I0201 01:23:55.241140 139923852027648 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1752156019210815, loss=3.579792022705078
I0201 01:24:41.618214 139923868813056 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8623014688491821, loss=5.031249523162842
I0201 01:25:28.539533 139923852027648 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.9489952921867371, loss=4.865878105163574
I0201 01:26:15.180393 139923868813056 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.9446361064910889, loss=5.467241287231445
I0201 01:27:01.802709 139923852027648 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.046435832977295, loss=3.700840950012207
I0201 01:27:48.615050 139923868813056 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.9808840751647949, loss=3.629509925842285
I0201 01:28:35.321229 139923852027648 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0933804512023926, loss=3.56864333152771
I0201 01:29:01.098948 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:29:11.489705 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:29:33.758754 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:29:35.410414 140085747812160 submission_runner.py:408] Time since start: 22776.64s, 	Step: 45557, 	{'train/accuracy': 0.7167187333106995, 'train/loss': 1.3568024635314941, 'validation/accuracy': 0.6495999693870544, 'validation/loss': 1.6522201299667358, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.260016918182373, 'test/num_examples': 10000, 'score': 21041.622383117676, 'total_duration': 22776.63697385788, 'accumulated_submission_time': 21041.622383117676, 'accumulated_eval_time': 1730.922518491745, 'accumulated_logging_time': 1.5553491115570068}
I0201 01:29:35.436242 139923868813056 logging_writer.py:48] [45557] accumulated_eval_time=1730.922518, accumulated_logging_time=1.555349, accumulated_submission_time=21041.622383, global_step=45557, preemption_count=0, score=21041.622383, test/accuracy=0.533300, test/loss=2.260017, test/num_examples=10000, total_duration=22776.636974, train/accuracy=0.716719, train/loss=1.356802, validation/accuracy=0.649600, validation/loss=1.652220, validation/num_examples=50000
I0201 01:29:52.715492 139923852027648 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.0547655820846558, loss=3.6016876697540283
I0201 01:30:37.044353 139923868813056 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.8921203017234802, loss=5.325369834899902
I0201 01:31:23.920797 139923852027648 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.9090803265571594, loss=4.91289758682251
I0201 01:32:10.627063 139923868813056 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0196489095687866, loss=3.6472907066345215
I0201 01:32:57.523405 139923852027648 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9401466846466064, loss=5.0681962966918945
I0201 01:33:44.334053 139923868813056 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.0825886726379395, loss=3.5986483097076416
I0201 01:34:31.057566 139923852027648 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.0982979536056519, loss=3.5583207607269287
I0201 01:35:18.156955 139923868813056 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0119085311889648, loss=4.14933443069458
I0201 01:36:04.545328 139923852027648 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.0691149234771729, loss=3.5460495948791504
I0201 01:36:35.516175 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:36:45.886680 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:37:08.517187 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:37:10.159296 140085747812160 submission_runner.py:408] Time since start: 23231.39s, 	Step: 46468, 	{'train/accuracy': 0.6974999904632568, 'train/loss': 1.5168081521987915, 'validation/accuracy': 0.6491599678993225, 'validation/loss': 1.7391897439956665, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.357267379760742, 'test/num_examples': 10000, 'score': 21461.639271736145, 'total_duration': 23231.38586783409, 'accumulated_submission_time': 21461.639271736145, 'accumulated_eval_time': 1765.5656650066376, 'accumulated_logging_time': 1.5932340621948242}
I0201 01:37:10.185342 139923868813056 logging_writer.py:48] [46468] accumulated_eval_time=1765.565665, accumulated_logging_time=1.593234, accumulated_submission_time=21461.639272, global_step=46468, preemption_count=0, score=21461.639272, test/accuracy=0.525500, test/loss=2.357267, test/num_examples=10000, total_duration=23231.385868, train/accuracy=0.697500, train/loss=1.516808, validation/accuracy=0.649160, validation/loss=1.739190, validation/num_examples=50000
I0201 01:37:23.159057 139923852027648 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0022127628326416, loss=3.688054084777832
I0201 01:38:06.683333 139923868813056 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.0292601585388184, loss=3.502934217453003
I0201 01:38:53.389939 139923852027648 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.0732576847076416, loss=3.6279635429382324
I0201 01:39:40.130716 139923868813056 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.9197941422462463, loss=5.241669654846191
I0201 01:40:26.745587 139923852027648 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.023914098739624, loss=4.680390357971191
I0201 01:41:13.431449 139923868813056 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9787611961364746, loss=4.559790134429932
I0201 01:41:59.878406 139923852027648 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.0900256633758545, loss=3.6147682666778564
I0201 01:42:46.605778 139923868813056 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.0320812463760376, loss=3.5405566692352295
I0201 01:43:33.320717 139923852027648 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.8967351913452148, loss=4.894179344177246
I0201 01:44:10.409591 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:44:20.956298 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:44:41.655740 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:44:43.297079 140085747812160 submission_runner.py:408] Time since start: 23684.52s, 	Step: 47381, 	{'train/accuracy': 0.7109375, 'train/loss': 1.4334065914154053, 'validation/accuracy': 0.6492399573326111, 'validation/loss': 1.697941541671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.307551622390747, 'test/num_examples': 10000, 'score': 21881.792940855026, 'total_duration': 23684.5236389637, 'accumulated_submission_time': 21881.792940855026, 'accumulated_eval_time': 1798.4531662464142, 'accumulated_logging_time': 1.6383380889892578}
I0201 01:44:43.326543 139923868813056 logging_writer.py:48] [47381] accumulated_eval_time=1798.453166, accumulated_logging_time=1.638338, accumulated_submission_time=21881.792941, global_step=47381, preemption_count=0, score=21881.792941, test/accuracy=0.531800, test/loss=2.307552, test/num_examples=10000, total_duration=23684.523639, train/accuracy=0.710938, train/loss=1.433407, validation/accuracy=0.649240, validation/loss=1.697942, validation/num_examples=50000
I0201 01:44:51.185229 139923852027648 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.891407310962677, loss=5.424881935119629
I0201 01:45:33.834287 139923868813056 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.022572636604309, loss=3.715623617172241
I0201 01:46:20.673213 139923852027648 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.0446572303771973, loss=3.640310287475586
I0201 01:47:07.521541 139923868813056 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9803707599639893, loss=4.417346954345703
I0201 01:47:53.978314 139923852027648 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9974958300590515, loss=4.221231460571289
I0201 01:48:40.903223 139923868813056 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.139143943786621, loss=3.589189291000366
I0201 01:49:27.463292 139923852027648 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0054938793182373, loss=4.251392364501953
I0201 01:50:14.429127 139923868813056 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.1407785415649414, loss=3.634711503982544
I0201 01:51:00.889146 139923852027648 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0222266912460327, loss=3.734618663787842
I0201 01:51:43.584506 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:51:54.097571 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:52:15.463356 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:52:17.109244 140085747812160 submission_runner.py:408] Time since start: 24138.34s, 	Step: 48293, 	{'train/accuracy': 0.73046875, 'train/loss': 1.3111317157745361, 'validation/accuracy': 0.6547600030899048, 'validation/loss': 1.646582007408142, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.266916036605835, 'test/num_examples': 10000, 'score': 22301.990122556686, 'total_duration': 24138.335812330246, 'accumulated_submission_time': 22301.990122556686, 'accumulated_eval_time': 1831.9779014587402, 'accumulated_logging_time': 1.6770923137664795}
I0201 01:52:17.133950 139923868813056 logging_writer.py:48] [48293] accumulated_eval_time=1831.977901, accumulated_logging_time=1.677092, accumulated_submission_time=22301.990123, global_step=48293, preemption_count=0, score=22301.990123, test/accuracy=0.533500, test/loss=2.266916, test/num_examples=10000, total_duration=24138.335812, train/accuracy=0.730469, train/loss=1.311132, validation/accuracy=0.654760, validation/loss=1.646582, validation/num_examples=50000
I0201 01:52:20.282151 139923852027648 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.0090398788452148, loss=3.6921772956848145
I0201 01:53:02.334322 139923868813056 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1182870864868164, loss=3.611234664916992
I0201 01:53:49.156669 139923852027648 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0720455646514893, loss=3.4726181030273438
I0201 01:54:35.989598 139923868813056 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.9840437173843384, loss=3.5828280448913574
I0201 01:55:22.382389 139923852027648 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.0754938125610352, loss=3.560687303543091
I0201 01:56:09.141089 139923868813056 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.0619362592697144, loss=3.615339994430542
I0201 01:56:55.422775 139923852027648 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.8897755146026611, loss=4.1694231033325195
I0201 01:57:42.249410 139923868813056 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9948543906211853, loss=4.601507186889648
I0201 01:58:28.758313 139923852027648 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.0288066864013672, loss=3.498011589050293
I0201 01:59:15.293590 139923868813056 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.0428673028945923, loss=3.5967977046966553
I0201 01:59:17.244548 140085747812160 spec.py:321] Evaluating on the training split.
I0201 01:59:27.507529 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 01:59:49.334841 140085747812160 spec.py:349] Evaluating on the test split.
I0201 01:59:50.976409 140085747812160 submission_runner.py:408] Time since start: 24592.20s, 	Step: 49206, 	{'train/accuracy': 0.710156261920929, 'train/loss': 1.4143155813217163, 'validation/accuracy': 0.6555399894714355, 'validation/loss': 1.6536281108856201, 'validation/num_examples': 50000, 'test/accuracy': 0.5358999967575073, 'test/loss': 2.262042999267578, 'test/num_examples': 10000, 'score': 22722.04015159607, 'total_duration': 24592.202979803085, 'accumulated_submission_time': 22722.04015159607, 'accumulated_eval_time': 1865.7097523212433, 'accumulated_logging_time': 1.711674690246582}
I0201 01:59:51.001642 139923852027648 logging_writer.py:48] [49206] accumulated_eval_time=1865.709752, accumulated_logging_time=1.711675, accumulated_submission_time=22722.040152, global_step=49206, preemption_count=0, score=22722.040152, test/accuracy=0.535900, test/loss=2.262043, test/num_examples=10000, total_duration=24592.202980, train/accuracy=0.710156, train/loss=1.414316, validation/accuracy=0.655540, validation/loss=1.653628, validation/num_examples=50000
I0201 02:00:30.343288 139923868813056 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9167611002922058, loss=4.58467960357666
I0201 02:01:17.195492 139923852027648 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.1044238805770874, loss=3.587731122970581
I0201 02:02:04.090324 139923868813056 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.0055296421051025, loss=3.4097578525543213
I0201 02:02:50.980043 139923852027648 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9773659110069275, loss=5.272256851196289
I0201 02:03:37.944422 139923868813056 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0404077768325806, loss=3.534213066101074
I0201 02:04:24.816541 139923852027648 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.9837914705276489, loss=3.4976961612701416
I0201 02:05:11.551503 139923868813056 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.9630672931671143, loss=3.6835720539093018
I0201 02:05:58.186933 139923852027648 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.0451608896255493, loss=3.5931613445281982
I0201 02:06:44.884277 139923868813056 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.9088490009307861, loss=5.268908500671387
I0201 02:06:51.120056 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:07:01.487798 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:07:24.094594 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:07:25.735624 140085747812160 submission_runner.py:408] Time since start: 25046.96s, 	Step: 50115, 	{'train/accuracy': 0.7168163657188416, 'train/loss': 1.348970651626587, 'validation/accuracy': 0.6584199666976929, 'validation/loss': 1.6088238954544067, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.217803955078125, 'test/num_examples': 10000, 'score': 23142.09687113762, 'total_duration': 25046.962195158005, 'accumulated_submission_time': 23142.09687113762, 'accumulated_eval_time': 1900.3253211975098, 'accumulated_logging_time': 1.7474756240844727}
I0201 02:07:25.761437 139923852027648 logging_writer.py:48] [50115] accumulated_eval_time=1900.325321, accumulated_logging_time=1.747476, accumulated_submission_time=23142.096871, global_step=50115, preemption_count=0, score=23142.096871, test/accuracy=0.536000, test/loss=2.217804, test/num_examples=10000, total_duration=25046.962195, train/accuracy=0.716816, train/loss=1.348971, validation/accuracy=0.658420, validation/loss=1.608824, validation/num_examples=50000
I0201 02:08:00.599254 139923868813056 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0624890327453613, loss=3.540635108947754
I0201 02:08:47.299776 139923852027648 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.1165399551391602, loss=3.8022260665893555
I0201 02:09:34.295727 139923868813056 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.0415219068527222, loss=5.353243350982666
I0201 02:10:21.039014 139923852027648 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.081547737121582, loss=3.654104709625244
I0201 02:11:07.790414 139923868813056 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1638041734695435, loss=3.566056251525879
I0201 02:11:54.538691 139923852027648 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.003100872039795, loss=4.17127799987793
I0201 02:12:41.220966 139923868813056 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1106963157653809, loss=3.4855475425720215
I0201 02:13:27.818720 139923852027648 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.210552453994751, loss=3.5198795795440674
I0201 02:14:14.450198 139923868813056 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.0746073722839355, loss=3.558408737182617
I0201 02:14:26.153071 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:14:36.611091 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:14:59.669645 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:15:01.313852 140085747812160 submission_runner.py:408] Time since start: 25502.54s, 	Step: 51027, 	{'train/accuracy': 0.7308984398841858, 'train/loss': 1.3000104427337646, 'validation/accuracy': 0.6588199734687805, 'validation/loss': 1.6060658693313599, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.230520725250244, 'test/num_examples': 10000, 'score': 23562.426542520523, 'total_duration': 25502.540413618088, 'accumulated_submission_time': 23562.426542520523, 'accumulated_eval_time': 1935.4861042499542, 'accumulated_logging_time': 1.7837295532226562}
I0201 02:15:01.337719 139923852027648 logging_writer.py:48] [51027] accumulated_eval_time=1935.486104, accumulated_logging_time=1.783730, accumulated_submission_time=23562.426543, global_step=51027, preemption_count=0, score=23562.426543, test/accuracy=0.532900, test/loss=2.230521, test/num_examples=10000, total_duration=25502.540414, train/accuracy=0.730898, train/loss=1.300010, validation/accuracy=0.658820, validation/loss=1.606066, validation/num_examples=50000
I0201 02:15:30.766743 139923868813056 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0857326984405518, loss=3.533125162124634
I0201 02:16:17.196107 139923852027648 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.9710098505020142, loss=4.852811336517334
I0201 02:17:04.189060 139923868813056 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1001557111740112, loss=3.5071535110473633
I0201 02:17:50.586915 139923852027648 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0958130359649658, loss=3.684475898742676
I0201 02:18:37.062649 139923868813056 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.096184492111206, loss=3.493799924850464
I0201 02:19:23.698304 139923852027648 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.0085684061050415, loss=5.349243640899658
I0201 02:20:10.278416 139923868813056 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.985636830329895, loss=4.1060943603515625
I0201 02:20:56.810854 139923852027648 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.9793874025344849, loss=5.076540946960449
I0201 02:21:43.350672 139923868813056 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.1387474536895752, loss=3.446784734725952
I0201 02:22:01.629003 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:22:12.303804 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:22:35.051364 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:22:36.695403 140085747812160 submission_runner.py:408] Time since start: 25957.92s, 	Step: 51941, 	{'train/accuracy': 0.71009761095047, 'train/loss': 1.3835265636444092, 'validation/accuracy': 0.6599999666213989, 'validation/loss': 1.6134287118911743, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.21612286567688, 'test/num_examples': 10000, 'score': 23982.65742635727, 'total_duration': 25957.92196583748, 'accumulated_submission_time': 23982.65742635727, 'accumulated_eval_time': 1970.5525019168854, 'accumulated_logging_time': 1.8164739608764648}
I0201 02:22:36.722032 139923852027648 logging_writer.py:48] [51941] accumulated_eval_time=1970.552502, accumulated_logging_time=1.816474, accumulated_submission_time=23982.657426, global_step=51941, preemption_count=0, score=23982.657426, test/accuracy=0.543000, test/loss=2.216123, test/num_examples=10000, total_duration=25957.921966, train/accuracy=0.710098, train/loss=1.383527, validation/accuracy=0.660000, validation/loss=1.613429, validation/num_examples=50000
I0201 02:23:00.271161 139923868813056 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.9963268637657166, loss=3.9196887016296387
I0201 02:23:45.845800 139923852027648 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9715728163719177, loss=4.90778112411499
I0201 02:24:32.835901 139923868813056 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.15234375, loss=3.468153238296509
I0201 02:25:19.684950 139923852027648 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.2597407102584839, loss=3.491121292114258
I0201 02:26:06.699983 139923868813056 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9254761338233948, loss=4.272913932800293
I0201 02:26:53.495195 139923852027648 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.0672889947891235, loss=3.8942644596099854
I0201 02:27:40.336432 139923868813056 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.071808099746704, loss=5.036097526550293
I0201 02:28:26.947711 139923852027648 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9132204651832581, loss=5.3144073486328125
I0201 02:29:13.908093 139923868813056 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.104948878288269, loss=3.7761788368225098
I0201 02:29:36.930962 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:29:47.229267 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:30:08.872322 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:30:10.514665 140085747812160 submission_runner.py:408] Time since start: 26411.74s, 	Step: 52851, 	{'train/accuracy': 0.7220507860183716, 'train/loss': 1.3369845151901245, 'validation/accuracy': 0.6620000004768372, 'validation/loss': 1.5932549238204956, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.2067039012908936, 'test/num_examples': 10000, 'score': 24402.8046002388, 'total_duration': 26411.741233587265, 'accumulated_submission_time': 24402.8046002388, 'accumulated_eval_time': 2004.1362063884735, 'accumulated_logging_time': 1.8542118072509766}
I0201 02:30:10.541664 139923852027648 logging_writer.py:48] [52851] accumulated_eval_time=2004.136206, accumulated_logging_time=1.854212, accumulated_submission_time=24402.804600, global_step=52851, preemption_count=0, score=24402.804600, test/accuracy=0.540600, test/loss=2.206704, test/num_examples=10000, total_duration=26411.741234, train/accuracy=0.722051, train/loss=1.336985, validation/accuracy=0.662000, validation/loss=1.593255, validation/num_examples=50000
I0201 02:30:30.175151 139923868813056 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.0379538536071777, loss=4.332165718078613
I0201 02:31:15.162705 139923852027648 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.076176404953003, loss=3.677469253540039
I0201 02:32:02.079360 139923868813056 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1597980260849, loss=3.6610782146453857
I0201 02:32:48.845762 139923852027648 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.9813040494918823, loss=3.630173444747925
I0201 02:33:35.451419 139923868813056 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.9935718774795532, loss=3.911980390548706
I0201 02:34:22.000023 139923852027648 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.0300885438919067, loss=3.7744741439819336
I0201 02:35:08.605026 139923868813056 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0202285051345825, loss=5.361390113830566
I0201 02:35:55.209836 139923852027648 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.9795551896095276, loss=4.501461982727051
I0201 02:36:41.822938 139923868813056 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1146425008773804, loss=3.5473740100860596
I0201 02:37:10.652049 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:37:20.981114 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:37:42.080638 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:37:43.721487 140085747812160 submission_runner.py:408] Time since start: 26864.95s, 	Step: 53763, 	{'train/accuracy': 0.7294335961341858, 'train/loss': 1.317986011505127, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.6028327941894531, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.219597339630127, 'test/num_examples': 10000, 'score': 24822.854472875595, 'total_duration': 26864.948058128357, 'accumulated_submission_time': 24822.854472875595, 'accumulated_eval_time': 2037.2056503295898, 'accumulated_logging_time': 1.8903439044952393}
I0201 02:37:43.751015 139923852027648 logging_writer.py:48] [53763] accumulated_eval_time=2037.205650, accumulated_logging_time=1.890344, accumulated_submission_time=24822.854473, global_step=53763, preemption_count=0, score=24822.854473, test/accuracy=0.542000, test/loss=2.219597, test/num_examples=10000, total_duration=26864.948058, train/accuracy=0.729434, train/loss=1.317986, validation/accuracy=0.665280, validation/loss=1.602833, validation/num_examples=50000
I0201 02:37:58.672648 139923868813056 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.9836551547050476, loss=5.155799865722656
I0201 02:38:42.521141 139923852027648 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.8931533098220825, loss=4.848276615142822
I0201 02:39:29.183585 139923868813056 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9733682870864868, loss=4.311697959899902
I0201 02:40:16.062358 139923852027648 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.047015905380249, loss=3.4994940757751465
I0201 02:41:02.556667 139923868813056 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1289966106414795, loss=3.589755058288574
I0201 02:41:49.225967 139923852027648 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0142335891723633, loss=3.8403806686401367
I0201 02:42:35.906326 139923868813056 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1494497060775757, loss=3.639469623565674
I0201 02:43:22.556781 139923852027648 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.1003228425979614, loss=3.4254279136657715
I0201 02:44:09.032439 139923868813056 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0798797607421875, loss=3.5321664810180664
I0201 02:44:44.025170 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:44:54.371033 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:45:15.917448 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:45:17.557122 140085747812160 submission_runner.py:408] Time since start: 27318.78s, 	Step: 54677, 	{'train/accuracy': 0.71937495470047, 'train/loss': 1.3365647792816162, 'validation/accuracy': 0.6654199957847595, 'validation/loss': 1.5735455751419067, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.1876070499420166, 'test/num_examples': 10000, 'score': 25243.066395998, 'total_duration': 27318.78368639946, 'accumulated_submission_time': 25243.066395998, 'accumulated_eval_time': 2070.7375979423523, 'accumulated_logging_time': 1.931077480316162}
I0201 02:45:17.586658 139923852027648 logging_writer.py:48] [54677] accumulated_eval_time=2070.737598, accumulated_logging_time=1.931077, accumulated_submission_time=25243.066396, global_step=54677, preemption_count=0, score=25243.066396, test/accuracy=0.541000, test/loss=2.187607, test/num_examples=10000, total_duration=27318.783686, train/accuracy=0.719375, train/loss=1.336565, validation/accuracy=0.665420, validation/loss=1.573546, validation/num_examples=50000
I0201 02:45:27.021097 139923868813056 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.063685417175293, loss=3.7014236450195312
I0201 02:46:10.022763 139923852027648 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0094075202941895, loss=4.075866222381592
I0201 02:46:56.603160 139923868813056 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1361334323883057, loss=3.6041152477264404
I0201 02:47:43.504045 139923852027648 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.0926507711410522, loss=3.4433109760284424
I0201 02:48:30.177576 139923868813056 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.022486925125122, loss=4.263558864593506
I0201 02:49:16.610750 139923852027648 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.106101393699646, loss=3.570418119430542
I0201 02:50:03.165728 139923868813056 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1295335292816162, loss=3.5588021278381348
I0201 02:50:49.654003 139923852027648 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2519699335098267, loss=3.5203797817230225
I0201 02:51:36.233283 139923868813056 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0254970788955688, loss=5.327284336090088
I0201 02:52:17.614562 140085747812160 spec.py:321] Evaluating on the training split.
I0201 02:52:27.836510 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 02:52:49.296395 140085747812160 spec.py:349] Evaluating on the test split.
I0201 02:52:50.942041 140085747812160 submission_runner.py:408] Time since start: 27772.17s, 	Step: 55591, 	{'train/accuracy': 0.7230077981948853, 'train/loss': 1.379983901977539, 'validation/accuracy': 0.6684600114822388, 'validation/loss': 1.6201144456863403, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.244570732116699, 'test/num_examples': 10000, 'score': 25663.03023004532, 'total_duration': 27772.168610334396, 'accumulated_submission_time': 25663.03023004532, 'accumulated_eval_time': 2104.065089225769, 'accumulated_logging_time': 1.9737062454223633}
I0201 02:52:50.970685 139923852027648 logging_writer.py:48] [55591] accumulated_eval_time=2104.065089, accumulated_logging_time=1.973706, accumulated_submission_time=25663.030230, global_step=55591, preemption_count=0, score=25663.030230, test/accuracy=0.543700, test/loss=2.244571, test/num_examples=10000, total_duration=27772.168610, train/accuracy=0.723008, train/loss=1.379984, validation/accuracy=0.668460, validation/loss=1.620114, validation/num_examples=50000
I0201 02:52:54.902772 139923868813056 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0766263008117676, loss=3.644771099090576
I0201 02:53:36.922374 139923852027648 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.0120998620986938, loss=4.545160293579102
I0201 02:54:23.296014 139923868813056 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.9533395767211914, loss=4.584000587463379
I0201 02:55:10.022534 139923852027648 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.060288906097412, loss=3.5646812915802
I0201 02:55:56.651335 139923868813056 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0343091487884521, loss=5.121752738952637
I0201 02:56:43.230303 139923852027648 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0634713172912598, loss=3.710489273071289
I0201 02:57:29.815150 139923868813056 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.0772500038146973, loss=3.6305792331695557
I0201 02:58:16.428104 139923852027648 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.006017804145813, loss=4.39202880859375
I0201 02:59:02.745177 139923868813056 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.119993805885315, loss=3.5042572021484375
I0201 02:59:49.269731 139923852027648 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1221867799758911, loss=3.963897228240967
I0201 02:59:51.245270 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:00:01.786583 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:00:25.126492 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:00:26.764966 140085747812160 submission_runner.py:408] Time since start: 28227.99s, 	Step: 56506, 	{'train/accuracy': 0.7322070002555847, 'train/loss': 1.3016204833984375, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.5884922742843628, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.196012496948242, 'test/num_examples': 10000, 'score': 26083.24350643158, 'total_duration': 28227.991536140442, 'accumulated_submission_time': 26083.24350643158, 'accumulated_eval_time': 2139.5847957134247, 'accumulated_logging_time': 2.0120248794555664}
I0201 03:00:26.791464 139923868813056 logging_writer.py:48] [56506] accumulated_eval_time=2139.584796, accumulated_logging_time=2.012025, accumulated_submission_time=26083.243506, global_step=56506, preemption_count=0, score=26083.243506, test/accuracy=0.547900, test/loss=2.196012, test/num_examples=10000, total_duration=28227.991536, train/accuracy=0.732207, train/loss=1.301620, validation/accuracy=0.669760, validation/loss=1.588492, validation/num_examples=50000
I0201 03:01:05.884236 139923852027648 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.0748863220214844, loss=3.499908208847046
I0201 03:01:52.602380 139923868813056 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.1335824728012085, loss=3.4942779541015625
I0201 03:02:39.729176 139923852027648 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1414815187454224, loss=3.4982006549835205
I0201 03:03:26.364397 139923868813056 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.1181858777999878, loss=3.4954440593719482
I0201 03:04:12.920298 139923852027648 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1541162729263306, loss=3.4520466327667236
I0201 03:04:59.432075 139923868813056 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.1590161323547363, loss=3.5669443607330322
I0201 03:05:46.132672 139923852027648 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.0678263902664185, loss=3.8138961791992188
I0201 03:06:32.704692 139923868813056 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.9972155094146729, loss=3.678506374359131
I0201 03:07:19.232227 139923852027648 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.268779993057251, loss=3.767335891723633
I0201 03:07:26.818999 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:07:37.070191 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:08:00.242577 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:08:01.914817 140085747812160 submission_runner.py:408] Time since start: 28683.14s, 	Step: 57418, 	{'train/accuracy': 0.7277148365974426, 'train/loss': 1.3060152530670166, 'validation/accuracy': 0.6685799956321716, 'validation/loss': 1.5581419467926025, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.177110433578491, 'test/num_examples': 10000, 'score': 26503.21027612686, 'total_duration': 28683.141376018524, 'accumulated_submission_time': 26503.21027612686, 'accumulated_eval_time': 2174.6806008815765, 'accumulated_logging_time': 2.048649311065674}
I0201 03:08:01.939745 139923868813056 logging_writer.py:48] [57418] accumulated_eval_time=2174.680601, accumulated_logging_time=2.048649, accumulated_submission_time=26503.210276, global_step=57418, preemption_count=0, score=26503.210276, test/accuracy=0.544900, test/loss=2.177110, test/num_examples=10000, total_duration=28683.141376, train/accuracy=0.727715, train/loss=1.306015, validation/accuracy=0.668580, validation/loss=1.558142, validation/num_examples=50000
I0201 03:08:35.454682 139923852027648 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.0983054637908936, loss=3.4324722290039062
I0201 03:09:21.752114 139923868813056 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9974094033241272, loss=4.561543941497803
I0201 03:10:08.303776 139923852027648 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.059916615486145, loss=3.7404260635375977
I0201 03:10:55.050170 139923868813056 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.201689600944519, loss=3.6535441875457764
I0201 03:11:41.976212 139923852027648 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.0753313302993774, loss=3.428563117980957
I0201 03:12:28.642103 139923868813056 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.0986064672470093, loss=5.2831807136535645
I0201 03:13:15.359863 139923852027648 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.1181434392929077, loss=5.283860683441162
I0201 03:14:02.045089 139923868813056 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.1058913469314575, loss=3.4850046634674072
I0201 03:14:48.653454 139923852027648 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0989714860916138, loss=3.559523105621338
I0201 03:15:02.354408 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:15:13.537333 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:15:35.374320 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:15:37.018493 140085747812160 submission_runner.py:408] Time since start: 29138.25s, 	Step: 58331, 	{'train/accuracy': 0.7244530916213989, 'train/loss': 1.3326257467269897, 'validation/accuracy': 0.6692999601364136, 'validation/loss': 1.571911096572876, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.1926655769348145, 'test/num_examples': 10000, 'score': 26923.5639295578, 'total_duration': 29138.245064258575, 'accumulated_submission_time': 26923.5639295578, 'accumulated_eval_time': 2209.344695329666, 'accumulated_logging_time': 2.083533763885498}
I0201 03:15:37.048706 139923868813056 logging_writer.py:48] [58331] accumulated_eval_time=2209.344695, accumulated_logging_time=2.083534, accumulated_submission_time=26923.563930, global_step=58331, preemption_count=0, score=26923.563930, test/accuracy=0.544300, test/loss=2.192666, test/num_examples=10000, total_duration=29138.245064, train/accuracy=0.724453, train/loss=1.332626, validation/accuracy=0.669300, validation/loss=1.571911, validation/num_examples=50000
I0201 03:16:04.504378 139923852027648 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9223676919937134, loss=4.779750823974609
I0201 03:16:50.732739 139923868813056 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1414965391159058, loss=3.5881714820861816
I0201 03:17:37.777341 139923852027648 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.0018149614334106, loss=5.009034156799316
I0201 03:18:24.577579 139923868813056 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2771351337432861, loss=3.4604902267456055
I0201 03:19:11.501424 139923852027648 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.0296176671981812, loss=4.927943706512451
I0201 03:19:57.797795 139923868813056 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.0401036739349365, loss=5.115907192230225
I0201 03:20:44.517215 139923852027648 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1574671268463135, loss=3.528585195541382
I0201 03:21:31.206937 139923868813056 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.180686593055725, loss=3.4190104007720947
I0201 03:22:17.974272 139923852027648 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.2004064321517944, loss=3.57875394821167
I0201 03:22:37.026018 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:22:47.368384 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:23:08.956851 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:23:10.594637 140085747812160 submission_runner.py:408] Time since start: 29591.82s, 	Step: 59242, 	{'train/accuracy': 0.7317968606948853, 'train/loss': 1.3039686679840088, 'validation/accuracy': 0.671999990940094, 'validation/loss': 1.5714185237884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.191420316696167, 'test/num_examples': 10000, 'score': 27343.48020362854, 'total_duration': 29591.821184158325, 'accumulated_submission_time': 27343.48020362854, 'accumulated_eval_time': 2242.9132976531982, 'accumulated_logging_time': 2.122434616088867}
I0201 03:23:10.618982 139923868813056 logging_writer.py:48] [59242] accumulated_eval_time=2242.913298, accumulated_logging_time=2.122435, accumulated_submission_time=27343.480204, global_step=59242, preemption_count=0, score=27343.480204, test/accuracy=0.546900, test/loss=2.191420, test/num_examples=10000, total_duration=29591.821184, train/accuracy=0.731797, train/loss=1.303969, validation/accuracy=0.672000, validation/loss=1.571419, validation/num_examples=50000
I0201 03:23:33.791360 139923852027648 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.0017262697219849, loss=4.722233772277832
I0201 03:24:19.392200 139923868813056 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.226700782775879, loss=3.51611328125
I0201 03:25:06.176525 139923852027648 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9610067009925842, loss=3.947028160095215
I0201 03:25:52.542965 139923868813056 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.2895058393478394, loss=3.5531585216522217
I0201 03:26:39.192800 139923852027648 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.086107611656189, loss=3.6449031829833984
I0201 03:27:25.656759 139923868813056 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1261663436889648, loss=5.145628929138184
I0201 03:28:12.262453 139923852027648 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2236039638519287, loss=3.5432231426239014
I0201 03:28:58.812632 139923868813056 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.012648582458496, loss=4.646271228790283
I0201 03:29:45.424309 139923852027648 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.138888955116272, loss=3.5368945598602295
I0201 03:30:10.679395 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:30:20.997038 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:30:43.901199 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:30:45.545510 140085747812160 submission_runner.py:408] Time since start: 30046.77s, 	Step: 60156, 	{'train/accuracy': 0.7542577981948853, 'train/loss': 1.2266101837158203, 'validation/accuracy': 0.671459972858429, 'validation/loss': 1.5660921335220337, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.186643362045288, 'test/num_examples': 10000, 'score': 27763.480479478836, 'total_duration': 30046.77208018303, 'accumulated_submission_time': 27763.480479478836, 'accumulated_eval_time': 2277.7794167995453, 'accumulated_logging_time': 2.156461715698242}
I0201 03:30:45.573191 139923868813056 logging_writer.py:48] [60156] accumulated_eval_time=2277.779417, accumulated_logging_time=2.156462, accumulated_submission_time=27763.480479, global_step=60156, preemption_count=0, score=27763.480479, test/accuracy=0.552800, test/loss=2.186643, test/num_examples=10000, total_duration=30046.772080, train/accuracy=0.754258, train/loss=1.226610, validation/accuracy=0.671460, validation/loss=1.566092, validation/num_examples=50000
I0201 03:31:03.376563 139923852027648 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.0511300563812256, loss=3.6792373657226562
I0201 03:31:47.649823 139923868813056 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1535613536834717, loss=3.4392592906951904
I0201 03:32:34.763835 139923852027648 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.0192530155181885, loss=3.595828056335449
I0201 03:33:21.429367 139923868813056 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.064389944076538, loss=4.0593390464782715
I0201 03:34:08.223401 139923852027648 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.1464736461639404, loss=3.5079360008239746
I0201 03:34:54.607597 139923868813056 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.051819086074829, loss=4.353288650512695
I0201 03:35:41.289773 139923852027648 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.1094321012496948, loss=3.5015251636505127
I0201 03:36:28.011362 139923868813056 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.0195207595825195, loss=5.17559814453125
I0201 03:37:14.778902 139923852027648 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0334793329238892, loss=3.926980495452881
I0201 03:37:45.815593 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:37:56.283964 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:38:17.855843 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:38:19.498044 140085747812160 submission_runner.py:408] Time since start: 30500.72s, 	Step: 61068, 	{'train/accuracy': 0.7308984398841858, 'train/loss': 1.2666162252426147, 'validation/accuracy': 0.6772399544715881, 'validation/loss': 1.4976990222930908, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.116438865661621, 'test/num_examples': 10000, 'score': 28183.66246366501, 'total_duration': 30500.72461438179, 'accumulated_submission_time': 28183.66246366501, 'accumulated_eval_time': 2311.4618620872498, 'accumulated_logging_time': 2.1936118602752686}
I0201 03:38:19.526929 139923868813056 logging_writer.py:48] [61068] accumulated_eval_time=2311.461862, accumulated_logging_time=2.193612, accumulated_submission_time=28183.662464, global_step=61068, preemption_count=0, score=28183.662464, test/accuracy=0.556500, test/loss=2.116439, test/num_examples=10000, total_duration=30500.724614, train/accuracy=0.730898, train/loss=1.266616, validation/accuracy=0.677240, validation/loss=1.497699, validation/num_examples=50000
I0201 03:38:32.491747 139923852027648 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.1020598411560059, loss=3.422868013381958
I0201 03:39:16.124183 139923868813056 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.1744219064712524, loss=3.46242618560791
I0201 03:40:03.124294 139923852027648 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.12002432346344, loss=4.899914264678955
I0201 03:40:49.798173 139923868813056 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.2328680753707886, loss=3.5369515419006348
I0201 03:41:36.068708 139923852027648 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0383375883102417, loss=3.59295654296875
I0201 03:42:22.849619 139923868813056 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.0314421653747559, loss=4.657843112945557
I0201 03:43:09.751579 139923852027648 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.0416988134384155, loss=3.5469369888305664
I0201 03:43:56.508345 139923868813056 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.173424482345581, loss=3.534461259841919
I0201 03:44:43.267408 139923852027648 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.0729918479919434, loss=3.7437782287597656
I0201 03:45:19.855427 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:45:30.390338 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:45:53.613194 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:45:55.255527 140085747812160 submission_runner.py:408] Time since start: 30956.48s, 	Step: 61980, 	{'train/accuracy': 0.73646479845047, 'train/loss': 1.2581452131271362, 'validation/accuracy': 0.6758399605751038, 'validation/loss': 1.5217695236206055, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.1416988372802734, 'test/num_examples': 10000, 'score': 28603.927173376083, 'total_duration': 30956.482098340988, 'accumulated_submission_time': 28603.927173376083, 'accumulated_eval_time': 2346.8619673252106, 'accumulated_logging_time': 2.2351157665252686}
I0201 03:45:55.285445 139923868813056 logging_writer.py:48] [61980] accumulated_eval_time=2346.861967, accumulated_logging_time=2.235116, accumulated_submission_time=28603.927173, global_step=61980, preemption_count=0, score=28603.927173, test/accuracy=0.552500, test/loss=2.141699, test/num_examples=10000, total_duration=30956.482098, train/accuracy=0.736465, train/loss=1.258145, validation/accuracy=0.675840, validation/loss=1.521770, validation/num_examples=50000
I0201 03:46:03.533983 139923852027648 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.9807679653167725, loss=4.731492519378662
I0201 03:46:46.418467 139923868813056 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.187319040298462, loss=3.5077555179595947
I0201 03:47:32.996125 139923852027648 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.0190117359161377, loss=4.85833740234375
I0201 03:48:19.749462 139923868813056 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.0305570363998413, loss=3.7592222690582275
I0201 03:49:06.437069 139923852027648 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.088850736618042, loss=3.644684314727783
I0201 03:49:53.007254 139923868813056 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0522794723510742, loss=4.081321716308594
I0201 03:50:39.663587 139923852027648 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.9907083511352539, loss=4.665809154510498
I0201 03:51:26.213120 139923868813056 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.1977342367172241, loss=3.6002321243286133
I0201 03:52:13.047416 139923852027648 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1287081241607666, loss=5.217545509338379
I0201 03:52:55.459935 140085747812160 spec.py:321] Evaluating on the training split.
I0201 03:53:05.905036 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 03:53:26.902841 140085747812160 spec.py:349] Evaluating on the test split.
I0201 03:53:28.545812 140085747812160 submission_runner.py:408] Time since start: 31409.77s, 	Step: 62892, 	{'train/accuracy': 0.7489648461341858, 'train/loss': 1.2218483686447144, 'validation/accuracy': 0.6783999800682068, 'validation/loss': 1.5327138900756836, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.162600517272949, 'test/num_examples': 10000, 'score': 29024.041388511658, 'total_duration': 31409.772384166718, 'accumulated_submission_time': 29024.041388511658, 'accumulated_eval_time': 2379.947853088379, 'accumulated_logging_time': 2.274474620819092}
I0201 03:53:28.577550 139923868813056 logging_writer.py:48] [62892] accumulated_eval_time=2379.947853, accumulated_logging_time=2.274475, accumulated_submission_time=29024.041389, global_step=62892, preemption_count=0, score=29024.041389, test/accuracy=0.555300, test/loss=2.162601, test/num_examples=10000, total_duration=31409.772384, train/accuracy=0.748965, train/loss=1.221848, validation/accuracy=0.678400, validation/loss=1.532714, validation/num_examples=50000
I0201 03:53:32.446779 139923852027648 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.096470594406128, loss=3.45043683052063
I0201 03:54:14.751739 139923868813056 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1395469903945923, loss=3.4442479610443115
I0201 03:55:01.080436 139923852027648 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.12831449508667, loss=3.3739540576934814
I0201 03:55:47.818116 139923868813056 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.081417202949524, loss=4.792149543762207
I0201 03:56:34.502640 139923852027648 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.161004662513733, loss=5.137451648712158
I0201 03:57:21.164851 139923868813056 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2195085287094116, loss=5.243382930755615
I0201 03:58:07.738806 139923852027648 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1651886701583862, loss=3.4171602725982666
I0201 03:58:54.554658 139923868813056 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.1016989946365356, loss=4.738827705383301
I0201 03:59:41.178955 139923852027648 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2691705226898193, loss=3.5314321517944336
I0201 04:00:28.114057 139923868813056 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0883243083953857, loss=3.45757794380188
I0201 04:00:28.781178 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:00:39.126049 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:00:59.398790 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:01:01.077388 140085747812160 submission_runner.py:408] Time since start: 31862.30s, 	Step: 63803, 	{'train/accuracy': 0.7317187190055847, 'train/loss': 1.283490777015686, 'validation/accuracy': 0.677899956703186, 'validation/loss': 1.5134638547897339, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.1319777965545654, 'test/num_examples': 10000, 'score': 29443.85234260559, 'total_duration': 31862.303948879242, 'accumulated_submission_time': 29443.85234260559, 'accumulated_eval_time': 2412.2440707683563, 'accumulated_logging_time': 2.648000955581665}
I0201 04:01:01.108696 139923852027648 logging_writer.py:48] [63803] accumulated_eval_time=2412.244071, accumulated_logging_time=2.648001, accumulated_submission_time=29443.852343, global_step=63803, preemption_count=0, score=29443.852343, test/accuracy=0.555600, test/loss=2.131978, test/num_examples=10000, total_duration=31862.303949, train/accuracy=0.731719, train/loss=1.283491, validation/accuracy=0.677900, validation/loss=1.513464, validation/num_examples=50000
I0201 04:01:41.146756 139923868813056 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0469319820404053, loss=4.034774303436279
I0201 04:02:27.797622 139923852027648 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1156294345855713, loss=3.514713764190674
I0201 04:03:15.035596 139923868813056 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.0877985954284668, loss=3.473004102706909
I0201 04:04:01.810043 139923852027648 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.0670024156570435, loss=4.31695032119751
I0201 04:04:48.720760 139923868813056 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.0894129276275635, loss=3.3560574054718018
I0201 04:05:35.387299 139923852027648 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.1786547899246216, loss=3.5162558555603027
I0201 04:06:21.935337 139923868813056 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1741435527801514, loss=3.4533262252807617
I0201 04:07:08.526095 139923852027648 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.1565858125686646, loss=3.5345137119293213
I0201 04:07:55.029552 139923868813056 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1094036102294922, loss=5.036419868469238
I0201 04:08:01.250147 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:08:11.942442 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:08:36.072732 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:08:37.712795 140085747812160 submission_runner.py:408] Time since start: 32318.94s, 	Step: 64715, 	{'train/accuracy': 0.740527331829071, 'train/loss': 1.269747018814087, 'validation/accuracy': 0.6794599890708923, 'validation/loss': 1.536699652671814, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.1582353115081787, 'test/num_examples': 10000, 'score': 29863.933977127075, 'total_duration': 32318.939363241196, 'accumulated_submission_time': 29863.933977127075, 'accumulated_eval_time': 2448.706707715988, 'accumulated_logging_time': 2.6889126300811768}
I0201 04:08:37.741848 139923852027648 logging_writer.py:48] [64715] accumulated_eval_time=2448.706708, accumulated_logging_time=2.688913, accumulated_submission_time=29863.933977, global_step=64715, preemption_count=0, score=29863.933977, test/accuracy=0.552000, test/loss=2.158235, test/num_examples=10000, total_duration=32318.939363, train/accuracy=0.740527, train/loss=1.269747, validation/accuracy=0.679460, validation/loss=1.536700, validation/num_examples=50000
I0201 04:09:12.797946 139923868813056 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.2649906873703003, loss=3.5198962688446045
I0201 04:09:59.535724 139923852027648 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1327009201049805, loss=3.8381826877593994
I0201 04:10:46.741020 139923868813056 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.1017671823501587, loss=3.5543644428253174
I0201 04:11:33.338673 139923852027648 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.2689487934112549, loss=3.4233455657958984
I0201 04:12:19.936302 139923868813056 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.1348295211791992, loss=3.3924143314361572
I0201 04:13:06.783546 139923852027648 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1194095611572266, loss=3.4267935752868652
I0201 04:13:53.344979 139923868813056 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1857973337173462, loss=3.4161550998687744
I0201 04:14:40.142793 139923852027648 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.316123127937317, loss=3.592210292816162
I0201 04:15:26.787079 139923868813056 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0633745193481445, loss=4.230868339538574
I0201 04:15:38.086097 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:15:48.364179 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:16:10.265708 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:16:11.904420 140085747812160 submission_runner.py:408] Time since start: 32773.13s, 	Step: 65626, 	{'train/accuracy': 0.7494140267372131, 'train/loss': 1.1988508701324463, 'validation/accuracy': 0.6782400012016296, 'validation/loss': 1.5024621486663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1161949634552, 'test/num_examples': 10000, 'score': 30284.219016075134, 'total_duration': 32773.13093471527, 'accumulated_submission_time': 30284.219016075134, 'accumulated_eval_time': 2482.5249683856964, 'accumulated_logging_time': 2.7266457080841064}
I0201 04:16:11.930818 139923852027648 logging_writer.py:48] [65626] accumulated_eval_time=2482.524968, accumulated_logging_time=2.726646, accumulated_submission_time=30284.219016, global_step=65626, preemption_count=0, score=30284.219016, test/accuracy=0.555100, test/loss=2.116195, test/num_examples=10000, total_duration=32773.130935, train/accuracy=0.749414, train/loss=1.198851, validation/accuracy=0.678240, validation/loss=1.502462, validation/num_examples=50000
I0201 04:16:41.631247 139923868813056 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2183949947357178, loss=3.759394407272339
I0201 04:17:28.100371 139923852027648 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.245424509048462, loss=3.359691858291626
I0201 04:18:14.913001 139923868813056 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.15920889377594, loss=5.224225997924805
I0201 04:19:01.583372 139923852027648 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0571073293685913, loss=3.843897581100464
I0201 04:19:48.165194 139923868813056 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.1843340396881104, loss=4.975855827331543
I0201 04:20:34.934716 139923852027648 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.0731967687606812, loss=4.6202473640441895
I0201 04:21:21.828408 139923868813056 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.049904704093933, loss=4.070725440979004
I0201 04:22:08.269219 139923852027648 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0387593507766724, loss=4.7353515625
I0201 04:22:54.989970 139923868813056 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.1227645874023438, loss=5.1661577224731445
I0201 04:23:11.909012 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:23:22.455830 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:23:44.972487 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:23:46.612493 140085747812160 submission_runner.py:408] Time since start: 33227.84s, 	Step: 66538, 	{'train/accuracy': 0.7292382717132568, 'train/loss': 1.3125627040863037, 'validation/accuracy': 0.6787999868392944, 'validation/loss': 1.5494202375411987, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.1752123832702637, 'test/num_examples': 10000, 'score': 30704.13455057144, 'total_duration': 33227.839062690735, 'accumulated_submission_time': 30704.13455057144, 'accumulated_eval_time': 2517.2284500598907, 'accumulated_logging_time': 2.763366460800171}
I0201 04:23:46.641980 139923852027648 logging_writer.py:48] [66538] accumulated_eval_time=2517.228450, accumulated_logging_time=2.763366, accumulated_submission_time=30704.134551, global_step=66538, preemption_count=0, score=30704.134551, test/accuracy=0.545500, test/loss=2.175212, test/num_examples=10000, total_duration=33227.839063, train/accuracy=0.729238, train/loss=1.312563, validation/accuracy=0.678800, validation/loss=1.549420, validation/num_examples=50000
I0201 04:24:11.363043 139923868813056 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1519086360931396, loss=3.402409553527832
I0201 04:24:57.206935 139923852027648 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.213138222694397, loss=3.444591999053955
I0201 04:25:44.310424 139923868813056 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1287224292755127, loss=3.474169969558716
I0201 04:26:30.916828 139923852027648 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1736116409301758, loss=3.4249231815338135
I0201 04:27:17.857809 139923868813056 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.291034460067749, loss=3.484152317047119
I0201 04:28:04.467668 139923852027648 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.1327911615371704, loss=3.693875551223755
I0201 04:28:51.208762 139923868813056 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.095945954322815, loss=3.640883445739746
I0201 04:29:37.730680 139923852027648 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1929131746292114, loss=3.5667781829833984
I0201 04:30:24.465085 139923868813056 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2078676223754883, loss=4.750590801239014
I0201 04:30:46.746481 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:30:57.191369 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:31:18.044475 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:31:19.692780 140085747812160 submission_runner.py:408] Time since start: 33680.92s, 	Step: 67449, 	{'train/accuracy': 0.7469726204872131, 'train/loss': 1.2171990871429443, 'validation/accuracy': 0.6873199939727783, 'validation/loss': 1.4766743183135986, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.096421718597412, 'test/num_examples': 10000, 'score': 31124.17865896225, 'total_duration': 33680.919352293015, 'accumulated_submission_time': 31124.17865896225, 'accumulated_eval_time': 2550.1747620105743, 'accumulated_logging_time': 2.8025145530700684}
I0201 04:31:19.721271 139923852027648 logging_writer.py:48] [67449] accumulated_eval_time=2550.174762, accumulated_logging_time=2.802515, accumulated_submission_time=31124.178659, global_step=67449, preemption_count=0, score=31124.178659, test/accuracy=0.558900, test/loss=2.096422, test/num_examples=10000, total_duration=33680.919352, train/accuracy=0.746973, train/loss=1.217199, validation/accuracy=0.687320, validation/loss=1.476674, validation/num_examples=50000
I0201 04:31:40.151220 139923868813056 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1424446105957031, loss=3.3975133895874023
I0201 04:32:25.393321 139923852027648 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.0531991720199585, loss=4.305850505828857
I0201 04:33:12.472046 139923868813056 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.1902292966842651, loss=3.5649573802948
I0201 04:33:59.390793 139923852027648 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2023171186447144, loss=3.4955101013183594
I0201 04:34:46.211100 139923868813056 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1083217859268188, loss=4.2431817054748535
I0201 04:35:33.007646 139923852027648 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1695207357406616, loss=3.584611415863037
I0201 04:36:19.741050 139923868813056 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2266887426376343, loss=3.419281482696533
I0201 04:37:06.425894 139923852027648 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2414271831512451, loss=3.4498772621154785
I0201 04:37:52.988162 139923868813056 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.2434217929840088, loss=4.441928863525391
I0201 04:38:19.709441 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:38:30.074050 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:38:52.484566 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:38:54.127052 140085747812160 submission_runner.py:408] Time since start: 34135.35s, 	Step: 68359, 	{'train/accuracy': 0.7477148175239563, 'train/loss': 1.2257516384124756, 'validation/accuracy': 0.6833999752998352, 'validation/loss': 1.5142306089401245, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.143460750579834, 'test/num_examples': 10000, 'score': 31544.106262922287, 'total_duration': 34135.353620529175, 'accumulated_submission_time': 31544.106262922287, 'accumulated_eval_time': 2584.592365026474, 'accumulated_logging_time': 2.8403160572052}
I0201 04:38:54.156376 139923852027648 logging_writer.py:48] [68359] accumulated_eval_time=2584.592365, accumulated_logging_time=2.840316, accumulated_submission_time=31544.106263, global_step=68359, preemption_count=0, score=31544.106263, test/accuracy=0.559800, test/loss=2.143461, test/num_examples=10000, total_duration=34135.353621, train/accuracy=0.747715, train/loss=1.225752, validation/accuracy=0.683400, validation/loss=1.514231, validation/num_examples=50000
I0201 04:39:10.666156 139923868813056 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.0993281602859497, loss=3.4245078563690186
I0201 04:39:55.051435 139923852027648 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.0628255605697632, loss=4.590410232543945
I0201 04:40:42.143682 139923868813056 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.2020010948181152, loss=5.124016761779785
I0201 04:41:28.668559 139923852027648 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1131995916366577, loss=3.8995110988616943
I0201 04:42:15.525981 139923868813056 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2232356071472168, loss=3.415337324142456
I0201 04:43:02.003041 139923852027648 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.1193674802780151, loss=3.475464105606079
I0201 04:43:48.485439 139923868813056 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.01725435256958, loss=4.096139430999756
I0201 04:44:35.038205 139923852027648 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.120330810546875, loss=4.740993499755859
I0201 04:45:21.850041 139923868813056 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.1114060878753662, loss=4.364735126495361
I0201 04:45:54.401835 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:46:04.769092 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:46:29.247715 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:46:30.885947 140085747812160 submission_runner.py:408] Time since start: 34592.11s, 	Step: 69272, 	{'train/accuracy': 0.7469335794448853, 'train/loss': 1.238759160041809, 'validation/accuracy': 0.6876199841499329, 'validation/loss': 1.490525484085083, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.0847983360290527, 'test/num_examples': 10000, 'score': 31964.290115594864, 'total_duration': 34592.112513780594, 'accumulated_submission_time': 31964.290115594864, 'accumulated_eval_time': 2621.0764644145966, 'accumulated_logging_time': 2.879974603652954}
I0201 04:46:30.913251 139923852027648 logging_writer.py:48] [69272] accumulated_eval_time=2621.076464, accumulated_logging_time=2.879975, accumulated_submission_time=31964.290116, global_step=69272, preemption_count=0, score=31964.290116, test/accuracy=0.568800, test/loss=2.084798, test/num_examples=10000, total_duration=34592.112514, train/accuracy=0.746934, train/loss=1.238759, validation/accuracy=0.687620, validation/loss=1.490525, validation/num_examples=50000
I0201 04:46:42.307736 139923868813056 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.1329065561294556, loss=3.7232141494750977
I0201 04:47:25.935595 139923852027648 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1808838844299316, loss=4.707025051116943
I0201 04:48:12.546192 139923868813056 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.1245707273483276, loss=3.624711036682129
I0201 04:48:59.205289 139923852027648 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.042680263519287, loss=3.965522289276123
I0201 04:49:46.191825 139923868813056 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.2540385723114014, loss=3.51558780670166
I0201 04:50:33.037409 139923852027648 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.1076020002365112, loss=3.3272173404693604
I0201 04:51:19.886547 139923868813056 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.175776481628418, loss=3.3455870151519775
I0201 04:52:06.721309 139923852027648 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1000033617019653, loss=4.039453983306885
I0201 04:52:53.672981 139923868813056 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.1108065843582153, loss=3.404203176498413
I0201 04:53:31.243495 140085747812160 spec.py:321] Evaluating on the training split.
I0201 04:53:41.745476 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 04:54:02.382664 140085747812160 spec.py:349] Evaluating on the test split.
I0201 04:54:04.031877 140085747812160 submission_runner.py:408] Time since start: 35045.26s, 	Step: 70182, 	{'train/accuracy': 0.7432226538658142, 'train/loss': 1.2668992280960083, 'validation/accuracy': 0.6850199699401855, 'validation/loss': 1.5202137231826782, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 2.132345676422119, 'test/num_examples': 10000, 'score': 32384.55722093582, 'total_duration': 35045.258445978165, 'accumulated_submission_time': 32384.55722093582, 'accumulated_eval_time': 2653.8648805618286, 'accumulated_logging_time': 2.919417381286621}
I0201 04:54:04.059728 139923852027648 logging_writer.py:48] [70182] accumulated_eval_time=2653.864881, accumulated_logging_time=2.919417, accumulated_submission_time=32384.557221, global_step=70182, preemption_count=0, score=32384.557221, test/accuracy=0.563600, test/loss=2.132346, test/num_examples=10000, total_duration=35045.258446, train/accuracy=0.743223, train/loss=1.266899, validation/accuracy=0.685020, validation/loss=1.520214, validation/num_examples=50000
I0201 04:54:11.523815 139923868813056 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0401722192764282, loss=3.8903093338012695
I0201 04:54:54.381268 139923852027648 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1770111322402954, loss=3.429835319519043
I0201 04:55:41.501594 139923868813056 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1314467191696167, loss=4.460846424102783
I0201 04:56:28.333396 139923852027648 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.17295241355896, loss=5.0778889656066895
I0201 04:57:14.991672 139923868813056 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.165799617767334, loss=3.37934947013855
I0201 04:58:01.898478 139923852027648 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.2782469987869263, loss=3.4419846534729004
I0201 04:58:48.446887 139923868813056 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2238049507141113, loss=3.4111883640289307
I0201 04:59:35.228155 139923852027648 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.1282497644424438, loss=4.46281623840332
I0201 05:00:22.131970 139923868813056 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.055307388305664, loss=3.5813889503479004
I0201 05:01:04.443159 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:01:14.961735 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:01:38.997210 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:01:40.635460 140085747812160 submission_runner.py:408] Time since start: 35501.86s, 	Step: 71092, 	{'train/accuracy': 0.7544921636581421, 'train/loss': 1.1773465871810913, 'validation/accuracy': 0.6906999945640564, 'validation/loss': 1.4612739086151123, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.0810532569885254, 'test/num_examples': 10000, 'score': 32804.878509521484, 'total_duration': 35501.86202931404, 'accumulated_submission_time': 32804.878509521484, 'accumulated_eval_time': 2690.0571892261505, 'accumulated_logging_time': 2.959210157394409}
I0201 05:01:40.664845 139923852027648 logging_writer.py:48] [71092] accumulated_eval_time=2690.057189, accumulated_logging_time=2.959210, accumulated_submission_time=32804.878510, global_step=71092, preemption_count=0, score=32804.878510, test/accuracy=0.562600, test/loss=2.081053, test/num_examples=10000, total_duration=35501.862029, train/accuracy=0.754492, train/loss=1.177347, validation/accuracy=0.690700, validation/loss=1.461274, validation/num_examples=50000
I0201 05:01:44.199051 139923868813056 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1351312398910522, loss=5.076594829559326
I0201 05:02:26.206860 139923852027648 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1731162071228027, loss=3.42046856880188
I0201 05:03:13.397835 139923868813056 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.048317313194275, loss=4.354109764099121
I0201 05:04:00.373513 139923852027648 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.2190181016921997, loss=3.4421072006225586
I0201 05:04:46.895386 139923868813056 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1573996543884277, loss=3.4730441570281982
I0201 05:05:33.848380 139923852027648 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.068701982498169, loss=4.20647668838501
I0201 05:06:20.619025 139923868813056 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1927460432052612, loss=3.3998165130615234
I0201 05:07:07.298710 139923852027648 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.123823642730713, loss=4.130921840667725
I0201 05:07:53.769973 139923868813056 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.0777473449707031, loss=3.6299681663513184
I0201 05:08:40.456382 139923852027648 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1785061359405518, loss=3.9427733421325684
I0201 05:08:41.084161 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:08:51.462588 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:09:12.688981 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:09:14.325389 140085747812160 submission_runner.py:408] Time since start: 35955.55s, 	Step: 72003, 	{'train/accuracy': 0.7497460842132568, 'train/loss': 1.197008490562439, 'validation/accuracy': 0.6877399682998657, 'validation/loss': 1.4694221019744873, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.097407341003418, 'test/num_examples': 10000, 'score': 33225.23718857765, 'total_duration': 35955.55195403099, 'accumulated_submission_time': 33225.23718857765, 'accumulated_eval_time': 2723.2984120845795, 'accumulated_logging_time': 2.999574899673462}
I0201 05:09:14.355251 139923868813056 logging_writer.py:48] [72003] accumulated_eval_time=2723.298412, accumulated_logging_time=2.999575, accumulated_submission_time=33225.237189, global_step=72003, preemption_count=0, score=33225.237189, test/accuracy=0.563700, test/loss=2.097407, test/num_examples=10000, total_duration=35955.551954, train/accuracy=0.749746, train/loss=1.197008, validation/accuracy=0.687740, validation/loss=1.469422, validation/num_examples=50000
I0201 05:09:54.823361 139923852027648 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.1844160556793213, loss=3.813917398452759
I0201 05:10:41.580773 139923868813056 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2101572751998901, loss=3.476750373840332
I0201 05:11:28.423972 139923852027648 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.150568962097168, loss=3.4084434509277344
I0201 05:12:15.173607 139923868813056 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.1318175792694092, loss=3.459587335586548
I0201 05:13:01.829407 139923852027648 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.1439008712768555, loss=4.946764945983887
I0201 05:13:48.745345 139923868813056 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.2083075046539307, loss=3.3638248443603516
I0201 05:14:35.303993 139923852027648 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1177408695220947, loss=4.952542781829834
I0201 05:15:21.928507 139923868813056 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.122815489768982, loss=3.4829678535461426
I0201 05:16:08.958206 139923852027648 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.1333268880844116, loss=4.68402099609375
I0201 05:16:14.378404 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:16:24.688935 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:16:46.239436 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:16:47.881117 140085747812160 submission_runner.py:408] Time since start: 36409.11s, 	Step: 72913, 	{'train/accuracy': 0.7474804520606995, 'train/loss': 1.2026795148849487, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.473946213722229, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1017489433288574, 'test/num_examples': 10000, 'score': 33645.199608802795, 'total_duration': 36409.107684612274, 'accumulated_submission_time': 33645.199608802795, 'accumulated_eval_time': 2756.8011240959167, 'accumulated_logging_time': 3.038689374923706}
I0201 05:16:47.914407 139923868813056 logging_writer.py:48] [72913] accumulated_eval_time=2756.801124, accumulated_logging_time=3.038689, accumulated_submission_time=33645.199609, global_step=72913, preemption_count=0, score=33645.199609, test/accuracy=0.558600, test/loss=2.101749, test/num_examples=10000, total_duration=36409.107685, train/accuracy=0.747480, train/loss=1.202680, validation/accuracy=0.686600, validation/loss=1.473946, validation/num_examples=50000
I0201 05:17:23.808168 139923852027648 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.1823254823684692, loss=3.955305337905884
I0201 05:18:10.465198 139923868813056 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2275559902191162, loss=3.348182201385498
I0201 05:18:57.260131 139923852027648 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.2048195600509644, loss=3.3760640621185303
I0201 05:19:43.768235 139923868813056 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.1361608505249023, loss=3.3063323497772217
I0201 05:20:30.258989 139923852027648 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.1752088069915771, loss=3.328733444213867
I0201 05:21:17.087318 139923868813056 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.0639841556549072, loss=3.6372580528259277
I0201 05:22:03.743389 139923852027648 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.2063851356506348, loss=3.360250234603882
I0201 05:22:50.618956 139923868813056 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.1873804330825806, loss=3.3780553340911865
I0201 05:23:37.494285 139923852027648 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1562094688415527, loss=3.3411762714385986
I0201 05:23:47.949469 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:23:58.571980 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:24:17.898942 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:24:19.547151 140085747812160 submission_runner.py:408] Time since start: 36860.77s, 	Step: 73824, 	{'train/accuracy': 0.7582421898841858, 'train/loss': 1.1599042415618896, 'validation/accuracy': 0.6906999945640564, 'validation/loss': 1.4475252628326416, 'validation/num_examples': 50000, 'test/accuracy': 0.569100022315979, 'test/loss': 2.0638108253479004, 'test/num_examples': 10000, 'score': 34065.1739525795, 'total_duration': 36860.7737197876, 'accumulated_submission_time': 34065.1739525795, 'accumulated_eval_time': 2788.3988111019135, 'accumulated_logging_time': 3.0819501876831055}
I0201 05:24:19.578647 139923868813056 logging_writer.py:48] [73824] accumulated_eval_time=2788.398811, accumulated_logging_time=3.081950, accumulated_submission_time=34065.173953, global_step=73824, preemption_count=0, score=34065.173953, test/accuracy=0.569100, test/loss=2.063811, test/num_examples=10000, total_duration=36860.773720, train/accuracy=0.758242, train/loss=1.159904, validation/accuracy=0.690700, validation/loss=1.447525, validation/num_examples=50000
I0201 05:24:50.399367 139923852027648 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.1446712017059326, loss=4.694238662719727
I0201 05:25:36.850285 139923868813056 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.142319679260254, loss=3.971536874771118
I0201 05:26:23.809948 139923852027648 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.1969916820526123, loss=3.533245086669922
I0201 05:27:10.551490 139923868813056 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.199578046798706, loss=5.130947113037109
I0201 05:27:57.291984 139923852027648 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.1033724546432495, loss=3.365478992462158
I0201 05:28:43.937551 139923868813056 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.2512329816818237, loss=3.5163302421569824
I0201 05:29:30.794973 139923852027648 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.1957672834396362, loss=3.4882616996765137
I0201 05:30:17.363360 139923868813056 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.2487579584121704, loss=3.4600908756256104
I0201 05:31:04.102376 139923852027648 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.237432599067688, loss=3.4533374309539795
I0201 05:31:19.707134 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:31:30.252629 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:31:52.892672 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:31:54.544971 140085747812160 submission_runner.py:408] Time since start: 37315.77s, 	Step: 74735, 	{'train/accuracy': 0.7710937261581421, 'train/loss': 1.1149591207504272, 'validation/accuracy': 0.6913999915122986, 'validation/loss': 1.4553277492523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.082012176513672, 'test/num_examples': 10000, 'score': 34485.24294900894, 'total_duration': 37315.77154183388, 'accumulated_submission_time': 34485.24294900894, 'accumulated_eval_time': 2823.236649990082, 'accumulated_logging_time': 3.122302770614624}
I0201 05:31:54.574384 139923868813056 logging_writer.py:48] [74735] accumulated_eval_time=2823.236650, accumulated_logging_time=3.122303, accumulated_submission_time=34485.242949, global_step=74735, preemption_count=0, score=34485.242949, test/accuracy=0.565800, test/loss=2.082012, test/num_examples=10000, total_duration=37315.771542, train/accuracy=0.771094, train/loss=1.114959, validation/accuracy=0.691400, validation/loss=1.455328, validation/num_examples=50000
I0201 05:32:20.535143 139923852027648 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.2969224452972412, loss=3.291121006011963
I0201 05:33:07.074240 139923868813056 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.0539594888687134, loss=4.024418830871582
I0201 05:33:54.309220 139923852027648 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2087328433990479, loss=3.3584117889404297
I0201 05:34:41.030048 139923868813056 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.1094669103622437, loss=3.406583786010742
I0201 05:35:27.677866 139923852027648 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1447677612304688, loss=3.637472152709961
I0201 05:36:14.450500 139923868813056 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.1297459602355957, loss=4.823911666870117
I0201 05:37:01.184302 139923852027648 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.175735592842102, loss=3.601806163787842
I0201 05:37:47.835801 139923868813056 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.2232543230056763, loss=3.346623182296753
I0201 05:38:34.534531 139923852027648 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.2665650844573975, loss=3.381622314453125
I0201 05:38:54.734745 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:39:05.004424 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:39:26.592130 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:39:28.235535 140085747812160 submission_runner.py:408] Time since start: 37769.46s, 	Step: 75645, 	{'train/accuracy': 0.7518945336341858, 'train/loss': 1.226989984512329, 'validation/accuracy': 0.6896199584007263, 'validation/loss': 1.4911552667617798, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 2.1078085899353027, 'test/num_examples': 10000, 'score': 34905.34354329109, 'total_duration': 37769.46210241318, 'accumulated_submission_time': 34905.34354329109, 'accumulated_eval_time': 2856.7374284267426, 'accumulated_logging_time': 3.161123752593994}
I0201 05:39:28.264273 139923868813056 logging_writer.py:48] [75645] accumulated_eval_time=2856.737428, accumulated_logging_time=3.161124, accumulated_submission_time=34905.343543, global_step=75645, preemption_count=0, score=34905.343543, test/accuracy=0.564500, test/loss=2.107809, test/num_examples=10000, total_duration=37769.462102, train/accuracy=0.751895, train/loss=1.226990, validation/accuracy=0.689620, validation/loss=1.491155, validation/num_examples=50000
I0201 05:39:50.253505 139923852027648 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.1819037199020386, loss=5.066523551940918
I0201 05:40:35.284894 139923852027648 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.2625348567962646, loss=3.498993158340454
I0201 05:41:22.063401 139923868813056 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.201850414276123, loss=4.684380054473877
I0201 05:42:08.605342 139923852027648 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.16790771484375, loss=3.4046072959899902
I0201 05:42:55.971738 139923868813056 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.1812337636947632, loss=3.452139139175415
I0201 05:43:42.638477 139923852027648 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2366782426834106, loss=3.313300132751465
I0201 05:44:29.481070 139923868813056 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.1531498432159424, loss=3.4608571529388428
I0201 05:45:16.166329 139923852027648 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.1488808393478394, loss=3.249195098876953
I0201 05:46:02.880056 139923868813056 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.246040940284729, loss=3.3394668102264404
I0201 05:46:28.369195 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:46:38.666473 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:46:59.454296 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:47:01.093102 140085747812160 submission_runner.py:408] Time since start: 38222.32s, 	Step: 76556, 	{'train/accuracy': 0.7568554282188416, 'train/loss': 1.1630686521530151, 'validation/accuracy': 0.694379985332489, 'validation/loss': 1.4409019947052002, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.063631772994995, 'test/num_examples': 10000, 'score': 35325.38757443428, 'total_duration': 38222.31966996193, 'accumulated_submission_time': 35325.38757443428, 'accumulated_eval_time': 2889.4613375663757, 'accumulated_logging_time': 3.200040578842163}
I0201 05:47:01.122215 139923852027648 logging_writer.py:48] [76556] accumulated_eval_time=2889.461338, accumulated_logging_time=3.200041, accumulated_submission_time=35325.387574, global_step=76556, preemption_count=0, score=35325.387574, test/accuracy=0.563700, test/loss=2.063632, test/num_examples=10000, total_duration=38222.319670, train/accuracy=0.756855, train/loss=1.163069, validation/accuracy=0.694380, validation/loss=1.440902, validation/num_examples=50000
I0201 05:47:18.791192 139923868813056 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.218807578086853, loss=3.4536638259887695
I0201 05:48:03.223062 139923852027648 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.297878623008728, loss=3.36185622215271
I0201 05:48:50.125350 139923868813056 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2715308666229248, loss=4.990670204162598
I0201 05:49:36.674130 139923852027648 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.280142903327942, loss=3.3634634017944336
I0201 05:50:23.350452 139923868813056 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.3158340454101562, loss=5.154548645019531
I0201 05:51:09.945029 139923852027648 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.1542969942092896, loss=4.668353080749512
I0201 05:51:56.533062 139923868813056 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.2954102754592896, loss=4.9904961585998535
I0201 05:52:43.307952 139923852027648 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0658690929412842, loss=4.500746250152588
I0201 05:53:30.098852 139923868813056 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.182138204574585, loss=3.3687405586242676
I0201 05:54:01.468789 140085747812160 spec.py:321] Evaluating on the training split.
I0201 05:54:11.702803 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 05:54:31.927822 140085747812160 spec.py:349] Evaluating on the test split.
I0201 05:54:33.572164 140085747812160 submission_runner.py:408] Time since start: 38674.80s, 	Step: 77469, 	{'train/accuracy': 0.7740234136581421, 'train/loss': 1.1011523008346558, 'validation/accuracy': 0.6961199641227722, 'validation/loss': 1.4330646991729736, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 2.0542802810668945, 'test/num_examples': 10000, 'score': 35745.66950130463, 'total_duration': 38674.79872870445, 'accumulated_submission_time': 35745.66950130463, 'accumulated_eval_time': 2921.5647172927856, 'accumulated_logging_time': 3.242464065551758}
I0201 05:54:33.602627 139923852027648 logging_writer.py:48] [77469] accumulated_eval_time=2921.564717, accumulated_logging_time=3.242464, accumulated_submission_time=35745.669501, global_step=77469, preemption_count=0, score=35745.669501, test/accuracy=0.572100, test/loss=2.054280, test/num_examples=10000, total_duration=38674.798729, train/accuracy=0.774023, train/loss=1.101152, validation/accuracy=0.696120, validation/loss=1.433065, validation/num_examples=50000
I0201 05:54:46.165706 139923868813056 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.2726081609725952, loss=4.673527717590332
I0201 05:55:30.133870 139923852027648 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.049743890762329, loss=3.8871357440948486
I0201 05:56:16.620941 139923868813056 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.1450971364974976, loss=3.381995916366577
I0201 05:57:03.322611 139923852027648 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1384315490722656, loss=4.264206409454346
I0201 05:57:49.987992 139923868813056 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2109014987945557, loss=3.4405951499938965
I0201 05:58:36.632886 139923852027648 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.2048289775848389, loss=3.3401496410369873
I0201 05:59:23.568692 139923868813056 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.2003509998321533, loss=3.3105459213256836
I0201 06:00:10.195910 139923852027648 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3048315048217773, loss=3.472494125366211
I0201 06:00:56.739096 139923868813056 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1786037683486938, loss=4.722002029418945
I0201 06:01:33.879434 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:01:44.602774 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:02:06.598942 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:02:08.245347 140085747812160 submission_runner.py:408] Time since start: 39129.47s, 	Step: 78381, 	{'train/accuracy': 0.7498828172683716, 'train/loss': 1.1963554620742798, 'validation/accuracy': 0.6926999688148499, 'validation/loss': 1.452593445777893, 'validation/num_examples': 50000, 'test/accuracy': 0.5669000148773193, 'test/loss': 2.084571599960327, 'test/num_examples': 10000, 'score': 36165.88693213463, 'total_duration': 39129.471900224686, 'accumulated_submission_time': 36165.88693213463, 'accumulated_eval_time': 2955.930624961853, 'accumulated_logging_time': 3.281820297241211}
I0201 06:02:08.282334 139923852027648 logging_writer.py:48] [78381] accumulated_eval_time=2955.930625, accumulated_logging_time=3.281820, accumulated_submission_time=36165.886932, global_step=78381, preemption_count=0, score=36165.886932, test/accuracy=0.566900, test/loss=2.084572, test/num_examples=10000, total_duration=39129.471900, train/accuracy=0.749883, train/loss=1.196355, validation/accuracy=0.692700, validation/loss=1.452593, validation/num_examples=50000
I0201 06:02:16.144175 139923868813056 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.4664770364761353, loss=3.332871913909912
I0201 06:02:58.953213 139923852027648 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.193684458732605, loss=4.884002685546875
I0201 06:03:45.896835 139923868813056 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.2540446519851685, loss=3.3488268852233887
I0201 06:04:32.653785 139923852027648 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.2626570463180542, loss=3.5586531162261963
I0201 06:05:19.565378 139923868813056 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2060905694961548, loss=3.359191417694092
I0201 06:06:06.205967 139923852027648 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.2221310138702393, loss=3.343794822692871
I0201 06:06:52.749288 139923868813056 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.1275368928909302, loss=3.635667085647583
I0201 06:07:39.409943 139923852027648 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.1944421529769897, loss=3.320676326751709
I0201 06:08:26.041861 139923868813056 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.222672939300537, loss=3.5552291870117188
I0201 06:09:08.253936 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:09:18.541265 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:09:42.089746 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:09:43.732993 140085747812160 submission_runner.py:408] Time since start: 39584.96s, 	Step: 79292, 	{'train/accuracy': 0.7596288919448853, 'train/loss': 1.14617919921875, 'validation/accuracy': 0.6948999762535095, 'validation/loss': 1.4263193607330322, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.044193744659424, 'test/num_examples': 10000, 'score': 36585.79311347008, 'total_duration': 39584.9595644474, 'accumulated_submission_time': 36585.79311347008, 'accumulated_eval_time': 2991.409699201584, 'accumulated_logging_time': 3.3323724269866943}
I0201 06:09:43.765538 139923852027648 logging_writer.py:48] [79292] accumulated_eval_time=2991.409699, accumulated_logging_time=3.332372, accumulated_submission_time=36585.793113, global_step=79292, preemption_count=0, score=36585.793113, test/accuracy=0.573600, test/loss=2.044194, test/num_examples=10000, total_duration=39584.959564, train/accuracy=0.759629, train/loss=1.146179, validation/accuracy=0.694900, validation/loss=1.426319, validation/num_examples=50000
I0201 06:09:47.297446 139923868813056 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.2057372331619263, loss=4.217871189117432
I0201 06:10:29.333528 139923852027648 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.2151838541030884, loss=3.542198419570923
I0201 06:11:15.928198 139923868813056 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.1765164136886597, loss=3.872896671295166
I0201 06:12:02.587012 139923852027648 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.0881365537643433, loss=3.6283836364746094
I0201 06:12:49.459492 139923868813056 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.2731645107269287, loss=3.3081130981445312
I0201 06:13:36.083745 139923852027648 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.3028146028518677, loss=3.395540952682495
I0201 06:14:22.856760 139923868813056 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.2522556781768799, loss=3.3845319747924805
I0201 06:15:09.411576 139923852027648 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.1771186590194702, loss=4.183383941650391
I0201 06:15:56.177345 139923868813056 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.2534842491149902, loss=3.48624324798584
I0201 06:16:43.022073 139923852027648 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.1649830341339111, loss=4.687068939208984
I0201 06:16:44.100902 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:16:54.346198 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:17:15.793443 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:17:17.433861 140085747812160 submission_runner.py:408] Time since start: 40038.66s, 	Step: 80204, 	{'train/accuracy': 0.7778124809265137, 'train/loss': 1.1191484928131104, 'validation/accuracy': 0.7011399865150452, 'validation/loss': 1.4412455558776855, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0484261512756348, 'test/num_examples': 10000, 'score': 37006.06656885147, 'total_duration': 40038.66042852402, 'accumulated_submission_time': 37006.06656885147, 'accumulated_eval_time': 3024.742676258087, 'accumulated_logging_time': 3.375904083251953}
I0201 06:17:17.464942 139923868813056 logging_writer.py:48] [80204] accumulated_eval_time=3024.742676, accumulated_logging_time=3.375904, accumulated_submission_time=37006.066569, global_step=80204, preemption_count=0, score=37006.066569, test/accuracy=0.577700, test/loss=2.048426, test/num_examples=10000, total_duration=40038.660429, train/accuracy=0.777812, train/loss=1.119148, validation/accuracy=0.701140, validation/loss=1.441246, validation/num_examples=50000
I0201 06:17:57.484469 139923852027648 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.2105404138565063, loss=3.504974842071533
I0201 06:18:44.173882 139923868813056 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.2739654779434204, loss=3.5011038780212402
I0201 06:19:31.198671 139923852027648 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.2704404592514038, loss=3.3351168632507324
I0201 06:20:18.071455 139923868813056 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.2761588096618652, loss=3.3100576400756836
I0201 06:21:04.718360 139923852027648 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.2580029964447021, loss=3.481961727142334
I0201 06:21:51.730067 139923868813056 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.2641403675079346, loss=3.492424249649048
I0201 06:22:38.608621 139923852027648 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.2858338356018066, loss=3.370059013366699
I0201 06:23:25.555097 139923868813056 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.2975293397903442, loss=3.2988643646240234
I0201 06:24:12.643491 139923852027648 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.2054052352905273, loss=3.818232774734497
I0201 06:24:17.447930 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:24:28.626339 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:24:52.440429 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:24:54.082309 140085747812160 submission_runner.py:408] Time since start: 40495.31s, 	Step: 81112, 	{'train/accuracy': 0.75927734375, 'train/loss': 1.1983951330184937, 'validation/accuracy': 0.6988799571990967, 'validation/loss': 1.469262957572937, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.0844738483428955, 'test/num_examples': 10000, 'score': 37425.9896273613, 'total_duration': 40495.30886769295, 'accumulated_submission_time': 37425.9896273613, 'accumulated_eval_time': 3061.377053260803, 'accumulated_logging_time': 3.4166457653045654}
I0201 06:24:54.111179 139923868813056 logging_writer.py:48] [81112] accumulated_eval_time=3061.377053, accumulated_logging_time=3.416646, accumulated_submission_time=37425.989627, global_step=81112, preemption_count=0, score=37425.989627, test/accuracy=0.570900, test/loss=2.084474, test/num_examples=10000, total_duration=40495.308868, train/accuracy=0.759277, train/loss=1.198395, validation/accuracy=0.698880, validation/loss=1.469263, validation/num_examples=50000
I0201 06:25:30.195587 139923852027648 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.346602439880371, loss=3.3087658882141113
I0201 06:26:16.911970 139923868813056 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.3310065269470215, loss=3.353006362915039
I0201 06:27:04.028008 139923852027648 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.3410872220993042, loss=3.400844097137451
I0201 06:27:50.565745 139923868813056 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.2738044261932373, loss=3.364044666290283
I0201 06:28:37.481957 139923852027648 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.2735964059829712, loss=3.6224684715270996
I0201 06:29:24.261390 139923868813056 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.366032361984253, loss=4.984288215637207
I0201 06:30:10.844888 139923852027648 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.2703994512557983, loss=3.32918381690979
I0201 06:30:57.535925 139923868813056 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.2501416206359863, loss=3.326802968978882
I0201 06:31:44.311303 139923852027648 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.2143489122390747, loss=3.8796849250793457
I0201 06:31:54.253135 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:32:04.935956 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:32:29.193459 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:32:30.832522 140085747812160 submission_runner.py:408] Time since start: 40952.06s, 	Step: 82023, 	{'train/accuracy': 0.7640624642372131, 'train/loss': 1.1621880531311035, 'validation/accuracy': 0.702019989490509, 'validation/loss': 1.4289867877960205, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 2.0627498626708984, 'test/num_examples': 10000, 'score': 37846.07184147835, 'total_duration': 40952.05909061432, 'accumulated_submission_time': 37846.07184147835, 'accumulated_eval_time': 3097.956475019455, 'accumulated_logging_time': 3.4548816680908203}
I0201 06:32:30.861104 139923868813056 logging_writer.py:48] [82023] accumulated_eval_time=3097.956475, accumulated_logging_time=3.454882, accumulated_submission_time=37846.071841, global_step=82023, preemption_count=0, score=37846.071841, test/accuracy=0.577300, test/loss=2.062750, test/num_examples=10000, total_duration=40952.059091, train/accuracy=0.764062, train/loss=1.162188, validation/accuracy=0.702020, validation/loss=1.428987, validation/num_examples=50000
I0201 06:33:01.917151 139923852027648 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.347855806350708, loss=3.3176560401916504
I0201 06:33:48.476006 139923868813056 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.158306360244751, loss=4.443120956420898
I0201 06:34:35.263024 139923852027648 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3697434663772583, loss=3.2595763206481934
I0201 06:35:21.882707 139923868813056 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3169431686401367, loss=3.3599660396575928
I0201 06:36:08.689785 139923852027648 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.3610953092575073, loss=5.020679950714111
I0201 06:36:55.371790 139923868813056 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.247910976409912, loss=3.3937790393829346
I0201 06:37:42.209280 139923852027648 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.218239188194275, loss=3.2973203659057617
I0201 06:38:28.982715 139923868813056 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.3079569339752197, loss=3.305429697036743
I0201 06:39:15.783037 139923852027648 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.2981159687042236, loss=4.03404426574707
I0201 06:39:31.296788 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:39:41.507991 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:40:01.873035 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:40:03.526963 140085747812160 submission_runner.py:408] Time since start: 41404.75s, 	Step: 82935, 	{'train/accuracy': 0.7707226276397705, 'train/loss': 1.1098370552062988, 'validation/accuracy': 0.7006799578666687, 'validation/loss': 1.4164754152297974, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 2.0360336303710938, 'test/num_examples': 10000, 'score': 38266.4472155571, 'total_duration': 41404.753534555435, 'accumulated_submission_time': 38266.4472155571, 'accumulated_eval_time': 3130.1866416931152, 'accumulated_logging_time': 3.492640733718872}
I0201 06:40:03.555283 139923868813056 logging_writer.py:48] [82935] accumulated_eval_time=3130.186642, accumulated_logging_time=3.492641, accumulated_submission_time=38266.447216, global_step=82935, preemption_count=0, score=38266.447216, test/accuracy=0.574600, test/loss=2.036034, test/num_examples=10000, total_duration=41404.753535, train/accuracy=0.770723, train/loss=1.109837, validation/accuracy=0.700680, validation/loss=1.416475, validation/num_examples=50000
I0201 06:40:29.459208 139923852027648 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.272372841835022, loss=3.4192721843719482
I0201 06:41:15.442693 139923868813056 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.1182729005813599, loss=4.692378520965576
I0201 06:42:02.176555 139923852027648 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.1546072959899902, loss=4.4375739097595215
I0201 06:42:48.915697 139923868813056 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.1957502365112305, loss=4.628789901733398
I0201 06:43:35.658655 139923852027648 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.2194627523422241, loss=3.669908285140991
I0201 06:44:22.475741 139923868813056 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2086237668991089, loss=4.035552024841309
I0201 06:45:09.406519 139923852027648 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.2709267139434814, loss=4.556845664978027
I0201 06:45:56.044065 139923868813056 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.2373290061950684, loss=3.3278274536132812
I0201 06:46:42.599807 139923852027648 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3332699537277222, loss=3.2833127975463867
I0201 06:47:03.580061 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:47:13.806033 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:47:37.250231 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:47:38.894500 140085747812160 submission_runner.py:408] Time since start: 41860.12s, 	Step: 83846, 	{'train/accuracy': 0.7615429759025574, 'train/loss': 1.1838170289993286, 'validation/accuracy': 0.700760006904602, 'validation/loss': 1.4477368593215942, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.065577745437622, 'test/num_examples': 10000, 'score': 38686.41108036041, 'total_duration': 41860.1210463047, 'accumulated_submission_time': 38686.41108036041, 'accumulated_eval_time': 3165.501063108444, 'accumulated_logging_time': 3.5313708782196045}
I0201 06:47:38.925967 139923868813056 logging_writer.py:48] [83846] accumulated_eval_time=3165.501063, accumulated_logging_time=3.531371, accumulated_submission_time=38686.411080, global_step=83846, preemption_count=0, score=38686.411080, test/accuracy=0.570300, test/loss=2.065578, test/num_examples=10000, total_duration=41860.121046, train/accuracy=0.761543, train/loss=1.183817, validation/accuracy=0.700760, validation/loss=1.447737, validation/num_examples=50000
I0201 06:48:00.530097 139923852027648 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.274395227432251, loss=4.381670951843262
I0201 06:48:45.208851 139923868813056 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.2105494737625122, loss=3.3380239009857178
I0201 06:49:31.973747 139923852027648 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.1774948835372925, loss=3.801910400390625
I0201 06:50:18.604057 139923868813056 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2284924983978271, loss=3.8180811405181885
I0201 06:51:05.440245 139923852027648 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.2451577186584473, loss=3.3656647205352783
I0201 06:51:52.082823 139923868813056 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.2398916482925415, loss=3.2741079330444336
I0201 06:52:39.125976 139923852027648 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.2950035333633423, loss=3.6133148670196533
I0201 06:53:25.860749 139923868813056 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.1555176973342896, loss=4.5889892578125
I0201 06:54:12.481032 139923852027648 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.162468433380127, loss=4.429121971130371
I0201 06:54:39.299096 140085747812160 spec.py:321] Evaluating on the training split.
I0201 06:54:50.076497 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 06:55:13.025879 140085747812160 spec.py:349] Evaluating on the test split.
I0201 06:55:14.666855 140085747812160 submission_runner.py:408] Time since start: 42315.89s, 	Step: 84759, 	{'train/accuracy': 0.7667187452316284, 'train/loss': 1.1391881704330444, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4073001146316528, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 2.0161125659942627, 'test/num_examples': 10000, 'score': 39106.723685503006, 'total_duration': 42315.893428087234, 'accumulated_submission_time': 39106.723685503006, 'accumulated_eval_time': 3200.868814229965, 'accumulated_logging_time': 3.5725789070129395}
I0201 06:55:14.699754 139923868813056 logging_writer.py:48] [84759] accumulated_eval_time=3200.868814, accumulated_logging_time=3.572579, accumulated_submission_time=39106.723686, global_step=84759, preemption_count=0, score=39106.723686, test/accuracy=0.585500, test/loss=2.016113, test/num_examples=10000, total_duration=42315.893428, train/accuracy=0.766719, train/loss=1.139188, validation/accuracy=0.703380, validation/loss=1.407300, validation/num_examples=50000
I0201 06:55:31.199498 139923852027648 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.330857515335083, loss=3.4167640209198
I0201 06:56:15.426034 139923868813056 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.43403160572052, loss=3.299678325653076
I0201 06:57:02.121807 139923852027648 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.2268590927124023, loss=4.42369270324707
I0201 06:57:49.006875 139923868813056 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.2223787307739258, loss=4.103335380554199
I0201 06:58:35.480145 139923852027648 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.2789336442947388, loss=4.918057441711426
I0201 06:59:22.203314 139923868813056 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.1071478128433228, loss=3.5814929008483887
I0201 07:00:08.855850 139923852027648 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.3001704216003418, loss=3.372544765472412
I0201 07:00:55.673694 139923868813056 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.1542773246765137, loss=3.5632450580596924
I0201 07:01:42.262629 139923852027648 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.3316326141357422, loss=3.431856632232666
I0201 07:02:14.724253 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:02:25.194224 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:02:50.317185 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:02:51.960517 140085747812160 submission_runner.py:408] Time since start: 42773.19s, 	Step: 85671, 	{'train/accuracy': 0.7760937213897705, 'train/loss': 1.1103529930114746, 'validation/accuracy': 0.7053200006484985, 'validation/loss': 1.4143160581588745, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 2.0242509841918945, 'test/num_examples': 10000, 'score': 39526.67753863335, 'total_duration': 42773.18708443642, 'accumulated_submission_time': 39526.67753863335, 'accumulated_eval_time': 3238.1050729751587, 'accumulated_logging_time': 3.6252217292785645}
I0201 07:02:51.993619 139923868813056 logging_writer.py:48] [85671] accumulated_eval_time=3238.105073, accumulated_logging_time=3.625222, accumulated_submission_time=39526.677539, global_step=85671, preemption_count=0, score=39526.677539, test/accuracy=0.583400, test/loss=2.024251, test/num_examples=10000, total_duration=42773.187084, train/accuracy=0.776094, train/loss=1.110353, validation/accuracy=0.705320, validation/loss=1.414316, validation/num_examples=50000
I0201 07:03:03.770516 139923852027648 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4073668718338013, loss=3.4685933589935303
I0201 07:03:47.249079 139923868813056 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.3077343702316284, loss=3.843608856201172
I0201 07:04:34.147914 139923852027648 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.235983967781067, loss=3.5374596118927
I0201 07:05:20.915471 139923868813056 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.3344045877456665, loss=4.737809658050537
I0201 07:06:07.806366 139923852027648 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.3001666069030762, loss=3.321178436279297
I0201 07:06:54.723670 139923868813056 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.2034863233566284, loss=3.707592248916626
I0201 07:07:41.470447 139923852027648 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.2281743288040161, loss=3.3657875061035156
I0201 07:08:28.183063 139923868813056 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.1396138668060303, loss=3.9867138862609863
I0201 07:09:15.046730 139923852027648 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.2404788732528687, loss=3.7878544330596924
I0201 07:09:52.137883 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:10:02.531167 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:10:28.726256 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:10:30.365466 140085747812160 submission_runner.py:408] Time since start: 43231.59s, 	Step: 86581, 	{'train/accuracy': 0.7747851610183716, 'train/loss': 1.1178936958312988, 'validation/accuracy': 0.7052599787712097, 'validation/loss': 1.4013491868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.003753900527954, 'test/num_examples': 10000, 'score': 39946.76177215576, 'total_duration': 43231.5920381546, 'accumulated_submission_time': 39946.76177215576, 'accumulated_eval_time': 3276.332666158676, 'accumulated_logging_time': 3.668002128601074}
I0201 07:10:30.398814 139923868813056 logging_writer.py:48] [86581] accumulated_eval_time=3276.332666, accumulated_logging_time=3.668002, accumulated_submission_time=39946.761772, global_step=86581, preemption_count=0, score=39946.761772, test/accuracy=0.583500, test/loss=2.003754, test/num_examples=10000, total_duration=43231.592038, train/accuracy=0.774785, train/loss=1.117894, validation/accuracy=0.705260, validation/loss=1.401349, validation/num_examples=50000
I0201 07:10:38.244592 139923852027648 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.438167929649353, loss=3.317512273788452
I0201 07:11:21.205916 139923868813056 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.2355077266693115, loss=3.965813636779785
I0201 07:12:08.006695 139923852027648 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.2560395002365112, loss=3.3989291191101074
I0201 07:12:54.750996 139923868813056 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.4355770349502563, loss=5.075553894042969
I0201 07:13:41.394690 139923852027648 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.2864973545074463, loss=3.259511947631836
I0201 07:14:28.134451 139923868813056 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.4698524475097656, loss=5.022145748138428
I0201 07:15:14.846489 139923852027648 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.2083719968795776, loss=3.4133219718933105
I0201 07:16:01.593993 139923868813056 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.2456607818603516, loss=4.732163429260254
I0201 07:16:48.289072 139923852027648 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3568702936172485, loss=3.375586748123169
I0201 07:17:30.535701 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:17:40.780599 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:18:04.297409 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:18:05.941143 140085747812160 submission_runner.py:408] Time since start: 43687.17s, 	Step: 87492, 	{'train/accuracy': 0.7694140672683716, 'train/loss': 1.1339526176452637, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4059618711471558, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 2.0230345726013184, 'test/num_examples': 10000, 'score': 40366.834473609924, 'total_duration': 43687.16769170761, 'accumulated_submission_time': 40366.834473609924, 'accumulated_eval_time': 3311.738111257553, 'accumulated_logging_time': 3.710970640182495}
I0201 07:18:05.971339 139923868813056 logging_writer.py:48] [87492] accumulated_eval_time=3311.738111, accumulated_logging_time=3.710971, accumulated_submission_time=40366.834474, global_step=87492, preemption_count=0, score=40366.834474, test/accuracy=0.578300, test/loss=2.023035, test/num_examples=10000, total_duration=43687.167692, train/accuracy=0.769414, train/loss=1.133953, validation/accuracy=0.703380, validation/loss=1.405962, validation/num_examples=50000
I0201 07:18:09.504435 139923852027648 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1508493423461914, loss=3.668851375579834
I0201 07:18:51.506731 139923868813056 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.2644035816192627, loss=3.6242635250091553
I0201 07:19:38.037535 139923852027648 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2486604452133179, loss=3.2802133560180664
I0201 07:20:24.749472 139923868813056 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.2494981288909912, loss=3.2841453552246094
I0201 07:21:11.506143 139923852027648 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.2260613441467285, loss=3.2644195556640625
I0201 07:21:58.128485 139923868813056 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.2885591983795166, loss=3.2741708755493164
I0201 07:22:45.137882 139923852027648 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.2208999395370483, loss=4.053247451782227
I0201 07:23:31.902932 139923868813056 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2652997970581055, loss=3.63435697555542
I0201 07:24:18.532737 139923852027648 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.171991229057312, loss=3.754150867462158
I0201 07:25:05.300759 139923868813056 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.2075605392456055, loss=4.143375873565674
I0201 07:25:06.397393 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:25:16.905013 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:25:38.802079 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:25:40.444021 140085747812160 submission_runner.py:408] Time since start: 44141.67s, 	Step: 88404, 	{'train/accuracy': 0.7787304520606995, 'train/loss': 1.0719337463378906, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.3819568157196045, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.9886709451675415, 'test/num_examples': 10000, 'score': 40787.19777345657, 'total_duration': 44141.67059183121, 'accumulated_submission_time': 40787.19777345657, 'accumulated_eval_time': 3345.784725189209, 'accumulated_logging_time': 3.7528860569000244}
I0201 07:25:40.473166 139923852027648 logging_writer.py:48] [88404] accumulated_eval_time=3345.784725, accumulated_logging_time=3.752886, accumulated_submission_time=40787.197773, global_step=88404, preemption_count=0, score=40787.197773, test/accuracy=0.580700, test/loss=1.988671, test/num_examples=10000, total_duration=44141.670592, train/accuracy=0.778730, train/loss=1.071934, validation/accuracy=0.706040, validation/loss=1.381957, validation/num_examples=50000
I0201 07:26:20.346977 139923868813056 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.3435801267623901, loss=3.3171133995056152
I0201 07:27:07.194795 139923852027648 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.4176709651947021, loss=3.3105316162109375
I0201 07:27:54.062927 139923868813056 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.3576488494873047, loss=4.524506568908691
I0201 07:28:40.662843 139923852027648 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3264083862304688, loss=3.2743992805480957
I0201 07:29:27.514657 139923868813056 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.396362066268921, loss=3.3278298377990723
I0201 07:30:14.022829 139923852027648 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.3608633279800415, loss=3.8742988109588623
I0201 07:31:00.857137 139923868813056 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.2391197681427002, loss=3.857468366622925
I0201 07:31:47.577041 139923852027648 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.345802664756775, loss=3.324492931365967
I0201 07:32:34.596532 139923868813056 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.2482610940933228, loss=3.3683152198791504
I0201 07:32:40.498439 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:32:50.862193 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:33:13.702798 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:33:15.341127 140085747812160 submission_runner.py:408] Time since start: 44596.57s, 	Step: 89314, 	{'train/accuracy': 0.7935351133346558, 'train/loss': 1.021241545677185, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.3768731355667114, 'validation/num_examples': 50000, 'test/accuracy': 0.5854000449180603, 'test/loss': 1.981370210647583, 'test/num_examples': 10000, 'score': 41207.16171503067, 'total_duration': 44596.56769442558, 'accumulated_submission_time': 41207.16171503067, 'accumulated_eval_time': 3380.6273963451385, 'accumulated_logging_time': 3.791879415512085}
I0201 07:33:15.374590 139923852027648 logging_writer.py:48] [89314] accumulated_eval_time=3380.627396, accumulated_logging_time=3.791879, accumulated_submission_time=41207.161715, global_step=89314, preemption_count=0, score=41207.161715, test/accuracy=0.585400, test/loss=1.981370, test/num_examples=10000, total_duration=44596.567694, train/accuracy=0.793535, train/loss=1.021242, validation/accuracy=0.706140, validation/loss=1.376873, validation/num_examples=50000
I0201 07:33:50.523566 139923868813056 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.317010521888733, loss=4.312498092651367
I0201 07:34:37.079308 139923852027648 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.3411056995391846, loss=3.3187808990478516
I0201 07:35:24.150131 139923868813056 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2425518035888672, loss=5.014036178588867
I0201 07:36:10.652645 139923852027648 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2609779834747314, loss=3.9330496788024902
I0201 07:36:57.424173 139923868813056 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.1422269344329834, loss=3.586833953857422
I0201 07:37:44.426630 139923852027648 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.2409801483154297, loss=3.318235397338867
I0201 07:38:31.227637 139923868813056 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.262349247932434, loss=3.334660768508911
I0201 07:39:18.080560 139923852027648 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.1474049091339111, loss=3.646455764770508
I0201 07:40:04.552431 139923868813056 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2346137762069702, loss=3.5283043384552
I0201 07:40:15.443988 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:40:25.654833 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:40:49.674704 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:40:51.316470 140085747812160 submission_runner.py:408] Time since start: 45052.54s, 	Step: 90225, 	{'train/accuracy': 0.7718163728713989, 'train/loss': 1.1109107732772827, 'validation/accuracy': 0.7108599543571472, 'validation/loss': 1.373457670211792, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.001279354095459, 'test/num_examples': 10000, 'score': 41627.17049217224, 'total_duration': 45052.54302406311, 'accumulated_submission_time': 41627.17049217224, 'accumulated_eval_time': 3416.4998614788055, 'accumulated_logging_time': 3.834437370300293}
I0201 07:40:51.350430 139923852027648 logging_writer.py:48] [90225] accumulated_eval_time=3416.499861, accumulated_logging_time=3.834437, accumulated_submission_time=41627.170492, global_step=90225, preemption_count=0, score=41627.170492, test/accuracy=0.581000, test/loss=2.001279, test/num_examples=10000, total_duration=45052.543024, train/accuracy=0.771816, train/loss=1.110911, validation/accuracy=0.710860, validation/loss=1.373458, validation/num_examples=50000
I0201 07:41:21.694995 139923868813056 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.4047834873199463, loss=3.3799307346343994
I0201 07:42:08.363880 139923852027648 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2741734981536865, loss=4.111584186553955
I0201 07:42:55.429187 139923868813056 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.4056689739227295, loss=3.2179622650146484
I0201 07:43:42.026126 139923852027648 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.2327754497528076, loss=4.307452201843262
I0201 07:44:28.715420 139923868813056 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.3024349212646484, loss=4.397398948669434
I0201 07:45:15.472707 139923852027648 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.3942890167236328, loss=3.2566092014312744
I0201 07:46:02.226129 139923868813056 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.241290807723999, loss=3.259766101837158
I0201 07:46:48.775337 139923852027648 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.335753083229065, loss=3.961536169052124
I0201 07:47:35.524770 139923868813056 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.1681585311889648, loss=4.093935012817383
I0201 07:47:51.640447 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:48:02.195547 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:48:27.973196 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:48:29.610955 140085747812160 submission_runner.py:408] Time since start: 45510.84s, 	Step: 91136, 	{'train/accuracy': 0.7748632431030273, 'train/loss': 1.0888638496398926, 'validation/accuracy': 0.7067999839782715, 'validation/loss': 1.3886761665344238, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9945118427276611, 'test/num_examples': 10000, 'score': 42047.39622235298, 'total_duration': 45510.83752632141, 'accumulated_submission_time': 42047.39622235298, 'accumulated_eval_time': 3454.4704039096832, 'accumulated_logging_time': 3.881521701812744}
I0201 07:48:29.644435 139923852027648 logging_writer.py:48] [91136] accumulated_eval_time=3454.470404, accumulated_logging_time=3.881522, accumulated_submission_time=42047.396222, global_step=91136, preemption_count=0, score=42047.396222, test/accuracy=0.582100, test/loss=1.994512, test/num_examples=10000, total_duration=45510.837526, train/accuracy=0.774863, train/loss=1.088864, validation/accuracy=0.706800, validation/loss=1.388676, validation/num_examples=50000
I0201 07:48:55.161050 139923868813056 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.2737888097763062, loss=4.580401420593262
I0201 07:49:41.322293 139923852027648 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.337020754814148, loss=3.453474760055542
I0201 07:50:28.144925 139923868813056 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.2672102451324463, loss=4.636327266693115
I0201 07:51:15.024694 139923852027648 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.188840627670288, loss=4.499086380004883
I0201 07:52:01.987315 139923868813056 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.3246710300445557, loss=3.4290831089019775
I0201 07:52:48.891683 139923852027648 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.4910866022109985, loss=3.33414363861084
I0201 07:53:35.812922 139923868813056 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.3114153146743774, loss=3.273101329803467
I0201 07:54:22.722035 139923852027648 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.298841118812561, loss=4.304313659667969
I0201 07:55:09.266523 139923868813056 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.4342780113220215, loss=3.2959048748016357
I0201 07:55:29.893871 140085747812160 spec.py:321] Evaluating on the training split.
I0201 07:55:40.357977 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 07:56:03.332450 140085747812160 spec.py:349] Evaluating on the test split.
I0201 07:56:04.968077 140085747812160 submission_runner.py:408] Time since start: 45966.19s, 	Step: 92046, 	{'train/accuracy': 0.7909570336341858, 'train/loss': 1.043149471282959, 'validation/accuracy': 0.7130199670791626, 'validation/loss': 1.3840327262878418, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 1.991669774055481, 'test/num_examples': 10000, 'score': 42467.585582733154, 'total_duration': 45966.19464612007, 'accumulated_submission_time': 42467.585582733154, 'accumulated_eval_time': 3489.5446133613586, 'accumulated_logging_time': 3.9240567684173584}
I0201 07:56:05.003001 139923852027648 logging_writer.py:48] [92046] accumulated_eval_time=3489.544613, accumulated_logging_time=3.924057, accumulated_submission_time=42467.585583, global_step=92046, preemption_count=0, score=42467.585583, test/accuracy=0.585800, test/loss=1.991670, test/num_examples=10000, total_duration=45966.194646, train/accuracy=0.790957, train/loss=1.043149, validation/accuracy=0.713020, validation/loss=1.384033, validation/num_examples=50000
I0201 07:56:26.611217 139923868813056 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.2665594816207886, loss=3.495988607406616
I0201 07:57:12.077141 139923852027648 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.323194980621338, loss=4.813529014587402
I0201 07:57:58.794239 139923868813056 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.3466002941131592, loss=3.2346019744873047
I0201 07:58:45.343403 139923852027648 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.2586699724197388, loss=3.416337013244629
I0201 07:59:32.155356 139923868813056 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.394106388092041, loss=3.164849281311035
I0201 08:00:18.797283 139923852027648 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.5031036138534546, loss=3.3035027980804443
I0201 08:01:05.415832 139923868813056 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.3793294429779053, loss=4.82048225402832
I0201 08:01:52.351679 139923852027648 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.2681915760040283, loss=3.508415460586548
I0201 08:02:39.304318 139923868813056 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.285573124885559, loss=3.337282180786133
I0201 08:03:05.163393 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:03:15.715411 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:03:39.532856 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:03:41.179852 140085747812160 submission_runner.py:408] Time since start: 46422.41s, 	Step: 92957, 	{'train/accuracy': 0.7769726514816284, 'train/loss': 1.0807150602340698, 'validation/accuracy': 0.7142399549484253, 'validation/loss': 1.3572258949279785, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.9467597007751465, 'test/num_examples': 10000, 'score': 42887.685487508774, 'total_duration': 46422.40642333031, 'accumulated_submission_time': 42887.685487508774, 'accumulated_eval_time': 3525.5610892772675, 'accumulated_logging_time': 3.9683220386505127}
I0201 08:03:41.210697 139923852027648 logging_writer.py:48] [92957] accumulated_eval_time=3525.561089, accumulated_logging_time=3.968322, accumulated_submission_time=42887.685488, global_step=92957, preemption_count=0, score=42887.685488, test/accuracy=0.595200, test/loss=1.946760, test/num_examples=10000, total_duration=46422.406423, train/accuracy=0.776973, train/loss=1.080715, validation/accuracy=0.714240, validation/loss=1.357226, validation/num_examples=50000
I0201 08:03:58.490863 139923868813056 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.426684021949768, loss=5.065610885620117
I0201 08:04:43.041493 139923852027648 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3762539625167847, loss=3.2119412422180176
I0201 08:05:29.807550 139923868813056 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.3098418712615967, loss=3.5107197761535645
I0201 08:06:16.192918 139923852027648 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.4346745014190674, loss=4.60969877243042
I0201 08:07:02.878613 139923868813056 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.3383326530456543, loss=3.1688361167907715
I0201 08:07:49.285521 139923852027648 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.4391937255859375, loss=3.2465627193450928
I0201 08:08:36.349369 139923868813056 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.271699070930481, loss=3.82643461227417
I0201 08:09:23.145246 139923852027648 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.4571067094802856, loss=3.2791059017181396
I0201 08:10:09.641207 139923868813056 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.2987079620361328, loss=4.669185161590576
I0201 08:10:41.462585 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:10:52.101925 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:11:13.710958 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:11:15.352292 140085747812160 submission_runner.py:408] Time since start: 46876.58s, 	Step: 93870, 	{'train/accuracy': 0.784863293170929, 'train/loss': 1.0583343505859375, 'validation/accuracy': 0.7152599692344666, 'validation/loss': 1.3549997806549072, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.9606016874313354, 'test/num_examples': 10000, 'score': 43307.87548875809, 'total_duration': 46876.57886219025, 'accumulated_submission_time': 43307.87548875809, 'accumulated_eval_time': 3559.4507846832275, 'accumulated_logging_time': 4.0101823806762695}
I0201 08:11:15.382266 139923852027648 logging_writer.py:48] [93870] accumulated_eval_time=3559.450785, accumulated_logging_time=4.010182, accumulated_submission_time=43307.875489, global_step=93870, preemption_count=0, score=43307.875489, test/accuracy=0.593000, test/loss=1.960602, test/num_examples=10000, total_duration=46876.578862, train/accuracy=0.784863, train/loss=1.058334, validation/accuracy=0.715260, validation/loss=1.355000, validation/num_examples=50000
I0201 08:11:27.555497 139923868813056 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.2484283447265625, loss=4.098675727844238
I0201 08:12:10.952275 139923852027648 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.372247338294983, loss=3.2114815711975098
I0201 08:12:57.700407 139923868813056 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.2893530130386353, loss=3.2828993797302246
I0201 08:13:44.657011 139923852027648 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.3356715440750122, loss=3.2861130237579346
I0201 08:14:31.289169 139923868813056 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.341221570968628, loss=3.3076364994049072
I0201 08:15:17.962757 139923852027648 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.255664587020874, loss=3.6512210369110107
I0201 08:16:04.762168 139923868813056 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.3623902797698975, loss=3.3116226196289062
I0201 08:16:51.411849 139923852027648 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.2801270484924316, loss=3.29228138923645
I0201 08:17:37.972720 139923868813056 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.3560494184494019, loss=4.81955623626709
I0201 08:18:15.514982 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:18:25.814746 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:18:49.061484 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:18:50.705000 140085747812160 submission_runner.py:408] Time since start: 47331.93s, 	Step: 94782, 	{'train/accuracy': 0.7880077958106995, 'train/loss': 1.056152582168579, 'validation/accuracy': 0.7136200070381165, 'validation/loss': 1.3756757974624634, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.9845054149627686, 'test/num_examples': 10000, 'score': 43727.9484167099, 'total_duration': 47331.931569337845, 'accumulated_submission_time': 43727.9484167099, 'accumulated_eval_time': 3594.6408054828644, 'accumulated_logging_time': 4.049016237258911}
I0201 08:18:50.742554 139923852027648 logging_writer.py:48] [94782] accumulated_eval_time=3594.640805, accumulated_logging_time=4.049016, accumulated_submission_time=43727.948417, global_step=94782, preemption_count=0, score=43727.948417, test/accuracy=0.588900, test/loss=1.984505, test/num_examples=10000, total_duration=47331.931569, train/accuracy=0.788008, train/loss=1.056153, validation/accuracy=0.713620, validation/loss=1.375676, validation/num_examples=50000
I0201 08:18:58.206853 139923868813056 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4140305519104004, loss=3.293790340423584
I0201 08:19:40.634449 139923852027648 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.271226406097412, loss=3.350438117980957
I0201 08:20:27.403221 139923868813056 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.3408095836639404, loss=3.3436357975006104
I0201 08:21:14.390722 139923852027648 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.333513617515564, loss=3.1936533451080322
I0201 08:22:00.875350 139923868813056 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.3926810026168823, loss=3.3785483837127686
I0201 08:22:47.756293 139923852027648 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.4857898950576782, loss=4.976571559906006
I0201 08:23:34.469975 139923868813056 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.3377771377563477, loss=4.502950191497803
I0201 08:24:21.088361 139923852027648 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.187338948249817, loss=3.8494415283203125
I0201 08:25:07.886554 139923868813056 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.3622064590454102, loss=3.254086971282959
I0201 08:25:50.839989 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:26:01.234474 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:26:23.959100 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:26:25.597966 140085747812160 submission_runner.py:408] Time since start: 47786.82s, 	Step: 95694, 	{'train/accuracy': 0.7802929282188416, 'train/loss': 1.084344744682312, 'validation/accuracy': 0.7172200083732605, 'validation/loss': 1.3568799495697021, 'validation/num_examples': 50000, 'test/accuracy': 0.5938000082969666, 'test/loss': 1.965236783027649, 'test/num_examples': 10000, 'score': 44147.98247885704, 'total_duration': 47786.82453203201, 'accumulated_submission_time': 44147.98247885704, 'accumulated_eval_time': 3629.39878821373, 'accumulated_logging_time': 4.098080635070801}
I0201 08:26:25.632162 139923852027648 logging_writer.py:48] [95694] accumulated_eval_time=3629.398788, accumulated_logging_time=4.098081, accumulated_submission_time=44147.982479, global_step=95694, preemption_count=0, score=44147.982479, test/accuracy=0.593800, test/loss=1.965237, test/num_examples=10000, total_duration=47786.824532, train/accuracy=0.780293, train/loss=1.084345, validation/accuracy=0.717220, validation/loss=1.356880, validation/num_examples=50000
I0201 08:26:28.389208 139923868813056 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3158409595489502, loss=3.185372829437256
I0201 08:27:10.224492 139923852027648 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3261828422546387, loss=3.4953887462615967
I0201 08:27:56.857724 139923868813056 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5310043096542358, loss=4.9209699630737305
I0201 08:28:43.752103 139923852027648 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.3607505559921265, loss=4.506300449371338
I0201 08:29:30.444294 139923868813056 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.4105198383331299, loss=3.2059082984924316
I0201 08:30:17.006529 139923852027648 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.391926884651184, loss=3.471996784210205
I0201 08:31:03.961872 139923868813056 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.366159439086914, loss=3.341378927230835
I0201 08:31:50.530076 139923852027648 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.5805097818374634, loss=4.860767364501953
I0201 08:32:37.346976 139923868813056 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.3387054204940796, loss=3.261993169784546
I0201 08:33:24.054430 139923852027648 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.34097421169281, loss=4.314184665679932
I0201 08:33:26.071787 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:33:36.285844 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:34:01.550833 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:34:03.191243 140085747812160 submission_runner.py:408] Time since start: 48244.42s, 	Step: 96606, 	{'train/accuracy': 0.7871288657188416, 'train/loss': 1.0491591691970825, 'validation/accuracy': 0.7164199948310852, 'validation/loss': 1.3515568971633911, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.9558128118515015, 'test/num_examples': 10000, 'score': 44568.36045074463, 'total_duration': 48244.41781044006, 'accumulated_submission_time': 44568.36045074463, 'accumulated_eval_time': 3666.5182325839996, 'accumulated_logging_time': 4.142136096954346}
I0201 08:34:03.225131 139923868813056 logging_writer.py:48] [96606] accumulated_eval_time=3666.518233, accumulated_logging_time=4.142136, accumulated_submission_time=44568.360451, global_step=96606, preemption_count=0, score=44568.360451, test/accuracy=0.592700, test/loss=1.955813, test/num_examples=10000, total_duration=48244.417810, train/accuracy=0.787129, train/loss=1.049159, validation/accuracy=0.716420, validation/loss=1.351557, validation/num_examples=50000
I0201 08:34:41.947835 139923852027648 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3754191398620605, loss=3.266364812850952
I0201 08:35:28.481776 139923868813056 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.2504372596740723, loss=4.160149097442627
I0201 08:36:15.590733 139923852027648 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.4254688024520874, loss=3.2910687923431396
I0201 08:37:02.178198 139923868813056 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.412410020828247, loss=3.3171510696411133
I0201 08:37:49.053324 139923852027648 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.275459885597229, loss=3.993534564971924
I0201 08:38:35.789925 139923868813056 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.3308730125427246, loss=4.359920978546143
I0201 08:39:22.629309 139923852027648 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.34449303150177, loss=3.241508960723877
I0201 08:40:09.243370 139923868813056 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.4065204858779907, loss=3.2551629543304443
I0201 08:40:56.147183 139923852027648 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.2747507095336914, loss=3.2584996223449707
I0201 08:41:03.329166 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:41:13.799394 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:41:37.191512 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:41:38.827126 140085747812160 submission_runner.py:408] Time since start: 48700.05s, 	Step: 97517, 	{'train/accuracy': 0.7954687476158142, 'train/loss': 1.0022492408752441, 'validation/accuracy': 0.7200999855995178, 'validation/loss': 1.323193907737732, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.929459810256958, 'test/num_examples': 10000, 'score': 44988.40380692482, 'total_duration': 48700.0536942482, 'accumulated_submission_time': 44988.40380692482, 'accumulated_eval_time': 3702.0162086486816, 'accumulated_logging_time': 4.1854517459869385}
I0201 08:41:38.858437 139923868813056 logging_writer.py:48] [97517] accumulated_eval_time=3702.016209, accumulated_logging_time=4.185452, accumulated_submission_time=44988.403807, global_step=97517, preemption_count=0, score=44988.403807, test/accuracy=0.593500, test/loss=1.929460, test/num_examples=10000, total_duration=48700.053694, train/accuracy=0.795469, train/loss=1.002249, validation/accuracy=0.720100, validation/loss=1.323194, validation/num_examples=50000
I0201 08:42:12.871047 139923852027648 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.3940973281860352, loss=3.287844657897949
I0201 08:42:59.369322 139923868813056 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.3442033529281616, loss=3.2222447395324707
I0201 08:43:46.212320 139923852027648 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.5230178833007812, loss=3.2358100414276123
I0201 08:44:32.986159 139923868813056 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.401525855064392, loss=3.3099896907806396
I0201 08:45:19.893081 139923852027648 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.2402271032333374, loss=3.1999382972717285
I0201 08:46:06.523568 139923868813056 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.490185022354126, loss=3.326017379760742
I0201 08:46:53.012820 139923852027648 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.404476284980774, loss=4.451183319091797
I0201 08:47:39.655853 139923868813056 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.2723469734191895, loss=3.4570889472961426
I0201 08:48:26.328476 139923852027648 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.4067442417144775, loss=3.2707626819610596
I0201 08:48:39.120234 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:48:49.288765 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:49:12.135885 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:49:13.781887 140085747812160 submission_runner.py:408] Time since start: 49155.01s, 	Step: 98429, 	{'train/accuracy': 0.7873241901397705, 'train/loss': 1.0458989143371582, 'validation/accuracy': 0.7177599668502808, 'validation/loss': 1.3348171710968018, 'validation/num_examples': 50000, 'test/accuracy': 0.5968000292778015, 'test/loss': 1.9434531927108765, 'test/num_examples': 10000, 'score': 45408.60495519638, 'total_duration': 49155.00846171379, 'accumulated_submission_time': 45408.60495519638, 'accumulated_eval_time': 3736.6778705120087, 'accumulated_logging_time': 4.226574182510376}
I0201 08:49:13.815083 139923868813056 logging_writer.py:48] [98429] accumulated_eval_time=3736.677871, accumulated_logging_time=4.226574, accumulated_submission_time=45408.604955, global_step=98429, preemption_count=0, score=45408.604955, test/accuracy=0.596800, test/loss=1.943453, test/num_examples=10000, total_duration=49155.008462, train/accuracy=0.787324, train/loss=1.045899, validation/accuracy=0.717760, validation/loss=1.334817, validation/num_examples=50000
I0201 08:49:42.105390 139923852027648 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.3553905487060547, loss=3.1150102615356445
I0201 08:50:28.723323 139923868813056 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.5856873989105225, loss=4.85342264175415
I0201 08:51:15.511363 139923852027648 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.3293845653533936, loss=3.191880941390991
I0201 08:52:02.080117 139923868813056 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.4059549570083618, loss=3.2950074672698975
I0201 08:52:48.865694 139923852027648 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.4569648504257202, loss=3.2503445148468018
I0201 08:53:35.478150 139923868813056 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.3222758769989014, loss=3.184925079345703
I0201 08:54:22.257057 139923852027648 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.4542686939239502, loss=4.276586532592773
I0201 08:55:08.901913 139923868813056 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.2275713682174683, loss=4.029261112213135
I0201 08:55:55.563778 139923852027648 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.3983590602874756, loss=3.2523796558380127
I0201 08:56:13.896049 140085747812160 spec.py:321] Evaluating on the training split.
I0201 08:56:24.095528 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 08:56:46.784249 140085747812160 spec.py:349] Evaluating on the test split.
I0201 08:56:48.421337 140085747812160 submission_runner.py:408] Time since start: 49609.65s, 	Step: 99341, 	{'train/accuracy': 0.7878515720367432, 'train/loss': 1.0436352491378784, 'validation/accuracy': 0.7171199917793274, 'validation/loss': 1.3446561098098755, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.933828592300415, 'test/num_examples': 10000, 'score': 45828.625912189484, 'total_duration': 49609.64790344238, 'accumulated_submission_time': 45828.625912189484, 'accumulated_eval_time': 3771.2031738758087, 'accumulated_logging_time': 4.268864870071411}
I0201 08:56:48.452894 139923868813056 logging_writer.py:48] [99341] accumulated_eval_time=3771.203174, accumulated_logging_time=4.268865, accumulated_submission_time=45828.625912, global_step=99341, preemption_count=0, score=45828.625912, test/accuracy=0.604100, test/loss=1.933829, test/num_examples=10000, total_duration=49609.647903, train/accuracy=0.787852, train/loss=1.043635, validation/accuracy=0.717120, validation/loss=1.344656, validation/num_examples=50000
I0201 08:57:12.015253 139923852027648 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.3243502378463745, loss=3.79085373878479
I0201 08:57:57.441150 139923868813056 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.5120930671691895, loss=4.992303371429443
I0201 08:58:44.394601 139923852027648 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.2995920181274414, loss=3.337747573852539
I0201 08:59:30.877858 139923868813056 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.274175763130188, loss=3.5042569637298584
I0201 09:00:17.683098 139923852027648 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.4210766553878784, loss=3.4001758098602295
I0201 09:01:04.314586 139923868813056 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.3425430059432983, loss=3.2467751502990723
I0201 09:01:51.093531 139923852027648 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.3940861225128174, loss=3.225048303604126
I0201 09:02:38.133937 139923868813056 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.4161133766174316, loss=3.2938249111175537
I0201 09:03:24.550959 139923852027648 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.2768831253051758, loss=4.008269786834717
I0201 09:03:48.507614 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:03:58.829386 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:04:21.279113 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:04:22.927618 140085747812160 submission_runner.py:408] Time since start: 50064.15s, 	Step: 100253, 	{'train/accuracy': 0.7922070026397705, 'train/loss': 1.0412719249725342, 'validation/accuracy': 0.7170599699020386, 'validation/loss': 1.350500226020813, 'validation/num_examples': 50000, 'test/accuracy': 0.5974000096321106, 'test/loss': 1.9491338729858398, 'test/num_examples': 10000, 'score': 46248.61977934837, 'total_duration': 50064.15418791771, 'accumulated_submission_time': 46248.61977934837, 'accumulated_eval_time': 3805.6231784820557, 'accumulated_logging_time': 4.309793710708618}
I0201 09:04:22.958709 139923868813056 logging_writer.py:48] [100253] accumulated_eval_time=3805.623178, accumulated_logging_time=4.309794, accumulated_submission_time=46248.619779, global_step=100253, preemption_count=0, score=46248.619779, test/accuracy=0.597400, test/loss=1.949134, test/num_examples=10000, total_duration=50064.154188, train/accuracy=0.792207, train/loss=1.041272, validation/accuracy=0.717060, validation/loss=1.350500, validation/num_examples=50000
I0201 09:04:41.813717 139923852027648 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4596457481384277, loss=3.269174814224243
I0201 09:05:26.468665 139923868813056 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.4543598890304565, loss=3.314582109451294
I0201 09:06:13.420732 139923852027648 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.3369035720825195, loss=4.3444623947143555
I0201 09:07:00.003471 139923868813056 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.350170373916626, loss=3.9099459648132324
I0201 09:07:46.805851 139923852027648 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.4436531066894531, loss=3.9341681003570557
I0201 09:08:33.595450 139923868813056 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.3615001440048218, loss=3.9451911449432373
I0201 09:09:20.201189 139923852027648 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3397035598754883, loss=4.210091590881348
I0201 09:10:07.083448 139923868813056 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.3845022916793823, loss=3.260427474975586
I0201 09:10:53.813891 139923852027648 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.4793237447738647, loss=3.3111462593078613
I0201 09:11:23.034869 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:11:33.359453 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:11:57.314640 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:11:58.962661 140085747812160 submission_runner.py:408] Time since start: 50520.19s, 	Step: 101164, 	{'train/accuracy': 0.8041796684265137, 'train/loss': 1.0106101036071777, 'validation/accuracy': 0.7200999855995178, 'validation/loss': 1.3571895360946655, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9695676565170288, 'test/num_examples': 10000, 'score': 46668.6330909729, 'total_duration': 50520.18922710419, 'accumulated_submission_time': 46668.6330909729, 'accumulated_eval_time': 3841.550982236862, 'accumulated_logging_time': 4.3530638217926025}
I0201 09:11:58.998988 139923868813056 logging_writer.py:48] [101164] accumulated_eval_time=3841.550982, accumulated_logging_time=4.353064, accumulated_submission_time=46668.633091, global_step=101164, preemption_count=0, score=46668.633091, test/accuracy=0.599500, test/loss=1.969568, test/num_examples=10000, total_duration=50520.189227, train/accuracy=0.804180, train/loss=1.010610, validation/accuracy=0.720100, validation/loss=1.357190, validation/num_examples=50000
I0201 09:12:13.539595 139923852027648 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.4364395141601562, loss=3.126631736755371
I0201 09:12:57.739434 139923868813056 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.385459542274475, loss=3.4274702072143555
I0201 09:13:44.692488 139923852027648 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.4572432041168213, loss=3.1920056343078613
I0201 09:14:31.697433 139923868813056 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.4252885580062866, loss=3.4031999111175537
I0201 09:15:18.187969 139923852027648 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.416634440422058, loss=3.4366233348846436
I0201 09:16:04.977937 139923868813056 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.5205199718475342, loss=3.7125558853149414
I0201 09:16:51.425989 139923852027648 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.4029985666275024, loss=4.897826194763184
I0201 09:17:38.138227 139923868813056 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.560288429260254, loss=4.711946487426758
I0201 09:18:24.763485 139923852027648 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.4390225410461426, loss=3.4277946949005127
I0201 09:18:59.349362 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:19:09.767645 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:19:35.123121 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:19:36.749417 140085747812160 submission_runner.py:408] Time since start: 50977.98s, 	Step: 102076, 	{'train/accuracy': 0.7922070026397705, 'train/loss': 1.0534024238586426, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.3498034477233887, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.9485112428665161, 'test/num_examples': 10000, 'score': 47088.92100191116, 'total_duration': 50977.975981235504, 'accumulated_submission_time': 47088.92100191116, 'accumulated_eval_time': 3878.951035261154, 'accumulated_logging_time': 4.401503562927246}
I0201 09:19:36.781356 139923868813056 logging_writer.py:48] [102076] accumulated_eval_time=3878.951035, accumulated_logging_time=4.401504, accumulated_submission_time=47088.921002, global_step=102076, preemption_count=0, score=47088.921002, test/accuracy=0.599100, test/loss=1.948511, test/num_examples=10000, total_duration=50977.975981, train/accuracy=0.792207, train/loss=1.053402, validation/accuracy=0.721800, validation/loss=1.349803, validation/num_examples=50000
I0201 09:19:46.607864 139923852027648 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.5136674642562866, loss=3.282273530960083
I0201 09:20:29.738392 139923868813056 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.433106541633606, loss=3.3359532356262207
I0201 09:21:16.401913 139923852027648 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.416759729385376, loss=3.254058599472046
I0201 09:22:03.019630 139923868813056 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.4232546091079712, loss=3.0398330688476562
I0201 09:22:49.880129 139923852027648 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.5515791177749634, loss=4.787806510925293
I0201 09:23:36.576329 139923868813056 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.6146206855773926, loss=3.245938539505005
I0201 09:24:23.185198 139923852027648 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.4391628503799438, loss=4.319848537445068
I0201 09:25:09.868067 139923868813056 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.561501383781433, loss=4.823719024658203
I0201 09:25:56.635007 139923852027648 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.4278757572174072, loss=3.860311508178711
I0201 09:26:36.851084 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:26:47.135546 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:27:12.779736 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:27:14.411647 140085747812160 submission_runner.py:408] Time since start: 51435.64s, 	Step: 102988, 	{'train/accuracy': 0.79798823595047, 'train/loss': 1.0308090448379517, 'validation/accuracy': 0.7218199968338013, 'validation/loss': 1.3451817035675049, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.9463989734649658, 'test/num_examples': 10000, 'score': 47508.928409576416, 'total_duration': 51435.638216257095, 'accumulated_submission_time': 47508.928409576416, 'accumulated_eval_time': 3916.5116069316864, 'accumulated_logging_time': 4.4449450969696045}
I0201 09:27:14.444490 139923868813056 logging_writer.py:48] [102988] accumulated_eval_time=3916.511607, accumulated_logging_time=4.444945, accumulated_submission_time=47508.928410, global_step=102988, preemption_count=0, score=47508.928410, test/accuracy=0.596900, test/loss=1.946399, test/num_examples=10000, total_duration=51435.638216, train/accuracy=0.797988, train/loss=1.030809, validation/accuracy=0.721820, validation/loss=1.345182, validation/num_examples=50000
I0201 09:27:19.559222 139923852027648 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.9081906080245972, loss=4.892941474914551
I0201 09:28:01.956789 139923868813056 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.478764533996582, loss=3.2950127124786377
I0201 09:28:48.690988 139923852027648 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.3680784702301025, loss=3.8956921100616455
I0201 09:29:35.591805 139923868813056 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.4414228200912476, loss=3.149671792984009
I0201 09:30:22.443917 139923852027648 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.3881278038024902, loss=3.188272476196289
I0201 09:31:09.218806 139923868813056 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.3794002532958984, loss=3.229630708694458
I0201 09:31:56.103240 139923852027648 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.3323783874511719, loss=4.027294158935547
I0201 09:32:42.921125 139923868813056 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.5289667844772339, loss=3.145376443862915
I0201 09:33:29.817180 139923852027648 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.5582729578018188, loss=3.252006769180298
I0201 09:34:14.852886 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:34:25.995409 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:34:48.063382 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:34:49.698472 140085747812160 submission_runner.py:408] Time since start: 51890.93s, 	Step: 103898, 	{'train/accuracy': 0.8116992115974426, 'train/loss': 0.9391869306564331, 'validation/accuracy': 0.7234399914741516, 'validation/loss': 1.3024083375930786, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.9161393642425537, 'test/num_examples': 10000, 'score': 47929.27530050278, 'total_duration': 51890.92503976822, 'accumulated_submission_time': 47929.27530050278, 'accumulated_eval_time': 3951.357241153717, 'accumulated_logging_time': 4.487648963928223}
I0201 09:34:49.732403 139923868813056 logging_writer.py:48] [103898] accumulated_eval_time=3951.357241, accumulated_logging_time=4.487649, accumulated_submission_time=47929.275301, global_step=103898, preemption_count=0, score=47929.275301, test/accuracy=0.603700, test/loss=1.916139, test/num_examples=10000, total_duration=51890.925040, train/accuracy=0.811699, train/loss=0.939187, validation/accuracy=0.723440, validation/loss=1.302408, validation/num_examples=50000
I0201 09:34:50.910659 139923852027648 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.4224467277526855, loss=3.226322889328003
I0201 09:35:32.091077 139923868813056 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.5053566694259644, loss=4.5503435134887695
I0201 09:36:18.704708 139923852027648 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.4467408657073975, loss=3.229506254196167
I0201 09:37:05.640528 139923868813056 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.416162371635437, loss=3.4131886959075928
I0201 09:37:52.096020 139923852027648 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.5789183378219604, loss=3.178990602493286
I0201 09:38:38.692319 139923868813056 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.5174833536148071, loss=4.610660552978516
I0201 09:39:25.210757 139923852027648 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.4648486375808716, loss=3.241861581802368
I0201 09:40:11.918635 139923868813056 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.4472126960754395, loss=3.357107400894165
I0201 09:40:58.460472 139923852027648 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.6554428339004517, loss=3.189922332763672
I0201 09:41:45.191555 139923868813056 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.3622713088989258, loss=3.748464822769165
I0201 09:41:49.937305 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:42:00.935442 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:42:22.871471 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:42:24.522216 140085747812160 submission_runner.py:408] Time since start: 52345.75s, 	Step: 104812, 	{'train/accuracy': 0.7913671731948853, 'train/loss': 1.040604829788208, 'validation/accuracy': 0.7239199876785278, 'validation/loss': 1.3334083557128906, 'validation/num_examples': 50000, 'test/accuracy': 0.605400025844574, 'test/loss': 1.936792254447937, 'test/num_examples': 10000, 'score': 48349.42050933838, 'total_duration': 52345.748777627945, 'accumulated_submission_time': 48349.42050933838, 'accumulated_eval_time': 3985.942140340805, 'accumulated_logging_time': 4.530482769012451}
I0201 09:42:24.553577 139923852027648 logging_writer.py:48] [104812] accumulated_eval_time=3985.942140, accumulated_logging_time=4.530483, accumulated_submission_time=48349.420509, global_step=104812, preemption_count=0, score=48349.420509, test/accuracy=0.605400, test/loss=1.936792, test/num_examples=10000, total_duration=52345.748778, train/accuracy=0.791367, train/loss=1.040605, validation/accuracy=0.723920, validation/loss=1.333408, validation/num_examples=50000
I0201 09:43:00.696644 139923868813056 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4982696771621704, loss=3.2121965885162354
I0201 09:43:47.217334 139923852027648 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.4290664196014404, loss=3.247483491897583
I0201 09:44:34.314811 139923868813056 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.3907124996185303, loss=3.2286250591278076
I0201 09:45:20.593576 139923852027648 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.4300487041473389, loss=4.57145881652832
I0201 09:46:07.212216 139923868813056 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3585389852523804, loss=3.477912664413452
I0201 09:46:53.753485 139923852027648 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.4512628316879272, loss=3.5399394035339355
I0201 09:47:40.516764 139923868813056 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.4588623046875, loss=3.1654951572418213
I0201 09:48:27.015498 139923852027648 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.4175745248794556, loss=3.1990203857421875
I0201 09:49:13.710162 139923868813056 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.3457952737808228, loss=4.047626972198486
I0201 09:49:24.609200 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:49:35.008750 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:49:59.832974 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:50:01.479317 140085747812160 submission_runner.py:408] Time since start: 52802.71s, 	Step: 105725, 	{'train/accuracy': 0.7983007431030273, 'train/loss': 1.0020376443862915, 'validation/accuracy': 0.7238799929618835, 'validation/loss': 1.3144477605819702, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9204466342926025, 'test/num_examples': 10000, 'score': 48769.416988134384, 'total_duration': 52802.70588493347, 'accumulated_submission_time': 48769.416988134384, 'accumulated_eval_time': 4022.8122441768646, 'accumulated_logging_time': 4.570789575576782}
I0201 09:50:01.513225 139923852027648 logging_writer.py:48] [105725] accumulated_eval_time=4022.812244, accumulated_logging_time=4.570790, accumulated_submission_time=48769.416988, global_step=105725, preemption_count=0, score=48769.416988, test/accuracy=0.596300, test/loss=1.920447, test/num_examples=10000, total_duration=52802.705885, train/accuracy=0.798301, train/loss=1.002038, validation/accuracy=0.723880, validation/loss=1.314448, validation/num_examples=50000
I0201 09:50:32.143558 139923868813056 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.5716875791549683, loss=3.2519659996032715
I0201 09:51:18.927119 139923852027648 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.5269352197647095, loss=4.399435520172119
I0201 09:52:05.298885 139923868813056 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.5430763959884644, loss=4.461462020874023
I0201 09:52:52.171732 139923852027648 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.6478559970855713, loss=3.239018440246582
I0201 09:53:39.160431 139923868813056 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.3604333400726318, loss=3.3404922485351562
I0201 09:54:25.952934 139923852027648 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.7038235664367676, loss=4.891693592071533
I0201 09:55:12.998645 139923868813056 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.5095328092575073, loss=3.2712483406066895
I0201 09:55:59.669151 139923852027648 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.520266056060791, loss=3.505561590194702
I0201 09:56:46.284394 139923868813056 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.4191479682922363, loss=3.221256732940674
I0201 09:57:01.849357 140085747812160 spec.py:321] Evaluating on the training split.
I0201 09:57:12.358050 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 09:57:35.830347 140085747812160 spec.py:349] Evaluating on the test split.
I0201 09:57:37.470610 140085747812160 submission_runner.py:408] Time since start: 53258.70s, 	Step: 106635, 	{'train/accuracy': 0.8101366758346558, 'train/loss': 0.9570842385292053, 'validation/accuracy': 0.725820004940033, 'validation/loss': 1.2992676496505737, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9089785814285278, 'test/num_examples': 10000, 'score': 49189.342555999756, 'total_duration': 53258.69718146324, 'accumulated_submission_time': 49189.342555999756, 'accumulated_eval_time': 4058.433498620987, 'accumulated_logging_time': 4.963132858276367}
I0201 09:57:37.506278 139923852027648 logging_writer.py:48] [106635] accumulated_eval_time=4058.433499, accumulated_logging_time=4.963133, accumulated_submission_time=49189.342556, global_step=106635, preemption_count=0, score=49189.342556, test/accuracy=0.599500, test/loss=1.908979, test/num_examples=10000, total_duration=53258.697181, train/accuracy=0.810137, train/loss=0.957084, validation/accuracy=0.725820, validation/loss=1.299268, validation/num_examples=50000
I0201 09:58:03.421437 139923868813056 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.4645886421203613, loss=3.19748592376709
I0201 09:58:49.633130 139923852027648 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.640712022781372, loss=3.312699794769287
I0201 09:59:36.648555 139923868813056 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.3729052543640137, loss=3.908735990524292
I0201 10:00:23.435203 139923852027648 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.4883043766021729, loss=3.4953067302703857
I0201 10:01:10.497765 139923868813056 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.5270116329193115, loss=3.2322754859924316
I0201 10:01:57.110611 139923852027648 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.425835132598877, loss=3.159677028656006
I0201 10:02:44.026951 139923868813056 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.4389134645462036, loss=3.2824866771698
I0201 10:03:31.050028 139923852027648 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.48196542263031, loss=3.381467342376709
I0201 10:04:17.963162 139923868813056 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.4569716453552246, loss=3.1800527572631836
I0201 10:04:37.814831 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:04:48.142532 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:05:12.627139 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:05:14.268619 140085747812160 submission_runner.py:408] Time since start: 53715.50s, 	Step: 107544, 	{'train/accuracy': 0.7962890267372131, 'train/loss': 1.0105478763580322, 'validation/accuracy': 0.7277799844741821, 'validation/loss': 1.2957388162612915, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.9033678770065308, 'test/num_examples': 10000, 'score': 49609.5889275074, 'total_duration': 53715.495185136795, 'accumulated_submission_time': 49609.5889275074, 'accumulated_eval_time': 4094.8872702121735, 'accumulated_logging_time': 5.010452508926392}
I0201 10:05:14.301491 139923852027648 logging_writer.py:48] [107544] accumulated_eval_time=4094.887270, accumulated_logging_time=5.010453, accumulated_submission_time=49609.588928, global_step=107544, preemption_count=0, score=49609.588928, test/accuracy=0.605000, test/loss=1.903368, test/num_examples=10000, total_duration=53715.495185, train/accuracy=0.796289, train/loss=1.010548, validation/accuracy=0.727780, validation/loss=1.295739, validation/num_examples=50000
I0201 10:05:36.680049 139923868813056 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.3552091121673584, loss=3.0994298458099365
I0201 10:06:22.193264 139923852027648 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.4388095140457153, loss=3.260937213897705
I0201 10:07:08.785771 139923868813056 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.5062851905822754, loss=3.2107231616973877
I0201 10:07:55.829924 139923852027648 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.4145228862762451, loss=4.01094388961792
I0201 10:08:42.477695 139923868813056 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.4037482738494873, loss=3.190197229385376
I0201 10:09:29.254356 139923852027648 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.3735883235931396, loss=3.5742578506469727
I0201 10:10:16.050497 139923868813056 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.5508360862731934, loss=4.829259395599365
I0201 10:11:02.924667 139923852027648 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.4889582395553589, loss=3.1836166381835938
I0201 10:11:49.412332 139923868813056 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.6038541793823242, loss=3.1932692527770996
I0201 10:12:14.411213 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:12:24.569215 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:12:50.644522 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:12:52.290579 140085747812160 submission_runner.py:408] Time since start: 54173.52s, 	Step: 108455, 	{'train/accuracy': 0.802539050579071, 'train/loss': 0.9898586273193359, 'validation/accuracy': 0.7298199534416199, 'validation/loss': 1.2934014797210693, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.8905043601989746, 'test/num_examples': 10000, 'score': 50029.638154029846, 'total_duration': 54173.51714682579, 'accumulated_submission_time': 50029.638154029846, 'accumulated_eval_time': 4132.766643047333, 'accumulated_logging_time': 5.052933216094971}
I0201 10:12:52.326133 139923852027648 logging_writer.py:48] [108455] accumulated_eval_time=4132.766643, accumulated_logging_time=5.052933, accumulated_submission_time=50029.638154, global_step=108455, preemption_count=0, score=50029.638154, test/accuracy=0.609400, test/loss=1.890504, test/num_examples=10000, total_duration=54173.517147, train/accuracy=0.802539, train/loss=0.989859, validation/accuracy=0.729820, validation/loss=1.293401, validation/num_examples=50000
I0201 10:13:10.393993 139923868813056 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.583729863166809, loss=4.689146518707275
I0201 10:13:55.233180 139923852027648 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.4290096759796143, loss=4.035425662994385
I0201 10:14:42.120672 139923868813056 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.5591044425964355, loss=3.1627798080444336
I0201 10:15:28.733268 139923852027648 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.5596873760223389, loss=3.1239867210388184
I0201 10:16:15.777024 139923868813056 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.5654352903366089, loss=3.2488086223602295
I0201 10:17:02.480875 139923852027648 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.4673566818237305, loss=3.368476390838623
I0201 10:17:49.096327 139923868813056 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.3850271701812744, loss=3.254668951034546
I0201 10:18:36.006289 139923852027648 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.6204322576522827, loss=3.4022114276885986
I0201 10:19:22.931539 139923868813056 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.4426873922348022, loss=3.9461047649383545
I0201 10:19:52.438169 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:20:02.888502 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:20:29.452712 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:20:31.095531 140085747812160 submission_runner.py:408] Time since start: 54632.32s, 	Step: 109365, 	{'train/accuracy': 0.8080468773841858, 'train/loss': 0.9750061631202698, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.2988848686218262, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.9144893884658813, 'test/num_examples': 10000, 'score': 50449.68918180466, 'total_duration': 54632.3221013546, 'accumulated_submission_time': 50449.68918180466, 'accumulated_eval_time': 4171.423996925354, 'accumulated_logging_time': 5.098599672317505}
I0201 10:20:31.129428 139923852027648 logging_writer.py:48] [109365] accumulated_eval_time=4171.423997, accumulated_logging_time=5.098600, accumulated_submission_time=50449.689182, global_step=109365, preemption_count=0, score=50449.689182, test/accuracy=0.603400, test/loss=1.914489, test/num_examples=10000, total_duration=54632.322101, train/accuracy=0.808047, train/loss=0.975006, validation/accuracy=0.730120, validation/loss=1.298885, validation/num_examples=50000
I0201 10:20:45.259812 139923868813056 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.5103425979614258, loss=3.121293306350708
I0201 10:21:29.156870 139923852027648 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.435634970664978, loss=4.062489986419678
I0201 10:22:15.977358 139923868813056 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.512087106704712, loss=3.2617390155792236
I0201 10:23:02.959030 139923852027648 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.385880470275879, loss=3.715804100036621
I0201 10:23:50.010333 139923868813056 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.4879144430160522, loss=3.120779514312744
I0201 10:24:36.856016 139923852027648 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.6387687921524048, loss=3.153074026107788
I0201 10:25:23.845040 139923868813056 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.3958001136779785, loss=3.655754566192627
I0201 10:26:10.901917 139923852027648 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.6870707273483276, loss=4.444011688232422
I0201 10:26:57.394241 139923868813056 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.3941835165023804, loss=3.149355888366699
I0201 10:27:31.212631 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:27:41.678079 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:28:06.095724 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:28:07.739400 140085747812160 submission_runner.py:408] Time since start: 55088.97s, 	Step: 110274, 	{'train/accuracy': 0.7994921803474426, 'train/loss': 1.0312963724136353, 'validation/accuracy': 0.7298199534416199, 'validation/loss': 1.3248251676559448, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.9229018688201904, 'test/num_examples': 10000, 'score': 50869.71073126793, 'total_duration': 55088.965970516205, 'accumulated_submission_time': 50869.71073126793, 'accumulated_eval_time': 4207.95077753067, 'accumulated_logging_time': 5.143129587173462}
I0201 10:28:07.774907 139923852027648 logging_writer.py:48] [110274] accumulated_eval_time=4207.950778, accumulated_logging_time=5.143130, accumulated_submission_time=50869.710731, global_step=110274, preemption_count=0, score=50869.710731, test/accuracy=0.603800, test/loss=1.922902, test/num_examples=10000, total_duration=55088.965971, train/accuracy=0.799492, train/loss=1.031296, validation/accuracy=0.729820, validation/loss=1.324825, validation/num_examples=50000
I0201 10:28:18.371167 139923868813056 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.5314303636550903, loss=4.590821266174316
I0201 10:29:01.556641 139923852027648 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.5859745740890503, loss=4.573056697845459
I0201 10:29:48.498288 139923868813056 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.4729797840118408, loss=4.274545669555664
I0201 10:30:35.353890 139923852027648 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.5124998092651367, loss=3.136470079421997
I0201 10:31:22.124117 139923868813056 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.5821524858474731, loss=3.150224208831787
I0201 10:32:08.964386 139923852027648 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.5007634162902832, loss=3.083662509918213
I0201 10:32:55.737873 139923868813056 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.5126910209655762, loss=3.383932590484619
I0201 10:33:42.393495 139923852027648 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.5116108655929565, loss=3.2621548175811768
I0201 10:34:29.383944 139923868813056 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.7065597772598267, loss=4.851342678070068
I0201 10:35:08.024146 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:35:18.587118 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:35:43.027861 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:35:44.672925 140085747812160 submission_runner.py:408] Time since start: 55545.90s, 	Step: 111184, 	{'train/accuracy': 0.8047069907188416, 'train/loss': 0.9976529479026794, 'validation/accuracy': 0.7334799766540527, 'validation/loss': 1.2964755296707153, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.9121514558792114, 'test/num_examples': 10000, 'score': 51289.89997005463, 'total_duration': 55545.89949464798, 'accumulated_submission_time': 51289.89997005463, 'accumulated_eval_time': 4244.599578619003, 'accumulated_logging_time': 5.188409090042114}
I0201 10:35:44.711961 139923852027648 logging_writer.py:48] [111184] accumulated_eval_time=4244.599579, accumulated_logging_time=5.188409, accumulated_submission_time=51289.899970, global_step=111184, preemption_count=0, score=51289.899970, test/accuracy=0.606600, test/loss=1.912151, test/num_examples=10000, total_duration=55545.899495, train/accuracy=0.804707, train/loss=0.997653, validation/accuracy=0.733480, validation/loss=1.296476, validation/num_examples=50000
I0201 10:35:51.380859 139923868813056 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.7801636457443237, loss=4.761680603027344
I0201 10:36:33.996868 139923852027648 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.5230457782745361, loss=3.5547690391540527
I0201 10:37:20.890624 139923868813056 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.471143126487732, loss=3.7745890617370605
I0201 10:38:08.039958 139923852027648 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.5298975706100464, loss=3.1303906440734863
I0201 10:38:54.601081 139923868813056 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.474162220954895, loss=3.9690260887145996
I0201 10:39:41.434096 139923852027648 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.5632044076919556, loss=3.3254075050354004
I0201 10:40:28.325914 139923868813056 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.710070252418518, loss=4.87379789352417
I0201 10:41:15.151130 139923852027648 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.6637977361679077, loss=3.9740054607391357
I0201 10:42:01.874944 139923868813056 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.5011072158813477, loss=3.2170372009277344
I0201 10:42:44.717182 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:42:54.942905 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:43:20.459985 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:43:22.143585 140085747812160 submission_runner.py:408] Time since start: 56003.37s, 	Step: 112093, 	{'train/accuracy': 0.8110741972923279, 'train/loss': 0.9474124908447266, 'validation/accuracy': 0.7343599796295166, 'validation/loss': 1.2785613536834717, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.8893661499023438, 'test/num_examples': 10000, 'score': 51709.84427022934, 'total_duration': 56003.3701505661, 'accumulated_submission_time': 51709.84427022934, 'accumulated_eval_time': 4282.025975942612, 'accumulated_logging_time': 5.237752914428711}
I0201 10:43:22.177818 139923852027648 logging_writer.py:48] [112093] accumulated_eval_time=4282.025976, accumulated_logging_time=5.237753, accumulated_submission_time=51709.844270, global_step=112093, preemption_count=0, score=51709.844270, test/accuracy=0.610600, test/loss=1.889366, test/num_examples=10000, total_duration=56003.370151, train/accuracy=0.811074, train/loss=0.947412, validation/accuracy=0.734360, validation/loss=1.278561, validation/num_examples=50000
I0201 10:43:25.315927 139923868813056 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.4688409566879272, loss=4.0871429443359375
I0201 10:44:07.279727 139923852027648 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.5174912214279175, loss=4.72099494934082
I0201 10:44:53.834593 139923868813056 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.3923741579055786, loss=3.681976556777954
I0201 10:45:41.277945 139923852027648 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.6491060256958008, loss=3.3404345512390137
I0201 10:46:27.843168 139923868813056 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.4990370273590088, loss=3.2005815505981445
I0201 10:47:15.332291 139923852027648 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.7042670249938965, loss=4.0755157470703125
I0201 10:48:01.665314 139923868813056 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.4602018594741821, loss=3.1506567001342773
I0201 10:48:48.566092 139923852027648 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.7188750505447388, loss=4.231905937194824
I0201 10:49:35.442166 139923868813056 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.611459732055664, loss=3.2688469886779785
I0201 10:50:22.256710 139923852027648 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.6905516386032104, loss=4.50777530670166
I0201 10:50:22.273050 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:50:32.698705 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:50:55.889549 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:50:57.526408 140085747812160 submission_runner.py:408] Time since start: 56458.75s, 	Step: 113001, 	{'train/accuracy': 0.8006640672683716, 'train/loss': 0.9824928045272827, 'validation/accuracy': 0.7322399616241455, 'validation/loss': 1.2828460931777954, 'validation/num_examples': 50000, 'test/accuracy': 0.6092000007629395, 'test/loss': 1.8907074928283691, 'test/num_examples': 10000, 'score': 52129.879022836685, 'total_duration': 56458.75296974182, 'accumulated_submission_time': 52129.879022836685, 'accumulated_eval_time': 4317.279319286346, 'accumulated_logging_time': 5.281320095062256}
I0201 10:50:57.563095 139923868813056 logging_writer.py:48] [113001] accumulated_eval_time=4317.279319, accumulated_logging_time=5.281320, accumulated_submission_time=52129.879023, global_step=113001, preemption_count=0, score=52129.879023, test/accuracy=0.609200, test/loss=1.890707, test/num_examples=10000, total_duration=56458.752970, train/accuracy=0.800664, train/loss=0.982493, validation/accuracy=0.732240, validation/loss=1.282846, validation/num_examples=50000
I0201 10:51:38.773664 139923852027648 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.6394962072372437, loss=3.269336700439453
I0201 10:52:25.585164 139923868813056 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.5154379606246948, loss=4.048202037811279
I0201 10:53:12.531155 139923852027648 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.5334275960922241, loss=3.2183165550231934
I0201 10:53:59.173812 139923868813056 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.4615890979766846, loss=3.1527764797210693
I0201 10:54:46.031599 139923852027648 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.568544864654541, loss=3.275784492492676
I0201 10:55:32.666906 139923868813056 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.6154752969741821, loss=3.1215097904205322
I0201 10:56:19.472728 139923852027648 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.729012370109558, loss=3.124049663543701
I0201 10:57:06.120691 139923868813056 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.5503835678100586, loss=3.1941447257995605
I0201 10:57:52.882276 139923852027648 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.5723164081573486, loss=4.439095497131348
I0201 10:57:57.605580 140085747812160 spec.py:321] Evaluating on the training split.
I0201 10:58:08.084271 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 10:58:31.059223 140085747812160 spec.py:349] Evaluating on the test split.
I0201 10:58:32.711195 140085747812160 submission_runner.py:408] Time since start: 56913.94s, 	Step: 113912, 	{'train/accuracy': 0.8100976347923279, 'train/loss': 0.9563019871711731, 'validation/accuracy': 0.7339800000190735, 'validation/loss': 1.2726399898529053, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.8746048212051392, 'test/num_examples': 10000, 'score': 52549.861157894135, 'total_duration': 56913.93776369095, 'accumulated_submission_time': 52549.861157894135, 'accumulated_eval_time': 4352.38493680954, 'accumulated_logging_time': 5.327167272567749}
I0201 10:58:32.744436 139923868813056 logging_writer.py:48] [113912] accumulated_eval_time=4352.384937, accumulated_logging_time=5.327167, accumulated_submission_time=52549.861158, global_step=113912, preemption_count=0, score=52549.861158, test/accuracy=0.609800, test/loss=1.874605, test/num_examples=10000, total_duration=56913.937764, train/accuracy=0.810098, train/loss=0.956302, validation/accuracy=0.733980, validation/loss=1.272640, validation/num_examples=50000
I0201 10:59:08.993166 139923852027648 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.6392619609832764, loss=4.524611949920654
I0201 10:59:55.794614 139923868813056 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.4931427240371704, loss=3.1590123176574707
I0201 11:00:42.874773 139923852027648 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.4761593341827393, loss=3.7309675216674805
I0201 11:01:29.395369 139923868813056 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.5554876327514648, loss=4.296082496643066
I0201 11:02:16.244115 139923852027648 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.478667140007019, loss=3.16640567779541
I0201 11:03:03.088914 139923868813056 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.4423788785934448, loss=3.6290738582611084
I0201 11:03:49.829241 139923852027648 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.635852336883545, loss=3.189755439758301
I0201 11:04:36.441985 139923868813056 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.5738142728805542, loss=3.1897106170654297
I0201 11:05:23.088757 139923852027648 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.4863505363464355, loss=4.141413688659668
I0201 11:05:33.101402 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:05:43.910020 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:06:06.445539 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:06:08.098662 140085747812160 submission_runner.py:408] Time since start: 57369.33s, 	Step: 114823, 	{'train/accuracy': 0.8125, 'train/loss': 0.9517192840576172, 'validation/accuracy': 0.733460009098053, 'validation/loss': 1.2863125801086426, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.896116852760315, 'test/num_examples': 10000, 'score': 52970.15888476372, 'total_duration': 57369.3252120018, 'accumulated_submission_time': 52970.15888476372, 'accumulated_eval_time': 4387.382179737091, 'accumulated_logging_time': 5.3694212436676025}
I0201 11:06:08.133152 139923868813056 logging_writer.py:48] [114823] accumulated_eval_time=4387.382180, accumulated_logging_time=5.369421, accumulated_submission_time=52970.158885, global_step=114823, preemption_count=0, score=52970.158885, test/accuracy=0.606000, test/loss=1.896117, test/num_examples=10000, total_duration=57369.325212, train/accuracy=0.812500, train/loss=0.951719, validation/accuracy=0.733460, validation/loss=1.286313, validation/num_examples=50000
I0201 11:06:39.106803 139923852027648 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.459535837173462, loss=3.1345839500427246
I0201 11:07:25.690336 139923868813056 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.636802077293396, loss=4.617881774902344
I0201 11:08:12.941424 139923852027648 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.6232603788375854, loss=3.1103837490081787
I0201 11:08:59.410782 139923868813056 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.5478626489639282, loss=3.1308484077453613
I0201 11:09:46.157410 139923852027648 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.5639582872390747, loss=3.0997474193573
I0201 11:10:33.054865 139923868813056 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.503043293952942, loss=3.3059983253479004
I0201 11:11:19.832938 139923852027648 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.4911952018737793, loss=3.6822500228881836
I0201 11:12:06.718504 139923868813056 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.5758488178253174, loss=4.320246696472168
I0201 11:12:53.765806 139923852027648 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.5485198497772217, loss=3.1460976600646973
I0201 11:13:08.374792 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:13:18.773118 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:13:43.541315 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:13:45.186444 140085747812160 submission_runner.py:408] Time since start: 57826.41s, 	Step: 115733, 	{'train/accuracy': 0.8245702981948853, 'train/loss': 0.9153898358345032, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.282979965209961, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8812453746795654, 'test/num_examples': 10000, 'score': 53390.33986020088, 'total_duration': 57826.413011312485, 'accumulated_submission_time': 53390.33986020088, 'accumulated_eval_time': 4424.193821430206, 'accumulated_logging_time': 5.413953542709351}
I0201 11:13:45.222262 139923868813056 logging_writer.py:48] [115733] accumulated_eval_time=4424.193821, accumulated_logging_time=5.413954, accumulated_submission_time=53390.339860, global_step=115733, preemption_count=0, score=53390.339860, test/accuracy=0.618400, test/loss=1.881245, test/num_examples=10000, total_duration=57826.413011, train/accuracy=0.824570, train/loss=0.915390, validation/accuracy=0.734600, validation/loss=1.282980, validation/num_examples=50000
I0201 11:14:11.929816 139923852027648 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.4892781972885132, loss=3.473353624343872
I0201 11:14:58.339346 139923868813056 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.5270932912826538, loss=3.042595863342285
I0201 11:15:45.467283 139923852027648 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.5899584293365479, loss=3.0780563354492188
I0201 11:16:32.231286 139923868813056 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.6085970401763916, loss=3.1365504264831543
I0201 11:17:19.430132 139923852027648 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.554946780204773, loss=3.178757905960083
I0201 11:18:06.179644 139923868813056 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.4797791242599487, loss=3.066422700881958
I0201 11:18:53.115602 139923852027648 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.715226411819458, loss=3.1525583267211914
I0201 11:19:39.812077 139923868813056 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.639271855354309, loss=3.2800791263580322
I0201 11:20:26.537133 139923852027648 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.5421233177185059, loss=3.2033026218414307
I0201 11:20:45.410762 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:20:56.005198 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:21:19.647204 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:21:21.283679 140085747812160 submission_runner.py:408] Time since start: 58282.51s, 	Step: 116642, 	{'train/accuracy': 0.8141601085662842, 'train/loss': 0.9461965560913086, 'validation/accuracy': 0.7366200089454651, 'validation/loss': 1.266655683517456, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8608219623565674, 'test/num_examples': 10000, 'score': 53810.46590876579, 'total_duration': 58282.51025009155, 'accumulated_submission_time': 53810.46590876579, 'accumulated_eval_time': 4460.06673002243, 'accumulated_logging_time': 5.46102499961853}
I0201 11:21:21.316850 139923868813056 logging_writer.py:48] [116642] accumulated_eval_time=4460.066730, accumulated_logging_time=5.461025, accumulated_submission_time=53810.465909, global_step=116642, preemption_count=0, score=53810.465909, test/accuracy=0.615600, test/loss=1.860822, test/num_examples=10000, total_duration=58282.510250, train/accuracy=0.814160, train/loss=0.946197, validation/accuracy=0.736620, validation/loss=1.266656, validation/num_examples=50000
I0201 11:21:44.498439 139923852027648 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.5032917261123657, loss=3.3558237552642822
I0201 11:22:30.014902 139923868813056 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.5329560041427612, loss=3.154994487762451
I0201 11:23:17.106469 139923852027648 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.7585560083389282, loss=3.2120139598846436
I0201 11:24:03.887546 139923868813056 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.6353378295898438, loss=4.4365386962890625
I0201 11:24:50.480017 139923852027648 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.6329479217529297, loss=3.164271116256714
I0201 11:25:37.006225 139923868813056 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.5717506408691406, loss=3.3175711631774902
I0201 11:26:23.773237 139923852027648 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.58852219581604, loss=3.1267504692077637
I0201 11:27:10.538382 139923868813056 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.399442195892334, loss=3.6296253204345703
I0201 11:27:57.179749 139923852027648 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.6634151935577393, loss=4.465310573577881
I0201 11:28:21.694919 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:28:32.474995 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:28:54.955258 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:28:56.602284 140085747812160 submission_runner.py:408] Time since start: 58737.83s, 	Step: 117554, 	{'train/accuracy': 0.8140038847923279, 'train/loss': 0.9657190442085266, 'validation/accuracy': 0.7342599630355835, 'validation/loss': 1.2892900705337524, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8896840810775757, 'test/num_examples': 10000, 'score': 54230.782210826874, 'total_duration': 58737.82883524895, 'accumulated_submission_time': 54230.782210826874, 'accumulated_eval_time': 4494.974093198776, 'accumulated_logging_time': 5.504605293273926}
I0201 11:28:56.637558 139923868813056 logging_writer.py:48] [117554] accumulated_eval_time=4494.974093, accumulated_logging_time=5.504605, accumulated_submission_time=54230.782211, global_step=117554, preemption_count=0, score=54230.782211, test/accuracy=0.616500, test/loss=1.889684, test/num_examples=10000, total_duration=58737.828835, train/accuracy=0.814004, train/loss=0.965719, validation/accuracy=0.734260, validation/loss=1.289290, validation/num_examples=50000
I0201 11:29:15.095632 139923852027648 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.6368939876556396, loss=3.184873342514038
I0201 11:29:59.711389 139923868813056 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.919863224029541, loss=4.68364143371582
I0201 11:30:46.278658 139923852027648 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.6062886714935303, loss=3.0898427963256836
I0201 11:31:33.378770 139923868813056 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.5613658428192139, loss=3.362586259841919
I0201 11:32:20.121750 139923852027648 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.725163221359253, loss=3.1093904972076416
I0201 11:33:07.051320 139923868813056 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.921900987625122, loss=4.691488265991211
I0201 11:33:53.903743 139923852027648 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.569716215133667, loss=4.264983177185059
I0201 11:34:40.525923 139923868813056 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.5212055444717407, loss=3.762775182723999
I0201 11:35:27.370823 139923852027648 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.7505717277526855, loss=3.132030725479126
I0201 11:35:56.759506 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:36:07.340850 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:36:31.860563 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:36:33.495942 140085747812160 submission_runner.py:408] Time since start: 59194.72s, 	Step: 118465, 	{'train/accuracy': 0.8282226324081421, 'train/loss': 0.9008296132087708, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.2724199295043945, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.8633793592453003, 'test/num_examples': 10000, 'score': 54650.84451293945, 'total_duration': 59194.7225048542, 'accumulated_submission_time': 54650.84451293945, 'accumulated_eval_time': 4531.71052479744, 'accumulated_logging_time': 5.548776865005493}
I0201 11:36:33.539447 139923868813056 logging_writer.py:48] [118465] accumulated_eval_time=4531.710525, accumulated_logging_time=5.548777, accumulated_submission_time=54650.844513, global_step=118465, preemption_count=0, score=54650.844513, test/accuracy=0.617000, test/loss=1.863379, test/num_examples=10000, total_duration=59194.722505, train/accuracy=0.828223, train/loss=0.900830, validation/accuracy=0.737660, validation/loss=1.272420, validation/num_examples=50000
I0201 11:36:47.676829 139923852027648 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.6718066930770874, loss=4.449377059936523
I0201 11:37:31.787485 139923868813056 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.511338233947754, loss=3.641387701034546
I0201 11:38:18.658269 139923852027648 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.586171269416809, loss=3.203373432159424
I0201 11:39:05.441420 139923868813056 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.4586689472198486, loss=3.8808929920196533
I0201 11:39:52.284933 139923852027648 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.805417776107788, loss=3.181459426879883
I0201 11:40:39.079933 139923868813056 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.628475546836853, loss=3.095381736755371
I0201 11:41:25.773038 139923852027648 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.8440465927124023, loss=4.735669136047363
I0201 11:42:12.622120 139923868813056 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.6494576930999756, loss=3.082536458969116
I0201 11:42:59.608560 139923852027648 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.469948172569275, loss=3.4856200218200684
I0201 11:43:33.524527 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:43:43.832767 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:44:06.521118 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:44:08.163131 140085747812160 submission_runner.py:408] Time since start: 59649.39s, 	Step: 119375, 	{'train/accuracy': 0.81787109375, 'train/loss': 0.9437991380691528, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.265620231628418, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.868046522140503, 'test/num_examples': 10000, 'score': 55070.77024292946, 'total_duration': 59649.38969564438, 'accumulated_submission_time': 55070.77024292946, 'accumulated_eval_time': 4566.3491423130035, 'accumulated_logging_time': 5.601795434951782}
I0201 11:44:08.201798 139923868813056 logging_writer.py:48] [119375] accumulated_eval_time=4566.349142, accumulated_logging_time=5.601795, accumulated_submission_time=55070.770243, global_step=119375, preemption_count=0, score=55070.770243, test/accuracy=0.614400, test/loss=1.868047, test/num_examples=10000, total_duration=59649.389696, train/accuracy=0.817871, train/loss=0.943799, validation/accuracy=0.740320, validation/loss=1.265620, validation/num_examples=50000
I0201 11:44:18.414021 139923852027648 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.5721487998962402, loss=3.4175643920898438
I0201 11:45:01.318402 139923868813056 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.059414863586426, loss=4.566982269287109
I0201 11:45:48.245162 139923852027648 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.5571879148483276, loss=3.078188896179199
I0201 11:46:34.862960 139923868813056 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.643462061882019, loss=3.0950472354888916
I0201 11:47:21.761399 139923852027648 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.6040558815002441, loss=2.963289260864258
I0201 11:48:08.746247 139923868813056 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.6611132621765137, loss=3.0419211387634277
I0201 11:48:55.676085 139923852027648 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.696549892425537, loss=3.115553617477417
I0201 11:49:42.579093 139923868813056 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5905203819274902, loss=3.2598276138305664
I0201 11:50:28.927057 139923852027648 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.5836560726165771, loss=3.125875234603882
I0201 11:51:08.204240 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:51:18.941091 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:51:42.446090 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:51:44.093083 140085747812160 submission_runner.py:408] Time since start: 60105.32s, 	Step: 120285, 	{'train/accuracy': 0.8151366710662842, 'train/loss': 0.9468899369239807, 'validation/accuracy': 0.7399799823760986, 'validation/loss': 1.2717607021331787, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.870540738105774, 'test/num_examples': 10000, 'score': 55490.71073937416, 'total_duration': 60105.319628715515, 'accumulated_submission_time': 55490.71073937416, 'accumulated_eval_time': 4602.237959384918, 'accumulated_logging_time': 5.651767730712891}
I0201 11:51:44.130891 139923868813056 logging_writer.py:48] [120285] accumulated_eval_time=4602.237959, accumulated_logging_time=5.651768, accumulated_submission_time=55490.710739, global_step=120285, preemption_count=0, score=55490.710739, test/accuracy=0.622900, test/loss=1.870541, test/num_examples=10000, total_duration=60105.319629, train/accuracy=0.815137, train/loss=0.946890, validation/accuracy=0.739980, validation/loss=1.271761, validation/num_examples=50000
I0201 11:51:50.415625 139923852027648 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.6857295036315918, loss=3.080517053604126
I0201 11:52:32.968656 139923868813056 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.6088027954101562, loss=3.296302080154419
I0201 11:53:19.571148 139923852027648 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.681174635887146, loss=3.192718505859375
I0201 11:54:06.214148 139923868813056 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.5012857913970947, loss=3.5885465145111084
I0201 11:54:52.863862 139923852027648 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.591761589050293, loss=4.021594524383545
I0201 11:55:39.495434 139923868813056 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.4972047805786133, loss=3.162228584289551
I0201 11:56:26.289197 139923852027648 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.4105209112167358, loss=3.4814231395721436
I0201 11:57:13.031403 139923868813056 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.9529920816421509, loss=4.686513900756836
I0201 11:58:00.065070 139923852027648 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.591125249862671, loss=3.896095037460327
I0201 11:58:44.141677 140085747812160 spec.py:321] Evaluating on the training split.
I0201 11:58:54.451889 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 11:59:19.830412 140085747812160 spec.py:349] Evaluating on the test split.
I0201 11:59:21.474222 140085747812160 submission_runner.py:408] Time since start: 60562.70s, 	Step: 121196, 	{'train/accuracy': 0.8320898413658142, 'train/loss': 0.8478189706802368, 'validation/accuracy': 0.742419958114624, 'validation/loss': 1.2173817157745361, 'validation/num_examples': 50000, 'test/accuracy': 0.6231000423431396, 'test/loss': 1.803972601890564, 'test/num_examples': 10000, 'score': 55910.660524606705, 'total_duration': 60562.70079231262, 'accumulated_submission_time': 55910.660524606705, 'accumulated_eval_time': 4639.570498466492, 'accumulated_logging_time': 5.699680805206299}
I0201 11:59:21.512429 139923868813056 logging_writer.py:48] [121196] accumulated_eval_time=4639.570498, accumulated_logging_time=5.699681, accumulated_submission_time=55910.660525, global_step=121196, preemption_count=0, score=55910.660525, test/accuracy=0.623100, test/loss=1.803973, test/num_examples=10000, total_duration=60562.700792, train/accuracy=0.832090, train/loss=0.847819, validation/accuracy=0.742420, validation/loss=1.217382, validation/num_examples=50000
I0201 11:59:23.480935 139923852027648 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.901391625404358, loss=4.645633220672607
I0201 12:00:05.183682 139923868813056 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.8474863767623901, loss=4.806991100311279
I0201 12:00:51.807330 139923852027648 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.7202811241149902, loss=4.62978458404541
I0201 12:01:38.831074 139923868813056 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.575311303138733, loss=3.839492082595825
I0201 12:02:25.574565 139923852027648 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.9584182500839233, loss=4.6924638748168945
I0201 12:03:12.431854 139923868813056 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.815148949623108, loss=3.0912768840789795
I0201 12:03:59.338567 139923852027648 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.5346122980117798, loss=4.204155445098877
I0201 12:04:45.981621 139923868813056 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.6934385299682617, loss=3.0156991481781006
I0201 12:05:32.774653 139923852027648 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.619234323501587, loss=3.3030056953430176
I0201 12:06:19.282204 139923868813056 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.491850733757019, loss=3.8909850120544434
I0201 12:06:21.717607 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:06:32.263791 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:06:54.874403 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:06:56.518298 140085747812160 submission_runner.py:408] Time since start: 61017.74s, 	Step: 122107, 	{'train/accuracy': 0.8207421898841858, 'train/loss': 0.926133930683136, 'validation/accuracy': 0.7434799671173096, 'validation/loss': 1.2434664964675903, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.8386973142623901, 'test/num_examples': 10000, 'score': 56330.80289840698, 'total_duration': 61017.744869470596, 'accumulated_submission_time': 56330.80289840698, 'accumulated_eval_time': 4674.371244430542, 'accumulated_logging_time': 5.749354600906372}
I0201 12:06:56.552917 139923852027648 logging_writer.py:48] [122107] accumulated_eval_time=4674.371244, accumulated_logging_time=5.749355, accumulated_submission_time=56330.802898, global_step=122107, preemption_count=0, score=56330.802898, test/accuracy=0.625800, test/loss=1.838697, test/num_examples=10000, total_duration=61017.744869, train/accuracy=0.820742, train/loss=0.926134, validation/accuracy=0.743480, validation/loss=1.243466, validation/num_examples=50000
I0201 12:07:34.977650 139923868813056 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.6005266904830933, loss=3.061643123626709
I0201 12:08:21.616442 139923852027648 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.8604782819747925, loss=4.737117767333984
I0201 12:09:08.435947 139923868813056 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.5989917516708374, loss=3.218979597091675
I0201 12:09:55.042852 139923852027648 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.6710954904556274, loss=3.783346176147461
I0201 12:10:42.004949 139923868813056 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.6452434062957764, loss=3.11255145072937
I0201 12:11:28.498715 139923852027648 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.67855703830719, loss=3.687809467315674
I0201 12:12:15.148345 139923868813056 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.712739109992981, loss=4.1714911460876465
I0201 12:13:02.274983 139923852027648 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.650152564048767, loss=3.213244676589966
I0201 12:13:48.821907 139923868813056 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.6765552759170532, loss=3.074252128601074
I0201 12:13:56.846961 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:14:07.187272 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:14:30.996185 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:14:32.638668 140085747812160 submission_runner.py:408] Time since start: 61473.87s, 	Step: 123019, 	{'train/accuracy': 0.8250390291213989, 'train/loss': 0.8884112238883972, 'validation/accuracy': 0.7450599670410156, 'validation/loss': 1.2220823764801025, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8158106803894043, 'test/num_examples': 10000, 'score': 56751.03601574898, 'total_duration': 61473.865228652954, 'accumulated_submission_time': 56751.03601574898, 'accumulated_eval_time': 4710.162944316864, 'accumulated_logging_time': 5.793039083480835}
I0201 12:14:32.673360 139923852027648 logging_writer.py:48] [123019] accumulated_eval_time=4710.162944, accumulated_logging_time=5.793039, accumulated_submission_time=56751.036016, global_step=123019, preemption_count=0, score=56751.036016, test/accuracy=0.624700, test/loss=1.815811, test/num_examples=10000, total_duration=61473.865229, train/accuracy=0.825039, train/loss=0.888411, validation/accuracy=0.745060, validation/loss=1.222082, validation/num_examples=50000
I0201 12:15:05.748115 139923868813056 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.7938594818115234, loss=4.382134914398193
I0201 12:15:52.268970 139923852027648 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.6202291250228882, loss=3.028395414352417
I0201 12:16:39.179334 139923868813056 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.6262931823730469, loss=3.488863945007324
I0201 12:17:25.552379 139923852027648 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.7178630828857422, loss=2.987729787826538
I0201 12:18:12.670329 139923868813056 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.573660969734192, loss=3.034998893737793
I0201 12:18:59.384370 139923852027648 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.7181082963943481, loss=4.18095064163208
I0201 12:19:46.121241 139923868813056 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.703037142753601, loss=3.0848076343536377
I0201 12:20:32.886450 139923852027648 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.5831267833709717, loss=3.1287789344787598
I0201 12:21:19.607386 139923868813056 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.8355644941329956, loss=3.0888454914093018
I0201 12:21:32.707539 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:21:43.113553 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:22:04.774033 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:22:06.436066 140085747812160 submission_runner.py:408] Time since start: 61927.66s, 	Step: 123930, 	{'train/accuracy': 0.8338476419448853, 'train/loss': 0.8774459362030029, 'validation/accuracy': 0.7470399737358093, 'validation/loss': 1.2402421236038208, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8210102319717407, 'test/num_examples': 10000, 'score': 57171.007404088974, 'total_duration': 61927.66263628006, 'accumulated_submission_time': 57171.007404088974, 'accumulated_eval_time': 4743.8915066719055, 'accumulated_logging_time': 5.839264869689941}
I0201 12:22:06.471293 139923852027648 logging_writer.py:48] [123930] accumulated_eval_time=4743.891507, accumulated_logging_time=5.839265, accumulated_submission_time=57171.007404, global_step=123930, preemption_count=0, score=57171.007404, test/accuracy=0.623400, test/loss=1.821010, test/num_examples=10000, total_duration=61927.662636, train/accuracy=0.833848, train/loss=0.877446, validation/accuracy=0.747040, validation/loss=1.240242, validation/num_examples=50000
I0201 12:22:34.342432 139923868813056 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.7286787033081055, loss=3.7706351280212402
I0201 12:23:20.772151 139923852027648 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.6264729499816895, loss=3.804003953933716
I0201 12:24:07.894005 139923868813056 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.6632813215255737, loss=3.209226369857788
I0201 12:24:54.665369 139923852027648 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.674456238746643, loss=3.1259021759033203
I0201 12:25:41.724871 139923868813056 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.7333675622940063, loss=3.009612560272217
I0201 12:26:28.374478 139923852027648 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.874178171157837, loss=4.065008640289307
I0201 12:27:15.712880 139923868813056 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.7358652353286743, loss=3.3759031295776367
I0201 12:28:02.188983 139923852027648 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.0034754276275635, loss=4.682597637176514
I0201 12:28:49.016000 139923868813056 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.8181709051132202, loss=3.083115816116333
I0201 12:29:06.553611 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:29:16.828852 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:29:42.700937 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:29:44.352105 140085747812160 submission_runner.py:408] Time since start: 62385.58s, 	Step: 124839, 	{'train/accuracy': 0.8262499570846558, 'train/loss': 0.8751652240753174, 'validation/accuracy': 0.7460199594497681, 'validation/loss': 1.2193933725357056, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8021245002746582, 'test/num_examples': 10000, 'score': 57591.029952049255, 'total_duration': 62385.57867670059, 'accumulated_submission_time': 57591.029952049255, 'accumulated_eval_time': 4781.690023899078, 'accumulated_logging_time': 5.884131908416748}
I0201 12:29:44.386988 139923852027648 logging_writer.py:48] [124839] accumulated_eval_time=4781.690024, accumulated_logging_time=5.884132, accumulated_submission_time=57591.029952, global_step=124839, preemption_count=0, score=57591.029952, test/accuracy=0.627800, test/loss=1.802125, test/num_examples=10000, total_duration=62385.578677, train/accuracy=0.826250, train/loss=0.875165, validation/accuracy=0.746020, validation/loss=1.219393, validation/num_examples=50000
I0201 12:30:08.866243 139923868813056 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.7859615087509155, loss=3.1347289085388184
I0201 12:30:54.768272 139923852027648 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.6972122192382812, loss=4.169699192047119
I0201 12:31:42.012155 139923868813056 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.5485466718673706, loss=3.617208242416382
I0201 12:32:28.663378 139923852027648 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.5880919694900513, loss=3.0217843055725098
I0201 12:33:15.451275 139923868813056 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.6838041543960571, loss=3.0754499435424805
I0201 12:34:02.062364 139923852027648 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.7402098178863525, loss=4.099297523498535
I0201 12:34:48.845237 139923868813056 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.742614507675171, loss=3.0788896083831787
I0201 12:35:35.626827 139923852027648 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6939514875411987, loss=3.0792760848999023
I0201 12:36:22.419103 139923868813056 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.7253457307815552, loss=3.0178630352020264
I0201 12:36:44.411129 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:36:54.967663 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:37:18.438720 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:37:20.075949 140085747812160 submission_runner.py:408] Time since start: 62841.30s, 	Step: 125749, 	{'train/accuracy': 0.8309765458106995, 'train/loss': 0.8596699833869934, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.2096494436264038, 'validation/num_examples': 50000, 'test/accuracy': 0.6221000552177429, 'test/loss': 1.8037065267562866, 'test/num_examples': 10000, 'score': 58010.99251627922, 'total_duration': 62841.30251932144, 'accumulated_submission_time': 58010.99251627922, 'accumulated_eval_time': 4817.354856729507, 'accumulated_logging_time': 5.930292129516602}
I0201 12:37:20.111039 139923852027648 logging_writer.py:48] [125749] accumulated_eval_time=4817.354857, accumulated_logging_time=5.930292, accumulated_submission_time=58010.992516, global_step=125749, preemption_count=0, score=58010.992516, test/accuracy=0.622100, test/loss=1.803707, test/num_examples=10000, total_duration=62841.302519, train/accuracy=0.830977, train/loss=0.859670, validation/accuracy=0.747240, validation/loss=1.209649, validation/num_examples=50000
I0201 12:37:40.541252 139923868813056 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.5891402959823608, loss=3.267530918121338
I0201 12:38:25.383882 139923852027648 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.7592600584030151, loss=3.0206139087677
I0201 12:39:12.255638 139923868813056 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.7671173810958862, loss=2.9688076972961426
I0201 12:39:58.761443 139923852027648 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.5855380296707153, loss=3.6730973720550537
I0201 12:40:45.619411 139923868813056 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.8437176942825317, loss=4.364388942718506
I0201 12:41:32.376935 139923852027648 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.7546252012252808, loss=3.044931411743164
I0201 12:42:19.228986 139923868813056 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.7916591167449951, loss=4.378639221191406
I0201 12:43:06.107612 139923852027648 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.684922218322754, loss=3.123744487762451
I0201 12:43:52.914302 139923868813056 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.7378016710281372, loss=3.068145275115967
I0201 12:44:20.234261 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:44:31.383614 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:44:54.262606 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:44:55.897695 140085747812160 submission_runner.py:408] Time since start: 63297.12s, 	Step: 126660, 	{'train/accuracy': 0.83447265625, 'train/loss': 0.8702915906906128, 'validation/accuracy': 0.7460399866104126, 'validation/loss': 1.243055820465088, 'validation/num_examples': 50000, 'test/accuracy': 0.6221000552177429, 'test/loss': 1.84426748752594, 'test/num_examples': 10000, 'score': 58431.05421996117, 'total_duration': 63297.124264001846, 'accumulated_submission_time': 58431.05421996117, 'accumulated_eval_time': 4853.018300771713, 'accumulated_logging_time': 5.975062370300293}
I0201 12:44:55.936714 139923852027648 logging_writer.py:48] [126660] accumulated_eval_time=4853.018301, accumulated_logging_time=5.975062, accumulated_submission_time=58431.054220, global_step=126660, preemption_count=0, score=58431.054220, test/accuracy=0.622100, test/loss=1.844267, test/num_examples=10000, total_duration=63297.124264, train/accuracy=0.834473, train/loss=0.870292, validation/accuracy=0.746040, validation/loss=1.243056, validation/num_examples=50000
I0201 12:45:12.029627 139923868813056 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.0908689498901367, loss=4.661052703857422
I0201 12:45:56.225414 139923852027648 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.77959144115448, loss=3.1778295040130615
I0201 12:46:43.134475 139923868813056 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.7096407413482666, loss=3.0289835929870605
I0201 12:47:29.838097 139923852027648 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.195699453353882, loss=4.622471809387207
I0201 12:48:16.780156 139923868813056 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.6750940084457397, loss=3.106149196624756
I0201 12:49:03.378218 139923852027648 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.6424165964126587, loss=3.094841957092285
I0201 12:49:50.182375 139923868813056 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.785070776939392, loss=3.0199995040893555
I0201 12:50:36.909428 139923852027648 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.768636703491211, loss=2.969593048095703
I0201 12:51:23.857241 139923868813056 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.0535826683044434, loss=4.654273986816406
I0201 12:51:56.216095 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:52:06.971790 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 12:52:31.540106 140085747812160 spec.py:349] Evaluating on the test split.
I0201 12:52:33.174823 140085747812160 submission_runner.py:408] Time since start: 63754.40s, 	Step: 127571, 	{'train/accuracy': 0.8270898461341858, 'train/loss': 0.900422215461731, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 1.2298741340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8229761123657227, 'test/num_examples': 10000, 'score': 58851.270611047745, 'total_duration': 63754.40138721466, 'accumulated_submission_time': 58851.270611047745, 'accumulated_eval_time': 4889.977033615112, 'accumulated_logging_time': 6.026872634887695}
I0201 12:52:33.215214 139923852027648 logging_writer.py:48] [127571] accumulated_eval_time=4889.977034, accumulated_logging_time=6.026873, accumulated_submission_time=58851.270611, global_step=127571, preemption_count=0, score=58851.270611, test/accuracy=0.624700, test/loss=1.822976, test/num_examples=10000, total_duration=63754.401387, train/accuracy=0.827090, train/loss=0.900422, validation/accuracy=0.750660, validation/loss=1.229874, validation/num_examples=50000
I0201 12:52:44.998285 139923868813056 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.8253182172775269, loss=3.0818209648132324
I0201 12:53:28.538956 139923852027648 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.763221263885498, loss=4.23026180267334
I0201 12:54:14.763730 139923868813056 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.0031652450561523, loss=4.328747749328613
I0201 12:55:01.739504 139923852027648 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.7587465047836304, loss=4.202477931976318
I0201 12:55:48.379801 139923868813056 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.851489543914795, loss=3.072612762451172
I0201 12:56:35.027429 139923852027648 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.768051266670227, loss=4.123122215270996
I0201 12:57:21.560944 139923868813056 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.003824234008789, loss=4.27186393737793
I0201 12:58:08.375868 139923852027648 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.0451362133026123, loss=4.291372299194336
I0201 12:58:54.831115 139923868813056 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.7771414518356323, loss=3.148745059967041
I0201 12:59:33.253495 140085747812160 spec.py:321] Evaluating on the training split.
I0201 12:59:43.589145 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:00:06.258520 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:00:07.906547 140085747812160 submission_runner.py:408] Time since start: 64209.13s, 	Step: 128484, 	{'train/accuracy': 0.8315820097923279, 'train/loss': 0.8549019694328308, 'validation/accuracy': 0.7501399517059326, 'validation/loss': 1.189975619316101, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.7846035957336426, 'test/num_examples': 10000, 'score': 59271.248304367065, 'total_duration': 64209.13311076164, 'accumulated_submission_time': 59271.248304367065, 'accumulated_eval_time': 4924.630076885223, 'accumulated_logging_time': 6.076805591583252}
I0201 13:00:07.945766 139923852027648 logging_writer.py:48] [128484] accumulated_eval_time=4924.630077, accumulated_logging_time=6.076806, accumulated_submission_time=59271.248304, global_step=128484, preemption_count=0, score=59271.248304, test/accuracy=0.627800, test/loss=1.784604, test/num_examples=10000, total_duration=64209.133111, train/accuracy=0.831582, train/loss=0.854902, validation/accuracy=0.750140, validation/loss=1.189976, validation/num_examples=50000
I0201 13:00:14.634853 139923868813056 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.6817642450332642, loss=3.2284154891967773
I0201 13:00:57.220939 139923852027648 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.7610993385314941, loss=3.0945956707000732
I0201 13:01:44.119507 139923868813056 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.9746850728988647, loss=3.0744497776031494
I0201 13:02:31.211985 139923852027648 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.8300485610961914, loss=3.027066230773926
I0201 13:03:18.185606 139923868813056 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.9637669324874878, loss=3.9531028270721436
I0201 13:04:05.131838 139923852027648 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.7691658735275269, loss=3.2737252712249756
I0201 13:04:51.874338 139923868813056 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.6689338684082031, loss=2.9364147186279297
I0201 13:05:38.588445 139923852027648 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.7847161293029785, loss=3.8051414489746094
I0201 13:06:25.269869 139923868813056 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.7883363962173462, loss=3.041269302368164
I0201 13:07:08.454916 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:07:18.726368 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:07:44.454227 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:07:46.097182 140085747812160 submission_runner.py:408] Time since start: 64667.32s, 	Step: 129393, 	{'train/accuracy': 0.8374413847923279, 'train/loss': 0.8287783861160278, 'validation/accuracy': 0.7497599720954895, 'validation/loss': 1.1921215057373047, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.7807656526565552, 'test/num_examples': 10000, 'score': 59691.69730448723, 'total_duration': 64667.323753118515, 'accumulated_submission_time': 59691.69730448723, 'accumulated_eval_time': 4962.272361755371, 'accumulated_logging_time': 6.125463962554932}
I0201 13:07:46.136433 139923852027648 logging_writer.py:48] [129393] accumulated_eval_time=4962.272362, accumulated_logging_time=6.125464, accumulated_submission_time=59691.697304, global_step=129393, preemption_count=0, score=59691.697304, test/accuracy=0.631900, test/loss=1.780766, test/num_examples=10000, total_duration=64667.323753, train/accuracy=0.837441, train/loss=0.828778, validation/accuracy=0.749760, validation/loss=1.192122, validation/num_examples=50000
I0201 13:07:49.280424 139923868813056 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.8561087846755981, loss=4.408466815948486
I0201 13:08:31.264854 139923852027648 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.7817656993865967, loss=4.194347858428955
I0201 13:09:18.020975 139923868813056 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.022047519683838, loss=4.570337772369385
I0201 13:10:04.835462 139923852027648 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.9587464332580566, loss=3.092315196990967
I0201 13:10:51.492385 139923868813056 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.5500519275665283, loss=4.692493915557861
I0201 13:11:38.671092 139923852027648 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.7298945188522339, loss=3.0061066150665283
I0201 13:12:25.445886 139923868813056 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.7255135774612427, loss=3.313577651977539
I0201 13:13:12.580396 139923852027648 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.9975723028182983, loss=3.062072515487671
I0201 13:13:59.316112 139923868813056 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.661057949066162, loss=3.010758399963379
I0201 13:14:46.185517 139923852027648 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.5532461404800415, loss=3.6846930980682373
I0201 13:14:46.198714 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:14:56.579501 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:15:19.100983 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:15:20.747615 140085747812160 submission_runner.py:408] Time since start: 65121.97s, 	Step: 130301, 	{'train/accuracy': 0.8481640219688416, 'train/loss': 0.8119232058525085, 'validation/accuracy': 0.7521599531173706, 'validation/loss': 1.201475977897644, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.7882546186447144, 'test/num_examples': 10000, 'score': 60111.69879961014, 'total_duration': 65121.97418880463, 'accumulated_submission_time': 60111.69879961014, 'accumulated_eval_time': 4996.821247339249, 'accumulated_logging_time': 6.174273729324341}
I0201 13:15:20.783611 139923868813056 logging_writer.py:48] [130301] accumulated_eval_time=4996.821247, accumulated_logging_time=6.174274, accumulated_submission_time=60111.698800, global_step=130301, preemption_count=0, score=60111.698800, test/accuracy=0.632800, test/loss=1.788255, test/num_examples=10000, total_duration=65121.974189, train/accuracy=0.848164, train/loss=0.811923, validation/accuracy=0.752160, validation/loss=1.201476, validation/num_examples=50000
I0201 13:16:01.813239 139923852027648 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.6109611988067627, loss=3.4335451126098633
I0201 13:16:48.496886 139923868813056 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.8330796957015991, loss=3.0720908641815186
I0201 13:17:35.574049 139923852027648 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.7723571062088013, loss=2.975986957550049
I0201 13:18:22.375550 139923868813056 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.7631332874298096, loss=3.2536890506744385
I0201 13:19:09.252286 139923852027648 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.1949777603149414, loss=4.642055511474609
I0201 13:19:55.749151 139923868813056 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.081808090209961, loss=4.063303470611572
I0201 13:20:42.632304 139923852027648 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.6747299432754517, loss=3.2364513874053955
I0201 13:21:29.349799 139923868813056 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.6555070877075195, loss=3.1134650707244873
I0201 13:22:16.269725 139923852027648 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.0446362495422363, loss=4.240739345550537
I0201 13:22:21.044519 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:22:31.636453 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:22:54.260042 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:22:55.897039 140085747812160 submission_runner.py:408] Time since start: 65577.12s, 	Step: 131212, 	{'train/accuracy': 0.8323046565055847, 'train/loss': 0.855158269405365, 'validation/accuracy': 0.750819981098175, 'validation/loss': 1.1929678916931152, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.7767421007156372, 'test/num_examples': 10000, 'score': 60531.898847579956, 'total_duration': 65577.1236114502, 'accumulated_submission_time': 60531.898847579956, 'accumulated_eval_time': 5031.673780918121, 'accumulated_logging_time': 6.2202746868133545}
I0201 13:22:55.932787 139923868813056 logging_writer.py:48] [131212] accumulated_eval_time=5031.673781, accumulated_logging_time=6.220275, accumulated_submission_time=60531.898848, global_step=131212, preemption_count=0, score=60531.898848, test/accuracy=0.636300, test/loss=1.776742, test/num_examples=10000, total_duration=65577.123611, train/accuracy=0.832305, train/loss=0.855158, validation/accuracy=0.750820, validation/loss=1.192968, validation/num_examples=50000
I0201 13:23:32.120630 139923852027648 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.7782062292099, loss=2.9875593185424805
I0201 13:24:18.838147 139923868813056 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.85421884059906, loss=3.4428844451904297
I0201 13:25:05.597529 139923852027648 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.8368786573410034, loss=2.9960126876831055
I0201 13:25:52.446363 139923868813056 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.7633047103881836, loss=3.047574758529663
I0201 13:26:39.069827 139923852027648 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.6820651292800903, loss=3.061497211456299
I0201 13:27:25.701291 139923868813056 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.7688307762145996, loss=3.176736831665039
I0201 13:28:12.521458 139923852027648 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.8224700689315796, loss=3.117720603942871
I0201 13:28:58.862620 139923868813056 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8818750381469727, loss=3.061415910720825
I0201 13:29:45.555590 139923852027648 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.199420928955078, loss=4.381676197052002
I0201 13:29:55.929604 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:30:06.137812 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:30:31.024715 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:30:32.660906 140085747812160 submission_runner.py:408] Time since start: 66033.89s, 	Step: 132124, 	{'train/accuracy': 0.8398241996765137, 'train/loss': 0.8325179815292358, 'validation/accuracy': 0.7538399696350098, 'validation/loss': 1.1827151775360107, 'validation/num_examples': 50000, 'test/accuracy': 0.6365000009536743, 'test/loss': 1.767372488975525, 'test/num_examples': 10000, 'score': 60951.835547208786, 'total_duration': 66033.88747620583, 'accumulated_submission_time': 60951.835547208786, 'accumulated_eval_time': 5068.405083656311, 'accumulated_logging_time': 6.265010595321655}
I0201 13:30:32.696978 139923868813056 logging_writer.py:48] [132124] accumulated_eval_time=5068.405084, accumulated_logging_time=6.265011, accumulated_submission_time=60951.835547, global_step=132124, preemption_count=0, score=60951.835547, test/accuracy=0.636500, test/loss=1.767372, test/num_examples=10000, total_duration=66033.887476, train/accuracy=0.839824, train/loss=0.832518, validation/accuracy=0.753840, validation/loss=1.182715, validation/num_examples=50000
I0201 13:31:03.336763 139923852027648 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.860893726348877, loss=3.0307278633117676
I0201 13:31:49.935766 139923868813056 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.8610202074050903, loss=4.372231960296631
I0201 13:32:37.175561 139923852027648 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.7299890518188477, loss=2.871678113937378
I0201 13:33:23.789165 139923868813056 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.9596288204193115, loss=4.3414411544799805
I0201 13:34:10.820015 139923852027648 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.8741248846054077, loss=3.0165677070617676
I0201 13:34:57.218482 139923868813056 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.7275331020355225, loss=3.1124420166015625
I0201 13:35:44.050884 139923852027648 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.8560601472854614, loss=3.050382375717163
I0201 13:36:30.755542 139923868813056 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.942062258720398, loss=3.0707244873046875
I0201 13:37:17.561056 139923852027648 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.958880066871643, loss=3.701732635498047
I0201 13:37:32.732093 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:37:43.187029 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:38:06.503136 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:38:08.141430 140085747812160 submission_runner.py:408] Time since start: 66489.37s, 	Step: 133034, 	{'train/accuracy': 0.8498437404632568, 'train/loss': 0.8029429912567139, 'validation/accuracy': 0.7543999552726746, 'validation/loss': 1.2014082670211792, 'validation/num_examples': 50000, 'test/accuracy': 0.636400043964386, 'test/loss': 1.7902463674545288, 'test/num_examples': 10000, 'score': 61371.809309244156, 'total_duration': 66489.36799764633, 'accumulated_submission_time': 61371.809309244156, 'accumulated_eval_time': 5103.814422369003, 'accumulated_logging_time': 6.311906576156616}
I0201 13:38:08.180263 139923868813056 logging_writer.py:48] [133034] accumulated_eval_time=5103.814422, accumulated_logging_time=6.311907, accumulated_submission_time=61371.809309, global_step=133034, preemption_count=0, score=61371.809309, test/accuracy=0.636400, test/loss=1.790246, test/num_examples=10000, total_duration=66489.367998, train/accuracy=0.849844, train/loss=0.802943, validation/accuracy=0.754400, validation/loss=1.201408, validation/num_examples=50000
I0201 13:38:34.474250 139923852027648 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.8468945026397705, loss=3.1721129417419434
I0201 13:39:20.681323 139923868813056 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.1939430236816406, loss=4.597550868988037
I0201 13:40:07.661530 139923852027648 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.8681086301803589, loss=2.983518600463867
I0201 13:40:54.319719 139923868813056 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.9310171604156494, loss=4.139580726623535
I0201 13:41:40.822345 139923852027648 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.9194841384887695, loss=3.0624704360961914
I0201 13:42:27.569288 139923868813056 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.8633311986923218, loss=3.0238499641418457
I0201 13:43:14.608977 139923852027648 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.8515238761901855, loss=4.179220199584961
I0201 13:44:01.448890 139923868813056 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.8604882955551147, loss=2.947484016418457
I0201 13:44:48.170033 139923852027648 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.900380253791809, loss=3.0406486988067627
I0201 13:45:08.279339 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:45:18.713378 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:45:42.103438 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:45:43.740048 140085747812160 submission_runner.py:408] Time since start: 66944.97s, 	Step: 133945, 	{'train/accuracy': 0.8379882574081421, 'train/loss': 0.8267109394073486, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 1.1843620538711548, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.7660489082336426, 'test/num_examples': 10000, 'score': 61791.847019433975, 'total_duration': 66944.96662092209, 'accumulated_submission_time': 61791.847019433975, 'accumulated_eval_time': 5139.275134801865, 'accumulated_logging_time': 6.361809492111206}
I0201 13:45:43.779053 139923868813056 logging_writer.py:48] [133945] accumulated_eval_time=5139.275135, accumulated_logging_time=6.361809, accumulated_submission_time=61791.847019, global_step=133945, preemption_count=0, score=61791.847019, test/accuracy=0.636100, test/loss=1.766049, test/num_examples=10000, total_duration=66944.966621, train/accuracy=0.837988, train/loss=0.826711, validation/accuracy=0.753920, validation/loss=1.184362, validation/num_examples=50000
I0201 13:46:05.765927 139923852027648 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.7615244388580322, loss=3.082782506942749
I0201 13:46:51.068519 139923868813056 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.8502671718597412, loss=3.0543386936187744
I0201 13:47:37.865012 139923852027648 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9937787055969238, loss=2.942894220352173
I0201 13:48:24.648722 139923868813056 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.8598774671554565, loss=3.0427446365356445
I0201 13:49:11.551115 139923852027648 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.7779399156570435, loss=3.2798566818237305
I0201 13:49:57.697286 139923868813056 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.7876702547073364, loss=3.008056163787842
I0201 13:50:44.500040 139923852027648 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.313983917236328, loss=4.559057712554932
I0201 13:51:31.172800 139923868813056 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.8193148374557495, loss=3.1589629650115967
I0201 13:52:17.829489 139923852027648 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.7814873456954956, loss=3.661095380783081
I0201 13:52:43.881177 140085747812160 spec.py:321] Evaluating on the training split.
I0201 13:52:54.286144 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 13:53:16.979691 140085747812160 spec.py:349] Evaluating on the test split.
I0201 13:53:18.627482 140085747812160 submission_runner.py:408] Time since start: 67399.85s, 	Step: 134857, 	{'train/accuracy': 0.8421288728713989, 'train/loss': 0.8246394395828247, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.183579444885254, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.7756555080413818, 'test/num_examples': 10000, 'score': 62211.888147592545, 'total_duration': 67399.85404849052, 'accumulated_submission_time': 62211.888147592545, 'accumulated_eval_time': 5174.0214302539825, 'accumulated_logging_time': 6.409978866577148}
I0201 13:53:18.664314 139923868813056 logging_writer.py:48] [134857] accumulated_eval_time=5174.021430, accumulated_logging_time=6.409979, accumulated_submission_time=62211.888148, global_step=134857, preemption_count=0, score=62211.888148, test/accuracy=0.637200, test/loss=1.775656, test/num_examples=10000, total_duration=67399.854048, train/accuracy=0.842129, train/loss=0.824639, validation/accuracy=0.755940, validation/loss=1.183579, validation/num_examples=50000
I0201 13:53:35.942918 139923852027648 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.8424979448318481, loss=2.976569414138794
I0201 13:54:20.456101 139923868813056 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.7893023490905762, loss=3.1844024658203125
I0201 13:55:07.606264 139923852027648 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.0342297554016113, loss=4.321256637573242
I0201 13:55:54.819783 139923868813056 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.9471186399459839, loss=3.749941349029541
I0201 13:56:41.442498 139923852027648 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.2798233032226562, loss=4.505260944366455
I0201 13:57:28.421173 139923868813056 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.7672030925750732, loss=3.3250741958618164
I0201 13:58:15.202722 139923852027648 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.8575153350830078, loss=2.987610340118408
I0201 13:59:01.960151 139923868813056 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.9758472442626953, loss=2.901029109954834
I0201 13:59:48.628664 139923852027648 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.909026861190796, loss=3.011608839035034
I0201 14:00:18.708737 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:00:28.858433 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:00:53.182491 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:00:54.831426 140085747812160 submission_runner.py:408] Time since start: 67856.06s, 	Step: 135766, 	{'train/accuracy': 0.847460925579071, 'train/loss': 0.8194810152053833, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.1992911100387573, 'validation/num_examples': 50000, 'test/accuracy': 0.635200023651123, 'test/loss': 1.788246989250183, 'test/num_examples': 10000, 'score': 62631.872718811035, 'total_duration': 67856.05798530579, 'accumulated_submission_time': 62631.872718811035, 'accumulated_eval_time': 5210.1441123485565, 'accumulated_logging_time': 6.4558424949646}
I0201 14:00:54.871857 139923868813056 logging_writer.py:48] [135766] accumulated_eval_time=5210.144112, accumulated_logging_time=6.455842, accumulated_submission_time=62631.872719, global_step=135766, preemption_count=0, score=62631.872719, test/accuracy=0.635200, test/loss=1.788247, test/num_examples=10000, total_duration=67856.057985, train/accuracy=0.847461, train/loss=0.819481, validation/accuracy=0.755780, validation/loss=1.199291, validation/num_examples=50000
I0201 14:01:08.638370 139923852027648 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.8460826873779297, loss=3.9084911346435547
I0201 14:01:52.284592 139923868813056 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.8437522649765015, loss=3.3207077980041504
I0201 14:02:39.409279 139923852027648 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.06412672996521, loss=4.0070061683654785
I0201 14:03:26.285620 139923868813056 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.8412011861801147, loss=3.3344616889953613
I0201 14:04:13.200590 139923852027648 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.0245494842529297, loss=3.0252890586853027
I0201 14:04:59.856626 139923868813056 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.060445547103882, loss=3.0498881340026855
I0201 14:05:46.721814 139923852027648 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.9100900888442993, loss=3.0383970737457275
I0201 14:06:33.293386 139923868813056 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.0543153285980225, loss=2.9850552082061768
I0201 14:07:19.900852 139923852027648 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.085456371307373, loss=4.173213005065918
I0201 14:07:55.072468 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:08:05.547136 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:08:31.563816 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:08:33.211951 140085747812160 submission_runner.py:408] Time since start: 68314.44s, 	Step: 136677, 	{'train/accuracy': 0.8434374928474426, 'train/loss': 0.8096722364425659, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.1676242351531982, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.7473360300064087, 'test/num_examples': 10000, 'score': 63052.01003956795, 'total_duration': 68314.43851542473, 'accumulated_submission_time': 63052.01003956795, 'accumulated_eval_time': 5248.283586978912, 'accumulated_logging_time': 6.5085837841033936}
I0201 14:08:33.251892 139923868813056 logging_writer.py:48] [136677] accumulated_eval_time=5248.283587, accumulated_logging_time=6.508584, accumulated_submission_time=63052.010040, global_step=136677, preemption_count=0, score=63052.010040, test/accuracy=0.641600, test/loss=1.747336, test/num_examples=10000, total_duration=68314.438515, train/accuracy=0.843437, train/loss=0.809672, validation/accuracy=0.757640, validation/loss=1.167624, validation/num_examples=50000
I0201 14:08:42.683655 139923852027648 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.0653088092803955, loss=2.957148551940918
I0201 14:09:25.783133 139923868813056 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.9783320426940918, loss=3.1138010025024414
I0201 14:10:12.788367 139923852027648 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.965585708618164, loss=3.222975730895996
I0201 14:10:59.470092 139923868813056 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.8098725080490112, loss=2.9892492294311523
I0201 14:11:46.419345 139923852027648 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.9283803701400757, loss=3.9066319465637207
I0201 14:12:33.153387 139923868813056 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.0054054260253906, loss=2.9546637535095215
I0201 14:13:19.884170 139923852027648 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.056617021560669, loss=3.7468769550323486
I0201 14:14:06.682602 139923868813056 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.793586254119873, loss=2.9499480724334717
I0201 14:14:53.601689 139923852027648 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.9644509553909302, loss=2.9966206550598145
I0201 14:15:33.629392 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:15:43.932583 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:16:09.596413 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:16:11.236453 140085747812160 submission_runner.py:408] Time since start: 68772.46s, 	Step: 137587, 	{'train/accuracy': 0.8507421612739563, 'train/loss': 0.7920365929603577, 'validation/accuracy': 0.7602199912071228, 'validation/loss': 1.160401463508606, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.75056791305542, 'test/num_examples': 10000, 'score': 63472.32473063469, 'total_duration': 68772.46301627159, 'accumulated_submission_time': 63472.32473063469, 'accumulated_eval_time': 5285.890654087067, 'accumulated_logging_time': 6.560576677322388}
I0201 14:16:11.275414 139923868813056 logging_writer.py:48] [137587] accumulated_eval_time=5285.890654, accumulated_logging_time=6.560577, accumulated_submission_time=63472.324731, global_step=137587, preemption_count=0, score=63472.324731, test/accuracy=0.640200, test/loss=1.750568, test/num_examples=10000, total_duration=68772.463016, train/accuracy=0.850742, train/loss=0.792037, validation/accuracy=0.760220, validation/loss=1.160401, validation/num_examples=50000
I0201 14:16:16.771948 139923852027648 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.77504301071167, loss=3.0449864864349365
I0201 14:16:58.987595 139923868813056 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.9665803909301758, loss=4.178813457489014
I0201 14:17:45.571214 139923852027648 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.071901321411133, loss=3.0808143615722656
I0201 14:18:32.524425 139923868813056 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.00048565864563, loss=3.8491926193237305
I0201 14:19:19.391592 139923852027648 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.8808249235153198, loss=3.2543387413024902
I0201 14:20:05.916421 139923868813056 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.8450216054916382, loss=3.5993316173553467
I0201 14:20:52.764743 139923852027648 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.3560221195220947, loss=4.30007266998291
I0201 14:21:39.587178 139923868813056 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.8998997211456299, loss=2.981722116470337
I0201 14:22:26.403347 139923852027648 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.3421425819396973, loss=4.34944486618042
I0201 14:23:11.665373 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:23:22.125892 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:23:45.606097 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:23:47.248550 140085747812160 submission_runner.py:408] Time since start: 69228.48s, 	Step: 138498, 	{'train/accuracy': 0.8550781011581421, 'train/loss': 0.7727980613708496, 'validation/accuracy': 0.7599799633026123, 'validation/loss': 1.1643576622009277, 'validation/num_examples': 50000, 'test/accuracy': 0.6422000527381897, 'test/loss': 1.75530207157135, 'test/num_examples': 10000, 'score': 63892.652406454086, 'total_duration': 69228.47512078285, 'accumulated_submission_time': 63892.652406454086, 'accumulated_eval_time': 5321.47384428978, 'accumulated_logging_time': 6.611251592636108}
I0201 14:23:47.286595 139923868813056 logging_writer.py:48] [138498] accumulated_eval_time=5321.473844, accumulated_logging_time=6.611252, accumulated_submission_time=63892.652406, global_step=138498, preemption_count=0, score=63892.652406, test/accuracy=0.642200, test/loss=1.755302, test/num_examples=10000, total_duration=69228.475121, train/accuracy=0.855078, train/loss=0.772798, validation/accuracy=0.759980, validation/loss=1.164358, validation/num_examples=50000
I0201 14:23:48.470240 139923852027648 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.2135226726531982, loss=4.526669979095459
I0201 14:24:30.217786 139923868813056 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.9043753147125244, loss=3.283578872680664
I0201 14:25:16.799995 139923852027648 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.9551379680633545, loss=3.384268283843994
I0201 14:26:03.805554 139923868813056 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.8376357555389404, loss=3.34702730178833
I0201 14:26:50.513191 139923852027648 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.018529176712036, loss=3.052917957305908
I0201 14:27:37.109939 139923868813056 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.001551866531372, loss=4.120861530303955
I0201 14:28:23.656916 139923852027648 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.907450556755066, loss=2.9806275367736816
I0201 14:29:10.274075 139923868813056 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.9188727140426636, loss=3.0441527366638184
I0201 14:29:57.017582 139923852027648 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.9807521104812622, loss=3.1342837810516357
I0201 14:30:43.779400 139923868813056 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.3863933086395264, loss=4.389166355133057
I0201 14:30:47.675013 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:30:58.176603 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:31:26.185985 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:31:27.829051 140085747812160 submission_runner.py:408] Time since start: 69689.06s, 	Step: 139410, 	{'train/accuracy': 0.8495312333106995, 'train/loss': 0.8162821531295776, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 1.1751127243041992, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.7389004230499268, 'test/num_examples': 10000, 'score': 64312.980843544006, 'total_duration': 69689.05562400818, 'accumulated_submission_time': 64312.980843544006, 'accumulated_eval_time': 5361.627897024155, 'accumulated_logging_time': 6.65903902053833}
I0201 14:31:27.866787 139923852027648 logging_writer.py:48] [139410] accumulated_eval_time=5361.627897, accumulated_logging_time=6.659039, accumulated_submission_time=64312.980844, global_step=139410, preemption_count=0, score=64312.980844, test/accuracy=0.650100, test/loss=1.738900, test/num_examples=10000, total_duration=69689.055624, train/accuracy=0.849531, train/loss=0.816282, validation/accuracy=0.762040, validation/loss=1.175113, validation/num_examples=50000
I0201 14:32:05.176621 139923868813056 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.0795137882232666, loss=4.047650337219238
I0201 14:32:51.858426 139923852027648 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.9320122003555298, loss=2.8847157955169678
I0201 14:33:38.864495 139923868813056 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.9913674592971802, loss=3.088325023651123
I0201 14:34:25.639864 139923852027648 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.9511998891830444, loss=2.996948719024658
I0201 14:35:12.451755 139923868813056 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.9922890663146973, loss=2.8459746837615967
I0201 14:35:59.177604 139923852027648 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.7785468101501465, loss=2.9166464805603027
I0201 14:36:46.112961 139923868813056 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.8897533416748047, loss=3.1573262214660645
I0201 14:37:32.865232 139923852027648 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.8808887004852295, loss=3.3972549438476562
I0201 14:38:19.437453 139923868813056 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.9305647611618042, loss=2.9777209758758545
I0201 14:38:27.929116 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:38:38.228138 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:39:01.831611 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:39:03.476670 140085747812160 submission_runner.py:408] Time since start: 70144.70s, 	Step: 140320, 	{'train/accuracy': 0.8509179353713989, 'train/loss': 0.797985315322876, 'validation/accuracy': 0.7607600092887878, 'validation/loss': 1.1660995483398438, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.7588180303573608, 'test/num_examples': 10000, 'score': 64732.98385691643, 'total_duration': 70144.70323824883, 'accumulated_submission_time': 64732.98385691643, 'accumulated_eval_time': 5397.175455093384, 'accumulated_logging_time': 6.705945253372192}
I0201 14:39:03.513344 139923852027648 logging_writer.py:48] [140320] accumulated_eval_time=5397.175455, accumulated_logging_time=6.705945, accumulated_submission_time=64732.983857, global_step=140320, preemption_count=0, score=64732.983857, test/accuracy=0.641200, test/loss=1.758818, test/num_examples=10000, total_duration=70144.703238, train/accuracy=0.850918, train/loss=0.797985, validation/accuracy=0.760760, validation/loss=1.166100, validation/num_examples=50000
I0201 14:39:36.085703 139923868813056 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.8382736444473267, loss=3.2887496948242188
I0201 14:40:22.473999 139923852027648 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.902454137802124, loss=3.9555296897888184
I0201 14:41:09.407099 139923868813056 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.0963401794433594, loss=2.8614699840545654
I0201 14:41:56.078191 139923852027648 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.114427089691162, loss=2.948685646057129
I0201 14:42:43.151375 139923868813056 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.8712828159332275, loss=2.938516616821289
I0201 14:43:29.774895 139923852027648 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.0026509761810303, loss=2.9340319633483887
I0201 14:44:16.735437 139923868813056 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.0246331691741943, loss=3.211317539215088
I0201 14:45:03.390802 139923852027648 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.2243566513061523, loss=4.339757919311523
I0201 14:45:50.346472 139923868813056 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.1178324222564697, loss=3.79316782951355
I0201 14:46:03.540086 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:46:13.762231 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:46:37.072186 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:46:38.718639 140085747812160 submission_runner.py:408] Time since start: 70599.95s, 	Step: 141230, 	{'train/accuracy': 0.8560156226158142, 'train/loss': 0.7755244374275208, 'validation/accuracy': 0.7628999948501587, 'validation/loss': 1.1578243970870972, 'validation/num_examples': 50000, 'test/accuracy': 0.648900032043457, 'test/loss': 1.7374924421310425, 'test/num_examples': 10000, 'score': 65152.95014190674, 'total_duration': 70599.94521164894, 'accumulated_submission_time': 65152.95014190674, 'accumulated_eval_time': 5432.35400223732, 'accumulated_logging_time': 6.752318620681763}
I0201 14:46:38.757398 139923852027648 logging_writer.py:48] [141230] accumulated_eval_time=5432.354002, accumulated_logging_time=6.752319, accumulated_submission_time=65152.950142, global_step=141230, preemption_count=0, score=65152.950142, test/accuracy=0.648900, test/loss=1.737492, test/num_examples=10000, total_duration=70599.945212, train/accuracy=0.856016, train/loss=0.775524, validation/accuracy=0.762900, validation/loss=1.157824, validation/num_examples=50000
I0201 14:47:06.624656 139923868813056 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.219792366027832, loss=4.241902828216553
I0201 14:47:53.367278 139923852027648 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.9888006448745728, loss=3.4374334812164307
I0201 14:48:40.011746 139923868813056 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.8562369346618652, loss=3.234419107437134
I0201 14:49:26.864504 139923852027648 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.3320248126983643, loss=4.113044261932373
I0201 14:50:13.557955 139923868813056 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.4344990253448486, loss=4.20412540435791
I0201 14:51:00.230979 139923852027648 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.1226541996002197, loss=2.90682053565979
I0201 14:51:46.850308 139923868813056 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.4930903911590576, loss=4.549906253814697
I0201 14:52:33.734430 139923852027648 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.0544276237487793, loss=3.0060455799102783
I0201 14:53:20.544727 139923868813056 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.0004379749298096, loss=3.613593816757202
I0201 14:53:38.898839 140085747812160 spec.py:321] Evaluating on the training split.
I0201 14:53:49.377525 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 14:54:15.441732 140085747812160 spec.py:349] Evaluating on the test split.
I0201 14:54:17.080944 140085747812160 submission_runner.py:408] Time since start: 71058.31s, 	Step: 142141, 	{'train/accuracy': 0.85267573595047, 'train/loss': 0.7707575559616089, 'validation/accuracy': 0.7630599737167358, 'validation/loss': 1.145895004272461, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.714654803276062, 'test/num_examples': 10000, 'score': 65573.02896523476, 'total_duration': 71058.30751371384, 'accumulated_submission_time': 65573.02896523476, 'accumulated_eval_time': 5470.536109209061, 'accumulated_logging_time': 6.80316686630249}
I0201 14:54:17.119949 139923852027648 logging_writer.py:48] [142141] accumulated_eval_time=5470.536109, accumulated_logging_time=6.803167, accumulated_submission_time=65573.028965, global_step=142141, preemption_count=0, score=65573.028965, test/accuracy=0.649900, test/loss=1.714655, test/num_examples=10000, total_duration=71058.307514, train/accuracy=0.852676, train/loss=0.770758, validation/accuracy=0.763060, validation/loss=1.145895, validation/num_examples=50000
I0201 14:54:40.679068 139923868813056 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.965683102607727, loss=2.961211681365967
I0201 14:55:26.330039 139923852027648 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.077454090118408, loss=3.7806668281555176
I0201 14:56:13.418458 139923868813056 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.018270254135132, loss=3.0649380683898926
I0201 14:56:59.879164 139923852027648 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.9105885028839111, loss=2.8010544776916504
I0201 14:57:46.713612 139923868813056 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.9953558444976807, loss=2.9023077487945557
I0201 14:58:33.234674 139923852027648 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.0024631023406982, loss=3.215298652648926
I0201 14:59:19.803997 139923868813056 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.936676025390625, loss=3.5570473670959473
I0201 15:00:06.580933 139923852027648 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.132418632507324, loss=3.004218101501465
I0201 15:00:53.374870 139923868813056 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.356208324432373, loss=4.291906833648682
I0201 15:01:17.306590 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:01:28.034568 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:01:54.639618 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:01:56.280783 140085747812160 submission_runner.py:408] Time since start: 71517.51s, 	Step: 143053, 	{'train/accuracy': 0.8552343845367432, 'train/loss': 0.7671908140182495, 'validation/accuracy': 0.7658599615097046, 'validation/loss': 1.1423100233078003, 'validation/num_examples': 50000, 'test/accuracy': 0.6479000449180603, 'test/loss': 1.7201541662216187, 'test/num_examples': 10000, 'score': 65993.15498352051, 'total_duration': 71517.50735139847, 'accumulated_submission_time': 65993.15498352051, 'accumulated_eval_time': 5509.510308504105, 'accumulated_logging_time': 6.851216554641724}
I0201 15:01:56.321486 139923852027648 logging_writer.py:48] [143053] accumulated_eval_time=5509.510309, accumulated_logging_time=6.851217, accumulated_submission_time=65993.154984, global_step=143053, preemption_count=0, score=65993.154984, test/accuracy=0.647900, test/loss=1.720154, test/num_examples=10000, total_duration=71517.507351, train/accuracy=0.855234, train/loss=0.767191, validation/accuracy=0.765860, validation/loss=1.142310, validation/num_examples=50000
I0201 15:02:15.168278 139923868813056 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.1874067783355713, loss=2.8682820796966553
I0201 15:03:00.246699 139923852027648 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.8866146802902222, loss=2.9405674934387207
I0201 15:03:47.174375 139923868813056 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.5705690383911133, loss=4.434075832366943
I0201 15:04:33.841970 139923852027648 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.152310848236084, loss=3.0371103286743164
I0201 15:05:20.632919 139923868813056 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.0620381832122803, loss=2.9932808876037598
I0201 15:06:07.340505 139923852027648 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.23457407951355, loss=3.299370765686035
I0201 15:06:54.315107 139923868813056 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.9258354902267456, loss=3.628145217895508
I0201 15:07:40.894855 139923852027648 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.0151360034942627, loss=2.9400739669799805
I0201 15:08:27.815556 139923868813056 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.0522892475128174, loss=3.6582329273223877
I0201 15:08:56.321074 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:09:06.970122 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:09:32.418316 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:09:34.066628 140085747812160 submission_runner.py:408] Time since start: 71975.29s, 	Step: 143963, 	{'train/accuracy': 0.8605859279632568, 'train/loss': 0.7456603646278381, 'validation/accuracy': 0.7659599781036377, 'validation/loss': 1.1388903856277466, 'validation/num_examples': 50000, 'test/accuracy': 0.648300051689148, 'test/loss': 1.7129008769989014, 'test/num_examples': 10000, 'score': 66413.09290719032, 'total_duration': 71975.2932009697, 'accumulated_submission_time': 66413.09290719032, 'accumulated_eval_time': 5547.255858421326, 'accumulated_logging_time': 6.90261173248291}
I0201 15:09:34.105628 139923852027648 logging_writer.py:48] [143963] accumulated_eval_time=5547.255858, accumulated_logging_time=6.902612, accumulated_submission_time=66413.092907, global_step=143963, preemption_count=0, score=66413.092907, test/accuracy=0.648300, test/loss=1.712901, test/num_examples=10000, total_duration=71975.293201, train/accuracy=0.860586, train/loss=0.745660, validation/accuracy=0.765960, validation/loss=1.138890, validation/num_examples=50000
I0201 15:09:49.034991 139923868813056 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.353633403778076, loss=2.894526958465576
I0201 15:10:32.768037 139923852027648 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.096470832824707, loss=4.318948745727539
I0201 15:11:19.748386 139923868813056 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.20591402053833, loss=2.9984683990478516
I0201 15:12:06.701378 139923852027648 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.034310817718506, loss=2.8293192386627197
I0201 15:12:53.859571 139923868813056 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.3431427478790283, loss=2.9195780754089355
I0201 15:13:40.690835 139923852027648 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.2545599937438965, loss=2.989290714263916
I0201 15:14:27.480759 139923868813056 logging_writer.py:48] [144600] global_step=144600, grad_norm=1.996726632118225, loss=2.9035696983337402
I0201 15:15:14.075861 139923852027648 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.056976318359375, loss=2.8715639114379883
I0201 15:16:00.726905 139923868813056 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.164324998855591, loss=3.2686922550201416
I0201 15:16:34.210219 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:16:44.589542 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:17:10.216524 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:17:11.855341 140085747812160 submission_runner.py:408] Time since start: 72433.08s, 	Step: 144873, 	{'train/accuracy': 0.8684179782867432, 'train/loss': 0.724058210849762, 'validation/accuracy': 0.7653599977493286, 'validation/loss': 1.1371018886566162, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.7198455333709717, 'test/num_examples': 10000, 'score': 66833.13764357567, 'total_duration': 72433.08190202713, 'accumulated_submission_time': 66833.13764357567, 'accumulated_eval_time': 5584.90097284317, 'accumulated_logging_time': 6.950519561767578}
I0201 15:17:11.898329 139923852027648 logging_writer.py:48] [144873] accumulated_eval_time=5584.900973, accumulated_logging_time=6.950520, accumulated_submission_time=66833.137644, global_step=144873, preemption_count=0, score=66833.137644, test/accuracy=0.647600, test/loss=1.719846, test/num_examples=10000, total_duration=72433.081902, train/accuracy=0.868418, train/loss=0.724058, validation/accuracy=0.765360, validation/loss=1.137102, validation/num_examples=50000
I0201 15:17:22.895142 139923868813056 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.034163475036621, loss=2.9306418895721436
I0201 15:18:06.505701 139923852027648 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.084505319595337, loss=3.529853343963623
I0201 15:18:53.244176 139923868813056 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.8685718774795532, loss=3.0399961471557617
I0201 15:19:40.123990 139923852027648 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.070672035217285, loss=2.9610812664031982
I0201 15:20:26.799713 139923868813056 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.147751569747925, loss=2.87526535987854
I0201 15:21:13.575032 139923852027648 logging_writer.py:48] [145400] global_step=145400, grad_norm=1.9874902963638306, loss=2.913816452026367
I0201 15:22:00.304960 139923868813056 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.982653021812439, loss=2.8696742057800293
I0201 15:22:47.397454 139923852027648 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.6055684089660645, loss=4.278634071350098
I0201 15:23:34.078965 139923868813056 logging_writer.py:48] [145700] global_step=145700, grad_norm=1.9567246437072754, loss=2.823843240737915
I0201 15:24:11.884122 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:24:22.234228 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:24:50.547578 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:24:52.189975 140085747812160 submission_runner.py:408] Time since start: 72893.42s, 	Step: 145782, 	{'train/accuracy': 0.8575780987739563, 'train/loss': 0.7682866454124451, 'validation/accuracy': 0.764519989490509, 'validation/loss': 1.1535091400146484, 'validation/num_examples': 50000, 'test/accuracy': 0.6463000178337097, 'test/loss': 1.7320133447647095, 'test/num_examples': 10000, 'score': 67253.06000828743, 'total_duration': 72893.4165430069, 'accumulated_submission_time': 67253.06000828743, 'accumulated_eval_time': 5625.206836462021, 'accumulated_logging_time': 7.006183862686157}
I0201 15:24:52.230657 139923852027648 logging_writer.py:48] [145782] accumulated_eval_time=5625.206836, accumulated_logging_time=7.006184, accumulated_submission_time=67253.060008, global_step=145782, preemption_count=0, score=67253.060008, test/accuracy=0.646300, test/loss=1.732013, test/num_examples=10000, total_duration=72893.416543, train/accuracy=0.857578, train/loss=0.768287, validation/accuracy=0.764520, validation/loss=1.153509, validation/num_examples=50000
I0201 15:24:59.685980 139923868813056 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.148400068283081, loss=3.757058620452881
I0201 15:25:42.451152 139923852027648 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.558361053466797, loss=4.018612384796143
I0201 15:26:29.405309 139923868813056 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.1207709312438965, loss=2.9952118396759033
I0201 15:27:16.594463 139923852027648 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.0757694244384766, loss=2.91740345954895
I0201 15:28:03.412140 139923868813056 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.23555326461792, loss=2.8646769523620605
I0201 15:28:50.225259 139923852027648 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.0086095333099365, loss=3.0962390899658203
I0201 15:29:37.060648 139923868813056 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.8513842821121216, loss=3.365084409713745
I0201 15:30:23.588792 139923852027648 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.736752510070801, loss=4.457645416259766
I0201 15:31:10.179956 139923868813056 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.155575752258301, loss=2.913938522338867
I0201 15:31:52.277924 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:32:02.898324 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:32:27.964187 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:32:29.602269 140085747812160 submission_runner.py:408] Time since start: 73350.83s, 	Step: 146692, 	{'train/accuracy': 0.8640234470367432, 'train/loss': 0.74609375, 'validation/accuracy': 0.7672399878501892, 'validation/loss': 1.1380559206008911, 'validation/num_examples': 50000, 'test/accuracy': 0.6510000228881836, 'test/loss': 1.7285139560699463, 'test/num_examples': 10000, 'score': 67673.04480743408, 'total_duration': 73350.82883524895, 'accumulated_submission_time': 67673.04480743408, 'accumulated_eval_time': 5662.5311868190765, 'accumulated_logging_time': 7.058150768280029}
I0201 15:32:29.642849 139923852027648 logging_writer.py:48] [146692] accumulated_eval_time=5662.531187, accumulated_logging_time=7.058151, accumulated_submission_time=67673.044807, global_step=146692, preemption_count=0, score=67673.044807, test/accuracy=0.651000, test/loss=1.728514, test/num_examples=10000, total_duration=73350.828835, train/accuracy=0.864023, train/loss=0.746094, validation/accuracy=0.767240, validation/loss=1.138056, validation/num_examples=50000
I0201 15:32:33.182231 139923868813056 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.0196127891540527, loss=2.933108329772949
I0201 15:33:15.118173 139923852027648 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.9104256629943848, loss=4.457558631896973
I0201 15:34:01.947399 139923868813056 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.0639045238494873, loss=3.118088722229004
I0201 15:34:48.806000 139923852027648 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.1031458377838135, loss=2.880171775817871
I0201 15:35:35.305882 139923868813056 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.104351282119751, loss=2.956054210662842
I0201 15:36:21.950635 139923852027648 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.235621452331543, loss=3.898613452911377
I0201 15:37:08.818694 139923868813056 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.077793598175049, loss=2.8670642375946045
I0201 15:37:55.636206 139923852027648 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.0821218490600586, loss=3.7857601642608643
I0201 15:38:42.334287 139923868813056 logging_writer.py:48] [147500] global_step=147500, grad_norm=1.9840493202209473, loss=3.480961561203003
I0201 15:39:28.826668 139923852027648 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.1919548511505127, loss=2.848074436187744
I0201 15:39:29.998202 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:39:40.208901 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:40:04.157522 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:40:05.801327 140085747812160 submission_runner.py:408] Time since start: 73807.03s, 	Step: 147604, 	{'train/accuracy': 0.8709960579872131, 'train/loss': 0.6939271092414856, 'validation/accuracy': 0.7691799998283386, 'validation/loss': 1.1134551763534546, 'validation/num_examples': 50000, 'test/accuracy': 0.6548000574111938, 'test/loss': 1.6992754936218262, 'test/num_examples': 10000, 'score': 68093.3377597332, 'total_duration': 73807.02789735794, 'accumulated_submission_time': 68093.3377597332, 'accumulated_eval_time': 5698.334320783615, 'accumulated_logging_time': 7.110531806945801}
I0201 15:40:05.839626 139923868813056 logging_writer.py:48] [147604] accumulated_eval_time=5698.334321, accumulated_logging_time=7.110532, accumulated_submission_time=68093.337760, global_step=147604, preemption_count=0, score=68093.337760, test/accuracy=0.654800, test/loss=1.699275, test/num_examples=10000, total_duration=73807.027897, train/accuracy=0.870996, train/loss=0.693927, validation/accuracy=0.769180, validation/loss=1.113455, validation/num_examples=50000
I0201 15:40:45.666674 139923852027648 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.0981595516204834, loss=3.116476058959961
I0201 15:41:31.869891 139923868813056 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.1742026805877686, loss=2.9216625690460205
I0201 15:42:18.962826 139923852027648 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.1287612915039062, loss=2.9106452465057373
I0201 15:43:05.900998 139923868813056 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.201288938522339, loss=2.922780990600586
I0201 15:43:52.767726 139923852027648 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.069668769836426, loss=3.0961897373199463
I0201 15:44:39.579700 139923868813056 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.2124598026275635, loss=2.95876407623291
I0201 15:45:26.427943 139923852027648 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.085695743560791, loss=3.0920369625091553
I0201 15:46:13.267780 139923868813056 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.0994348526000977, loss=3.7663931846618652
I0201 15:47:00.063466 139923852027648 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.2675390243530273, loss=4.044885635375977
I0201 15:47:05.892449 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:47:16.255615 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:47:41.263813 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:47:42.902844 140085747812160 submission_runner.py:408] Time since start: 74264.13s, 	Step: 148514, 	{'train/accuracy': 0.8633202910423279, 'train/loss': 0.7357399463653564, 'validation/accuracy': 0.7691999673843384, 'validation/loss': 1.1223732233047485, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.6979526281356812, 'test/num_examples': 10000, 'score': 68513.33066034317, 'total_duration': 74264.12940835953, 'accumulated_submission_time': 68513.33066034317, 'accumulated_eval_time': 5735.344721794128, 'accumulated_logging_time': 7.158812522888184}
I0201 15:47:42.943247 139923868813056 logging_writer.py:48] [148514] accumulated_eval_time=5735.344722, accumulated_logging_time=7.158813, accumulated_submission_time=68513.330660, global_step=148514, preemption_count=0, score=68513.330660, test/accuracy=0.654100, test/loss=1.697953, test/num_examples=10000, total_duration=74264.129408, train/accuracy=0.863320, train/loss=0.735740, validation/accuracy=0.769200, validation/loss=1.122373, validation/num_examples=50000
I0201 15:48:18.197521 139923852027648 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.114254951477051, loss=2.865367889404297
I0201 15:49:04.803634 139923868813056 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.002671480178833, loss=2.808297634124756
I0201 15:49:52.669271 139923852027648 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.584273338317871, loss=4.36163330078125
I0201 15:50:39.470815 139923868813056 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.262035369873047, loss=3.0276293754577637
I0201 15:51:26.311101 139923852027648 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.0636327266693115, loss=2.803450345993042
I0201 15:52:13.116293 139923868813056 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.2040035724639893, loss=2.871142625808716
I0201 15:53:00.042546 139923852027648 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.3112189769744873, loss=2.9025368690490723
I0201 15:53:46.761960 139923868813056 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.440305471420288, loss=3.214733600616455
I0201 15:54:33.645181 139923852027648 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.0239148139953613, loss=4.3701066970825195
I0201 15:54:43.093629 140085747812160 spec.py:321] Evaluating on the training split.
I0201 15:54:54.054082 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 15:55:19.871537 140085747812160 spec.py:349] Evaluating on the test split.
I0201 15:55:21.526063 140085747812160 submission_runner.py:408] Time since start: 74722.75s, 	Step: 149422, 	{'train/accuracy': 0.8650000095367432, 'train/loss': 0.7219473123550415, 'validation/accuracy': 0.7705199718475342, 'validation/loss': 1.1146769523620605, 'validation/num_examples': 50000, 'test/accuracy': 0.65420001745224, 'test/loss': 1.6978703737258911, 'test/num_examples': 10000, 'score': 68933.41969394684, 'total_duration': 74722.75263619423, 'accumulated_submission_time': 68933.41969394684, 'accumulated_eval_time': 5773.777179718018, 'accumulated_logging_time': 7.209390163421631}
I0201 15:55:21.564622 139923868813056 logging_writer.py:48] [149422] accumulated_eval_time=5773.777180, accumulated_logging_time=7.209390, accumulated_submission_time=68933.419694, global_step=149422, preemption_count=0, score=68933.419694, test/accuracy=0.654200, test/loss=1.697870, test/num_examples=10000, total_duration=74722.752636, train/accuracy=0.865000, train/loss=0.721947, validation/accuracy=0.770520, validation/loss=1.114677, validation/num_examples=50000
I0201 15:55:53.219310 139923852027648 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.0970041751861572, loss=2.7717084884643555
I0201 15:56:39.883710 139923868813056 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.0282022953033447, loss=3.429171562194824
I0201 15:57:26.972419 139923852027648 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.226191997528076, loss=3.660066843032837
I0201 15:58:13.613888 139923868813056 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.227522373199463, loss=2.976001262664795
I0201 15:59:01.463593 139923852027648 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.1264758110046387, loss=2.9277172088623047
I0201 15:59:47.745641 139923868813056 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.234391212463379, loss=2.9574103355407715
I0201 16:00:34.603929 139923852027648 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.159728765487671, loss=3.6245386600494385
I0201 16:01:21.423243 139923868813056 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.217179775238037, loss=2.899233818054199
I0201 16:02:08.207939 139923852027648 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.4087018966674805, loss=4.140587329864502
I0201 16:02:21.558164 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:02:32.045513 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:02:58.555398 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:03:00.201970 140085747812160 submission_runner.py:408] Time since start: 75181.43s, 	Step: 150330, 	{'train/accuracy': 0.8721874952316284, 'train/loss': 0.741978645324707, 'validation/accuracy': 0.7699599862098694, 'validation/loss': 1.1527972221374512, 'validation/num_examples': 50000, 'test/accuracy': 0.652400016784668, 'test/loss': 1.7470016479492188, 'test/num_examples': 10000, 'score': 69353.35374689102, 'total_duration': 75181.42854118347, 'accumulated_submission_time': 69353.35374689102, 'accumulated_eval_time': 5812.420975446701, 'accumulated_logging_time': 7.256996393203735}
I0201 16:03:00.242237 139923868813056 logging_writer.py:48] [150330] accumulated_eval_time=5812.420975, accumulated_logging_time=7.256996, accumulated_submission_time=69353.353747, global_step=150330, preemption_count=0, score=69353.353747, test/accuracy=0.652400, test/loss=1.747002, test/num_examples=10000, total_duration=75181.428541, train/accuracy=0.872187, train/loss=0.741979, validation/accuracy=0.769960, validation/loss=1.152797, validation/num_examples=50000
I0201 16:03:28.265370 139923852027648 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.124056339263916, loss=3.808847665786743
I0201 16:04:14.906594 139923868813056 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.007850408554077, loss=2.89439058303833
I0201 16:05:01.987144 139923852027648 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.104130268096924, loss=2.9726407527923584
I0201 16:05:48.963601 139923868813056 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.7705798149108887, loss=4.246709823608398
I0201 16:06:35.957442 139923852027648 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.080183982849121, loss=3.4036214351654053
I0201 16:07:22.673667 139923868813056 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.0176455974578857, loss=2.815829038619995
I0201 16:08:09.744447 139923852027648 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.23061203956604, loss=2.7590527534484863
I0201 16:08:56.660050 139923868813056 logging_writer.py:48] [151100] global_step=151100, grad_norm=1.9869937896728516, loss=2.9364511966705322
I0201 16:09:43.472246 139923852027648 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.1055848598480225, loss=2.834580659866333
I0201 16:10:00.510205 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:10:10.866055 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:10:37.674131 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:10:39.322836 140085747812160 submission_runner.py:408] Time since start: 75640.55s, 	Step: 151238, 	{'train/accuracy': 0.8633593320846558, 'train/loss': 0.7349731922149658, 'validation/accuracy': 0.7711600065231323, 'validation/loss': 1.12005615234375, 'validation/num_examples': 50000, 'test/accuracy': 0.6549000144004822, 'test/loss': 1.6994355916976929, 'test/num_examples': 10000, 'score': 69773.56124687195, 'total_duration': 75640.54940104485, 'accumulated_submission_time': 69773.56124687195, 'accumulated_eval_time': 5851.233624219894, 'accumulated_logging_time': 7.306897401809692}
I0201 16:10:39.361688 139923868813056 logging_writer.py:48] [151238] accumulated_eval_time=5851.233624, accumulated_logging_time=7.306897, accumulated_submission_time=69773.561247, global_step=151238, preemption_count=0, score=69773.561247, test/accuracy=0.654900, test/loss=1.699436, test/num_examples=10000, total_duration=75640.549401, train/accuracy=0.863359, train/loss=0.734973, validation/accuracy=0.771160, validation/loss=1.120056, validation/num_examples=50000
I0201 16:11:04.094257 139923852027648 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.6057066917419434, loss=4.20065975189209
I0201 16:11:50.477703 139923868813056 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.2750537395477295, loss=3.965224266052246
I0201 16:12:37.441733 139923852027648 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.2339038848876953, loss=3.5447237491607666
I0201 16:13:24.801236 139923868813056 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.347911834716797, loss=2.8792943954467773
I0201 16:14:11.513272 139923852027648 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.1527159214019775, loss=2.912386417388916
I0201 16:14:58.481792 139923868813056 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.038686513900757, loss=3.2144100666046143
I0201 16:15:45.232463 139923852027648 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.0667107105255127, loss=2.8185763359069824
I0201 16:16:32.079197 139923868813056 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.44417142868042, loss=2.9437150955200195
I0201 16:17:18.935630 139923852027648 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.3039119243621826, loss=2.912907123565674
I0201 16:17:39.689051 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:17:50.093924 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:18:13.205586 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:18:14.845857 140085747812160 submission_runner.py:408] Time since start: 76096.07s, 	Step: 152146, 	{'train/accuracy': 0.8692968487739563, 'train/loss': 0.7008588314056396, 'validation/accuracy': 0.7726799845695496, 'validation/loss': 1.1004048585891724, 'validation/num_examples': 50000, 'test/accuracy': 0.6581000089645386, 'test/loss': 1.682053804397583, 'test/num_examples': 10000, 'score': 70193.82568836212, 'total_duration': 76096.07240009308, 'accumulated_submission_time': 70193.82568836212, 'accumulated_eval_time': 5886.390437841415, 'accumulated_logging_time': 7.358208894729614}
I0201 16:18:14.888463 139923868813056 logging_writer.py:48] [152146] accumulated_eval_time=5886.390438, accumulated_logging_time=7.358209, accumulated_submission_time=70193.825688, global_step=152146, preemption_count=0, score=70193.825688, test/accuracy=0.658100, test/loss=1.682054, test/num_examples=10000, total_duration=76096.072400, train/accuracy=0.869297, train/loss=0.700859, validation/accuracy=0.772680, validation/loss=1.100405, validation/num_examples=50000
I0201 16:18:36.488440 139923852027648 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.210942268371582, loss=2.892200469970703
I0201 16:19:21.969086 139923868813056 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.4091503620147705, loss=3.8469038009643555
I0201 16:20:09.206584 139923852027648 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.156217336654663, loss=2.9663994312286377
I0201 16:20:56.080349 139923868813056 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.313284158706665, loss=3.0333480834960938
I0201 16:21:42.931308 139923852027648 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.288266181945801, loss=3.112525701522827
I0201 16:22:29.877797 139923868813056 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.2454593181610107, loss=2.8270883560180664
I0201 16:23:16.961769 139923852027648 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.355541706085205, loss=2.8636474609375
I0201 16:24:03.749044 139923868813056 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.387782335281372, loss=4.096744060516357
I0201 16:24:50.611495 139923852027648 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.3489038944244385, loss=3.682966947555542
I0201 16:25:14.866013 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:25:25.241863 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:25:50.947107 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:25:52.588426 140085747812160 submission_runner.py:408] Time since start: 76553.81s, 	Step: 153053, 	{'train/accuracy': 0.8724218606948853, 'train/loss': 0.7150508761405945, 'validation/accuracy': 0.7715199589729309, 'validation/loss': 1.1307733058929443, 'validation/num_examples': 50000, 'test/accuracy': 0.6557000279426575, 'test/loss': 1.7197359800338745, 'test/num_examples': 10000, 'score': 70613.74158143997, 'total_duration': 76553.8149971962, 'accumulated_submission_time': 70613.74158143997, 'accumulated_eval_time': 5924.112850666046, 'accumulated_logging_time': 7.41174578666687}
I0201 16:25:52.628129 139923868813056 logging_writer.py:48] [153053] accumulated_eval_time=5924.112851, accumulated_logging_time=7.411746, accumulated_submission_time=70613.741581, global_step=153053, preemption_count=0, score=70613.741581, test/accuracy=0.655700, test/loss=1.719736, test/num_examples=10000, total_duration=76553.814997, train/accuracy=0.872422, train/loss=0.715051, validation/accuracy=0.771520, validation/loss=1.130773, validation/num_examples=50000
I0201 16:26:11.468384 139923852027648 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.4631004333496094, loss=3.9935765266418457
I0201 16:26:56.170444 139923868813056 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.4535951614379883, loss=2.9401638507843018
I0201 16:27:43.267325 139923852027648 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.0564332008361816, loss=3.23241925239563
I0201 16:28:30.475247 139923868813056 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.266237735748291, loss=2.845301628112793
I0201 16:29:17.729967 139923852027648 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.2259058952331543, loss=3.5644431114196777
I0201 16:30:04.585919 139923868813056 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.229745864868164, loss=2.9042983055114746
I0201 16:30:51.538018 139923852027648 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.242528200149536, loss=2.9286859035491943
I0201 16:31:38.547614 139923868813056 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.1461291313171387, loss=3.4344730377197266
I0201 16:32:25.733265 139923852027648 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.248521089553833, loss=2.806825876235962
I0201 16:32:52.655636 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:33:03.349758 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:33:26.322775 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:33:27.961158 140085747812160 submission_runner.py:408] Time since start: 77009.19s, 	Step: 153959, 	{'train/accuracy': 0.8653124570846558, 'train/loss': 0.7207576632499695, 'validation/accuracy': 0.7726999521255493, 'validation/loss': 1.1108520030975342, 'validation/num_examples': 50000, 'test/accuracy': 0.6573000550270081, 'test/loss': 1.6898996829986572, 'test/num_examples': 10000, 'score': 71033.70966124535, 'total_duration': 77009.18772745132, 'accumulated_submission_time': 71033.70966124535, 'accumulated_eval_time': 5959.418373346329, 'accumulated_logging_time': 7.46028733253479}
I0201 16:33:28.003014 139923868813056 logging_writer.py:48] [153959] accumulated_eval_time=5959.418373, accumulated_logging_time=7.460287, accumulated_submission_time=71033.709661, global_step=153959, preemption_count=0, score=71033.709661, test/accuracy=0.657300, test/loss=1.689900, test/num_examples=10000, total_duration=77009.187727, train/accuracy=0.865312, train/loss=0.720758, validation/accuracy=0.772700, validation/loss=1.110852, validation/num_examples=50000
I0201 16:33:44.498729 139923852027648 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.2873034477233887, loss=2.8612256050109863
I0201 16:34:28.965616 139923868813056 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.9964120388031006, loss=4.329251289367676
I0201 16:35:15.869389 139923852027648 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.3484010696411133, loss=2.9359445571899414
I0201 16:36:02.789267 139923868813056 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.3726847171783447, loss=3.08198881149292
I0201 16:36:49.733501 139923852027648 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.3726277351379395, loss=2.800906181335449
I0201 16:37:36.299780 139923868813056 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.3083279132843018, loss=3.3221778869628906
I0201 16:38:22.884272 139923852027648 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.150587558746338, loss=2.8010106086730957
I0201 16:39:09.717932 139923868813056 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.328960418701172, loss=2.841672658920288
I0201 16:39:56.427935 139923852027648 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.281491756439209, loss=2.996737241744995
I0201 16:40:28.404419 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:40:38.798939 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:41:00.751942 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:41:02.407805 140085747812160 submission_runner.py:408] Time since start: 77463.63s, 	Step: 154870, 	{'train/accuracy': 0.8743554353713989, 'train/loss': 0.6960504651069641, 'validation/accuracy': 0.7752000093460083, 'validation/loss': 1.1026690006256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6573000550270081, 'test/loss': 1.684720754623413, 'test/num_examples': 10000, 'score': 71454.04873561859, 'total_duration': 77463.63437724113, 'accumulated_submission_time': 71454.04873561859, 'accumulated_eval_time': 5993.421750068665, 'accumulated_logging_time': 7.513123273849487}
I0201 16:41:02.450251 139923868813056 logging_writer.py:48] [154870] accumulated_eval_time=5993.421750, accumulated_logging_time=7.513123, accumulated_submission_time=71454.048736, global_step=154870, preemption_count=0, score=71454.048736, test/accuracy=0.657300, test/loss=1.684721, test/num_examples=10000, total_duration=77463.634377, train/accuracy=0.874355, train/loss=0.696050, validation/accuracy=0.775200, validation/loss=1.102669, validation/num_examples=50000
I0201 16:41:14.630109 139923852027648 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.1030449867248535, loss=2.872453212738037
I0201 16:41:57.907831 139923868813056 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.334568738937378, loss=3.258584976196289
I0201 16:42:45.132982 139923852027648 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.215027093887329, loss=2.971428871154785
I0201 16:43:32.033546 139923868813056 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.25722074508667, loss=2.837399482727051
I0201 16:44:18.751462 139923852027648 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.2493603229522705, loss=3.1951797008514404
I0201 16:45:05.531091 139923868813056 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.40262508392334, loss=2.8728930950164795
I0201 16:45:52.093253 139923852027648 logging_writer.py:48] [155500] global_step=155500, grad_norm=3.0651633739471436, loss=4.338187217712402
I0201 16:46:38.663694 139923868813056 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.0862367153167725, loss=2.872804641723633
I0201 16:47:25.436424 139923852027648 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.3206074237823486, loss=3.3491392135620117
I0201 16:48:02.915510 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:48:13.273611 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:48:37.541346 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:48:39.188780 140085747812160 submission_runner.py:408] Time since start: 77920.42s, 	Step: 155782, 	{'train/accuracy': 0.872851550579071, 'train/loss': 0.6959668397903442, 'validation/accuracy': 0.7761799693107605, 'validation/loss': 1.1069529056549072, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.697216510772705, 'test/num_examples': 10000, 'score': 71874.4541144371, 'total_duration': 77920.41535067558, 'accumulated_submission_time': 71874.4541144371, 'accumulated_eval_time': 6029.695031404495, 'accumulated_logging_time': 7.564982175827026}
I0201 16:48:39.234269 139923868813056 logging_writer.py:48] [155782] accumulated_eval_time=6029.695031, accumulated_logging_time=7.564982, accumulated_submission_time=71874.454114, global_step=155782, preemption_count=0, score=71874.454114, test/accuracy=0.658300, test/loss=1.697217, test/num_examples=10000, total_duration=77920.415351, train/accuracy=0.872852, train/loss=0.695967, validation/accuracy=0.776180, validation/loss=1.106953, validation/num_examples=50000
I0201 16:48:46.691024 139923852027648 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.3784451484680176, loss=2.8746042251586914
I0201 16:49:29.531674 139923868813056 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.303642988204956, loss=3.1533896923065186
I0201 16:50:16.085638 139923852027648 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.583942174911499, loss=2.9082651138305664
I0201 16:51:02.791188 139923868813056 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.4550042152404785, loss=2.942101240158081
I0201 16:51:49.473895 139923852027648 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.327913522720337, loss=2.8528239727020264
I0201 16:52:36.456858 139923868813056 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.365649938583374, loss=3.950662136077881
I0201 16:53:23.381071 139923852027648 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.195112705230713, loss=4.413699150085449
I0201 16:54:10.125017 139923868813056 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.2056541442871094, loss=2.868912935256958
I0201 16:54:56.852539 139923852027648 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.4440114498138428, loss=2.9418418407440186
I0201 16:55:39.419890 140085747812160 spec.py:321] Evaluating on the training split.
I0201 16:55:49.850240 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 16:56:12.726714 140085747812160 spec.py:349] Evaluating on the test split.
I0201 16:56:14.363197 140085747812160 submission_runner.py:408] Time since start: 78375.59s, 	Step: 156692, 	{'train/accuracy': 0.8742382526397705, 'train/loss': 0.7012655735015869, 'validation/accuracy': 0.7750200033187866, 'validation/loss': 1.1078037023544312, 'validation/num_examples': 50000, 'test/accuracy': 0.6554000377655029, 'test/loss': 1.695989727973938, 'test/num_examples': 10000, 'score': 72294.57926630974, 'total_duration': 78375.58976507187, 'accumulated_submission_time': 72294.57926630974, 'accumulated_eval_time': 6064.638352155685, 'accumulated_logging_time': 7.620237112045288}
I0201 16:56:14.408249 139923868813056 logging_writer.py:48] [156692] accumulated_eval_time=6064.638352, accumulated_logging_time=7.620237, accumulated_submission_time=72294.579266, global_step=156692, preemption_count=0, score=72294.579266, test/accuracy=0.655400, test/loss=1.695990, test/num_examples=10000, total_duration=78375.589765, train/accuracy=0.874238, train/loss=0.701266, validation/accuracy=0.775020, validation/loss=1.107804, validation/num_examples=50000
I0201 16:56:17.948670 139923852027648 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.3258213996887207, loss=2.859113931655884
I0201 16:56:59.731515 139923868813056 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.369839668273926, loss=2.805330991744995
I0201 16:57:46.470879 139923852027648 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.2765204906463623, loss=2.9034223556518555
I0201 16:58:33.244397 139923868813056 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.461864948272705, loss=2.7755846977233887
I0201 16:59:19.992761 139923852027648 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.801206111907959, loss=2.850010395050049
I0201 17:00:07.017721 139923868813056 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.368321418762207, loss=3.7007195949554443
I0201 17:00:53.595802 139923852027648 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.256761312484741, loss=2.8640642166137695
I0201 17:01:40.830874 139923868813056 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.360457181930542, loss=2.850731611251831
I0201 17:02:27.532659 139923852027648 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.324169158935547, loss=2.7646775245666504
I0201 17:03:14.366559 139923868813056 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.3501381874084473, loss=2.8416748046875
I0201 17:03:14.380303 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:03:24.841510 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:03:48.139105 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:03:49.775990 140085747812160 submission_runner.py:408] Time since start: 78831.00s, 	Step: 157601, 	{'train/accuracy': 0.8741992115974426, 'train/loss': 0.7010714411735535, 'validation/accuracy': 0.776919960975647, 'validation/loss': 1.1080583333969116, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6827137470245361, 'test/num_examples': 10000, 'score': 72714.48941850662, 'total_duration': 78831.00256085396, 'accumulated_submission_time': 72714.48941850662, 'accumulated_eval_time': 6100.034034729004, 'accumulated_logging_time': 7.676239013671875}
I0201 17:03:49.815887 139923852027648 logging_writer.py:48] [157601] accumulated_eval_time=6100.034035, accumulated_logging_time=7.676239, accumulated_submission_time=72714.489419, global_step=157601, preemption_count=0, score=72714.489419, test/accuracy=0.660500, test/loss=1.682714, test/num_examples=10000, total_duration=78831.002561, train/accuracy=0.874199, train/loss=0.701071, validation/accuracy=0.776920, validation/loss=1.108058, validation/num_examples=50000
I0201 17:04:30.994893 139923868813056 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.3443849086761475, loss=2.8190131187438965
I0201 17:05:17.542235 139923852027648 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.2376914024353027, loss=2.9719271659851074
I0201 17:06:04.242981 139923868813056 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.3989505767822266, loss=2.8703348636627197
I0201 17:06:51.201195 139923852027648 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.2486417293548584, loss=2.844424247741699
I0201 17:07:38.082793 139923868813056 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.3564751148223877, loss=2.7989284992218018
I0201 17:08:24.692215 139923852027648 logging_writer.py:48] [158200] global_step=158200, grad_norm=6.249833106994629, loss=3.8377060890197754
I0201 17:09:11.374683 139923868813056 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.557082176208496, loss=2.9479169845581055
I0201 17:09:58.106550 139923852027648 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.3291513919830322, loss=2.898982524871826
I0201 17:10:44.789882 139923868813056 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.3483827114105225, loss=2.999474287033081
I0201 17:10:50.064561 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:11:00.479685 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:11:23.945554 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:11:25.589733 140085747812160 submission_runner.py:408] Time since start: 79286.82s, 	Step: 158513, 	{'train/accuracy': 0.8779296875, 'train/loss': 0.6749905943870544, 'validation/accuracy': 0.7773799896240234, 'validation/loss': 1.0918878316879272, 'validation/num_examples': 50000, 'test/accuracy': 0.6610000133514404, 'test/loss': 1.6656324863433838, 'test/num_examples': 10000, 'score': 73134.6789803505, 'total_duration': 79286.81630277634, 'accumulated_submission_time': 73134.6789803505, 'accumulated_eval_time': 6135.559215307236, 'accumulated_logging_time': 7.725278377532959}
I0201 17:11:25.638321 139923852027648 logging_writer.py:48] [158513] accumulated_eval_time=6135.559215, accumulated_logging_time=7.725278, accumulated_submission_time=73134.678980, global_step=158513, preemption_count=0, score=73134.678980, test/accuracy=0.661000, test/loss=1.665632, test/num_examples=10000, total_duration=79286.816303, train/accuracy=0.877930, train/loss=0.674991, validation/accuracy=0.777380, validation/loss=1.091888, validation/num_examples=50000
I0201 17:12:01.287452 139923868813056 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.7713451385498047, loss=4.1746978759765625
I0201 17:12:47.917316 139923852027648 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.3121256828308105, loss=3.574016809463501
I0201 17:13:34.807983 139923868813056 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.7338030338287354, loss=4.081709861755371
I0201 17:14:21.742106 139923852027648 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.255749225616455, loss=3.397204637527466
I0201 17:15:08.735806 139923868813056 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.3858883380889893, loss=2.834886074066162
I0201 17:15:55.370574 139923852027648 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.1704161167144775, loss=3.11816143989563
I0201 17:16:42.301881 139923868813056 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.464954376220703, loss=3.4414825439453125
I0201 17:17:28.694752 139923852027648 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.445678234100342, loss=2.9056828022003174
I0201 17:18:15.427890 139923868813056 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.3584232330322266, loss=2.893249273300171
I0201 17:18:25.866940 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:18:36.368780 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:19:00.329077 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:19:01.996558 140085747812160 submission_runner.py:408] Time since start: 79743.22s, 	Step: 159424, 	{'train/accuracy': 0.8819335699081421, 'train/loss': 0.6788507103919983, 'validation/accuracy': 0.7778399586677551, 'validation/loss': 1.1027580499649048, 'validation/num_examples': 50000, 'test/accuracy': 0.6589000225067139, 'test/loss': 1.6929190158843994, 'test/num_examples': 10000, 'score': 73554.84628415108, 'total_duration': 79743.22312545776, 'accumulated_submission_time': 73554.84628415108, 'accumulated_eval_time': 6171.688840389252, 'accumulated_logging_time': 7.782891035079956}
I0201 17:19:02.038881 139923852027648 logging_writer.py:48] [159424] accumulated_eval_time=6171.688840, accumulated_logging_time=7.782891, accumulated_submission_time=73554.846284, global_step=159424, preemption_count=0, score=73554.846284, test/accuracy=0.658900, test/loss=1.692919, test/num_examples=10000, total_duration=79743.223125, train/accuracy=0.881934, train/loss=0.678851, validation/accuracy=0.777840, validation/loss=1.102758, validation/num_examples=50000
I0201 17:19:32.526786 139923868813056 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.4132862091064453, loss=2.7889201641082764
I0201 17:20:19.230489 139923852027648 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.3587417602539062, loss=3.8222126960754395
I0201 17:21:06.349483 139923868813056 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.493323802947998, loss=3.859320640563965
I0201 17:21:52.786165 139923852027648 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.3137784004211426, loss=2.812121868133545
I0201 17:22:39.618058 139923868813056 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.3718764781951904, loss=3.59542179107666
I0201 17:23:26.268112 139923852027648 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.6968185901641846, loss=4.158177852630615
I0201 17:24:12.889853 139923868813056 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.486405849456787, loss=3.762695074081421
I0201 17:24:59.445360 139923852027648 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.7940855026245117, loss=3.879289150238037
I0201 17:25:46.096324 139923868813056 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.052102565765381, loss=4.229607582092285
I0201 17:26:02.034825 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:26:12.748351 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:26:37.751852 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:26:39.388089 140085747812160 submission_runner.py:408] Time since start: 80200.61s, 	Step: 160336, 	{'train/accuracy': 0.8783202767372131, 'train/loss': 0.6747952103614807, 'validation/accuracy': 0.7796799540519714, 'validation/loss': 1.08854079246521, 'validation/num_examples': 50000, 'test/accuracy': 0.6620000600814819, 'test/loss': 1.66422700881958, 'test/num_examples': 10000, 'score': 73974.78279566765, 'total_duration': 80200.6146595478, 'accumulated_submission_time': 73974.78279566765, 'accumulated_eval_time': 6209.042106866837, 'accumulated_logging_time': 7.834174156188965}
I0201 17:26:39.429443 139923852027648 logging_writer.py:48] [160336] accumulated_eval_time=6209.042107, accumulated_logging_time=7.834174, accumulated_submission_time=73974.782796, global_step=160336, preemption_count=0, score=73974.782796, test/accuracy=0.662000, test/loss=1.664227, test/num_examples=10000, total_duration=80200.614660, train/accuracy=0.878320, train/loss=0.674795, validation/accuracy=0.779680, validation/loss=1.088541, validation/num_examples=50000
I0201 17:27:04.938403 139923868813056 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.3800010681152344, loss=2.8981196880340576
I0201 17:27:51.295655 139923852027648 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.2920684814453125, loss=2.6998190879821777
I0201 17:28:38.702866 139923868813056 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.7686257362365723, loss=4.135026931762695
I0201 17:29:25.358652 139923852027648 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.4115724563598633, loss=2.8315999507904053
I0201 17:30:12.188645 139923868813056 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.615492582321167, loss=3.913470506668091
I0201 17:30:58.835831 139923852027648 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.5200307369232178, loss=2.9857242107391357
I0201 17:31:45.588248 139923868813056 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.306011915206909, loss=3.0822789669036865
I0201 17:32:32.592926 139923852027648 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.193572998046875, loss=2.745028018951416
I0201 17:33:19.423491 139923868813056 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.412222385406494, loss=2.863232374191284
I0201 17:33:39.771579 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:33:50.338504 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:34:13.924845 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:34:15.559255 140085747812160 submission_runner.py:408] Time since start: 80656.79s, 	Step: 161245, 	{'train/accuracy': 0.8847070336341858, 'train/loss': 0.6641880869865417, 'validation/accuracy': 0.7793799638748169, 'validation/loss': 1.094588279724121, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.676234245300293, 'test/num_examples': 10000, 'score': 74395.06458425522, 'total_duration': 80656.78582334518, 'accumulated_submission_time': 74395.06458425522, 'accumulated_eval_time': 6244.829773902893, 'accumulated_logging_time': 7.885188102722168}
I0201 17:34:15.603863 139923852027648 logging_writer.py:48] [161245] accumulated_eval_time=6244.829774, accumulated_logging_time=7.885188, accumulated_submission_time=74395.064584, global_step=161245, preemption_count=0, score=74395.064584, test/accuracy=0.661900, test/loss=1.676234, test/num_examples=10000, total_duration=80656.785823, train/accuracy=0.884707, train/loss=0.664188, validation/accuracy=0.779380, validation/loss=1.094588, validation/num_examples=50000
I0201 17:34:37.575535 139923868813056 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.609010934829712, loss=2.841668128967285
I0201 17:35:23.090777 139923852027648 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.2096219062805176, loss=3.221414089202881
I0201 17:36:09.707172 139923868813056 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.328139543533325, loss=3.595905303955078
I0201 17:36:56.565187 139923852027648 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.555103063583374, loss=2.8182859420776367
I0201 17:37:43.420378 139923868813056 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.53255033493042, loss=2.821725845336914
I0201 17:38:30.529933 139923852027648 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.39353346824646, loss=2.75675892829895
I0201 17:39:17.455157 139923868813056 logging_writer.py:48] [161900] global_step=161900, grad_norm=3.136338472366333, loss=4.138383865356445
I0201 17:40:04.149301 139923852027648 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.5978217124938965, loss=2.849254608154297
I0201 17:40:50.934545 139923868813056 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.3962039947509766, loss=4.300001621246338
I0201 17:41:15.939314 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:41:26.132525 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:41:50.003933 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:41:51.646414 140085747812160 submission_runner.py:408] Time since start: 81112.87s, 	Step: 162155, 	{'train/accuracy': 0.8860546946525574, 'train/loss': 0.6558742523193359, 'validation/accuracy': 0.7773999571800232, 'validation/loss': 1.0941129922866821, 'validation/num_examples': 50000, 'test/accuracy': 0.6637000441551208, 'test/loss': 1.6746059656143188, 'test/num_examples': 10000, 'score': 74815.33997106552, 'total_duration': 81112.87297201157, 'accumulated_submission_time': 74815.33997106552, 'accumulated_eval_time': 6280.536881446838, 'accumulated_logging_time': 7.93939733505249}
I0201 17:41:51.688368 139923852027648 logging_writer.py:48] [162155] accumulated_eval_time=6280.536881, accumulated_logging_time=7.939397, accumulated_submission_time=74815.339971, global_step=162155, preemption_count=0, score=74815.339971, test/accuracy=0.663700, test/loss=1.674606, test/num_examples=10000, total_duration=81112.872972, train/accuracy=0.886055, train/loss=0.655874, validation/accuracy=0.777400, validation/loss=1.094113, validation/num_examples=50000
I0201 17:42:09.743518 139923868813056 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.653665542602539, loss=2.8860559463500977
I0201 17:42:54.631021 139923852027648 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.5002810955047607, loss=2.791033983230591
I0201 17:43:41.562973 139923868813056 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.3703877925872803, loss=2.7452778816223145
I0201 17:44:28.447365 139923852027648 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.5085983276367188, loss=2.8415870666503906
I0201 17:45:15.125768 139923868813056 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.3066246509552, loss=2.7705986499786377
I0201 17:46:01.970077 139923852027648 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.4311740398406982, loss=2.7208733558654785
I0201 17:46:48.478876 139923868813056 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.7202045917510986, loss=4.105852127075195
I0201 17:47:35.363706 139923852027648 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.822359561920166, loss=3.835404396057129
I0201 17:48:22.269844 139923868813056 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.379992961883545, loss=2.844085931777954
I0201 17:48:51.820160 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:49:02.349933 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:49:25.642431 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:49:27.280385 140085747812160 submission_runner.py:408] Time since start: 81568.51s, 	Step: 163065, 	{'train/accuracy': 0.8809179663658142, 'train/loss': 0.6672109961509705, 'validation/accuracy': 0.7792999744415283, 'validation/loss': 1.0870453119277954, 'validation/num_examples': 50000, 'test/accuracy': 0.6628000140190125, 'test/loss': 1.6721868515014648, 'test/num_examples': 10000, 'score': 75235.40973472595, 'total_duration': 81568.50692629814, 'accumulated_submission_time': 75235.40973472595, 'accumulated_eval_time': 6315.99707698822, 'accumulated_logging_time': 7.991906642913818}
I0201 17:49:27.322843 139923852027648 logging_writer.py:48] [163065] accumulated_eval_time=6315.997077, accumulated_logging_time=7.991907, accumulated_submission_time=75235.409735, global_step=163065, preemption_count=0, score=75235.409735, test/accuracy=0.662800, test/loss=1.672187, test/num_examples=10000, total_duration=81568.506926, train/accuracy=0.880918, train/loss=0.667211, validation/accuracy=0.779300, validation/loss=1.087045, validation/num_examples=50000
I0201 17:49:41.463279 139923868813056 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.567803144454956, loss=3.40446138381958
I0201 17:50:25.119599 139923852027648 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.559523820877075, loss=2.763331890106201
I0201 17:51:11.880423 139923868813056 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.570089101791382, loss=2.8930671215057373
I0201 17:51:58.656292 139923852027648 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.2900288105010986, loss=2.851677417755127
I0201 17:52:45.559181 139923868813056 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.5277678966522217, loss=3.7343873977661133
I0201 17:53:32.223525 139923852027648 logging_writer.py:48] [163600] global_step=163600, grad_norm=3.1164448261260986, loss=4.06178092956543
I0201 17:54:18.854808 139923868813056 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.364856243133545, loss=3.429025173187256
I0201 17:55:05.577103 139923852027648 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.608210325241089, loss=2.8438925743103027
I0201 17:55:52.253889 139923868813056 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.5164642333984375, loss=2.8458399772644043
I0201 17:56:27.610331 140085747812160 spec.py:321] Evaluating on the training split.
I0201 17:56:37.983830 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 17:57:02.550738 140085747812160 spec.py:349] Evaluating on the test split.
I0201 17:57:04.201419 140085747812160 submission_runner.py:408] Time since start: 82025.43s, 	Step: 163977, 	{'train/accuracy': 0.8843554258346558, 'train/loss': 0.6567904353141785, 'validation/accuracy': 0.7809799909591675, 'validation/loss': 1.085119605064392, 'validation/num_examples': 50000, 'test/accuracy': 0.6634000539779663, 'test/loss': 1.6664263010025024, 'test/num_examples': 10000, 'score': 75655.63436961174, 'total_duration': 82025.42798662186, 'accumulated_submission_time': 75655.63436961174, 'accumulated_eval_time': 6352.588169336319, 'accumulated_logging_time': 8.046016693115234}
I0201 17:57:04.244705 139923852027648 logging_writer.py:48] [163977] accumulated_eval_time=6352.588169, accumulated_logging_time=8.046017, accumulated_submission_time=75655.634370, global_step=163977, preemption_count=0, score=75655.634370, test/accuracy=0.663400, test/loss=1.666426, test/num_examples=10000, total_duration=82025.427987, train/accuracy=0.884355, train/loss=0.656790, validation/accuracy=0.780980, validation/loss=1.085120, validation/num_examples=50000
I0201 17:57:13.662804 139923868813056 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.6264760494232178, loss=3.7374889850616455
I0201 17:57:56.544292 139923852027648 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.27587890625, loss=4.344205856323242
I0201 17:58:43.254802 139923868813056 logging_writer.py:48] [164200] global_step=164200, grad_norm=3.5007481575012207, loss=4.251074314117432
I0201 17:59:30.190528 139923852027648 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.776705026626587, loss=4.015199184417725
I0201 18:00:16.819243 139923868813056 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.6753456592559814, loss=2.8783962726593018
I0201 18:01:03.664465 139923852027648 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.3031296730041504, loss=2.7583231925964355
I0201 18:01:50.368400 139923868813056 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.5803539752960205, loss=2.8079676628112793
I0201 18:02:37.152976 139923852027648 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.9310250282287598, loss=3.8675458431243896
I0201 18:03:23.859474 139923868813056 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.410885810852051, loss=3.1572487354278564
I0201 18:04:04.345668 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:04:14.693156 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:04:40.846150 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:04:42.486501 140085747812160 submission_runner.py:408] Time since start: 82483.71s, 	Step: 164889, 	{'train/accuracy': 0.8860155940055847, 'train/loss': 0.6490659117698669, 'validation/accuracy': 0.7811999917030334, 'validation/loss': 1.085237979888916, 'validation/num_examples': 50000, 'test/accuracy': 0.6682000160217285, 'test/loss': 1.6649689674377441, 'test/num_examples': 10000, 'score': 76075.67520737648, 'total_duration': 82483.71307039261, 'accumulated_submission_time': 76075.67520737648, 'accumulated_eval_time': 6390.728995323181, 'accumulated_logging_time': 8.098177194595337}
I0201 18:04:42.534107 139923852027648 logging_writer.py:48] [164889] accumulated_eval_time=6390.728995, accumulated_logging_time=8.098177, accumulated_submission_time=76075.675207, global_step=164889, preemption_count=0, score=76075.675207, test/accuracy=0.668200, test/loss=1.664969, test/num_examples=10000, total_duration=82483.713070, train/accuracy=0.886016, train/loss=0.649066, validation/accuracy=0.781200, validation/loss=1.085238, validation/num_examples=50000
I0201 18:04:47.246331 139923868813056 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.3267099857330322, loss=2.9471702575683594
I0201 18:05:29.480137 139923852027648 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.848328113555908, loss=2.8280863761901855
I0201 18:06:16.161123 139923868813056 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.6519784927368164, loss=3.7616679668426514
I0201 18:07:03.161974 139923852027648 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.3688530921936035, loss=2.913949728012085
I0201 18:07:49.996362 139923868813056 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.4451510906219482, loss=2.9946303367614746
I0201 18:08:36.558286 139923852027648 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.2125895023345947, loss=2.8342299461364746
I0201 18:09:23.337696 139923868813056 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.4929559230804443, loss=3.5574147701263428
I0201 18:10:09.973477 139923852027648 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.3473410606384277, loss=4.241330146789551
I0201 18:10:56.607342 139923868813056 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.541314125061035, loss=2.834007740020752
I0201 18:11:42.869137 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:11:53.352486 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:12:20.190994 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:12:21.835207 140085747812160 submission_runner.py:408] Time since start: 82943.06s, 	Step: 165800, 	{'train/accuracy': 0.8830273151397705, 'train/loss': 0.6618872880935669, 'validation/accuracy': 0.7819199562072754, 'validation/loss': 1.0855050086975098, 'validation/num_examples': 50000, 'test/accuracy': 0.6676000356674194, 'test/loss': 1.659148931503296, 'test/num_examples': 10000, 'score': 76495.9494357109, 'total_duration': 82943.06176996231, 'accumulated_submission_time': 76495.9494357109, 'accumulated_eval_time': 6429.695057630539, 'accumulated_logging_time': 8.155537843704224}
I0201 18:12:21.881855 139923852027648 logging_writer.py:48] [165800] accumulated_eval_time=6429.695058, accumulated_logging_time=8.155538, accumulated_submission_time=76495.949436, global_step=165800, preemption_count=0, score=76495.949436, test/accuracy=0.667600, test/loss=1.659149, test/num_examples=10000, total_duration=82943.061770, train/accuracy=0.883027, train/loss=0.661887, validation/accuracy=0.781920, validation/loss=1.085505, validation/num_examples=50000
I0201 18:12:22.279366 139923868813056 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.3996264934539795, loss=2.793459415435791
I0201 18:13:03.787711 139923852027648 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.4421701431274414, loss=3.113496780395508
I0201 18:13:50.186411 139923868813056 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.4772820472717285, loss=2.926253080368042
I0201 18:14:37.089377 139923852027648 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.5526819229125977, loss=3.1593496799468994
I0201 18:15:23.812165 139923868813056 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.5290586948394775, loss=2.9667739868164062
I0201 18:16:10.763550 139923852027648 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.4406042098999023, loss=2.9179234504699707
I0201 18:16:57.440628 139923868813056 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.567594528198242, loss=3.161456823348999
I0201 18:17:44.027274 139923852027648 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.7827534675598145, loss=2.75313138961792
I0201 18:18:31.010603 139923868813056 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.5788350105285645, loss=3.6946120262145996
I0201 18:19:17.882238 139923852027648 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.576148748397827, loss=3.0320048332214355
I0201 18:19:22.207946 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:19:32.818265 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:19:55.457629 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:19:57.104657 140085747812160 submission_runner.py:408] Time since start: 83398.33s, 	Step: 166711, 	{'train/accuracy': 0.8886913657188416, 'train/loss': 0.645067036151886, 'validation/accuracy': 0.7826399803161621, 'validation/loss': 1.0786688327789307, 'validation/num_examples': 50000, 'test/accuracy': 0.6674000024795532, 'test/loss': 1.6567219495773315, 'test/num_examples': 10000, 'score': 76916.21601438522, 'total_duration': 83398.33122730255, 'accumulated_submission_time': 76916.21601438522, 'accumulated_eval_time': 6464.591763496399, 'accumulated_logging_time': 8.211509466171265}
I0201 18:19:57.152720 139923868813056 logging_writer.py:48] [166711] accumulated_eval_time=6464.591763, accumulated_logging_time=8.211509, accumulated_submission_time=76916.216014, global_step=166711, preemption_count=0, score=76916.216014, test/accuracy=0.667400, test/loss=1.656722, test/num_examples=10000, total_duration=83398.331227, train/accuracy=0.888691, train/loss=0.645067, validation/accuracy=0.782640, validation/loss=1.078669, validation/num_examples=50000
I0201 18:20:33.749887 139923852027648 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.5215210914611816, loss=2.840773105621338
I0201 18:21:20.365381 139923868813056 logging_writer.py:48] [166900] global_step=166900, grad_norm=3.113034725189209, loss=3.997368574142456
I0201 18:22:07.471475 139923852027648 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.964613437652588, loss=3.9172067642211914
I0201 18:22:54.510964 139923868813056 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.6200053691864014, loss=2.845224142074585
I0201 18:23:41.555614 139923852027648 logging_writer.py:48] [167200] global_step=167200, grad_norm=2.4702672958374023, loss=3.201235055923462
I0201 18:24:28.344103 139923868813056 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.551314115524292, loss=2.9166014194488525
I0201 18:25:15.038151 139923852027648 logging_writer.py:48] [167400] global_step=167400, grad_norm=2.853602170944214, loss=3.8739683628082275
I0201 18:26:01.821604 139923868813056 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.6823372840881348, loss=2.7536749839782715
I0201 18:26:48.653534 139923852027648 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.544294595718384, loss=2.7743449211120605
I0201 18:26:57.240256 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:27:07.706144 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:27:32.221153 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:27:33.860516 140085747812160 submission_runner.py:408] Time since start: 83855.09s, 	Step: 167620, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.6586396098136902, 'validation/accuracy': 0.7811399698257446, 'validation/loss': 1.0905895233154297, 'validation/num_examples': 50000, 'test/accuracy': 0.6629000306129456, 'test/loss': 1.6669039726257324, 'test/num_examples': 10000, 'score': 77336.24432229996, 'total_duration': 83855.08708715439, 'accumulated_submission_time': 77336.24432229996, 'accumulated_eval_time': 6501.212018728256, 'accumulated_logging_time': 8.268371820449829}
I0201 18:27:33.904191 139923868813056 logging_writer.py:48] [167620] accumulated_eval_time=6501.212019, accumulated_logging_time=8.268372, accumulated_submission_time=77336.244322, global_step=167620, preemption_count=0, score=77336.244322, test/accuracy=0.662900, test/loss=1.666904, test/num_examples=10000, total_duration=83855.087087, train/accuracy=0.888184, train/loss=0.658640, validation/accuracy=0.781140, validation/loss=1.090590, validation/num_examples=50000
I0201 18:28:06.250598 139923852027648 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.658463954925537, loss=3.9387946128845215
I0201 18:28:52.484549 139923868813056 logging_writer.py:48] [167800] global_step=167800, grad_norm=2.5290515422821045, loss=3.7527976036071777
I0201 18:29:39.008136 139923852027648 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.4550323486328125, loss=3.397627115249634
I0201 18:30:25.818715 139923868813056 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.833696126937866, loss=2.759720802307129
I0201 18:30:38.024552 139923852027648 logging_writer.py:48] [168028] global_step=168028, preemption_count=0, score=77520.280705
I0201 18:30:38.703316 140085747812160 checkpoints.py:490] Saving checkpoint at step: 168028
I0201 18:30:39.980696 140085747812160 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2/checkpoint_168028
I0201 18:30:39.997261 140085747812160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_2/checkpoint_168028.
I0201 18:30:40.766834 140085747812160 submission_runner.py:583] Tuning trial 2/5
I0201 18:30:40.767057 140085747812160 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0201 18:30:40.776233 140085747812160 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008007812430150807, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 33.638116121292114, 'total_duration': 61.886141538619995, 'accumulated_submission_time': 33.638116121292114, 'accumulated_eval_time': 28.24734091758728, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (861, {'train/accuracy': 0.01630859263241291, 'train/loss': 6.390534400939941, 'validation/accuracy': 0.016039999201893806, 'validation/loss': 6.40871000289917, 'validation/num_examples': 50000, 'test/accuracy': 0.01380000077188015, 'test/loss': 6.452201843261719, 'test/num_examples': 10000, 'score': 453.676509141922, 'total_duration': 517.1768567562103, 'accumulated_submission_time': 453.676509141922, 'accumulated_eval_time': 63.434080839157104, 'accumulated_logging_time': 0.017125606536865234, 'global_step': 861, 'preemption_count': 0}), (1772, {'train/accuracy': 0.04798828065395355, 'train/loss': 5.8042778968811035, 'validation/accuracy': 0.04601999744772911, 'validation/loss': 5.834495544433594, 'validation/num_examples': 50000, 'test/accuracy': 0.03670000284910202, 'test/loss': 5.948112487792969, 'test/num_examples': 10000, 'score': 873.8910715579987, 'total_duration': 972.0015864372253, 'accumulated_submission_time': 873.8910715579987, 'accumulated_eval_time': 97.96224021911621, 'accumulated_logging_time': 0.04842209815979004, 'global_step': 1772, 'preemption_count': 0}), (2684, {'train/accuracy': 0.07507812231779099, 'train/loss': 5.424489498138428, 'validation/accuracy': 0.06735999882221222, 'validation/loss': 5.472591876983643, 'validation/num_examples': 50000, 'test/accuracy': 0.051600001752376556, 'test/loss': 5.641368389129639, 'test/num_examples': 10000, 'score': 1294.1512134075165, 'total_duration': 1429.2565631866455, 'accumulated_submission_time': 1294.1512134075165, 'accumulated_eval_time': 134.87805843353271, 'accumulated_logging_time': 0.07643413543701172, 'global_step': 2684, 'preemption_count': 0}), (3598, {'train/accuracy': 0.10935546457767487, 'train/loss': 5.114729881286621, 'validation/accuracy': 0.10179999470710754, 'validation/loss': 5.160167217254639, 'validation/num_examples': 50000, 'test/accuracy': 0.07700000703334808, 'test/loss': 5.386642932891846, 'test/num_examples': 10000, 'score': 1714.4163200855255, 'total_duration': 1883.0908553600311, 'accumulated_submission_time': 1714.4163200855255, 'accumulated_eval_time': 168.3693425655365, 'accumulated_logging_time': 0.10368227958679199, 'global_step': 3598, 'preemption_count': 0}), (4511, {'train/accuracy': 0.16072265803813934, 'train/loss': 4.637515068054199, 'validation/accuracy': 0.1462000012397766, 'validation/loss': 4.709775924682617, 'validation/num_examples': 50000, 'test/accuracy': 0.11030000448226929, 'test/loss': 4.995343208312988, 'test/num_examples': 10000, 'score': 2134.3432302474976, 'total_duration': 2339.002152442932, 'accumulated_submission_time': 2134.3432302474976, 'accumulated_eval_time': 204.27332472801208, 'accumulated_logging_time': 0.13352179527282715, 'global_step': 4511, 'preemption_count': 0}), (5424, {'train/accuracy': 0.20499999821186066, 'train/loss': 4.282425880432129, 'validation/accuracy': 0.18549999594688416, 'validation/loss': 4.378244400024414, 'validation/num_examples': 50000, 'test/accuracy': 0.14100000262260437, 'test/loss': 4.718472480773926, 'test/num_examples': 10000, 'score': 2554.3411922454834, 'total_duration': 2794.102832555771, 'accumulated_submission_time': 2554.3411922454834, 'accumulated_eval_time': 239.3009958267212, 'accumulated_logging_time': 0.15807175636291504, 'global_step': 5424, 'preemption_count': 0}), (6337, {'train/accuracy': 0.24888671934604645, 'train/loss': 3.9398257732391357, 'validation/accuracy': 0.23002000153064728, 'validation/loss': 4.036154747009277, 'validation/num_examples': 50000, 'test/accuracy': 0.17490001022815704, 'test/loss': 4.422224998474121, 'test/num_examples': 10000, 'score': 2974.4840099811554, 'total_duration': 3249.46443939209, 'accumulated_submission_time': 2974.4840099811554, 'accumulated_eval_time': 274.43955540657043, 'accumulated_logging_time': 0.18840432167053223, 'global_step': 6337, 'preemption_count': 0}), (7251, {'train/accuracy': 0.28951171040534973, 'train/loss': 3.659607172012329, 'validation/accuracy': 0.25793999433517456, 'validation/loss': 3.8160948753356934, 'validation/num_examples': 50000, 'test/accuracy': 0.20260000228881836, 'test/loss': 4.218814849853516, 'test/num_examples': 10000, 'score': 3394.577829360962, 'total_duration': 3703.3408839702606, 'accumulated_submission_time': 3394.577829360962, 'accumulated_eval_time': 308.14124870300293, 'accumulated_logging_time': 0.21875262260437012, 'global_step': 7251, 'preemption_count': 0}), (8163, {'train/accuracy': 0.3244921863079071, 'train/loss': 3.4557206630706787, 'validation/accuracy': 0.30140000581741333, 'validation/loss': 3.572089910507202, 'validation/num_examples': 50000, 'test/accuracy': 0.2290000170469284, 'test/loss': 4.028704643249512, 'test/num_examples': 10000, 'score': 3814.550952911377, 'total_duration': 4159.377365589142, 'accumulated_submission_time': 3814.550952911377, 'accumulated_eval_time': 344.1261923313141, 'accumulated_logging_time': 0.24541378021240234, 'global_step': 8163, 'preemption_count': 0}), (9074, {'train/accuracy': 0.35783201456069946, 'train/loss': 3.234337329864502, 'validation/accuracy': 0.32916000485420227, 'validation/loss': 3.3736839294433594, 'validation/num_examples': 50000, 'test/accuracy': 0.24990001320838928, 'test/loss': 3.8573837280273438, 'test/num_examples': 10000, 'score': 4234.478483200073, 'total_duration': 4613.234064817429, 'accumulated_submission_time': 4234.478483200073, 'accumulated_eval_time': 377.9731593132019, 'accumulated_logging_time': 0.2774481773376465, 'global_step': 9074, 'preemption_count': 0}), (9986, {'train/accuracy': 0.3981249928474426, 'train/loss': 2.985623359680176, 'validation/accuracy': 0.3610599935054779, 'validation/loss': 3.1759495735168457, 'validation/num_examples': 50000, 'test/accuracy': 0.27790001034736633, 'test/loss': 3.6841766834259033, 'test/num_examples': 10000, 'score': 4654.747854232788, 'total_duration': 5066.819159269333, 'accumulated_submission_time': 4654.747854232788, 'accumulated_eval_time': 411.2083342075348, 'accumulated_logging_time': 0.30759716033935547, 'global_step': 9986, 'preemption_count': 0}), (10899, {'train/accuracy': 0.42244139313697815, 'train/loss': 2.8559553623199463, 'validation/accuracy': 0.391759991645813, 'validation/loss': 2.990187644958496, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.5289344787597656, 'test/num_examples': 10000, 'score': 5074.991655111313, 'total_duration': 5520.955825805664, 'accumulated_submission_time': 5074.991655111313, 'accumulated_eval_time': 445.02062249183655, 'accumulated_logging_time': 0.33704257011413574, 'global_step': 10899, 'preemption_count': 0}), (11811, {'train/accuracy': 0.4569726586341858, 'train/loss': 2.680199146270752, 'validation/accuracy': 0.42282000184059143, 'validation/loss': 2.834578275680542, 'validation/num_examples': 50000, 'test/accuracy': 0.3346000015735626, 'test/loss': 3.379513740539551, 'test/num_examples': 10000, 'score': 5494.998826980591, 'total_duration': 5975.380021810532, 'accumulated_submission_time': 5494.998826980591, 'accumulated_eval_time': 479.35635805130005, 'accumulated_logging_time': 0.36748218536376953, 'global_step': 11811, 'preemption_count': 0}), (12722, {'train/accuracy': 0.4777539074420929, 'train/loss': 2.5539870262145996, 'validation/accuracy': 0.4357999861240387, 'validation/loss': 2.754807949066162, 'validation/num_examples': 50000, 'test/accuracy': 0.34210002422332764, 'test/loss': 3.313959836959839, 'test/num_examples': 10000, 'score': 5914.95384311676, 'total_duration': 6429.472145318985, 'accumulated_submission_time': 5914.95384311676, 'accumulated_eval_time': 513.4131627082825, 'accumulated_logging_time': 0.39693355560302734, 'global_step': 12722, 'preemption_count': 0}), (13634, {'train/accuracy': 0.4913281202316284, 'train/loss': 2.4966847896575928, 'validation/accuracy': 0.45917999744415283, 'validation/loss': 2.6533918380737305, 'validation/num_examples': 50000, 'test/accuracy': 0.3613000214099884, 'test/loss': 3.21649432182312, 'test/num_examples': 10000, 'score': 6335.079059839249, 'total_duration': 6884.6356201171875, 'accumulated_submission_time': 6335.079059839249, 'accumulated_eval_time': 548.3734202384949, 'accumulated_logging_time': 0.42502331733703613, 'global_step': 13634, 'preemption_count': 0}), (14550, {'train/accuracy': 0.514355480670929, 'train/loss': 2.332359790802002, 'validation/accuracy': 0.4775199890136719, 'validation/loss': 2.504133939743042, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.088716983795166, 'test/num_examples': 10000, 'score': 6755.355276584625, 'total_duration': 7339.585505962372, 'accumulated_submission_time': 6755.355276584625, 'accumulated_eval_time': 582.966285943985, 'accumulated_logging_time': 0.45445990562438965, 'global_step': 14550, 'preemption_count': 0}), (15464, {'train/accuracy': 0.5285546779632568, 'train/loss': 2.330787420272827, 'validation/accuracy': 0.48503997921943665, 'validation/loss': 2.5376617908477783, 'validation/num_examples': 50000, 'test/accuracy': 0.38200002908706665, 'test/loss': 3.105388879776001, 'test/num_examples': 10000, 'score': 7175.678120136261, 'total_duration': 7793.58558678627, 'accumulated_submission_time': 7175.678120136261, 'accumulated_eval_time': 616.5665156841278, 'accumulated_logging_time': 0.4815404415130615, 'global_step': 15464, 'preemption_count': 0}), (16379, {'train/accuracy': 0.5498241782188416, 'train/loss': 2.2195632457733154, 'validation/accuracy': 0.5060399770736694, 'validation/loss': 2.409464120864868, 'validation/num_examples': 50000, 'test/accuracy': 0.38840001821517944, 'test/loss': 3.026716470718384, 'test/num_examples': 10000, 'score': 7596.0130705833435, 'total_duration': 8248.45794916153, 'accumulated_submission_time': 7596.0130705833435, 'accumulated_eval_time': 651.0233614444733, 'accumulated_logging_time': 0.5114836692810059, 'global_step': 16379, 'preemption_count': 0}), (17293, {'train/accuracy': 0.551562488079071, 'train/loss': 2.1806676387786865, 'validation/accuracy': 0.5150399804115295, 'validation/loss': 2.3470511436462402, 'validation/num_examples': 50000, 'test/accuracy': 0.40960001945495605, 'test/loss': 2.956143617630005, 'test/num_examples': 10000, 'score': 8016.122918605804, 'total_duration': 8700.978935956955, 'accumulated_submission_time': 8016.122918605804, 'accumulated_eval_time': 683.3534235954285, 'accumulated_logging_time': 0.5411381721496582, 'global_step': 17293, 'preemption_count': 0}), (18206, {'train/accuracy': 0.5733984112739563, 'train/loss': 2.0888290405273438, 'validation/accuracy': 0.5263000130653381, 'validation/loss': 2.2876055240631104, 'validation/num_examples': 50000, 'test/accuracy': 0.4150000214576721, 'test/loss': 2.885838031768799, 'test/num_examples': 10000, 'score': 8436.345466375351, 'total_duration': 9155.736872911453, 'accumulated_submission_time': 8436.345466375351, 'accumulated_eval_time': 717.8099949359894, 'accumulated_logging_time': 0.5694966316223145, 'global_step': 18206, 'preemption_count': 0}), (19119, {'train/accuracy': 0.6010546684265137, 'train/loss': 1.9360231161117554, 'validation/accuracy': 0.5326799750328064, 'validation/loss': 2.2370855808258057, 'validation/num_examples': 50000, 'test/accuracy': 0.42100003361701965, 'test/loss': 2.853330612182617, 'test/num_examples': 10000, 'score': 8856.345707178116, 'total_duration': 9609.926443576813, 'accumulated_submission_time': 8856.345707178116, 'accumulated_eval_time': 751.9210438728333, 'accumulated_logging_time': 0.5966720581054688, 'global_step': 19119, 'preemption_count': 0}), (20024, {'train/accuracy': 0.581347644329071, 'train/loss': 2.0001742839813232, 'validation/accuracy': 0.541920006275177, 'validation/loss': 2.18528151512146, 'validation/num_examples': 50000, 'test/accuracy': 0.4229000210762024, 'test/loss': 2.806061267852783, 'test/num_examples': 10000, 'score': 9276.2863240242, 'total_duration': 10064.165142297745, 'accumulated_submission_time': 9276.2863240242, 'accumulated_eval_time': 786.1423320770264, 'accumulated_logging_time': 0.6236920356750488, 'global_step': 20024, 'preemption_count': 0}), (20935, {'train/accuracy': 0.5981249809265137, 'train/loss': 1.9761009216308594, 'validation/accuracy': 0.5450199842453003, 'validation/loss': 2.1948776245117188, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.791691541671753, 'test/num_examples': 10000, 'score': 9696.536899328232, 'total_duration': 10519.46850657463, 'accumulated_submission_time': 9696.536899328232, 'accumulated_eval_time': 821.1112020015717, 'accumulated_logging_time': 0.6576018333435059, 'global_step': 20935, 'preemption_count': 0}), (21847, {'train/accuracy': 0.6225780844688416, 'train/loss': 1.8207433223724365, 'validation/accuracy': 0.5593400001525879, 'validation/loss': 2.095564365386963, 'validation/num_examples': 50000, 'test/accuracy': 0.4457000195980072, 'test/loss': 2.695769786834717, 'test/num_examples': 10000, 'score': 10116.994480848312, 'total_duration': 10974.814060688019, 'accumulated_submission_time': 10116.994480848312, 'accumulated_eval_time': 855.9199452400208, 'accumulated_logging_time': 0.6856215000152588, 'global_step': 21847, 'preemption_count': 0}), (22761, {'train/accuracy': 0.6095117330551147, 'train/loss': 1.9055193662643433, 'validation/accuracy': 0.5639399886131287, 'validation/loss': 2.1037421226501465, 'validation/num_examples': 50000, 'test/accuracy': 0.44860002398490906, 'test/loss': 2.720872640609741, 'test/num_examples': 10000, 'score': 10537.393913984299, 'total_duration': 11429.881940364838, 'accumulated_submission_time': 10537.393913984299, 'accumulated_eval_time': 890.505363702774, 'accumulated_logging_time': 0.7179117202758789, 'global_step': 22761, 'preemption_count': 0}), (23672, {'train/accuracy': 0.6233788728713989, 'train/loss': 1.8191262483596802, 'validation/accuracy': 0.5717200040817261, 'validation/loss': 2.032801628112793, 'validation/num_examples': 50000, 'test/accuracy': 0.4562000334262848, 'test/loss': 2.6657521724700928, 'test/num_examples': 10000, 'score': 10957.388006210327, 'total_duration': 11882.973408699036, 'accumulated_submission_time': 10957.388006210327, 'accumulated_eval_time': 923.5195500850677, 'accumulated_logging_time': 0.7508177757263184, 'global_step': 23672, 'preemption_count': 0}), (24585, {'train/accuracy': 0.6335155963897705, 'train/loss': 1.7896169424057007, 'validation/accuracy': 0.5715799927711487, 'validation/loss': 2.054870367050171, 'validation/num_examples': 50000, 'test/accuracy': 0.456900030374527, 'test/loss': 2.6737120151519775, 'test/num_examples': 10000, 'score': 11377.642796039581, 'total_duration': 12337.427982330322, 'accumulated_submission_time': 11377.642796039581, 'accumulated_eval_time': 957.6350147724152, 'accumulated_logging_time': 0.784376859664917, 'global_step': 24585, 'preemption_count': 0}), (25496, {'train/accuracy': 0.6327929496765137, 'train/loss': 1.8049424886703491, 'validation/accuracy': 0.5851399898529053, 'validation/loss': 2.0230395793914795, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.616346597671509, 'test/num_examples': 10000, 'score': 11797.979766845703, 'total_duration': 12791.94000005722, 'accumulated_submission_time': 11797.979766845703, 'accumulated_eval_time': 991.7295281887054, 'accumulated_logging_time': 0.8135378360748291, 'global_step': 25496, 'preemption_count': 0}), (26406, {'train/accuracy': 0.6333593726158142, 'train/loss': 1.802080750465393, 'validation/accuracy': 0.5868799686431885, 'validation/loss': 2.013918876647949, 'validation/num_examples': 50000, 'test/accuracy': 0.47290003299713135, 'test/loss': 2.6314289569854736, 'test/num_examples': 10000, 'score': 12218.137630224228, 'total_duration': 13246.270922422409, 'accumulated_submission_time': 12218.137630224228, 'accumulated_eval_time': 1025.8188734054565, 'accumulated_logging_time': 0.8469099998474121, 'global_step': 26406, 'preemption_count': 0}), (27317, {'train/accuracy': 0.6479687094688416, 'train/loss': 1.6960623264312744, 'validation/accuracy': 0.5909599661827087, 'validation/loss': 1.9587397575378418, 'validation/num_examples': 50000, 'test/accuracy': 0.4727000296115875, 'test/loss': 2.574725866317749, 'test/num_examples': 10000, 'score': 12638.156475067139, 'total_duration': 13700.801671981812, 'accumulated_submission_time': 12638.156475067139, 'accumulated_eval_time': 1060.2494506835938, 'accumulated_logging_time': 0.8771259784698486, 'global_step': 27317, 'preemption_count': 0}), (28229, {'train/accuracy': 0.6434375047683716, 'train/loss': 1.7335150241851807, 'validation/accuracy': 0.5952199697494507, 'validation/loss': 1.9401875734329224, 'validation/num_examples': 50000, 'test/accuracy': 0.47530001401901245, 'test/loss': 2.569722890853882, 'test/num_examples': 10000, 'score': 13058.312211036682, 'total_duration': 14154.731746673584, 'accumulated_submission_time': 13058.312211036682, 'accumulated_eval_time': 1093.944759130478, 'accumulated_logging_time': 0.9052963256835938, 'global_step': 28229, 'preemption_count': 0}), (29139, {'train/accuracy': 0.6492577791213989, 'train/loss': 1.7050681114196777, 'validation/accuracy': 0.601639986038208, 'validation/loss': 1.9144740104675293, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.5420379638671875, 'test/num_examples': 10000, 'score': 13478.41717338562, 'total_duration': 14608.866518974304, 'accumulated_submission_time': 13478.41717338562, 'accumulated_eval_time': 1127.892193555832, 'accumulated_logging_time': 0.9366669654846191, 'global_step': 29139, 'preemption_count': 0}), (30052, {'train/accuracy': 0.6590429544448853, 'train/loss': 1.6291348934173584, 'validation/accuracy': 0.6089199781417847, 'validation/loss': 1.8620059490203857, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.49440336227417, 'test/num_examples': 10000, 'score': 13898.656394004822, 'total_duration': 15063.104083538055, 'accumulated_submission_time': 13898.656394004822, 'accumulated_eval_time': 1161.8061337471008, 'accumulated_logging_time': 0.9697074890136719, 'global_step': 30052, 'preemption_count': 0}), (30963, {'train/accuracy': 0.6642773151397705, 'train/loss': 1.6285508871078491, 'validation/accuracy': 0.6131199598312378, 'validation/loss': 1.841789960861206, 'validation/num_examples': 50000, 'test/accuracy': 0.49630001187324524, 'test/loss': 2.465266227722168, 'test/num_examples': 10000, 'score': 14318.683321475983, 'total_duration': 15516.47614622116, 'accumulated_submission_time': 14318.683321475983, 'accumulated_eval_time': 1195.0702483654022, 'accumulated_logging_time': 1.0004019737243652, 'global_step': 30963, 'preemption_count': 0}), (31875, {'train/accuracy': 0.6638085842132568, 'train/loss': 1.6437461376190186, 'validation/accuracy': 0.6118999719619751, 'validation/loss': 1.870315432548523, 'validation/num_examples': 50000, 'test/accuracy': 0.4912000298500061, 'test/loss': 2.4833028316497803, 'test/num_examples': 10000, 'score': 14739.014737844467, 'total_duration': 15970.657630205154, 'accumulated_submission_time': 14739.014737844467, 'accumulated_eval_time': 1228.8334305286407, 'accumulated_logging_time': 1.0361483097076416, 'global_step': 31875, 'preemption_count': 0}), (32789, {'train/accuracy': 0.6706835627555847, 'train/loss': 1.6040401458740234, 'validation/accuracy': 0.614139974117279, 'validation/loss': 1.8436777591705322, 'validation/num_examples': 50000, 'test/accuracy': 0.4969000220298767, 'test/loss': 2.4702181816101074, 'test/num_examples': 10000, 'score': 15159.29290342331, 'total_duration': 16425.24866938591, 'accumulated_submission_time': 15159.29290342331, 'accumulated_eval_time': 1263.0607657432556, 'accumulated_logging_time': 1.0695884227752686, 'global_step': 32789, 'preemption_count': 0}), (33701, {'train/accuracy': 0.6911327838897705, 'train/loss': 1.4933984279632568, 'validation/accuracy': 0.6180599927902222, 'validation/loss': 1.8183931112289429, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.447704315185547, 'test/num_examples': 10000, 'score': 15579.493244409561, 'total_duration': 16880.903116226196, 'accumulated_submission_time': 15579.493244409561, 'accumulated_eval_time': 1298.4260349273682, 'accumulated_logging_time': 1.1072144508361816, 'global_step': 33701, 'preemption_count': 0}), (34614, {'train/accuracy': 0.6721093654632568, 'train/loss': 1.5771911144256592, 'validation/accuracy': 0.619219958782196, 'validation/loss': 1.7937535047531128, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.422393560409546, 'test/num_examples': 10000, 'score': 15999.567883491516, 'total_duration': 17334.343224287033, 'accumulated_submission_time': 15999.567883491516, 'accumulated_eval_time': 1331.7058236598969, 'accumulated_logging_time': 1.141645908355713, 'global_step': 34614, 'preemption_count': 0}), (35525, {'train/accuracy': 0.6826562285423279, 'train/loss': 1.5192320346832275, 'validation/accuracy': 0.6253799796104431, 'validation/loss': 1.7672829627990723, 'validation/num_examples': 50000, 'test/accuracy': 0.506600022315979, 'test/loss': 2.3795220851898193, 'test/num_examples': 10000, 'score': 16419.87308859825, 'total_duration': 17788.572286605835, 'accumulated_submission_time': 16419.87308859825, 'accumulated_eval_time': 1365.5460460186005, 'accumulated_logging_time': 1.1748707294464111, 'global_step': 35525, 'preemption_count': 0}), (36438, {'train/accuracy': 0.6946093440055847, 'train/loss': 1.4783991575241089, 'validation/accuracy': 0.6278199553489685, 'validation/loss': 1.7693872451782227, 'validation/num_examples': 50000, 'test/accuracy': 0.5051000118255615, 'test/loss': 2.390291929244995, 'test/num_examples': 10000, 'score': 16839.959003686905, 'total_duration': 18242.025453329086, 'accumulated_submission_time': 16839.959003686905, 'accumulated_eval_time': 1398.829402923584, 'accumulated_logging_time': 1.20875883102417, 'global_step': 36438, 'preemption_count': 0}), (37351, {'train/accuracy': 0.6821093559265137, 'train/loss': 1.5482853651046753, 'validation/accuracy': 0.6320799589157104, 'validation/loss': 1.7631151676177979, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.3797619342803955, 'test/num_examples': 10000, 'score': 17260.044524669647, 'total_duration': 18695.517024755478, 'accumulated_submission_time': 17260.044524669647, 'accumulated_eval_time': 1432.1537964344025, 'accumulated_logging_time': 1.2387840747833252, 'global_step': 37351, 'preemption_count': 0}), (38264, {'train/accuracy': 0.6850000023841858, 'train/loss': 1.5315663814544678, 'validation/accuracy': 0.6313199996948242, 'validation/loss': 1.7667924165725708, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.3829877376556396, 'test/num_examples': 10000, 'score': 17680.28241252899, 'total_duration': 19148.84790277481, 'accumulated_submission_time': 17680.28241252899, 'accumulated_eval_time': 1465.1609783172607, 'accumulated_logging_time': 1.273481845855713, 'global_step': 38264, 'preemption_count': 0}), (39176, {'train/accuracy': 0.6991210579872131, 'train/loss': 1.461413025856018, 'validation/accuracy': 0.6354999542236328, 'validation/loss': 1.740901231765747, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.35638165473938, 'test/num_examples': 10000, 'score': 18100.411046266556, 'total_duration': 19602.334558963776, 'accumulated_submission_time': 18100.411046266556, 'accumulated_eval_time': 1498.4337601661682, 'accumulated_logging_time': 1.3083534240722656, 'global_step': 39176, 'preemption_count': 0}), (40088, {'train/accuracy': 0.6839648485183716, 'train/loss': 1.4925146102905273, 'validation/accuracy': 0.6367599964141846, 'validation/loss': 1.7041206359863281, 'validation/num_examples': 50000, 'test/accuracy': 0.5170000195503235, 'test/loss': 2.318955183029175, 'test/num_examples': 10000, 'score': 18520.329745292664, 'total_duration': 20055.471518993378, 'accumulated_submission_time': 18520.329745292664, 'accumulated_eval_time': 1531.565257549286, 'accumulated_logging_time': 1.3446745872497559, 'global_step': 40088, 'preemption_count': 0}), (41001, {'train/accuracy': 0.6875976324081421, 'train/loss': 1.500531792640686, 'validation/accuracy': 0.6361799836158752, 'validation/loss': 1.7311961650848389, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.3340766429901123, 'test/num_examples': 10000, 'score': 18940.719674110413, 'total_duration': 20509.467386484146, 'accumulated_submission_time': 18940.719674110413, 'accumulated_eval_time': 1565.088744878769, 'accumulated_logging_time': 1.376600980758667, 'global_step': 41001, 'preemption_count': 0}), (41912, {'train/accuracy': 0.7040234208106995, 'train/loss': 1.4264371395111084, 'validation/accuracy': 0.6458399891853333, 'validation/loss': 1.6851632595062256, 'validation/num_examples': 50000, 'test/accuracy': 0.5248000025749207, 'test/loss': 2.2989399433135986, 'test/num_examples': 10000, 'score': 19360.97429251671, 'total_duration': 20962.109224796295, 'accumulated_submission_time': 19360.97429251671, 'accumulated_eval_time': 1597.391491651535, 'accumulated_logging_time': 1.4102954864501953, 'global_step': 41912, 'preemption_count': 0}), (42821, {'train/accuracy': 0.6966992020606995, 'train/loss': 1.4827358722686768, 'validation/accuracy': 0.6440399885177612, 'validation/loss': 1.7102241516113281, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.3438947200775146, 'test/num_examples': 10000, 'score': 19781.339330911636, 'total_duration': 21416.576837062836, 'accumulated_submission_time': 19781.339330911636, 'accumulated_eval_time': 1631.4078161716461, 'accumulated_logging_time': 1.445805549621582, 'global_step': 42821, 'preemption_count': 0}), (43733, {'train/accuracy': 0.6943163871765137, 'train/loss': 1.5012829303741455, 'validation/accuracy': 0.6460599899291992, 'validation/loss': 1.7175486087799072, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.349139451980591, 'test/num_examples': 10000, 'score': 20201.28215122223, 'total_duration': 21870.283143281937, 'accumulated_submission_time': 20201.28215122223, 'accumulated_eval_time': 1665.082776069641, 'accumulated_logging_time': 1.4831054210662842, 'global_step': 43733, 'preemption_count': 0}), (44646, {'train/accuracy': 0.70361328125, 'train/loss': 1.4847627878189087, 'validation/accuracy': 0.643559992313385, 'validation/loss': 1.7350969314575195, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.3520002365112305, 'test/num_examples': 10000, 'score': 20621.605316638947, 'total_duration': 22322.22277355194, 'accumulated_submission_time': 20621.605316638947, 'accumulated_eval_time': 1696.6110389232635, 'accumulated_logging_time': 1.5203256607055664, 'global_step': 44646, 'preemption_count': 0}), (45557, {'train/accuracy': 0.7167187333106995, 'train/loss': 1.3568024635314941, 'validation/accuracy': 0.6495999693870544, 'validation/loss': 1.6522201299667358, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.260016918182373, 'test/num_examples': 10000, 'score': 21041.622383117676, 'total_duration': 22776.63697385788, 'accumulated_submission_time': 21041.622383117676, 'accumulated_eval_time': 1730.922518491745, 'accumulated_logging_time': 1.5553491115570068, 'global_step': 45557, 'preemption_count': 0}), (46468, {'train/accuracy': 0.6974999904632568, 'train/loss': 1.5168081521987915, 'validation/accuracy': 0.6491599678993225, 'validation/loss': 1.7391897439956665, 'validation/num_examples': 50000, 'test/accuracy': 0.5254999995231628, 'test/loss': 2.357267379760742, 'test/num_examples': 10000, 'score': 21461.639271736145, 'total_duration': 23231.38586783409, 'accumulated_submission_time': 21461.639271736145, 'accumulated_eval_time': 1765.5656650066376, 'accumulated_logging_time': 1.5932340621948242, 'global_step': 46468, 'preemption_count': 0}), (47381, {'train/accuracy': 0.7109375, 'train/loss': 1.4334065914154053, 'validation/accuracy': 0.6492399573326111, 'validation/loss': 1.697941541671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5318000316619873, 'test/loss': 2.307551622390747, 'test/num_examples': 10000, 'score': 21881.792940855026, 'total_duration': 23684.5236389637, 'accumulated_submission_time': 21881.792940855026, 'accumulated_eval_time': 1798.4531662464142, 'accumulated_logging_time': 1.6383380889892578, 'global_step': 47381, 'preemption_count': 0}), (48293, {'train/accuracy': 0.73046875, 'train/loss': 1.3111317157745361, 'validation/accuracy': 0.6547600030899048, 'validation/loss': 1.646582007408142, 'validation/num_examples': 50000, 'test/accuracy': 0.5335000157356262, 'test/loss': 2.266916036605835, 'test/num_examples': 10000, 'score': 22301.990122556686, 'total_duration': 24138.335812330246, 'accumulated_submission_time': 22301.990122556686, 'accumulated_eval_time': 1831.9779014587402, 'accumulated_logging_time': 1.6770923137664795, 'global_step': 48293, 'preemption_count': 0}), (49206, {'train/accuracy': 0.710156261920929, 'train/loss': 1.4143155813217163, 'validation/accuracy': 0.6555399894714355, 'validation/loss': 1.6536281108856201, 'validation/num_examples': 50000, 'test/accuracy': 0.5358999967575073, 'test/loss': 2.262042999267578, 'test/num_examples': 10000, 'score': 22722.04015159607, 'total_duration': 24592.202979803085, 'accumulated_submission_time': 22722.04015159607, 'accumulated_eval_time': 1865.7097523212433, 'accumulated_logging_time': 1.711674690246582, 'global_step': 49206, 'preemption_count': 0}), (50115, {'train/accuracy': 0.7168163657188416, 'train/loss': 1.348970651626587, 'validation/accuracy': 0.6584199666976929, 'validation/loss': 1.6088238954544067, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.217803955078125, 'test/num_examples': 10000, 'score': 23142.09687113762, 'total_duration': 25046.962195158005, 'accumulated_submission_time': 23142.09687113762, 'accumulated_eval_time': 1900.3253211975098, 'accumulated_logging_time': 1.7474756240844727, 'global_step': 50115, 'preemption_count': 0}), (51027, {'train/accuracy': 0.7308984398841858, 'train/loss': 1.3000104427337646, 'validation/accuracy': 0.6588199734687805, 'validation/loss': 1.6060658693313599, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.230520725250244, 'test/num_examples': 10000, 'score': 23562.426542520523, 'total_duration': 25502.540413618088, 'accumulated_submission_time': 23562.426542520523, 'accumulated_eval_time': 1935.4861042499542, 'accumulated_logging_time': 1.7837295532226562, 'global_step': 51027, 'preemption_count': 0}), (51941, {'train/accuracy': 0.71009761095047, 'train/loss': 1.3835265636444092, 'validation/accuracy': 0.6599999666213989, 'validation/loss': 1.6134287118911743, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.21612286567688, 'test/num_examples': 10000, 'score': 23982.65742635727, 'total_duration': 25957.92196583748, 'accumulated_submission_time': 23982.65742635727, 'accumulated_eval_time': 1970.5525019168854, 'accumulated_logging_time': 1.8164739608764648, 'global_step': 51941, 'preemption_count': 0}), (52851, {'train/accuracy': 0.7220507860183716, 'train/loss': 1.3369845151901245, 'validation/accuracy': 0.6620000004768372, 'validation/loss': 1.5932549238204956, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.2067039012908936, 'test/num_examples': 10000, 'score': 24402.8046002388, 'total_duration': 26411.741233587265, 'accumulated_submission_time': 24402.8046002388, 'accumulated_eval_time': 2004.1362063884735, 'accumulated_logging_time': 1.8542118072509766, 'global_step': 52851, 'preemption_count': 0}), (53763, {'train/accuracy': 0.7294335961341858, 'train/loss': 1.317986011505127, 'validation/accuracy': 0.6652799844741821, 'validation/loss': 1.6028327941894531, 'validation/num_examples': 50000, 'test/accuracy': 0.5420000553131104, 'test/loss': 2.219597339630127, 'test/num_examples': 10000, 'score': 24822.854472875595, 'total_duration': 26864.948058128357, 'accumulated_submission_time': 24822.854472875595, 'accumulated_eval_time': 2037.2056503295898, 'accumulated_logging_time': 1.8903439044952393, 'global_step': 53763, 'preemption_count': 0}), (54677, {'train/accuracy': 0.71937495470047, 'train/loss': 1.3365647792816162, 'validation/accuracy': 0.6654199957847595, 'validation/loss': 1.5735455751419067, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.1876070499420166, 'test/num_examples': 10000, 'score': 25243.066395998, 'total_duration': 27318.78368639946, 'accumulated_submission_time': 25243.066395998, 'accumulated_eval_time': 2070.7375979423523, 'accumulated_logging_time': 1.931077480316162, 'global_step': 54677, 'preemption_count': 0}), (55591, {'train/accuracy': 0.7230077981948853, 'train/loss': 1.379983901977539, 'validation/accuracy': 0.6684600114822388, 'validation/loss': 1.6201144456863403, 'validation/num_examples': 50000, 'test/accuracy': 0.5437000393867493, 'test/loss': 2.244570732116699, 'test/num_examples': 10000, 'score': 25663.03023004532, 'total_duration': 27772.168610334396, 'accumulated_submission_time': 25663.03023004532, 'accumulated_eval_time': 2104.065089225769, 'accumulated_logging_time': 1.9737062454223633, 'global_step': 55591, 'preemption_count': 0}), (56506, {'train/accuracy': 0.7322070002555847, 'train/loss': 1.3016204833984375, 'validation/accuracy': 0.66975998878479, 'validation/loss': 1.5884922742843628, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.196012496948242, 'test/num_examples': 10000, 'score': 26083.24350643158, 'total_duration': 28227.991536140442, 'accumulated_submission_time': 26083.24350643158, 'accumulated_eval_time': 2139.5847957134247, 'accumulated_logging_time': 2.0120248794555664, 'global_step': 56506, 'preemption_count': 0}), (57418, {'train/accuracy': 0.7277148365974426, 'train/loss': 1.3060152530670166, 'validation/accuracy': 0.6685799956321716, 'validation/loss': 1.5581419467926025, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.177110433578491, 'test/num_examples': 10000, 'score': 26503.21027612686, 'total_duration': 28683.141376018524, 'accumulated_submission_time': 26503.21027612686, 'accumulated_eval_time': 2174.6806008815765, 'accumulated_logging_time': 2.048649311065674, 'global_step': 57418, 'preemption_count': 0}), (58331, {'train/accuracy': 0.7244530916213989, 'train/loss': 1.3326257467269897, 'validation/accuracy': 0.6692999601364136, 'validation/loss': 1.571911096572876, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.1926655769348145, 'test/num_examples': 10000, 'score': 26923.5639295578, 'total_duration': 29138.245064258575, 'accumulated_submission_time': 26923.5639295578, 'accumulated_eval_time': 2209.344695329666, 'accumulated_logging_time': 2.083533763885498, 'global_step': 58331, 'preemption_count': 0}), (59242, {'train/accuracy': 0.7317968606948853, 'train/loss': 1.3039686679840088, 'validation/accuracy': 0.671999990940094, 'validation/loss': 1.5714185237884521, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.191420316696167, 'test/num_examples': 10000, 'score': 27343.48020362854, 'total_duration': 29591.821184158325, 'accumulated_submission_time': 27343.48020362854, 'accumulated_eval_time': 2242.9132976531982, 'accumulated_logging_time': 2.122434616088867, 'global_step': 59242, 'preemption_count': 0}), (60156, {'train/accuracy': 0.7542577981948853, 'train/loss': 1.2266101837158203, 'validation/accuracy': 0.671459972858429, 'validation/loss': 1.5660921335220337, 'validation/num_examples': 50000, 'test/accuracy': 0.5527999997138977, 'test/loss': 2.186643362045288, 'test/num_examples': 10000, 'score': 27763.480479478836, 'total_duration': 30046.77208018303, 'accumulated_submission_time': 27763.480479478836, 'accumulated_eval_time': 2277.7794167995453, 'accumulated_logging_time': 2.156461715698242, 'global_step': 60156, 'preemption_count': 0}), (61068, {'train/accuracy': 0.7308984398841858, 'train/loss': 1.2666162252426147, 'validation/accuracy': 0.6772399544715881, 'validation/loss': 1.4976990222930908, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 2.116438865661621, 'test/num_examples': 10000, 'score': 28183.66246366501, 'total_duration': 30500.72461438179, 'accumulated_submission_time': 28183.66246366501, 'accumulated_eval_time': 2311.4618620872498, 'accumulated_logging_time': 2.1936118602752686, 'global_step': 61068, 'preemption_count': 0}), (61980, {'train/accuracy': 0.73646479845047, 'train/loss': 1.2581452131271362, 'validation/accuracy': 0.6758399605751038, 'validation/loss': 1.5217695236206055, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 2.1416988372802734, 'test/num_examples': 10000, 'score': 28603.927173376083, 'total_duration': 30956.482098340988, 'accumulated_submission_time': 28603.927173376083, 'accumulated_eval_time': 2346.8619673252106, 'accumulated_logging_time': 2.2351157665252686, 'global_step': 61980, 'preemption_count': 0}), (62892, {'train/accuracy': 0.7489648461341858, 'train/loss': 1.2218483686447144, 'validation/accuracy': 0.6783999800682068, 'validation/loss': 1.5327138900756836, 'validation/num_examples': 50000, 'test/accuracy': 0.5552999973297119, 'test/loss': 2.162600517272949, 'test/num_examples': 10000, 'score': 29024.041388511658, 'total_duration': 31409.772384166718, 'accumulated_submission_time': 29024.041388511658, 'accumulated_eval_time': 2379.947853088379, 'accumulated_logging_time': 2.274474620819092, 'global_step': 62892, 'preemption_count': 0}), (63803, {'train/accuracy': 0.7317187190055847, 'train/loss': 1.283490777015686, 'validation/accuracy': 0.677899956703186, 'validation/loss': 1.5134638547897339, 'validation/num_examples': 50000, 'test/accuracy': 0.5556000471115112, 'test/loss': 2.1319777965545654, 'test/num_examples': 10000, 'score': 29443.85234260559, 'total_duration': 31862.303948879242, 'accumulated_submission_time': 29443.85234260559, 'accumulated_eval_time': 2412.2440707683563, 'accumulated_logging_time': 2.648000955581665, 'global_step': 63803, 'preemption_count': 0}), (64715, {'train/accuracy': 0.740527331829071, 'train/loss': 1.269747018814087, 'validation/accuracy': 0.6794599890708923, 'validation/loss': 1.536699652671814, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.1582353115081787, 'test/num_examples': 10000, 'score': 29863.933977127075, 'total_duration': 32318.939363241196, 'accumulated_submission_time': 29863.933977127075, 'accumulated_eval_time': 2448.706707715988, 'accumulated_logging_time': 2.6889126300811768, 'global_step': 64715, 'preemption_count': 0}), (65626, {'train/accuracy': 0.7494140267372131, 'train/loss': 1.1988508701324463, 'validation/accuracy': 0.6782400012016296, 'validation/loss': 1.5024621486663818, 'validation/num_examples': 50000, 'test/accuracy': 0.5551000237464905, 'test/loss': 2.1161949634552, 'test/num_examples': 10000, 'score': 30284.219016075134, 'total_duration': 32773.13093471527, 'accumulated_submission_time': 30284.219016075134, 'accumulated_eval_time': 2482.5249683856964, 'accumulated_logging_time': 2.7266457080841064, 'global_step': 65626, 'preemption_count': 0}), (66538, {'train/accuracy': 0.7292382717132568, 'train/loss': 1.3125627040863037, 'validation/accuracy': 0.6787999868392944, 'validation/loss': 1.5494202375411987, 'validation/num_examples': 50000, 'test/accuracy': 0.5455000400543213, 'test/loss': 2.1752123832702637, 'test/num_examples': 10000, 'score': 30704.13455057144, 'total_duration': 33227.839062690735, 'accumulated_submission_time': 30704.13455057144, 'accumulated_eval_time': 2517.2284500598907, 'accumulated_logging_time': 2.763366460800171, 'global_step': 66538, 'preemption_count': 0}), (67449, {'train/accuracy': 0.7469726204872131, 'train/loss': 1.2171990871429443, 'validation/accuracy': 0.6873199939727783, 'validation/loss': 1.4766743183135986, 'validation/num_examples': 50000, 'test/accuracy': 0.558899998664856, 'test/loss': 2.096421718597412, 'test/num_examples': 10000, 'score': 31124.17865896225, 'total_duration': 33680.919352293015, 'accumulated_submission_time': 31124.17865896225, 'accumulated_eval_time': 2550.1747620105743, 'accumulated_logging_time': 2.8025145530700684, 'global_step': 67449, 'preemption_count': 0}), (68359, {'train/accuracy': 0.7477148175239563, 'train/loss': 1.2257516384124756, 'validation/accuracy': 0.6833999752998352, 'validation/loss': 1.5142306089401245, 'validation/num_examples': 50000, 'test/accuracy': 0.5598000288009644, 'test/loss': 2.143460750579834, 'test/num_examples': 10000, 'score': 31544.106262922287, 'total_duration': 34135.353620529175, 'accumulated_submission_time': 31544.106262922287, 'accumulated_eval_time': 2584.592365026474, 'accumulated_logging_time': 2.8403160572052, 'global_step': 68359, 'preemption_count': 0}), (69272, {'train/accuracy': 0.7469335794448853, 'train/loss': 1.238759160041809, 'validation/accuracy': 0.6876199841499329, 'validation/loss': 1.490525484085083, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.0847983360290527, 'test/num_examples': 10000, 'score': 31964.290115594864, 'total_duration': 34592.112513780594, 'accumulated_submission_time': 31964.290115594864, 'accumulated_eval_time': 2621.0764644145966, 'accumulated_logging_time': 2.879974603652954, 'global_step': 69272, 'preemption_count': 0}), (70182, {'train/accuracy': 0.7432226538658142, 'train/loss': 1.2668992280960083, 'validation/accuracy': 0.6850199699401855, 'validation/loss': 1.5202137231826782, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 2.132345676422119, 'test/num_examples': 10000, 'score': 32384.55722093582, 'total_duration': 35045.258445978165, 'accumulated_submission_time': 32384.55722093582, 'accumulated_eval_time': 2653.8648805618286, 'accumulated_logging_time': 2.919417381286621, 'global_step': 70182, 'preemption_count': 0}), (71092, {'train/accuracy': 0.7544921636581421, 'train/loss': 1.1773465871810913, 'validation/accuracy': 0.6906999945640564, 'validation/loss': 1.4612739086151123, 'validation/num_examples': 50000, 'test/accuracy': 0.5626000165939331, 'test/loss': 2.0810532569885254, 'test/num_examples': 10000, 'score': 32804.878509521484, 'total_duration': 35501.86202931404, 'accumulated_submission_time': 32804.878509521484, 'accumulated_eval_time': 2690.0571892261505, 'accumulated_logging_time': 2.959210157394409, 'global_step': 71092, 'preemption_count': 0}), (72003, {'train/accuracy': 0.7497460842132568, 'train/loss': 1.197008490562439, 'validation/accuracy': 0.6877399682998657, 'validation/loss': 1.4694221019744873, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.097407341003418, 'test/num_examples': 10000, 'score': 33225.23718857765, 'total_duration': 35955.55195403099, 'accumulated_submission_time': 33225.23718857765, 'accumulated_eval_time': 2723.2984120845795, 'accumulated_logging_time': 2.999574899673462, 'global_step': 72003, 'preemption_count': 0}), (72913, {'train/accuracy': 0.7474804520606995, 'train/loss': 1.2026795148849487, 'validation/accuracy': 0.6865999698638916, 'validation/loss': 1.473946213722229, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1017489433288574, 'test/num_examples': 10000, 'score': 33645.199608802795, 'total_duration': 36409.107684612274, 'accumulated_submission_time': 33645.199608802795, 'accumulated_eval_time': 2756.8011240959167, 'accumulated_logging_time': 3.038689374923706, 'global_step': 72913, 'preemption_count': 0}), (73824, {'train/accuracy': 0.7582421898841858, 'train/loss': 1.1599042415618896, 'validation/accuracy': 0.6906999945640564, 'validation/loss': 1.4475252628326416, 'validation/num_examples': 50000, 'test/accuracy': 0.569100022315979, 'test/loss': 2.0638108253479004, 'test/num_examples': 10000, 'score': 34065.1739525795, 'total_duration': 36860.7737197876, 'accumulated_submission_time': 34065.1739525795, 'accumulated_eval_time': 2788.3988111019135, 'accumulated_logging_time': 3.0819501876831055, 'global_step': 73824, 'preemption_count': 0}), (74735, {'train/accuracy': 0.7710937261581421, 'train/loss': 1.1149591207504272, 'validation/accuracy': 0.6913999915122986, 'validation/loss': 1.4553277492523193, 'validation/num_examples': 50000, 'test/accuracy': 0.5658000111579895, 'test/loss': 2.082012176513672, 'test/num_examples': 10000, 'score': 34485.24294900894, 'total_duration': 37315.77154183388, 'accumulated_submission_time': 34485.24294900894, 'accumulated_eval_time': 2823.236649990082, 'accumulated_logging_time': 3.122302770614624, 'global_step': 74735, 'preemption_count': 0}), (75645, {'train/accuracy': 0.7518945336341858, 'train/loss': 1.226989984512329, 'validation/accuracy': 0.6896199584007263, 'validation/loss': 1.4911552667617798, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 2.1078085899353027, 'test/num_examples': 10000, 'score': 34905.34354329109, 'total_duration': 37769.46210241318, 'accumulated_submission_time': 34905.34354329109, 'accumulated_eval_time': 2856.7374284267426, 'accumulated_logging_time': 3.161123752593994, 'global_step': 75645, 'preemption_count': 0}), (76556, {'train/accuracy': 0.7568554282188416, 'train/loss': 1.1630686521530151, 'validation/accuracy': 0.694379985332489, 'validation/loss': 1.4409019947052002, 'validation/num_examples': 50000, 'test/accuracy': 0.5637000203132629, 'test/loss': 2.063631772994995, 'test/num_examples': 10000, 'score': 35325.38757443428, 'total_duration': 38222.31966996193, 'accumulated_submission_time': 35325.38757443428, 'accumulated_eval_time': 2889.4613375663757, 'accumulated_logging_time': 3.200040578842163, 'global_step': 76556, 'preemption_count': 0}), (77469, {'train/accuracy': 0.7740234136581421, 'train/loss': 1.1011523008346558, 'validation/accuracy': 0.6961199641227722, 'validation/loss': 1.4330646991729736, 'validation/num_examples': 50000, 'test/accuracy': 0.572100043296814, 'test/loss': 2.0542802810668945, 'test/num_examples': 10000, 'score': 35745.66950130463, 'total_duration': 38674.79872870445, 'accumulated_submission_time': 35745.66950130463, 'accumulated_eval_time': 2921.5647172927856, 'accumulated_logging_time': 3.242464065551758, 'global_step': 77469, 'preemption_count': 0}), (78381, {'train/accuracy': 0.7498828172683716, 'train/loss': 1.1963554620742798, 'validation/accuracy': 0.6926999688148499, 'validation/loss': 1.452593445777893, 'validation/num_examples': 50000, 'test/accuracy': 0.5669000148773193, 'test/loss': 2.084571599960327, 'test/num_examples': 10000, 'score': 36165.88693213463, 'total_duration': 39129.471900224686, 'accumulated_submission_time': 36165.88693213463, 'accumulated_eval_time': 2955.930624961853, 'accumulated_logging_time': 3.281820297241211, 'global_step': 78381, 'preemption_count': 0}), (79292, {'train/accuracy': 0.7596288919448853, 'train/loss': 1.14617919921875, 'validation/accuracy': 0.6948999762535095, 'validation/loss': 1.4263193607330322, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 2.044193744659424, 'test/num_examples': 10000, 'score': 36585.79311347008, 'total_duration': 39584.9595644474, 'accumulated_submission_time': 36585.79311347008, 'accumulated_eval_time': 2991.409699201584, 'accumulated_logging_time': 3.3323724269866943, 'global_step': 79292, 'preemption_count': 0}), (80204, {'train/accuracy': 0.7778124809265137, 'train/loss': 1.1191484928131104, 'validation/accuracy': 0.7011399865150452, 'validation/loss': 1.4412455558776855, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0484261512756348, 'test/num_examples': 10000, 'score': 37006.06656885147, 'total_duration': 40038.66042852402, 'accumulated_submission_time': 37006.06656885147, 'accumulated_eval_time': 3024.742676258087, 'accumulated_logging_time': 3.375904083251953, 'global_step': 80204, 'preemption_count': 0}), (81112, {'train/accuracy': 0.75927734375, 'train/loss': 1.1983951330184937, 'validation/accuracy': 0.6988799571990967, 'validation/loss': 1.469262957572937, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.0844738483428955, 'test/num_examples': 10000, 'score': 37425.9896273613, 'total_duration': 40495.30886769295, 'accumulated_submission_time': 37425.9896273613, 'accumulated_eval_time': 3061.377053260803, 'accumulated_logging_time': 3.4166457653045654, 'global_step': 81112, 'preemption_count': 0}), (82023, {'train/accuracy': 0.7640624642372131, 'train/loss': 1.1621880531311035, 'validation/accuracy': 0.702019989490509, 'validation/loss': 1.4289867877960205, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 2.0627498626708984, 'test/num_examples': 10000, 'score': 37846.07184147835, 'total_duration': 40952.05909061432, 'accumulated_submission_time': 37846.07184147835, 'accumulated_eval_time': 3097.956475019455, 'accumulated_logging_time': 3.4548816680908203, 'global_step': 82023, 'preemption_count': 0}), (82935, {'train/accuracy': 0.7707226276397705, 'train/loss': 1.1098370552062988, 'validation/accuracy': 0.7006799578666687, 'validation/loss': 1.4164754152297974, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 2.0360336303710938, 'test/num_examples': 10000, 'score': 38266.4472155571, 'total_duration': 41404.753534555435, 'accumulated_submission_time': 38266.4472155571, 'accumulated_eval_time': 3130.1866416931152, 'accumulated_logging_time': 3.492640733718872, 'global_step': 82935, 'preemption_count': 0}), (83846, {'train/accuracy': 0.7615429759025574, 'train/loss': 1.1838170289993286, 'validation/accuracy': 0.700760006904602, 'validation/loss': 1.4477368593215942, 'validation/num_examples': 50000, 'test/accuracy': 0.5703000426292419, 'test/loss': 2.065577745437622, 'test/num_examples': 10000, 'score': 38686.41108036041, 'total_duration': 41860.1210463047, 'accumulated_submission_time': 38686.41108036041, 'accumulated_eval_time': 3165.501063108444, 'accumulated_logging_time': 3.5313708782196045, 'global_step': 83846, 'preemption_count': 0}), (84759, {'train/accuracy': 0.7667187452316284, 'train/loss': 1.1391881704330444, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4073001146316528, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 2.0161125659942627, 'test/num_examples': 10000, 'score': 39106.723685503006, 'total_duration': 42315.893428087234, 'accumulated_submission_time': 39106.723685503006, 'accumulated_eval_time': 3200.868814229965, 'accumulated_logging_time': 3.5725789070129395, 'global_step': 84759, 'preemption_count': 0}), (85671, {'train/accuracy': 0.7760937213897705, 'train/loss': 1.1103529930114746, 'validation/accuracy': 0.7053200006484985, 'validation/loss': 1.4143160581588745, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 2.0242509841918945, 'test/num_examples': 10000, 'score': 39526.67753863335, 'total_duration': 42773.18708443642, 'accumulated_submission_time': 39526.67753863335, 'accumulated_eval_time': 3238.1050729751587, 'accumulated_logging_time': 3.6252217292785645, 'global_step': 85671, 'preemption_count': 0}), (86581, {'train/accuracy': 0.7747851610183716, 'train/loss': 1.1178936958312988, 'validation/accuracy': 0.7052599787712097, 'validation/loss': 1.4013491868972778, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 2.003753900527954, 'test/num_examples': 10000, 'score': 39946.76177215576, 'total_duration': 43231.5920381546, 'accumulated_submission_time': 39946.76177215576, 'accumulated_eval_time': 3276.332666158676, 'accumulated_logging_time': 3.668002128601074, 'global_step': 86581, 'preemption_count': 0}), (87492, {'train/accuracy': 0.7694140672683716, 'train/loss': 1.1339526176452637, 'validation/accuracy': 0.7033799886703491, 'validation/loss': 1.4059618711471558, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 2.0230345726013184, 'test/num_examples': 10000, 'score': 40366.834473609924, 'total_duration': 43687.16769170761, 'accumulated_submission_time': 40366.834473609924, 'accumulated_eval_time': 3311.738111257553, 'accumulated_logging_time': 3.710970640182495, 'global_step': 87492, 'preemption_count': 0}), (88404, {'train/accuracy': 0.7787304520606995, 'train/loss': 1.0719337463378906, 'validation/accuracy': 0.7060399651527405, 'validation/loss': 1.3819568157196045, 'validation/num_examples': 50000, 'test/accuracy': 0.5807000398635864, 'test/loss': 1.9886709451675415, 'test/num_examples': 10000, 'score': 40787.19777345657, 'total_duration': 44141.67059183121, 'accumulated_submission_time': 40787.19777345657, 'accumulated_eval_time': 3345.784725189209, 'accumulated_logging_time': 3.7528860569000244, 'global_step': 88404, 'preemption_count': 0}), (89314, {'train/accuracy': 0.7935351133346558, 'train/loss': 1.021241545677185, 'validation/accuracy': 0.7061399817466736, 'validation/loss': 1.3768731355667114, 'validation/num_examples': 50000, 'test/accuracy': 0.5854000449180603, 'test/loss': 1.981370210647583, 'test/num_examples': 10000, 'score': 41207.16171503067, 'total_duration': 44596.56769442558, 'accumulated_submission_time': 41207.16171503067, 'accumulated_eval_time': 3380.6273963451385, 'accumulated_logging_time': 3.791879415512085, 'global_step': 89314, 'preemption_count': 0}), (90225, {'train/accuracy': 0.7718163728713989, 'train/loss': 1.1109107732772827, 'validation/accuracy': 0.7108599543571472, 'validation/loss': 1.373457670211792, 'validation/num_examples': 50000, 'test/accuracy': 0.581000030040741, 'test/loss': 2.001279354095459, 'test/num_examples': 10000, 'score': 41627.17049217224, 'total_duration': 45052.54302406311, 'accumulated_submission_time': 41627.17049217224, 'accumulated_eval_time': 3416.4998614788055, 'accumulated_logging_time': 3.834437370300293, 'global_step': 90225, 'preemption_count': 0}), (91136, {'train/accuracy': 0.7748632431030273, 'train/loss': 1.0888638496398926, 'validation/accuracy': 0.7067999839782715, 'validation/loss': 1.3886761665344238, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9945118427276611, 'test/num_examples': 10000, 'score': 42047.39622235298, 'total_duration': 45510.83752632141, 'accumulated_submission_time': 42047.39622235298, 'accumulated_eval_time': 3454.4704039096832, 'accumulated_logging_time': 3.881521701812744, 'global_step': 91136, 'preemption_count': 0}), (92046, {'train/accuracy': 0.7909570336341858, 'train/loss': 1.043149471282959, 'validation/accuracy': 0.7130199670791626, 'validation/loss': 1.3840327262878418, 'validation/num_examples': 50000, 'test/accuracy': 0.585800051689148, 'test/loss': 1.991669774055481, 'test/num_examples': 10000, 'score': 42467.585582733154, 'total_duration': 45966.19464612007, 'accumulated_submission_time': 42467.585582733154, 'accumulated_eval_time': 3489.5446133613586, 'accumulated_logging_time': 3.9240567684173584, 'global_step': 92046, 'preemption_count': 0}), (92957, {'train/accuracy': 0.7769726514816284, 'train/loss': 1.0807150602340698, 'validation/accuracy': 0.7142399549484253, 'validation/loss': 1.3572258949279785, 'validation/num_examples': 50000, 'test/accuracy': 0.5952000021934509, 'test/loss': 1.9467597007751465, 'test/num_examples': 10000, 'score': 42887.685487508774, 'total_duration': 46422.40642333031, 'accumulated_submission_time': 42887.685487508774, 'accumulated_eval_time': 3525.5610892772675, 'accumulated_logging_time': 3.9683220386505127, 'global_step': 92957, 'preemption_count': 0}), (93870, {'train/accuracy': 0.784863293170929, 'train/loss': 1.0583343505859375, 'validation/accuracy': 0.7152599692344666, 'validation/loss': 1.3549997806549072, 'validation/num_examples': 50000, 'test/accuracy': 0.593000054359436, 'test/loss': 1.9606016874313354, 'test/num_examples': 10000, 'score': 43307.87548875809, 'total_duration': 46876.57886219025, 'accumulated_submission_time': 43307.87548875809, 'accumulated_eval_time': 3559.4507846832275, 'accumulated_logging_time': 4.0101823806762695, 'global_step': 93870, 'preemption_count': 0}), (94782, {'train/accuracy': 0.7880077958106995, 'train/loss': 1.056152582168579, 'validation/accuracy': 0.7136200070381165, 'validation/loss': 1.3756757974624634, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.9845054149627686, 'test/num_examples': 10000, 'score': 43727.9484167099, 'total_duration': 47331.931569337845, 'accumulated_submission_time': 43727.9484167099, 'accumulated_eval_time': 3594.6408054828644, 'accumulated_logging_time': 4.049016237258911, 'global_step': 94782, 'preemption_count': 0}), (95694, {'train/accuracy': 0.7802929282188416, 'train/loss': 1.084344744682312, 'validation/accuracy': 0.7172200083732605, 'validation/loss': 1.3568799495697021, 'validation/num_examples': 50000, 'test/accuracy': 0.5938000082969666, 'test/loss': 1.965236783027649, 'test/num_examples': 10000, 'score': 44147.98247885704, 'total_duration': 47786.82453203201, 'accumulated_submission_time': 44147.98247885704, 'accumulated_eval_time': 3629.39878821373, 'accumulated_logging_time': 4.098080635070801, 'global_step': 95694, 'preemption_count': 0}), (96606, {'train/accuracy': 0.7871288657188416, 'train/loss': 1.0491591691970825, 'validation/accuracy': 0.7164199948310852, 'validation/loss': 1.3515568971633911, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.9558128118515015, 'test/num_examples': 10000, 'score': 44568.36045074463, 'total_duration': 48244.41781044006, 'accumulated_submission_time': 44568.36045074463, 'accumulated_eval_time': 3666.5182325839996, 'accumulated_logging_time': 4.142136096954346, 'global_step': 96606, 'preemption_count': 0}), (97517, {'train/accuracy': 0.7954687476158142, 'train/loss': 1.0022492408752441, 'validation/accuracy': 0.7200999855995178, 'validation/loss': 1.323193907737732, 'validation/num_examples': 50000, 'test/accuracy': 0.593500018119812, 'test/loss': 1.929459810256958, 'test/num_examples': 10000, 'score': 44988.40380692482, 'total_duration': 48700.0536942482, 'accumulated_submission_time': 44988.40380692482, 'accumulated_eval_time': 3702.0162086486816, 'accumulated_logging_time': 4.1854517459869385, 'global_step': 97517, 'preemption_count': 0}), (98429, {'train/accuracy': 0.7873241901397705, 'train/loss': 1.0458989143371582, 'validation/accuracy': 0.7177599668502808, 'validation/loss': 1.3348171710968018, 'validation/num_examples': 50000, 'test/accuracy': 0.5968000292778015, 'test/loss': 1.9434531927108765, 'test/num_examples': 10000, 'score': 45408.60495519638, 'total_duration': 49155.00846171379, 'accumulated_submission_time': 45408.60495519638, 'accumulated_eval_time': 3736.6778705120087, 'accumulated_logging_time': 4.226574182510376, 'global_step': 98429, 'preemption_count': 0}), (99341, {'train/accuracy': 0.7878515720367432, 'train/loss': 1.0436352491378784, 'validation/accuracy': 0.7171199917793274, 'validation/loss': 1.3446561098098755, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.933828592300415, 'test/num_examples': 10000, 'score': 45828.625912189484, 'total_duration': 49609.64790344238, 'accumulated_submission_time': 45828.625912189484, 'accumulated_eval_time': 3771.2031738758087, 'accumulated_logging_time': 4.268864870071411, 'global_step': 99341, 'preemption_count': 0}), (100253, {'train/accuracy': 0.7922070026397705, 'train/loss': 1.0412719249725342, 'validation/accuracy': 0.7170599699020386, 'validation/loss': 1.350500226020813, 'validation/num_examples': 50000, 'test/accuracy': 0.5974000096321106, 'test/loss': 1.9491338729858398, 'test/num_examples': 10000, 'score': 46248.61977934837, 'total_duration': 50064.15418791771, 'accumulated_submission_time': 46248.61977934837, 'accumulated_eval_time': 3805.6231784820557, 'accumulated_logging_time': 4.309793710708618, 'global_step': 100253, 'preemption_count': 0}), (101164, {'train/accuracy': 0.8041796684265137, 'train/loss': 1.0106101036071777, 'validation/accuracy': 0.7200999855995178, 'validation/loss': 1.3571895360946655, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9695676565170288, 'test/num_examples': 10000, 'score': 46668.6330909729, 'total_duration': 50520.18922710419, 'accumulated_submission_time': 46668.6330909729, 'accumulated_eval_time': 3841.550982236862, 'accumulated_logging_time': 4.3530638217926025, 'global_step': 101164, 'preemption_count': 0}), (102076, {'train/accuracy': 0.7922070026397705, 'train/loss': 1.0534024238586426, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.3498034477233887, 'validation/num_examples': 50000, 'test/accuracy': 0.5991000533103943, 'test/loss': 1.9485112428665161, 'test/num_examples': 10000, 'score': 47088.92100191116, 'total_duration': 50977.975981235504, 'accumulated_submission_time': 47088.92100191116, 'accumulated_eval_time': 3878.951035261154, 'accumulated_logging_time': 4.401503562927246, 'global_step': 102076, 'preemption_count': 0}), (102988, {'train/accuracy': 0.79798823595047, 'train/loss': 1.0308090448379517, 'validation/accuracy': 0.7218199968338013, 'validation/loss': 1.3451817035675049, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.9463989734649658, 'test/num_examples': 10000, 'score': 47508.928409576416, 'total_duration': 51435.638216257095, 'accumulated_submission_time': 47508.928409576416, 'accumulated_eval_time': 3916.5116069316864, 'accumulated_logging_time': 4.4449450969696045, 'global_step': 102988, 'preemption_count': 0}), (103898, {'train/accuracy': 0.8116992115974426, 'train/loss': 0.9391869306564331, 'validation/accuracy': 0.7234399914741516, 'validation/loss': 1.3024083375930786, 'validation/num_examples': 50000, 'test/accuracy': 0.6037000417709351, 'test/loss': 1.9161393642425537, 'test/num_examples': 10000, 'score': 47929.27530050278, 'total_duration': 51890.92503976822, 'accumulated_submission_time': 47929.27530050278, 'accumulated_eval_time': 3951.357241153717, 'accumulated_logging_time': 4.487648963928223, 'global_step': 103898, 'preemption_count': 0}), (104812, {'train/accuracy': 0.7913671731948853, 'train/loss': 1.040604829788208, 'validation/accuracy': 0.7239199876785278, 'validation/loss': 1.3334083557128906, 'validation/num_examples': 50000, 'test/accuracy': 0.605400025844574, 'test/loss': 1.936792254447937, 'test/num_examples': 10000, 'score': 48349.42050933838, 'total_duration': 52345.748777627945, 'accumulated_submission_time': 48349.42050933838, 'accumulated_eval_time': 3985.942140340805, 'accumulated_logging_time': 4.530482769012451, 'global_step': 104812, 'preemption_count': 0}), (105725, {'train/accuracy': 0.7983007431030273, 'train/loss': 1.0020376443862915, 'validation/accuracy': 0.7238799929618835, 'validation/loss': 1.3144477605819702, 'validation/num_examples': 50000, 'test/accuracy': 0.5963000059127808, 'test/loss': 1.9204466342926025, 'test/num_examples': 10000, 'score': 48769.416988134384, 'total_duration': 52802.70588493347, 'accumulated_submission_time': 48769.416988134384, 'accumulated_eval_time': 4022.8122441768646, 'accumulated_logging_time': 4.570789575576782, 'global_step': 105725, 'preemption_count': 0}), (106635, {'train/accuracy': 0.8101366758346558, 'train/loss': 0.9570842385292053, 'validation/accuracy': 0.725820004940033, 'validation/loss': 1.2992676496505737, 'validation/num_examples': 50000, 'test/accuracy': 0.5995000004768372, 'test/loss': 1.9089785814285278, 'test/num_examples': 10000, 'score': 49189.342555999756, 'total_duration': 53258.69718146324, 'accumulated_submission_time': 49189.342555999756, 'accumulated_eval_time': 4058.433498620987, 'accumulated_logging_time': 4.963132858276367, 'global_step': 106635, 'preemption_count': 0}), (107544, {'train/accuracy': 0.7962890267372131, 'train/loss': 1.0105478763580322, 'validation/accuracy': 0.7277799844741821, 'validation/loss': 1.2957388162612915, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.9033678770065308, 'test/num_examples': 10000, 'score': 49609.5889275074, 'total_duration': 53715.495185136795, 'accumulated_submission_time': 49609.5889275074, 'accumulated_eval_time': 4094.8872702121735, 'accumulated_logging_time': 5.010452508926392, 'global_step': 107544, 'preemption_count': 0}), (108455, {'train/accuracy': 0.802539050579071, 'train/loss': 0.9898586273193359, 'validation/accuracy': 0.7298199534416199, 'validation/loss': 1.2934014797210693, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.8905043601989746, 'test/num_examples': 10000, 'score': 50029.638154029846, 'total_duration': 54173.51714682579, 'accumulated_submission_time': 50029.638154029846, 'accumulated_eval_time': 4132.766643047333, 'accumulated_logging_time': 5.052933216094971, 'global_step': 108455, 'preemption_count': 0}), (109365, {'train/accuracy': 0.8080468773841858, 'train/loss': 0.9750061631202698, 'validation/accuracy': 0.7301200032234192, 'validation/loss': 1.2988848686218262, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.9144893884658813, 'test/num_examples': 10000, 'score': 50449.68918180466, 'total_duration': 54632.3221013546, 'accumulated_submission_time': 50449.68918180466, 'accumulated_eval_time': 4171.423996925354, 'accumulated_logging_time': 5.098599672317505, 'global_step': 109365, 'preemption_count': 0}), (110274, {'train/accuracy': 0.7994921803474426, 'train/loss': 1.0312963724136353, 'validation/accuracy': 0.7298199534416199, 'validation/loss': 1.3248251676559448, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.9229018688201904, 'test/num_examples': 10000, 'score': 50869.71073126793, 'total_duration': 55088.965970516205, 'accumulated_submission_time': 50869.71073126793, 'accumulated_eval_time': 4207.95077753067, 'accumulated_logging_time': 5.143129587173462, 'global_step': 110274, 'preemption_count': 0}), (111184, {'train/accuracy': 0.8047069907188416, 'train/loss': 0.9976529479026794, 'validation/accuracy': 0.7334799766540527, 'validation/loss': 1.2964755296707153, 'validation/num_examples': 50000, 'test/accuracy': 0.6066000461578369, 'test/loss': 1.9121514558792114, 'test/num_examples': 10000, 'score': 51289.89997005463, 'total_duration': 55545.89949464798, 'accumulated_submission_time': 51289.89997005463, 'accumulated_eval_time': 4244.599578619003, 'accumulated_logging_time': 5.188409090042114, 'global_step': 111184, 'preemption_count': 0}), (112093, {'train/accuracy': 0.8110741972923279, 'train/loss': 0.9474124908447266, 'validation/accuracy': 0.7343599796295166, 'validation/loss': 1.2785613536834717, 'validation/num_examples': 50000, 'test/accuracy': 0.6106000542640686, 'test/loss': 1.8893661499023438, 'test/num_examples': 10000, 'score': 51709.84427022934, 'total_duration': 56003.3701505661, 'accumulated_submission_time': 51709.84427022934, 'accumulated_eval_time': 4282.025975942612, 'accumulated_logging_time': 5.237752914428711, 'global_step': 112093, 'preemption_count': 0}), (113001, {'train/accuracy': 0.8006640672683716, 'train/loss': 0.9824928045272827, 'validation/accuracy': 0.7322399616241455, 'validation/loss': 1.2828460931777954, 'validation/num_examples': 50000, 'test/accuracy': 0.6092000007629395, 'test/loss': 1.8907074928283691, 'test/num_examples': 10000, 'score': 52129.879022836685, 'total_duration': 56458.75296974182, 'accumulated_submission_time': 52129.879022836685, 'accumulated_eval_time': 4317.279319286346, 'accumulated_logging_time': 5.281320095062256, 'global_step': 113001, 'preemption_count': 0}), (113912, {'train/accuracy': 0.8100976347923279, 'train/loss': 0.9563019871711731, 'validation/accuracy': 0.7339800000190735, 'validation/loss': 1.2726399898529053, 'validation/num_examples': 50000, 'test/accuracy': 0.6098000407218933, 'test/loss': 1.8746048212051392, 'test/num_examples': 10000, 'score': 52549.861157894135, 'total_duration': 56913.93776369095, 'accumulated_submission_time': 52549.861157894135, 'accumulated_eval_time': 4352.38493680954, 'accumulated_logging_time': 5.327167272567749, 'global_step': 113912, 'preemption_count': 0}), (114823, {'train/accuracy': 0.8125, 'train/loss': 0.9517192840576172, 'validation/accuracy': 0.733460009098053, 'validation/loss': 1.2863125801086426, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.896116852760315, 'test/num_examples': 10000, 'score': 52970.15888476372, 'total_duration': 57369.3252120018, 'accumulated_submission_time': 52970.15888476372, 'accumulated_eval_time': 4387.382179737091, 'accumulated_logging_time': 5.3694212436676025, 'global_step': 114823, 'preemption_count': 0}), (115733, {'train/accuracy': 0.8245702981948853, 'train/loss': 0.9153898358345032, 'validation/accuracy': 0.7346000075340271, 'validation/loss': 1.282979965209961, 'validation/num_examples': 50000, 'test/accuracy': 0.6184000372886658, 'test/loss': 1.8812453746795654, 'test/num_examples': 10000, 'score': 53390.33986020088, 'total_duration': 57826.413011312485, 'accumulated_submission_time': 53390.33986020088, 'accumulated_eval_time': 4424.193821430206, 'accumulated_logging_time': 5.413953542709351, 'global_step': 115733, 'preemption_count': 0}), (116642, {'train/accuracy': 0.8141601085662842, 'train/loss': 0.9461965560913086, 'validation/accuracy': 0.7366200089454651, 'validation/loss': 1.266655683517456, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.8608219623565674, 'test/num_examples': 10000, 'score': 53810.46590876579, 'total_duration': 58282.51025009155, 'accumulated_submission_time': 53810.46590876579, 'accumulated_eval_time': 4460.06673002243, 'accumulated_logging_time': 5.46102499961853, 'global_step': 116642, 'preemption_count': 0}), (117554, {'train/accuracy': 0.8140038847923279, 'train/loss': 0.9657190442085266, 'validation/accuracy': 0.7342599630355835, 'validation/loss': 1.2892900705337524, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8896840810775757, 'test/num_examples': 10000, 'score': 54230.782210826874, 'total_duration': 58737.82883524895, 'accumulated_submission_time': 54230.782210826874, 'accumulated_eval_time': 4494.974093198776, 'accumulated_logging_time': 5.504605293273926, 'global_step': 117554, 'preemption_count': 0}), (118465, {'train/accuracy': 0.8282226324081421, 'train/loss': 0.9008296132087708, 'validation/accuracy': 0.7376599907875061, 'validation/loss': 1.2724199295043945, 'validation/num_examples': 50000, 'test/accuracy': 0.6170000433921814, 'test/loss': 1.8633793592453003, 'test/num_examples': 10000, 'score': 54650.84451293945, 'total_duration': 59194.7225048542, 'accumulated_submission_time': 54650.84451293945, 'accumulated_eval_time': 4531.71052479744, 'accumulated_logging_time': 5.548776865005493, 'global_step': 118465, 'preemption_count': 0}), (119375, {'train/accuracy': 0.81787109375, 'train/loss': 0.9437991380691528, 'validation/accuracy': 0.7403199672698975, 'validation/loss': 1.265620231628418, 'validation/num_examples': 50000, 'test/accuracy': 0.6144000291824341, 'test/loss': 1.868046522140503, 'test/num_examples': 10000, 'score': 55070.77024292946, 'total_duration': 59649.38969564438, 'accumulated_submission_time': 55070.77024292946, 'accumulated_eval_time': 4566.3491423130035, 'accumulated_logging_time': 5.601795434951782, 'global_step': 119375, 'preemption_count': 0}), (120285, {'train/accuracy': 0.8151366710662842, 'train/loss': 0.9468899369239807, 'validation/accuracy': 0.7399799823760986, 'validation/loss': 1.2717607021331787, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.870540738105774, 'test/num_examples': 10000, 'score': 55490.71073937416, 'total_duration': 60105.319628715515, 'accumulated_submission_time': 55490.71073937416, 'accumulated_eval_time': 4602.237959384918, 'accumulated_logging_time': 5.651767730712891, 'global_step': 120285, 'preemption_count': 0}), (121196, {'train/accuracy': 0.8320898413658142, 'train/loss': 0.8478189706802368, 'validation/accuracy': 0.742419958114624, 'validation/loss': 1.2173817157745361, 'validation/num_examples': 50000, 'test/accuracy': 0.6231000423431396, 'test/loss': 1.803972601890564, 'test/num_examples': 10000, 'score': 55910.660524606705, 'total_duration': 60562.70079231262, 'accumulated_submission_time': 55910.660524606705, 'accumulated_eval_time': 4639.570498466492, 'accumulated_logging_time': 5.699680805206299, 'global_step': 121196, 'preemption_count': 0}), (122107, {'train/accuracy': 0.8207421898841858, 'train/loss': 0.926133930683136, 'validation/accuracy': 0.7434799671173096, 'validation/loss': 1.2434664964675903, 'validation/num_examples': 50000, 'test/accuracy': 0.6258000135421753, 'test/loss': 1.8386973142623901, 'test/num_examples': 10000, 'score': 56330.80289840698, 'total_duration': 61017.744869470596, 'accumulated_submission_time': 56330.80289840698, 'accumulated_eval_time': 4674.371244430542, 'accumulated_logging_time': 5.749354600906372, 'global_step': 122107, 'preemption_count': 0}), (123019, {'train/accuracy': 0.8250390291213989, 'train/loss': 0.8884112238883972, 'validation/accuracy': 0.7450599670410156, 'validation/loss': 1.2220823764801025, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8158106803894043, 'test/num_examples': 10000, 'score': 56751.03601574898, 'total_duration': 61473.865228652954, 'accumulated_submission_time': 56751.03601574898, 'accumulated_eval_time': 4710.162944316864, 'accumulated_logging_time': 5.793039083480835, 'global_step': 123019, 'preemption_count': 0}), (123930, {'train/accuracy': 0.8338476419448853, 'train/loss': 0.8774459362030029, 'validation/accuracy': 0.7470399737358093, 'validation/loss': 1.2402421236038208, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.8210102319717407, 'test/num_examples': 10000, 'score': 57171.007404088974, 'total_duration': 61927.66263628006, 'accumulated_submission_time': 57171.007404088974, 'accumulated_eval_time': 4743.8915066719055, 'accumulated_logging_time': 5.839264869689941, 'global_step': 123930, 'preemption_count': 0}), (124839, {'train/accuracy': 0.8262499570846558, 'train/loss': 0.8751652240753174, 'validation/accuracy': 0.7460199594497681, 'validation/loss': 1.2193933725357056, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.8021245002746582, 'test/num_examples': 10000, 'score': 57591.029952049255, 'total_duration': 62385.57867670059, 'accumulated_submission_time': 57591.029952049255, 'accumulated_eval_time': 4781.690023899078, 'accumulated_logging_time': 5.884131908416748, 'global_step': 124839, 'preemption_count': 0}), (125749, {'train/accuracy': 0.8309765458106995, 'train/loss': 0.8596699833869934, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.2096494436264038, 'validation/num_examples': 50000, 'test/accuracy': 0.6221000552177429, 'test/loss': 1.8037065267562866, 'test/num_examples': 10000, 'score': 58010.99251627922, 'total_duration': 62841.30251932144, 'accumulated_submission_time': 58010.99251627922, 'accumulated_eval_time': 4817.354856729507, 'accumulated_logging_time': 5.930292129516602, 'global_step': 125749, 'preemption_count': 0}), (126660, {'train/accuracy': 0.83447265625, 'train/loss': 0.8702915906906128, 'validation/accuracy': 0.7460399866104126, 'validation/loss': 1.243055820465088, 'validation/num_examples': 50000, 'test/accuracy': 0.6221000552177429, 'test/loss': 1.84426748752594, 'test/num_examples': 10000, 'score': 58431.05421996117, 'total_duration': 63297.124264001846, 'accumulated_submission_time': 58431.05421996117, 'accumulated_eval_time': 4853.018300771713, 'accumulated_logging_time': 5.975062370300293, 'global_step': 126660, 'preemption_count': 0}), (127571, {'train/accuracy': 0.8270898461341858, 'train/loss': 0.900422215461731, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 1.2298741340637207, 'validation/num_examples': 50000, 'test/accuracy': 0.6247000098228455, 'test/loss': 1.8229761123657227, 'test/num_examples': 10000, 'score': 58851.270611047745, 'total_duration': 63754.40138721466, 'accumulated_submission_time': 58851.270611047745, 'accumulated_eval_time': 4889.977033615112, 'accumulated_logging_time': 6.026872634887695, 'global_step': 127571, 'preemption_count': 0}), (128484, {'train/accuracy': 0.8315820097923279, 'train/loss': 0.8549019694328308, 'validation/accuracy': 0.7501399517059326, 'validation/loss': 1.189975619316101, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.7846035957336426, 'test/num_examples': 10000, 'score': 59271.248304367065, 'total_duration': 64209.13311076164, 'accumulated_submission_time': 59271.248304367065, 'accumulated_eval_time': 4924.630076885223, 'accumulated_logging_time': 6.076805591583252, 'global_step': 128484, 'preemption_count': 0}), (129393, {'train/accuracy': 0.8374413847923279, 'train/loss': 0.8287783861160278, 'validation/accuracy': 0.7497599720954895, 'validation/loss': 1.1921215057373047, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.7807656526565552, 'test/num_examples': 10000, 'score': 59691.69730448723, 'total_duration': 64667.323753118515, 'accumulated_submission_time': 59691.69730448723, 'accumulated_eval_time': 4962.272361755371, 'accumulated_logging_time': 6.125463962554932, 'global_step': 129393, 'preemption_count': 0}), (130301, {'train/accuracy': 0.8481640219688416, 'train/loss': 0.8119232058525085, 'validation/accuracy': 0.7521599531173706, 'validation/loss': 1.201475977897644, 'validation/num_examples': 50000, 'test/accuracy': 0.6328000426292419, 'test/loss': 1.7882546186447144, 'test/num_examples': 10000, 'score': 60111.69879961014, 'total_duration': 65121.97418880463, 'accumulated_submission_time': 60111.69879961014, 'accumulated_eval_time': 4996.821247339249, 'accumulated_logging_time': 6.174273729324341, 'global_step': 130301, 'preemption_count': 0}), (131212, {'train/accuracy': 0.8323046565055847, 'train/loss': 0.855158269405365, 'validation/accuracy': 0.750819981098175, 'validation/loss': 1.1929678916931152, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.7767421007156372, 'test/num_examples': 10000, 'score': 60531.898847579956, 'total_duration': 65577.1236114502, 'accumulated_submission_time': 60531.898847579956, 'accumulated_eval_time': 5031.673780918121, 'accumulated_logging_time': 6.2202746868133545, 'global_step': 131212, 'preemption_count': 0}), (132124, {'train/accuracy': 0.8398241996765137, 'train/loss': 0.8325179815292358, 'validation/accuracy': 0.7538399696350098, 'validation/loss': 1.1827151775360107, 'validation/num_examples': 50000, 'test/accuracy': 0.6365000009536743, 'test/loss': 1.767372488975525, 'test/num_examples': 10000, 'score': 60951.835547208786, 'total_duration': 66033.88747620583, 'accumulated_submission_time': 60951.835547208786, 'accumulated_eval_time': 5068.405083656311, 'accumulated_logging_time': 6.265010595321655, 'global_step': 132124, 'preemption_count': 0}), (133034, {'train/accuracy': 0.8498437404632568, 'train/loss': 0.8029429912567139, 'validation/accuracy': 0.7543999552726746, 'validation/loss': 1.2014082670211792, 'validation/num_examples': 50000, 'test/accuracy': 0.636400043964386, 'test/loss': 1.7902463674545288, 'test/num_examples': 10000, 'score': 61371.809309244156, 'total_duration': 66489.36799764633, 'accumulated_submission_time': 61371.809309244156, 'accumulated_eval_time': 5103.814422369003, 'accumulated_logging_time': 6.311906576156616, 'global_step': 133034, 'preemption_count': 0}), (133945, {'train/accuracy': 0.8379882574081421, 'train/loss': 0.8267109394073486, 'validation/accuracy': 0.7539199590682983, 'validation/loss': 1.1843620538711548, 'validation/num_examples': 50000, 'test/accuracy': 0.6361000537872314, 'test/loss': 1.7660489082336426, 'test/num_examples': 10000, 'score': 61791.847019433975, 'total_duration': 66944.96662092209, 'accumulated_submission_time': 61791.847019433975, 'accumulated_eval_time': 5139.275134801865, 'accumulated_logging_time': 6.361809492111206, 'global_step': 133945, 'preemption_count': 0}), (134857, {'train/accuracy': 0.8421288728713989, 'train/loss': 0.8246394395828247, 'validation/accuracy': 0.7559399604797363, 'validation/loss': 1.183579444885254, 'validation/num_examples': 50000, 'test/accuracy': 0.6372000575065613, 'test/loss': 1.7756555080413818, 'test/num_examples': 10000, 'score': 62211.888147592545, 'total_duration': 67399.85404849052, 'accumulated_submission_time': 62211.888147592545, 'accumulated_eval_time': 5174.0214302539825, 'accumulated_logging_time': 6.409978866577148, 'global_step': 134857, 'preemption_count': 0}), (135766, {'train/accuracy': 0.847460925579071, 'train/loss': 0.8194810152053833, 'validation/accuracy': 0.7557799816131592, 'validation/loss': 1.1992911100387573, 'validation/num_examples': 50000, 'test/accuracy': 0.635200023651123, 'test/loss': 1.788246989250183, 'test/num_examples': 10000, 'score': 62631.872718811035, 'total_duration': 67856.05798530579, 'accumulated_submission_time': 62631.872718811035, 'accumulated_eval_time': 5210.1441123485565, 'accumulated_logging_time': 6.4558424949646, 'global_step': 135766, 'preemption_count': 0}), (136677, {'train/accuracy': 0.8434374928474426, 'train/loss': 0.8096722364425659, 'validation/accuracy': 0.75764000415802, 'validation/loss': 1.1676242351531982, 'validation/num_examples': 50000, 'test/accuracy': 0.6416000127792358, 'test/loss': 1.7473360300064087, 'test/num_examples': 10000, 'score': 63052.01003956795, 'total_duration': 68314.43851542473, 'accumulated_submission_time': 63052.01003956795, 'accumulated_eval_time': 5248.283586978912, 'accumulated_logging_time': 6.5085837841033936, 'global_step': 136677, 'preemption_count': 0}), (137587, {'train/accuracy': 0.8507421612739563, 'train/loss': 0.7920365929603577, 'validation/accuracy': 0.7602199912071228, 'validation/loss': 1.160401463508606, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.75056791305542, 'test/num_examples': 10000, 'score': 63472.32473063469, 'total_duration': 68772.46301627159, 'accumulated_submission_time': 63472.32473063469, 'accumulated_eval_time': 5285.890654087067, 'accumulated_logging_time': 6.560576677322388, 'global_step': 137587, 'preemption_count': 0}), (138498, {'train/accuracy': 0.8550781011581421, 'train/loss': 0.7727980613708496, 'validation/accuracy': 0.7599799633026123, 'validation/loss': 1.1643576622009277, 'validation/num_examples': 50000, 'test/accuracy': 0.6422000527381897, 'test/loss': 1.75530207157135, 'test/num_examples': 10000, 'score': 63892.652406454086, 'total_duration': 69228.47512078285, 'accumulated_submission_time': 63892.652406454086, 'accumulated_eval_time': 5321.47384428978, 'accumulated_logging_time': 6.611251592636108, 'global_step': 138498, 'preemption_count': 0}), (139410, {'train/accuracy': 0.8495312333106995, 'train/loss': 0.8162821531295776, 'validation/accuracy': 0.7620399594306946, 'validation/loss': 1.1751127243041992, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.7389004230499268, 'test/num_examples': 10000, 'score': 64312.980843544006, 'total_duration': 69689.05562400818, 'accumulated_submission_time': 64312.980843544006, 'accumulated_eval_time': 5361.627897024155, 'accumulated_logging_time': 6.65903902053833, 'global_step': 139410, 'preemption_count': 0}), (140320, {'train/accuracy': 0.8509179353713989, 'train/loss': 0.797985315322876, 'validation/accuracy': 0.7607600092887878, 'validation/loss': 1.1660995483398438, 'validation/num_examples': 50000, 'test/accuracy': 0.6412000060081482, 'test/loss': 1.7588180303573608, 'test/num_examples': 10000, 'score': 64732.98385691643, 'total_duration': 70144.70323824883, 'accumulated_submission_time': 64732.98385691643, 'accumulated_eval_time': 5397.175455093384, 'accumulated_logging_time': 6.705945253372192, 'global_step': 140320, 'preemption_count': 0}), (141230, {'train/accuracy': 0.8560156226158142, 'train/loss': 0.7755244374275208, 'validation/accuracy': 0.7628999948501587, 'validation/loss': 1.1578243970870972, 'validation/num_examples': 50000, 'test/accuracy': 0.648900032043457, 'test/loss': 1.7374924421310425, 'test/num_examples': 10000, 'score': 65152.95014190674, 'total_duration': 70599.94521164894, 'accumulated_submission_time': 65152.95014190674, 'accumulated_eval_time': 5432.35400223732, 'accumulated_logging_time': 6.752318620681763, 'global_step': 141230, 'preemption_count': 0}), (142141, {'train/accuracy': 0.85267573595047, 'train/loss': 0.7707575559616089, 'validation/accuracy': 0.7630599737167358, 'validation/loss': 1.145895004272461, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.714654803276062, 'test/num_examples': 10000, 'score': 65573.02896523476, 'total_duration': 71058.30751371384, 'accumulated_submission_time': 65573.02896523476, 'accumulated_eval_time': 5470.536109209061, 'accumulated_logging_time': 6.80316686630249, 'global_step': 142141, 'preemption_count': 0}), (143053, {'train/accuracy': 0.8552343845367432, 'train/loss': 0.7671908140182495, 'validation/accuracy': 0.7658599615097046, 'validation/loss': 1.1423100233078003, 'validation/num_examples': 50000, 'test/accuracy': 0.6479000449180603, 'test/loss': 1.7201541662216187, 'test/num_examples': 10000, 'score': 65993.15498352051, 'total_duration': 71517.50735139847, 'accumulated_submission_time': 65993.15498352051, 'accumulated_eval_time': 5509.510308504105, 'accumulated_logging_time': 6.851216554641724, 'global_step': 143053, 'preemption_count': 0}), (143963, {'train/accuracy': 0.8605859279632568, 'train/loss': 0.7456603646278381, 'validation/accuracy': 0.7659599781036377, 'validation/loss': 1.1388903856277466, 'validation/num_examples': 50000, 'test/accuracy': 0.648300051689148, 'test/loss': 1.7129008769989014, 'test/num_examples': 10000, 'score': 66413.09290719032, 'total_duration': 71975.2932009697, 'accumulated_submission_time': 66413.09290719032, 'accumulated_eval_time': 5547.255858421326, 'accumulated_logging_time': 6.90261173248291, 'global_step': 143963, 'preemption_count': 0}), (144873, {'train/accuracy': 0.8684179782867432, 'train/loss': 0.724058210849762, 'validation/accuracy': 0.7653599977493286, 'validation/loss': 1.1371018886566162, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.7198455333709717, 'test/num_examples': 10000, 'score': 66833.13764357567, 'total_duration': 72433.08190202713, 'accumulated_submission_time': 66833.13764357567, 'accumulated_eval_time': 5584.90097284317, 'accumulated_logging_time': 6.950519561767578, 'global_step': 144873, 'preemption_count': 0}), (145782, {'train/accuracy': 0.8575780987739563, 'train/loss': 0.7682866454124451, 'validation/accuracy': 0.764519989490509, 'validation/loss': 1.1535091400146484, 'validation/num_examples': 50000, 'test/accuracy': 0.6463000178337097, 'test/loss': 1.7320133447647095, 'test/num_examples': 10000, 'score': 67253.06000828743, 'total_duration': 72893.4165430069, 'accumulated_submission_time': 67253.06000828743, 'accumulated_eval_time': 5625.206836462021, 'accumulated_logging_time': 7.006183862686157, 'global_step': 145782, 'preemption_count': 0}), (146692, {'train/accuracy': 0.8640234470367432, 'train/loss': 0.74609375, 'validation/accuracy': 0.7672399878501892, 'validation/loss': 1.1380559206008911, 'validation/num_examples': 50000, 'test/accuracy': 0.6510000228881836, 'test/loss': 1.7285139560699463, 'test/num_examples': 10000, 'score': 67673.04480743408, 'total_duration': 73350.82883524895, 'accumulated_submission_time': 67673.04480743408, 'accumulated_eval_time': 5662.5311868190765, 'accumulated_logging_time': 7.058150768280029, 'global_step': 146692, 'preemption_count': 0}), (147604, {'train/accuracy': 0.8709960579872131, 'train/loss': 0.6939271092414856, 'validation/accuracy': 0.7691799998283386, 'validation/loss': 1.1134551763534546, 'validation/num_examples': 50000, 'test/accuracy': 0.6548000574111938, 'test/loss': 1.6992754936218262, 'test/num_examples': 10000, 'score': 68093.3377597332, 'total_duration': 73807.02789735794, 'accumulated_submission_time': 68093.3377597332, 'accumulated_eval_time': 5698.334320783615, 'accumulated_logging_time': 7.110531806945801, 'global_step': 147604, 'preemption_count': 0}), (148514, {'train/accuracy': 0.8633202910423279, 'train/loss': 0.7357399463653564, 'validation/accuracy': 0.7691999673843384, 'validation/loss': 1.1223732233047485, 'validation/num_examples': 50000, 'test/accuracy': 0.6541000604629517, 'test/loss': 1.6979526281356812, 'test/num_examples': 10000, 'score': 68513.33066034317, 'total_duration': 74264.12940835953, 'accumulated_submission_time': 68513.33066034317, 'accumulated_eval_time': 5735.344721794128, 'accumulated_logging_time': 7.158812522888184, 'global_step': 148514, 'preemption_count': 0}), (149422, {'train/accuracy': 0.8650000095367432, 'train/loss': 0.7219473123550415, 'validation/accuracy': 0.7705199718475342, 'validation/loss': 1.1146769523620605, 'validation/num_examples': 50000, 'test/accuracy': 0.65420001745224, 'test/loss': 1.6978703737258911, 'test/num_examples': 10000, 'score': 68933.41969394684, 'total_duration': 74722.75263619423, 'accumulated_submission_time': 68933.41969394684, 'accumulated_eval_time': 5773.777179718018, 'accumulated_logging_time': 7.209390163421631, 'global_step': 149422, 'preemption_count': 0}), (150330, {'train/accuracy': 0.8721874952316284, 'train/loss': 0.741978645324707, 'validation/accuracy': 0.7699599862098694, 'validation/loss': 1.1527972221374512, 'validation/num_examples': 50000, 'test/accuracy': 0.652400016784668, 'test/loss': 1.7470016479492188, 'test/num_examples': 10000, 'score': 69353.35374689102, 'total_duration': 75181.42854118347, 'accumulated_submission_time': 69353.35374689102, 'accumulated_eval_time': 5812.420975446701, 'accumulated_logging_time': 7.256996393203735, 'global_step': 150330, 'preemption_count': 0}), (151238, {'train/accuracy': 0.8633593320846558, 'train/loss': 0.7349731922149658, 'validation/accuracy': 0.7711600065231323, 'validation/loss': 1.12005615234375, 'validation/num_examples': 50000, 'test/accuracy': 0.6549000144004822, 'test/loss': 1.6994355916976929, 'test/num_examples': 10000, 'score': 69773.56124687195, 'total_duration': 75640.54940104485, 'accumulated_submission_time': 69773.56124687195, 'accumulated_eval_time': 5851.233624219894, 'accumulated_logging_time': 7.306897401809692, 'global_step': 151238, 'preemption_count': 0}), (152146, {'train/accuracy': 0.8692968487739563, 'train/loss': 0.7008588314056396, 'validation/accuracy': 0.7726799845695496, 'validation/loss': 1.1004048585891724, 'validation/num_examples': 50000, 'test/accuracy': 0.6581000089645386, 'test/loss': 1.682053804397583, 'test/num_examples': 10000, 'score': 70193.82568836212, 'total_duration': 76096.07240009308, 'accumulated_submission_time': 70193.82568836212, 'accumulated_eval_time': 5886.390437841415, 'accumulated_logging_time': 7.358208894729614, 'global_step': 152146, 'preemption_count': 0}), (153053, {'train/accuracy': 0.8724218606948853, 'train/loss': 0.7150508761405945, 'validation/accuracy': 0.7715199589729309, 'validation/loss': 1.1307733058929443, 'validation/num_examples': 50000, 'test/accuracy': 0.6557000279426575, 'test/loss': 1.7197359800338745, 'test/num_examples': 10000, 'score': 70613.74158143997, 'total_duration': 76553.8149971962, 'accumulated_submission_time': 70613.74158143997, 'accumulated_eval_time': 5924.112850666046, 'accumulated_logging_time': 7.41174578666687, 'global_step': 153053, 'preemption_count': 0}), (153959, {'train/accuracy': 0.8653124570846558, 'train/loss': 0.7207576632499695, 'validation/accuracy': 0.7726999521255493, 'validation/loss': 1.1108520030975342, 'validation/num_examples': 50000, 'test/accuracy': 0.6573000550270081, 'test/loss': 1.6898996829986572, 'test/num_examples': 10000, 'score': 71033.70966124535, 'total_duration': 77009.18772745132, 'accumulated_submission_time': 71033.70966124535, 'accumulated_eval_time': 5959.418373346329, 'accumulated_logging_time': 7.46028733253479, 'global_step': 153959, 'preemption_count': 0}), (154870, {'train/accuracy': 0.8743554353713989, 'train/loss': 0.6960504651069641, 'validation/accuracy': 0.7752000093460083, 'validation/loss': 1.1026690006256104, 'validation/num_examples': 50000, 'test/accuracy': 0.6573000550270081, 'test/loss': 1.684720754623413, 'test/num_examples': 10000, 'score': 71454.04873561859, 'total_duration': 77463.63437724113, 'accumulated_submission_time': 71454.04873561859, 'accumulated_eval_time': 5993.421750068665, 'accumulated_logging_time': 7.513123273849487, 'global_step': 154870, 'preemption_count': 0}), (155782, {'train/accuracy': 0.872851550579071, 'train/loss': 0.6959668397903442, 'validation/accuracy': 0.7761799693107605, 'validation/loss': 1.1069529056549072, 'validation/num_examples': 50000, 'test/accuracy': 0.6583000421524048, 'test/loss': 1.697216510772705, 'test/num_examples': 10000, 'score': 71874.4541144371, 'total_duration': 77920.41535067558, 'accumulated_submission_time': 71874.4541144371, 'accumulated_eval_time': 6029.695031404495, 'accumulated_logging_time': 7.564982175827026, 'global_step': 155782, 'preemption_count': 0}), (156692, {'train/accuracy': 0.8742382526397705, 'train/loss': 0.7012655735015869, 'validation/accuracy': 0.7750200033187866, 'validation/loss': 1.1078037023544312, 'validation/num_examples': 50000, 'test/accuracy': 0.6554000377655029, 'test/loss': 1.695989727973938, 'test/num_examples': 10000, 'score': 72294.57926630974, 'total_duration': 78375.58976507187, 'accumulated_submission_time': 72294.57926630974, 'accumulated_eval_time': 6064.638352155685, 'accumulated_logging_time': 7.620237112045288, 'global_step': 156692, 'preemption_count': 0}), (157601, {'train/accuracy': 0.8741992115974426, 'train/loss': 0.7010714411735535, 'validation/accuracy': 0.776919960975647, 'validation/loss': 1.1080583333969116, 'validation/num_examples': 50000, 'test/accuracy': 0.6605000495910645, 'test/loss': 1.6827137470245361, 'test/num_examples': 10000, 'score': 72714.48941850662, 'total_duration': 78831.00256085396, 'accumulated_submission_time': 72714.48941850662, 'accumulated_eval_time': 6100.034034729004, 'accumulated_logging_time': 7.676239013671875, 'global_step': 157601, 'preemption_count': 0}), (158513, {'train/accuracy': 0.8779296875, 'train/loss': 0.6749905943870544, 'validation/accuracy': 0.7773799896240234, 'validation/loss': 1.0918878316879272, 'validation/num_examples': 50000, 'test/accuracy': 0.6610000133514404, 'test/loss': 1.6656324863433838, 'test/num_examples': 10000, 'score': 73134.6789803505, 'total_duration': 79286.81630277634, 'accumulated_submission_time': 73134.6789803505, 'accumulated_eval_time': 6135.559215307236, 'accumulated_logging_time': 7.725278377532959, 'global_step': 158513, 'preemption_count': 0}), (159424, {'train/accuracy': 0.8819335699081421, 'train/loss': 0.6788507103919983, 'validation/accuracy': 0.7778399586677551, 'validation/loss': 1.1027580499649048, 'validation/num_examples': 50000, 'test/accuracy': 0.6589000225067139, 'test/loss': 1.6929190158843994, 'test/num_examples': 10000, 'score': 73554.84628415108, 'total_duration': 79743.22312545776, 'accumulated_submission_time': 73554.84628415108, 'accumulated_eval_time': 6171.688840389252, 'accumulated_logging_time': 7.782891035079956, 'global_step': 159424, 'preemption_count': 0}), (160336, {'train/accuracy': 0.8783202767372131, 'train/loss': 0.6747952103614807, 'validation/accuracy': 0.7796799540519714, 'validation/loss': 1.08854079246521, 'validation/num_examples': 50000, 'test/accuracy': 0.6620000600814819, 'test/loss': 1.66422700881958, 'test/num_examples': 10000, 'score': 73974.78279566765, 'total_duration': 80200.6146595478, 'accumulated_submission_time': 73974.78279566765, 'accumulated_eval_time': 6209.042106866837, 'accumulated_logging_time': 7.834174156188965, 'global_step': 160336, 'preemption_count': 0}), (161245, {'train/accuracy': 0.8847070336341858, 'train/loss': 0.6641880869865417, 'validation/accuracy': 0.7793799638748169, 'validation/loss': 1.094588279724121, 'validation/num_examples': 50000, 'test/accuracy': 0.6619000434875488, 'test/loss': 1.676234245300293, 'test/num_examples': 10000, 'score': 74395.06458425522, 'total_duration': 80656.78582334518, 'accumulated_submission_time': 74395.06458425522, 'accumulated_eval_time': 6244.829773902893, 'accumulated_logging_time': 7.885188102722168, 'global_step': 161245, 'preemption_count': 0}), (162155, {'train/accuracy': 0.8860546946525574, 'train/loss': 0.6558742523193359, 'validation/accuracy': 0.7773999571800232, 'validation/loss': 1.0941129922866821, 'validation/num_examples': 50000, 'test/accuracy': 0.6637000441551208, 'test/loss': 1.6746059656143188, 'test/num_examples': 10000, 'score': 74815.33997106552, 'total_duration': 81112.87297201157, 'accumulated_submission_time': 74815.33997106552, 'accumulated_eval_time': 6280.536881446838, 'accumulated_logging_time': 7.93939733505249, 'global_step': 162155, 'preemption_count': 0}), (163065, {'train/accuracy': 0.8809179663658142, 'train/loss': 0.6672109961509705, 'validation/accuracy': 0.7792999744415283, 'validation/loss': 1.0870453119277954, 'validation/num_examples': 50000, 'test/accuracy': 0.6628000140190125, 'test/loss': 1.6721868515014648, 'test/num_examples': 10000, 'score': 75235.40973472595, 'total_duration': 81568.50692629814, 'accumulated_submission_time': 75235.40973472595, 'accumulated_eval_time': 6315.99707698822, 'accumulated_logging_time': 7.991906642913818, 'global_step': 163065, 'preemption_count': 0}), (163977, {'train/accuracy': 0.8843554258346558, 'train/loss': 0.6567904353141785, 'validation/accuracy': 0.7809799909591675, 'validation/loss': 1.085119605064392, 'validation/num_examples': 50000, 'test/accuracy': 0.6634000539779663, 'test/loss': 1.6664263010025024, 'test/num_examples': 10000, 'score': 75655.63436961174, 'total_duration': 82025.42798662186, 'accumulated_submission_time': 75655.63436961174, 'accumulated_eval_time': 6352.588169336319, 'accumulated_logging_time': 8.046016693115234, 'global_step': 163977, 'preemption_count': 0}), (164889, {'train/accuracy': 0.8860155940055847, 'train/loss': 0.6490659117698669, 'validation/accuracy': 0.7811999917030334, 'validation/loss': 1.085237979888916, 'validation/num_examples': 50000, 'test/accuracy': 0.6682000160217285, 'test/loss': 1.6649689674377441, 'test/num_examples': 10000, 'score': 76075.67520737648, 'total_duration': 82483.71307039261, 'accumulated_submission_time': 76075.67520737648, 'accumulated_eval_time': 6390.728995323181, 'accumulated_logging_time': 8.098177194595337, 'global_step': 164889, 'preemption_count': 0}), (165800, {'train/accuracy': 0.8830273151397705, 'train/loss': 0.6618872880935669, 'validation/accuracy': 0.7819199562072754, 'validation/loss': 1.0855050086975098, 'validation/num_examples': 50000, 'test/accuracy': 0.6676000356674194, 'test/loss': 1.659148931503296, 'test/num_examples': 10000, 'score': 76495.9494357109, 'total_duration': 82943.06176996231, 'accumulated_submission_time': 76495.9494357109, 'accumulated_eval_time': 6429.695057630539, 'accumulated_logging_time': 8.155537843704224, 'global_step': 165800, 'preemption_count': 0}), (166711, {'train/accuracy': 0.8886913657188416, 'train/loss': 0.645067036151886, 'validation/accuracy': 0.7826399803161621, 'validation/loss': 1.0786688327789307, 'validation/num_examples': 50000, 'test/accuracy': 0.6674000024795532, 'test/loss': 1.6567219495773315, 'test/num_examples': 10000, 'score': 76916.21601438522, 'total_duration': 83398.33122730255, 'accumulated_submission_time': 76916.21601438522, 'accumulated_eval_time': 6464.591763496399, 'accumulated_logging_time': 8.211509466171265, 'global_step': 166711, 'preemption_count': 0}), (167620, {'train/accuracy': 0.88818359375, 'train/loss': 0.6586396098136902, 'validation/accuracy': 0.7811399698257446, 'validation/loss': 1.0905895233154297, 'validation/num_examples': 50000, 'test/accuracy': 0.6629000306129456, 'test/loss': 1.6669039726257324, 'test/num_examples': 10000, 'score': 77336.24432229996, 'total_duration': 83855.08708715439, 'accumulated_submission_time': 77336.24432229996, 'accumulated_eval_time': 6501.212018728256, 'accumulated_logging_time': 8.268371820449829, 'global_step': 167620, 'preemption_count': 0})], 'global_step': 168028}
I0201 18:30:40.777157 140085747812160 submission_runner.py:586] Timing: 77520.28070497513
I0201 18:30:40.777272 140085747812160 submission_runner.py:588] Total number of evals: 185
I0201 18:30:40.777343 140085747812160 submission_runner.py:589] ====================
I0201 18:30:40.777408 140085747812160 submission_runner.py:542] Using RNG seed 1800903789
I0201 18:30:40.779038 140085747812160 submission_runner.py:551] --- Tuning run 3/5 ---
I0201 18:30:40.779146 140085747812160 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3.
I0201 18:30:40.781286 140085747812160 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3/hparams.json.
I0201 18:30:40.782210 140085747812160 submission_runner.py:206] Initializing dataset.
I0201 18:30:40.791486 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0201 18:30:40.802141 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0201 18:30:40.998695 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0201 18:30:45.325745 140085747812160 submission_runner.py:213] Initializing model.
I0201 18:30:51.823197 140085747812160 submission_runner.py:255] Initializing optimizer.
I0201 18:30:52.307537 140085747812160 submission_runner.py:262] Initializing metrics bundle.
I0201 18:30:52.307693 140085747812160 submission_runner.py:280] Initializing checkpoint and logger.
I0201 18:30:52.322409 140085747812160 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3 with prefix checkpoint_
I0201 18:30:52.322527 140085747812160 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0201 18:31:08.624234 140085747812160 logger_utils.py:220] Unable to record git information. Continuing without it.
I0201 18:31:24.668313 140085747812160 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3/flags_0.json.
I0201 18:31:24.672738 140085747812160 submission_runner.py:314] Starting training loop.
I0201 18:32:00.150362 139923826849536 logging_writer.py:48] [0] global_step=0, grad_norm=0.3663758337497711, loss=6.907756328582764
I0201 18:32:00.164936 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:32:08.397638 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:32:25.444844 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:32:27.084354 140085747812160 submission_runner.py:408] Time since start: 62.41s, 	Step: 1, 	{'train/accuracy': 0.0008398437057621777, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.492053747177124, 'total_duration': 62.411550998687744, 'accumulated_submission_time': 35.492053747177124, 'accumulated_eval_time': 26.919355869293213, 'accumulated_logging_time': 0}
I0201 18:32:27.092869 139923835242240 logging_writer.py:48] [1] accumulated_eval_time=26.919356, accumulated_logging_time=0, accumulated_submission_time=35.492054, global_step=1, preemption_count=0, score=35.492054, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=62.411551, train/accuracy=0.000840, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0201 18:33:32.720630 139923868813056 logging_writer.py:48] [100] global_step=100, grad_norm=0.37267133593559265, loss=6.906713962554932
I0201 18:34:18.846394 139923852027648 logging_writer.py:48] [200] global_step=200, grad_norm=0.45638954639434814, loss=6.891656875610352
I0201 18:35:06.171711 139923868813056 logging_writer.py:48] [300] global_step=300, grad_norm=0.6337916851043701, loss=6.849297523498535
I0201 18:35:53.412003 139923852027648 logging_writer.py:48] [400] global_step=400, grad_norm=0.7851418852806091, loss=6.791940212249756
I0201 18:36:40.446511 139923868813056 logging_writer.py:48] [500] global_step=500, grad_norm=0.6963331699371338, loss=6.762592792510986
I0201 18:37:27.491475 139923852027648 logging_writer.py:48] [600] global_step=600, grad_norm=1.2280362844467163, loss=6.7156596183776855
I0201 18:38:14.487841 139923868813056 logging_writer.py:48] [700] global_step=700, grad_norm=0.8617647886276245, loss=6.795323848724365
I0201 18:39:01.465134 139923852027648 logging_writer.py:48] [800] global_step=800, grad_norm=1.2471343278884888, loss=6.6191792488098145
I0201 18:39:27.403178 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:39:38.396462 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:40:02.297977 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:40:03.940145 140085747812160 submission_runner.py:408] Time since start: 519.27s, 	Step: 857, 	{'train/accuracy': 0.012304686941206455, 'train/loss': 6.458219051361084, 'validation/accuracy': 0.0127399992197752, 'validation/loss': 6.469983100891113, 'validation/num_examples': 50000, 'test/accuracy': 0.009000000543892384, 'test/loss': 6.5088372230529785, 'test/num_examples': 10000, 'score': 455.7455909252167, 'total_duration': 519.2673494815826, 'accumulated_submission_time': 455.7455909252167, 'accumulated_eval_time': 63.456319093704224, 'accumulated_logging_time': 0.017473220825195312}
I0201 18:40:03.960569 139923868813056 logging_writer.py:48] [857] accumulated_eval_time=63.456319, accumulated_logging_time=0.017473, accumulated_submission_time=455.745591, global_step=857, preemption_count=0, score=455.745591, test/accuracy=0.009000, test/loss=6.508837, test/num_examples=10000, total_duration=519.267349, train/accuracy=0.012305, train/loss=6.458219, validation/accuracy=0.012740, validation/loss=6.469983, validation/num_examples=50000
I0201 18:40:21.187375 139923852027648 logging_writer.py:48] [900] global_step=900, grad_norm=3.3303751945495605, loss=6.514043807983398
I0201 18:41:05.874329 139923868813056 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.465820074081421, loss=6.500889301300049
I0201 18:41:53.060474 139923852027648 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.1978654861450195, loss=6.372172832489014
I0201 18:42:40.185580 139923868813056 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.715915322303772, loss=6.344222068786621
I0201 18:43:27.151129 139923852027648 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.2682371139526367, loss=6.365481376647949
I0201 18:44:13.788458 139923868813056 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5309667587280273, loss=6.317447185516357
I0201 18:45:00.485174 139923852027648 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.8203628063201904, loss=6.337348937988281
I0201 18:45:47.322238 139923868813056 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.7355629205703735, loss=6.1556501388549805
I0201 18:46:34.118064 139923852027648 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.5362015962600708, loss=6.511645317077637
I0201 18:47:04.042356 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:47:14.752033 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:47:41.077971 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:47:42.717346 140085747812160 submission_runner.py:408] Time since start: 978.04s, 	Step: 1766, 	{'train/accuracy': 0.036210935562849045, 'train/loss': 5.856694221496582, 'validation/accuracy': 0.034940000623464584, 'validation/loss': 5.884265422821045, 'validation/num_examples': 50000, 'test/accuracy': 0.03189999982714653, 'test/loss': 5.995128154754639, 'test/num_examples': 10000, 'score': 875.765299320221, 'total_duration': 978.044549703598, 'accumulated_submission_time': 875.765299320221, 'accumulated_eval_time': 102.13130497932434, 'accumulated_logging_time': 0.047756195068359375}
I0201 18:47:42.733286 139923868813056 logging_writer.py:48] [1766] accumulated_eval_time=102.131305, accumulated_logging_time=0.047756, accumulated_submission_time=875.765299, global_step=1766, preemption_count=0, score=875.765299, test/accuracy=0.031900, test/loss=5.995128, test/num_examples=10000, total_duration=978.044550, train/accuracy=0.036211, train/loss=5.856694, validation/accuracy=0.034940, validation/loss=5.884265, validation/num_examples=50000
I0201 18:47:56.454608 139923852027648 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.7808159589767456, loss=6.434474945068359
I0201 18:48:40.794206 139923868813056 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.494837999343872, loss=6.6531829833984375
I0201 18:49:27.629991 139923852027648 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.1567938327789307, loss=5.952784061431885
I0201 18:50:14.191103 139923868813056 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.6400920152664185, loss=5.955570220947266
I0201 18:51:00.784577 139923852027648 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.1688034534454346, loss=5.949237823486328
I0201 18:51:47.661505 139923868813056 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.9563422203063965, loss=5.882806777954102
I0201 18:52:34.307301 139923852027648 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.7088563442230225, loss=5.890681743621826
I0201 18:53:21.316260 139923868813056 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.1137049198150635, loss=5.860344886779785
I0201 18:54:07.941903 139923852027648 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.234065294265747, loss=5.80458927154541
I0201 18:54:43.200342 140085747812160 spec.py:321] Evaluating on the training split.
I0201 18:54:53.618267 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 18:55:20.160965 140085747812160 spec.py:349] Evaluating on the test split.
I0201 18:55:21.794659 140085747812160 submission_runner.py:408] Time since start: 1437.12s, 	Step: 2677, 	{'train/accuracy': 0.06332030892372131, 'train/loss': 5.436893939971924, 'validation/accuracy': 0.06027999892830849, 'validation/loss': 5.480632305145264, 'validation/num_examples': 50000, 'test/accuracy': 0.04780000075697899, 'test/loss': 5.662439346313477, 'test/num_examples': 10000, 'score': 1296.1715610027313, 'total_duration': 1437.1218678951263, 'accumulated_submission_time': 1296.1715610027313, 'accumulated_eval_time': 140.7256236076355, 'accumulated_logging_time': 0.07358026504516602}
I0201 18:55:21.810835 139923868813056 logging_writer.py:48] [2677] accumulated_eval_time=140.725624, accumulated_logging_time=0.073580, accumulated_submission_time=1296.171561, global_step=2677, preemption_count=0, score=1296.171561, test/accuracy=0.047800, test/loss=5.662439, test/num_examples=10000, total_duration=1437.121868, train/accuracy=0.063320, train/loss=5.436894, validation/accuracy=0.060280, validation/loss=5.480632, validation/num_examples=50000
I0201 18:55:31.222631 139923852027648 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.4122589826583862, loss=6.414305210113525
I0201 18:56:14.511188 139923868813056 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.224289655685425, loss=5.702967166900635
I0201 18:57:01.334172 139923852027648 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.178828239440918, loss=5.69125509262085
I0201 18:57:48.197527 139923868813056 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.6709916591644287, loss=5.815974712371826
I0201 18:58:34.896070 139923852027648 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.0272858142852783, loss=5.675493240356445
I0201 18:59:21.763732 139923868813056 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.8537448644638062, loss=6.426035404205322
I0201 19:00:08.394719 139923852027648 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.0567541122436523, loss=5.554440975189209
I0201 19:00:54.876207 139923868813056 logging_writer.py:48] [3400] global_step=3400, grad_norm=2.113476037979126, loss=5.6552205085754395
I0201 19:01:41.744144 139923852027648 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.6596628427505493, loss=5.8842339515686035
I0201 19:02:21.811823 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:02:33.457504 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:02:58.929858 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:03:00.567570 140085747812160 submission_runner.py:408] Time since start: 1895.89s, 	Step: 3587, 	{'train/accuracy': 0.09263671934604645, 'train/loss': 5.093984603881836, 'validation/accuracy': 0.08709999918937683, 'validation/loss': 5.130605220794678, 'validation/num_examples': 50000, 'test/accuracy': 0.06840000301599503, 'test/loss': 5.372107982635498, 'test/num_examples': 10000, 'score': 1716.1118450164795, 'total_duration': 1895.8947789669037, 'accumulated_submission_time': 1716.1118450164795, 'accumulated_eval_time': 179.481369972229, 'accumulated_logging_time': 0.09964895248413086}
I0201 19:03:00.584055 139923868813056 logging_writer.py:48] [3587] accumulated_eval_time=179.481370, accumulated_logging_time=0.099649, accumulated_submission_time=1716.111845, global_step=3587, preemption_count=0, score=1716.111845, test/accuracy=0.068400, test/loss=5.372108, test/num_examples=10000, total_duration=1895.894779, train/accuracy=0.092637, train/loss=5.093985, validation/accuracy=0.087100, validation/loss=5.130605, validation/num_examples=50000
I0201 19:03:06.090754 139923852027648 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.647371768951416, loss=6.307277679443359
I0201 19:03:48.815674 139923868813056 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.8785700798034668, loss=5.494929313659668
I0201 19:04:35.637693 139923852027648 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.9505921602249146, loss=5.521139144897461
I0201 19:05:22.636441 139923868813056 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.596766471862793, loss=5.602398872375488
I0201 19:06:09.491678 139923852027648 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.3040363788604736, loss=5.489656925201416
I0201 19:06:56.335280 139923868813056 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.3844995498657227, loss=5.42954683303833
I0201 19:07:42.938007 139923852027648 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.1010971069335938, loss=5.350515365600586
I0201 19:08:29.788398 139923868813056 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.093331813812256, loss=5.31718635559082
I0201 19:09:16.426370 139923852027648 logging_writer.py:48] [4400] global_step=4400, grad_norm=1.4331656694412231, loss=6.345272064208984
I0201 19:10:00.583198 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:10:11.256358 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:10:37.822964 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:10:39.456017 140085747812160 submission_runner.py:408] Time since start: 2354.78s, 	Step: 4496, 	{'train/accuracy': 0.12886717915534973, 'train/loss': 4.745182514190674, 'validation/accuracy': 0.12067999690771103, 'validation/loss': 4.8043293952941895, 'validation/num_examples': 50000, 'test/accuracy': 0.09400000423192978, 'test/loss': 5.104016304016113, 'test/num_examples': 10000, 'score': 2136.0499653816223, 'total_duration': 2354.783221721649, 'accumulated_submission_time': 2136.0499653816223, 'accumulated_eval_time': 218.35417985916138, 'accumulated_logging_time': 0.12586569786071777}
I0201 19:10:39.475760 139923868813056 logging_writer.py:48] [4496] accumulated_eval_time=218.354180, accumulated_logging_time=0.125866, accumulated_submission_time=2136.049965, global_step=4496, preemption_count=0, score=2136.049965, test/accuracy=0.094000, test/loss=5.104016, test/num_examples=10000, total_duration=2354.783222, train/accuracy=0.128867, train/loss=4.745183, validation/accuracy=0.120680, validation/loss=4.804329, validation/num_examples=50000
I0201 19:10:41.438903 139923852027648 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.11789870262146, loss=5.281281471252441
I0201 19:11:23.387268 139923868813056 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.3009986877441406, loss=5.2433342933654785
I0201 19:12:09.985400 139923852027648 logging_writer.py:48] [4700] global_step=4700, grad_norm=1.7357866764068604, loss=5.213918685913086
I0201 19:12:57.158203 139923868813056 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.5750877857208252, loss=6.38057804107666
I0201 19:13:43.965501 139923852027648 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.9872174263000488, loss=5.12479305267334
I0201 19:14:30.808840 139923868813056 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.303884744644165, loss=5.372860908508301
I0201 19:15:17.386808 139923852027648 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.7576816082000732, loss=5.155461311340332
I0201 19:16:04.154669 139923868813056 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.11104679107666, loss=5.058659076690674
I0201 19:16:51.064670 139923852027648 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.6145926713943481, loss=6.252840042114258
I0201 19:17:37.959335 139923868813056 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.0652875900268555, loss=6.422878265380859
I0201 19:17:39.473101 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:17:49.980445 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:18:17.230240 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:18:18.867284 140085747812160 submission_runner.py:408] Time since start: 2814.19s, 	Step: 5405, 	{'train/accuracy': 0.16783203184604645, 'train/loss': 4.406538486480713, 'validation/accuracy': 0.15267999470233917, 'validation/loss': 4.4869513511657715, 'validation/num_examples': 50000, 'test/accuracy': 0.11730000376701355, 'test/loss': 4.828131675720215, 'test/num_examples': 10000, 'score': 2555.9860396385193, 'total_duration': 2814.194491147995, 'accumulated_submission_time': 2555.9860396385193, 'accumulated_eval_time': 257.7483706474304, 'accumulated_logging_time': 0.15589284896850586}
I0201 19:18:18.884010 139923852027648 logging_writer.py:48] [5405] accumulated_eval_time=257.748371, accumulated_logging_time=0.155893, accumulated_submission_time=2555.986040, global_step=5405, preemption_count=0, score=2555.986040, test/accuracy=0.117300, test/loss=4.828132, test/num_examples=10000, total_duration=2814.194491, train/accuracy=0.167832, train/loss=4.406538, validation/accuracy=0.152680, validation/loss=4.486951, validation/num_examples=50000
I0201 19:18:58.787325 139923868813056 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.8964495658874512, loss=5.512840747833252
I0201 19:19:45.414316 139923852027648 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.8536466360092163, loss=4.936145782470703
I0201 19:20:32.196651 139923868813056 logging_writer.py:48] [5700] global_step=5700, grad_norm=1.957190752029419, loss=5.334084510803223
I0201 19:21:19.061387 139923852027648 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.5802017450332642, loss=5.648900032043457
I0201 19:22:05.703495 139923868813056 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.9654427766799927, loss=4.969081401824951
I0201 19:22:52.625749 139923852027648 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.7130887508392334, loss=5.3793416023254395
I0201 19:23:39.534963 139923868813056 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.1198341846466064, loss=4.774157524108887
I0201 19:24:26.429815 139923852027648 logging_writer.py:48] [6200] global_step=6200, grad_norm=1.768576741218567, loss=5.416329383850098
I0201 19:25:13.506842 139923868813056 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.9791529178619385, loss=4.745744705200195
I0201 19:25:19.045035 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:25:29.522502 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:25:51.224691 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:25:52.872128 140085747812160 submission_runner.py:408] Time since start: 3268.20s, 	Step: 6314, 	{'train/accuracy': 0.2112695276737213, 'train/loss': 4.0574235916137695, 'validation/accuracy': 0.19563999772071838, 'validation/loss': 4.138259410858154, 'validation/num_examples': 50000, 'test/accuracy': 0.14390000700950623, 'test/loss': 4.525965213775635, 'test/num_examples': 10000, 'score': 2976.0876848697662, 'total_duration': 3268.1993370056152, 'accumulated_submission_time': 2976.0876848697662, 'accumulated_eval_time': 291.5754690170288, 'accumulated_logging_time': 0.18158912658691406}
I0201 19:25:52.888134 139923852027648 logging_writer.py:48] [6314] accumulated_eval_time=291.575469, accumulated_logging_time=0.181589, accumulated_submission_time=2976.087685, global_step=6314, preemption_count=0, score=2976.087685, test/accuracy=0.143900, test/loss=4.525965, test/num_examples=10000, total_duration=3268.199337, train/accuracy=0.211270, train/loss=4.057424, validation/accuracy=0.195640, validation/loss=4.138259, validation/num_examples=50000
I0201 19:26:28.252196 139923868813056 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.9061272144317627, loss=4.942928314208984
I0201 19:27:14.719851 139923852027648 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.2208361625671387, loss=5.07149600982666
I0201 19:28:01.675566 139923868813056 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.070997953414917, loss=5.310601234436035
I0201 19:28:48.269562 139923852027648 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.1671881675720215, loss=6.272613525390625
I0201 19:29:34.918010 139923868813056 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.1357197761535645, loss=5.028833389282227
I0201 19:30:21.596353 139923852027648 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.0398101806640625, loss=4.533389091491699
I0201 19:31:07.876553 139923868813056 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.5711331367492676, loss=6.317354679107666
I0201 19:31:54.467527 139923852027648 logging_writer.py:48] [7100] global_step=7100, grad_norm=1.664380669593811, loss=6.217877388000488
I0201 19:32:41.380282 139923868813056 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.7987877130508423, loss=6.178681373596191
I0201 19:32:53.143876 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:33:03.714169 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:33:29.654433 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:33:31.298398 140085747812160 submission_runner.py:408] Time since start: 3726.63s, 	Step: 7227, 	{'train/accuracy': 0.2490820288658142, 'train/loss': 3.757694959640503, 'validation/accuracy': 0.23157998919487, 'validation/loss': 3.856245756149292, 'validation/num_examples': 50000, 'test/accuracy': 0.1714000105857849, 'test/loss': 4.288461685180664, 'test/num_examples': 10000, 'score': 3396.280373096466, 'total_duration': 3726.625602722168, 'accumulated_submission_time': 3396.280373096466, 'accumulated_eval_time': 329.72999024391174, 'accumulated_logging_time': 0.2092578411102295}
I0201 19:33:31.314177 139923852027648 logging_writer.py:48] [7227] accumulated_eval_time=329.729990, accumulated_logging_time=0.209258, accumulated_submission_time=3396.280373, global_step=7227, preemption_count=0, score=3396.280373, test/accuracy=0.171400, test/loss=4.288462, test/num_examples=10000, total_duration=3726.625603, train/accuracy=0.249082, train/loss=3.757695, validation/accuracy=0.231580, validation/loss=3.856246, validation/num_examples=50000
I0201 19:34:00.786388 139923868813056 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.4165337085723877, loss=6.098141193389893
I0201 19:34:47.434623 139923852027648 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.7240698337554932, loss=4.348634719848633
I0201 19:35:34.481841 139923868813056 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.6236766576766968, loss=5.232666969299316
I0201 19:36:20.941088 139923852027648 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.961992621421814, loss=4.4498186111450195
I0201 19:37:07.591991 139923868813056 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.8880677223205566, loss=4.437030792236328
I0201 19:37:54.363750 139923852027648 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.125692367553711, loss=4.452157497406006
I0201 19:38:41.221222 139923868813056 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.5328115224838257, loss=5.4159932136535645
I0201 19:39:27.873535 139923852027648 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.151270627975464, loss=4.260217666625977
I0201 19:40:14.479945 139923868813056 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.8038691282272339, loss=4.2859697341918945
I0201 19:40:31.591907 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:40:42.380648 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:41:06.678991 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:41:08.314481 140085747812160 submission_runner.py:408] Time since start: 4183.64s, 	Step: 8138, 	{'train/accuracy': 0.2967578172683716, 'train/loss': 3.435223340988159, 'validation/accuracy': 0.2703399956226349, 'validation/loss': 3.561870574951172, 'validation/num_examples': 50000, 'test/accuracy': 0.20630000531673431, 'test/loss': 4.033924579620361, 'test/num_examples': 10000, 'score': 3816.497978210449, 'total_duration': 4183.641686677933, 'accumulated_submission_time': 3816.497978210449, 'accumulated_eval_time': 366.4525589942932, 'accumulated_logging_time': 0.23423242568969727}
I0201 19:41:08.330910 139923852027648 logging_writer.py:48] [8138] accumulated_eval_time=366.452559, accumulated_logging_time=0.234232, accumulated_submission_time=3816.497978, global_step=8138, preemption_count=0, score=3816.497978, test/accuracy=0.206300, test/loss=4.033925, test/num_examples=10000, total_duration=4183.641687, train/accuracy=0.296758, train/loss=3.435223, validation/accuracy=0.270340, validation/loss=3.561871, validation/num_examples=50000
I0201 19:41:33.049618 139923868813056 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.4337878227233887, loss=6.110095024108887
I0201 19:42:19.378844 139923852027648 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8845946788787842, loss=4.4034810066223145
I0201 19:43:06.542835 139923868813056 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.3766098022460938, loss=4.206100940704346
I0201 19:43:53.166334 139923852027648 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.9703010320663452, loss=4.1341023445129395
I0201 19:44:39.966359 139923868813056 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.1805224418640137, loss=4.406674861907959
I0201 19:45:26.672864 139923852027648 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.9098258018493652, loss=4.79904842376709
I0201 19:46:13.600367 139923868813056 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.0577409267425537, loss=4.108538627624512
I0201 19:47:00.219561 139923852027648 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.5480711460113525, loss=5.46108341217041
I0201 19:47:46.762657 139923868813056 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.8860820531845093, loss=4.191699504852295
I0201 19:48:08.411360 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:48:19.124942 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:48:45.842046 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:48:47.486172 140085747812160 submission_runner.py:408] Time since start: 4642.81s, 	Step: 9048, 	{'train/accuracy': 0.34144529700279236, 'train/loss': 3.161966562271118, 'validation/accuracy': 0.30653998255729675, 'validation/loss': 3.330437660217285, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.8473939895629883, 'test/num_examples': 10000, 'score': 4236.516438961029, 'total_duration': 4642.813379764557, 'accumulated_submission_time': 4236.516438961029, 'accumulated_eval_time': 405.5273621082306, 'accumulated_logging_time': 0.2614433765411377}
I0201 19:48:47.503004 139923852027648 logging_writer.py:48] [9048] accumulated_eval_time=405.527362, accumulated_logging_time=0.261443, accumulated_submission_time=4236.516439, global_step=9048, preemption_count=0, score=4236.516439, test/accuracy=0.229700, test/loss=3.847394, test/num_examples=10000, total_duration=4642.813380, train/accuracy=0.341445, train/loss=3.161967, validation/accuracy=0.306540, validation/loss=3.330438, validation/num_examples=50000
I0201 19:49:08.273740 139923868813056 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.3707361221313477, loss=6.037209510803223
I0201 19:49:53.859364 139923852027648 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.244832992553711, loss=5.987325668334961
I0201 19:50:40.942084 139923868813056 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.4534646272659302, loss=5.748018741607666
I0201 19:51:27.655550 139923852027648 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.1946942806243896, loss=6.052431106567383
I0201 19:52:14.384088 139923868813056 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3053264617919922, loss=6.051271438598633
I0201 19:53:01.121212 139923852027648 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8705636262893677, loss=4.186032772064209
I0201 19:53:47.858776 139923868813056 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.1139590740203857, loss=4.100350379943848
I0201 19:54:34.616112 139923852027648 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.8161824941635132, loss=4.056280136108398
I0201 19:55:21.406030 139923868813056 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.0596938133239746, loss=3.9637789726257324
I0201 19:55:47.750261 140085747812160 spec.py:321] Evaluating on the training split.
I0201 19:55:58.187988 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 19:56:22.063496 140085747812160 spec.py:349] Evaluating on the test split.
I0201 19:56:23.714126 140085747812160 submission_runner.py:408] Time since start: 5099.04s, 	Step: 9958, 	{'train/accuracy': 0.3555664122104645, 'train/loss': 3.0396194458007812, 'validation/accuracy': 0.33122000098228455, 'validation/loss': 3.1730129718780518, 'validation/num_examples': 50000, 'test/accuracy': 0.25140002369880676, 'test/loss': 3.7302799224853516, 'test/num_examples': 10000, 'score': 4656.703306913376, 'total_duration': 5099.041333913803, 'accumulated_submission_time': 4656.703306913376, 'accumulated_eval_time': 441.491233587265, 'accumulated_logging_time': 0.2879626750946045}
I0201 19:56:23.735608 139923852027648 logging_writer.py:48] [9958] accumulated_eval_time=441.491234, accumulated_logging_time=0.287963, accumulated_submission_time=4656.703307, global_step=9958, preemption_count=0, score=4656.703307, test/accuracy=0.251400, test/loss=3.730280, test/num_examples=10000, total_duration=5099.041334, train/accuracy=0.355566, train/loss=3.039619, validation/accuracy=0.331220, validation/loss=3.173013, validation/num_examples=50000
I0201 19:56:40.612396 139923868813056 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.7953275442123413, loss=4.016690254211426
I0201 19:57:25.032459 139923852027648 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.341611862182617, loss=4.9929375648498535
I0201 19:58:11.741955 139923868813056 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.4188648462295532, loss=5.42069149017334
I0201 19:58:58.656844 139923852027648 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.8505688905715942, loss=3.936671018600464
I0201 19:59:45.436312 139923868813056 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.9195908308029175, loss=4.250163555145264
I0201 20:00:31.957340 139923852027648 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.686475157737732, loss=3.8272151947021484
I0201 20:01:18.695551 139923868813056 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.83362877368927, loss=4.271946430206299
I0201 20:02:05.283677 139923852027648 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.4950339794158936, loss=4.568526268005371
I0201 20:02:52.075481 139923868813056 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.2626909017562866, loss=5.971924304962158
I0201 20:03:24.056147 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:03:34.373553 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:03:58.081923 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:03:59.725687 140085747812160 submission_runner.py:408] Time since start: 5555.05s, 	Step: 10870, 	{'train/accuracy': 0.3751562535762787, 'train/loss': 2.954991340637207, 'validation/accuracy': 0.34797999262809753, 'validation/loss': 3.092005491256714, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.626384973526001, 'test/num_examples': 10000, 'score': 5076.962865829468, 'total_duration': 5555.052897453308, 'accumulated_submission_time': 5076.962865829468, 'accumulated_eval_time': 477.1607825756073, 'accumulated_logging_time': 0.3193974494934082}
I0201 20:03:59.745464 139923852027648 logging_writer.py:48] [10870] accumulated_eval_time=477.160783, accumulated_logging_time=0.319397, accumulated_submission_time=5076.962866, global_step=10870, preemption_count=0, score=5076.962866, test/accuracy=0.272500, test/loss=3.626385, test/num_examples=10000, total_duration=5555.052897, train/accuracy=0.375156, train/loss=2.954991, validation/accuracy=0.347980, validation/loss=3.092005, validation/num_examples=50000
I0201 20:04:11.909213 139923868813056 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.8446767330169678, loss=3.920393705368042
I0201 20:04:55.560848 139923852027648 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.7880054712295532, loss=4.214111328125
I0201 20:05:42.371714 139923868813056 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.75901460647583, loss=3.8203330039978027
I0201 20:06:29.119354 139923852027648 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.3012771606445312, loss=5.735969543457031
I0201 20:07:15.764114 139923868813056 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6812916994094849, loss=4.053106307983398
I0201 20:08:02.329260 139923852027648 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.3513033390045166, loss=5.698794364929199
I0201 20:08:48.953310 139923868813056 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.9252980947494507, loss=3.6680305004119873
I0201 20:09:36.085991 139923852027648 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.9258397817611694, loss=3.7257137298583984
I0201 20:10:22.621454 139923868813056 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.8729277849197388, loss=3.7735023498535156
I0201 20:11:00.184131 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:11:10.729519 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:11:37.431813 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:11:39.076051 140085747812160 submission_runner.py:408] Time since start: 6014.40s, 	Step: 11782, 	{'train/accuracy': 0.4147265553474426, 'train/loss': 2.7238945960998535, 'validation/accuracy': 0.37109997868537903, 'validation/loss': 2.9334120750427246, 'validation/num_examples': 50000, 'test/accuracy': 0.28710001707077026, 'test/loss': 3.4888088703155518, 'test/num_examples': 10000, 'score': 5497.338998794556, 'total_duration': 6014.403256416321, 'accumulated_submission_time': 5497.338998794556, 'accumulated_eval_time': 516.0527064800262, 'accumulated_logging_time': 0.3488492965698242}
I0201 20:11:39.093784 139923852027648 logging_writer.py:48] [11782] accumulated_eval_time=516.052706, accumulated_logging_time=0.348849, accumulated_submission_time=5497.338999, global_step=11782, preemption_count=0, score=5497.338999, test/accuracy=0.287100, test/loss=3.488809, test/num_examples=10000, total_duration=6014.403256, train/accuracy=0.414727, train/loss=2.723895, validation/accuracy=0.371100, validation/loss=2.933412, validation/num_examples=50000
I0201 20:11:46.555249 139923868813056 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.8882430791854858, loss=3.852935314178467
I0201 20:12:29.648894 139923852027648 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.9244883060455322, loss=3.629185199737549
I0201 20:13:16.696398 139923868813056 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6739805936813354, loss=3.6318764686584473
I0201 20:14:03.643979 139923852027648 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.042111396789551, loss=3.649249315261841
I0201 20:14:50.205418 139923868813056 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.3770886659622192, loss=5.74247407913208
I0201 20:15:37.276086 139923852027648 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.2895457744598389, loss=5.059751987457275
I0201 20:16:24.177654 139923868813056 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2089399099349976, loss=5.804120063781738
I0201 20:17:11.063414 139923852027648 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.1315410137176514, loss=3.6936376094818115
I0201 20:17:57.586886 139923868813056 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.319705843925476, loss=5.849680423736572
I0201 20:18:39.576958 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:18:50.382255 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:19:13.861509 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:19:15.497481 140085747812160 submission_runner.py:408] Time since start: 6470.82s, 	Step: 12691, 	{'train/accuracy': 0.4253320097923279, 'train/loss': 2.623037099838257, 'validation/accuracy': 0.3962000012397766, 'validation/loss': 2.782031536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.3758649826049805, 'test/num_examples': 10000, 'score': 5917.761694431305, 'total_duration': 6470.8246874809265, 'accumulated_submission_time': 5917.761694431305, 'accumulated_eval_time': 551.9732129573822, 'accumulated_logging_time': 0.3765082359313965}
I0201 20:19:15.516734 139923852027648 logging_writer.py:48] [12691] accumulated_eval_time=551.973213, accumulated_logging_time=0.376508, accumulated_submission_time=5917.761694, global_step=12691, preemption_count=0, score=5917.761694, test/accuracy=0.303200, test/loss=3.375865, test/num_examples=10000, total_duration=6470.824687, train/accuracy=0.425332, train/loss=2.623037, validation/accuracy=0.396200, validation/loss=2.782032, validation/num_examples=50000
I0201 20:19:19.444214 139923868813056 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.026200532913208, loss=3.746053457260132
I0201 20:20:01.606361 139923852027648 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.7467446327209473, loss=3.6864850521087646
I0201 20:20:48.166150 139923868813056 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.8301734924316406, loss=3.5035934448242188
I0201 20:21:34.914481 139923852027648 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9955838918685913, loss=5.869458198547363
I0201 20:22:21.851363 139923868813056 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8121317625045776, loss=3.865427017211914
I0201 20:23:08.768199 139923852027648 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.6957920789718628, loss=3.5592689514160156
I0201 20:23:55.287963 139923868813056 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.760607123374939, loss=3.548191547393799
I0201 20:24:41.889057 139923852027648 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.168585777282715, loss=3.8558900356292725
I0201 20:25:28.611505 139923868813056 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.2333320379257202, loss=4.889925003051758
I0201 20:26:15.496344 139923852027648 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.6691728830337524, loss=3.539555788040161
I0201 20:26:15.511222 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:26:25.916153 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:26:49.321371 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:26:50.974095 140085747812160 submission_runner.py:408] Time since start: 6926.30s, 	Step: 13601, 	{'train/accuracy': 0.43861326575279236, 'train/loss': 2.5734517574310303, 'validation/accuracy': 0.40821999311447144, 'validation/loss': 2.7287282943725586, 'validation/num_examples': 50000, 'test/accuracy': 0.3160000145435333, 'test/loss': 3.3127293586730957, 'test/num_examples': 10000, 'score': 6337.694573402405, 'total_duration': 6926.301305532455, 'accumulated_submission_time': 6337.694573402405, 'accumulated_eval_time': 587.4360723495483, 'accumulated_logging_time': 0.40647292137145996}
I0201 20:26:50.991820 139923868813056 logging_writer.py:48] [13601] accumulated_eval_time=587.436072, accumulated_logging_time=0.406473, accumulated_submission_time=6337.694573, global_step=13601, preemption_count=0, score=6337.694573, test/accuracy=0.316000, test/loss=3.312729, test/num_examples=10000, total_duration=6926.301306, train/accuracy=0.438613, train/loss=2.573452, validation/accuracy=0.408220, validation/loss=2.728728, validation/num_examples=50000
I0201 20:27:32.455559 139923852027648 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.709163784980774, loss=3.552647829055786
I0201 20:28:19.222961 139923868813056 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.9304027557373047, loss=3.525005578994751
I0201 20:29:06.144696 139923852027648 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9637194871902466, loss=4.530271053314209
I0201 20:29:52.669667 139923868813056 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.50095534324646, loss=3.922642946243286
I0201 20:30:39.548614 139923852027648 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.8027784824371338, loss=3.5461537837982178
I0201 20:31:26.068918 139923868813056 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.6978422403335571, loss=3.390644073486328
I0201 20:32:12.842895 139923852027648 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.933091640472412, loss=3.638117790222168
I0201 20:32:59.742800 139923868813056 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0512421131134033, loss=5.369678497314453
I0201 20:33:46.462851 139923852027648 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.9089444875717163, loss=3.4543850421905518
I0201 20:33:51.230769 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:34:01.734284 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:34:24.995389 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:34:26.653001 140085747812160 submission_runner.py:408] Time since start: 7381.98s, 	Step: 14512, 	{'train/accuracy': 0.46240234375, 'train/loss': 2.4503259658813477, 'validation/accuracy': 0.4207199811935425, 'validation/loss': 2.656090021133423, 'validation/num_examples': 50000, 'test/accuracy': 0.3279000222682953, 'test/loss': 3.24593186378479, 'test/num_examples': 10000, 'score': 6757.871794700623, 'total_duration': 7381.980210542679, 'accumulated_submission_time': 6757.871794700623, 'accumulated_eval_time': 622.858304977417, 'accumulated_logging_time': 0.43531012535095215}
I0201 20:34:26.671657 139923868813056 logging_writer.py:48] [14512] accumulated_eval_time=622.858305, accumulated_logging_time=0.435310, accumulated_submission_time=6757.871795, global_step=14512, preemption_count=0, score=6757.871795, test/accuracy=0.327900, test/loss=3.245932, test/num_examples=10000, total_duration=7381.980211, train/accuracy=0.462402, train/loss=2.450326, validation/accuracy=0.420720, validation/loss=2.656090, validation/num_examples=50000
I0201 20:35:03.094336 139923852027648 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.751212239265442, loss=3.45047664642334
I0201 20:35:50.366417 139923868813056 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.1757699251174927, loss=5.554591178894043
I0201 20:36:37.307535 139923852027648 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.6316845417022705, loss=3.375704050064087
I0201 20:37:24.029035 139923868813056 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0511746406555176, loss=5.462294578552246
I0201 20:38:10.842097 139923852027648 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.3362352848052979, loss=5.804858207702637
I0201 20:38:57.326849 139923868813056 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.539907455444336, loss=3.407262086868286
I0201 20:39:44.110928 139923852027648 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.6626356840133667, loss=3.3190383911132812
I0201 20:40:30.608122 139923868813056 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.7118501663208008, loss=3.685917615890503
I0201 20:41:17.273125 139923852027648 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8511894941329956, loss=3.4906222820281982
I0201 20:41:26.700904 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:41:37.156400 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:42:03.092736 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:42:04.737462 140085747812160 submission_runner.py:408] Time since start: 7840.06s, 	Step: 15422, 	{'train/accuracy': 0.45710936188697815, 'train/loss': 2.456397771835327, 'validation/accuracy': 0.4252599775791168, 'validation/loss': 2.6171255111694336, 'validation/num_examples': 50000, 'test/accuracy': 0.32710000872612, 'test/loss': 3.216073989868164, 'test/num_examples': 10000, 'score': 7177.840661525726, 'total_duration': 7840.064656257629, 'accumulated_submission_time': 7177.840661525726, 'accumulated_eval_time': 660.8948771953583, 'accumulated_logging_time': 0.4631009101867676}
I0201 20:42:04.756662 139923868813056 logging_writer.py:48] [15422] accumulated_eval_time=660.894877, accumulated_logging_time=0.463101, accumulated_submission_time=7177.840662, global_step=15422, preemption_count=0, score=7177.840662, test/accuracy=0.327100, test/loss=3.216074, test/num_examples=10000, total_duration=7840.064656, train/accuracy=0.457109, train/loss=2.456398, validation/accuracy=0.425260, validation/loss=2.617126, validation/num_examples=50000
I0201 20:42:36.774569 139923852027648 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5392892360687256, loss=3.8604650497436523
I0201 20:43:23.276513 139923868813056 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.450053095817566, loss=3.3466365337371826
I0201 20:44:10.054323 139923852027648 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.6157859563827515, loss=3.7460849285125732
I0201 20:44:56.492272 139923868813056 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.1659438610076904, loss=4.971489429473877
I0201 20:45:43.059661 139923852027648 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5997196435928345, loss=3.2981019020080566
I0201 20:46:29.754316 139923868813056 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.6251102685928345, loss=3.3101723194122314
I0201 20:47:16.581320 139923852027648 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.664618968963623, loss=3.4168872833251953
I0201 20:48:03.326918 139923868813056 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.351158618927002, loss=4.016932964324951
I0201 20:48:50.156921 139923852027648 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.001089572906494, loss=3.471198558807373
I0201 20:49:04.746222 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:49:15.439234 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:49:42.535655 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:49:44.173156 140085747812160 submission_runner.py:408] Time since start: 8299.50s, 	Step: 16333, 	{'train/accuracy': 0.4747656285762787, 'train/loss': 2.3626480102539062, 'validation/accuracy': 0.43915998935699463, 'validation/loss': 2.550366163253784, 'validation/num_examples': 50000, 'test/accuracy': 0.34140002727508545, 'test/loss': 3.161466598510742, 'test/num_examples': 10000, 'score': 7597.769626379013, 'total_duration': 8299.500366926193, 'accumulated_submission_time': 7597.769626379013, 'accumulated_eval_time': 700.3218250274658, 'accumulated_logging_time': 0.4916057586669922}
I0201 20:49:44.191326 139923868813056 logging_writer.py:48] [16333] accumulated_eval_time=700.321825, accumulated_logging_time=0.491606, accumulated_submission_time=7597.769626, global_step=16333, preemption_count=0, score=7597.769626, test/accuracy=0.341400, test/loss=3.161467, test/num_examples=10000, total_duration=8299.500367, train/accuracy=0.474766, train/loss=2.362648, validation/accuracy=0.439160, validation/loss=2.550366, validation/num_examples=50000
I0201 20:50:10.956971 139923852027648 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.7841230630874634, loss=3.6544337272644043
I0201 20:50:57.736421 139923868813056 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4934229850769043, loss=3.8453142642974854
I0201 20:51:44.966924 139923852027648 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6501154899597168, loss=3.3086323738098145
I0201 20:52:31.939566 139923868813056 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.7545756101608276, loss=3.849477767944336
I0201 20:53:18.925923 139923852027648 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.6515390872955322, loss=4.050985813140869
I0201 20:54:05.742149 139923868813056 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.7877553701400757, loss=3.419888734817505
I0201 20:54:52.544415 139923852027648 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.5767501592636108, loss=4.3109564781188965
I0201 20:55:39.365921 139923868813056 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.4876459836959839, loss=3.269268751144409
I0201 20:56:26.294671 139923852027648 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.8691221475601196, loss=3.3457117080688477
I0201 20:56:44.593934 140085747812160 spec.py:321] Evaluating on the training split.
I0201 20:56:55.245499 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 20:57:21.903912 140085747812160 spec.py:349] Evaluating on the test split.
I0201 20:57:23.552015 140085747812160 submission_runner.py:408] Time since start: 8758.88s, 	Step: 17241, 	{'train/accuracy': 0.4889843761920929, 'train/loss': 2.295726776123047, 'validation/accuracy': 0.4437599778175354, 'validation/loss': 2.507998466491699, 'validation/num_examples': 50000, 'test/accuracy': 0.344400018453598, 'test/loss': 3.1372451782226562, 'test/num_examples': 10000, 'score': 8018.110145807266, 'total_duration': 8758.87922167778, 'accumulated_submission_time': 8018.110145807266, 'accumulated_eval_time': 739.2799081802368, 'accumulated_logging_time': 0.520301103591919}
I0201 20:57:23.571139 139923868813056 logging_writer.py:48] [17241] accumulated_eval_time=739.279908, accumulated_logging_time=0.520301, accumulated_submission_time=8018.110146, global_step=17241, preemption_count=0, score=8018.110146, test/accuracy=0.344400, test/loss=3.137245, test/num_examples=10000, total_duration=8758.879222, train/accuracy=0.488984, train/loss=2.295727, validation/accuracy=0.443760, validation/loss=2.507998, validation/num_examples=50000
I0201 20:57:47.091187 139923852027648 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.6062545776367188, loss=3.3299918174743652
I0201 20:58:33.320111 139923868813056 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.5371040105819702, loss=3.3367714881896973
I0201 20:59:20.422469 139923852027648 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2577612400054932, loss=4.399012088775635
I0201 21:00:06.755712 139923868813056 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.1495649814605713, loss=5.139002323150635
I0201 21:00:53.517546 139923852027648 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8468401432037354, loss=3.386227607727051
I0201 21:01:40.404550 139923868813056 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.525971531867981, loss=3.3698582649230957
I0201 21:02:26.991043 139923852027648 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.1436762809753418, loss=5.761127948760986
I0201 21:03:13.995779 139923868813056 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.5221309661865234, loss=3.494696855545044
I0201 21:04:00.674077 139923852027648 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.6804856061935425, loss=3.34287691116333
I0201 21:04:23.960020 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:04:34.407819 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:05:04.066329 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:05:05.704685 140085747812160 submission_runner.py:408] Time since start: 9221.03s, 	Step: 18151, 	{'train/accuracy': 0.48261716961860657, 'train/loss': 2.3039026260375977, 'validation/accuracy': 0.44777998328208923, 'validation/loss': 2.470780372619629, 'validation/num_examples': 50000, 'test/accuracy': 0.35030001401901245, 'test/loss': 3.0864689350128174, 'test/num_examples': 10000, 'score': 8438.437040090561, 'total_duration': 9221.031891822815, 'accumulated_submission_time': 8438.437040090561, 'accumulated_eval_time': 781.0245745182037, 'accumulated_logging_time': 0.5498020648956299}
I0201 21:05:05.724210 139923868813056 logging_writer.py:48] [18151] accumulated_eval_time=781.024575, accumulated_logging_time=0.549802, accumulated_submission_time=8438.437040, global_step=18151, preemption_count=0, score=8438.437040, test/accuracy=0.350300, test/loss=3.086469, test/num_examples=10000, total_duration=9221.031892, train/accuracy=0.482617, train/loss=2.303903, validation/accuracy=0.447780, validation/loss=2.470780, validation/num_examples=50000
I0201 21:05:25.338857 139923852027648 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.6317384243011475, loss=3.226335287094116
I0201 21:06:10.615401 139923868813056 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5395005941390991, loss=3.480889081954956
I0201 21:06:57.595364 139923852027648 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.9711389541625977, loss=3.153538942337036
I0201 21:07:44.417424 139923868813056 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.501960039138794, loss=3.5158164501190186
I0201 21:08:31.166897 139923852027648 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.11931312084198, loss=4.316284656524658
I0201 21:09:17.991865 139923868813056 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3531222343444824, loss=5.68778657913208
I0201 21:10:04.946481 139923852027648 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7546480894088745, loss=3.2173118591308594
I0201 21:10:51.494798 139923868813056 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4256541728973389, loss=3.6583311557769775
I0201 21:11:38.185571 139923852027648 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4493533372879028, loss=3.2391581535339355
I0201 21:12:06.178140 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:12:16.713590 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:12:45.190523 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:12:46.831546 140085747812160 submission_runner.py:408] Time since start: 9682.16s, 	Step: 19061, 	{'train/accuracy': 0.49947264790534973, 'train/loss': 2.22053599357605, 'validation/accuracy': 0.462039977312088, 'validation/loss': 2.4049911499023438, 'validation/num_examples': 50000, 'test/accuracy': 0.35850000381469727, 'test/loss': 3.022629976272583, 'test/num_examples': 10000, 'score': 8858.829084157944, 'total_duration': 9682.15875673294, 'accumulated_submission_time': 8858.829084157944, 'accumulated_eval_time': 821.6779868602753, 'accumulated_logging_time': 0.579658031463623}
I0201 21:12:46.850861 139923868813056 logging_writer.py:48] [19061] accumulated_eval_time=821.677987, accumulated_logging_time=0.579658, accumulated_submission_time=8858.829084, global_step=19061, preemption_count=0, score=8858.829084, test/accuracy=0.358500, test/loss=3.022630, test/num_examples=10000, total_duration=9682.158757, train/accuracy=0.499473, train/loss=2.220536, validation/accuracy=0.462040, validation/loss=2.404991, validation/num_examples=50000
I0201 21:13:02.521298 139923852027648 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.8067792654037476, loss=3.2599496841430664
I0201 21:13:47.074132 139923868813056 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.4496941566467285, loss=4.181397438049316
I0201 21:14:33.933323 139923852027648 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6424189805984497, loss=3.6322433948516846
I0201 21:15:20.558842 139923868813056 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.4763405323028564, loss=3.5802857875823975
I0201 21:16:07.381636 139923852027648 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6251115798950195, loss=3.4132018089294434
I0201 21:16:54.240177 139923868813056 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.7935975790023804, loss=3.296157121658325
I0201 21:17:40.953059 139923852027648 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.3560510873794556, loss=4.118657112121582
I0201 21:18:28.135060 139923868813056 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.722928762435913, loss=3.245439052581787
I0201 21:19:14.779641 139923852027648 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.2242419719696045, loss=3.199826240539551
I0201 21:19:46.905855 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:19:57.535090 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:20:23.769358 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:20:25.415908 140085747812160 submission_runner.py:408] Time since start: 10140.74s, 	Step: 19970, 	{'train/accuracy': 0.5083202719688416, 'train/loss': 2.1640000343322754, 'validation/accuracy': 0.4681599736213684, 'validation/loss': 2.3670108318328857, 'validation/num_examples': 50000, 'test/accuracy': 0.35830003023147583, 'test/loss': 3.0018560886383057, 'test/num_examples': 10000, 'score': 9278.822909593582, 'total_duration': 10140.743117809296, 'accumulated_submission_time': 9278.822909593582, 'accumulated_eval_time': 860.1880419254303, 'accumulated_logging_time': 0.6096100807189941}
I0201 21:20:25.435655 139923868813056 logging_writer.py:48] [19970] accumulated_eval_time=860.188042, accumulated_logging_time=0.609610, accumulated_submission_time=9278.822910, global_step=19970, preemption_count=0, score=9278.822910, test/accuracy=0.358300, test/loss=3.001856, test/num_examples=10000, total_duration=10140.743118, train/accuracy=0.508320, train/loss=2.164000, validation/accuracy=0.468160, validation/loss=2.367011, validation/num_examples=50000
I0201 21:20:37.824590 139923852027648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9684225916862488, loss=5.3563642501831055
I0201 21:21:21.554537 139923868813056 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6041733026504517, loss=3.304931402206421
I0201 21:22:07.956208 139923852027648 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.2313265800476074, loss=4.89251184463501
I0201 21:22:54.943342 139923868813056 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.4479297399520874, loss=3.251741409301758
I0201 21:23:41.613066 139923852027648 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.4501532316207886, loss=3.356444835662842
I0201 21:24:28.344480 139923868813056 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.6847068071365356, loss=3.1368417739868164
I0201 21:25:15.057440 139923852027648 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.4809246063232422, loss=3.461333751678467
I0201 21:26:01.651864 139923868813056 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.0864611864089966, loss=5.55483341217041
I0201 21:26:48.205606 139923852027648 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.8501077890396118, loss=5.49445915222168
I0201 21:27:25.752454 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:27:36.245754 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:27:59.436604 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:28:01.116943 140085747812160 submission_runner.py:408] Time since start: 10596.44s, 	Step: 20882, 	{'train/accuracy': 0.5099218487739563, 'train/loss': 2.1630897521972656, 'validation/accuracy': 0.4744599759578705, 'validation/loss': 2.3403162956237793, 'validation/num_examples': 50000, 'test/accuracy': 0.37070003151893616, 'test/loss': 2.948025941848755, 'test/num_examples': 10000, 'score': 9699.075863838196, 'total_duration': 10596.444150209427, 'accumulated_submission_time': 9699.075863838196, 'accumulated_eval_time': 895.5525405406952, 'accumulated_logging_time': 0.6403217315673828}
I0201 21:28:01.136907 139923868813056 logging_writer.py:48] [20882] accumulated_eval_time=895.552541, accumulated_logging_time=0.640322, accumulated_submission_time=9699.075864, global_step=20882, preemption_count=0, score=9699.075864, test/accuracy=0.370700, test/loss=2.948026, test/num_examples=10000, total_duration=10596.444150, train/accuracy=0.509922, train/loss=2.163090, validation/accuracy=0.474460, validation/loss=2.340316, validation/num_examples=50000
I0201 21:28:08.597513 139923852027648 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1377941370010376, loss=5.453763008117676
I0201 21:28:51.552003 139923868813056 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2665823698043823, loss=4.414874076843262
I0201 21:29:38.151293 139923852027648 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.3510924577713013, loss=5.475469589233398
I0201 21:30:24.992894 139923868813056 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.4724754095077515, loss=3.429680824279785
I0201 21:31:11.561128 139923852027648 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.4853018522262573, loss=3.2196438312530518
I0201 21:31:58.274066 139923868813056 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1043429374694824, loss=4.888280391693115
I0201 21:32:45.270907 139923852027648 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.627919316291809, loss=3.0911741256713867
I0201 21:33:32.025251 139923868813056 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.1048827171325684, loss=5.415372848510742
I0201 21:34:18.750046 139923852027648 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2973729372024536, loss=4.297575950622559
I0201 21:35:01.480298 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:35:12.195617 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:35:36.298936 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:35:37.938278 140085747812160 submission_runner.py:408] Time since start: 11053.27s, 	Step: 21793, 	{'train/accuracy': 0.5142382979393005, 'train/loss': 2.1343255043029785, 'validation/accuracy': 0.4792400002479553, 'validation/loss': 2.311788320541382, 'validation/num_examples': 50000, 'test/accuracy': 0.37470000982284546, 'test/loss': 2.934434413909912, 'test/num_examples': 10000, 'score': 10119.355597019196, 'total_duration': 11053.265462636948, 'accumulated_submission_time': 10119.355597019196, 'accumulated_eval_time': 932.0105512142181, 'accumulated_logging_time': 0.6730847358703613}
I0201 21:35:37.960976 139923868813056 logging_writer.py:48] [21793] accumulated_eval_time=932.010551, accumulated_logging_time=0.673085, accumulated_submission_time=10119.355597, global_step=21793, preemption_count=0, score=10119.355597, test/accuracy=0.374700, test/loss=2.934434, test/num_examples=10000, total_duration=11053.265463, train/accuracy=0.514238, train/loss=2.134326, validation/accuracy=0.479240, validation/loss=2.311788, validation/num_examples=50000
I0201 21:35:41.111158 139923852027648 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.5180377960205078, loss=3.4329066276550293
I0201 21:36:23.121789 139923868813056 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5916129350662231, loss=3.05546236038208
I0201 21:37:10.074013 139923852027648 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.2249460220336914, loss=4.521969795227051
I0201 21:37:56.664680 139923868813056 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7034835815429688, loss=3.0602340698242188
I0201 21:38:43.383557 139923852027648 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.6931517124176025, loss=3.220646858215332
I0201 21:39:30.303626 139923868813056 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.518363356590271, loss=3.1164698600769043
I0201 21:40:17.172163 139923852027648 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.1254899501800537, loss=5.457398891448975
I0201 21:41:03.910273 139923868813056 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.006337285041809, loss=5.4888811111450195
I0201 21:41:50.682779 139923852027648 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.3165451288223267, loss=3.9419543743133545
I0201 21:42:37.755884 139923868813056 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.4208382368087769, loss=3.392789840698242
I0201 21:42:38.317266 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:42:48.685863 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:43:12.647131 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:43:14.284239 140085747812160 submission_runner.py:408] Time since start: 11509.61s, 	Step: 22703, 	{'train/accuracy': 0.5282812118530273, 'train/loss': 2.0714240074157715, 'validation/accuracy': 0.48555999994277954, 'validation/loss': 2.2779319286346436, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.8995566368103027, 'test/num_examples': 10000, 'score': 10539.649500846863, 'total_duration': 11509.611449241638, 'accumulated_submission_time': 10539.649500846863, 'accumulated_eval_time': 967.9775204658508, 'accumulated_logging_time': 0.7074997425079346}
I0201 21:43:14.303910 139923852027648 logging_writer.py:48] [22703] accumulated_eval_time=967.977520, accumulated_logging_time=0.707500, accumulated_submission_time=10539.649501, global_step=22703, preemption_count=0, score=10539.649501, test/accuracy=0.382300, test/loss=2.899557, test/num_examples=10000, total_duration=11509.611449, train/accuracy=0.528281, train/loss=2.071424, validation/accuracy=0.485560, validation/loss=2.277932, validation/num_examples=50000
I0201 21:43:54.863277 139923868813056 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.6011431217193604, loss=3.50506854057312
I0201 21:44:41.649455 139923852027648 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.4595133066177368, loss=3.11612868309021
I0201 21:45:28.712587 139923868813056 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.1570804119110107, loss=5.539855480194092
I0201 21:46:15.287153 139923852027648 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.6036235094070435, loss=3.0602540969848633
I0201 21:47:02.063058 139923868813056 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3120481967926025, loss=5.126997947692871
I0201 21:47:48.441262 139923852027648 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.5862720012664795, loss=3.173044443130493
I0201 21:48:35.413768 139923868813056 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.3541522026062012, loss=3.697373390197754
I0201 21:49:22.382353 139923852027648 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.8990901708602905, loss=3.087639808654785
I0201 21:50:09.085782 139923868813056 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.5586565732955933, loss=3.0082345008850098
I0201 21:50:14.505197 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:50:24.879195 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:50:52.191609 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:50:53.830428 140085747812160 submission_runner.py:408] Time since start: 11969.16s, 	Step: 23613, 	{'train/accuracy': 0.5564648509025574, 'train/loss': 1.9199002981185913, 'validation/accuracy': 0.4958999752998352, 'validation/loss': 2.209649085998535, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.859225273132324, 'test/num_examples': 10000, 'score': 10959.790457248688, 'total_duration': 11969.157633304596, 'accumulated_submission_time': 10959.790457248688, 'accumulated_eval_time': 1007.3027441501617, 'accumulated_logging_time': 0.7358355522155762}
I0201 21:50:53.852392 139923852027648 logging_writer.py:48] [23613] accumulated_eval_time=1007.302744, accumulated_logging_time=0.735836, accumulated_submission_time=10959.790457, global_step=23613, preemption_count=0, score=10959.790457, test/accuracy=0.382300, test/loss=2.859225, test/num_examples=10000, total_duration=11969.157633, train/accuracy=0.556465, train/loss=1.919900, validation/accuracy=0.495900, validation/loss=2.209649, validation/num_examples=50000
I0201 21:51:30.117672 139923868813056 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3608226776123047, loss=3.693634271621704
I0201 21:52:16.873927 139923852027648 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.317442536354065, loss=3.721193790435791
I0201 21:53:03.802850 139923868813056 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.0888173580169678, loss=5.50423526763916
I0201 21:53:50.525799 139923852027648 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.2087522745132446, loss=3.7964541912078857
I0201 21:54:37.293594 139923868813056 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.953869342803955, loss=3.0686209201812744
I0201 21:55:23.961827 139923852027648 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.3707019090652466, loss=5.490780353546143
I0201 21:56:10.769983 139923868813056 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.1030628681182861, loss=4.096532344818115
I0201 21:56:57.367664 139923852027648 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.2098666429519653, loss=4.489708423614502
I0201 21:57:44.103665 139923868813056 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.656927466392517, loss=3.128387689590454
I0201 21:57:54.093782 140085747812160 spec.py:321] Evaluating on the training split.
I0201 21:58:04.475213 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 21:58:32.101840 140085747812160 spec.py:349] Evaluating on the test split.
I0201 21:58:33.745823 140085747812160 submission_runner.py:408] Time since start: 12429.07s, 	Step: 24523, 	{'train/accuracy': 0.5373241901397705, 'train/loss': 2.020864248275757, 'validation/accuracy': 0.5016999840736389, 'validation/loss': 2.1879594326019287, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8223278522491455, 'test/num_examples': 10000, 'score': 11379.97221159935, 'total_duration': 12429.073031425476, 'accumulated_submission_time': 11379.97221159935, 'accumulated_eval_time': 1046.9547855854034, 'accumulated_logging_time': 0.7669098377227783}
I0201 21:58:33.766057 139923852027648 logging_writer.py:48] [24523] accumulated_eval_time=1046.954786, accumulated_logging_time=0.766910, accumulated_submission_time=11379.972212, global_step=24523, preemption_count=0, score=11379.972212, test/accuracy=0.392400, test/loss=2.822328, test/num_examples=10000, total_duration=12429.073031, train/accuracy=0.537324, train/loss=2.020864, validation/accuracy=0.501700, validation/loss=2.187959, validation/num_examples=50000
I0201 21:59:05.280049 139923868813056 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.579249620437622, loss=3.1768136024475098
I0201 21:59:51.795767 139923852027648 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.141327142715454, loss=5.563908576965332
I0201 22:00:38.964558 139923868813056 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.675695538520813, loss=2.899880886077881
I0201 22:01:25.514937 139923852027648 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.451055645942688, loss=3.0744540691375732
I0201 22:02:12.362549 139923868813056 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.0564171075820923, loss=5.091308116912842
I0201 22:02:59.089263 139923852027648 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2170090675354004, loss=4.746859073638916
I0201 22:03:45.686278 139923868813056 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.5479769706726074, loss=3.09588623046875
I0201 22:04:32.541561 139923852027648 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6390454769134521, loss=2.97853946685791
I0201 22:05:19.247689 139923868813056 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.4078116416931152, loss=3.159856081008911
I0201 22:05:34.189085 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:05:44.544143 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:06:08.272848 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:06:09.910422 140085747812160 submission_runner.py:408] Time since start: 12885.24s, 	Step: 25434, 	{'train/accuracy': 0.5425586104393005, 'train/loss': 1.9881305694580078, 'validation/accuracy': 0.5017399787902832, 'validation/loss': 2.190019130706787, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8353707790374756, 'test/num_examples': 10000, 'score': 11800.332841157913, 'total_duration': 12885.237624645233, 'accumulated_submission_time': 11800.332841157913, 'accumulated_eval_time': 1082.676115512848, 'accumulated_logging_time': 0.7968857288360596}
I0201 22:06:09.929994 139923852027648 logging_writer.py:48] [25434] accumulated_eval_time=1082.676116, accumulated_logging_time=0.796886, accumulated_submission_time=11800.332841, global_step=25434, preemption_count=0, score=11800.332841, test/accuracy=0.392400, test/loss=2.835371, test/num_examples=10000, total_duration=12885.237625, train/accuracy=0.542559, train/loss=1.988131, validation/accuracy=0.501740, validation/loss=2.190019, validation/num_examples=50000
I0201 22:06:36.344500 139923868813056 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.3154274225234985, loss=3.9738998413085938
I0201 22:07:22.717397 139923852027648 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.1683106422424316, loss=5.537078857421875
I0201 22:08:09.702128 139923868813056 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.378713369369507, loss=3.1349751949310303
I0201 22:08:56.338243 139923852027648 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.6664732694625854, loss=2.995279312133789
I0201 22:09:43.124786 139923868813056 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.6032499074935913, loss=3.0105319023132324
I0201 22:10:30.193643 139923852027648 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.3213438987731934, loss=4.287905693054199
I0201 22:11:17.055018 139923868813056 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.6194785833358765, loss=2.9955008029937744
I0201 22:12:03.927761 139923852027648 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.6024218797683716, loss=3.0194218158721924
I0201 22:12:51.026395 139923868813056 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.2459067106246948, loss=4.4470415115356445
I0201 22:13:10.287884 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:13:21.522320 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:13:44.155741 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:13:45.793530 140085747812160 submission_runner.py:408] Time since start: 13341.12s, 	Step: 26343, 	{'train/accuracy': 0.573046863079071, 'train/loss': 1.8160487413406372, 'validation/accuracy': 0.5158599615097046, 'validation/loss': 2.1130800247192383, 'validation/num_examples': 50000, 'test/accuracy': 0.40790000557899475, 'test/loss': 2.7394766807556152, 'test/num_examples': 10000, 'score': 12220.630915403366, 'total_duration': 13341.120736837387, 'accumulated_submission_time': 12220.630915403366, 'accumulated_eval_time': 1118.181747674942, 'accumulated_logging_time': 0.8260171413421631}
I0201 22:13:45.815719 139923852027648 logging_writer.py:48] [26343] accumulated_eval_time=1118.181748, accumulated_logging_time=0.826017, accumulated_submission_time=12220.630915, global_step=26343, preemption_count=0, score=12220.630915, test/accuracy=0.407900, test/loss=2.739477, test/num_examples=10000, total_duration=13341.120737, train/accuracy=0.573047, train/loss=1.816049, validation/accuracy=0.515860, validation/loss=2.113080, validation/num_examples=50000
I0201 22:14:08.574285 139923868813056 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.3062036037445068, loss=4.247155666351318
I0201 22:14:54.159356 139923852027648 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.605362892150879, loss=3.0563931465148926
I0201 22:15:41.236387 139923868813056 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6111983060836792, loss=3.122877836227417
I0201 22:16:27.938587 139923852027648 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.378886103630066, loss=3.960803508758545
I0201 22:17:14.753899 139923868813056 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.4228012561798096, loss=3.443289279937744
I0201 22:18:01.616614 139923852027648 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.572439432144165, loss=2.8918628692626953
I0201 22:18:48.112848 139923868813056 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.3406336307525635, loss=4.865362644195557
I0201 22:19:34.850281 139923852027648 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2240058183670044, loss=4.643664836883545
I0201 22:20:21.690943 139923868813056 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.2836459875106812, loss=4.634676456451416
I0201 22:20:46.108625 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:20:56.891869 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:21:20.899561 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:21:22.539202 140085747812160 submission_runner.py:408] Time since start: 13797.87s, 	Step: 27254, 	{'train/accuracy': 0.5526952743530273, 'train/loss': 1.9304035902023315, 'validation/accuracy': 0.5169399976730347, 'validation/loss': 2.108940362930298, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.7597310543060303, 'test/num_examples': 10000, 'score': 12640.863479614258, 'total_duration': 13797.866409778595, 'accumulated_submission_time': 12640.863479614258, 'accumulated_eval_time': 1154.6123263835907, 'accumulated_logging_time': 0.8571968078613281}
I0201 22:21:22.563337 139923852027648 logging_writer.py:48] [27254] accumulated_eval_time=1154.612326, accumulated_logging_time=0.857197, accumulated_submission_time=12640.863480, global_step=27254, preemption_count=0, score=12640.863480, test/accuracy=0.407000, test/loss=2.759731, test/num_examples=10000, total_duration=13797.866410, train/accuracy=0.552695, train/loss=1.930404, validation/accuracy=0.516940, validation/loss=2.108940, validation/num_examples=50000
I0201 22:21:41.002048 139923868813056 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.5410563945770264, loss=2.828866481781006
I0201 22:22:25.956107 139923852027648 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.315371036529541, loss=5.026711940765381
I0201 22:23:12.795076 139923868813056 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5359033346176147, loss=3.4858689308166504
I0201 22:23:59.267764 139923852027648 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.4999665021896362, loss=3.3507933616638184
I0201 22:24:46.020672 139923868813056 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.7307075262069702, loss=2.9638655185699463
I0201 22:25:32.830759 139923852027648 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.5391665697097778, loss=2.915679693222046
I0201 22:26:19.844332 139923868813056 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.5972957611083984, loss=2.9743525981903076
I0201 22:27:06.765115 139923852027648 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.211753010749817, loss=5.0480122566223145
I0201 22:27:53.587418 139923868813056 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7038171291351318, loss=2.8656411170959473
I0201 22:28:22.800586 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:28:33.137684 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:28:55.159605 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:28:56.791976 140085747812160 submission_runner.py:408] Time since start: 14252.12s, 	Step: 28164, 	{'train/accuracy': 0.5553905963897705, 'train/loss': 1.9481656551361084, 'validation/accuracy': 0.51528000831604, 'validation/loss': 2.139265298843384, 'validation/num_examples': 50000, 'test/accuracy': 0.4034000337123871, 'test/loss': 2.77888560295105, 'test/num_examples': 10000, 'score': 13061.040511369705, 'total_duration': 14252.119184017181, 'accumulated_submission_time': 13061.040511369705, 'accumulated_eval_time': 1188.6037278175354, 'accumulated_logging_time': 0.8905489444732666}
I0201 22:28:56.814353 139923852027648 logging_writer.py:48] [28164] accumulated_eval_time=1188.603728, accumulated_logging_time=0.890549, accumulated_submission_time=13061.040511, global_step=28164, preemption_count=0, score=13061.040511, test/accuracy=0.403400, test/loss=2.778886, test/num_examples=10000, total_duration=14252.119184, train/accuracy=0.555391, train/loss=1.948166, validation/accuracy=0.515280, validation/loss=2.139265, validation/num_examples=50000
I0201 22:29:11.338766 139923868813056 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.7364050149917603, loss=2.9676132202148438
I0201 22:29:55.507600 139923852027648 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6122208833694458, loss=2.933331251144409
I0201 22:30:42.424416 139923868813056 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.6650272607803345, loss=3.369960308074951
I0201 22:31:29.162621 139923852027648 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.3537490367889404, loss=4.968056678771973
I0201 22:32:15.992187 139923868813056 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.5764663219451904, loss=2.7761785984039307
I0201 22:33:02.993838 139923852027648 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.6289039850234985, loss=2.945949077606201
I0201 22:33:49.770016 139923868813056 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.9378557205200195, loss=3.118143081665039
I0201 22:34:36.386021 139923852027648 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.5959869623184204, loss=3.219832181930542
I0201 22:35:22.923285 139923868813056 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.223436713218689, loss=4.235145568847656
I0201 22:35:56.843315 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:36:07.123753 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:36:29.643567 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:36:31.282147 140085747812160 submission_runner.py:408] Time since start: 14706.61s, 	Step: 29074, 	{'train/accuracy': 0.576171875, 'train/loss': 1.8145115375518799, 'validation/accuracy': 0.5225200057029724, 'validation/loss': 2.0690627098083496, 'validation/num_examples': 50000, 'test/accuracy': 0.4146000146865845, 'test/loss': 2.6903810501098633, 'test/num_examples': 10000, 'score': 13481.007174015045, 'total_duration': 14706.609322786331, 'accumulated_submission_time': 13481.007174015045, 'accumulated_eval_time': 1223.0425362586975, 'accumulated_logging_time': 0.9243690967559814}
I0201 22:36:31.305108 139923852027648 logging_writer.py:48] [29074] accumulated_eval_time=1223.042536, accumulated_logging_time=0.924369, accumulated_submission_time=13481.007174, global_step=29074, preemption_count=0, score=13481.007174, test/accuracy=0.414600, test/loss=2.690381, test/num_examples=10000, total_duration=14706.609323, train/accuracy=0.576172, train/loss=1.814512, validation/accuracy=0.522520, validation/loss=2.069063, validation/num_examples=50000
I0201 22:36:41.911964 139923868813056 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.205980896949768, loss=5.54628324508667
I0201 22:37:25.017982 139923852027648 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.7093480825424194, loss=2.86527681350708
I0201 22:38:11.874156 139923868813056 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.150221824645996, loss=5.6053385734558105
I0201 22:38:58.491544 139923852027648 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.2791879177093506, loss=3.983767032623291
I0201 22:39:45.442880 139923868813056 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.472575068473816, loss=3.746284246444702
I0201 22:40:32.253171 139923852027648 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1997586488723755, loss=4.6854777336120605
I0201 22:41:19.104225 139923868813056 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0998072624206543, loss=5.429131507873535
I0201 22:42:05.838391 139923852027648 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7005752325057983, loss=2.909242630004883
I0201 22:42:52.481503 139923868813056 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6660206317901611, loss=3.05203914642334
I0201 22:43:31.404683 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:43:41.990777 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:44:03.343698 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:44:05.002398 140085747812160 submission_runner.py:408] Time since start: 15160.33s, 	Step: 29985, 	{'train/accuracy': 0.5616992115974426, 'train/loss': 1.8721139430999756, 'validation/accuracy': 0.5232599973678589, 'validation/loss': 2.0561981201171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.7167224884033203, 'test/num_examples': 10000, 'score': 13901.04438996315, 'total_duration': 15160.329586267471, 'accumulated_submission_time': 13901.04438996315, 'accumulated_eval_time': 1256.6402442455292, 'accumulated_logging_time': 0.9586703777313232}
I0201 22:44:05.024070 139923852027648 logging_writer.py:48] [29985] accumulated_eval_time=1256.640244, accumulated_logging_time=0.958670, accumulated_submission_time=13901.044390, global_step=29985, preemption_count=0, score=13901.044390, test/accuracy=0.408600, test/loss=2.716722, test/num_examples=10000, total_duration=15160.329586, train/accuracy=0.561699, train/loss=1.872114, validation/accuracy=0.523260, validation/loss=2.056198, validation/num_examples=50000
I0201 22:44:11.320953 139923868813056 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.6015474796295166, loss=5.509914398193359
I0201 22:44:53.957583 139923852027648 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.781664252281189, loss=2.916193723678589
I0201 22:45:40.354436 139923868813056 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.5747019052505493, loss=3.654111385345459
I0201 22:46:27.278473 139923852027648 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.6599050760269165, loss=2.941896677017212
I0201 22:47:13.894118 139923868813056 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.5843793153762817, loss=3.0111608505249023
I0201 22:48:00.855078 139923852027648 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.6430256366729736, loss=2.811544895172119
I0201 22:48:47.458599 139923868813056 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.149231195449829, loss=5.4783525466918945
I0201 22:49:34.289011 139923852027648 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.7227511405944824, loss=2.758777141571045
I0201 22:50:21.016416 139923868813056 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.6679543256759644, loss=2.8265368938446045
I0201 22:51:05.263528 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:51:15.595177 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:51:42.024159 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:51:43.663841 140085747812160 submission_runner.py:408] Time since start: 15618.99s, 	Step: 30896, 	{'train/accuracy': 0.5642382502555847, 'train/loss': 1.8705335855484009, 'validation/accuracy': 0.519819974899292, 'validation/loss': 2.0736615657806396, 'validation/num_examples': 50000, 'test/accuracy': 0.4124000072479248, 'test/loss': 2.723940849304199, 'test/num_examples': 10000, 'score': 14321.219009399414, 'total_duration': 15618.991045713425, 'accumulated_submission_time': 14321.219009399414, 'accumulated_eval_time': 1295.0405583381653, 'accumulated_logging_time': 0.9944887161254883}
I0201 22:51:43.687096 139923852027648 logging_writer.py:48] [30896] accumulated_eval_time=1295.040558, accumulated_logging_time=0.994489, accumulated_submission_time=14321.219009, global_step=30896, preemption_count=0, score=14321.219009, test/accuracy=0.412400, test/loss=2.723941, test/num_examples=10000, total_duration=15618.991046, train/accuracy=0.564238, train/loss=1.870534, validation/accuracy=0.519820, validation/loss=2.073662, validation/num_examples=50000
I0201 22:51:45.655854 139923868813056 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.4792085886001587, loss=2.855170965194702
I0201 22:52:27.754034 139923852027648 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.7950868606567383, loss=2.9881272315979004
I0201 22:53:14.492163 139923868813056 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6567565202713013, loss=2.83084774017334
I0201 22:54:01.303643 139923852027648 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.5779242515563965, loss=3.2915780544281006
I0201 22:54:48.131638 139923868813056 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.5589213371276855, loss=3.271174192428589
I0201 22:55:34.990040 139923852027648 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.5615404844284058, loss=3.9385452270507812
I0201 22:56:21.479972 139923868813056 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.5007426738739014, loss=2.7809488773345947
I0201 22:57:08.127350 139923852027648 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.7975269556045532, loss=2.8726234436035156
I0201 22:57:54.856648 139923868813056 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.908332109451294, loss=2.826875686645508
I0201 22:58:41.614423 139923852027648 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.6126435995101929, loss=2.910435438156128
I0201 22:58:44.073645 140085747812160 spec.py:321] Evaluating on the training split.
I0201 22:58:54.379062 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 22:59:16.492287 140085747812160 spec.py:349] Evaluating on the test split.
I0201 22:59:18.128454 140085747812160 submission_runner.py:408] Time since start: 16073.46s, 	Step: 31807, 	{'train/accuracy': 0.5814062356948853, 'train/loss': 1.8077421188354492, 'validation/accuracy': 0.5366799831390381, 'validation/loss': 2.019350290298462, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.674529790878296, 'test/num_examples': 10000, 'score': 14741.544480085373, 'total_duration': 16073.455638170242, 'accumulated_submission_time': 14741.544480085373, 'accumulated_eval_time': 1329.0953433513641, 'accumulated_logging_time': 1.027569055557251}
I0201 22:59:18.150277 139923868813056 logging_writer.py:48] [31807] accumulated_eval_time=1329.095343, accumulated_logging_time=1.027569, accumulated_submission_time=14741.544480, global_step=31807, preemption_count=0, score=14741.544480, test/accuracy=0.423900, test/loss=2.674530, test/num_examples=10000, total_duration=16073.455638, train/accuracy=0.581406, train/loss=1.807742, validation/accuracy=0.536680, validation/loss=2.019350, validation/num_examples=50000
I0201 22:59:56.595426 139923852027648 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.6321773529052734, loss=2.7347092628479004
I0201 23:00:43.348548 139923868813056 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.264703392982483, loss=4.110182762145996
I0201 23:01:29.986145 139923852027648 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.6756854057312012, loss=2.7831873893737793
I0201 23:02:16.691570 139923868813056 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.6547925472259521, loss=2.789515972137451
I0201 23:03:03.684153 139923852027648 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.6284197568893433, loss=2.8510403633117676
I0201 23:03:50.649504 139923868813056 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.518325924873352, loss=3.4183497428894043
I0201 23:04:37.323908 139923852027648 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.8140809535980225, loss=2.8989293575286865
I0201 23:05:24.072830 139923868813056 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.7315044403076172, loss=3.354309320449829
I0201 23:06:10.638688 139923852027648 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.5142186880111694, loss=3.4208338260650635
I0201 23:06:18.351908 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:06:29.229932 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:06:55.812429 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:06:57.459559 140085747812160 submission_runner.py:408] Time since start: 16532.79s, 	Step: 32718, 	{'train/accuracy': 0.5716406106948853, 'train/loss': 1.8151863813400269, 'validation/accuracy': 0.5365599989891052, 'validation/loss': 1.983070731163025, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.6434519290924072, 'test/num_examples': 10000, 'score': 15161.686965703964, 'total_duration': 16532.786767959595, 'accumulated_submission_time': 15161.686965703964, 'accumulated_eval_time': 1368.2029874324799, 'accumulated_logging_time': 1.0579285621643066}
I0201 23:06:57.480261 139923868813056 logging_writer.py:48] [32718] accumulated_eval_time=1368.202987, accumulated_logging_time=1.057929, accumulated_submission_time=15161.686966, global_step=32718, preemption_count=0, score=15161.686966, test/accuracy=0.426400, test/loss=2.643452, test/num_examples=10000, total_duration=16532.786768, train/accuracy=0.571641, train/loss=1.815186, validation/accuracy=0.536560, validation/loss=1.983071, validation/num_examples=50000
I0201 23:07:31.279142 139923852027648 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.590003490447998, loss=3.0383944511413574
I0201 23:08:17.901274 139923868813056 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.058288335800171, loss=2.855806350708008
I0201 23:09:04.785045 139923852027648 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.8775008916854858, loss=2.8682518005371094
I0201 23:09:51.333075 139923868813056 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.5866893529891968, loss=2.9513657093048096
I0201 23:10:37.992333 139923852027648 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.286576747894287, loss=3.9354825019836426
I0201 23:11:24.810479 139923868813056 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.726543664932251, loss=2.756779909133911
I0201 23:12:11.621886 139923852027648 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.3488798141479492, loss=3.86845326423645
I0201 23:12:58.508652 139923868813056 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.712588906288147, loss=2.9813232421875
I0201 23:13:45.111130 139923852027648 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.54280424118042, loss=3.44364070892334
I0201 23:13:57.593870 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:14:08.064467 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:14:32.857152 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:14:34.493772 140085747812160 submission_runner.py:408] Time since start: 16989.82s, 	Step: 33628, 	{'train/accuracy': 0.5784569978713989, 'train/loss': 1.8230026960372925, 'validation/accuracy': 0.5387799739837646, 'validation/loss': 2.005711555480957, 'validation/num_examples': 50000, 'test/accuracy': 0.42920002341270447, 'test/loss': 2.636946439743042, 'test/num_examples': 10000, 'score': 15581.740196228027, 'total_duration': 16989.820979833603, 'accumulated_submission_time': 15581.740196228027, 'accumulated_eval_time': 1405.1028938293457, 'accumulated_logging_time': 1.087789535522461}
I0201 23:14:34.514670 139923868813056 logging_writer.py:48] [33628] accumulated_eval_time=1405.102894, accumulated_logging_time=1.087790, accumulated_submission_time=15581.740196, global_step=33628, preemption_count=0, score=15581.740196, test/accuracy=0.429200, test/loss=2.636946, test/num_examples=10000, total_duration=16989.820980, train/accuracy=0.578457, train/loss=1.823003, validation/accuracy=0.538780, validation/loss=2.005712, validation/num_examples=50000
I0201 23:15:03.677183 139923852027648 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.6279680728912354, loss=2.8828787803649902
I0201 23:15:50.301419 139923868813056 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.6341745853424072, loss=2.8037047386169434
I0201 23:16:37.084050 139923852027648 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2938776016235352, loss=4.764259338378906
I0201 23:17:23.626277 139923868813056 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.6245124340057373, loss=2.742147207260132
I0201 23:18:10.592206 139923852027648 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.3791178464889526, loss=5.138505458831787
I0201 23:18:57.250511 139923868813056 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.2007254362106323, loss=5.429337501525879
I0201 23:19:44.256916 139923852027648 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.5762763023376465, loss=2.797769784927368
I0201 23:20:30.901758 139923868813056 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.4889967441558838, loss=3.527568817138672
I0201 23:21:17.523969 139923852027648 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.4215648174285889, loss=3.5081658363342285
I0201 23:21:34.823340 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:21:44.945475 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:22:07.570858 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:22:09.210608 140085747812160 submission_runner.py:408] Time since start: 17444.54s, 	Step: 34539, 	{'train/accuracy': 0.5900781154632568, 'train/loss': 1.7298932075500488, 'validation/accuracy': 0.5426799654960632, 'validation/loss': 1.9463582038879395, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.602484941482544, 'test/num_examples': 10000, 'score': 16001.988677024841, 'total_duration': 17444.537808418274, 'accumulated_submission_time': 16001.988677024841, 'accumulated_eval_time': 1439.4901642799377, 'accumulated_logging_time': 1.1178858280181885}
I0201 23:22:09.232595 139923868813056 logging_writer.py:48] [34539] accumulated_eval_time=1439.490164, accumulated_logging_time=1.117886, accumulated_submission_time=16001.988677, global_step=34539, preemption_count=0, score=16001.988677, test/accuracy=0.425100, test/loss=2.602485, test/num_examples=10000, total_duration=17444.537808, train/accuracy=0.590078, train/loss=1.729893, validation/accuracy=0.542680, validation/loss=1.946358, validation/num_examples=50000
I0201 23:22:33.568345 139923852027648 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.5613423585891724, loss=3.0990753173828125
I0201 23:23:19.806398 139923868813056 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.9664151668548584, loss=2.855931520462036
I0201 23:24:06.984286 139923852027648 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.582071304321289, loss=2.7119557857513428
I0201 23:24:53.597940 139923868813056 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.3003060817718506, loss=5.175588130950928
I0201 23:25:40.231202 139923852027648 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.6582720279693604, loss=2.745541572570801
I0201 23:26:26.911629 139923868813056 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5983312129974365, loss=2.76631498336792
I0201 23:27:13.626407 139923852027648 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.3096917867660522, loss=5.391072750091553
I0201 23:28:00.321740 139923868813056 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.3297175168991089, loss=5.156780242919922
I0201 23:28:47.130886 139923852027648 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.603043556213379, loss=3.4425387382507324
I0201 23:29:09.472382 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:29:19.948580 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:29:44.275015 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:29:45.919679 140085747812160 submission_runner.py:408] Time since start: 17901.25s, 	Step: 35449, 	{'train/accuracy': 0.5878710746765137, 'train/loss': 1.7523179054260254, 'validation/accuracy': 0.5478000044822693, 'validation/loss': 1.9458662271499634, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.59553599357605, 'test/num_examples': 10000, 'score': 16422.16807770729, 'total_duration': 17901.24688887596, 'accumulated_submission_time': 16422.16807770729, 'accumulated_eval_time': 1475.937474489212, 'accumulated_logging_time': 1.149817943572998}
I0201 23:29:45.943244 139923868813056 logging_writer.py:48] [35449] accumulated_eval_time=1475.937474, accumulated_logging_time=1.149818, accumulated_submission_time=16422.168078, global_step=35449, preemption_count=0, score=16422.168078, test/accuracy=0.433100, test/loss=2.595536, test/num_examples=10000, total_duration=17901.246889, train/accuracy=0.587871, train/loss=1.752318, validation/accuracy=0.547800, validation/loss=1.945866, validation/num_examples=50000
I0201 23:30:06.356487 139923852027648 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5344202518463135, loss=2.7101891040802
I0201 23:30:51.767931 139923868813056 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.928134799003601, loss=2.955693244934082
I0201 23:31:38.812708 139923852027648 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.5815694332122803, loss=2.703555107116699
I0201 23:32:25.303124 139923868813056 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.3371769189834595, loss=4.337279319763184
I0201 23:33:12.444893 139923852027648 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.6536099910736084, loss=3.194711208343506
I0201 23:33:59.209215 139923868813056 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.3599926233291626, loss=3.7684102058410645
I0201 23:34:46.200390 139923852027648 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.702354073524475, loss=2.7330434322357178
I0201 23:35:32.847613 139923868813056 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.0378477573394775, loss=2.774120330810547
I0201 23:36:19.703865 139923852027648 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.3492487668991089, loss=4.587769985198975
I0201 23:36:46.375711 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:36:56.795234 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:37:22.981039 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:37:24.638477 140085747812160 submission_runner.py:408] Time since start: 18359.97s, 	Step: 36359, 	{'train/accuracy': 0.5913280844688416, 'train/loss': 1.7497471570968628, 'validation/accuracy': 0.5470399856567383, 'validation/loss': 1.9463144540786743, 'validation/num_examples': 50000, 'test/accuracy': 0.4359000325202942, 'test/loss': 2.588318109512329, 'test/num_examples': 10000, 'score': 16842.540033340454, 'total_duration': 18359.965654611588, 'accumulated_submission_time': 16842.540033340454, 'accumulated_eval_time': 1514.2002153396606, 'accumulated_logging_time': 1.1828999519348145}
I0201 23:37:24.661477 139923868813056 logging_writer.py:48] [36359] accumulated_eval_time=1514.200215, accumulated_logging_time=1.182900, accumulated_submission_time=16842.540033, global_step=36359, preemption_count=0, score=16842.540033, test/accuracy=0.435900, test/loss=2.588318, test/num_examples=10000, total_duration=18359.965655, train/accuracy=0.591328, train/loss=1.749747, validation/accuracy=0.547040, validation/loss=1.946314, validation/num_examples=50000
I0201 23:37:41.128962 139923852027648 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.9845579862594604, loss=2.8381810188293457
I0201 23:38:26.607674 139923868813056 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.888118863105774, loss=2.9754815101623535
I0201 23:39:12.817578 139923852027648 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.7190812826156616, loss=2.7733559608459473
I0201 23:39:59.554288 139923868813056 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.8303370475769043, loss=2.7869858741760254
I0201 23:40:46.315726 139923852027648 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.826531171798706, loss=2.8966803550720215
I0201 23:41:32.934342 139923868813056 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7650964260101318, loss=2.9491066932678223
I0201 23:42:19.573749 139923852027648 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.4946653842926025, loss=4.822768211364746
I0201 23:43:06.347049 139923868813056 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.6317787170410156, loss=2.8707008361816406
I0201 23:43:53.048123 139923852027648 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.491885781288147, loss=4.688072204589844
I0201 23:44:24.776391 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:44:35.237826 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:45:02.517149 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:45:04.167425 140085747812160 submission_runner.py:408] Time since start: 18819.49s, 	Step: 37270, 	{'train/accuracy': 0.5938476324081421, 'train/loss': 1.7227494716644287, 'validation/accuracy': 0.5518400073051453, 'validation/loss': 1.9233359098434448, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5716423988342285, 'test/num_examples': 10000, 'score': 17262.59283065796, 'total_duration': 18819.49461555481, 'accumulated_submission_time': 17262.59283065796, 'accumulated_eval_time': 1553.5912311077118, 'accumulated_logging_time': 1.2173235416412354}
I0201 23:45:04.194187 139923868813056 logging_writer.py:48] [37270] accumulated_eval_time=1553.591231, accumulated_logging_time=1.217324, accumulated_submission_time=17262.592831, global_step=37270, preemption_count=0, score=17262.592831, test/accuracy=0.433000, test/loss=2.571642, test/num_examples=10000, total_duration=18819.494616, train/accuracy=0.593848, train/loss=1.722749, validation/accuracy=0.551840, validation/loss=1.923336, validation/num_examples=50000
I0201 23:45:16.381226 139923852027648 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.7867521047592163, loss=3.0915372371673584
I0201 23:46:00.120150 139923868813056 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5787447690963745, loss=2.6612954139709473
I0201 23:46:46.796314 139923852027648 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.5811322927474976, loss=2.7829833030700684
I0201 23:47:33.649526 139923868813056 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.3338489532470703, loss=4.4289469718933105
I0201 23:48:20.341969 139923852027648 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.663085699081421, loss=2.683248281478882
I0201 23:49:07.121132 139923868813056 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.7005456686019897, loss=2.7376506328582764
I0201 23:49:53.594338 139923852027648 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1327873468399048, loss=5.153895854949951
I0201 23:50:40.132350 139923868813056 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.5290327072143555, loss=3.2746686935424805
I0201 23:51:26.936111 139923852027648 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.6938446760177612, loss=2.7828176021575928
I0201 23:52:04.360136 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:52:15.064246 140085747812160 spec.py:333] Evaluating on the validation split.
I0201 23:52:36.519655 140085747812160 spec.py:349] Evaluating on the test split.
I0201 23:52:38.163893 140085747812160 submission_runner.py:408] Time since start: 19273.49s, 	Step: 38182, 	{'train/accuracy': 0.6218359470367432, 'train/loss': 1.614800214767456, 'validation/accuracy': 0.5530799627304077, 'validation/loss': 1.9338055849075317, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.5792202949523926, 'test/num_examples': 10000, 'score': 17682.696738004684, 'total_duration': 19273.491097211838, 'accumulated_submission_time': 17682.696738004684, 'accumulated_eval_time': 1587.3950009346008, 'accumulated_logging_time': 1.255021095275879}
I0201 23:52:38.187555 139923868813056 logging_writer.py:48] [38182] accumulated_eval_time=1587.395001, accumulated_logging_time=1.255021, accumulated_submission_time=17682.696738, global_step=38182, preemption_count=0, score=17682.696738, test/accuracy=0.439400, test/loss=2.579220, test/num_examples=10000, total_duration=19273.491097, train/accuracy=0.621836, train/loss=1.614800, validation/accuracy=0.553080, validation/loss=1.933806, validation/num_examples=50000
I0201 23:52:45.644727 139923852027648 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.6328091621398926, loss=2.898439407348633
I0201 23:53:28.299308 139923868813056 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.957193374633789, loss=5.455041885375977
I0201 23:54:15.277724 139923852027648 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.7388510704040527, loss=2.79809832572937
I0201 23:55:01.747821 139923868813056 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.6974984407424927, loss=2.8229854106903076
I0201 23:55:48.692734 139923852027648 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.2188993692398071, loss=5.012413024902344
I0201 23:56:35.612299 139923868813056 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.6693602800369263, loss=3.1971840858459473
I0201 23:57:22.309177 139923852027648 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.3993024826049805, loss=4.384085655212402
I0201 23:58:08.854183 139923868813056 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.3809489011764526, loss=4.652706146240234
I0201 23:58:55.748687 139923852027648 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.7844858169555664, loss=2.657956600189209
I0201 23:59:38.396179 140085747812160 spec.py:321] Evaluating on the training split.
I0201 23:59:48.761354 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:00:12.029077 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:00:13.669089 140085747812160 submission_runner.py:408] Time since start: 19729.00s, 	Step: 39093, 	{'train/accuracy': 0.5911718606948853, 'train/loss': 1.776065468788147, 'validation/accuracy': 0.5468999743461609, 'validation/loss': 1.9785503149032593, 'validation/num_examples': 50000, 'test/accuracy': 0.4311000108718872, 'test/loss': 2.6091208457946777, 'test/num_examples': 10000, 'score': 18102.84414291382, 'total_duration': 19728.996298074722, 'accumulated_submission_time': 18102.84414291382, 'accumulated_eval_time': 1622.6679162979126, 'accumulated_logging_time': 1.2887091636657715}
I0202 00:00:13.691984 139923868813056 logging_writer.py:48] [39093] accumulated_eval_time=1622.667916, accumulated_logging_time=1.288709, accumulated_submission_time=18102.844143, global_step=39093, preemption_count=0, score=18102.844143, test/accuracy=0.431100, test/loss=2.609121, test/num_examples=10000, total_duration=19728.996298, train/accuracy=0.591172, train/loss=1.776065, validation/accuracy=0.546900, validation/loss=1.978550, validation/num_examples=50000
I0202 00:00:16.834940 139923852027648 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.3709064722061157, loss=3.7552661895751953
I0202 00:00:58.768649 139923868813056 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.4993586540222168, loss=5.462069511413574
I0202 00:01:45.689857 139923852027648 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7651035785675049, loss=3.0162596702575684
I0202 00:02:32.776798 139923868813056 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.479384183883667, loss=5.318905830383301
I0202 00:03:19.743196 139923852027648 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.319201111793518, loss=5.470981597900391
I0202 00:04:06.591186 139923868813056 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.6134212017059326, loss=2.8672451972961426
I0202 00:04:53.364654 139923852027648 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.596830129623413, loss=2.7875986099243164
I0202 00:05:40.151332 139923868813056 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.4252275228500366, loss=4.544665813446045
I0202 00:06:27.028463 139923852027648 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.380789875984192, loss=3.651928186416626
I0202 00:07:13.874211 139923868813056 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.712935447692871, loss=2.992948055267334
I0202 00:07:13.889015 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:07:24.244859 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:07:47.593159 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:07:49.233801 140085747812160 submission_runner.py:408] Time since start: 20184.56s, 	Step: 40001, 	{'train/accuracy': 0.6050390601158142, 'train/loss': 1.6630072593688965, 'validation/accuracy': 0.5593199729919434, 'validation/loss': 1.883844017982483, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.5480105876922607, 'test/num_examples': 10000, 'score': 18522.978043079376, 'total_duration': 20184.561008930206, 'accumulated_submission_time': 18522.978043079376, 'accumulated_eval_time': 1658.0126931667328, 'accumulated_logging_time': 1.3234169483184814}
I0202 00:07:49.256016 139923852027648 logging_writer.py:48] [40001] accumulated_eval_time=1658.012693, accumulated_logging_time=1.323417, accumulated_submission_time=18522.978043, global_step=40001, preemption_count=0, score=18522.978043, test/accuracy=0.435600, test/loss=2.548011, test/num_examples=10000, total_duration=20184.561009, train/accuracy=0.605039, train/loss=1.663007, validation/accuracy=0.559320, validation/loss=1.883844, validation/num_examples=50000
I0202 00:08:30.557361 139923868813056 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.949230432510376, loss=2.8428902626037598
I0202 00:09:16.963283 139923852027648 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2021703720092773, loss=5.330966949462891
I0202 00:10:03.862598 139923868813056 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.8697718381881714, loss=2.764601945877075
I0202 00:10:50.437724 139923852027648 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8051897287368774, loss=2.7918002605438232
I0202 00:11:37.131727 139923868813056 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.4395976066589355, loss=3.0885162353515625
I0202 00:12:23.769042 139923852027648 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.894268274307251, loss=2.780174493789673
I0202 00:13:10.465059 139923868813056 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.3968520164489746, loss=3.7384250164031982
I0202 00:13:57.042879 139923852027648 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6640032529830933, loss=2.636267900466919
I0202 00:14:43.711190 139923868813056 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.7288528680801392, loss=2.9027256965637207
I0202 00:14:49.403863 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:14:59.743588 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:15:25.280845 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:15:26.932096 140085747812160 submission_runner.py:408] Time since start: 20642.26s, 	Step: 40914, 	{'train/accuracy': 0.6123827695846558, 'train/loss': 1.6324337720870972, 'validation/accuracy': 0.5535199642181396, 'validation/loss': 1.9175604581832886, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.5651254653930664, 'test/num_examples': 10000, 'score': 18943.064480304718, 'total_duration': 20642.259303569794, 'accumulated_submission_time': 18943.064480304718, 'accumulated_eval_time': 1695.5409202575684, 'accumulated_logging_time': 1.355790376663208}
I0202 00:15:26.956535 139923852027648 logging_writer.py:48] [40914] accumulated_eval_time=1695.540920, accumulated_logging_time=1.355790, accumulated_submission_time=18943.064480, global_step=40914, preemption_count=0, score=18943.064480, test/accuracy=0.438900, test/loss=2.565125, test/num_examples=10000, total_duration=20642.259304, train/accuracy=0.612383, train/loss=1.632434, validation/accuracy=0.553520, validation/loss=1.917560, validation/num_examples=50000
I0202 00:16:02.628706 139923868813056 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.6571009159088135, loss=2.783114194869995
I0202 00:16:49.388326 139923852027648 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7780051231384277, loss=2.8576831817626953
I0202 00:17:36.352710 139923868813056 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.6795939207077026, loss=3.1523849964141846
I0202 00:18:23.553828 139923852027648 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.263285517692566, loss=5.249609470367432
I0202 00:19:10.417888 139923868813056 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6036800146102905, loss=2.6525423526763916
I0202 00:19:57.213186 139923852027648 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2572625875473022, loss=4.705451488494873
I0202 00:20:44.167971 139923868813056 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.8715418577194214, loss=2.9387495517730713
I0202 00:21:31.101901 139923852027648 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.275578260421753, loss=4.289722442626953
I0202 00:22:17.963533 139923868813056 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.7774627208709717, loss=2.826569080352783
I0202 00:22:27.073384 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:22:37.662538 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:23:02.841802 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:23:04.480960 140085747812160 submission_runner.py:408] Time since start: 21099.81s, 	Step: 41821, 	{'train/accuracy': 0.5993554592132568, 'train/loss': 1.6984341144561768, 'validation/accuracy': 0.5589599609375, 'validation/loss': 1.893854022026062, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.5530781745910645, 'test/num_examples': 10000, 'score': 19363.120608329773, 'total_duration': 21099.808168172836, 'accumulated_submission_time': 19363.120608329773, 'accumulated_eval_time': 1732.9484958648682, 'accumulated_logging_time': 1.3894102573394775}
I0202 00:23:04.504512 139923852027648 logging_writer.py:48] [41821] accumulated_eval_time=1732.948496, accumulated_logging_time=1.389410, accumulated_submission_time=19363.120608, global_step=41821, preemption_count=0, score=19363.120608, test/accuracy=0.437400, test/loss=2.553078, test/num_examples=10000, total_duration=21099.808168, train/accuracy=0.599355, train/loss=1.698434, validation/accuracy=0.558960, validation/loss=1.893854, validation/num_examples=50000
I0202 00:23:37.022550 139923868813056 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.5953587293624878, loss=3.0746426582336426
I0202 00:24:23.574064 139923852027648 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7386845350265503, loss=2.7066144943237305
I0202 00:25:10.549354 139923868813056 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.7311614751815796, loss=2.9496660232543945
I0202 00:25:57.106241 139923852027648 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.8875563144683838, loss=2.8300299644470215
I0202 00:26:43.837353 139923868813056 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.2202463150024414, loss=4.836658000946045
I0202 00:27:30.574995 139923852027648 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.5441696643829346, loss=3.6036479473114014
I0202 00:28:17.307725 139923868813056 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.6464594602584839, loss=2.7057292461395264
I0202 00:29:03.922245 139923852027648 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6257535219192505, loss=2.7276771068573
I0202 00:29:50.559830 139923868813056 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.3652318716049194, loss=4.305418491363525
I0202 00:30:04.723894 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:30:15.210393 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:30:40.001802 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:30:41.638386 140085747812160 submission_runner.py:408] Time since start: 21556.97s, 	Step: 42732, 	{'train/accuracy': 0.6033788919448853, 'train/loss': 1.6613901853561401, 'validation/accuracy': 0.5579400062561035, 'validation/loss': 1.870342493057251, 'validation/num_examples': 50000, 'test/accuracy': 0.4466000199317932, 'test/loss': 2.5125553607940674, 'test/num_examples': 10000, 'score': 19783.279280662537, 'total_duration': 21556.965598344803, 'accumulated_submission_time': 19783.279280662537, 'accumulated_eval_time': 1769.8629813194275, 'accumulated_logging_time': 1.42185640335083}
I0202 00:30:41.661488 139923852027648 logging_writer.py:48] [42732] accumulated_eval_time=1769.862981, accumulated_logging_time=1.421856, accumulated_submission_time=19783.279281, global_step=42732, preemption_count=0, score=19783.279281, test/accuracy=0.446600, test/loss=2.512555, test/num_examples=10000, total_duration=21556.965598, train/accuracy=0.603379, train/loss=1.661390, validation/accuracy=0.557940, validation/loss=1.870342, validation/num_examples=50000
I0202 00:31:08.760419 139923868813056 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5860666036605835, loss=4.516199111938477
I0202 00:31:55.197406 139923852027648 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7430312633514404, loss=2.7955353260040283
I0202 00:32:42.440082 139923868813056 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.7466078996658325, loss=2.685166358947754
I0202 00:33:29.128857 139923852027648 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.7022987604141235, loss=2.6437270641326904
I0202 00:34:15.846169 139923868813056 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.4735058546066284, loss=3.237549304962158
I0202 00:35:02.559693 139923852027648 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7071408033370972, loss=2.8471100330352783
I0202 00:35:49.397020 139923868813056 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.6390950679779053, loss=2.7545390129089355
I0202 00:36:36.106097 139923852027648 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.960005521774292, loss=2.767857551574707
I0202 00:37:22.867548 139923868813056 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.68856680393219, loss=2.789186477661133
I0202 00:37:41.680928 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:37:51.999176 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:38:15.162971 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:38:16.804759 140085747812160 submission_runner.py:408] Time since start: 22012.13s, 	Step: 43642, 	{'train/accuracy': 0.6136132478713989, 'train/loss': 1.6449350118637085, 'validation/accuracy': 0.5623199939727783, 'validation/loss': 1.889671802520752, 'validation/num_examples': 50000, 'test/accuracy': 0.44210001826286316, 'test/loss': 2.542107105255127, 'test/num_examples': 10000, 'score': 20203.238243579865, 'total_duration': 22012.131967306137, 'accumulated_submission_time': 20203.238243579865, 'accumulated_eval_time': 1804.9868636131287, 'accumulated_logging_time': 1.4544131755828857}
I0202 00:38:16.829666 139923852027648 logging_writer.py:48] [43642] accumulated_eval_time=1804.986864, accumulated_logging_time=1.454413, accumulated_submission_time=20203.238244, global_step=43642, preemption_count=0, score=20203.238244, test/accuracy=0.442100, test/loss=2.542107, test/num_examples=10000, total_duration=22012.131967, train/accuracy=0.613613, train/loss=1.644935, validation/accuracy=0.562320, validation/loss=1.889672, validation/num_examples=50000
I0202 00:38:39.986255 139923868813056 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.2274396419525146, loss=4.789087295532227
I0202 00:39:25.967749 139923852027648 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.8663631677627563, loss=2.8422367572784424
I0202 00:40:12.974905 139923868813056 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7413707971572876, loss=2.7987377643585205
I0202 00:40:59.722867 139923852027648 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7284131050109863, loss=2.770127773284912
I0202 00:41:46.768364 139923868813056 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.3983403444290161, loss=4.5768585205078125
I0202 00:42:33.973560 139923852027648 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.6707911491394043, loss=2.610487461090088
I0202 00:43:20.975795 139923868813056 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.6843211650848389, loss=2.796187400817871
I0202 00:44:08.033532 139923852027648 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.4650002717971802, loss=4.193523406982422
I0202 00:44:54.495314 139923868813056 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.842821717262268, loss=2.6849899291992188
I0202 00:45:17.017165 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:45:27.315377 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:45:49.764916 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:45:51.411466 140085747812160 submission_runner.py:408] Time since start: 22466.74s, 	Step: 44550, 	{'train/accuracy': 0.6031249761581421, 'train/loss': 1.6811929941177368, 'validation/accuracy': 0.5647599697113037, 'validation/loss': 1.8677431344985962, 'validation/num_examples': 50000, 'test/accuracy': 0.4440000355243683, 'test/loss': 2.530787944793701, 'test/num_examples': 10000, 'score': 20623.362012386322, 'total_duration': 22466.73867583275, 'accumulated_submission_time': 20623.362012386322, 'accumulated_eval_time': 1839.3811695575714, 'accumulated_logging_time': 1.4913108348846436}
I0202 00:45:51.434956 139923852027648 logging_writer.py:48] [44550] accumulated_eval_time=1839.381170, accumulated_logging_time=1.491311, accumulated_submission_time=20623.362012, global_step=44550, preemption_count=0, score=20623.362012, test/accuracy=0.444000, test/loss=2.530788, test/num_examples=10000, total_duration=22466.738676, train/accuracy=0.603125, train/loss=1.681193, validation/accuracy=0.564760, validation/loss=1.867743, validation/num_examples=50000
I0202 00:46:11.445713 139923868813056 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.8443552255630493, loss=2.775681495666504
I0202 00:46:56.473521 139923852027648 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6049364805221558, loss=3.4213151931762695
I0202 00:47:43.387280 139923868813056 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.6217069625854492, loss=4.5242414474487305
I0202 00:48:30.185049 139923852027648 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7132803201675415, loss=2.698751449584961
I0202 00:49:16.919213 139923868813056 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.3498600721359253, loss=4.798344612121582
I0202 00:50:03.672735 139923852027648 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.5026193857192993, loss=4.523445129394531
I0202 00:50:50.362754 139923868813056 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.4618396759033203, loss=5.407558917999268
I0202 00:51:37.200673 139923852027648 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.7158102989196777, loss=2.9009013175964355
I0202 00:52:23.962206 139923868813056 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.6311311721801758, loss=2.7880475521087646
I0202 00:52:51.796154 140085747812160 spec.py:321] Evaluating on the training split.
I0202 00:53:02.155066 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 00:53:24.277949 140085747812160 spec.py:349] Evaluating on the test split.
I0202 00:53:25.916353 140085747812160 submission_runner.py:408] Time since start: 22921.24s, 	Step: 45461, 	{'train/accuracy': 0.6108788847923279, 'train/loss': 1.6417521238327026, 'validation/accuracy': 0.567579984664917, 'validation/loss': 1.847375750541687, 'validation/num_examples': 50000, 'test/accuracy': 0.4482000172138214, 'test/loss': 2.4963107109069824, 'test/num_examples': 10000, 'score': 21043.66229391098, 'total_duration': 22921.24355506897, 'accumulated_submission_time': 21043.66229391098, 'accumulated_eval_time': 1873.5013628005981, 'accumulated_logging_time': 1.5242390632629395}
I0202 00:53:25.943260 139923852027648 logging_writer.py:48] [45461] accumulated_eval_time=1873.501363, accumulated_logging_time=1.524239, accumulated_submission_time=21043.662294, global_step=45461, preemption_count=0, score=21043.662294, test/accuracy=0.448200, test/loss=2.496311, test/num_examples=10000, total_duration=22921.243555, train/accuracy=0.610879, train/loss=1.641752, validation/accuracy=0.567580, validation/loss=1.847376, validation/num_examples=50000
I0202 00:53:41.649263 139923868813056 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.7252541780471802, loss=2.6355509757995605
I0202 00:54:26.208041 139923852027648 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.678022861480713, loss=2.5788862705230713
I0202 00:55:12.884082 139923868813056 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.2824881076812744, loss=5.174908638000488
I0202 00:55:59.454021 139923852027648 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.3342121839523315, loss=4.653911590576172
I0202 00:56:46.098387 139923868813056 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.656166434288025, loss=2.7544217109680176
I0202 00:57:32.867292 139923852027648 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.4921313524246216, loss=4.904113292694092
I0202 00:58:19.642712 139923868813056 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.7193872928619385, loss=2.7153453826904297
I0202 00:59:06.433662 139923852027648 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.6659051179885864, loss=2.724121332168579
I0202 00:59:53.443228 139923868813056 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6294680833816528, loss=3.549724578857422
I0202 01:00:26.016006 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:00:36.384626 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:01:02.916468 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:01:04.561955 140085747812160 submission_runner.py:408] Time since start: 23379.89s, 	Step: 46372, 	{'train/accuracy': 0.6156249642372131, 'train/loss': 1.6239218711853027, 'validation/accuracy': 0.5699399709701538, 'validation/loss': 1.8485853672027588, 'validation/num_examples': 50000, 'test/accuracy': 0.44610002636909485, 'test/loss': 2.515526056289673, 'test/num_examples': 10000, 'score': 21463.673819065094, 'total_duration': 23379.889157772064, 'accumulated_submission_time': 21463.673819065094, 'accumulated_eval_time': 1912.0473115444183, 'accumulated_logging_time': 1.5611541271209717}
I0202 01:01:04.584905 139923852027648 logging_writer.py:48] [46372] accumulated_eval_time=1912.047312, accumulated_logging_time=1.561154, accumulated_submission_time=21463.673819, global_step=46372, preemption_count=0, score=21463.673819, test/accuracy=0.446100, test/loss=2.515526, test/num_examples=10000, total_duration=23379.889158, train/accuracy=0.615625, train/loss=1.623922, validation/accuracy=0.569940, validation/loss=1.848585, validation/num_examples=50000
I0202 01:01:15.964070 139923868813056 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.7133671045303345, loss=2.6480295658111572
I0202 01:01:59.557487 139923852027648 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6622856855392456, loss=2.861865997314453
I0202 01:02:46.473337 139923868813056 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6475428342819214, loss=2.548046112060547
I0202 01:03:32.959270 139923852027648 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8800125122070312, loss=2.759765625
I0202 01:04:19.601089 139923868813056 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.2158401012420654, loss=5.114592552185059
I0202 01:05:06.149339 139923852027648 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.5302568674087524, loss=4.324436187744141
I0202 01:05:52.830116 139923868813056 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.399336814880371, loss=4.190271854400635
I0202 01:06:39.307919 139923852027648 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7086304426193237, loss=2.7367494106292725
I0202 01:07:26.167440 139923868813056 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.720418930053711, loss=2.626959800720215
I0202 01:08:04.822514 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:08:15.359303 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:08:37.223826 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:08:38.872543 140085747812160 submission_runner.py:408] Time since start: 23834.20s, 	Step: 47284, 	{'train/accuracy': 0.6090039014816284, 'train/loss': 1.6575889587402344, 'validation/accuracy': 0.5688999891281128, 'validation/loss': 1.8510668277740479, 'validation/num_examples': 50000, 'test/accuracy': 0.44610002636909485, 'test/loss': 2.4953420162200928, 'test/num_examples': 10000, 'score': 21883.85037612915, 'total_duration': 23834.19975042343, 'accumulated_submission_time': 21883.85037612915, 'accumulated_eval_time': 1946.0973546504974, 'accumulated_logging_time': 1.593156099319458}
I0202 01:08:38.898745 139923852027648 logging_writer.py:48] [47284] accumulated_eval_time=1946.097355, accumulated_logging_time=1.593156, accumulated_submission_time=21883.850376, global_step=47284, preemption_count=0, score=21883.850376, test/accuracy=0.446100, test/loss=2.495342, test/num_examples=10000, total_duration=23834.199750, train/accuracy=0.609004, train/loss=1.657589, validation/accuracy=0.568900, validation/loss=1.851067, validation/num_examples=50000
I0202 01:08:45.579809 139923868813056 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.5912878513336182, loss=4.670756816864014
I0202 01:09:28.046688 139923852027648 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.2763900756835938, loss=5.301762104034424
I0202 01:10:14.893366 139923868813056 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6515297889709473, loss=2.918498992919922
I0202 01:11:01.619115 139923852027648 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.5881476402282715, loss=2.757697820663452
I0202 01:11:48.129799 139923868813056 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.5543593168258667, loss=3.9154505729675293
I0202 01:12:34.907839 139923852027648 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.4995121955871582, loss=3.6263537406921387
I0202 01:13:21.468763 139923868813056 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.822061538696289, loss=2.681309938430786
I0202 01:14:08.381042 139923852027648 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.668413519859314, loss=3.7041475772857666
I0202 01:14:54.818183 139923868813056 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.758678913116455, loss=2.7457387447357178
I0202 01:15:39.305637 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:15:49.698563 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:16:14.264303 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:16:15.913738 140085747812160 submission_runner.py:408] Time since start: 24291.24s, 	Step: 48197, 	{'train/accuracy': 0.611621081829071, 'train/loss': 1.6552009582519531, 'validation/accuracy': 0.5723199844360352, 'validation/loss': 1.836769938468933, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.478607177734375, 'test/num_examples': 10000, 'score': 22304.195620536804, 'total_duration': 24291.240940332413, 'accumulated_submission_time': 22304.195620536804, 'accumulated_eval_time': 1982.7054772377014, 'accumulated_logging_time': 1.6298718452453613}
I0202 01:16:15.940756 139923852027648 logging_writer.py:48] [48197] accumulated_eval_time=1982.705477, accumulated_logging_time=1.629872, accumulated_submission_time=22304.195621, global_step=48197, preemption_count=0, score=22304.195621, test/accuracy=0.458000, test/loss=2.478607, test/num_examples=10000, total_duration=24291.240940, train/accuracy=0.611621, train/loss=1.655201, validation/accuracy=0.572320, validation/loss=1.836770, validation/num_examples=50000
I0202 01:16:17.514150 139923868813056 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.7940243482589722, loss=2.9820592403411865
I0202 01:16:59.233909 139923852027648 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.8784409761428833, loss=2.813659191131592
I0202 01:17:45.766757 139923868813056 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.6910728216171265, loss=2.7659833431243896
I0202 01:18:32.652301 139923852027648 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.802289366722107, loss=2.5278077125549316
I0202 01:19:19.513119 139923868813056 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.669519066810608, loss=2.738968849182129
I0202 01:20:06.134606 139923852027648 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.9809448719024658, loss=2.6253767013549805
I0202 01:20:52.885268 139923868813056 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.6427158117294312, loss=2.616176128387451
I0202 01:21:39.525091 139923852027648 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6627506017684937, loss=3.5641822814941406
I0202 01:22:26.029842 139923868813056 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.6519566774368286, loss=4.16175651550293
I0202 01:23:13.108256 139923852027648 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.746541976928711, loss=2.6722629070281982
I0202 01:23:16.004943 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:23:27.108522 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:23:55.614669 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:23:57.258509 140085747812160 submission_runner.py:408] Time since start: 24752.59s, 	Step: 49108, 	{'train/accuracy': 0.6163281202316284, 'train/loss': 1.6063055992126465, 'validation/accuracy': 0.5687999725341797, 'validation/loss': 1.8405550718307495, 'validation/num_examples': 50000, 'test/accuracy': 0.4492000341415405, 'test/loss': 2.4828150272369385, 'test/num_examples': 10000, 'score': 22724.199105739594, 'total_duration': 24752.58571910858, 'accumulated_submission_time': 22724.199105739594, 'accumulated_eval_time': 2023.9590499401093, 'accumulated_logging_time': 1.666623592376709}
I0202 01:23:57.285720 139923868813056 logging_writer.py:48] [49108] accumulated_eval_time=2023.959050, accumulated_logging_time=1.666624, accumulated_submission_time=22724.199106, global_step=49108, preemption_count=0, score=22724.199106, test/accuracy=0.449200, test/loss=2.482815, test/num_examples=10000, total_duration=24752.585719, train/accuracy=0.616328, train/loss=1.606306, validation/accuracy=0.568800, validation/loss=1.840555, validation/num_examples=50000
I0202 01:24:35.420362 139923852027648 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.6953058242797852, loss=2.6693079471588135
I0202 01:25:22.198918 139923868813056 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.4407119750976562, loss=4.212642669677734
I0202 01:26:09.170692 139923852027648 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.9197827577590942, loss=2.725619077682495
I0202 01:26:55.864580 139923868813056 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.639251708984375, loss=2.5046238899230957
I0202 01:27:42.547814 139923852027648 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.5285974740982056, loss=5.250279903411865
I0202 01:28:29.471653 139923868813056 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.954275131225586, loss=2.633995294570923
I0202 01:29:16.151350 139923852027648 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.688101887702942, loss=2.564120054244995
I0202 01:30:02.729869 139923868813056 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.5732358694076538, loss=2.8557491302490234
I0202 01:30:49.342749 139923852027648 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.825137972831726, loss=2.6812922954559326
I0202 01:30:57.389683 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:31:08.127087 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:31:30.884359 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:31:32.528636 140085747812160 submission_runner.py:408] Time since start: 25207.86s, 	Step: 50019, 	{'train/accuracy': 0.6085546612739563, 'train/loss': 1.6156573295593262, 'validation/accuracy': 0.5709399580955505, 'validation/loss': 1.8096331357955933, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.4651927947998047, 'test/num_examples': 10000, 'score': 23144.242423772812, 'total_duration': 25207.855845689774, 'accumulated_submission_time': 23144.242423772812, 'accumulated_eval_time': 2059.097989797592, 'accumulated_logging_time': 1.7026152610778809}
I0202 01:31:32.553088 139923868813056 logging_writer.py:48] [50019] accumulated_eval_time=2059.097990, accumulated_logging_time=1.702615, accumulated_submission_time=23144.242424, global_step=50019, preemption_count=0, score=23144.242424, test/accuracy=0.454200, test/loss=2.465193, test/num_examples=10000, total_duration=25207.855846, train/accuracy=0.608555, train/loss=1.615657, validation/accuracy=0.570940, validation/loss=1.809633, validation/num_examples=50000
I0202 01:32:05.656374 139923852027648 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.3414533138275146, loss=5.1705803871154785
I0202 01:32:51.859764 139923868813056 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.8795846700668335, loss=2.6120481491088867
I0202 01:33:38.797880 139923852027648 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.804011583328247, loss=3.0626673698425293
I0202 01:34:25.482132 139923868813056 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.3958262205123901, loss=5.268229007720947
I0202 01:35:12.298570 139923852027648 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.9247729778289795, loss=2.781399965286255
I0202 01:35:58.765694 139923868813056 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8366936445236206, loss=2.6900625228881836
I0202 01:36:45.541072 139923852027648 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.5052459239959717, loss=3.5438108444213867
I0202 01:37:32.552624 139923868813056 logging_writer.py:48] [50800] global_step=50800, grad_norm=2.010244607925415, loss=2.5073788166046143
I0202 01:38:19.212110 139923852027648 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.9064841270446777, loss=2.5927348136901855
I0202 01:38:32.836418 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:38:43.233453 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:39:04.505790 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:39:06.167872 140085747812160 submission_runner.py:408] Time since start: 25661.50s, 	Step: 50931, 	{'train/accuracy': 0.6145898103713989, 'train/loss': 1.6268950700759888, 'validation/accuracy': 0.5734800100326538, 'validation/loss': 1.8265553712844849, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.4841372966766357, 'test/num_examples': 10000, 'score': 23564.4644010067, 'total_duration': 25661.495080709457, 'accumulated_submission_time': 23564.4644010067, 'accumulated_eval_time': 2092.429440975189, 'accumulated_logging_time': 1.737779140472412}
I0202 01:39:06.191558 139923868813056 logging_writer.py:48] [50931] accumulated_eval_time=2092.429441, accumulated_logging_time=1.737779, accumulated_submission_time=23564.464401, global_step=50931, preemption_count=0, score=23564.464401, test/accuracy=0.453700, test/loss=2.484137, test/num_examples=10000, total_duration=25661.495081, train/accuracy=0.614590, train/loss=1.626895, validation/accuracy=0.573480, validation/loss=1.826555, validation/num_examples=50000
I0202 01:39:33.888582 139923852027648 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.759222149848938, loss=2.680138111114502
I0202 01:40:20.491286 139923868813056 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8086977005004883, loss=2.575561046600342
I0202 01:41:07.511710 139923852027648 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.4825432300567627, loss=4.64896821975708
I0202 01:41:54.054362 139923868813056 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8553459644317627, loss=2.6162095069885254
I0202 01:42:41.034544 139923852027648 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7301549911499023, loss=2.8684680461883545
I0202 01:43:27.475053 139923868813056 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.857283353805542, loss=2.59905743598938
I0202 01:44:14.437594 139923852027648 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.3049331903457642, loss=5.2751030921936035
I0202 01:45:01.116767 139923868813056 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.6214559078216553, loss=3.513314962387085
I0202 01:45:47.792220 139923852027648 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.3412251472473145, loss=4.870785236358643
I0202 01:46:06.614011 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:46:17.096585 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:46:45.303056 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:46:46.949038 140085747812160 submission_runner.py:408] Time since start: 26122.28s, 	Step: 51842, 	{'train/accuracy': 0.619140625, 'train/loss': 1.6129734516143799, 'validation/accuracy': 0.5745399594306946, 'validation/loss': 1.8239986896514893, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.465365409851074, 'test/num_examples': 10000, 'score': 23984.826218366623, 'total_duration': 26122.27624464035, 'accumulated_submission_time': 23984.826218366623, 'accumulated_eval_time': 2132.76446890831, 'accumulated_logging_time': 1.7713980674743652}
I0202 01:46:46.974227 139923868813056 logging_writer.py:48] [51842] accumulated_eval_time=2132.764469, accumulated_logging_time=1.771398, accumulated_submission_time=23984.826218, global_step=51842, preemption_count=0, score=23984.826218, test/accuracy=0.458000, test/loss=2.465365, test/num_examples=10000, total_duration=26122.276245, train/accuracy=0.619141, train/loss=1.612973, validation/accuracy=0.574540, validation/loss=1.823999, validation/num_examples=50000
I0202 01:47:10.128002 139923852027648 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.970879077911377, loss=2.52656888961792
I0202 01:47:56.180249 139923868813056 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.0869109630584717, loss=3.2268710136413574
I0202 01:48:42.938313 139923852027648 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.3680278062820435, loss=4.726505279541016
I0202 01:49:29.631957 139923868813056 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.8545259237289429, loss=2.4455881118774414
I0202 01:50:16.481319 139923852027648 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8891215324401855, loss=2.5564088821411133
I0202 01:51:03.250942 139923868813056 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.5765877962112427, loss=3.816521644592285
I0202 01:51:49.853407 139923852027648 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7005164623260498, loss=3.1740684509277344
I0202 01:52:37.015915 139923868813056 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.552255630493164, loss=4.888283729553223
I0202 01:53:23.593951 139923852027648 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.3798987865447998, loss=5.198394298553467
I0202 01:53:47.182594 140085747812160 spec.py:321] Evaluating on the training split.
I0202 01:53:57.501660 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 01:54:26.081717 140085747812160 spec.py:349] Evaluating on the test split.
I0202 01:54:27.714082 140085747812160 submission_runner.py:408] Time since start: 26583.04s, 	Step: 52752, 	{'train/accuracy': 0.6456249952316284, 'train/loss': 1.4840998649597168, 'validation/accuracy': 0.5798799991607666, 'validation/loss': 1.7855687141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4369454383850098, 'test/num_examples': 10000, 'score': 24404.973826408386, 'total_duration': 26583.041292905807, 'accumulated_submission_time': 24404.973826408386, 'accumulated_eval_time': 2173.295962333679, 'accumulated_logging_time': 1.8063812255859375}
I0202 01:54:27.740148 139923868813056 logging_writer.py:48] [52752] accumulated_eval_time=2173.295962, accumulated_logging_time=1.806381, accumulated_submission_time=24404.973826, global_step=52752, preemption_count=0, score=24404.973826, test/accuracy=0.461100, test/loss=2.436945, test/num_examples=10000, total_duration=26583.041293, train/accuracy=0.645625, train/loss=1.484100, validation/accuracy=0.579880, validation/loss=1.785569, validation/num_examples=50000
I0202 01:54:46.962141 139923852027648 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.5452133417129517, loss=3.041666030883789
I0202 01:55:32.336204 139923868813056 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.4183287620544434, loss=3.767031192779541
I0202 01:56:19.174183 139923852027648 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.8121081590652466, loss=2.828613758087158
I0202 01:57:05.917532 139923868813056 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.8814018964767456, loss=2.8396546840667725
I0202 01:57:52.563012 139923852027648 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7156299352645874, loss=2.8407135009765625
I0202 01:58:39.158041 139923868813056 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.5353690385818481, loss=3.239737033843994
I0202 01:59:25.792751 139923852027648 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.5798735618591309, loss=3.096656322479248
I0202 02:00:12.396444 139923868813056 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.5123177766799927, loss=5.323856830596924
I0202 02:00:59.277095 139923852027648 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3987784385681152, loss=4.144754886627197
I0202 02:01:27.890614 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:01:38.314871 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:02:01.007308 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:02:02.664546 140085747812160 submission_runner.py:408] Time since start: 27037.99s, 	Step: 53663, 	{'train/accuracy': 0.6150586009025574, 'train/loss': 1.6262010335922241, 'validation/accuracy': 0.5754199624061584, 'validation/loss': 1.8177956342697144, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.470672130584717, 'test/num_examples': 10000, 'score': 24825.06359386444, 'total_duration': 27037.99173426628, 'accumulated_submission_time': 24825.06359386444, 'accumulated_eval_time': 2208.069890022278, 'accumulated_logging_time': 1.8420429229736328}
I0202 02:02:02.695631 139923868813056 logging_writer.py:48] [53663] accumulated_eval_time=2208.069890, accumulated_logging_time=1.842043, accumulated_submission_time=24825.063594, global_step=53663, preemption_count=0, score=24825.063594, test/accuracy=0.455900, test/loss=2.470672, test/num_examples=10000, total_duration=27037.991734, train/accuracy=0.615059, train/loss=1.626201, validation/accuracy=0.575420, validation/loss=1.817796, validation/num_examples=50000
I0202 02:02:17.614293 139923852027648 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.639505386352539, loss=2.669969081878662
I0202 02:03:02.019826 139923868813056 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.4864106178283691, loss=4.977136611938477
I0202 02:03:48.855489 139923852027648 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.3720016479492188, loss=4.608649730682373
I0202 02:04:36.086004 139923868813056 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.440185308456421, loss=3.7846481800079346
I0202 02:05:22.807149 139923852027648 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.7562731504440308, loss=2.607356071472168
I0202 02:06:09.550969 139923868813056 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8767443895339966, loss=2.6885554790496826
I0202 02:06:56.030181 139923852027648 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.6012589931488037, loss=3.1657752990722656
I0202 02:07:42.810472 139923868813056 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.9025976657867432, loss=2.8350515365600586
I0202 02:08:29.512461 139923852027648 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.6734882593154907, loss=2.442162036895752
I0202 02:09:02.773901 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:09:13.193955 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:09:40.783406 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:09:42.426600 140085747812160 submission_runner.py:408] Time since start: 27497.75s, 	Step: 54573, 	{'train/accuracy': 0.6263281106948853, 'train/loss': 1.5781675577163696, 'validation/accuracy': 0.5829600095748901, 'validation/loss': 1.770058512687683, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.430996894836426, 'test/num_examples': 10000, 'score': 25245.0795879364, 'total_duration': 27497.753811597824, 'accumulated_submission_time': 25245.0795879364, 'accumulated_eval_time': 2247.7225930690765, 'accumulated_logging_time': 1.88374662399292}
I0202 02:09:42.452204 139923868813056 logging_writer.py:48] [54573] accumulated_eval_time=2247.722593, accumulated_logging_time=1.883747, accumulated_submission_time=25245.079588, global_step=54573, preemption_count=0, score=25245.079588, test/accuracy=0.460900, test/loss=2.430997, test/num_examples=10000, total_duration=27497.753812, train/accuracy=0.626328, train/loss=1.578168, validation/accuracy=0.582960, validation/loss=1.770059, validation/num_examples=50000
I0202 02:09:53.435082 139923852027648 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.6838363409042358, loss=2.6269946098327637
I0202 02:10:37.237375 139923868813056 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.7985458374023438, loss=2.941624641418457
I0202 02:11:24.225374 139923852027648 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.6538885831832886, loss=3.533238172531128
I0202 02:12:10.839725 139923868813056 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9846961498260498, loss=2.7253060340881348
I0202 02:12:57.622979 139923852027648 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.5585887432098389, loss=2.4926419258117676
I0202 02:13:44.501945 139923868813056 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.5136271715164185, loss=3.6830475330352783
I0202 02:14:30.906831 139923852027648 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9912786483764648, loss=2.6881215572357178
I0202 02:15:17.600883 139923868813056 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.744431734085083, loss=2.6995513439178467
I0202 02:16:04.409214 139923852027648 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8888285160064697, loss=2.606562376022339
I0202 02:16:42.832632 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:16:53.141778 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:17:21.774330 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:17:23.424241 140085747812160 submission_runner.py:408] Time since start: 27958.75s, 	Step: 55484, 	{'train/accuracy': 0.6351562142372131, 'train/loss': 1.5078805685043335, 'validation/accuracy': 0.5780400037765503, 'validation/loss': 1.778692603111267, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4479312896728516, 'test/num_examples': 10000, 'score': 25665.398136615753, 'total_duration': 27958.751446008682, 'accumulated_submission_time': 25665.398136615753, 'accumulated_eval_time': 2288.314208507538, 'accumulated_logging_time': 1.9204604625701904}
I0202 02:17:23.452273 139923868813056 logging_writer.py:48] [55484] accumulated_eval_time=2288.314209, accumulated_logging_time=1.920460, accumulated_submission_time=25665.398137, global_step=55484, preemption_count=0, score=25665.398137, test/accuracy=0.461100, test/loss=2.447931, test/num_examples=10000, total_duration=27958.751446, train/accuracy=0.635156, train/loss=1.507881, validation/accuracy=0.578040, validation/loss=1.778693, validation/num_examples=50000
I0202 02:17:30.113392 139923852027648 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.303144097328186, loss=5.249355316162109
I0202 02:18:12.917988 139923868813056 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.673991084098816, loss=2.7938613891601562
I0202 02:18:59.791513 139923852027648 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.4432834386825562, loss=4.0933332443237305
I0202 02:19:46.656721 139923868813056 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.5031260251998901, loss=4.1556477546691895
I0202 02:20:33.465008 139923852027648 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.7759026288986206, loss=2.6113972663879395
I0202 02:21:20.329297 139923868813056 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.39436936378479, loss=5.040287971496582
I0202 02:22:07.114897 139923852027648 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.7105743885040283, loss=2.879701614379883
I0202 02:22:53.875463 139923868813056 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.8685343265533447, loss=2.749797821044922
I0202 02:23:40.822024 139923852027648 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.3595449924468994, loss=3.978482961654663
I0202 02:24:23.683139 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:24:34.040283 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:25:01.685714 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:25:03.330184 140085747812160 submission_runner.py:408] Time since start: 28418.66s, 	Step: 56394, 	{'train/accuracy': 0.6251562237739563, 'train/loss': 1.5547682046890259, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 1.764073371887207, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.4148685932159424, 'test/num_examples': 10000, 'score': 26085.56778717041, 'total_duration': 28418.657384634018, 'accumulated_submission_time': 26085.56778717041, 'accumulated_eval_time': 2327.961247444153, 'accumulated_logging_time': 1.9579541683197021}
I0202 02:25:03.359749 139923868813056 logging_writer.py:48] [56394] accumulated_eval_time=2327.961247, accumulated_logging_time=1.957954, accumulated_submission_time=26085.567787, global_step=56394, preemption_count=0, score=26085.567787, test/accuracy=0.460900, test/loss=2.414869, test/num_examples=10000, total_duration=28418.657385, train/accuracy=0.625156, train/loss=1.554768, validation/accuracy=0.582120, validation/loss=1.764073, validation/num_examples=50000
I0202 02:25:06.104473 139923852027648 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.9045984745025635, loss=2.725795269012451
I0202 02:25:48.273820 139923868813056 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.7910981178283691, loss=3.298483371734619
I0202 02:26:35.266901 139923852027648 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.9368476867675781, loss=2.5494256019592285
I0202 02:27:22.151565 139923868813056 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9173331260681152, loss=2.5880634784698486
I0202 02:28:08.976112 139923852027648 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.0587258338928223, loss=2.6685996055603027
I0202 02:28:55.620773 139923868813056 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.088186502456665, loss=2.626195192337036
I0202 02:29:42.243489 139923852027648 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8113391399383545, loss=2.5643045902252197
I0202 02:30:29.043907 139923868813056 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.9367457628250122, loss=2.6404356956481934
I0202 02:31:15.872482 139923852027648 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.5801440477371216, loss=3.1605312824249268
I0202 02:32:02.658428 139923868813056 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.6270841360092163, loss=2.8034071922302246
I0202 02:32:03.763758 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:32:14.008213 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:32:38.321078 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:32:39.956532 140085747812160 submission_runner.py:408] Time since start: 28875.28s, 	Step: 57304, 	{'train/accuracy': 0.6250194907188416, 'train/loss': 1.5971978902816772, 'validation/accuracy': 0.5831599831581116, 'validation/loss': 1.794812798500061, 'validation/num_examples': 50000, 'test/accuracy': 0.46240001916885376, 'test/loss': 2.4551639556884766, 'test/num_examples': 10000, 'score': 26505.91107916832, 'total_duration': 28875.283742427826, 'accumulated_submission_time': 26505.91107916832, 'accumulated_eval_time': 2364.1540179252625, 'accumulated_logging_time': 1.9971649646759033}
I0202 02:32:39.981451 139923852027648 logging_writer.py:48] [57304] accumulated_eval_time=2364.154018, accumulated_logging_time=1.997165, accumulated_submission_time=26505.911079, global_step=57304, preemption_count=0, score=26505.911079, test/accuracy=0.462400, test/loss=2.455164, test/num_examples=10000, total_duration=28875.283742, train/accuracy=0.625019, train/loss=1.597198, validation/accuracy=0.583160, validation/loss=1.794813, validation/num_examples=50000
I0202 02:33:19.965000 139923868813056 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.6838973760604858, loss=2.9910178184509277
I0202 02:34:06.641848 139923852027648 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.9230101108551025, loss=2.4499855041503906
I0202 02:34:53.351682 139923868813056 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.4598474502563477, loss=4.172463417053223
I0202 02:35:39.809374 139923852027648 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.7675734758377075, loss=2.953847646713257
I0202 02:36:26.616692 139923868813056 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.7104820013046265, loss=2.7790422439575195
I0202 02:37:14.624886 139923852027648 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.7551391124725342, loss=2.4991655349731445
I0202 02:38:01.494542 139923868813056 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.4803097248077393, loss=5.24066162109375
I0202 02:38:48.342515 139923852027648 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2771068811416626, loss=5.181563854217529
I0202 02:39:35.321455 139923868813056 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.8801742792129517, loss=2.563859224319458
I0202 02:39:40.206089 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:39:50.846803 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:40:19.064518 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:40:20.698773 140085747812160 submission_runner.py:408] Time since start: 29336.03s, 	Step: 58212, 	{'train/accuracy': 0.6395898461341858, 'train/loss': 1.5047837495803833, 'validation/accuracy': 0.5877599716186523, 'validation/loss': 1.7553316354751587, 'validation/num_examples': 50000, 'test/accuracy': 0.4683000147342682, 'test/loss': 2.4041175842285156, 'test/num_examples': 10000, 'score': 26926.0757522583, 'total_duration': 29336.025983572006, 'accumulated_submission_time': 26926.0757522583, 'accumulated_eval_time': 2404.646691799164, 'accumulated_logging_time': 2.0316531658172607}
I0202 02:40:20.723459 139923852027648 logging_writer.py:48] [58212] accumulated_eval_time=2404.646692, accumulated_logging_time=2.031653, accumulated_submission_time=26926.075752, global_step=58212, preemption_count=0, score=26926.075752, test/accuracy=0.468300, test/loss=2.404118, test/num_examples=10000, total_duration=29336.025984, train/accuracy=0.639590, train/loss=1.504784, validation/accuracy=0.587760, validation/loss=1.755332, validation/num_examples=50000
I0202 02:40:57.412110 139923868813056 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8649346828460693, loss=2.6793415546417236
I0202 02:41:44.037555 139923852027648 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3427411317825317, loss=4.552003860473633
I0202 02:42:30.670666 139923868813056 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8644696474075317, loss=2.7695531845092773
I0202 02:43:17.486062 139923852027648 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.5630569458007812, loss=4.833414077758789
I0202 02:44:04.350701 139923868813056 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9849549531936646, loss=2.523408889770508
I0202 02:44:51.249120 139923852027648 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3169560432434082, loss=4.717721462249756
I0202 02:45:38.040884 139923868813056 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.2388763427734375, loss=5.008089542388916
I0202 02:46:24.872500 139923852027648 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8136777877807617, loss=2.664426803588867
I0202 02:47:11.686769 139923868813056 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.920591950416565, loss=2.4626264572143555
I0202 02:47:21.116891 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:47:31.442306 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:48:01.790908 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:48:03.434126 140085747812160 submission_runner.py:408] Time since start: 29798.76s, 	Step: 59122, 	{'train/accuracy': 0.6267773509025574, 'train/loss': 1.5606385469436646, 'validation/accuracy': 0.5853599905967712, 'validation/loss': 1.7512341737747192, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.407226085662842, 'test/num_examples': 10000, 'score': 27346.409598112106, 'total_duration': 29798.761336803436, 'accumulated_submission_time': 27346.409598112106, 'accumulated_eval_time': 2446.9639501571655, 'accumulated_logging_time': 2.0650646686553955}
I0202 02:48:03.460514 139923852027648 logging_writer.py:48] [59122] accumulated_eval_time=2446.963950, accumulated_logging_time=2.065065, accumulated_submission_time=27346.409598, global_step=59122, preemption_count=0, score=27346.409598, test/accuracy=0.466600, test/loss=2.407226, test/num_examples=10000, total_duration=29798.761337, train/accuracy=0.626777, train/loss=1.560639, validation/accuracy=0.585360, validation/loss=1.751234, validation/num_examples=50000
I0202 02:48:35.436936 139923868813056 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.932520866394043, loss=2.6008071899414062
I0202 02:49:21.953500 139923852027648 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.374908447265625, loss=4.431082725524902
I0202 02:50:08.856614 139923868813056 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.912842035293579, loss=2.605546712875366
I0202 02:50:55.253940 139923852027648 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.5441486835479736, loss=3.2563531398773193
I0202 02:51:41.996540 139923868813056 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.027766466140747, loss=2.635071039199829
I0202 02:52:28.902518 139923852027648 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9882113933563232, loss=2.9201161861419678
I0202 02:53:15.534916 139923868813056 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.3882248401641846, loss=5.061004638671875
I0202 02:54:02.428111 139923852027648 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.9012436866760254, loss=2.6674304008483887
I0202 02:54:49.257457 139923868813056 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.5167666673660278, loss=4.363447189331055
I0202 02:55:03.459289 140085747812160 spec.py:321] Evaluating on the training split.
I0202 02:55:13.901759 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 02:55:39.810324 140085747812160 spec.py:349] Evaluating on the test split.
I0202 02:55:41.447787 140085747812160 submission_runner.py:408] Time since start: 30256.77s, 	Step: 60032, 	{'train/accuracy': 0.6316601634025574, 'train/loss': 1.5331214666366577, 'validation/accuracy': 0.5899999737739563, 'validation/loss': 1.7361007928848267, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.382453680038452, 'test/num_examples': 10000, 'score': 27766.347306251526, 'total_duration': 30256.77499818802, 'accumulated_submission_time': 27766.347306251526, 'accumulated_eval_time': 2484.952459335327, 'accumulated_logging_time': 2.1022024154663086}
I0202 02:55:41.472539 139923852027648 logging_writer.py:48] [60032] accumulated_eval_time=2484.952459, accumulated_logging_time=2.102202, accumulated_submission_time=27766.347306, global_step=60032, preemption_count=0, score=27766.347306, test/accuracy=0.475200, test/loss=2.382454, test/num_examples=10000, total_duration=30256.774998, train/accuracy=0.631660, train/loss=1.533121, validation/accuracy=0.590000, validation/loss=1.736101, validation/num_examples=50000
I0202 02:56:08.672483 139923868813056 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.823225975036621, loss=2.6336216926574707
I0202 02:56:54.848595 139923852027648 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.801108717918396, loss=2.941650867462158
I0202 02:57:41.974288 139923868813056 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9395297765731812, loss=2.503772020339966
I0202 02:58:28.915265 139923852027648 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8250689506530762, loss=2.77423357963562
I0202 02:59:15.799740 139923868813056 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.7051820755004883, loss=3.441974401473999
I0202 03:00:02.542047 139923852027648 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.0422046184539795, loss=2.6556477546691895
I0202 03:00:49.244191 139923868813056 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.5307221412658691, loss=3.831338405609131
I0202 03:01:35.950758 139923852027648 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.9157061576843262, loss=2.6010847091674805
I0202 03:02:22.847649 139923868813056 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.5656611919403076, loss=5.155204772949219
I0202 03:02:41.489559 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:02:51.929508 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:03:19.735873 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:03:21.376119 140085747812160 submission_runner.py:408] Time since start: 30716.70s, 	Step: 60941, 	{'train/accuracy': 0.6475585699081421, 'train/loss': 1.4857375621795654, 'validation/accuracy': 0.5921599864959717, 'validation/loss': 1.733931541442871, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.3912408351898193, 'test/num_examples': 10000, 'score': 28186.30168414116, 'total_duration': 30716.703328847885, 'accumulated_submission_time': 28186.30168414116, 'accumulated_eval_time': 2524.839050769806, 'accumulated_logging_time': 2.1383426189422607}
I0202 03:03:21.403810 139923852027648 logging_writer.py:48] [60941] accumulated_eval_time=2524.839051, accumulated_logging_time=2.138343, accumulated_submission_time=28186.301684, global_step=60941, preemption_count=0, score=28186.301684, test/accuracy=0.471100, test/loss=2.391241, test/num_examples=10000, total_duration=30716.703329, train/accuracy=0.647559, train/loss=1.485738, validation/accuracy=0.592160, validation/loss=1.733932, validation/num_examples=50000
I0202 03:03:44.953001 139923868813056 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.5362969636917114, loss=3.2733466625213623
I0202 03:04:31.006767 139923852027648 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.7864876985549927, loss=2.402784585952759
I0202 03:05:18.088286 139923868813056 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.2941994667053223, loss=2.571563243865967
I0202 03:06:04.987039 139923852027648 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.505414366722107, loss=4.6794891357421875
I0202 03:06:51.715987 139923868813056 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.1724774837493896, loss=2.636279344558716
I0202 03:07:38.392611 139923852027648 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.1027069091796875, loss=2.712833881378174
I0202 03:08:25.127197 139923868813056 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.4717234373092651, loss=4.324957847595215
I0202 03:09:11.859570 139923852027648 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.7163654565811157, loss=2.6502180099487305
I0202 03:09:58.595475 139923868813056 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9966357946395874, loss=2.5878982543945312
I0202 03:10:21.740643 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:10:32.068102 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:11:00.204838 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:11:01.847087 140085747812160 submission_runner.py:408] Time since start: 31177.17s, 	Step: 61851, 	{'train/accuracy': 0.6359570026397705, 'train/loss': 1.5239770412445068, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7153202295303345, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.3749277591705322, 'test/num_examples': 10000, 'score': 28606.5775911808, 'total_duration': 31177.174296855927, 'accumulated_submission_time': 28606.5775911808, 'accumulated_eval_time': 2564.9455330371857, 'accumulated_logging_time': 2.1757090091705322}
I0202 03:11:01.875557 139923852027648 logging_writer.py:48] [61851] accumulated_eval_time=2564.945533, accumulated_logging_time=2.175709, accumulated_submission_time=28606.577591, global_step=61851, preemption_count=0, score=28606.577591, test/accuracy=0.468000, test/loss=2.374928, test/num_examples=10000, total_duration=31177.174297, train/accuracy=0.635957, train/loss=1.523977, validation/accuracy=0.593660, validation/loss=1.715320, validation/num_examples=50000
I0202 03:11:21.474697 139923868813056 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.7566676139831543, loss=2.9450581073760986
I0202 03:12:07.033558 139923852027648 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.6486399173736572, loss=4.46426248550415
I0202 03:12:53.905757 139923868813056 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.7253960371017456, loss=2.6132376194000244
I0202 03:13:40.482267 139923852027648 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.4715566635131836, loss=4.629477500915527
I0202 03:14:27.287728 139923868813056 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.6603165864944458, loss=2.9949402809143066
I0202 03:15:13.952429 139923852027648 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.7440588474273682, loss=2.8281803131103516
I0202 03:16:00.763672 139923868813056 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.5213162899017334, loss=3.5633544921875
I0202 03:16:47.673993 139923852027648 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4682520627975464, loss=4.3263139724731445
I0202 03:17:34.446992 139923868813056 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.8529479503631592, loss=2.781773090362549
I0202 03:18:02.285505 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:18:12.434191 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:18:43.576432 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:18:45.216126 140085747812160 submission_runner.py:408] Time since start: 31640.54s, 	Step: 62761, 	{'train/accuracy': 0.6364062428474426, 'train/loss': 1.5226380825042725, 'validation/accuracy': 0.592960000038147, 'validation/loss': 1.7290226221084595, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.3883395195007324, 'test/num_examples': 10000, 'score': 29026.926725625992, 'total_duration': 31640.543329954147, 'accumulated_submission_time': 29026.926725625992, 'accumulated_eval_time': 2607.8761575222015, 'accumulated_logging_time': 2.214055299758911}
I0202 03:18:45.244787 139923852027648 logging_writer.py:48] [62761] accumulated_eval_time=2607.876158, accumulated_logging_time=2.214055, accumulated_submission_time=29026.926726, global_step=62761, preemption_count=0, score=29026.926726, test/accuracy=0.473100, test/loss=2.388340, test/num_examples=10000, total_duration=31640.543330, train/accuracy=0.636406, train/loss=1.522638, validation/accuracy=0.592960, validation/loss=1.729023, validation/num_examples=50000
I0202 03:19:00.935336 139923868813056 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.532999873161316, loss=5.167840957641602
I0202 03:19:45.495891 139923852027648 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.128647565841675, loss=2.519483804702759
I0202 03:20:32.603508 139923868813056 logging_writer.py:48] [63000] global_step=63000, grad_norm=2.023568630218506, loss=2.590998649597168
I0202 03:21:19.388314 139923852027648 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.876069188117981, loss=2.4395437240600586
I0202 03:22:06.112228 139923868813056 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.7362618446350098, loss=4.587812423706055
I0202 03:22:53.154522 139923852027648 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.560536503791809, loss=5.047003269195557
I0202 03:23:40.028462 139923868813056 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.5078063011169434, loss=5.127654075622559
I0202 03:24:26.766035 139923852027648 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.036358594894409, loss=2.4737956523895264
I0202 03:25:13.434443 139923868813056 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.683072566986084, loss=4.5568389892578125
I0202 03:25:45.399001 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:25:55.748371 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:26:19.470248 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:26:21.117722 140085747812160 submission_runner.py:408] Time since start: 32096.44s, 	Step: 63670, 	{'train/accuracy': 0.6468554735183716, 'train/loss': 1.4551808834075928, 'validation/accuracy': 0.5985000133514404, 'validation/loss': 1.690281867980957, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.3399128913879395, 'test/num_examples': 10000, 'score': 29447.01801109314, 'total_duration': 32096.44493150711, 'accumulated_submission_time': 29447.01801109314, 'accumulated_eval_time': 2643.5948944091797, 'accumulated_logging_time': 2.2544357776641846}
I0202 03:26:21.146048 139923852027648 logging_writer.py:48] [63670] accumulated_eval_time=2643.594894, accumulated_logging_time=2.254436, accumulated_submission_time=29447.018011, global_step=63670, preemption_count=0, score=29447.018011, test/accuracy=0.478600, test/loss=2.339913, test/num_examples=10000, total_duration=32096.444932, train/accuracy=0.646855, train/loss=1.455181, validation/accuracy=0.598500, validation/loss=1.690282, validation/num_examples=50000
I0202 03:26:33.313000 139923868813056 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9695205688476562, loss=2.65545654296875
I0202 03:27:17.099479 139923852027648 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.140648365020752, loss=2.538416862487793
I0202 03:28:03.677543 139923868813056 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.5273449420928955, loss=3.4655120372772217
I0202 03:28:50.595724 139923852027648 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.9421027898788452, loss=2.6049017906188965
I0202 03:29:37.447050 139923868813056 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.8492194414138794, loss=2.565617084503174
I0202 03:30:24.135818 139923852027648 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.7693992853164673, loss=3.8738481998443604
I0202 03:31:10.948635 139923868813056 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.6797316074371338, loss=2.3795864582061768
I0202 03:31:57.718880 139923852027648 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.2329132556915283, loss=2.6944825649261475
I0202 03:32:44.805721 139923868813056 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9847649335861206, loss=2.545548439025879
I0202 03:33:21.393469 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:33:31.532755 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:33:59.992669 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:34:01.631893 140085747812160 submission_runner.py:408] Time since start: 32556.96s, 	Step: 64580, 	{'train/accuracy': 0.6409569978713989, 'train/loss': 1.5243161916732788, 'validation/accuracy': 0.5908799767494202, 'validation/loss': 1.7485175132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.47040003538131714, 'test/loss': 2.3861141204833984, 'test/num_examples': 10000, 'score': 29867.20395731926, 'total_duration': 32556.95909023285, 'accumulated_submission_time': 29867.20395731926, 'accumulated_eval_time': 2683.8333218097687, 'accumulated_logging_time': 2.2939212322235107}
I0202 03:34:01.661276 139923852027648 logging_writer.py:48] [64580] accumulated_eval_time=2683.833322, accumulated_logging_time=2.293921, accumulated_submission_time=29867.203957, global_step=64580, preemption_count=0, score=29867.203957, test/accuracy=0.470400, test/loss=2.386114, test/num_examples=10000, total_duration=32556.959090, train/accuracy=0.640957, train/loss=1.524316, validation/accuracy=0.590880, validation/loss=1.748518, validation/num_examples=50000
I0202 03:34:09.893666 139923868813056 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.9157809019088745, loss=2.683152437210083
I0202 03:34:52.833451 139923852027648 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.7092533111572266, loss=4.893845558166504
I0202 03:35:39.697033 139923868813056 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0074448585510254, loss=2.6315300464630127
I0202 03:36:26.872889 139923852027648 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.8159046173095703, loss=3.1043269634246826
I0202 03:37:13.643181 139923868813056 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.799700379371643, loss=2.737013816833496
I0202 03:38:00.559648 139923852027648 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.904806137084961, loss=2.5458898544311523
I0202 03:38:47.408128 139923868813056 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8837577104568481, loss=2.56190824508667
I0202 03:39:34.223987 139923852027648 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9116885662078857, loss=2.4756674766540527
I0202 03:40:20.858201 139923868813056 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.012160301208496, loss=2.539340019226074
I0202 03:41:01.678541 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:41:12.099577 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:41:40.334249 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:41:41.988685 140085747812160 submission_runner.py:408] Time since start: 33017.32s, 	Step: 65489, 	{'train/accuracy': 0.63232421875, 'train/loss': 1.5296992063522339, 'validation/accuracy': 0.5950599908828735, 'validation/loss': 1.7079485654830933, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.3629984855651855, 'test/num_examples': 10000, 'score': 30287.157656669617, 'total_duration': 33017.315880060196, 'accumulated_submission_time': 30287.157656669617, 'accumulated_eval_time': 2724.1434757709503, 'accumulated_logging_time': 2.335206985473633}
I0202 03:41:42.017079 139923852027648 logging_writer.py:48] [65489] accumulated_eval_time=2724.143476, accumulated_logging_time=2.335207, accumulated_submission_time=30287.157657, global_step=65489, preemption_count=0, score=30287.157657, test/accuracy=0.476400, test/loss=2.362998, test/num_examples=10000, total_duration=33017.315880, train/accuracy=0.632324, train/loss=1.529699, validation/accuracy=0.595060, validation/loss=1.707949, validation/num_examples=50000
I0202 03:41:46.723122 139923868813056 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.884934663772583, loss=2.7145886421203613
I0202 03:42:29.471985 139923852027648 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.600733757019043, loss=3.7415318489074707
I0202 03:43:16.225639 139923868813056 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.6242384910583496, loss=2.996124505996704
I0202 03:44:02.952338 139923852027648 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.9653228521347046, loss=2.46748685836792
I0202 03:44:49.595828 139923868813056 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.5055115222930908, loss=5.155982971191406
I0202 03:45:36.341472 139923852027648 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.6290833950042725, loss=3.0695464611053467
I0202 03:46:23.108320 139923868813056 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4469660520553589, loss=4.82246732711792
I0202 03:47:09.712104 139923852027648 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.360230565071106, loss=4.307700157165527
I0202 03:47:56.632534 139923868813056 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.6526758670806885, loss=3.4916586875915527
I0202 03:48:42.024487 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:48:52.345442 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:49:18.305821 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:49:19.938521 140085747812160 submission_runner.py:408] Time since start: 33475.27s, 	Step: 66399, 	{'train/accuracy': 0.6452734470367432, 'train/loss': 1.4815714359283447, 'validation/accuracy': 0.5956999659538269, 'validation/loss': 1.7074819803237915, 'validation/num_examples': 50000, 'test/accuracy': 0.4799000322818756, 'test/loss': 2.360117197036743, 'test/num_examples': 10000, 'score': 30707.103160381317, 'total_duration': 33475.26572585106, 'accumulated_submission_time': 30707.103160381317, 'accumulated_eval_time': 2762.0575156211853, 'accumulated_logging_time': 2.3746211528778076}
I0202 03:49:19.968206 139923852027648 logging_writer.py:48] [66399] accumulated_eval_time=2762.057516, accumulated_logging_time=2.374621, accumulated_submission_time=30707.103160, global_step=66399, preemption_count=0, score=30707.103160, test/accuracy=0.479900, test/loss=2.360117, test/num_examples=10000, total_duration=33475.265726, train/accuracy=0.645273, train/loss=1.481571, validation/accuracy=0.595700, validation/loss=1.707482, validation/num_examples=50000
I0202 03:49:20.766219 139923868813056 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.539359450340271, loss=4.476033687591553
I0202 03:50:02.408971 139923852027648 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.5135854482650757, loss=5.020713806152344
I0202 03:50:49.128965 139923868813056 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.824155330657959, loss=2.497987985610962
I0202 03:51:36.081317 139923852027648 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0021188259124756, loss=2.5787734985351562
I0202 03:52:22.814126 139923868813056 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9957200288772583, loss=2.5873963832855225
I0202 03:53:09.849612 139923852027648 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.042954921722412, loss=2.5088775157928467
I0202 03:53:56.437494 139923868813056 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9078340530395508, loss=2.597773790359497
I0202 03:54:43.245044 139923852027648 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.7355186939239502, loss=2.9685943126678467
I0202 03:55:29.784457 139923868813056 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.7384707927703857, loss=2.8097169399261475
I0202 03:56:16.663924 139923852027648 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.828903079032898, loss=2.6616673469543457
I0202 03:56:20.105714 140085747812160 spec.py:321] Evaluating on the training split.
I0202 03:56:30.426076 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 03:57:00.028648 140085747812160 spec.py:349] Evaluating on the test split.
I0202 03:57:01.674942 140085747812160 submission_runner.py:408] Time since start: 33937.00s, 	Step: 67309, 	{'train/accuracy': 0.6687890291213989, 'train/loss': 1.3784257173538208, 'validation/accuracy': 0.5956799983978271, 'validation/loss': 1.7057280540466309, 'validation/num_examples': 50000, 'test/accuracy': 0.47280001640319824, 'test/loss': 2.3525753021240234, 'test/num_examples': 10000, 'score': 31127.178329706192, 'total_duration': 33937.002141714096, 'accumulated_submission_time': 31127.178329706192, 'accumulated_eval_time': 2803.6267223358154, 'accumulated_logging_time': 2.4154183864593506}
I0202 03:57:01.709334 139923868813056 logging_writer.py:48] [67309] accumulated_eval_time=2803.626722, accumulated_logging_time=2.415418, accumulated_submission_time=31127.178330, global_step=67309, preemption_count=0, score=31127.178330, test/accuracy=0.472800, test/loss=2.352575, test/num_examples=10000, total_duration=33937.002142, train/accuracy=0.668789, train/loss=1.378426, validation/accuracy=0.595680, validation/loss=1.705728, validation/num_examples=50000
I0202 03:57:39.774691 139923852027648 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.7748820781707764, loss=4.503531455993652
I0202 03:58:26.536608 139923868813056 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.8736042976379395, loss=2.589116096496582
I0202 03:59:13.515444 139923852027648 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.7255094051361084, loss=3.9069604873657227
I0202 04:00:00.046377 139923868813056 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.014596939086914, loss=2.7806713581085205
I0202 04:00:46.906627 139923852027648 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.9919984340667725, loss=2.4920341968536377
I0202 04:01:33.550540 139923868813056 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.4886584281921387, loss=3.7246880531311035
I0202 04:02:20.289277 139923852027648 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.8280116319656372, loss=2.782390832901001
I0202 04:03:07.261960 139923868813056 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9254344701766968, loss=2.5295019149780273
I0202 04:03:53.987412 139923852027648 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0617692470550537, loss=2.5133426189422607
I0202 04:04:02.091157 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:04:12.477669 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:04:42.314885 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:04:43.965683 140085747812160 submission_runner.py:408] Time since start: 34399.29s, 	Step: 68219, 	{'train/accuracy': 0.6403710842132568, 'train/loss': 1.5155787467956543, 'validation/accuracy': 0.5939199924468994, 'validation/loss': 1.7145018577575684, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.364424467086792, 'test/num_examples': 10000, 'score': 31547.497469186783, 'total_duration': 34399.2928917408, 'accumulated_submission_time': 31547.497469186783, 'accumulated_eval_time': 2845.501267194748, 'accumulated_logging_time': 2.461395263671875}
I0202 04:04:43.994179 139923868813056 logging_writer.py:48] [68219] accumulated_eval_time=2845.501267, accumulated_logging_time=2.461395, accumulated_submission_time=31547.497469, global_step=68219, preemption_count=0, score=31547.497469, test/accuracy=0.474500, test/loss=2.364424, test/num_examples=10000, total_duration=34399.292892, train/accuracy=0.640371, train/loss=1.515579, validation/accuracy=0.593920, validation/loss=1.714502, validation/num_examples=50000
I0202 04:05:17.115883 139923852027648 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.652141809463501, loss=4.077239036560059
I0202 04:06:03.649248 139923868813056 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.002922534942627, loss=2.5296735763549805
I0202 04:06:50.644091 139923852027648 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.5597310066223145, loss=4.301116466522217
I0202 04:07:37.615874 139923868813056 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.4177156686782837, loss=5.0834550857543945
I0202 04:08:24.228401 139923852027648 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.739758849143982, loss=3.2676970958709717
I0202 04:09:11.100720 139923868813056 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.9144062995910645, loss=2.482689619064331
I0202 04:09:57.535461 139923852027648 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0621864795684814, loss=2.6624417304992676
I0202 04:10:44.298335 139923868813056 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.5784977674484253, loss=3.4828639030456543
I0202 04:11:30.846752 139923852027648 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3876514434814453, loss=4.5036773681640625
I0202 04:11:44.132316 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:11:54.341490 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:12:22.725457 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:12:24.378502 140085747812160 submission_runner.py:408] Time since start: 34859.71s, 	Step: 69130, 	{'train/accuracy': 0.6488866806030273, 'train/loss': 1.4620481729507446, 'validation/accuracy': 0.6020599603652954, 'validation/loss': 1.6721429824829102, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.311363458633423, 'test/num_examples': 10000, 'score': 31967.57464170456, 'total_duration': 34859.705701351166, 'accumulated_submission_time': 31967.57464170456, 'accumulated_eval_time': 2885.74742937088, 'accumulated_logging_time': 2.4994466304779053}
I0202 04:12:24.405229 139923868813056 logging_writer.py:48] [69130] accumulated_eval_time=2885.747429, accumulated_logging_time=2.499447, accumulated_submission_time=31967.574642, global_step=69130, preemption_count=0, score=31967.574642, test/accuracy=0.478300, test/loss=2.311363, test/num_examples=10000, total_duration=34859.705701, train/accuracy=0.648887, train/loss=1.462048, validation/accuracy=0.602060, validation/loss=1.672143, validation/num_examples=50000
I0202 04:12:52.587057 139923852027648 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4923449754714966, loss=3.997344970703125
I0202 04:13:39.111416 139923868813056 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.72086501121521, loss=2.952047348022461
I0202 04:14:25.890887 139923852027648 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.5657764673233032, loss=4.42585563659668
I0202 04:15:12.329556 139923868813056 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.7743916511535645, loss=2.8714046478271484
I0202 04:15:59.114855 139923852027648 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.5826009511947632, loss=3.435451030731201
I0202 04:16:45.693564 139923868813056 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.8826720714569092, loss=2.617570400238037
I0202 04:17:32.507506 139923852027648 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.8679590225219727, loss=2.3606138229370117
I0202 04:18:19.144515 139923868813056 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.9913171529769897, loss=2.3526530265808105
I0202 04:19:05.823005 139923852027648 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.80662202835083, loss=3.509038209915161
I0202 04:19:24.578668 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:19:34.641570 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:20:03.140376 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:20:04.787312 140085747812160 submission_runner.py:408] Time since start: 35320.11s, 	Step: 70042, 	{'train/accuracy': 0.6694726347923279, 'train/loss': 1.3626357316970825, 'validation/accuracy': 0.6027399897575378, 'validation/loss': 1.665413737297058, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.3237833976745605, 'test/num_examples': 10000, 'score': 32387.686763048172, 'total_duration': 35320.11451506615, 'accumulated_submission_time': 32387.686763048172, 'accumulated_eval_time': 2925.9560775756836, 'accumulated_logging_time': 2.5358903408050537}
I0202 04:20:04.814052 139923868813056 logging_writer.py:48] [70042] accumulated_eval_time=2925.956078, accumulated_logging_time=2.535890, accumulated_submission_time=32387.686763, global_step=70042, preemption_count=0, score=32387.686763, test/accuracy=0.480700, test/loss=2.323783, test/num_examples=10000, total_duration=35320.114515, train/accuracy=0.669473, train/loss=1.362636, validation/accuracy=0.602740, validation/loss=1.665414, validation/num_examples=50000
I0202 04:20:27.999754 139923852027648 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.9168781042099, loss=2.5122616291046143
I0202 04:21:14.141967 139923868813056 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.634002685546875, loss=3.2262113094329834
I0202 04:22:00.758424 139923852027648 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.7923403978347778, loss=2.4625110626220703
I0202 04:22:47.958958 139923868813056 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.6626996994018555, loss=4.1066179275512695
I0202 04:23:34.757153 139923852027648 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.947157621383667, loss=4.9735870361328125
I0202 04:24:21.524649 139923868813056 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0712428092956543, loss=2.5227365493774414
I0202 04:25:08.308514 139923852027648 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.8104791641235352, loss=2.5126688480377197
I0202 04:25:55.113925 139923868813056 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.03922700881958, loss=2.491528272628784
I0202 04:26:41.769136 139923852027648 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.7177494764328003, loss=4.086009502410889
I0202 04:27:05.089055 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:27:15.389378 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:27:43.349718 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:27:44.990442 140085747812160 submission_runner.py:408] Time since start: 35780.32s, 	Step: 70951, 	{'train/accuracy': 0.6404492259025574, 'train/loss': 1.498771071434021, 'validation/accuracy': 0.6004399657249451, 'validation/loss': 1.6853318214416504, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.3453047275543213, 'test/num_examples': 10000, 'score': 32807.90108847618, 'total_duration': 35780.31765007973, 'accumulated_submission_time': 32807.90108847618, 'accumulated_eval_time': 2965.8574674129486, 'accumulated_logging_time': 2.5720467567443848}
I0202 04:27:45.019445 139923868813056 logging_writer.py:48] [70951] accumulated_eval_time=2965.857467, accumulated_logging_time=2.572047, accumulated_submission_time=32807.901088, global_step=70951, preemption_count=0, score=32807.901088, test/accuracy=0.483600, test/loss=2.345305, test/num_examples=10000, total_duration=35780.317650, train/accuracy=0.640449, train/loss=1.498771, validation/accuracy=0.600440, validation/loss=1.685332, validation/num_examples=50000
I0202 04:28:04.625655 139923852027648 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.8913508653640747, loss=2.830068349838257
I0202 04:28:49.800134 139923868813056 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.994263768196106, loss=5.043315887451172
I0202 04:29:36.785146 139923852027648 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.129676342010498, loss=2.5201590061187744
I0202 04:30:23.612385 139923868813056 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.4822518825531006, loss=3.9768404960632324
I0202 04:31:10.532418 139923852027648 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.111578941345215, loss=2.5363540649414062
I0202 04:31:57.227637 139923868813056 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.246110200881958, loss=2.5525295734405518
I0202 04:32:44.383664 139923852027648 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.560945987701416, loss=3.714081287384033
I0202 04:33:31.306729 139923868813056 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.9482841491699219, loss=2.4451022148132324
I0202 04:34:18.025908 139923852027648 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.667702078819275, loss=3.501028299331665
I0202 04:34:45.251414 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:34:56.311867 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:35:24.506207 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:35:26.142033 140085747812160 submission_runner.py:408] Time since start: 36241.47s, 	Step: 71860, 	{'train/accuracy': 0.6525781154632568, 'train/loss': 1.4528508186340332, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.674795389175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.3266289234161377, 'test/num_examples': 10000, 'score': 33228.07022809982, 'total_duration': 36241.46924185753, 'accumulated_submission_time': 33228.07022809982, 'accumulated_eval_time': 3006.7480852603912, 'accumulated_logging_time': 2.6135470867156982}
I0202 04:35:26.171250 139923868813056 logging_writer.py:48] [71860] accumulated_eval_time=3006.748085, accumulated_logging_time=2.613547, accumulated_submission_time=33228.070228, global_step=71860, preemption_count=0, score=33228.070228, test/accuracy=0.480800, test/loss=2.326629, test/num_examples=10000, total_duration=36241.469242, train/accuracy=0.652578, train/loss=1.452851, validation/accuracy=0.604700, validation/loss=1.674795, validation/num_examples=50000
I0202 04:35:42.246241 139923852027648 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.8877067565917969, loss=2.884140729904175
I0202 04:36:26.758668 139923868813056 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.5755536556243896, loss=3.2620086669921875
I0202 04:37:13.459320 139923852027648 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.6625205278396606, loss=3.0913796424865723
I0202 04:38:00.268945 139923868813056 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.950402021408081, loss=2.6655163764953613
I0202 04:38:47.100701 139923852027648 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.227351188659668, loss=2.4830820560455322
I0202 04:39:34.028894 139923868813056 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9997446537017822, loss=2.58571457862854
I0202 04:40:20.941941 139923852027648 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.6918985843658447, loss=4.814604759216309
I0202 04:41:07.922955 139923868813056 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9343303442001343, loss=2.4411497116088867
I0202 04:41:54.349530 139923852027648 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.5129870176315308, loss=4.829378604888916
I0202 04:42:26.326001 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:42:36.698840 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:43:01.677238 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:43:03.311954 140085747812160 submission_runner.py:408] Time since start: 36698.64s, 	Step: 72770, 	{'train/accuracy': 0.6627148389816284, 'train/loss': 1.4005804061889648, 'validation/accuracy': 0.6051599979400635, 'validation/loss': 1.6614387035369873, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.316112756729126, 'test/num_examples': 10000, 'score': 33648.16423654556, 'total_duration': 36698.63916397095, 'accumulated_submission_time': 33648.16423654556, 'accumulated_eval_time': 3043.734041452408, 'accumulated_logging_time': 2.652384042739868}
I0202 04:43:03.339339 139923868813056 logging_writer.py:48] [72770] accumulated_eval_time=3043.734041, accumulated_logging_time=2.652384, accumulated_submission_time=33648.164237, global_step=72770, preemption_count=0, score=33648.164237, test/accuracy=0.483300, test/loss=2.316113, test/num_examples=10000, total_duration=36698.639164, train/accuracy=0.662715, train/loss=1.400580, validation/accuracy=0.605160, validation/loss=1.661439, validation/num_examples=50000
I0202 04:43:15.504130 139923852027648 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9294506311416626, loss=2.554410934448242
I0202 04:43:59.421813 139923868813056 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.72231924533844, loss=4.505213737487793
I0202 04:44:46.070213 139923852027648 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.7623629570007324, loss=3.3286149501800537
I0202 04:45:32.681328 139923868813056 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9423518180847168, loss=2.3814239501953125
I0202 04:46:19.273672 139923852027648 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.8248651027679443, loss=2.442117691040039
I0202 04:47:05.796174 139923868813056 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1315479278564453, loss=2.3569602966308594
I0202 04:47:52.277791 139923852027648 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.9003702402114868, loss=2.4024498462677
I0202 04:48:39.141170 139923868813056 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.863516092300415, loss=2.886322259902954
I0202 04:49:25.916436 139923852027648 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.8393495082855225, loss=2.4211485385894775
I0202 04:50:03.454944 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:50:13.857195 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:50:42.448788 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:50:44.093073 140085747812160 submission_runner.py:408] Time since start: 37159.42s, 	Step: 73682, 	{'train/accuracy': 0.6526562571525574, 'train/loss': 1.441790223121643, 'validation/accuracy': 0.6084200143814087, 'validation/loss': 1.6414271593093872, 'validation/num_examples': 50000, 'test/accuracy': 0.4877000153064728, 'test/loss': 2.2946367263793945, 'test/num_examples': 10000, 'score': 34068.218707084656, 'total_duration': 37159.420274972916, 'accumulated_submission_time': 34068.218707084656, 'accumulated_eval_time': 3084.372179746628, 'accumulated_logging_time': 2.68951416015625}
I0202 04:50:44.122792 139923868813056 logging_writer.py:48] [73682] accumulated_eval_time=3084.372180, accumulated_logging_time=2.689514, accumulated_submission_time=34068.218707, global_step=73682, preemption_count=0, score=34068.218707, test/accuracy=0.487700, test/loss=2.294637, test/num_examples=10000, total_duration=37159.420275, train/accuracy=0.652656, train/loss=1.441790, validation/accuracy=0.608420, validation/loss=1.641427, validation/num_examples=50000
I0202 04:50:51.573056 139923852027648 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.861185908317566, loss=2.51069974899292
I0202 04:51:34.805789 139923868813056 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.850840449333191, loss=2.4930319786071777
I0202 04:52:21.672558 139923852027648 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.5990310907363892, loss=4.445745944976807
I0202 04:53:08.740549 139923868813056 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.0829639434814453, loss=3.4143927097320557
I0202 04:53:55.322863 139923852027648 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.0158839225769043, loss=2.736837863922119
I0202 04:54:42.355891 139923868813056 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.6955983638763428, loss=5.107168197631836
I0202 04:55:29.235616 139923852027648 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.1245107650756836, loss=2.4240732192993164
I0202 04:56:16.030462 139923868813056 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.9870775938034058, loss=2.574022054672241
I0202 04:57:02.697988 139923852027648 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.019619941711426, loss=2.629093885421753
I0202 04:57:44.333496 140085747812160 spec.py:321] Evaluating on the training split.
I0202 04:57:54.863096 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 04:58:20.789014 140085747812160 spec.py:349] Evaluating on the test split.
I0202 04:58:22.433190 140085747812160 submission_runner.py:408] Time since start: 37617.76s, 	Step: 74591, 	{'train/accuracy': 0.6527929306030273, 'train/loss': 1.446118950843811, 'validation/accuracy': 0.6092000007629395, 'validation/loss': 1.6501826047897339, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.3193719387054443, 'test/num_examples': 10000, 'score': 34488.36898994446, 'total_duration': 37617.76039338112, 'accumulated_submission_time': 34488.36898994446, 'accumulated_eval_time': 3122.4718701839447, 'accumulated_logging_time': 2.7291154861450195}
I0202 04:58:22.463886 139923868813056 logging_writer.py:48] [74591] accumulated_eval_time=3122.471870, accumulated_logging_time=2.729115, accumulated_submission_time=34488.368990, global_step=74591, preemption_count=0, score=34488.368990, test/accuracy=0.485800, test/loss=2.319372, test/num_examples=10000, total_duration=37617.760393, train/accuracy=0.652793, train/loss=1.446119, validation/accuracy=0.609200, validation/loss=1.650183, validation/num_examples=50000
I0202 04:58:26.387014 139923852027648 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.1141796112060547, loss=2.5732266902923584
I0202 04:59:08.837738 139923868813056 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.9756747484207153, loss=2.533726215362549
I0202 04:59:55.544039 139923852027648 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.030618906021118, loss=2.3026647567749023
I0202 05:00:42.309028 139923868813056 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.8172178268432617, loss=3.400240421295166
I0202 05:01:29.096778 139923852027648 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.927432656288147, loss=2.436389446258545
I0202 05:02:15.957257 139923868813056 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.8461247682571411, loss=2.5029330253601074
I0202 05:03:02.454956 139923852027648 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.8134640455245972, loss=2.8353097438812256
I0202 05:03:48.932361 139923868813056 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.5600260496139526, loss=4.641140937805176
I0202 05:04:35.621809 139923852027648 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.9575088024139404, loss=2.7747015953063965
I0202 05:05:22.218632 139923868813056 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.9811015129089355, loss=2.320835590362549
I0202 05:05:22.854277 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:05:33.155822 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:06:00.841497 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:06:02.514703 140085747812160 submission_runner.py:408] Time since start: 38077.84s, 	Step: 75503, 	{'train/accuracy': 0.6614648103713989, 'train/loss': 1.3912386894226074, 'validation/accuracy': 0.606719970703125, 'validation/loss': 1.6329096555709839, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2832729816436768, 'test/num_examples': 10000, 'score': 34908.69756317139, 'total_duration': 38077.84191131592, 'accumulated_submission_time': 34908.69756317139, 'accumulated_eval_time': 3162.1323087215424, 'accumulated_logging_time': 2.7706854343414307}
I0202 05:06:02.542198 139923852027648 logging_writer.py:48] [75503] accumulated_eval_time=3162.132309, accumulated_logging_time=2.770685, accumulated_submission_time=34908.697563, global_step=75503, preemption_count=0, score=34908.697563, test/accuracy=0.494300, test/loss=2.283273, test/num_examples=10000, total_duration=38077.841911, train/accuracy=0.661465, train/loss=1.391239, validation/accuracy=0.606720, validation/loss=1.632910, validation/num_examples=50000
I0202 05:06:43.327315 139923868813056 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.9357227087020874, loss=2.476773262023926
I0202 05:07:30.108161 139923852027648 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.5751060247421265, loss=5.058093070983887
I0202 05:08:16.894432 139923868813056 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.96780526638031, loss=2.6674001216888428
I0202 05:09:03.718905 139923852027648 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.5871555805206299, loss=4.439355850219727
I0202 05:09:50.336481 139923868813056 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.8203966617584229, loss=2.424142837524414
I0202 05:10:37.334590 139923852027648 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.8320683240890503, loss=2.58854341506958
I0202 05:11:24.179555 139923868813056 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.97648024559021, loss=2.4527878761291504
I0202 05:12:10.762759 139923852027648 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.944291591644287, loss=2.539259910583496
I0202 05:12:57.685395 139923868813056 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.8577507734298706, loss=2.344494104385376
I0202 05:13:02.579762 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:13:13.070669 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:13:36.726855 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:13:38.369610 140085747812160 submission_runner.py:408] Time since start: 38533.70s, 	Step: 76412, 	{'train/accuracy': 0.6544336080551147, 'train/loss': 1.4055346250534058, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.6018518209457397, 'validation/num_examples': 50000, 'test/accuracy': 0.4888000190258026, 'test/loss': 2.2715227603912354, 'test/num_examples': 10000, 'score': 35328.6748585701, 'total_duration': 38533.69681978226, 'accumulated_submission_time': 35328.6748585701, 'accumulated_eval_time': 3197.922151327133, 'accumulated_logging_time': 2.8070130348205566}
I0202 05:13:38.396485 139923852027648 logging_writer.py:48] [76412] accumulated_eval_time=3197.922151, accumulated_logging_time=2.807013, accumulated_submission_time=35328.674859, global_step=76412, preemption_count=0, score=35328.674859, test/accuracy=0.488800, test/loss=2.271523, test/num_examples=10000, total_duration=38533.696820, train/accuracy=0.654434, train/loss=1.405535, validation/accuracy=0.614820, validation/loss=1.601852, validation/num_examples=50000
I0202 05:14:14.782801 139923868813056 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.2955551147460938, loss=2.4464809894561768
I0202 05:15:01.670791 139923852027648 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.9899920225143433, loss=2.5751559734344482
I0202 05:15:48.624611 139923868813056 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.8915855884552002, loss=2.377326250076294
I0202 05:16:35.412606 139923852027648 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.7258901596069336, loss=4.906805515289307
I0202 05:17:21.928158 139923868813056 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.0170907974243164, loss=2.4426939487457275
I0202 05:18:08.704104 139923852027648 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.5243403911590576, loss=5.016209125518799
I0202 05:18:55.217510 139923868813056 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.6515852212905884, loss=4.383966445922852
I0202 05:19:41.760928 139923852027648 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.9601954221725464, loss=4.827260971069336
I0202 05:20:28.448850 139923868813056 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5889447927474976, loss=4.185120105743408
I0202 05:20:38.412680 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:20:48.781099 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:21:16.321099 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:21:17.959270 140085747812160 submission_runner.py:408] Time since start: 38993.29s, 	Step: 77323, 	{'train/accuracy': 0.6539453268051147, 'train/loss': 1.4418593645095825, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.63576340675354, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.295750617980957, 'test/num_examples': 10000, 'score': 35748.62918996811, 'total_duration': 38993.28647398949, 'accumulated_submission_time': 35748.62918996811, 'accumulated_eval_time': 3237.468738079071, 'accumulated_logging_time': 2.8454105854034424}
I0202 05:21:17.990245 139923852027648 logging_writer.py:48] [77323] accumulated_eval_time=3237.468738, accumulated_logging_time=2.845411, accumulated_submission_time=35748.629190, global_step=77323, preemption_count=0, score=35748.629190, test/accuracy=0.485800, test/loss=2.295751, test/num_examples=10000, total_duration=38993.286474, train/accuracy=0.653945, train/loss=1.441859, validation/accuracy=0.612680, validation/loss=1.635763, validation/num_examples=50000
I0202 05:21:49.378890 139923868813056 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.9059724807739258, loss=2.4191789627075195
I0202 05:22:36.198800 139923852027648 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.6158990859985352, loss=4.478028297424316
I0202 05:23:23.344461 139923868813056 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.6487096548080444, loss=3.2966666221618652
I0202 05:24:09.812186 139923852027648 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.9464507102966309, loss=2.453869342803955
I0202 05:24:56.452523 139923868813056 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.68244206905365, loss=3.801421642303467
I0202 05:25:43.152965 139923852027648 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.0054543018341064, loss=2.519070863723755
I0202 05:26:29.629389 139923868813056 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.41409969329834, loss=2.436950206756592
I0202 05:27:16.275712 139923852027648 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.000363826751709, loss=2.405292272567749
I0202 05:28:02.754691 139923868813056 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.032749891281128, loss=2.646568775177002
I0202 05:28:18.259206 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:28:28.596166 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:28:52.911096 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:28:54.549815 140085747812160 submission_runner.py:408] Time since start: 39449.88s, 	Step: 78235, 	{'train/accuracy': 0.665722668170929, 'train/loss': 1.3743427991867065, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.6258063316345215, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.2690629959106445, 'test/num_examples': 10000, 'score': 36168.83649253845, 'total_duration': 39449.87701368332, 'accumulated_submission_time': 36168.83649253845, 'accumulated_eval_time': 3273.759335756302, 'accumulated_logging_time': 2.88728928565979}
I0202 05:28:54.577781 139923852027648 logging_writer.py:48] [78235] accumulated_eval_time=3273.759336, accumulated_logging_time=2.887289, accumulated_submission_time=36168.836493, global_step=78235, preemption_count=0, score=36168.836493, test/accuracy=0.493500, test/loss=2.269063, test/num_examples=10000, total_duration=39449.877014, train/accuracy=0.665723, train/loss=1.374343, validation/accuracy=0.613340, validation/loss=1.625806, validation/num_examples=50000
I0202 05:29:20.468392 139923868813056 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.681191086769104, loss=4.523321628570557
I0202 05:30:06.929741 139923852027648 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.9620273113250732, loss=2.3573691844940186
I0202 05:30:53.668188 139923868813056 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.6316258907318115, loss=4.777655601501465
I0202 05:31:40.228841 139923852027648 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.0887696743011475, loss=2.45865535736084
I0202 05:32:27.197988 139923868813056 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.9687340259552002, loss=2.641033411026001
I0202 05:33:14.034128 139923852027648 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.20686936378479, loss=2.4715588092803955
I0202 05:34:00.650387 139923868813056 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.9358071088790894, loss=2.5129220485687256
I0202 05:34:47.248512 139923852027648 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.890865683555603, loss=2.9536662101745605
I0202 05:35:34.209177 139923868813056 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.017803907394409, loss=2.4311623573303223
I0202 05:35:54.975316 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:36:05.188065 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:36:32.622925 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:36:34.269449 140085747812160 submission_runner.py:408] Time since start: 39909.60s, 	Step: 79146, 	{'train/accuracy': 0.6544336080551147, 'train/loss': 1.4474139213562012, 'validation/accuracy': 0.6101999878883362, 'validation/loss': 1.6504663228988647, 'validation/num_examples': 50000, 'test/accuracy': 0.4942000210285187, 'test/loss': 2.2950570583343506, 'test/num_examples': 10000, 'score': 36589.174001932144, 'total_duration': 39909.596652030945, 'accumulated_submission_time': 36589.174001932144, 'accumulated_eval_time': 3313.0534660816193, 'accumulated_logging_time': 2.9241793155670166}
I0202 05:36:34.303272 139923852027648 logging_writer.py:48] [79146] accumulated_eval_time=3313.053466, accumulated_logging_time=2.924179, accumulated_submission_time=36589.174002, global_step=79146, preemption_count=0, score=36589.174002, test/accuracy=0.494200, test/loss=2.295057, test/num_examples=10000, total_duration=39909.596652, train/accuracy=0.654434, train/loss=1.447414, validation/accuracy=0.610200, validation/loss=1.650466, validation/num_examples=50000
I0202 05:36:55.881532 139923868813056 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.9281668663024902, loss=2.6988134384155273
I0202 05:37:41.932382 139923852027648 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.702232003211975, loss=3.781686305999756
I0202 05:38:29.086708 139923868813056 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.9939075708389282, loss=2.721137523651123
I0202 05:39:15.547284 139923852027648 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.7976454496383667, loss=3.30928897857666
I0202 05:40:02.229789 139923868813056 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.8218129873275757, loss=2.8588078022003174
I0202 05:40:48.931384 139923852027648 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.86943519115448, loss=2.389901876449585
I0202 05:41:35.553173 139923868813056 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.161076068878174, loss=2.424309253692627
I0202 05:42:22.450816 139923852027648 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.113813877105713, loss=2.426238775253296
I0202 05:43:09.417107 139923868813056 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.7259854078292847, loss=3.7289233207702637
I0202 05:43:34.277113 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:43:44.698405 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:44:09.154071 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:44:10.809366 140085747812160 submission_runner.py:408] Time since start: 40366.14s, 	Step: 80055, 	{'train/accuracy': 0.6633593440055847, 'train/loss': 1.3997061252593994, 'validation/accuracy': 0.6201199889183044, 'validation/loss': 1.590294361114502, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.22688364982605, 'test/num_examples': 10000, 'score': 37009.086544275284, 'total_duration': 40366.136556625366, 'accumulated_submission_time': 37009.086544275284, 'accumulated_eval_time': 3349.5856976509094, 'accumulated_logging_time': 2.9685537815093994}
I0202 05:44:10.839510 139923852027648 logging_writer.py:48] [80055] accumulated_eval_time=3349.585698, accumulated_logging_time=2.968554, accumulated_submission_time=37009.086544, global_step=80055, preemption_count=0, score=37009.086544, test/accuracy=0.499400, test/loss=2.226884, test/num_examples=10000, total_duration=40366.136557, train/accuracy=0.663359, train/loss=1.399706, validation/accuracy=0.620120, validation/loss=1.590294, validation/num_examples=50000
I0202 05:44:28.883823 139923868813056 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.00108003616333, loss=2.5719730854034424
I0202 05:45:13.789330 139923852027648 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.6364243030548096, loss=4.485180854797363
I0202 05:46:00.256128 139923868813056 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.8008368015289307, loss=2.718780755996704
I0202 05:46:46.980255 139923852027648 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.0532195568084717, loss=2.6336467266082764
I0202 05:47:33.479712 139923868813056 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.9893676042556763, loss=2.424421787261963
I0202 05:48:20.162348 139923852027648 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.99299955368042, loss=2.3189358711242676
I0202 05:49:06.492191 139923868813056 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.8323826789855957, loss=2.63098406791687
I0202 05:49:53.482964 139923852027648 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.038818836212158, loss=2.519019603729248
I0202 05:50:40.084018 139923868813056 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.9046854972839355, loss=2.443984031677246
I0202 05:51:11.217307 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:51:21.638292 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:51:44.922484 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:51:46.565880 140085747812160 submission_runner.py:408] Time since start: 40821.89s, 	Step: 80968, 	{'train/accuracy': 0.6633203029632568, 'train/loss': 1.383533239364624, 'validation/accuracy': 0.6125400066375732, 'validation/loss': 1.6144942045211792, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.279834032058716, 'test/num_examples': 10000, 'score': 37429.40017914772, 'total_duration': 40821.89309167862, 'accumulated_submission_time': 37429.40017914772, 'accumulated_eval_time': 3384.9342653751373, 'accumulated_logging_time': 3.0103461742401123}
I0202 05:51:46.596963 139923852027648 logging_writer.py:48] [80968] accumulated_eval_time=3384.934265, accumulated_logging_time=3.010346, accumulated_submission_time=37429.400179, global_step=80968, preemption_count=0, score=37429.400179, test/accuracy=0.491500, test/loss=2.279834, test/num_examples=10000, total_duration=40821.893092, train/accuracy=0.663320, train/loss=1.383533, validation/accuracy=0.612540, validation/loss=1.614494, validation/num_examples=50000
I0202 05:51:59.556271 139923868813056 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.1290090084075928, loss=2.4135003089904785
I0202 05:52:43.666174 139923852027648 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.9667357206344604, loss=3.1215016841888428
I0202 05:53:30.753307 139923868813056 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.2314085960388184, loss=2.3894851207733154
I0202 05:54:17.308121 139923852027648 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.0712807178497314, loss=2.3886985778808594
I0202 05:55:04.244889 139923868813056 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.9364038705825806, loss=2.4187052249908447
I0202 05:55:51.128294 139923852027648 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.0374441146850586, loss=2.3834500312805176
I0202 05:56:37.695067 139923868813056 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.0240776538848877, loss=2.831364870071411
I0202 05:57:24.610355 139923852027648 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.6174677610397339, loss=4.9293951988220215
I0202 05:58:11.118341 139923868813056 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.9096858501434326, loss=2.336179256439209
I0202 05:58:46.591197 140085747812160 spec.py:321] Evaluating on the training split.
I0202 05:58:56.774886 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 05:59:24.051986 140085747812160 spec.py:349] Evaluating on the test split.
I0202 05:59:25.702715 140085747812160 submission_runner.py:408] Time since start: 41281.03s, 	Step: 81878, 	{'train/accuracy': 0.6926171779632568, 'train/loss': 1.2652881145477295, 'validation/accuracy': 0.6225599646568298, 'validation/loss': 1.5752804279327393, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2313928604125977, 'test/num_examples': 10000, 'score': 37849.333032131195, 'total_duration': 41281.0299179554, 'accumulated_submission_time': 37849.333032131195, 'accumulated_eval_time': 3424.0457706451416, 'accumulated_logging_time': 3.0522875785827637}
I0202 05:59:25.735822 139923852027648 logging_writer.py:48] [81878] accumulated_eval_time=3424.045771, accumulated_logging_time=3.052288, accumulated_submission_time=37849.333032, global_step=81878, preemption_count=0, score=37849.333032, test/accuracy=0.494300, test/loss=2.231393, test/num_examples=10000, total_duration=41281.029918, train/accuracy=0.692617, train/loss=1.265288, validation/accuracy=0.622560, validation/loss=1.575280, validation/num_examples=50000
I0202 05:59:34.762516 139923868813056 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1953818798065186, loss=2.442089557647705
I0202 06:00:18.205069 139923852027648 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.7230664491653442, loss=3.252509117126465
I0202 06:01:05.049933 139923868813056 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.464427947998047, loss=2.4513792991638184
I0202 06:01:51.756570 139923852027648 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.658240795135498, loss=4.071885585784912
I0202 06:02:38.492737 139923868813056 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.124783754348755, loss=2.331684112548828
I0202 06:03:25.054232 139923852027648 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.042874574661255, loss=2.356294870376587
I0202 06:04:11.954594 139923868813056 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.7345707416534424, loss=4.97578239440918
I0202 06:04:58.695518 139923852027648 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.03308367729187, loss=2.4947915077209473
I0202 06:05:45.271119 139923868813056 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.0608420372009277, loss=2.2859416007995605
I0202 06:06:25.775850 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:06:35.875874 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:07:01.117714 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:07:02.767195 140085747812160 submission_runner.py:408] Time since start: 41738.09s, 	Step: 82788, 	{'train/accuracy': 0.6661523580551147, 'train/loss': 1.387743592262268, 'validation/accuracy': 0.6181600093841553, 'validation/loss': 1.5939165353775024, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.2245521545410156, 'test/num_examples': 10000, 'score': 38269.31256151199, 'total_duration': 41738.09440588951, 'accumulated_submission_time': 38269.31256151199, 'accumulated_eval_time': 3461.0371272563934, 'accumulated_logging_time': 3.0954391956329346}
I0202 06:07:02.796175 139923852027648 logging_writer.py:48] [82788] accumulated_eval_time=3461.037127, accumulated_logging_time=3.095439, accumulated_submission_time=38269.312562, global_step=82788, preemption_count=0, score=38269.312562, test/accuracy=0.497000, test/loss=2.224552, test/num_examples=10000, total_duration=41738.094406, train/accuracy=0.666152, train/loss=1.387744, validation/accuracy=0.618160, validation/loss=1.593917, validation/num_examples=50000
I0202 06:07:07.905805 139923868813056 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.1648550033569336, loss=2.2926583290100098
I0202 06:07:50.065461 139923852027648 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.8975281715393066, loss=3.4530580043792725
I0202 06:08:36.866270 139923868813056 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.1435511112213135, loss=2.4876999855041504
I0202 06:09:23.884349 139923852027648 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.8093523979187012, loss=4.498208522796631
I0202 06:10:10.803387 139923868813056 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.7777278423309326, loss=4.13422966003418
I0202 06:10:57.375022 139923852027648 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.753517985343933, loss=4.418230056762695
I0202 06:11:44.103414 139923868813056 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.8923425674438477, loss=2.88293194770813
I0202 06:12:30.930874 139923852027648 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.696657419204712, loss=3.4267778396606445
I0202 06:13:17.951286 139923868813056 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.7746678590774536, loss=4.238008975982666
I0202 06:14:03.010176 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:14:13.462714 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:14:40.414870 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:14:42.059211 140085747812160 submission_runner.py:408] Time since start: 42197.39s, 	Step: 83698, 	{'train/accuracy': 0.6722851395606995, 'train/loss': 1.3354723453521729, 'validation/accuracy': 0.6222999691963196, 'validation/loss': 1.5551286935806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.2071619033813477, 'test/num_examples': 10000, 'score': 38689.46293449402, 'total_duration': 42197.38641667366, 'accumulated_submission_time': 38689.46293449402, 'accumulated_eval_time': 3500.0861682891846, 'accumulated_logging_time': 3.1364245414733887}
I0202 06:14:42.096294 139923852027648 logging_writer.py:48] [83698] accumulated_eval_time=3500.086168, accumulated_logging_time=3.136425, accumulated_submission_time=38689.462934, global_step=83698, preemption_count=0, score=38689.462934, test/accuracy=0.500400, test/loss=2.207162, test/num_examples=10000, total_duration=42197.386417, train/accuracy=0.672285, train/loss=1.335472, validation/accuracy=0.622300, validation/loss=1.555129, validation/num_examples=50000
I0202 06:14:43.273262 139923868813056 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.2238521575927734, loss=2.3808414936065674
I0202 06:15:25.280321 139923852027648 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.0639877319335938, loss=2.2913618087768555
I0202 06:16:11.934761 139923868813056 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.692049503326416, loss=4.0234551429748535
I0202 06:16:58.476470 139923852027648 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.9428613185882568, loss=2.4302408695220947
I0202 06:17:45.122943 139923868813056 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.7623533010482788, loss=3.0801339149475098
I0202 06:18:31.835108 139923852027648 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.7051565647125244, loss=3.1109399795532227
I0202 06:19:18.571291 139923868813056 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.0103631019592285, loss=2.4244842529296875
I0202 06:20:05.291046 139923852027648 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.1418051719665527, loss=2.2777493000030518
I0202 06:20:51.971290 139923868813056 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.9719657897949219, loss=2.837085485458374
I0202 06:21:38.785609 139923852027648 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.7731050252914429, loss=4.245964527130127
I0202 06:21:42.191304 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:21:52.686227 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:22:19.943174 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:22:21.575276 140085747812160 submission_runner.py:408] Time since start: 42656.90s, 	Step: 84609, 	{'train/accuracy': 0.6911913752555847, 'train/loss': 1.2636808156967163, 'validation/accuracy': 0.6240800023078918, 'validation/loss': 1.56609046459198, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.2192747592926025, 'test/num_examples': 10000, 'score': 39109.49723100662, 'total_duration': 42656.90248131752, 'accumulated_submission_time': 39109.49723100662, 'accumulated_eval_time': 3539.4701313972473, 'accumulated_logging_time': 3.183518648147583}
I0202 06:22:21.606077 139923868813056 logging_writer.py:48] [84609] accumulated_eval_time=3539.470131, accumulated_logging_time=3.183519, accumulated_submission_time=39109.497231, global_step=84609, preemption_count=0, score=39109.497231, test/accuracy=0.500900, test/loss=2.219275, test/num_examples=10000, total_duration=42656.902481, train/accuracy=0.691191, train/loss=1.263681, validation/accuracy=0.624080, validation/loss=1.566090, validation/num_examples=50000
I0202 06:22:59.616409 139923852027648 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.7747641801834106, loss=4.118687629699707
I0202 06:23:46.356153 139923868813056 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.0114026069641113, loss=2.506187915802002
I0202 06:24:33.045762 139923852027648 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.059861660003662, loss=2.3339955806732178
I0202 06:25:19.787714 139923868813056 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.9310671091079712, loss=4.007516860961914
I0202 06:26:06.704384 139923852027648 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.7215195894241333, loss=3.580552577972412
I0202 06:26:53.435146 139923868813056 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.8151236772537231, loss=4.885353088378906
I0202 06:27:40.279228 139923852027648 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.7731821537017822, loss=2.858372688293457
I0202 06:28:27.196754 139923868813056 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.2108514308929443, loss=2.5218570232391357
I0202 06:29:13.828444 139923852027648 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.1251015663146973, loss=2.756239414215088
I0202 06:29:21.940277 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:29:32.338038 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:30:03.017097 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:30:04.659795 140085747812160 submission_runner.py:408] Time since start: 43119.99s, 	Step: 85519, 	{'train/accuracy': 0.6674023270606995, 'train/loss': 1.393503189086914, 'validation/accuracy': 0.6214599609375, 'validation/loss': 1.5956655740737915, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2568812370300293, 'test/num_examples': 10000, 'score': 39529.7694542408, 'total_duration': 43119.98700237274, 'accumulated_submission_time': 39529.7694542408, 'accumulated_eval_time': 3582.189652442932, 'accumulated_logging_time': 3.224731683731079}
I0202 06:30:04.688667 139923868813056 logging_writer.py:48] [85519] accumulated_eval_time=3582.189652, accumulated_logging_time=3.224732, accumulated_submission_time=39529.769454, global_step=85519, preemption_count=0, score=39529.769454, test/accuracy=0.501000, test/loss=2.256881, test/num_examples=10000, total_duration=43119.987002, train/accuracy=0.667402, train/loss=1.393503, validation/accuracy=0.621460, validation/loss=1.595666, validation/num_examples=50000
I0202 06:30:38.086018 139923852027648 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.3013956546783447, loss=2.4224741458892822
I0202 06:31:24.657560 139923868813056 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.293868064880371, loss=2.609534502029419
I0202 06:32:11.752107 139923852027648 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.9521737098693848, loss=3.177922010421753
I0202 06:32:58.774080 139923868813056 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.0604617595672607, loss=2.6746902465820312
I0202 06:33:45.624114 139923852027648 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.7425835132598877, loss=4.538514137268066
I0202 06:34:32.857082 139923868813056 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.9693011045455933, loss=2.433979034423828
I0202 06:35:19.476911 139923852027648 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.9584486484527588, loss=2.979012966156006
I0202 06:36:06.328347 139923868813056 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.075598955154419, loss=2.3963663578033447
I0202 06:36:53.183374 139923852027648 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.884439468383789, loss=3.3653810024261475
I0202 06:37:05.070647 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:37:15.424788 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:37:41.248775 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:37:42.888695 140085747812160 submission_runner.py:408] Time since start: 43578.22s, 	Step: 86427, 	{'train/accuracy': 0.6750780940055847, 'train/loss': 1.347376823425293, 'validation/accuracy': 0.6284599900245667, 'validation/loss': 1.5590183734893799, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.208587408065796, 'test/num_examples': 10000, 'score': 39950.09155750275, 'total_duration': 43578.21590304375, 'accumulated_submission_time': 39950.09155750275, 'accumulated_eval_time': 3620.007707118988, 'accumulated_logging_time': 3.262495994567871}
I0202 06:37:42.924107 139923868813056 logging_writer.py:48] [86427] accumulated_eval_time=3620.007707, accumulated_logging_time=3.262496, accumulated_submission_time=39950.091558, global_step=86427, preemption_count=0, score=39950.091558, test/accuracy=0.505400, test/loss=2.208587, test/num_examples=10000, total_duration=43578.215903, train/accuracy=0.675078, train/loss=1.347377, validation/accuracy=0.628460, validation/loss=1.559018, validation/num_examples=50000
I0202 06:38:12.269198 139923852027648 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.8604532480239868, loss=3.0783660411834717
I0202 06:38:58.898220 139923868813056 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.119962692260742, loss=2.300715684890747
I0202 06:39:46.002826 139923852027648 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.8369593620300293, loss=3.3647913932800293
I0202 06:40:32.674346 139923868813056 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.171396017074585, loss=2.507113218307495
I0202 06:41:19.335074 139923852027648 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.8384953737258911, loss=5.031373500823975
I0202 06:42:06.025227 139923868813056 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.1316542625427246, loss=2.2071373462677
I0202 06:42:53.109841 139923852027648 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.1162898540496826, loss=4.951618194580078
I0202 06:43:39.673044 139923868813056 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.9759199619293213, loss=2.485434055328369
I0202 06:44:26.253161 139923852027648 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.575690507888794, loss=4.507494926452637
I0202 06:44:43.304512 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:44:53.967725 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:45:21.140941 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:45:22.785375 140085747812160 submission_runner.py:408] Time since start: 44038.11s, 	Step: 87338, 	{'train/accuracy': 0.6869726181030273, 'train/loss': 1.2832924127578735, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.5575437545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.1890664100646973, 'test/num_examples': 10000, 'score': 40370.41019010544, 'total_duration': 44038.112585783005, 'accumulated_submission_time': 40370.41019010544, 'accumulated_eval_time': 3659.488579750061, 'accumulated_logging_time': 3.3086326122283936}
I0202 06:45:22.818541 139923868813056 logging_writer.py:48] [87338] accumulated_eval_time=3659.488580, accumulated_logging_time=3.308633, accumulated_submission_time=40370.410190, global_step=87338, preemption_count=0, score=40370.410190, test/accuracy=0.503000, test/loss=2.189066, test/num_examples=10000, total_duration=44038.112586, train/accuracy=0.686973, train/loss=1.283292, validation/accuracy=0.626060, validation/loss=1.557544, validation/num_examples=50000
I0202 06:45:47.531252 139923852027648 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.295570135116577, loss=2.358184576034546
I0202 06:46:33.887907 139923868813056 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.8632335662841797, loss=2.9016716480255127
I0202 06:47:20.991732 139923852027648 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.9964052438735962, loss=2.81699538230896
I0202 06:48:07.914753 139923868813056 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.171494960784912, loss=2.3094489574432373
I0202 06:48:55.043473 139923852027648 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.0504403114318848, loss=2.340477228164673
I0202 06:49:41.965379 139923868813056 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.93326735496521, loss=2.229844808578491
I0202 06:50:28.778475 139923852027648 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.207745313644409, loss=2.3648922443389893
I0202 06:51:15.791686 139923868813056 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.6593555212020874, loss=3.4459805488586426
I0202 06:52:02.761266 139923852027648 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.0095744132995605, loss=2.8802871704101562
I0202 06:52:23.054825 140085747812160 spec.py:321] Evaluating on the training split.
I0202 06:52:33.354395 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 06:53:00.005528 140085747812160 spec.py:349] Evaluating on the test split.
I0202 06:53:01.666527 140085747812160 submission_runner.py:408] Time since start: 44496.99s, 	Step: 88245, 	{'train/accuracy': 0.6767382621765137, 'train/loss': 1.3393456935882568, 'validation/accuracy': 0.6292399764060974, 'validation/loss': 1.5438076257705688, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.1848161220550537, 'test/num_examples': 10000, 'score': 40790.58680820465, 'total_duration': 44496.99372458458, 'accumulated_submission_time': 40790.58680820465, 'accumulated_eval_time': 3698.1002700328827, 'accumulated_logging_time': 3.3508591651916504}
I0202 06:53:01.701296 139923868813056 logging_writer.py:48] [88245] accumulated_eval_time=3698.100270, accumulated_logging_time=3.350859, accumulated_submission_time=40790.586808, global_step=88245, preemption_count=0, score=40790.586808, test/accuracy=0.508300, test/loss=2.184816, test/num_examples=10000, total_duration=44496.993725, train/accuracy=0.676738, train/loss=1.339346, validation/accuracy=0.629240, validation/loss=1.543808, validation/num_examples=50000
I0202 06:53:23.674246 139923852027648 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.9921714067459106, loss=3.097446918487549
I0202 06:54:09.338549 139923868813056 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.7379368543624878, loss=3.6221110820770264
I0202 06:54:56.351333 139923852027648 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.151378870010376, loss=2.3672120571136475
I0202 06:55:43.193001 139923868813056 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.4447340965270996, loss=2.367032051086426
I0202 06:56:30.032764 139923852027648 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4598257541656494, loss=4.292953968048096
I0202 06:57:16.638076 139923868813056 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.0457987785339355, loss=2.2998766899108887
I0202 06:58:03.480704 139923852027648 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.353800058364868, loss=2.4421446323394775
I0202 06:58:50.034276 139923868813056 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.8789291381835938, loss=3.242628335952759
I0202 06:59:36.941298 139923852027648 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.8573458194732666, loss=3.205239772796631
I0202 07:00:01.866565 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:00:12.526897 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:00:36.399845 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:00:38.047492 140085747812160 submission_runner.py:408] Time since start: 44953.37s, 	Step: 89155, 	{'train/accuracy': 0.6765820384025574, 'train/loss': 1.3568689823150635, 'validation/accuracy': 0.6251199841499329, 'validation/loss': 1.592879056930542, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2331268787384033, 'test/num_examples': 10000, 'score': 41210.69068980217, 'total_duration': 44953.37469482422, 'accumulated_submission_time': 41210.69068980217, 'accumulated_eval_time': 3734.2811844348907, 'accumulated_logging_time': 3.3964245319366455}
I0202 07:00:38.078784 139923868813056 logging_writer.py:48] [89155] accumulated_eval_time=3734.281184, accumulated_logging_time=3.396425, accumulated_submission_time=41210.690690, global_step=89155, preemption_count=0, score=41210.690690, test/accuracy=0.497800, test/loss=2.233127, test/num_examples=10000, total_duration=44953.374695, train/accuracy=0.676582, train/loss=1.356869, validation/accuracy=0.625120, validation/loss=1.592879, validation/num_examples=50000
I0202 07:00:56.123619 139923852027648 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.0755419731140137, loss=2.3594741821289062
I0202 07:01:40.859375 139923868813056 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.1661016941070557, loss=2.496227264404297
I0202 07:02:27.835510 139923852027648 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.7503010034561157, loss=3.8102502822875977
I0202 07:03:14.756390 139923868813056 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.161858320236206, loss=2.3587183952331543
I0202 07:04:01.626762 139923852027648 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.6564291715621948, loss=4.858151435852051
I0202 07:04:48.301891 139923868813056 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.8635820150375366, loss=3.37552547454834
I0202 07:05:35.048222 139923852027648 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.7317111492156982, loss=2.754301071166992
I0202 07:06:21.661188 139923868813056 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.1272592544555664, loss=2.3981380462646484
I0202 07:07:08.411790 139923852027648 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.0860652923583984, loss=2.3867149353027344
I0202 07:07:38.614052 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:07:49.132196 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:08:17.144490 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:08:18.793049 140085747812160 submission_runner.py:408] Time since start: 45414.12s, 	Step: 90066, 	{'train/accuracy': 0.6912695169448853, 'train/loss': 1.2604010105133057, 'validation/accuracy': 0.6340000033378601, 'validation/loss': 1.5193389654159546, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.1705994606018066, 'test/num_examples': 10000, 'score': 41631.16453623772, 'total_duration': 45414.1202609539, 'accumulated_submission_time': 41631.16453623772, 'accumulated_eval_time': 3774.4601967334747, 'accumulated_logging_time': 3.4379467964172363}
I0202 07:08:18.825116 139923868813056 logging_writer.py:48] [90066] accumulated_eval_time=3774.460197, accumulated_logging_time=3.437947, accumulated_submission_time=41631.164536, global_step=90066, preemption_count=0, score=41631.164536, test/accuracy=0.508400, test/loss=2.170599, test/num_examples=10000, total_duration=45414.120261, train/accuracy=0.691270, train/loss=1.260401, validation/accuracy=0.634000, validation/loss=1.519339, validation/num_examples=50000
I0202 07:08:32.563274 139923852027648 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.815825343132019, loss=2.8243026733398438
I0202 07:09:16.864480 139923868813056 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.0576813220977783, loss=2.7363805770874023
I0202 07:10:03.418413 139923852027648 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.2667489051818848, loss=2.4513752460479736
I0202 07:10:50.433312 139923868813056 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.7550405263900757, loss=3.571706771850586
I0202 07:11:37.185775 139923852027648 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.344425916671753, loss=2.268131971359253
I0202 07:12:23.911715 139923868813056 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.7746307849884033, loss=3.866791248321533
I0202 07:13:10.767527 139923852027648 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.857142448425293, loss=4.0477399826049805
I0202 07:13:57.540976 139923868813056 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.3591084480285645, loss=2.2889251708984375
I0202 07:14:44.079185 139923852027648 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.121973752975464, loss=2.2976810932159424
I0202 07:15:19.070639 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:15:29.564866 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:15:51.710928 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:15:53.350533 140085747812160 submission_runner.py:408] Time since start: 45868.68s, 	Step: 90976, 	{'train/accuracy': 0.6769921779632568, 'train/loss': 1.3207937479019165, 'validation/accuracy': 0.6307799816131592, 'validation/loss': 1.5352376699447632, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.1914098262786865, 'test/num_examples': 10000, 'score': 42051.34705209732, 'total_duration': 45868.677743434906, 'accumulated_submission_time': 42051.34705209732, 'accumulated_eval_time': 3808.74009847641, 'accumulated_logging_time': 3.4811456203460693}
I0202 07:15:53.383885 139923868813056 logging_writer.py:48] [90976] accumulated_eval_time=3808.740098, accumulated_logging_time=3.481146, accumulated_submission_time=42051.347052, global_step=90976, preemption_count=0, score=42051.347052, test/accuracy=0.504300, test/loss=2.191410, test/num_examples=10000, total_duration=45868.677743, train/accuracy=0.676992, train/loss=1.320794, validation/accuracy=0.630780, validation/loss=1.535238, validation/num_examples=50000
I0202 07:16:03.200124 139923852027648 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.8139320611953735, loss=3.3657243251800537
I0202 07:16:46.209627 139923868813056 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.6467489004135132, loss=3.531048536300659
I0202 07:17:33.296674 139923852027648 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.7444586753845215, loss=4.259289264678955
I0202 07:18:20.087657 139923868813056 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.1795496940612793, loss=2.5899741649627686
I0202 07:19:07.045786 139923852027648 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.011502742767334, loss=4.439393043518066
I0202 07:19:53.800582 139923868813056 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.6685606241226196, loss=4.232569217681885
I0202 07:20:40.866426 139923852027648 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.1022908687591553, loss=2.502955198287964
I0202 07:21:27.541507 139923868813056 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.174023151397705, loss=2.393742799758911
I0202 07:22:14.412021 139923852027648 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.03609561920166, loss=2.2829973697662354
I0202 07:22:53.673734 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:23:04.166013 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:23:28.496585 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:23:30.139615 140085747812160 submission_runner.py:408] Time since start: 46325.47s, 	Step: 91885, 	{'train/accuracy': 0.6859374642372131, 'train/loss': 1.277827262878418, 'validation/accuracy': 0.6394599676132202, 'validation/loss': 1.4927690029144287, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.1332180500030518, 'test/num_examples': 10000, 'score': 42471.574355363846, 'total_duration': 46325.46682262421, 'accumulated_submission_time': 42471.574355363846, 'accumulated_eval_time': 3845.2059903144836, 'accumulated_logging_time': 3.5260469913482666}
I0202 07:23:30.174150 139923868813056 logging_writer.py:48] [91885] accumulated_eval_time=3845.205990, accumulated_logging_time=3.526047, accumulated_submission_time=42471.574355, global_step=91885, preemption_count=0, score=42471.574355, test/accuracy=0.510600, test/loss=2.133218, test/num_examples=10000, total_duration=46325.466823, train/accuracy=0.685937, train/loss=1.277827, validation/accuracy=0.639460, validation/loss=1.492769, validation/num_examples=50000
I0202 07:23:36.461685 139923852027648 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.8078813552856445, loss=3.890883684158325
I0202 07:24:19.214781 139923868813056 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.2108428478240967, loss=2.264040470123291
I0202 07:25:06.030200 139923852027648 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.9714714288711548, loss=2.6660633087158203
I0202 07:25:52.773321 139923868813056 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.8022314310073853, loss=4.6466288566589355
I0202 07:26:39.917535 139923852027648 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.0636489391326904, loss=2.1881701946258545
I0202 07:27:26.655626 139923868813056 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.1160476207733154, loss=2.5274674892425537
I0202 07:28:13.543077 139923852027648 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.446042776107788, loss=2.1810922622680664
I0202 07:29:00.719502 139923868813056 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.0609290599823, loss=2.323275566101074
I0202 07:29:47.635714 139923852027648 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.787491798400879, loss=4.616467475891113
I0202 07:30:30.499753 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:30:40.928421 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:31:08.027484 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:31:09.670967 140085747812160 submission_runner.py:408] Time since start: 46785.00s, 	Step: 92793, 	{'train/accuracy': 0.6937304735183716, 'train/loss': 1.268678903579712, 'validation/accuracy': 0.6373999714851379, 'validation/loss': 1.5291402339935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.167919874191284, 'test/num_examples': 10000, 'score': 42891.8378636837, 'total_duration': 46784.998166799545, 'accumulated_submission_time': 42891.8378636837, 'accumulated_eval_time': 3884.377197265625, 'accumulated_logging_time': 3.5714974403381348}
I0202 07:31:09.706426 139923868813056 logging_writer.py:48] [92793] accumulated_eval_time=3884.377197, accumulated_logging_time=3.571497, accumulated_submission_time=42891.837864, global_step=92793, preemption_count=0, score=42891.837864, test/accuracy=0.519300, test/loss=2.167920, test/num_examples=10000, total_duration=46784.998167, train/accuracy=0.693730, train/loss=1.268679, validation/accuracy=0.637400, validation/loss=1.529140, validation/num_examples=50000
I0202 07:31:12.850874 139923852027648 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.0502870082855225, loss=2.654745101928711
I0202 07:31:55.207195 139923868813056 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.9976330995559692, loss=2.323392868041992
I0202 07:32:42.372735 139923852027648 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.9228191375732422, loss=4.943507671356201
I0202 07:33:29.420957 139923868813056 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.4718127250671387, loss=2.297247886657715
I0202 07:34:16.339591 139923852027648 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.1157872676849365, loss=2.6787261962890625
I0202 07:35:03.202416 139923868813056 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.1502115726470947, loss=4.374994277954102
I0202 07:35:50.057014 139923852027648 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.1458301544189453, loss=2.1816864013671875
I0202 07:36:36.665984 139923868813056 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.188323497772217, loss=2.235424041748047
I0202 07:37:23.606526 139923852027648 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.9169119596481323, loss=3.1521036624908447
I0202 07:38:09.842125 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:38:20.227289 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:38:44.012066 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:38:45.654518 140085747812160 submission_runner.py:408] Time since start: 47240.98s, 	Step: 93700, 	{'train/accuracy': 0.684374988079071, 'train/loss': 1.302620768547058, 'validation/accuracy': 0.6352800130844116, 'validation/loss': 1.5236302614212036, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.174853563308716, 'test/num_examples': 10000, 'score': 43311.91257548332, 'total_duration': 47240.98172211647, 'accumulated_submission_time': 43311.91257548332, 'accumulated_eval_time': 3920.189643383026, 'accumulated_logging_time': 3.617255449295044}
I0202 07:38:45.687445 139923868813056 logging_writer.py:48] [93700] accumulated_eval_time=3920.189643, accumulated_logging_time=3.617255, accumulated_submission_time=43311.912575, global_step=93700, preemption_count=0, score=43311.912575, test/accuracy=0.507400, test/loss=2.174854, test/num_examples=10000, total_duration=47240.981722, train/accuracy=0.684375, train/loss=1.302621, validation/accuracy=0.635280, validation/loss=1.523630, validation/num_examples=50000
I0202 07:38:46.085944 139923852027648 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.0885379314422607, loss=2.310237407684326
I0202 07:39:27.797033 139923868813056 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.655697226524353, loss=4.4127607345581055
I0202 07:40:14.751049 139923852027648 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.8383044004440308, loss=3.615847110748291
I0202 07:41:01.604109 139923868813056 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.227532386779785, loss=2.236186981201172
I0202 07:41:48.678809 139923852027648 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.17934250831604, loss=2.301852226257324
I0202 07:42:35.866563 139923868813056 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.0714027881622314, loss=2.3087852001190186
I0202 07:43:22.623574 139923852027648 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.250481605529785, loss=2.318582057952881
I0202 07:44:09.261647 139923868813056 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.012847900390625, loss=2.8852663040161133
I0202 07:44:55.982648 139923852027648 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.399223804473877, loss=2.3606739044189453
I0202 07:45:42.625788 139923868813056 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.268982172012329, loss=2.3104467391967773
I0202 07:45:46.041470 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:45:57.248790 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:46:23.096935 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:46:24.727877 140085747812160 submission_runner.py:408] Time since start: 47700.06s, 	Step: 94609, 	{'train/accuracy': 0.6906640529632568, 'train/loss': 1.2588465213775635, 'validation/accuracy': 0.6386399865150452, 'validation/loss': 1.485780119895935, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.115467071533203, 'test/num_examples': 10000, 'score': 43732.20620751381, 'total_duration': 47700.05508303642, 'accumulated_submission_time': 43732.20620751381, 'accumulated_eval_time': 3958.876034975052, 'accumulated_logging_time': 3.6599841117858887}
I0202 07:46:24.760340 139923852027648 logging_writer.py:48] [94609] accumulated_eval_time=3958.876035, accumulated_logging_time=3.659984, accumulated_submission_time=43732.206208, global_step=94609, preemption_count=0, score=43732.206208, test/accuracy=0.519000, test/loss=2.115467, test/num_examples=10000, total_duration=47700.055083, train/accuracy=0.690664, train/loss=1.258847, validation/accuracy=0.638640, validation/loss=1.485780, validation/num_examples=50000
I0202 07:47:02.531018 139923868813056 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.7068651914596558, loss=4.765961170196533
I0202 07:47:49.230064 139923852027648 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.2188029289245605, loss=2.3174147605895996
I0202 07:48:36.301249 139923868813056 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.1438772678375244, loss=2.3721466064453125
I0202 07:49:23.048309 139923852027648 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.0306577682495117, loss=2.3749148845672607
I0202 07:50:09.834953 139923868813056 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.1836400032043457, loss=2.123112916946411
I0202 07:50:56.544579 139923852027648 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.176192283630371, loss=2.419243335723877
I0202 07:51:43.249532 139923868813056 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.9373693466186523, loss=4.935985565185547
I0202 07:52:30.004399 139923852027648 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.8238722085952759, loss=4.1543779373168945
I0202 07:53:16.871261 139923868813056 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.863113522529602, loss=3.2246108055114746
I0202 07:53:24.970191 140085747812160 spec.py:321] Evaluating on the training split.
I0202 07:53:35.480522 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 07:54:00.158062 140085747812160 spec.py:349] Evaluating on the test split.
I0202 07:54:01.799663 140085747812160 submission_runner.py:408] Time since start: 48157.13s, 	Step: 95519, 	{'train/accuracy': 0.6943945288658142, 'train/loss': 1.260308861732483, 'validation/accuracy': 0.6425999999046326, 'validation/loss': 1.499654769897461, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.149538278579712, 'test/num_examples': 10000, 'score': 44152.353734731674, 'total_duration': 48157.12686371803, 'accumulated_submission_time': 44152.353734731674, 'accumulated_eval_time': 3995.7055275440216, 'accumulated_logging_time': 3.703071117401123}
I0202 07:54:01.830089 139923852027648 logging_writer.py:48] [95519] accumulated_eval_time=3995.705528, accumulated_logging_time=3.703071, accumulated_submission_time=44152.353735, global_step=95519, preemption_count=0, score=44152.353735, test/accuracy=0.516800, test/loss=2.149538, test/num_examples=10000, total_duration=48157.126864, train/accuracy=0.694395, train/loss=1.260309, validation/accuracy=0.642600, validation/loss=1.499655, validation/num_examples=50000
I0202 07:54:35.327322 139923868813056 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.8141655921936035, loss=2.218014717102051
I0202 07:55:21.907207 139923852027648 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4550487995147705, loss=2.1932358741760254
I0202 07:56:08.658739 139923868813056 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.246812105178833, loss=2.6591429710388184
I0202 07:56:55.056691 139923852027648 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.1093392372131348, loss=4.787560939788818
I0202 07:57:41.924604 139923868813056 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.8327633142471313, loss=4.150689125061035
I0202 07:58:28.886800 139923852027648 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.330294132232666, loss=2.285944700241089
I0202 07:59:15.539467 139923868813056 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.1446990966796875, loss=2.6367952823638916
I0202 08:00:02.334232 139923852027648 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.547132968902588, loss=2.44377064704895
I0202 08:00:49.093626 139923868813056 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.0979018211364746, loss=4.757930278778076
I0202 08:01:01.855376 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:01:12.239909 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:01:37.951067 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:01:39.593332 140085747812160 submission_runner.py:408] Time since start: 48614.92s, 	Step: 96429, 	{'train/accuracy': 0.7122656106948853, 'train/loss': 1.1745492219924927, 'validation/accuracy': 0.6404199600219727, 'validation/loss': 1.485430121421814, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.13046932220459, 'test/num_examples': 10000, 'score': 44572.31914424896, 'total_duration': 48614.920499801636, 'accumulated_submission_time': 44572.31914424896, 'accumulated_eval_time': 4033.443476676941, 'accumulated_logging_time': 3.7422258853912354}
I0202 08:01:39.623599 139923852027648 logging_writer.py:48] [96429] accumulated_eval_time=4033.443477, accumulated_logging_time=3.742226, accumulated_submission_time=44572.319144, global_step=96429, preemption_count=0, score=44572.319144, test/accuracy=0.515700, test/loss=2.130469, test/num_examples=10000, total_duration=48614.920500, train/accuracy=0.712266, train/loss=1.174549, validation/accuracy=0.640420, validation/loss=1.485430, validation/num_examples=50000
I0202 08:02:08.163093 139923868813056 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.143953800201416, loss=2.210813522338867
I0202 08:02:54.969254 139923852027648 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.8950802087783813, loss=3.9436988830566406
I0202 08:03:41.928338 139923868813056 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.3743085861206055, loss=2.2723548412323
I0202 08:04:28.709353 139923852027648 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.8604437112808228, loss=3.735943555831909
I0202 08:05:15.579971 139923868813056 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.4525773525238037, loss=2.269766330718994
I0202 08:06:02.268169 139923852027648 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.212771415710449, loss=2.274671792984009
I0202 08:06:49.054919 139923868813056 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.013298273086548, loss=3.3583452701568604
I0202 08:07:35.844625 139923852027648 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.9635416269302368, loss=3.968489408493042
I0202 08:08:22.675140 139923868813056 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.111447811126709, loss=2.266056776046753
I0202 08:08:39.930669 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:08:50.273365 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:09:17.679935 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:09:19.313214 140085747812160 submission_runner.py:408] Time since start: 49074.64s, 	Step: 97339, 	{'train/accuracy': 0.6898632645606995, 'train/loss': 1.2658751010894775, 'validation/accuracy': 0.6431399583816528, 'validation/loss': 1.4787806272506714, 'validation/num_examples': 50000, 'test/accuracy': 0.5144000053405762, 'test/loss': 2.1275625228881836, 'test/num_examples': 10000, 'score': 44992.565873384476, 'total_duration': 49074.64042210579, 'accumulated_submission_time': 44992.565873384476, 'accumulated_eval_time': 4072.8260283470154, 'accumulated_logging_time': 3.7807199954986572}
I0202 08:09:19.348708 139923852027648 logging_writer.py:48] [97339] accumulated_eval_time=4072.826028, accumulated_logging_time=3.780720, accumulated_submission_time=44992.565873, global_step=97339, preemption_count=0, score=44992.565873, test/accuracy=0.514400, test/loss=2.127563, test/num_examples=10000, total_duration=49074.640422, train/accuracy=0.689863, train/loss=1.265875, validation/accuracy=0.643140, validation/loss=1.478781, validation/num_examples=50000
I0202 08:09:43.661116 139923868813056 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.4104886054992676, loss=2.270021677017212
I0202 08:10:30.128388 139923852027648 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.2807414531707764, loss=2.273991584777832
I0202 08:11:17.326811 139923868813056 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.17142653465271, loss=2.2838947772979736
I0202 08:12:03.976249 139923852027648 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.322300672531128, loss=2.269106388092041
I0202 08:12:50.985204 139923868813056 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.1453802585601807, loss=2.2396442890167236
I0202 08:13:37.773006 139923852027648 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.3449172973632812, loss=2.295156478881836
I0202 08:14:24.555154 139923868813056 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.310422420501709, loss=2.183445930480957
I0202 08:15:11.205602 139923852027648 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.1721532344818115, loss=2.321488380432129
I0202 08:15:58.014633 139923868813056 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.9149585962295532, loss=4.162280082702637
I0202 08:16:19.749973 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:16:30.332063 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:16:54.992443 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:16:56.633131 140085747812160 submission_runner.py:408] Time since start: 49531.96s, 	Step: 98248, 	{'train/accuracy': 0.6936132907867432, 'train/loss': 1.285739541053772, 'validation/accuracy': 0.6411199569702148, 'validation/loss': 1.5151735544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.1551709175109863, 'test/num_examples': 10000, 'score': 45412.90539312363, 'total_duration': 49531.960334300995, 'accumulated_submission_time': 45412.90539312363, 'accumulated_eval_time': 4109.709174156189, 'accumulated_logging_time': 3.8271048069000244}
I0202 08:16:56.669257 139923852027648 logging_writer.py:48] [98248] accumulated_eval_time=4109.709174, accumulated_logging_time=3.827105, accumulated_submission_time=45412.905393, global_step=98248, preemption_count=0, score=45412.905393, test/accuracy=0.520300, test/loss=2.155171, test/num_examples=10000, total_duration=49531.960334, train/accuracy=0.693613, train/loss=1.285740, validation/accuracy=0.641120, validation/loss=1.515174, validation/num_examples=50000
I0202 08:17:17.448100 139923868813056 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.1147515773773193, loss=2.572801351547241
I0202 08:18:02.717481 139923852027648 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.303736448287964, loss=2.2918922901153564
I0202 08:18:49.862124 139923868813056 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.1855766773223877, loss=2.0990982055664062
I0202 08:19:36.645922 139923852027648 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.941939115524292, loss=4.738437652587891
I0202 08:20:23.522135 139923868813056 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.2633447647094727, loss=2.1950466632843018
I0202 08:21:10.336504 139923852027648 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.5949015617370605, loss=2.2557125091552734
I0202 08:21:57.053057 139923868813056 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.1630334854125977, loss=2.197808265686035
I0202 08:22:43.836442 139923852027648 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.333810329437256, loss=2.242233991622925
I0202 08:23:30.369520 139923868813056 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.9506405591964722, loss=3.8779137134552
I0202 08:23:56.773249 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:24:07.281065 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:24:35.331615 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:24:36.967465 140085747812160 submission_runner.py:408] Time since start: 49992.29s, 	Step: 99158, 	{'train/accuracy': 0.7100781202316284, 'train/loss': 1.1822881698608398, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4824674129486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1205554008483887, 'test/num_examples': 10000, 'score': 45832.94949889183, 'total_duration': 49992.294642686844, 'accumulated_submission_time': 45832.94949889183, 'accumulated_eval_time': 4149.903354167938, 'accumulated_logging_time': 3.872529983520508}
I0202 08:24:37.002998 139923852027648 logging_writer.py:48] [99158] accumulated_eval_time=4149.903354, accumulated_logging_time=3.872530, accumulated_submission_time=45832.949499, global_step=99158, preemption_count=0, score=45832.949499, test/accuracy=0.519900, test/loss=2.120555, test/num_examples=10000, total_duration=49992.294643, train/accuracy=0.710078, train/loss=1.182288, validation/accuracy=0.642360, validation/loss=1.482467, validation/num_examples=50000
I0202 08:24:53.882143 139923868813056 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.0163004398345947, loss=3.479936361312866
I0202 08:25:38.824130 139923852027648 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.270108222961426, loss=2.235591411590576
I0202 08:26:25.891556 139923868813056 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.076470136642456, loss=3.1106903553009033
I0202 08:27:12.611706 139923852027648 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.0821495056152344, loss=4.895930290222168
I0202 08:27:59.349559 139923868813056 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.3119025230407715, loss=2.3379902839660645
I0202 08:28:45.930130 139923852027648 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.2698018550872803, loss=2.7225406169891357
I0202 08:29:32.532527 139923868813056 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.077077627182007, loss=2.4285435676574707
I0202 08:30:19.506267 139923852027648 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.194898843765259, loss=2.255563497543335
I0202 08:31:06.511692 139923868813056 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.387460470199585, loss=2.1956870555877686
I0202 08:31:37.390683 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:31:47.642959 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:32:14.787466 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:32:16.431397 140085747812160 submission_runner.py:408] Time since start: 50451.76s, 	Step: 100068, 	{'train/accuracy': 0.69677734375, 'train/loss': 1.2403640747070312, 'validation/accuracy': 0.6484400033950806, 'validation/loss': 1.4505292177200317, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.0834946632385254, 'test/num_examples': 10000, 'score': 46253.277426958084, 'total_duration': 50451.75859832764, 'accumulated_submission_time': 46253.277426958084, 'accumulated_eval_time': 4188.944055318832, 'accumulated_logging_time': 3.9171104431152344}
I0202 08:32:16.465519 139923852027648 logging_writer.py:48] [100068] accumulated_eval_time=4188.944055, accumulated_logging_time=3.917110, accumulated_submission_time=46253.277427, global_step=100068, preemption_count=0, score=46253.277427, test/accuracy=0.526800, test/loss=2.083495, test/num_examples=10000, total_duration=50451.758598, train/accuracy=0.696777, train/loss=1.240364, validation/accuracy=0.648440, validation/loss=1.450529, validation/num_examples=50000
I0202 08:32:29.417792 139923868813056 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.364454507827759, loss=2.3037257194519043
I0202 08:33:13.490018 139923852027648 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.8030409812927246, loss=3.4582133293151855
I0202 08:34:00.008538 139923868813056 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.398282766342163, loss=2.2473950386047363
I0202 08:34:47.092749 139923852027648 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.390462875366211, loss=2.288041353225708
I0202 08:35:33.741025 139923868813056 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.270629405975342, loss=3.9921746253967285
I0202 08:36:20.728936 139923852027648 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.9731870889663696, loss=3.310368537902832
I0202 08:37:07.441655 139923868813056 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.9418284893035889, loss=3.266554355621338
I0202 08:37:54.160212 139923852027648 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.8928654193878174, loss=3.3901474475860596
I0202 08:38:41.044193 139923868813056 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.1817309856414795, loss=3.7436718940734863
I0202 08:39:16.870928 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:39:27.086116 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:39:52.588649 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:39:54.233692 140085747812160 submission_runner.py:408] Time since start: 50909.56s, 	Step: 100978, 	{'train/accuracy': 0.7009961009025574, 'train/loss': 1.234709620475769, 'validation/accuracy': 0.6490600109100342, 'validation/loss': 1.4626942873001099, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.1023662090301514, 'test/num_examples': 10000, 'score': 46673.61992907524, 'total_duration': 50909.56090068817, 'accumulated_submission_time': 46673.61992907524, 'accumulated_eval_time': 4226.306841373444, 'accumulated_logging_time': 3.9611897468566895}
I0202 08:39:54.269242 139923852027648 logging_writer.py:48] [100978] accumulated_eval_time=4226.306841, accumulated_logging_time=3.961190, accumulated_submission_time=46673.619929, global_step=100978, preemption_count=0, score=46673.619929, test/accuracy=0.520000, test/loss=2.102366, test/num_examples=10000, total_duration=50909.560901, train/accuracy=0.700996, train/loss=1.234710, validation/accuracy=0.649060, validation/loss=1.462694, validation/num_examples=50000
I0202 08:40:03.718755 139923868813056 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.3578662872314453, loss=2.2396202087402344
I0202 08:40:46.951811 139923852027648 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.241116523742676, loss=2.31234073638916
I0202 08:41:33.890510 139923868813056 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.1938700675964355, loss=2.1426453590393066
I0202 08:42:20.771737 139923852027648 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.0680651664733887, loss=2.5490922927856445
I0202 08:43:08.097185 139923868813056 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.1816604137420654, loss=2.1454944610595703
I0202 08:43:54.831093 139923852027648 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.418201208114624, loss=2.5274925231933594
I0202 08:44:41.684794 139923868813056 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.2130513191223145, loss=2.529445171356201
I0202 08:45:28.589546 139923852027648 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.2277333736419678, loss=3.0230712890625
I0202 08:46:15.373506 139923868813056 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.234612464904785, loss=4.8475728034973145
I0202 08:46:54.332054 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:47:04.802075 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:47:33.602894 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:47:35.237851 140085747812160 submission_runner.py:408] Time since start: 51370.57s, 	Step: 101885, 	{'train/accuracy': 0.7134765386581421, 'train/loss': 1.1484699249267578, 'validation/accuracy': 0.6495400071144104, 'validation/loss': 1.434063196182251, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.0676376819610596, 'test/num_examples': 10000, 'score': 47093.20279479027, 'total_duration': 51370.565054893494, 'accumulated_submission_time': 47093.20279479027, 'accumulated_eval_time': 4267.212629556656, 'accumulated_logging_time': 4.426010370254517}
I0202 08:47:35.270056 139923852027648 logging_writer.py:48] [101885] accumulated_eval_time=4267.212630, accumulated_logging_time=4.426010, accumulated_submission_time=47093.202795, global_step=101885, preemption_count=0, score=47093.202795, test/accuracy=0.525000, test/loss=2.067638, test/num_examples=10000, total_duration=51370.565055, train/accuracy=0.713477, train/loss=1.148470, validation/accuracy=0.649540, validation/loss=1.434063, validation/num_examples=50000
I0202 08:47:41.552174 139923868813056 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.156513214111328, loss=4.460211277008057
I0202 08:48:24.403965 139923852027648 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.114546537399292, loss=2.4592361450195312
I0202 08:49:11.245168 139923868813056 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.9411407709121704, loss=2.166414737701416
I0202 08:49:58.097554 139923852027648 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.152202844619751, loss=2.366246223449707
I0202 08:50:44.920667 139923868813056 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.220144033432007, loss=2.3121321201324463
I0202 08:51:31.681623 139923852027648 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.4916114807128906, loss=2.072633743286133
I0202 08:52:18.526980 139923868813056 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.2618603706359863, loss=4.633038520812988
I0202 08:53:05.630718 139923852027648 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.3293116092681885, loss=2.1636383533477783
I0202 08:53:52.262646 139923868813056 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.019505739212036, loss=3.897324562072754
I0202 08:54:35.544016 140085747812160 spec.py:321] Evaluating on the training split.
I0202 08:54:45.826325 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 08:55:10.105581 140085747812160 spec.py:349] Evaluating on the test split.
I0202 08:55:11.741794 140085747812160 submission_runner.py:408] Time since start: 51827.07s, 	Step: 102794, 	{'train/accuracy': 0.69691401720047, 'train/loss': 1.2173900604248047, 'validation/accuracy': 0.6519399881362915, 'validation/loss': 1.4335988759994507, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.074958086013794, 'test/num_examples': 10000, 'score': 47513.41642546654, 'total_duration': 51827.06900215149, 'accumulated_submission_time': 47513.41642546654, 'accumulated_eval_time': 4303.410413980484, 'accumulated_logging_time': 4.468097686767578}
I0202 08:55:11.778057 139923852027648 logging_writer.py:48] [102794] accumulated_eval_time=4303.410414, accumulated_logging_time=4.468098, accumulated_submission_time=47513.416425, global_step=102794, preemption_count=0, score=47513.416425, test/accuracy=0.530100, test/loss=2.074958, test/num_examples=10000, total_duration=51827.069002, train/accuracy=0.696914, train/loss=1.217390, validation/accuracy=0.651940, validation/loss=1.433599, validation/num_examples=50000
I0202 08:55:14.527645 139923868813056 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.8880302906036377, loss=4.7149553298950195
I0202 08:55:56.741857 139923852027648 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.06160044670105, loss=3.1841013431549072
I0202 08:56:43.348561 139923868813056 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.017942428588867, loss=4.7845659255981445
I0202 08:57:30.253534 139923852027648 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.2447667121887207, loss=2.3273096084594727
I0202 08:58:16.813854 139923868813056 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.139767646789551, loss=3.2386207580566406
I0202 08:59:03.655177 139923852027648 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.3035147190093994, loss=2.091182231903076
I0202 08:59:50.385439 139923868813056 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.33174991607666, loss=2.1303327083587646
I0202 09:00:37.115658 139923852027648 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.2292985916137695, loss=2.2639150619506836
I0202 09:01:24.144545 139923868813056 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.121708869934082, loss=3.544635772705078
I0202 09:02:10.809094 139923852027648 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.2772202491760254, loss=2.135314702987671
I0202 09:02:11.924603 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:02:22.539430 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:02:53.216594 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:02:54.854155 140085747812160 submission_runner.py:408] Time since start: 52290.18s, 	Step: 103704, 	{'train/accuracy': 0.7031054496765137, 'train/loss': 1.2242692708969116, 'validation/accuracy': 0.6475200057029724, 'validation/loss': 1.4630873203277588, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.095916271209717, 'test/num_examples': 10000, 'score': 47933.50048518181, 'total_duration': 52290.1813583374, 'accumulated_submission_time': 47933.50048518181, 'accumulated_eval_time': 4346.339977025986, 'accumulated_logging_time': 4.515044212341309}
I0202 09:02:54.885215 139923868813056 logging_writer.py:48] [103704] accumulated_eval_time=4346.339977, accumulated_logging_time=4.515044, accumulated_submission_time=47933.500485, global_step=103704, preemption_count=0, score=47933.500485, test/accuracy=0.526600, test/loss=2.095916, test/num_examples=10000, total_duration=52290.181358, train/accuracy=0.703105, train/loss=1.224269, validation/accuracy=0.647520, validation/loss=1.463087, validation/num_examples=50000
I0202 09:03:35.193405 139923852027648 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.3857929706573486, loss=2.1678690910339355
I0202 09:04:21.800200 139923868813056 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.337357759475708, loss=2.2448008060455322
I0202 09:05:08.706639 139923852027648 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.92851984500885, loss=4.317103862762451
I0202 09:05:55.440340 139923868813056 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.3526854515075684, loss=2.228776693344116
I0202 09:06:42.241473 139923852027648 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.294518232345581, loss=2.5033578872680664
I0202 09:07:28.898273 139923868813056 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.341752290725708, loss=2.132336378097534
I0202 09:08:15.917701 139923852027648 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.2147862911224365, loss=4.325581073760986
I0202 09:09:02.617969 139923868813056 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.357492208480835, loss=2.208118200302124
I0202 09:09:49.368433 139923852027648 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.239518165588379, loss=2.431440830230713
I0202 09:09:55.088880 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:10:05.689374 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:10:37.648877 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:10:39.287024 140085747812160 submission_runner.py:408] Time since start: 52754.61s, 	Step: 104614, 	{'train/accuracy': 0.7122460603713989, 'train/loss': 1.1754224300384521, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.444462776184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.080199718475342, 'test/num_examples': 10000, 'score': 48353.644204854965, 'total_duration': 52754.61422896385, 'accumulated_submission_time': 48353.644204854965, 'accumulated_eval_time': 4390.538145542145, 'accumulated_logging_time': 4.555092096328735}
I0202 09:10:39.318738 139923868813056 logging_writer.py:48] [104614] accumulated_eval_time=4390.538146, accumulated_logging_time=4.555092, accumulated_submission_time=48353.644205, global_step=104614, preemption_count=0, score=48353.644205, test/accuracy=0.529300, test/loss=2.080200, test/num_examples=10000, total_duration=52754.614229, train/accuracy=0.712246, train/loss=1.175422, validation/accuracy=0.654560, validation/loss=1.444463, validation/num_examples=50000
I0202 09:11:15.185136 139923852027648 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.115309953689575, loss=2.165212392807007
I0202 09:12:01.835347 139923868813056 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.0593111515045166, loss=3.041842460632324
I0202 09:12:49.051324 139923852027648 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.441937208175659, loss=2.1833910942077637
I0202 09:13:35.764243 139923868813056 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.2759876251220703, loss=2.2133231163024902
I0202 09:14:22.741377 139923852027648 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.5576059818267822, loss=2.217571258544922
I0202 09:15:09.329942 139923868813056 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.8568528890609741, loss=4.357265472412109
I0202 09:15:56.231800 139923852027648 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.0527143478393555, loss=2.5765738487243652
I0202 09:16:42.713600 139923868813056 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.1462175846099854, loss=2.690263032913208
I0202 09:17:29.695476 139923852027648 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.314220905303955, loss=2.1329405307769775
I0202 09:17:39.292752 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:17:49.729588 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:18:19.130535 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:18:20.769446 140085747812160 submission_runner.py:408] Time since start: 53216.10s, 	Step: 105522, 	{'train/accuracy': 0.7056835889816284, 'train/loss': 1.2042386531829834, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.425550937652588, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.077850341796875, 'test/num_examples': 10000, 'score': 48773.55797100067, 'total_duration': 53216.09664773941, 'accumulated_submission_time': 48773.55797100067, 'accumulated_eval_time': 4432.014835357666, 'accumulated_logging_time': 4.596234560012817}
I0202 09:18:20.804272 139923868813056 logging_writer.py:48] [105522] accumulated_eval_time=4432.014835, accumulated_logging_time=4.596235, accumulated_submission_time=48773.557971, global_step=105522, preemption_count=0, score=48773.557971, test/accuracy=0.525600, test/loss=2.077850, test/num_examples=10000, total_duration=53216.096648, train/accuracy=0.705684, train/loss=1.204239, validation/accuracy=0.656540, validation/loss=1.425551, validation/num_examples=50000
I0202 09:18:52.990488 139923852027648 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.574221611022949, loss=2.1432242393493652
I0202 09:19:39.607105 139923868813056 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.0193052291870117, loss=3.4420459270477295
I0202 09:20:26.559681 139923852027648 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.4276316165924072, loss=2.2415151596069336
I0202 09:21:13.173130 139923868813056 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.0679240226745605, loss=3.963139533996582
I0202 09:22:00.053274 139923852027648 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.0034255981445312, loss=4.063728332519531
I0202 09:22:46.954513 139923868813056 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.4470338821411133, loss=2.2544095516204834
I0202 09:23:33.897726 139923852027648 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.1187310218811035, loss=2.423562526702881
I0202 09:24:20.720441 139923868813056 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.4649806022644043, loss=4.735362529754639
I0202 09:25:07.553490 139923852027648 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.665402889251709, loss=2.2788493633270264
I0202 09:25:20.824038 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:25:31.204961 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:26:02.734510 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:26:04.375916 140085747812160 submission_runner.py:408] Time since start: 53679.70s, 	Step: 106430, 	{'train/accuracy': 0.7126562595367432, 'train/loss': 1.1635273694992065, 'validation/accuracy': 0.6616799831390381, 'validation/loss': 1.3938157558441162, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.0336849689483643, 'test/num_examples': 10000, 'score': 49193.51614046097, 'total_duration': 53679.70312547684, 'accumulated_submission_time': 49193.51614046097, 'accumulated_eval_time': 4475.56670665741, 'accumulated_logging_time': 4.640961647033691}
I0202 09:26:04.410710 139923868813056 logging_writer.py:48] [106430] accumulated_eval_time=4475.566707, accumulated_logging_time=4.640962, accumulated_submission_time=49193.516140, global_step=106430, preemption_count=0, score=49193.516140, test/accuracy=0.539400, test/loss=2.033685, test/num_examples=10000, total_duration=53679.703125, train/accuracy=0.712656, train/loss=1.163527, validation/accuracy=0.661680, validation/loss=1.393816, validation/num_examples=50000
I0202 09:26:32.570883 139923852027648 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.235513687133789, loss=2.6839544773101807
I0202 09:27:19.305422 139923868813056 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.4214413166046143, loss=2.2449684143066406
I0202 09:28:06.506301 139923852027648 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.444519281387329, loss=2.1185309886932373
I0202 09:28:53.104980 139923868813056 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.2241463661193848, loss=2.295574426651001
I0202 09:29:40.000631 139923852027648 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.1636340618133545, loss=3.251828193664551
I0202 09:30:26.808751 139923868813056 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.181360960006714, loss=2.5885798931121826
I0202 09:31:13.686240 139923852027648 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.3070523738861084, loss=2.229462146759033
I0202 09:32:00.761766 139923868813056 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.4527461528778076, loss=2.097125768661499
I0202 09:32:47.939488 139923852027648 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.491351842880249, loss=2.269221782684326
I0202 09:33:04.600960 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:33:15.135856 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:33:42.720009 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:33:44.357778 140085747812160 submission_runner.py:408] Time since start: 54139.68s, 	Step: 107337, 	{'train/accuracy': 0.717968761920929, 'train/loss': 1.1395562887191772, 'validation/accuracy': 0.6581799983978271, 'validation/loss': 1.4131308794021606, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.072439670562744, 'test/num_examples': 10000, 'score': 49613.643409490585, 'total_duration': 54139.6849834919, 'accumulated_submission_time': 49613.643409490585, 'accumulated_eval_time': 4515.323534250259, 'accumulated_logging_time': 4.687347173690796}
I0202 09:33:44.389778 139923868813056 logging_writer.py:48] [107337] accumulated_eval_time=4515.323534, accumulated_logging_time=4.687347, accumulated_submission_time=49613.643409, global_step=107337, preemption_count=0, score=49613.643409, test/accuracy=0.532200, test/loss=2.072440, test/num_examples=10000, total_duration=54139.684983, train/accuracy=0.717969, train/loss=1.139556, validation/accuracy=0.658180, validation/loss=1.413131, validation/num_examples=50000
I0202 09:34:09.626720 139923852027648 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.291602611541748, loss=2.5294058322906494
I0202 09:34:55.995187 139923868813056 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.2596688270568848, loss=2.2283506393432617
I0202 09:35:43.280616 139923852027648 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.281270980834961, loss=2.0091915130615234
I0202 09:36:29.642220 139923868813056 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.3691470623016357, loss=2.300739288330078
I0202 09:37:16.564788 139923852027648 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.7555012702941895, loss=2.1956634521484375
I0202 09:38:03.589101 139923868813056 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.0061943531036377, loss=3.47930908203125
I0202 09:38:50.529360 139923852027648 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.6516611576080322, loss=2.1461122035980225
I0202 09:39:37.336347 139923868813056 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.1538665294647217, loss=2.7733774185180664
I0202 09:40:24.070689 139923852027648 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.2531301975250244, loss=4.76251745223999
I0202 09:40:44.767874 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:40:55.192331 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:41:21.338888 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:41:22.976355 140085747812160 submission_runner.py:408] Time since start: 54598.30s, 	Step: 108246, 	{'train/accuracy': 0.7106640338897705, 'train/loss': 1.1932311058044434, 'validation/accuracy': 0.6593999862670898, 'validation/loss': 1.424089789390564, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.076124429702759, 'test/num_examples': 10000, 'score': 50033.96087384224, 'total_duration': 54598.303564071655, 'accumulated_submission_time': 50033.96087384224, 'accumulated_eval_time': 4553.532001495361, 'accumulated_logging_time': 4.729357481002808}
I0202 09:41:23.011525 139923868813056 logging_writer.py:48] [108246] accumulated_eval_time=4553.532001, accumulated_logging_time=4.729357, accumulated_submission_time=50033.960874, global_step=108246, preemption_count=0, score=50033.960874, test/accuracy=0.530900, test/loss=2.076124, test/num_examples=10000, total_duration=54598.303564, train/accuracy=0.710664, train/loss=1.193231, validation/accuracy=0.659400, validation/loss=1.424090, validation/num_examples=50000
I0202 09:41:44.586588 139923852027648 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.4267072677612305, loss=2.103886365890503
I0202 09:42:30.311989 139923868813056 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.3272359371185303, loss=2.181144952774048
I0202 09:43:17.345108 139923852027648 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.3575830459594727, loss=4.495597839355469
I0202 09:44:04.243055 139923868813056 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.9634090662002563, loss=3.486299991607666
I0202 09:44:50.907959 139923852027648 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.6440927982330322, loss=2.0553221702575684
I0202 09:45:37.556569 139923868813056 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.577524423599243, loss=2.1331846714019775
I0202 09:46:24.547142 139923852027648 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.343400478363037, loss=2.2106528282165527
I0202 09:47:11.418755 139923868813056 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.115997314453125, loss=2.422551155090332
I0202 09:47:58.183412 139923852027648 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.1687326431274414, loss=2.3193368911743164
I0202 09:48:23.231364 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:48:33.568110 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:49:02.908264 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:49:04.551191 140085747812160 submission_runner.py:408] Time since start: 55059.88s, 	Step: 109155, 	{'train/accuracy': 0.7136523127555847, 'train/loss': 1.1612012386322021, 'validation/accuracy': 0.6612799763679504, 'validation/loss': 1.3903865814208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.0257108211517334, 'test/num_examples': 10000, 'score': 50454.120992183685, 'total_duration': 55059.87839961052, 'accumulated_submission_time': 50454.120992183685, 'accumulated_eval_time': 4594.851831197739, 'accumulated_logging_time': 4.773675441741943}
I0202 09:49:04.587723 139923868813056 logging_writer.py:48] [109155] accumulated_eval_time=4594.851831, accumulated_logging_time=4.773675, accumulated_submission_time=50454.120992, global_step=109155, preemption_count=0, score=50454.120992, test/accuracy=0.536700, test/loss=2.025711, test/num_examples=10000, total_duration=55059.878400, train/accuracy=0.713652, train/loss=1.161201, validation/accuracy=0.661280, validation/loss=1.390387, validation/num_examples=50000
I0202 09:49:22.629932 139923852027648 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.27860426902771, loss=2.5197489261627197
I0202 09:50:07.742094 139923868813056 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.0669000148773193, loss=3.3607592582702637
I0202 09:50:54.563983 139923852027648 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5598270893096924, loss=2.187833547592163
I0202 09:51:41.471163 139923868813056 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.304469347000122, loss=3.552255868911743
I0202 09:52:28.476499 139923852027648 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.389915943145752, loss=2.204735040664673
I0202 09:53:15.196348 139923868813056 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.0241546630859375, loss=3.062044382095337
I0202 09:54:01.946372 139923852027648 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.2425527572631836, loss=2.062743663787842
I0202 09:54:48.716483 139923868813056 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.295572519302368, loss=2.1516129970550537
I0202 09:55:35.574665 139923852027648 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.3048086166381836, loss=2.938610553741455
I0202 09:56:04.574196 140085747812160 spec.py:321] Evaluating on the training split.
I0202 09:56:15.201352 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 09:56:43.751439 140085747812160 spec.py:349] Evaluating on the test split.
I0202 09:56:45.402628 140085747812160 submission_runner.py:408] Time since start: 55520.73s, 	Step: 110063, 	{'train/accuracy': 0.724804699420929, 'train/loss': 1.1234993934631348, 'validation/accuracy': 0.6626799702644348, 'validation/loss': 1.3978769779205322, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.0300371646881104, 'test/num_examples': 10000, 'score': 50874.046491622925, 'total_duration': 55520.72982406616, 'accumulated_submission_time': 50874.046491622925, 'accumulated_eval_time': 4635.680241346359, 'accumulated_logging_time': 4.820109605789185}
I0202 09:56:45.442986 139923868813056 logging_writer.py:48] [110063] accumulated_eval_time=4635.680241, accumulated_logging_time=4.820110, accumulated_submission_time=50874.046492, global_step=110063, preemption_count=0, score=50874.046492, test/accuracy=0.538800, test/loss=2.030037, test/num_examples=10000, total_duration=55520.729824, train/accuracy=0.724805, train/loss=1.123499, validation/accuracy=0.662680, validation/loss=1.397877, validation/num_examples=50000
I0202 09:57:00.356332 139923852027648 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.4713666439056396, loss=4.217562198638916
I0202 09:57:44.610150 139923868813056 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.3338754177093506, loss=2.1319124698638916
I0202 09:58:31.186862 139923852027648 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.995105504989624, loss=4.325577259063721
I0202 09:59:18.087205 139923868813056 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.2049427032470703, loss=4.34901762008667
I0202 10:00:05.020195 139923852027648 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.1449859142303467, loss=3.8993921279907227
I0202 10:00:51.820977 139923868813056 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.3567044734954834, loss=2.024350643157959
I0202 10:01:38.706374 139923852027648 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.7084527015686035, loss=2.099508762359619
I0202 10:02:25.352669 139923868813056 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.576202392578125, loss=1.9809088706970215
I0202 10:03:12.126032 139923852027648 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.333850860595703, loss=2.4482040405273438
I0202 10:03:45.435213 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:03:55.793856 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:04:21.344556 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:04:22.989911 140085747812160 submission_runner.py:408] Time since start: 55978.32s, 	Step: 110973, 	{'train/accuracy': 0.7303124666213989, 'train/loss': 1.0987309217453003, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.3945322036743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.026243209838867, 'test/num_examples': 10000, 'score': 51293.97769546509, 'total_duration': 55978.31710648537, 'accumulated_submission_time': 51293.97769546509, 'accumulated_eval_time': 4673.234929800034, 'accumulated_logging_time': 4.870450496673584}
I0202 10:04:23.026522 139923868813056 logging_writer.py:48] [110973] accumulated_eval_time=4673.234930, accumulated_logging_time=4.870450, accumulated_submission_time=51293.977695, global_step=110973, preemption_count=0, score=51293.977695, test/accuracy=0.536400, test/loss=2.026243, test/num_examples=10000, total_duration=55978.317106, train/accuracy=0.730312, train/loss=1.098731, validation/accuracy=0.664720, validation/loss=1.394532, validation/num_examples=50000
I0202 10:04:34.025532 139923852027648 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.598113536834717, loss=2.2646265029907227
I0202 10:05:17.605697 139923868813056 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.2566866874694824, loss=4.6864495277404785
I0202 10:06:04.462577 139923852027648 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.2319064140319824, loss=4.602121829986572
I0202 10:06:51.278411 139923868813056 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.248518705368042, loss=2.674203395843506
I0202 10:07:38.173813 139923852027648 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.1220414638519287, loss=3.0838253498077393
I0202 10:08:24.875141 139923868813056 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.4901022911071777, loss=2.103935718536377
I0202 10:09:11.515598 139923852027648 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.524624824523926, loss=3.4522268772125244
I0202 10:09:58.206783 139923868813056 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.3073599338531494, loss=2.3929595947265625
I0202 10:10:44.849690 139923852027648 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.2100930213928223, loss=4.814971923828125
I0202 10:11:23.412036 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:11:33.559019 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:12:03.219565 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:12:04.853837 140085747812160 submission_runner.py:408] Time since start: 56440.18s, 	Step: 111884, 	{'train/accuracy': 0.7140820026397705, 'train/loss': 1.1694566011428833, 'validation/accuracy': 0.6618799567222595, 'validation/loss': 1.3972446918487549, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.036576271057129, 'test/num_examples': 10000, 'score': 51714.3005130291, 'total_duration': 56440.18104696274, 'accumulated_submission_time': 51714.3005130291, 'accumulated_eval_time': 4714.676728963852, 'accumulated_logging_time': 4.919151782989502}
I0202 10:12:04.887939 139923868813056 logging_writer.py:48] [111884] accumulated_eval_time=4714.676729, accumulated_logging_time=4.919152, accumulated_submission_time=51714.300513, global_step=111884, preemption_count=0, score=51714.300513, test/accuracy=0.537900, test/loss=2.036576, test/num_examples=10000, total_duration=56440.181047, train/accuracy=0.714082, train/loss=1.169457, validation/accuracy=0.661880, validation/loss=1.397245, validation/num_examples=50000
I0202 10:12:11.571437 139923852027648 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.195488452911377, loss=3.3636398315429688
I0202 10:12:54.664901 139923868813056 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.4962172508239746, loss=2.1138622760772705
I0202 10:13:41.552116 139923852027648 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.3019843101501465, loss=3.533764362335205
I0202 10:14:28.459957 139923868813056 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.1481106281280518, loss=4.5973734855651855
I0202 10:15:15.230845 139923852027648 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.2245137691497803, loss=2.954190731048584
I0202 10:16:01.997661 139923868813056 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.7749545574188232, loss=2.3596067428588867
I0202 10:16:48.543498 139923852027648 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.4636383056640625, loss=2.0950002670288086
I0202 10:17:35.627689 139923868813056 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.375358819961548, loss=3.5226783752441406
I0202 10:18:22.284526 139923852027648 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.5869338512420654, loss=2.1404528617858887
I0202 10:19:05.037733 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:19:15.500744 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:19:42.579784 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:19:44.212457 140085747812160 submission_runner.py:408] Time since start: 56899.54s, 	Step: 112793, 	{'train/accuracy': 0.7234960794448853, 'train/loss': 1.1330419778823853, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.3698722124099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0242462158203125, 'test/num_examples': 10000, 'score': 52134.38846230507, 'total_duration': 56899.53965449333, 'accumulated_submission_time': 52134.38846230507, 'accumulated_eval_time': 4753.851475954056, 'accumulated_logging_time': 4.964050054550171}
I0202 10:19:44.246772 139923868813056 logging_writer.py:48] [112793] accumulated_eval_time=4753.851476, accumulated_logging_time=4.964050, accumulated_submission_time=52134.388462, global_step=112793, preemption_count=0, score=52134.388462, test/accuracy=0.540600, test/loss=2.024246, test/num_examples=10000, total_duration=56899.539654, train/accuracy=0.723496, train/loss=1.133042, validation/accuracy=0.668740, validation/loss=1.369872, validation/num_examples=50000
I0202 10:19:47.398306 139923852027648 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.9982279539108276, loss=3.7659451961517334
I0202 10:20:29.421222 139923868813056 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.562347173690796, loss=2.1785054206848145
I0202 10:21:16.392313 139923852027648 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.189000368118286, loss=4.21330451965332
I0202 10:22:03.385413 139923868813056 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.552133321762085, loss=2.1783266067504883
I0202 10:22:50.538494 139923852027648 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.2580726146698, loss=3.4969537258148193
I0202 10:23:37.236483 139923868813056 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.4781010150909424, loss=2.100576877593994
I0202 10:24:24.121772 139923852027648 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.4500298500061035, loss=2.007016181945801
I0202 10:25:11.031778 139923868813056 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.2688870429992676, loss=2.3069686889648438
I0202 10:25:57.817092 139923852027648 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.532207489013672, loss=2.031214475631714
I0202 10:26:44.989980 139923868813056 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.7383334636688232, loss=2.0919270515441895
I0202 10:26:45.006299 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:26:55.602514 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:27:23.244440 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:27:24.888504 140085747812160 submission_runner.py:408] Time since start: 57360.22s, 	Step: 113701, 	{'train/accuracy': 0.74085932970047, 'train/loss': 1.0626214742660522, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.3853868246078491, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.020291805267334, 'test/num_examples': 10000, 'score': 52555.08496952057, 'total_duration': 57360.21570634842, 'accumulated_submission_time': 52555.08496952057, 'accumulated_eval_time': 4793.7336666584015, 'accumulated_logging_time': 5.01096510887146}
I0202 10:27:24.922321 139923852027648 logging_writer.py:48] [113701] accumulated_eval_time=4793.733667, accumulated_logging_time=5.010965, accumulated_submission_time=52555.084970, global_step=113701, preemption_count=0, score=52555.084970, test/accuracy=0.544900, test/loss=2.020292, test/num_examples=10000, total_duration=57360.215706, train/accuracy=0.740859, train/loss=1.062621, validation/accuracy=0.667460, validation/loss=1.385387, validation/num_examples=50000
I0202 10:28:06.822771 139923868813056 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.7688863277435303, loss=2.1450886726379395
I0202 10:28:53.667829 139923852027648 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.3018953800201416, loss=4.139878749847412
I0202 10:29:40.912165 139923868813056 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.2383272647857666, loss=4.344146728515625
I0202 10:30:27.967694 139923852027648 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.364297389984131, loss=2.1691360473632812
I0202 10:31:14.963773 139923868813056 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.5061163902282715, loss=3.0475423336029053
I0202 10:32:01.651937 139923852027648 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.3670501708984375, loss=3.835827589035034
I0202 10:32:48.925520 139923868813056 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.4297189712524414, loss=2.163675308227539
I0202 10:33:35.853111 139923852027648 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.2748148441314697, loss=2.8627023696899414
I0202 10:34:22.677820 139923868813056 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.6512811183929443, loss=2.212770938873291
I0202 10:34:25.154928 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:34:35.515578 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:35:05.677240 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:35:07.318673 140085747812160 submission_runner.py:408] Time since start: 57822.65s, 	Step: 114607, 	{'train/accuracy': 0.7202929258346558, 'train/loss': 1.1391555070877075, 'validation/accuracy': 0.6705399751663208, 'validation/loss': 1.3693618774414062, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.0146656036376953, 'test/num_examples': 10000, 'score': 52975.2583398819, 'total_duration': 57822.64588141441, 'accumulated_submission_time': 52975.2583398819, 'accumulated_eval_time': 4835.897415399551, 'accumulated_logging_time': 5.053654193878174}
I0202 10:35:07.351632 139923852027648 logging_writer.py:48] [114607] accumulated_eval_time=4835.897415, accumulated_logging_time=5.053654, accumulated_submission_time=52975.258340, global_step=114607, preemption_count=0, score=52975.258340, test/accuracy=0.546100, test/loss=2.014666, test/num_examples=10000, total_duration=57822.645881, train/accuracy=0.720293, train/loss=1.139156, validation/accuracy=0.670540, validation/loss=1.369362, validation/num_examples=50000
I0202 10:35:46.309824 139923868813056 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.492192268371582, loss=2.1969308853149414
I0202 10:36:32.904291 139923852027648 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.2830982208251953, loss=3.6781904697418213
I0202 10:37:20.049877 139923868813056 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.530627965927124, loss=2.0615148544311523
I0202 10:38:06.836045 139923852027648 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.330437183380127, loss=4.438460350036621
I0202 10:38:53.808141 139923868813056 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.9321439266204834, loss=2.0661215782165527
I0202 10:39:40.583981 139923852027648 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.796031951904297, loss=2.1590282917022705
I0202 10:40:27.477388 139923868813056 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.808525800704956, loss=2.071117401123047
I0202 10:41:14.527226 139923852027648 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.3663887977600098, loss=2.414473295211792
I0202 10:42:01.549769 139923868813056 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.2318131923675537, loss=2.904818058013916
I0202 10:42:07.384600 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:42:17.831268 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:42:46.846760 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:42:48.491678 140085747812160 submission_runner.py:408] Time since start: 58283.82s, 	Step: 115514, 	{'train/accuracy': 0.7275585532188416, 'train/loss': 1.0852470397949219, 'validation/accuracy': 0.6701200008392334, 'validation/loss': 1.3525265455245972, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 1.9930286407470703, 'test/num_examples': 10000, 'score': 53395.23126530647, 'total_duration': 58283.81888151169, 'accumulated_submission_time': 53395.23126530647, 'accumulated_eval_time': 4877.004498958588, 'accumulated_logging_time': 5.096050024032593}
I0202 10:42:48.527868 139923852027648 logging_writer.py:48] [115514] accumulated_eval_time=4877.004499, accumulated_logging_time=5.096050, accumulated_submission_time=53395.231265, global_step=115514, preemption_count=0, score=53395.231265, test/accuracy=0.544900, test/loss=1.993029, test/num_examples=10000, total_duration=58283.818882, train/accuracy=0.727559, train/loss=1.085247, validation/accuracy=0.670120, validation/loss=1.352527, validation/num_examples=50000
I0202 10:43:24.431719 139923868813056 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.3603389263153076, loss=3.9522833824157715
I0202 10:44:10.946858 139923852027648 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.4754819869995117, loss=2.084270477294922
I0202 10:44:58.084527 139923868813056 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.229642152786255, loss=2.6242737770080566
I0202 10:45:44.740575 139923852027648 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7063369750976562, loss=1.9746594429016113
I0202 10:46:31.736428 139923868813056 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.483181953430176, loss=2.0434110164642334
I0202 10:47:18.640715 139923852027648 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.8679990768432617, loss=2.072481155395508
I0202 10:48:05.761807 139923868813056 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.4677135944366455, loss=2.1662862300872803
I0202 10:48:52.613648 139923852027648 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.3880372047424316, loss=1.9261842966079712
I0202 10:49:39.768095 139923868813056 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.5599427223205566, loss=2.0961592197418213
I0202 10:49:48.726746 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:49:58.988603 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:50:29.512885 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:50:31.150588 140085747812160 submission_runner.py:408] Time since start: 58746.48s, 	Step: 116421, 	{'train/accuracy': 0.7390038967132568, 'train/loss': 1.067587971687317, 'validation/accuracy': 0.6695599555969238, 'validation/loss': 1.372552514076233, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.023348093032837, 'test/num_examples': 10000, 'score': 53815.36892461777, 'total_duration': 58746.47779226303, 'accumulated_submission_time': 53815.36892461777, 'accumulated_eval_time': 4919.428336620331, 'accumulated_logging_time': 5.142207384109497}
I0202 10:50:31.187163 139923852027648 logging_writer.py:48] [116421] accumulated_eval_time=4919.428337, accumulated_logging_time=5.142207, accumulated_submission_time=53815.368925, global_step=116421, preemption_count=0, score=53815.368925, test/accuracy=0.544300, test/loss=2.023348, test/num_examples=10000, total_duration=58746.477792, train/accuracy=0.739004, train/loss=1.067588, validation/accuracy=0.669560, validation/loss=1.372553, validation/num_examples=50000
I0202 10:51:03.774304 139923868813056 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.6969590187072754, loss=2.2843713760375977
I0202 10:51:50.468598 139923852027648 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.532819986343384, loss=2.2774505615234375
I0202 10:52:37.595285 139923868813056 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.416961669921875, loss=2.436824321746826
I0202 10:53:24.413349 139923852027648 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.541424036026001, loss=2.1111412048339844
I0202 10:54:11.476686 139923868813056 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.6401288509368896, loss=2.1982288360595703
I0202 10:54:58.716700 139923852027648 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.22959566116333, loss=4.064014434814453
I0202 10:55:45.668322 139923868813056 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.610783576965332, loss=1.995044469833374
I0202 10:56:32.492904 139923852027648 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.363163471221924, loss=2.333519458770752
I0202 10:57:19.278750 139923868813056 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.550656795501709, loss=2.0833730697631836
I0202 10:57:31.156092 140085747812160 spec.py:321] Evaluating on the training split.
I0202 10:57:42.433907 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 10:58:10.764352 140085747812160 spec.py:349] Evaluating on the test split.
I0202 10:58:12.408931 140085747812160 submission_runner.py:408] Time since start: 59207.74s, 	Step: 117327, 	{'train/accuracy': 0.7241405844688416, 'train/loss': 1.1141245365142822, 'validation/accuracy': 0.6716200113296509, 'validation/loss': 1.3542362451553345, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 1.9819422960281372, 'test/num_examples': 10000, 'score': 54235.277552843094, 'total_duration': 59207.736137628555, 'accumulated_submission_time': 54235.277552843094, 'accumulated_eval_time': 4960.681174516678, 'accumulated_logging_time': 5.188116788864136}
I0202 10:58:12.445309 139923852027648 logging_writer.py:48] [117327] accumulated_eval_time=4960.681175, accumulated_logging_time=5.188117, accumulated_submission_time=54235.277553, global_step=117327, preemption_count=0, score=54235.277553, test/accuracy=0.548100, test/loss=1.981942, test/num_examples=10000, total_duration=59207.736138, train/accuracy=0.724141, train/loss=1.114125, validation/accuracy=0.671620, validation/loss=1.354236, validation/num_examples=50000
I0202 10:58:42.159028 139923868813056 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.2015390396118164, loss=2.8526339530944824
I0202 10:59:28.462249 139923852027648 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.2908968925476074, loss=4.169345855712891
I0202 11:00:15.566595 139923868813056 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.7573227882385254, loss=2.1700258255004883
I0202 11:01:02.026614 139923852027648 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.421967029571533, loss=4.507406234741211
I0202 11:01:48.708692 139923868813056 logging_writer.py:48] [117800] global_step=117800, grad_norm=3.2188496589660645, loss=2.0654242038726807
I0202 11:02:35.892359 139923852027648 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.616361379623413, loss=2.4561071395874023
I0202 11:03:22.813637 139923868813056 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.7127199172973633, loss=1.9951550960540771
I0202 11:04:09.783846 139923852027648 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.4662134647369385, loss=4.4942946434021
I0202 11:04:56.519785 139923868813056 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.3285176753997803, loss=3.8485376834869385
I0202 11:05:12.654757 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:05:23.182333 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:05:48.085169 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:05:49.730641 140085747812160 submission_runner.py:408] Time since start: 59665.06s, 	Step: 118236, 	{'train/accuracy': 0.7311132550239563, 'train/loss': 1.0923360586166382, 'validation/accuracy': 0.674079954624176, 'validation/loss': 1.3469245433807373, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 1.9686278104782104, 'test/num_examples': 10000, 'score': 54655.425716876984, 'total_duration': 59665.05784249306, 'accumulated_submission_time': 54655.425716876984, 'accumulated_eval_time': 4997.757047891617, 'accumulated_logging_time': 5.235001087188721}
I0202 11:05:49.765246 139923852027648 logging_writer.py:48] [118236] accumulated_eval_time=4997.757048, accumulated_logging_time=5.235001, accumulated_submission_time=54655.425717, global_step=118236, preemption_count=0, score=54655.425717, test/accuracy=0.549900, test/loss=1.968628, test/num_examples=10000, total_duration=59665.057842, train/accuracy=0.731113, train/loss=1.092336, validation/accuracy=0.674080, validation/loss=1.346925, validation/num_examples=50000
I0202 11:06:15.247385 139923868813056 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.2865047454833984, loss=3.0213005542755127
I0202 11:07:01.530707 139923852027648 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.8384666442871094, loss=2.047914981842041
I0202 11:07:48.425863 139923868813056 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.48219633102417, loss=4.0462822914123535
I0202 11:08:35.247634 139923852027648 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.303572654724121, loss=2.878274917602539
I0202 11:09:22.148826 139923868813056 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.7534713745117188, loss=2.132251739501953
I0202 11:10:08.857935 139923852027648 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.312040328979492, loss=3.302699089050293
I0202 11:10:55.695129 139923868813056 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.8131372928619385, loss=2.1279494762420654
I0202 11:11:42.528667 139923852027648 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.471968650817871, loss=2.0188560485839844
I0202 11:12:29.315225 139923868813056 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.43198823928833, loss=4.661434173583984
I0202 11:12:50.147615 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:13:00.423641 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:13:30.667126 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:13:32.307271 140085747812160 submission_runner.py:408] Time since start: 60127.63s, 	Step: 119146, 	{'train/accuracy': 0.7421093583106995, 'train/loss': 1.022621989250183, 'validation/accuracy': 0.6779999732971191, 'validation/loss': 1.3137280941009521, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 1.954906702041626, 'test/num_examples': 10000, 'score': 55075.748200416565, 'total_duration': 60127.634479761124, 'accumulated_submission_time': 55075.748200416565, 'accumulated_eval_time': 5039.916709423065, 'accumulated_logging_time': 5.278695106506348}
I0202 11:13:32.346754 139923852027648 logging_writer.py:48] [119146] accumulated_eval_time=5039.916709, accumulated_logging_time=5.278695, accumulated_submission_time=55075.748200, global_step=119146, preemption_count=0, score=55075.748200, test/accuracy=0.553400, test/loss=1.954907, test/num_examples=10000, total_duration=60127.634480, train/accuracy=0.742109, train/loss=1.022622, validation/accuracy=0.678000, validation/loss=1.313728, validation/num_examples=50000
I0202 11:13:53.920488 139923868813056 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.817378282546997, loss=1.9895617961883545
I0202 11:14:39.589472 139923852027648 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.5017309188842773, loss=2.6020493507385254
I0202 11:15:26.445219 139923868813056 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.6397485733032227, loss=2.498194456100464
I0202 11:16:13.061404 139923852027648 logging_writer.py:48] [119500] global_step=119500, grad_norm=3.163349151611328, loss=4.408782005310059
I0202 11:17:00.043123 139923868813056 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.7011375427246094, loss=2.0126445293426514
I0202 11:17:46.862177 139923852027648 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.694270372390747, loss=2.035508155822754
I0202 11:18:33.646353 139923868813056 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.491299629211426, loss=1.8824691772460938
I0202 11:19:20.605223 139923852027648 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.6093668937683105, loss=1.9118564128875732
I0202 11:20:07.532995 139923868813056 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.0232133865356445, loss=2.049276351928711
I0202 11:20:32.568987 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:20:42.988183 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:21:09.220562 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:21:10.867187 140085747812160 submission_runner.py:408] Time since start: 60586.19s, 	Step: 120055, 	{'train/accuracy': 0.7331249713897705, 'train/loss': 1.0569225549697876, 'validation/accuracy': 0.6812399625778198, 'validation/loss': 1.2919137477874756, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 1.9266986846923828, 'test/num_examples': 10000, 'score': 55495.910600185394, 'total_duration': 60586.19439768791, 'accumulated_submission_time': 55495.910600185394, 'accumulated_eval_time': 5078.214918136597, 'accumulated_logging_time': 5.3276238441467285}
I0202 11:21:10.906596 139923852027648 logging_writer.py:48] [120055] accumulated_eval_time=5078.214918, accumulated_logging_time=5.327624, accumulated_submission_time=55495.910600, global_step=120055, preemption_count=0, score=55495.910600, test/accuracy=0.556900, test/loss=1.926699, test/num_examples=10000, total_duration=60586.194398, train/accuracy=0.733125, train/loss=1.056923, validation/accuracy=0.681240, validation/loss=1.291914, validation/num_examples=50000
I0202 11:21:28.968195 139923868813056 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.5703041553497314, loss=2.3213343620300293
I0202 11:22:14.027283 139923852027648 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.4826812744140625, loss=1.929451823234558
I0202 11:23:01.484897 139923868813056 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.5014188289642334, loss=1.9480085372924805
I0202 11:23:48.610034 139923852027648 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.7863309383392334, loss=2.3493683338165283
I0202 11:24:35.370424 139923868813056 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.8549137115478516, loss=2.1833128929138184
I0202 11:25:22.187983 139923852027648 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.3114023208618164, loss=2.7945618629455566
I0202 11:26:08.895767 139923868813056 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.3402817249298096, loss=3.468780517578125
I0202 11:26:55.736827 139923852027648 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.810351610183716, loss=2.266101837158203
I0202 11:27:42.606011 139923868813056 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.422910451889038, loss=2.663729190826416
I0202 11:28:11.283993 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:28:21.666116 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:28:50.718478 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:28:52.359484 140085747812160 submission_runner.py:408] Time since start: 61047.69s, 	Step: 120963, 	{'train/accuracy': 0.7388867139816284, 'train/loss': 1.049467921257019, 'validation/accuracy': 0.6840800046920776, 'validation/loss': 1.2940298318862915, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 1.9345511198043823, 'test/num_examples': 10000, 'score': 55916.225331544876, 'total_duration': 61047.686688899994, 'accumulated_submission_time': 55916.225331544876, 'accumulated_eval_time': 5119.290406227112, 'accumulated_logging_time': 5.378458261489868}
I0202 11:28:52.394835 139923852027648 logging_writer.py:48] [120963] accumulated_eval_time=5119.290406, accumulated_logging_time=5.378458, accumulated_submission_time=55916.225332, global_step=120963, preemption_count=0, score=55916.225332, test/accuracy=0.556100, test/loss=1.934551, test/num_examples=10000, total_duration=61047.686689, train/accuracy=0.738887, train/loss=1.049468, validation/accuracy=0.684080, validation/loss=1.294030, validation/num_examples=50000
I0202 11:29:07.304376 139923868813056 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.7481443881988525, loss=4.482360363006592
I0202 11:29:51.967353 139923852027648 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.544459342956543, loss=3.252274513244629
I0202 11:30:38.953940 139923868813056 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.4540140628814697, loss=4.376559257507324
I0202 11:31:25.964249 139923852027648 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.90873122215271, loss=4.686751365661621
I0202 11:32:12.964287 139923868813056 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.0962576866149902, loss=4.378363132476807
I0202 11:32:59.960068 139923852027648 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.3036136627197266, loss=3.1776063442230225
I0202 11:33:46.881897 139923868813056 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.3023908138275146, loss=4.55987548828125
I0202 11:34:33.723876 139923852027648 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.693913698196411, loss=1.9511743783950806
I0202 11:35:20.755690 139923868813056 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.2971136569976807, loss=3.8109545707702637
I0202 11:35:52.770262 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:36:03.380211 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:36:28.708522 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:36:30.357453 140085747812160 submission_runner.py:408] Time since start: 61505.68s, 	Step: 121870, 	{'train/accuracy': 0.7439062595367432, 'train/loss': 1.030897855758667, 'validation/accuracy': 0.6851599812507629, 'validation/loss': 1.2949132919311523, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 1.9299076795578003, 'test/num_examples': 10000, 'score': 56336.53861951828, 'total_duration': 61505.68466210365, 'accumulated_submission_time': 56336.53861951828, 'accumulated_eval_time': 5156.877597808838, 'accumulated_logging_time': 5.425642251968384}
I0202 11:36:30.393603 139923852027648 logging_writer.py:48] [121870] accumulated_eval_time=5156.877598, accumulated_logging_time=5.425642, accumulated_submission_time=56336.538620, global_step=121870, preemption_count=0, score=56336.538620, test/accuracy=0.559200, test/loss=1.929908, test/num_examples=10000, total_duration=61505.684662, train/accuracy=0.743906, train/loss=1.030898, validation/accuracy=0.685160, validation/loss=1.294913, validation/num_examples=50000
I0202 11:36:42.570981 139923868813056 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.95402193069458, loss=1.8372665643692017
I0202 11:37:26.474520 139923852027648 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.6325883865356445, loss=2.326077461242676
I0202 11:38:13.432550 139923868813056 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.347520112991333, loss=3.331843852996826
I0202 11:39:00.343119 139923852027648 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.811554193496704, loss=1.9912463426589966
I0202 11:39:47.091924 139923868813056 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.5583155155181885, loss=4.556916236877441
I0202 11:40:33.932904 139923852027648 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.53623104095459, loss=2.203354835510254
I0202 11:41:20.686586 139923868813056 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.3296141624450684, loss=3.1382393836975098
I0202 11:42:07.522901 139923852027648 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.253787040710449, loss=2.024566411972046
I0202 11:42:54.504152 139923868813056 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.637460231781006, loss=3.0342118740081787
I0202 11:43:30.612995 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:43:41.283471 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:44:07.767964 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:44:09.406317 140085747812160 submission_runner.py:408] Time since start: 61964.73s, 	Step: 122779, 	{'train/accuracy': 0.7379491925239563, 'train/loss': 1.0452035665512085, 'validation/accuracy': 0.685979962348938, 'validation/loss': 1.2754040956497192, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.8941129446029663, 'test/num_examples': 10000, 'score': 56756.696388721466, 'total_duration': 61964.73351264, 'accumulated_submission_time': 56756.696388721466, 'accumulated_eval_time': 5195.670909404755, 'accumulated_logging_time': 5.473036527633667}
I0202 11:44:09.443514 139923852027648 logging_writer.py:48] [122779] accumulated_eval_time=5195.670909, accumulated_logging_time=5.473037, accumulated_submission_time=56756.696389, global_step=122779, preemption_count=0, score=56756.696389, test/accuracy=0.561100, test/loss=1.894113, test/num_examples=10000, total_duration=61964.733513, train/accuracy=0.737949, train/loss=1.045204, validation/accuracy=0.685980, validation/loss=1.275404, validation/num_examples=50000
I0202 11:44:18.079532 139923868813056 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.5831222534179688, loss=3.682307720184326
I0202 11:45:01.323424 139923852027648 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.694488048553467, loss=2.1232752799987793
I0202 11:45:48.234363 139923868813056 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.7669646739959717, loss=1.9401772022247314
I0202 11:46:35.120969 139923852027648 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.6951069831848145, loss=4.063617706298828
I0202 11:47:21.771554 139923868813056 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.8735971450805664, loss=1.8933649063110352
I0202 11:48:08.684129 139923852027648 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.439154863357544, loss=2.663398265838623
I0202 11:48:55.191222 139923868813056 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.1835615634918213, loss=1.8597866296768188
I0202 11:49:41.832314 139923852027648 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.654531717300415, loss=1.865393877029419
I0202 11:50:28.651058 139923868813056 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.4383718967437744, loss=3.7326812744140625
I0202 11:51:09.488021 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:51:19.793383 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:51:51.351234 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:51:52.984449 140085747812160 submission_runner.py:408] Time since start: 62428.31s, 	Step: 123689, 	{'train/accuracy': 0.7442382574081421, 'train/loss': 1.0326616764068604, 'validation/accuracy': 0.6847800016403198, 'validation/loss': 1.2847895622253418, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 1.901072382926941, 'test/num_examples': 10000, 'score': 57176.67815852165, 'total_duration': 62428.31165552139, 'accumulated_submission_time': 57176.67815852165, 'accumulated_eval_time': 5239.167343139648, 'accumulated_logging_time': 5.522738933563232}
I0202 11:51:53.022333 139923852027648 logging_writer.py:48] [123689] accumulated_eval_time=5239.167343, accumulated_logging_time=5.522739, accumulated_submission_time=57176.678159, global_step=123689, preemption_count=0, score=57176.678159, test/accuracy=0.558600, test/loss=1.901072, test/num_examples=10000, total_duration=62428.311656, train/accuracy=0.744238, train/loss=1.032662, validation/accuracy=0.684780, validation/loss=1.284790, validation/num_examples=50000
I0202 11:51:57.728386 139923868813056 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.6904332637786865, loss=2.051206588745117
I0202 11:52:40.568535 139923852027648 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.556649684906006, loss=1.9890459775924683
I0202 11:53:27.504838 139923868813056 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.7852747440338135, loss=2.066225290298462
I0202 11:54:14.341649 139923852027648 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.400383710861206, loss=3.0861847400665283
I0202 11:55:01.075255 139923868813056 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.4187610149383545, loss=3.1182446479797363
I0202 11:55:47.962341 139923852027648 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.8812901973724365, loss=2.220741033554077
I0202 11:56:34.646025 139923868813056 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.9771595001220703, loss=2.00479793548584
I0202 11:57:21.375460 139923852027648 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.778512477874756, loss=1.866162657737732
I0202 11:58:08.180685 139923868813056 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.731604814529419, loss=3.5711734294891357
I0202 11:58:53.189713 140085747812160 spec.py:321] Evaluating on the training split.
I0202 11:59:03.906802 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 11:59:29.107790 140085747812160 spec.py:349] Evaluating on the test split.
I0202 11:59:30.746859 140085747812160 submission_runner.py:408] Time since start: 62886.07s, 	Step: 124598, 	{'train/accuracy': 0.7477929592132568, 'train/loss': 1.0206478834152222, 'validation/accuracy': 0.6859599947929382, 'validation/loss': 1.2963595390319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9213535785675049, 'test/num_examples': 10000, 'score': 57596.78480887413, 'total_duration': 62886.074068784714, 'accumulated_submission_time': 57596.78480887413, 'accumulated_eval_time': 5276.72448348999, 'accumulated_logging_time': 5.570141315460205}
I0202 11:59:30.785954 139923852027648 logging_writer.py:48] [124598] accumulated_eval_time=5276.724483, accumulated_logging_time=5.570141, accumulated_submission_time=57596.784809, global_step=124598, preemption_count=0, score=57596.784809, test/accuracy=0.565200, test/loss=1.921354, test/num_examples=10000, total_duration=62886.074069, train/accuracy=0.747793, train/loss=1.020648, validation/accuracy=0.685960, validation/loss=1.296360, validation/num_examples=50000
I0202 11:59:31.973373 139923868813056 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.853961944580078, loss=2.484043598175049
I0202 12:00:13.626965 139923852027648 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.1526997089385986, loss=4.463107109069824
I0202 12:01:00.315009 139923868813056 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.985787868499756, loss=2.0512046813964844
I0202 12:01:47.169821 139923852027648 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.6183035373687744, loss=2.0457072257995605
I0202 12:02:34.079087 139923868813056 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.333022356033325, loss=3.705986499786377
I0202 12:03:20.960286 139923852027648 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.6700549125671387, loss=2.801844596862793
I0202 12:04:07.657813 139923868813056 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.937849283218384, loss=1.9249920845031738
I0202 12:04:54.364946 139923852027648 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.585923433303833, loss=1.8918887376785278
I0202 12:05:41.043599 139923868813056 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.237790822982788, loss=3.6443471908569336
I0202 12:06:27.832984 139923852027648 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.8720178604125977, loss=1.9332376718521118
I0202 12:06:30.840886 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:06:41.330915 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:07:08.239862 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:07:09.874430 140085747812160 submission_runner.py:408] Time since start: 63345.20s, 	Step: 125508, 	{'train/accuracy': 0.7463671565055847, 'train/loss': 1.0201128721237183, 'validation/accuracy': 0.6872999668121338, 'validation/loss': 1.2751214504241943, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 1.9046622514724731, 'test/num_examples': 10000, 'score': 58016.77628803253, 'total_duration': 63345.201642513275, 'accumulated_submission_time': 58016.77628803253, 'accumulated_eval_time': 5315.758058547974, 'accumulated_logging_time': 5.622182130813599}
I0202 12:07:09.911402 139923868813056 logging_writer.py:48] [125508] accumulated_eval_time=5315.758059, accumulated_logging_time=5.622182, accumulated_submission_time=58016.776288, global_step=125508, preemption_count=0, score=58016.776288, test/accuracy=0.560900, test/loss=1.904662, test/num_examples=10000, total_duration=63345.201643, train/accuracy=0.746367, train/loss=1.020113, validation/accuracy=0.687300, validation/loss=1.275121, validation/num_examples=50000
I0202 12:07:48.565849 139923852027648 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.032304286956787, loss=2.0195367336273193
I0202 12:08:35.309626 139923868813056 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.9360342025756836, loss=1.9861412048339844
I0202 12:09:22.331020 139923852027648 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.9736456871032715, loss=2.2699129581451416
I0202 12:10:08.961252 139923868813056 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.8474531173706055, loss=1.9297504425048828
I0202 12:10:56.005809 139923852027648 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.0867207050323486, loss=1.846675992012024
I0202 12:11:42.845625 139923868813056 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.580390691757202, loss=2.8708295822143555
I0202 12:12:29.740761 139923852027648 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.6305689811706543, loss=4.027163028717041
I0202 12:13:16.770248 139923868813056 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.900198221206665, loss=1.9368304014205933
I0202 12:14:03.963610 139923852027648 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.0467190742492676, loss=4.101402759552002
I0202 12:14:10.238990 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:14:20.540482 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:14:46.579794 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:14:48.240902 140085747812160 submission_runner.py:408] Time since start: 63803.57s, 	Step: 126415, 	{'train/accuracy': 0.7504687309265137, 'train/loss': 0.9982839822769165, 'validation/accuracy': 0.6909799575805664, 'validation/loss': 1.2527966499328613, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.8792650699615479, 'test/num_examples': 10000, 'score': 58437.0435795784, 'total_duration': 63803.568054914474, 'accumulated_submission_time': 58437.0435795784, 'accumulated_eval_time': 5353.759917020798, 'accumulated_logging_time': 5.668635845184326}
I0202 12:14:48.281322 139923868813056 logging_writer.py:48] [126415] accumulated_eval_time=5353.759917, accumulated_logging_time=5.668636, accumulated_submission_time=58437.043580, global_step=126415, preemption_count=0, score=58437.043580, test/accuracy=0.567800, test/loss=1.879265, test/num_examples=10000, total_duration=63803.568055, train/accuracy=0.750469, train/loss=0.998284, validation/accuracy=0.690980, validation/loss=1.252797, validation/num_examples=50000
I0202 12:15:23.558321 139923852027648 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.0490002632141113, loss=2.1650683879852295
I0202 12:16:10.489738 139923868813056 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.1001994609832764, loss=2.0219614505767822
I0202 12:16:57.411655 139923852027648 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.786210775375366, loss=4.556546688079834
I0202 12:17:43.988393 139923868813056 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.857234477996826, loss=2.13234806060791
I0202 12:18:30.829689 139923852027648 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.870737075805664, loss=1.8410181999206543
I0202 12:19:17.544197 139923868813056 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.860882520675659, loss=4.36795711517334
I0202 12:20:04.387975 139923852027648 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.241544008255005, loss=2.0545506477355957
I0202 12:20:51.089379 139923868813056 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.0228116512298584, loss=2.0812408924102783
I0202 12:21:38.009379 139923852027648 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.0719101428985596, loss=1.9233806133270264
I0202 12:21:48.421925 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:21:58.722279 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:22:24.817419 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:22:26.460538 140085747812160 submission_runner.py:408] Time since start: 64261.79s, 	Step: 127324, 	{'train/accuracy': 0.7506445050239563, 'train/loss': 0.9825835824012756, 'validation/accuracy': 0.6895599961280823, 'validation/loss': 1.252158761024475, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.8777422904968262, 'test/num_examples': 10000, 'score': 58857.124438762665, 'total_duration': 64261.78774547577, 'accumulated_submission_time': 58857.124438762665, 'accumulated_eval_time': 5391.798512220383, 'accumulated_logging_time': 5.717786550521851}
I0202 12:22:26.498044 139923868813056 logging_writer.py:48] [127324] accumulated_eval_time=5391.798512, accumulated_logging_time=5.717787, accumulated_submission_time=58857.124439, global_step=127324, preemption_count=0, score=58857.124439, test/accuracy=0.567300, test/loss=1.877742, test/num_examples=10000, total_duration=64261.787745, train/accuracy=0.750645, train/loss=0.982584, validation/accuracy=0.689560, validation/loss=1.252159, validation/num_examples=50000
I0202 12:22:57.272223 139923852027648 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.904001235961914, loss=1.863769769668579
I0202 12:23:43.959084 139923868813056 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.901383399963379, loss=4.523955345153809
I0202 12:24:31.063162 139923852027648 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.8514108657836914, loss=1.940502643585205
I0202 12:25:17.730596 139923868813056 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.623866319656372, loss=3.698209285736084
I0202 12:26:04.242727 139923852027648 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.5439157485961914, loss=4.024603843688965
I0202 12:26:50.915828 139923868813056 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.682793140411377, loss=3.737135171890259
I0202 12:27:37.437132 139923852027648 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.2378077507019043, loss=1.9579507112503052
I0202 12:28:24.087285 139923868813056 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.8830692768096924, loss=3.610137462615967
I0202 12:29:10.729243 139923852027648 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.7496321201324463, loss=3.9511618614196777
I0202 12:29:26.780854 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:29:37.164011 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:30:02.849163 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:30:04.493584 140085747812160 submission_runner.py:408] Time since start: 64719.82s, 	Step: 128236, 	{'train/accuracy': 0.7712695002555847, 'train/loss': 0.9164721369743347, 'validation/accuracy': 0.6928399801254272, 'validation/loss': 1.2563698291778564, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 1.882158637046814, 'test/num_examples': 10000, 'score': 59277.34511780739, 'total_duration': 64719.820784807205, 'accumulated_submission_time': 59277.34511780739, 'accumulated_eval_time': 5429.511273860931, 'accumulated_logging_time': 5.765959978103638}
I0202 12:30:04.531847 139923868813056 logging_writer.py:48] [128236] accumulated_eval_time=5429.511274, accumulated_logging_time=5.765960, accumulated_submission_time=59277.345118, global_step=128236, preemption_count=0, score=59277.345118, test/accuracy=0.566500, test/loss=1.882159, test/num_examples=10000, total_duration=64719.820785, train/accuracy=0.771270, train/loss=0.916472, validation/accuracy=0.692840, validation/loss=1.256370, validation/num_examples=50000
I0202 12:30:30.017238 139923852027648 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.9041988849639893, loss=3.9381728172302246
I0202 12:31:16.528767 139923868813056 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.0148470401763916, loss=2.0485641956329346
I0202 12:32:03.706868 139923852027648 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.0175631046295166, loss=2.1772937774658203
I0202 12:32:50.936315 139923868813056 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.0262534618377686, loss=1.9709230661392212
I0202 12:33:38.151361 139923852027648 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.804553508758545, loss=1.9631037712097168
I0202 12:34:24.965974 139923868813056 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.4595580101013184, loss=1.9736249446868896
I0202 12:35:11.849137 139923852027648 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.5912270545959473, loss=3.296743392944336
I0202 12:35:58.662209 139923868813056 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.870105504989624, loss=2.3832197189331055
I0202 12:36:45.499713 139923852027648 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.1050589084625244, loss=1.8778386116027832
I0202 12:37:04.957341 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:37:15.394991 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:37:44.937111 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:37:46.589729 140085747812160 submission_runner.py:408] Time since start: 65181.92s, 	Step: 129143, 	{'train/accuracy': 0.753222644329071, 'train/loss': 1.0042492151260376, 'validation/accuracy': 0.6917200088500977, 'validation/loss': 1.2587978839874268, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 1.8835062980651855, 'test/num_examples': 10000, 'score': 59697.71044564247, 'total_duration': 65181.916934490204, 'accumulated_submission_time': 59697.71044564247, 'accumulated_eval_time': 5471.143674373627, 'accumulated_logging_time': 5.814197540283203}
I0202 12:37:46.626716 139923868813056 logging_writer.py:48] [129143] accumulated_eval_time=5471.143674, accumulated_logging_time=5.814198, accumulated_submission_time=59697.710446, global_step=129143, preemption_count=0, score=59697.710446, test/accuracy=0.564000, test/loss=1.883506, test/num_examples=10000, total_duration=65181.916934, train/accuracy=0.753223, train/loss=1.004249, validation/accuracy=0.691720, validation/loss=1.258798, validation/num_examples=50000
I0202 12:38:09.375561 139923852027648 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.5697174072265625, loss=3.1246185302734375
I0202 12:38:55.179641 139923868813056 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.9472453594207764, loss=1.874074935913086
I0202 12:39:42.295155 139923852027648 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.9410717487335205, loss=4.011188507080078
I0202 12:40:29.023893 139923868813056 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.6991870403289795, loss=3.7312262058258057
I0202 12:41:16.094521 139923852027648 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.8516831398010254, loss=4.367955684661865
I0202 12:42:02.731559 139923868813056 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.8787641525268555, loss=1.8562755584716797
I0202 12:42:49.605522 139923852027648 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.042722225189209, loss=4.507076740264893
I0202 12:43:36.480875 139923868813056 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.1718010902404785, loss=1.9533387422561646
I0202 12:44:23.230944 139923852027648 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.8072218894958496, loss=2.3283042907714844
I0202 12:44:46.813179 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:44:57.225268 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:45:20.259832 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:45:21.898894 140085747812160 submission_runner.py:408] Time since start: 65637.23s, 	Step: 130052, 	{'train/accuracy': 0.7607616782188416, 'train/loss': 0.9540216326713562, 'validation/accuracy': 0.6956999897956848, 'validation/loss': 1.2329747676849365, 'validation/num_examples': 50000, 'test/accuracy': 0.5714000463485718, 'test/loss': 1.8770899772644043, 'test/num_examples': 10000, 'score': 60117.83626127243, 'total_duration': 65637.22610259056, 'accumulated_submission_time': 60117.83626127243, 'accumulated_eval_time': 5506.229387283325, 'accumulated_logging_time': 5.861069679260254}
I0202 12:45:21.937263 139923868813056 logging_writer.py:48] [130052] accumulated_eval_time=5506.229387, accumulated_logging_time=5.861070, accumulated_submission_time=60117.836261, global_step=130052, preemption_count=0, score=60117.836261, test/accuracy=0.571400, test/loss=1.877090, test/num_examples=10000, total_duration=65637.226103, train/accuracy=0.760762, train/loss=0.954022, validation/accuracy=0.695700, validation/loss=1.232975, validation/num_examples=50000
I0202 12:45:41.184890 139923852027648 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.2441039085388184, loss=1.920586347579956
I0202 12:46:26.368563 139923868813056 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.85071063041687, loss=1.8306223154067993
I0202 12:47:13.040033 139923852027648 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.504823923110962, loss=2.910186529159546
I0202 12:47:59.996763 139923868813056 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.727588176727295, loss=2.54840087890625
I0202 12:48:46.796609 139923852027648 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.2510833740234375, loss=1.9945522546768188
I0202 12:49:33.551577 139923868813056 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.8664588928222656, loss=1.8238792419433594
I0202 12:50:20.090822 139923852027648 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.9970874786376953, loss=2.2334017753601074
I0202 12:51:06.963636 139923868813056 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.1917331218719482, loss=4.442471504211426
I0202 12:51:53.799217 139923852027648 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.9435222148895264, loss=3.5376243591308594
I0202 12:52:21.983136 140085747812160 spec.py:321] Evaluating on the training split.
I0202 12:52:32.537930 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 12:53:01.220694 140085747812160 spec.py:349] Evaluating on the test split.
I0202 12:53:02.864612 140085747812160 submission_runner.py:408] Time since start: 66098.19s, 	Step: 130962, 	{'train/accuracy': 0.7698827981948853, 'train/loss': 0.9147235155105591, 'validation/accuracy': 0.6962400078773499, 'validation/loss': 1.2309191226959229, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.8582838773727417, 'test/num_examples': 10000, 'score': 60537.820308446884, 'total_duration': 66098.19182014465, 'accumulated_submission_time': 60537.820308446884, 'accumulated_eval_time': 5547.110866785049, 'accumulated_logging_time': 5.910343647003174}
I0202 12:53:02.905113 139923868813056 logging_writer.py:48] [130962] accumulated_eval_time=5547.110867, accumulated_logging_time=5.910344, accumulated_submission_time=60537.820308, global_step=130962, preemption_count=0, score=60537.820308, test/accuracy=0.574200, test/loss=1.858284, test/num_examples=10000, total_duration=66098.191820, train/accuracy=0.769883, train/loss=0.914724, validation/accuracy=0.696240, validation/loss=1.230919, validation/num_examples=50000
I0202 12:53:18.207113 139923852027648 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.1626195907592773, loss=2.274453639984131
I0202 12:54:02.729454 139923868813056 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.8965110778808594, loss=2.072275400161743
I0202 12:54:49.663342 139923852027648 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.724503755569458, loss=3.7530510425567627
I0202 12:55:36.264993 139923868813056 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.166628837585449, loss=1.7736256122589111
I0202 12:56:23.364387 139923852027648 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.9449570178985596, loss=2.5808305740356445
I0202 12:57:10.258388 139923868813056 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.926046848297119, loss=1.825716495513916
I0202 12:57:57.079126 139923852027648 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.192272186279297, loss=1.996192455291748
I0202 12:58:44.061148 139923868813056 logging_writer.py:48] [131700] global_step=131700, grad_norm=2.932168960571289, loss=1.9500360488891602
I0202 12:59:30.869164 139923852027648 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.2225747108459473, loss=2.0742406845092773
I0202 13:00:03.119322 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:00:13.519230 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:00:43.359096 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:00:45.013914 140085747812160 submission_runner.py:408] Time since start: 66560.34s, 	Step: 131870, 	{'train/accuracy': 0.7575585842132568, 'train/loss': 0.9641546607017517, 'validation/accuracy': 0.7005599737167358, 'validation/loss': 1.2190051078796387, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.8357348442077637, 'test/num_examples': 10000, 'score': 60957.97349905968, 'total_duration': 66560.34111618996, 'accumulated_submission_time': 60957.97349905968, 'accumulated_eval_time': 5589.005449295044, 'accumulated_logging_time': 5.961735725402832}
I0202 13:00:45.052978 139923868813056 logging_writer.py:48] [131870] accumulated_eval_time=5589.005449, accumulated_logging_time=5.961736, accumulated_submission_time=60957.973499, global_step=131870, preemption_count=0, score=60957.973499, test/accuracy=0.572500, test/loss=1.835735, test/num_examples=10000, total_duration=66560.341116, train/accuracy=0.757559, train/loss=0.964155, validation/accuracy=0.700560, validation/loss=1.219005, validation/num_examples=50000
I0202 13:00:57.211440 139923852027648 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.3508400917053223, loss=2.10306715965271
I0202 13:01:41.173648 139923868813056 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.1949894428253174, loss=1.9700695276260376
I0202 13:02:28.279057 139923852027648 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3141679763793945, loss=4.063032627105713
I0202 13:03:15.262833 139923868813056 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.1506457328796387, loss=1.9295810461044312
I0202 13:04:02.016901 139923852027648 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.7545955181121826, loss=4.029374122619629
I0202 13:04:48.714877 139923868813056 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.105888843536377, loss=1.6977057456970215
I0202 13:05:35.790979 139923852027648 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.0246315002441406, loss=3.975520133972168
I0202 13:06:22.619090 139923868813056 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.249812364578247, loss=1.8617234230041504
I0202 13:07:09.610506 139923852027648 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.9298040866851807, loss=1.968996524810791
I0202 13:07:45.376588 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:07:55.574224 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:08:23.091734 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:08:24.732819 140085747812160 submission_runner.py:408] Time since start: 67020.06s, 	Step: 132778, 	{'train/accuracy': 0.7624413967132568, 'train/loss': 0.9442371129989624, 'validation/accuracy': 0.6990199685096741, 'validation/loss': 1.2210444211959839, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.856716513633728, 'test/num_examples': 10000, 'score': 61378.235782146454, 'total_duration': 67020.0600142479, 'accumulated_submission_time': 61378.235782146454, 'accumulated_eval_time': 5628.361680984497, 'accumulated_logging_time': 6.011536359786987}
I0202 13:08:24.778294 139923868813056 logging_writer.py:48] [132778] accumulated_eval_time=5628.361681, accumulated_logging_time=6.011536, accumulated_submission_time=61378.235782, global_step=132778, preemption_count=0, score=61378.235782, test/accuracy=0.577300, test/loss=1.856717, test/num_examples=10000, total_duration=67020.060014, train/accuracy=0.762441, train/loss=0.944237, validation/accuracy=0.699020, validation/loss=1.221044, validation/num_examples=50000
I0202 13:08:33.815274 139923852027648 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.418508529663086, loss=1.9496943950653076
I0202 13:09:17.288512 139923868813056 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.014430522918701, loss=1.91346275806427
I0202 13:10:04.222983 139923852027648 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.913412570953369, loss=2.964444637298584
I0202 13:10:51.027109 139923868813056 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.0924031734466553, loss=2.0316450595855713
I0202 13:11:37.884839 139923852027648 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.9990124702453613, loss=4.364404201507568
I0202 13:12:24.578623 139923868813056 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.124915361404419, loss=1.7810466289520264
I0202 13:13:11.453748 139923852027648 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.9567782878875732, loss=3.6833109855651855
I0202 13:13:58.104487 139923868813056 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.264263868331909, loss=1.9700815677642822
I0202 13:14:44.606957 139923852027648 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.3398725986480713, loss=1.872837781906128
I0202 13:15:25.007033 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:15:35.028312 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:16:00.543892 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:16:02.185516 140085747812160 submission_runner.py:408] Time since start: 67477.51s, 	Step: 133688, 	{'train/accuracy': 0.7743359208106995, 'train/loss': 0.8995473384857178, 'validation/accuracy': 0.7016800045967102, 'validation/loss': 1.211005687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.8353841304779053, 'test/num_examples': 10000, 'score': 61798.403445243835, 'total_duration': 67477.51272082329, 'accumulated_submission_time': 61798.403445243835, 'accumulated_eval_time': 5665.5401656627655, 'accumulated_logging_time': 6.066869735717773}
I0202 13:16:02.223248 139923868813056 logging_writer.py:48] [133688] accumulated_eval_time=5665.540166, accumulated_logging_time=6.066870, accumulated_submission_time=61798.403445, global_step=133688, preemption_count=0, score=61798.403445, test/accuracy=0.577500, test/loss=1.835384, test/num_examples=10000, total_duration=67477.512721, train/accuracy=0.774336, train/loss=0.899547, validation/accuracy=0.701680, validation/loss=1.211006, validation/num_examples=50000
I0202 13:16:07.326833 139923852027648 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.80570650100708, loss=3.7389605045318604
I0202 13:16:49.877833 139923868813056 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.1270580291748047, loss=1.797508955001831
I0202 13:17:36.733472 139923852027648 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.006707191467285, loss=1.8721466064453125
I0202 13:18:23.525732 139923868813056 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.129518508911133, loss=1.9869880676269531
I0202 13:19:10.239257 139923852027648 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.1845462322235107, loss=1.9564532041549683
I0202 13:19:56.847775 139923868813056 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.1916427612304688, loss=1.80522620677948
I0202 13:20:43.571909 139923852027648 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.530362606048584, loss=1.9713160991668701
I0202 13:21:30.144932 139923868813056 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.3313517570495605, loss=2.2889976501464844
I0202 13:22:16.788655 139923852027648 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.133430004119873, loss=1.8774763345718384
I0202 13:23:02.341432 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:23:12.643003 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:23:42.988174 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:23:44.622422 140085747812160 submission_runner.py:408] Time since start: 67939.95s, 	Step: 134599, 	{'train/accuracy': 0.76185542345047, 'train/loss': 0.9467484951019287, 'validation/accuracy': 0.7027999758720398, 'validation/loss': 1.2054047584533691, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.8349767923355103, 'test/num_examples': 10000, 'score': 62218.46045303345, 'total_duration': 67939.94962263107, 'accumulated_submission_time': 62218.46045303345, 'accumulated_eval_time': 5707.8211581707, 'accumulated_logging_time': 6.115529537200928}
I0202 13:23:44.660387 139923868813056 logging_writer.py:48] [134599] accumulated_eval_time=5707.821158, accumulated_logging_time=6.115530, accumulated_submission_time=62218.460453, global_step=134599, preemption_count=0, score=62218.460453, test/accuracy=0.578900, test/loss=1.834977, test/num_examples=10000, total_duration=67939.949623, train/accuracy=0.761855, train/loss=0.946748, validation/accuracy=0.702800, validation/loss=1.205405, validation/num_examples=50000
I0202 13:23:45.453194 139923852027648 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.3213257789611816, loss=4.33302116394043
I0202 13:24:27.378391 139923868813056 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.3129827976226807, loss=2.084796190261841
I0202 13:25:14.059224 139923852027648 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.9213085174560547, loss=2.8757224082946777
I0202 13:26:00.948309 139923868813056 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.356538772583008, loss=1.8050084114074707
I0202 13:26:47.701087 139923852027648 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.041991949081421, loss=2.1213338375091553
I0202 13:27:34.578443 139923868813056 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.0625321865081787, loss=3.986463785171509
I0202 13:28:21.739636 139923852027648 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.8610830307006836, loss=3.015014171600342
I0202 13:29:08.399882 139923868813056 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.465545892715454, loss=4.226127624511719
I0202 13:29:55.269273 139923852027648 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.9647083282470703, loss=2.3389601707458496
I0202 13:30:42.150811 139923868813056 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.266134262084961, loss=1.7839101552963257
I0202 13:30:44.672074 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:30:55.356072 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:31:23.254028 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:31:24.893719 140085747812160 submission_runner.py:408] Time since start: 68400.22s, 	Step: 135507, 	{'train/accuracy': 0.7659569978713989, 'train/loss': 0.9456735253334045, 'validation/accuracy': 0.7011799812316895, 'validation/loss': 1.2311320304870605, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.8514093160629272, 'test/num_examples': 10000, 'score': 62638.41057395935, 'total_duration': 68400.22089409828, 'accumulated_submission_time': 62638.41057395935, 'accumulated_eval_time': 5748.042767763138, 'accumulated_logging_time': 6.164484024047852}
I0202 13:31:24.936192 139923852027648 logging_writer.py:48] [135507] accumulated_eval_time=5748.042768, accumulated_logging_time=6.164484, accumulated_submission_time=62638.410574, global_step=135507, preemption_count=0, score=62638.410574, test/accuracy=0.577600, test/loss=1.851409, test/num_examples=10000, total_duration=68400.220894, train/accuracy=0.765957, train/loss=0.945674, validation/accuracy=0.701180, validation/loss=1.231132, validation/num_examples=50000
I0202 13:32:03.824668 139923868813056 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.18350887298584, loss=1.7539007663726807
I0202 13:32:50.943337 139923852027648 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3708746433258057, loss=1.8491055965423584
I0202 13:33:38.211738 139923868813056 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.922574281692505, loss=3.305975914001465
I0202 13:34:24.817100 139923852027648 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.0335144996643066, loss=2.3996036052703857
I0202 13:35:11.689332 139923868813056 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.9961066246032715, loss=3.4396395683288574
I0202 13:35:58.480878 139923852027648 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.1987688541412354, loss=2.351564645767212
I0202 13:36:45.491185 139923868813056 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.313814163208008, loss=1.832712173461914
I0202 13:37:32.397080 139923852027648 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.4751083850860596, loss=1.9123848676681519
I0202 13:38:19.328108 139923868813056 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.4221973419189453, loss=1.8961334228515625
I0202 13:38:25.137074 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:38:35.541503 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:38:59.708251 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:39:01.351403 140085747812160 submission_runner.py:408] Time since start: 68856.68s, 	Step: 136414, 	{'train/accuracy': 0.7757421731948853, 'train/loss': 0.9015170931816101, 'validation/accuracy': 0.704539954662323, 'validation/loss': 1.1993683576583862, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.8206918239593506, 'test/num_examples': 10000, 'score': 63058.551265239716, 'total_duration': 68856.67860889435, 'accumulated_submission_time': 63058.551265239716, 'accumulated_eval_time': 5784.257081747055, 'accumulated_logging_time': 6.215606927871704}
I0202 13:39:01.390716 139923852027648 logging_writer.py:48] [136414] accumulated_eval_time=5784.257082, accumulated_logging_time=6.215607, accumulated_submission_time=63058.551265, global_step=136414, preemption_count=0, score=63058.551265, test/accuracy=0.582400, test/loss=1.820692, test/num_examples=10000, total_duration=68856.678609, train/accuracy=0.775742, train/loss=0.901517, validation/accuracy=0.704540, validation/loss=1.199368, validation/num_examples=50000
I0202 13:39:37.076131 139923868813056 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.650055170059204, loss=1.8249305486679077
I0202 13:40:23.683944 139923852027648 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.0634078979492188, loss=3.6891822814941406
I0202 13:41:10.955456 139923868813056 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.3570313453674316, loss=1.7456867694854736
I0202 13:41:57.664022 139923852027648 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.2993900775909424, loss=2.022580146789551
I0202 13:42:44.809037 139923868813056 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.468740224838257, loss=2.2005205154418945
I0202 13:43:31.646352 139923852027648 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.24849534034729, loss=1.822447657585144
I0202 13:44:18.649466 139923868813056 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.882444381713867, loss=3.255743980407715
I0202 13:45:05.136559 139923852027648 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.294548988342285, loss=1.7769044637680054
I0202 13:45:51.876260 139923868813056 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.124366283416748, loss=2.982806444168091
I0202 13:46:01.427078 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:46:12.022306 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:46:40.394722 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:46:42.042411 140085747812160 submission_runner.py:408] Time since start: 69317.37s, 	Step: 137322, 	{'train/accuracy': 0.7714648246765137, 'train/loss': 0.9188026189804077, 'validation/accuracy': 0.7074999809265137, 'validation/loss': 1.1961042881011963, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.8227862119674683, 'test/num_examples': 10000, 'score': 63478.52360081673, 'total_duration': 69317.36960935593, 'accumulated_submission_time': 63478.52360081673, 'accumulated_eval_time': 5824.872405529022, 'accumulated_logging_time': 6.267154216766357}
I0202 13:46:42.080308 139923852027648 logging_writer.py:48] [137322] accumulated_eval_time=5824.872406, accumulated_logging_time=6.267154, accumulated_submission_time=63478.523601, global_step=137322, preemption_count=0, score=63478.523601, test/accuracy=0.577700, test/loss=1.822786, test/num_examples=10000, total_duration=69317.369609, train/accuracy=0.771465, train/loss=0.918803, validation/accuracy=0.707500, validation/loss=1.196104, validation/num_examples=50000
I0202 13:47:14.170440 139923868813056 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.3006863594055176, loss=1.7646100521087646
I0202 13:48:00.903823 139923852027648 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.2261266708374023, loss=1.7903428077697754
I0202 13:48:47.860155 139923868813056 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2334554195404053, loss=1.9310845136642456
I0202 13:49:34.627205 139923852027648 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.450762987136841, loss=3.8738412857055664
I0202 13:50:21.395936 139923868813056 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.652756452560425, loss=1.9030811786651611
I0202 13:51:08.416967 139923852027648 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.0848522186279297, loss=3.185201406478882
I0202 13:51:55.387959 139923868813056 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.226445436477661, loss=2.241323709487915
I0202 13:52:42.520492 139923852027648 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.153695821762085, loss=2.8251969814300537
I0202 13:53:29.454303 139923868813056 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.6658313274383545, loss=3.8499276638031006
I0202 13:53:42.282203 140085747812160 spec.py:321] Evaluating on the training split.
I0202 13:53:52.783796 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 13:54:19.593596 140085747812160 spec.py:349] Evaluating on the test split.
I0202 13:54:21.231637 140085747812160 submission_runner.py:408] Time since start: 69776.56s, 	Step: 138229, 	{'train/accuracy': 0.777636706829071, 'train/loss': 0.8658644556999207, 'validation/accuracy': 0.7101799845695496, 'validation/loss': 1.1642639636993408, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.7642264366149902, 'test/num_examples': 10000, 'score': 63898.665610551834, 'total_duration': 69776.55884242058, 'accumulated_submission_time': 63898.665610551834, 'accumulated_eval_time': 5863.8218557834625, 'accumulated_logging_time': 6.314066648483276}
I0202 13:54:21.269132 139923852027648 logging_writer.py:48] [138229] accumulated_eval_time=5863.821856, accumulated_logging_time=6.314067, accumulated_submission_time=63898.665611, global_step=138229, preemption_count=0, score=63898.665611, test/accuracy=0.587500, test/loss=1.764226, test/num_examples=10000, total_duration=69776.558842, train/accuracy=0.777637, train/loss=0.865864, validation/accuracy=0.710180, validation/loss=1.164264, validation/num_examples=50000
I0202 13:54:49.710180 139923868813056 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.507357597351074, loss=1.8038463592529297
I0202 13:55:36.353008 139923852027648 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.0537641048431396, loss=3.982473611831665
I0202 13:56:23.285606 139923868813056 logging_writer.py:48] [138500] global_step=138500, grad_norm=4.07781457901001, loss=4.368705749511719
I0202 13:57:09.937484 139923852027648 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.2243549823760986, loss=2.2540643215179443
I0202 13:57:56.616968 139923868813056 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.187635898590088, loss=2.492452621459961
I0202 13:58:43.115625 139923852027648 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.024920701980591, loss=2.3945765495300293
I0202 13:59:30.070487 139923868813056 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.356760025024414, loss=1.864955186843872
I0202 14:00:16.500784 139923852027648 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.5432536602020264, loss=3.6899173259735107
I0202 14:01:03.393415 139923868813056 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.4250471591949463, loss=1.7593815326690674
I0202 14:01:21.598649 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:01:31.741959 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:01:59.638617 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:02:01.278461 140085747812160 submission_runner.py:408] Time since start: 70236.61s, 	Step: 139141, 	{'train/accuracy': 0.7803515195846558, 'train/loss': 0.868783712387085, 'validation/accuracy': 0.7113199830055237, 'validation/loss': 1.1666555404663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 1.7911489009857178, 'test/num_examples': 10000, 'score': 64318.93400526047, 'total_duration': 70236.60565376282, 'accumulated_submission_time': 64318.93400526047, 'accumulated_eval_time': 5903.501656532288, 'accumulated_logging_time': 6.361291170120239}
I0202 14:02:01.322231 139923852027648 logging_writer.py:48] [139141] accumulated_eval_time=5903.501657, accumulated_logging_time=6.361291, accumulated_submission_time=64318.934005, global_step=139141, preemption_count=0, score=64318.934005, test/accuracy=0.584900, test/loss=1.791149, test/num_examples=10000, total_duration=70236.605654, train/accuracy=0.780352, train/loss=0.868784, validation/accuracy=0.711320, validation/loss=1.166656, validation/num_examples=50000
I0202 14:02:24.846832 139923868813056 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.2991912364959717, loss=1.919726014137268
I0202 14:03:11.223825 139923852027648 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.7338695526123047, loss=2.080040454864502
I0202 14:03:58.086693 139923868813056 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.3389952182769775, loss=4.041871547698975
I0202 14:04:44.786536 139923852027648 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.4286789894104004, loss=3.5538220405578613
I0202 14:05:31.784148 139923868813056 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4789488315582275, loss=1.7473276853561401
I0202 14:06:18.371960 139923852027648 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.4377009868621826, loss=1.9498450756072998
I0202 14:07:05.199863 139923868813056 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.664537191390991, loss=1.8198328018188477
I0202 14:07:51.869695 139923852027648 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.3502037525177, loss=1.6526806354522705
I0202 14:08:38.785824 139923868813056 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.634747266769409, loss=1.730196475982666
I0202 14:09:01.283853 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:09:12.521681 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:09:41.183330 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:09:42.822076 140085747812160 submission_runner.py:408] Time since start: 70698.15s, 	Step: 140050, 	{'train/accuracy': 0.7785937190055847, 'train/loss': 0.8846719861030579, 'validation/accuracy': 0.7123799920082092, 'validation/loss': 1.1717824935913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.7813829183578491, 'test/num_examples': 10000, 'score': 64738.83408522606, 'total_duration': 70698.14928531647, 'accumulated_submission_time': 64738.83408522606, 'accumulated_eval_time': 5945.039888620377, 'accumulated_logging_time': 6.41525673866272}
I0202 14:09:42.860818 139923852027648 logging_writer.py:48] [140050] accumulated_eval_time=5945.039889, accumulated_logging_time=6.415257, accumulated_submission_time=64738.834085, global_step=140050, preemption_count=0, score=64738.834085, test/accuracy=0.590700, test/loss=1.781383, test/num_examples=10000, total_duration=70698.149285, train/accuracy=0.778594, train/loss=0.884672, validation/accuracy=0.712380, validation/loss=1.171782, validation/num_examples=50000
I0202 14:10:02.869536 139923868813056 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.405263900756836, loss=2.0875988006591797
I0202 14:10:48.255798 139923852027648 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.18261981010437, loss=2.5650315284729004
I0202 14:11:35.055592 139923868813056 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.7178032398223877, loss=1.8049423694610596
I0202 14:12:22.093176 139923852027648 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.182762861251831, loss=2.3387255668640137
I0202 14:13:08.884589 139923868813056 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.0750811100006104, loss=3.3571908473968506
I0202 14:13:55.519717 139923852027648 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.787925958633423, loss=1.7069439888000488
I0202 14:14:42.469071 139923868813056 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.4843146800994873, loss=1.7070201635360718
I0202 14:15:29.303564 139923852027648 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.1754708290100098, loss=1.8025543689727783
I0202 14:16:16.109131 139923868813056 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.28178334236145, loss=1.7377451658248901
I0202 14:16:42.957614 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:16:53.439309 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:17:17.306578 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:17:18.950257 140085747812160 submission_runner.py:408] Time since start: 71154.28s, 	Step: 140959, 	{'train/accuracy': 0.775390625, 'train/loss': 0.8824504017829895, 'validation/accuracy': 0.7128199934959412, 'validation/loss': 1.150181770324707, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.7759381532669067, 'test/num_examples': 10000, 'score': 65158.86788415909, 'total_duration': 71154.27746748924, 'accumulated_submission_time': 65158.86788415909, 'accumulated_eval_time': 5981.03254365921, 'accumulated_logging_time': 6.46516752243042}
I0202 14:17:18.992467 139923852027648 logging_writer.py:48] [140959] accumulated_eval_time=5981.032544, accumulated_logging_time=6.465168, accumulated_submission_time=65158.867884, global_step=140959, preemption_count=0, score=65158.867884, test/accuracy=0.589800, test/loss=1.775938, test/num_examples=10000, total_duration=71154.277467, train/accuracy=0.775391, train/loss=0.882450, validation/accuracy=0.712820, validation/loss=1.150182, validation/num_examples=50000
I0202 14:17:35.473929 139923868813056 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.4117324352264404, loss=2.2224106788635254
I0202 14:18:20.200130 139923852027648 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.7856760025024414, loss=4.030558109283447
I0202 14:19:07.367705 139923868813056 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.4073915481567383, loss=3.062592029571533
I0202 14:19:54.090873 139923852027648 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.5933167934417725, loss=3.8917622566223145
I0202 14:20:41.134244 139923868813056 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.442335844039917, loss=2.5498924255371094
I0202 14:21:27.974511 139923852027648 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.304091453552246, loss=2.1668713092803955
I0202 14:22:14.848462 139923868813056 logging_writer.py:48] [141600] global_step=141600, grad_norm=4.012026309967041, loss=3.6556859016418457
I0202 14:23:01.772466 139923852027648 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.618142604827881, loss=3.7345657348632812
I0202 14:23:48.725439 139923868813056 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4867289066314697, loss=1.6760796308517456
I0202 14:24:19.195831 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:24:29.641102 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:24:57.750241 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:24:59.391016 140085747812160 submission_runner.py:408] Time since start: 71614.72s, 	Step: 141867, 	{'train/accuracy': 0.7857617139816284, 'train/loss': 0.8551717400550842, 'validation/accuracy': 0.7181199789047241, 'validation/loss': 1.148403525352478, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.753408432006836, 'test/num_examples': 10000, 'score': 65579.00881290436, 'total_duration': 71614.71821403503, 'accumulated_submission_time': 65579.00881290436, 'accumulated_eval_time': 6021.227711677551, 'accumulated_logging_time': 6.5184853076934814}
I0202 14:24:59.432520 139923852027648 logging_writer.py:48] [141867] accumulated_eval_time=6021.227712, accumulated_logging_time=6.518485, accumulated_submission_time=65579.008813, global_step=141867, preemption_count=0, score=65579.008813, test/accuracy=0.596200, test/loss=1.753408, test/num_examples=10000, total_duration=71614.718214, train/accuracy=0.785762, train/loss=0.855172, validation/accuracy=0.718120, validation/loss=1.148404, validation/num_examples=50000
I0202 14:25:12.776150 139923868813056 logging_writer.py:48] [141900] global_step=141900, grad_norm=4.317616939544678, loss=4.315063953399658
I0202 14:25:56.844960 139923852027648 logging_writer.py:48] [142000] global_step=142000, grad_norm=4.423186779022217, loss=1.8226556777954102
I0202 14:26:43.807958 139923868813056 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.41790771484375, loss=2.8631844520568848
I0202 14:27:30.720662 139923852027648 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.332068681716919, loss=1.696828842163086
I0202 14:28:17.560593 139923868813056 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.4393320083618164, loss=3.078312873840332
I0202 14:29:04.708435 139923852027648 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.3486287593841553, loss=1.8486371040344238
I0202 14:29:51.591037 139923868813056 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.632373571395874, loss=1.565212368965149
I0202 14:30:38.544002 139923852027648 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.326432943344116, loss=1.72576105594635
I0202 14:31:25.379711 139923868813056 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.4967563152313232, loss=2.1709864139556885
I0202 14:31:59.622793 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:32:09.993666 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:32:40.430844 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:32:42.065324 140085747812160 submission_runner.py:408] Time since start: 72077.39s, 	Step: 142775, 	{'train/accuracy': 0.7930663824081421, 'train/loss': 0.8114429116249084, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.1399496793746948, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.7673008441925049, 'test/num_examples': 10000, 'score': 65999.13509559631, 'total_duration': 72077.39252829552, 'accumulated_submission_time': 65999.13509559631, 'accumulated_eval_time': 6063.670241594315, 'accumulated_logging_time': 6.572426080703735}
I0202 14:32:42.106633 139923852027648 logging_writer.py:48] [142775] accumulated_eval_time=6063.670242, accumulated_logging_time=6.572426, accumulated_submission_time=65999.135096, global_step=142775, preemption_count=0, score=65999.135096, test/accuracy=0.594900, test/loss=1.767301, test/num_examples=10000, total_duration=72077.392528, train/accuracy=0.793066, train/loss=0.811443, validation/accuracy=0.716800, validation/loss=1.139950, validation/num_examples=50000
I0202 14:32:52.307252 139923868813056 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.1527278423309326, loss=2.7378571033477783
I0202 14:33:36.013613 139923852027648 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.9912915229797363, loss=1.8611007928848267
I0202 14:34:23.086847 139923868813056 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.679652690887451, loss=3.943633556365967
I0202 14:35:10.046198 139923852027648 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.547189474105835, loss=1.6809264421463013
I0202 14:35:57.115065 139923868813056 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.650163412094116, loss=1.7376962900161743
I0202 14:36:43.795000 139923852027648 logging_writer.py:48] [143300] global_step=143300, grad_norm=4.138828277587891, loss=4.185516357421875
I0202 14:37:30.649128 139923868813056 logging_writer.py:48] [143400] global_step=143400, grad_norm=4.235240459442139, loss=1.8719531297683716
I0202 14:38:17.616500 139923852027648 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.6412975788116455, loss=1.8269964456558228
I0202 14:39:04.553589 139923868813056 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.388442277908325, loss=2.2784008979797363
I0202 14:39:42.200292 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:39:52.480299 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:40:20.359876 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:40:21.991011 140085747812160 submission_runner.py:408] Time since start: 72537.32s, 	Step: 143682, 	{'train/accuracy': 0.7851952910423279, 'train/loss': 0.8426395058631897, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.118364691734314, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.724225640296936, 'test/num_examples': 10000, 'score': 66419.16562604904, 'total_duration': 72537.31822061539, 'accumulated_submission_time': 66419.16562604904, 'accumulated_eval_time': 6103.460964918137, 'accumulated_logging_time': 6.625840425491333}
I0202 14:40:22.034884 139923852027648 logging_writer.py:48] [143682] accumulated_eval_time=6103.460965, accumulated_logging_time=6.625840, accumulated_submission_time=66419.165626, global_step=143682, preemption_count=0, score=66419.165626, test/accuracy=0.601000, test/loss=1.724226, test/num_examples=10000, total_duration=72537.318221, train/accuracy=0.785195, train/loss=0.842640, validation/accuracy=0.722800, validation/loss=1.118365, validation/num_examples=50000
I0202 14:40:29.914783 139923868813056 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.958554267883301, loss=2.879214286804199
I0202 14:41:12.879332 139923852027648 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.562682628631592, loss=1.6989508867263794
I0202 14:41:59.864984 139923868813056 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.4242677688598633, loss=2.9882826805114746
I0202 14:42:47.165029 139923852027648 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.4927096366882324, loss=1.7088065147399902
I0202 14:43:34.094818 139923868813056 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.638282537460327, loss=3.89021635055542
I0202 14:44:21.175859 139923852027648 logging_writer.py:48] [144200] global_step=144200, grad_norm=4.075185775756836, loss=1.737597942352295
I0202 14:45:08.080426 139923868813056 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.8218955993652344, loss=1.6630232334136963
I0202 14:45:55.000444 139923852027648 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.533818244934082, loss=1.7488001585006714
I0202 14:46:41.911887 139923868813056 logging_writer.py:48] [144500] global_step=144500, grad_norm=4.0443010330200195, loss=1.8142454624176025
I0202 14:47:22.486201 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:47:32.748757 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:47:59.222139 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:48:00.860680 140085747812160 submission_runner.py:408] Time since start: 72996.19s, 	Step: 144588, 	{'train/accuracy': 0.7878710627555847, 'train/loss': 0.8365593552589417, 'validation/accuracy': 0.7193599939346313, 'validation/loss': 1.1317459344863892, 'validation/num_examples': 50000, 'test/accuracy': 0.5928000211715698, 'test/loss': 1.7537528276443481, 'test/num_examples': 10000, 'score': 66839.12981963158, 'total_duration': 72996.18788266182, 'accumulated_submission_time': 66839.12981963158, 'accumulated_eval_time': 6141.8354551792145, 'accumulated_logging_time': 7.105395555496216}
I0202 14:48:00.902647 139923852027648 logging_writer.py:48] [144588] accumulated_eval_time=6141.835455, accumulated_logging_time=7.105396, accumulated_submission_time=66839.129820, global_step=144588, preemption_count=0, score=66839.129820, test/accuracy=0.592800, test/loss=1.753753, test/num_examples=10000, total_duration=72996.187883, train/accuracy=0.787871, train/loss=0.836559, validation/accuracy=0.719360, validation/loss=1.131746, validation/num_examples=50000
I0202 14:48:06.010423 139923868813056 logging_writer.py:48] [144600] global_step=144600, grad_norm=4.0171122550964355, loss=1.7283650636672974
I0202 14:48:48.729560 139923852027648 logging_writer.py:48] [144700] global_step=144700, grad_norm=4.016247272491455, loss=1.7090766429901123
I0202 14:49:35.542463 139923868813056 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.528369665145874, loss=2.305178165435791
I0202 14:50:22.683375 139923852027648 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.6930830478668213, loss=1.7201497554779053
I0202 14:51:09.646192 139923868813056 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.272387981414795, loss=2.6642708778381348
I0202 14:51:56.451456 139923852027648 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.5823633670806885, loss=1.8522160053253174
I0202 14:52:43.562002 139923868813056 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.597365140914917, loss=1.7353260517120361
I0202 14:53:30.385882 139923852027648 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.9864277839660645, loss=1.6427632570266724
I0202 14:54:17.363000 139923868813056 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.916719436645508, loss=1.674176812171936
I0202 14:55:01.189270 140085747812160 spec.py:321] Evaluating on the training split.
I0202 14:55:11.829097 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 14:55:41.132839 140085747812160 spec.py:349] Evaluating on the test split.
I0202 14:55:42.777232 140085747812160 submission_runner.py:408] Time since start: 73458.10s, 	Step: 145495, 	{'train/accuracy': 0.7982421517372131, 'train/loss': 0.8103423714637756, 'validation/accuracy': 0.7236799597740173, 'validation/loss': 1.131292462348938, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.7381134033203125, 'test/num_examples': 10000, 'score': 67259.35423231125, 'total_duration': 73458.10443782806, 'accumulated_submission_time': 67259.35423231125, 'accumulated_eval_time': 6183.4234120845795, 'accumulated_logging_time': 7.158540964126587}
I0202 14:55:42.816605 139923852027648 logging_writer.py:48] [145495] accumulated_eval_time=6183.423412, accumulated_logging_time=7.158541, accumulated_submission_time=67259.354232, global_step=145495, preemption_count=0, score=67259.354232, test/accuracy=0.604400, test/loss=1.738113, test/num_examples=10000, total_duration=73458.104438, train/accuracy=0.798242, train/loss=0.810342, validation/accuracy=0.723680, validation/loss=1.131292, validation/num_examples=50000
I0202 14:55:45.176671 139923868813056 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.775820016860962, loss=1.6315219402313232
I0202 14:56:27.576339 139923852027648 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.75537371635437, loss=3.8617944717407227
I0202 14:57:14.317912 139923868813056 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.994147777557373, loss=1.5595427751541138
I0202 14:58:01.292547 139923852027648 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.386126756668091, loss=3.0271482467651367
I0202 14:58:47.758794 139923868813056 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.653097152709961, loss=3.4922523498535156
I0202 14:59:34.521270 139923852027648 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.133284091949463, loss=1.8511393070220947
I0202 15:00:21.300898 139923868813056 logging_writer.py:48] [146100] global_step=146100, grad_norm=4.071462154388428, loss=1.7047104835510254
I0202 15:01:08.196440 139923852027648 logging_writer.py:48] [146200] global_step=146200, grad_norm=4.168550968170166, loss=1.6876380443572998
I0202 15:01:54.970442 139923868813056 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.891141653060913, loss=2.020127296447754
I0202 15:02:42.014722 139923852027648 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.5749351978302, loss=2.5363242626190186
I0202 15:02:43.086114 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:02:53.316556 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:03:22.794222 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:03:24.430203 140085747812160 submission_runner.py:408] Time since start: 73919.76s, 	Step: 146404, 	{'train/accuracy': 0.7885546684265137, 'train/loss': 0.8147965669631958, 'validation/accuracy': 0.7246999740600586, 'validation/loss': 1.102521538734436, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.729142427444458, 'test/num_examples': 10000, 'score': 67679.5624153614, 'total_duration': 73919.75740528107, 'accumulated_submission_time': 67679.5624153614, 'accumulated_eval_time': 6224.7675149440765, 'accumulated_logging_time': 7.207941770553589}
I0202 15:03:24.471826 139923868813056 logging_writer.py:48] [146404] accumulated_eval_time=6224.767515, accumulated_logging_time=7.207942, accumulated_submission_time=67679.562415, global_step=146404, preemption_count=0, score=67679.562415, test/accuracy=0.600700, test/loss=1.729142, test/num_examples=10000, total_duration=73919.757405, train/accuracy=0.788555, train/loss=0.814797, validation/accuracy=0.724700, validation/loss=1.102522, validation/num_examples=50000
I0202 15:04:04.757924 139923852027648 logging_writer.py:48] [146500] global_step=146500, grad_norm=4.185532093048096, loss=4.155547142028809
I0202 15:04:51.244692 139923868813056 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.1657795906066895, loss=1.7655353546142578
I0202 15:05:38.254537 139923852027648 logging_writer.py:48] [146700] global_step=146700, grad_norm=4.381467819213867, loss=1.681591510772705
I0202 15:06:24.962141 139923868813056 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.549067497253418, loss=4.172938823699951
I0202 15:07:11.775857 139923852027648 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.8292367458343506, loss=2.077453851699829
I0202 15:07:58.337901 139923868813056 logging_writer.py:48] [147000] global_step=147000, grad_norm=4.15720272064209, loss=1.6919139623641968
I0202 15:08:44.986241 139923852027648 logging_writer.py:48] [147100] global_step=147100, grad_norm=4.216563701629639, loss=1.7662063837051392
I0202 15:09:31.586028 139923868813056 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.5006449222564697, loss=3.241787910461426
I0202 15:10:18.388495 139923852027648 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.9128005504608154, loss=1.6000946760177612
I0202 15:10:24.637040 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:10:34.928406 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:11:06.745343 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:11:08.398906 140085747812160 submission_runner.py:408] Time since start: 74383.73s, 	Step: 147315, 	{'train/accuracy': 0.7919921875, 'train/loss': 0.8182185888290405, 'validation/accuracy': 0.7255399823188782, 'validation/loss': 1.1136888265609741, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.728710412979126, 'test/num_examples': 10000, 'score': 68099.6663825512, 'total_duration': 74383.72610616684, 'accumulated_submission_time': 68099.6663825512, 'accumulated_eval_time': 6268.529381752014, 'accumulated_logging_time': 7.259737014770508}
I0202 15:11:08.439342 139923868813056 logging_writer.py:48] [147315] accumulated_eval_time=6268.529382, accumulated_logging_time=7.259737, accumulated_submission_time=68099.666383, global_step=147315, preemption_count=0, score=68099.666383, test/accuracy=0.601600, test/loss=1.728710, test/num_examples=10000, total_duration=74383.726106, train/accuracy=0.791992, train/loss=0.818219, validation/accuracy=0.725540, validation/loss=1.113689, validation/num_examples=50000
I0202 15:11:43.697154 139923852027648 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.6681454181671143, loss=3.055119514465332
I0202 15:12:30.326960 139923868813056 logging_writer.py:48] [147500] global_step=147500, grad_norm=4.053282260894775, loss=2.637075185775757
I0202 15:13:17.212281 139923852027648 logging_writer.py:48] [147600] global_step=147600, grad_norm=4.288137435913086, loss=1.6669632196426392
I0202 15:14:04.024672 139923868813056 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.519418239593506, loss=2.107248544692993
I0202 15:14:50.673648 139923852027648 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.863653659820557, loss=1.6474756002426147
I0202 15:15:37.328561 139923868813056 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.004147052764893, loss=1.6251347064971924
I0202 15:16:24.020535 139923852027648 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.9846439361572266, loss=1.7044475078582764
I0202 15:17:11.070937 139923868813056 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.858757972717285, loss=2.0260462760925293
I0202 15:17:57.963770 139923852027648 logging_writer.py:48] [148200] global_step=148200, grad_norm=4.307051181793213, loss=1.780631184577942
I0202 15:18:08.492802 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:18:18.999023 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:18:43.131727 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:18:44.772323 140085747812160 submission_runner.py:408] Time since start: 74840.10s, 	Step: 148224, 	{'train/accuracy': 0.8016796708106995, 'train/loss': 0.7689365744590759, 'validation/accuracy': 0.7294999957084656, 'validation/loss': 1.0898467302322388, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.6969178915023804, 'test/num_examples': 10000, 'score': 68519.65882515907, 'total_duration': 74840.09953451157, 'accumulated_submission_time': 68519.65882515907, 'accumulated_eval_time': 6304.80890417099, 'accumulated_logging_time': 7.309703350067139}
I0202 15:18:44.809907 139923868813056 logging_writer.py:48] [148224] accumulated_eval_time=6304.808904, accumulated_logging_time=7.309703, accumulated_submission_time=68519.658825, global_step=148224, preemption_count=0, score=68519.658825, test/accuracy=0.604000, test/loss=1.696918, test/num_examples=10000, total_duration=74840.099535, train/accuracy=0.801680, train/loss=0.768937, validation/accuracy=0.729500, validation/loss=1.089847, validation/num_examples=50000
I0202 15:19:15.776476 139923852027648 logging_writer.py:48] [148300] global_step=148300, grad_norm=4.161864280700684, loss=1.9930742979049683
I0202 15:20:02.538213 139923868813056 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.9899048805236816, loss=3.079010486602783
I0202 15:20:49.496571 139923852027648 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.8340742588043213, loss=3.484935998916626
I0202 15:21:36.141439 139923868813056 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.7852730751037598, loss=1.6516402959823608
I0202 15:22:23.074785 139923852027648 logging_writer.py:48] [148700] global_step=148700, grad_norm=4.080778121948242, loss=1.5277656316757202
I0202 15:23:09.927658 139923868813056 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.218863487243652, loss=4.047430515289307
I0202 15:23:56.792858 139923852027648 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.9911036491394043, loss=1.8335764408111572
I0202 15:24:43.499693 139923868813056 logging_writer.py:48] [149000] global_step=149000, grad_norm=4.835583686828613, loss=1.53936767578125
I0202 15:25:30.773039 139923852027648 logging_writer.py:48] [149100] global_step=149100, grad_norm=5.0049147605896, loss=1.6337230205535889
I0202 15:25:44.937828 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:25:55.663026 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:26:25.105726 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:26:26.746039 140085747812160 submission_runner.py:408] Time since start: 75302.07s, 	Step: 149132, 	{'train/accuracy': 0.7948632836341858, 'train/loss': 0.8024986982345581, 'validation/accuracy': 0.7267000079154968, 'validation/loss': 1.0969750881195068, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.711686372756958, 'test/num_examples': 10000, 'score': 68939.72495675087, 'total_duration': 75302.07324838638, 'accumulated_submission_time': 68939.72495675087, 'accumulated_eval_time': 6346.617129325867, 'accumulated_logging_time': 7.357788801193237}
I0202 15:26:26.786914 139923868813056 logging_writer.py:48] [149132] accumulated_eval_time=6346.617129, accumulated_logging_time=7.357789, accumulated_submission_time=68939.724957, global_step=149132, preemption_count=0, score=68939.724957, test/accuracy=0.604600, test/loss=1.711686, test/num_examples=10000, total_duration=75302.073248, train/accuracy=0.794863, train/loss=0.802499, validation/accuracy=0.726700, validation/loss=1.096975, validation/num_examples=50000
I0202 15:26:54.309003 139923852027648 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.069060325622559, loss=1.6296881437301636
I0202 15:27:40.872513 139923868813056 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.100327014923096, loss=2.2043569087982178
I0202 15:28:27.731446 139923852027648 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.640232086181641, loss=4.006595611572266
I0202 15:29:14.374439 139923868813056 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.838805198669434, loss=1.506966471672058
I0202 15:30:01.254243 139923852027648 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.937498092651367, loss=2.510186195373535
I0202 15:30:48.043218 139923868813056 logging_writer.py:48] [149700] global_step=149700, grad_norm=4.587201118469238, loss=2.922762155532837
I0202 15:31:34.956903 139923852027648 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.380248546600342, loss=1.7427579164505005
I0202 15:32:22.407660 139923868813056 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.836404800415039, loss=1.6951879262924194
I0202 15:33:09.335466 139923852027648 logging_writer.py:48] [150000] global_step=150000, grad_norm=4.394236087799072, loss=1.6880881786346436
I0202 15:33:26.867604 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:33:37.270347 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:34:04.597439 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:34:06.238594 140085747812160 submission_runner.py:408] Time since start: 75761.57s, 	Step: 150039, 	{'train/accuracy': 0.7957812547683716, 'train/loss': 0.812127411365509, 'validation/accuracy': 0.7278800010681152, 'validation/loss': 1.094635009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.6082000136375427, 'test/loss': 1.716665267944336, 'test/num_examples': 10000, 'score': 69359.74498486519, 'total_duration': 75761.56579613686, 'accumulated_submission_time': 69359.74498486519, 'accumulated_eval_time': 6385.988118648529, 'accumulated_logging_time': 7.408616781234741}
I0202 15:34:06.281148 139923868813056 logging_writer.py:48] [150039] accumulated_eval_time=6385.988119, accumulated_logging_time=7.408617, accumulated_submission_time=69359.744985, global_step=150039, preemption_count=0, score=69359.744985, test/accuracy=0.608200, test/loss=1.716665, test/num_examples=10000, total_duration=75761.565796, train/accuracy=0.795781, train/loss=0.812127, validation/accuracy=0.727880, validation/loss=1.094635, validation/num_examples=50000
I0202 15:34:30.875828 139923852027648 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.29106330871582, loss=2.828928232192993
I0202 15:35:16.818542 139923868813056 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.467524528503418, loss=1.652215838432312
I0202 15:36:03.710914 139923852027648 logging_writer.py:48] [150300] global_step=150300, grad_norm=4.30401611328125, loss=3.6371572017669678
I0202 15:36:50.662605 139923868813056 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.8370566368103027, loss=3.087290048599243
I0202 15:37:37.440221 139923852027648 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.060591220855713, loss=1.659124732017517
I0202 15:38:24.343948 139923868813056 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.10196590423584, loss=1.7997081279754639
I0202 15:39:10.974277 139923852027648 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.547215461730957, loss=3.8108348846435547
I0202 15:39:57.710937 139923868813056 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.880155563354492, loss=2.5055654048919678
I0202 15:40:44.374473 139923852027648 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.503925800323486, loss=1.5234711170196533
I0202 15:41:06.650223 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:41:16.942555 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:41:48.219231 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:41:49.863900 140085747812160 submission_runner.py:408] Time since start: 76225.19s, 	Step: 150949, 	{'train/accuracy': 0.8050976395606995, 'train/loss': 0.7490441799163818, 'validation/accuracy': 0.7310199737548828, 'validation/loss': 1.0718308687210083, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.6876506805419922, 'test/num_examples': 10000, 'score': 69780.05288887024, 'total_duration': 76225.19110178947, 'accumulated_submission_time': 69780.05288887024, 'accumulated_eval_time': 6429.201789140701, 'accumulated_logging_time': 7.461533546447754}
I0202 15:41:49.903891 139923868813056 logging_writer.py:48] [150949] accumulated_eval_time=6429.201789, accumulated_logging_time=7.461534, accumulated_submission_time=69780.052889, global_step=150949, preemption_count=0, score=69780.052889, test/accuracy=0.610300, test/loss=1.687651, test/num_examples=10000, total_duration=76225.191102, train/accuracy=0.805098, train/loss=0.749044, validation/accuracy=0.731020, validation/loss=1.071831, validation/num_examples=50000
I0202 15:42:10.281563 139923852027648 logging_writer.py:48] [151000] global_step=151000, grad_norm=4.459344863891602, loss=1.5254547595977783
I0202 15:42:55.885935 139923868813056 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.297399044036865, loss=1.7445929050445557
I0202 15:43:42.888914 139923852027648 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.425479888916016, loss=1.610794186592102
I0202 15:44:29.564779 139923868813056 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.370275974273682, loss=3.6732070446014404
I0202 15:45:16.381382 139923852027648 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.476482391357422, loss=3.341238260269165
I0202 15:46:03.114927 139923868813056 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.043020725250244, loss=2.6792542934417725
I0202 15:46:49.866967 139923852027648 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.315211296081543, loss=1.585217833518982
I0202 15:47:36.742769 139923868813056 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.3411173820495605, loss=1.6762043237686157
I0202 15:48:23.465744 139923852027648 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.118387222290039, loss=2.176081657409668
I0202 15:48:50.254919 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:49:00.619162 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:49:30.988624 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:49:32.634666 140085747812160 submission_runner.py:408] Time since start: 76687.96s, 	Step: 151859, 	{'train/accuracy': 0.8020703196525574, 'train/loss': 0.7700572609901428, 'validation/accuracy': 0.7359799742698669, 'validation/loss': 1.066151738166809, 'validation/num_examples': 50000, 'test/accuracy': 0.6122000217437744, 'test/loss': 1.6726754903793335, 'test/num_examples': 10000, 'score': 70200.3424217701, 'total_duration': 76687.96187376976, 'accumulated_submission_time': 70200.3424217701, 'accumulated_eval_time': 6471.581538200378, 'accumulated_logging_time': 7.511557102203369}
I0202 15:49:32.676540 139923868813056 logging_writer.py:48] [151859] accumulated_eval_time=6471.581538, accumulated_logging_time=7.511557, accumulated_submission_time=70200.342422, global_step=151859, preemption_count=0, score=70200.342422, test/accuracy=0.612200, test/loss=1.672675, test/num_examples=10000, total_duration=76687.961874, train/accuracy=0.802070, train/loss=0.770057, validation/accuracy=0.735980, validation/loss=1.066152, validation/num_examples=50000
I0202 15:49:49.151456 139923852027648 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.344813346862793, loss=1.5924146175384521
I0202 15:50:33.846209 139923868813056 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.82590913772583, loss=1.6209263801574707
I0202 15:51:20.589489 139923852027648 logging_writer.py:48] [152100] global_step=152100, grad_norm=4.565739631652832, loss=1.6451148986816406
I0202 15:52:07.106827 139923868813056 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.376587867736816, loss=1.6532151699066162
I0202 15:52:54.039049 139923852027648 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.006399154663086, loss=3.10214900970459
I0202 15:53:40.758178 139923868813056 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.131655693054199, loss=1.7790675163269043
I0202 15:54:27.655804 139923852027648 logging_writer.py:48] [152500] global_step=152500, grad_norm=4.39253568649292, loss=1.844020128250122
I0202 15:55:14.291637 139923868813056 logging_writer.py:48] [152600] global_step=152600, grad_norm=4.1339521408081055, loss=2.02225399017334
I0202 15:56:01.198727 139923852027648 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.373337745666504, loss=1.5220999717712402
I0202 15:56:32.679757 140085747812160 spec.py:321] Evaluating on the training split.
I0202 15:56:43.053900 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 15:57:10.364914 140085747812160 spec.py:349] Evaluating on the test split.
I0202 15:57:12.021395 140085747812160 submission_runner.py:408] Time since start: 77147.35s, 	Step: 152769, 	{'train/accuracy': 0.8058202862739563, 'train/loss': 0.7566875219345093, 'validation/accuracy': 0.7363399863243103, 'validation/loss': 1.0597726106643677, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.6564711332321167, 'test/num_examples': 10000, 'score': 70620.28472137451, 'total_duration': 77147.348580122, 'accumulated_submission_time': 70620.28472137451, 'accumulated_eval_time': 6510.923154830933, 'accumulated_logging_time': 7.5636162757873535}
I0202 15:57:12.063066 139923868813056 logging_writer.py:48] [152769] accumulated_eval_time=6510.923155, accumulated_logging_time=7.563616, accumulated_submission_time=70620.284721, global_step=152769, preemption_count=0, score=70620.284721, test/accuracy=0.614300, test/loss=1.656471, test/num_examples=10000, total_duration=77147.348580, train/accuracy=0.805820, train/loss=0.756688, validation/accuracy=0.736340, validation/loss=1.059773, validation/num_examples=50000
I0202 15:57:24.614720 139923852027648 logging_writer.py:48] [152800] global_step=152800, grad_norm=4.3816609382629395, loss=1.6448642015457153
I0202 15:58:08.783907 139923868813056 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.571039199829102, loss=3.6765472888946533
I0202 15:58:55.463770 139923852027648 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.439173698425293, loss=2.9168691635131836
I0202 15:59:42.546582 139923868813056 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.162277698516846, loss=3.4541378021240234
I0202 16:00:29.184507 139923852027648 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.6261091232299805, loss=1.7370381355285645
I0202 16:01:15.974970 139923868813056 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.190977573394775, loss=2.180006265640259
I0202 16:02:02.653139 139923852027648 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.189558982849121, loss=1.503458023071289
I0202 16:02:49.424355 139923868813056 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.20546293258667, loss=2.650848150253296
I0202 16:03:36.338318 139923852027648 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.3156046867370605, loss=1.6376638412475586
I0202 16:04:12.080669 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:04:23.041700 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:04:47.887258 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:04:49.531676 140085747812160 submission_runner.py:408] Time since start: 77604.86s, 	Step: 153678, 	{'train/accuracy': 0.81298828125, 'train/loss': 0.7200682759284973, 'validation/accuracy': 0.7379800081253052, 'validation/loss': 1.0499589443206787, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.6612064838409424, 'test/num_examples': 10000, 'score': 71040.23987174034, 'total_duration': 77604.85888195038, 'accumulated_submission_time': 71040.23987174034, 'accumulated_eval_time': 6548.3741619586945, 'accumulated_logging_time': 7.616273880004883}
I0202 16:04:49.573858 139923868813056 logging_writer.py:48] [153678] accumulated_eval_time=6548.374162, accumulated_logging_time=7.616274, accumulated_submission_time=71040.239872, global_step=153678, preemption_count=0, score=71040.239872, test/accuracy=0.616600, test/loss=1.661206, test/num_examples=10000, total_duration=77604.858882, train/accuracy=0.812988, train/loss=0.720068, validation/accuracy=0.737980, validation/loss=1.049959, validation/num_examples=50000
I0202 16:04:58.602479 139923852027648 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.575050354003906, loss=1.6558682918548584
I0202 16:05:41.869326 139923868813056 logging_writer.py:48] [153800] global_step=153800, grad_norm=4.274038314819336, loss=2.5366711616516113
I0202 16:06:28.850402 139923852027648 logging_writer.py:48] [153900] global_step=153900, grad_norm=5.19786262512207, loss=1.5907312631607056
I0202 16:07:15.832846 139923868813056 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.740888595581055, loss=1.6020710468292236
I0202 16:08:02.451169 139923852027648 logging_writer.py:48] [154100] global_step=154100, grad_norm=5.4338226318359375, loss=3.953543186187744
I0202 16:08:49.322360 139923868813056 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.436410903930664, loss=1.8054933547973633
I0202 16:09:36.093367 139923852027648 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.293390274047852, loss=1.9138550758361816
I0202 16:10:23.144382 139923868813056 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.5704779624938965, loss=1.4989501237869263
I0202 16:11:09.811946 139923852027648 logging_writer.py:48] [154500] global_step=154500, grad_norm=4.237086772918701, loss=2.3442282676696777
I0202 16:11:49.922001 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:12:00.580502 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:12:26.651917 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:12:28.304243 140085747812160 submission_runner.py:408] Time since start: 78063.63s, 	Step: 154587, 	{'train/accuracy': 0.8049609065055847, 'train/loss': 0.746151864528656, 'validation/accuracy': 0.737339973449707, 'validation/loss': 1.0490286350250244, 'validation/num_examples': 50000, 'test/accuracy': 0.6158000230789185, 'test/loss': 1.6725051403045654, 'test/num_examples': 10000, 'score': 71460.52765202522, 'total_duration': 78063.63143539429, 'accumulated_submission_time': 71460.52765202522, 'accumulated_eval_time': 6586.756381750107, 'accumulated_logging_time': 7.667668581008911}
I0202 16:12:28.348646 139923868813056 logging_writer.py:48] [154587] accumulated_eval_time=6586.756382, accumulated_logging_time=7.667669, accumulated_submission_time=71460.527652, global_step=154587, preemption_count=0, score=71460.527652, test/accuracy=0.615800, test/loss=1.672505, test/num_examples=10000, total_duration=78063.631435, train/accuracy=0.804961, train/loss=0.746152, validation/accuracy=0.737340, validation/loss=1.049029, validation/num_examples=50000
I0202 16:12:33.841673 139923852027648 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.763759613037109, loss=1.536761999130249
I0202 16:13:16.293344 139923868813056 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.710376739501953, loss=1.5920825004577637
I0202 16:14:02.969244 139923852027648 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.3573222160339355, loss=1.821207880973816
I0202 16:14:49.773102 139923868813056 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.761257171630859, loss=1.6284277439117432
I0202 16:15:36.582862 139923852027648 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.325211524963379, loss=2.2123069763183594
I0202 16:16:23.270830 139923868813056 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.7854413986206055, loss=1.7812652587890625
I0202 16:17:10.293831 139923852027648 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.682929515838623, loss=1.6282968521118164
I0202 16:17:57.119774 139923868813056 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.431112766265869, loss=2.138489246368408
I0202 16:18:43.870820 139923852027648 logging_writer.py:48] [155400] global_step=155400, grad_norm=5.133558750152588, loss=1.6296340227127075
I0202 16:19:28.571635 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:19:39.203073 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:20:07.908937 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:20:09.561655 140085747812160 submission_runner.py:408] Time since start: 78524.89s, 	Step: 155497, 	{'train/accuracy': 0.810839831829071, 'train/loss': 0.723305344581604, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.035240888595581, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.652098298072815, 'test/num_examples': 10000, 'score': 71880.68959569931, 'total_duration': 78524.88885855675, 'accumulated_submission_time': 71880.68959569931, 'accumulated_eval_time': 6627.746407985687, 'accumulated_logging_time': 7.721628189086914}
I0202 16:20:09.604318 139923868813056 logging_writer.py:48] [155497] accumulated_eval_time=6627.746408, accumulated_logging_time=7.721628, accumulated_submission_time=71880.689596, global_step=155497, preemption_count=0, score=71880.689596, test/accuracy=0.616400, test/loss=1.652098, test/num_examples=10000, total_duration=78524.888859, train/accuracy=0.810840, train/loss=0.723305, validation/accuracy=0.739920, validation/loss=1.035241, validation/num_examples=50000
I0202 16:20:11.178365 139923852027648 logging_writer.py:48] [155500] global_step=155500, grad_norm=5.243609428405762, loss=4.001055717468262
I0202 16:20:53.242396 139923868813056 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.3179450035095215, loss=1.5426665544509888
I0202 16:21:39.923629 139923852027648 logging_writer.py:48] [155700] global_step=155700, grad_norm=5.25909423828125, loss=2.4000606536865234
I0202 16:22:27.019319 139923868813056 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.603498935699463, loss=1.5239826440811157
I0202 16:23:13.827101 139923852027648 logging_writer.py:48] [155900] global_step=155900, grad_norm=4.407323360443115, loss=2.0371334552764893
I0202 16:24:00.595371 139923868813056 logging_writer.py:48] [156000] global_step=156000, grad_norm=5.260271072387695, loss=1.738994836807251
I0202 16:24:47.158362 139923852027648 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.912743091583252, loss=1.8014874458312988
I0202 16:25:33.929699 139923868813056 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.899171829223633, loss=1.5620217323303223
I0202 16:26:20.673905 139923852027648 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.410290718078613, loss=3.3068201541900635
I0202 16:27:07.701030 139923868813056 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.787018299102783, loss=3.98946475982666
I0202 16:27:09.731154 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:27:20.218919 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:27:44.417328 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:27:46.067347 140085747812160 submission_runner.py:408] Time since start: 78981.39s, 	Step: 156406, 	{'train/accuracy': 0.8157812356948853, 'train/loss': 0.7162489295005798, 'validation/accuracy': 0.7403799891471863, 'validation/loss': 1.0414212942123413, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.653515338897705, 'test/num_examples': 10000, 'score': 72300.75505638123, 'total_duration': 78981.39456152916, 'accumulated_submission_time': 72300.75505638123, 'accumulated_eval_time': 6664.082603693008, 'accumulated_logging_time': 7.774799346923828}
I0202 16:27:46.109265 139923852027648 logging_writer.py:48] [156406] accumulated_eval_time=6664.082604, accumulated_logging_time=7.774799, accumulated_submission_time=72300.755056, global_step=156406, preemption_count=0, score=72300.755056, test/accuracy=0.620900, test/loss=1.653515, test/num_examples=10000, total_duration=78981.394562, train/accuracy=0.815781, train/loss=0.716249, validation/accuracy=0.740380, validation/loss=1.041421, validation/num_examples=50000
I0202 16:28:25.314052 139923868813056 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.933385848999023, loss=1.5653860569000244
I0202 16:29:11.884777 139923852027648 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.685064315795898, loss=1.704930067062378
I0202 16:29:59.234725 139923868813056 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.716920852661133, loss=1.5791114568710327
I0202 16:30:46.040337 139923852027648 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.627549171447754, loss=1.4996693134307861
I0202 16:31:33.062005 139923868813056 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.384672164916992, loss=1.6975529193878174
I0202 16:32:19.760384 139923852027648 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.601858615875244, loss=1.4279332160949707
I0202 16:33:06.847323 139923868813056 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.807559013366699, loss=1.503347635269165
I0202 16:33:53.640933 139923852027648 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.464277744293213, loss=3.0189101696014404
I0202 16:34:40.441425 139923868813056 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.83752965927124, loss=1.530811071395874
I0202 16:34:46.213399 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:34:56.494736 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:35:26.159663 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:35:27.800841 140085747812160 submission_runner.py:408] Time since start: 79443.13s, 	Step: 157314, 	{'train/accuracy': 0.8150194883346558, 'train/loss': 0.7227945327758789, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.0487637519836426, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.6517783403396606, 'test/num_examples': 10000, 'score': 72720.79807853699, 'total_duration': 79443.12803387642, 'accumulated_submission_time': 72720.79807853699, 'accumulated_eval_time': 6705.670013189316, 'accumulated_logging_time': 7.826829195022583}
I0202 16:35:27.841660 139923852027648 logging_writer.py:48] [157314] accumulated_eval_time=6705.670013, accumulated_logging_time=7.826829, accumulated_submission_time=72720.798079, global_step=157314, preemption_count=0, score=72720.798079, test/accuracy=0.623700, test/loss=1.651778, test/num_examples=10000, total_duration=79443.128034, train/accuracy=0.815019, train/loss=0.722795, validation/accuracy=0.741580, validation/loss=1.048764, validation/num_examples=50000
I0202 16:36:03.753067 139923868813056 logging_writer.py:48] [157400] global_step=157400, grad_norm=5.083042144775391, loss=1.5723904371261597
I0202 16:36:50.258246 139923852027648 logging_writer.py:48] [157500] global_step=157500, grad_norm=5.05473518371582, loss=1.4954237937927246
I0202 16:37:37.448300 139923868813056 logging_writer.py:48] [157600] global_step=157600, grad_norm=5.104836463928223, loss=1.5005306005477905
I0202 16:38:24.424202 139923852027648 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.916972637176514, loss=1.5438661575317383
I0202 16:39:11.170875 139923868813056 logging_writer.py:48] [157800] global_step=157800, grad_norm=4.662670135498047, loss=1.7648251056671143
I0202 16:39:57.868426 139923852027648 logging_writer.py:48] [157900] global_step=157900, grad_norm=5.167236804962158, loss=1.5799083709716797
I0202 16:40:44.730113 139923868813056 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.430182456970215, loss=1.5461783409118652
I0202 16:41:31.450028 139923852027648 logging_writer.py:48] [158100] global_step=158100, grad_norm=5.105147361755371, loss=1.4561907052993774
I0202 16:42:18.076720 139923868813056 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.7339253425598145, loss=3.0928075313568115
I0202 16:42:28.149626 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:42:38.667714 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:43:09.496835 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:43:11.134326 140085747812160 submission_runner.py:408] Time since start: 79906.46s, 	Step: 158223, 	{'train/accuracy': 0.8199414014816284, 'train/loss': 0.7127540707588196, 'validation/accuracy': 0.7445399761199951, 'validation/loss': 1.0261250734329224, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.6282966136932373, 'test/num_examples': 10000, 'score': 73141.04412603378, 'total_duration': 79906.46152997017, 'accumulated_submission_time': 73141.04412603378, 'accumulated_eval_time': 6748.654703617096, 'accumulated_logging_time': 7.878124952316284}
I0202 16:43:11.174992 139923852027648 logging_writer.py:48] [158223] accumulated_eval_time=6748.654704, accumulated_logging_time=7.878125, accumulated_submission_time=73141.044126, global_step=158223, preemption_count=0, score=73141.044126, test/accuracy=0.626800, test/loss=1.628297, test/num_examples=10000, total_duration=79906.461530, train/accuracy=0.819941, train/loss=0.712754, validation/accuracy=0.744540, validation/loss=1.026125, validation/num_examples=50000
I0202 16:43:42.621402 139923868813056 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.988520622253418, loss=1.7198940515518188
I0202 16:44:29.596478 139923852027648 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.814085483551025, loss=1.6355459690093994
I0202 16:45:16.979344 139923868813056 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.5922369956970215, loss=1.7996199131011963
I0202 16:46:03.643836 139923852027648 logging_writer.py:48] [158600] global_step=158600, grad_norm=5.660983562469482, loss=3.571927547454834
I0202 16:46:50.555926 139923868813056 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.426351547241211, loss=2.719813585281372
I0202 16:47:37.210672 139923852027648 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.941738605499268, loss=3.503655433654785
I0202 16:48:24.483210 139923868813056 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.965267181396484, loss=2.3934452533721924
I0202 16:49:11.078894 139923852027648 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.879825115203857, loss=1.5127543210983276
I0202 16:49:57.963055 139923868813056 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.4475860595703125, loss=1.9919943809509277
I0202 16:50:11.324208 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:50:22.038038 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:50:53.635051 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:50:55.273068 140085747812160 submission_runner.py:408] Time since start: 80370.60s, 	Step: 159130, 	{'train/accuracy': 0.8240038752555847, 'train/loss': 0.6871118545532227, 'validation/accuracy': 0.747439980506897, 'validation/loss': 1.0210548639297485, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.622787356376648, 'test/num_examples': 10000, 'score': 73561.13456368446, 'total_duration': 80370.60028123856, 'accumulated_submission_time': 73561.13456368446, 'accumulated_eval_time': 6792.603569984436, 'accumulated_logging_time': 7.927542448043823}
I0202 16:50:55.315260 139923852027648 logging_writer.py:48] [159130] accumulated_eval_time=6792.603570, accumulated_logging_time=7.927542, accumulated_submission_time=73561.134564, global_step=159130, preemption_count=0, score=73561.134564, test/accuracy=0.626900, test/loss=1.622787, test/num_examples=10000, total_duration=80370.600281, train/accuracy=0.824004, train/loss=0.687112, validation/accuracy=0.747440, validation/loss=1.021055, validation/num_examples=50000
I0202 16:51:23.675931 139923868813056 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.99225378036499, loss=2.561349630355835
I0202 16:52:10.454380 139923852027648 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.931804656982422, loss=1.6221439838409424
I0202 16:52:57.362637 139923868813056 logging_writer.py:48] [159400] global_step=159400, grad_norm=5.26008939743042, loss=1.6406923532485962
I0202 16:53:44.011570 139923852027648 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.9147186279296875, loss=1.509065866470337
I0202 16:54:30.871806 139923868813056 logging_writer.py:48] [159600] global_step=159600, grad_norm=5.314946174621582, loss=3.1008663177490234
I0202 16:55:17.631288 139923852027648 logging_writer.py:48] [159700] global_step=159700, grad_norm=5.100300312042236, loss=3.1707019805908203
I0202 16:56:04.386645 139923868813056 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.7947916984558105, loss=1.5447384119033813
I0202 16:56:51.305949 139923852027648 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.783010482788086, loss=2.7421493530273438
I0202 16:57:37.948508 139923868813056 logging_writer.py:48] [160000] global_step=160000, grad_norm=5.087599754333496, loss=3.586073398590088
I0202 16:57:55.394697 140085747812160 spec.py:321] Evaluating on the training split.
I0202 16:58:05.952669 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 16:58:32.718511 140085747812160 spec.py:349] Evaluating on the test split.
I0202 16:58:34.364948 140085747812160 submission_runner.py:408] Time since start: 80829.69s, 	Step: 160039, 	{'train/accuracy': 0.8271484375, 'train/loss': 0.6677748560905457, 'validation/accuracy': 0.7458199858665466, 'validation/loss': 1.0177862644195557, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.6190569400787354, 'test/num_examples': 10000, 'score': 73981.15388822556, 'total_duration': 80829.69215226173, 'accumulated_submission_time': 73981.15388822556, 'accumulated_eval_time': 6831.573830366135, 'accumulated_logging_time': 7.979724168777466}
I0202 16:58:34.405538 139923852027648 logging_writer.py:48] [160039] accumulated_eval_time=6831.573830, accumulated_logging_time=7.979724, accumulated_submission_time=73981.153888, global_step=160039, preemption_count=0, score=73981.153888, test/accuracy=0.622000, test/loss=1.619057, test/num_examples=10000, total_duration=80829.692152, train/accuracy=0.827148, train/loss=0.667775, validation/accuracy=0.745820, validation/loss=1.017786, validation/num_examples=50000
I0202 16:58:58.707957 139923868813056 logging_writer.py:48] [160100] global_step=160100, grad_norm=5.291052341461182, loss=3.0568175315856934
I0202 16:59:44.901525 139923852027648 logging_writer.py:48] [160200] global_step=160200, grad_norm=5.228343486785889, loss=3.20662260055542
I0202 17:00:31.526272 139923868813056 logging_writer.py:48] [160300] global_step=160300, grad_norm=5.461129665374756, loss=3.7558488845825195
I0202 17:01:18.341750 139923852027648 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.834954261779785, loss=1.5644649267196655
I0202 17:02:05.215570 139923868813056 logging_writer.py:48] [160500] global_step=160500, grad_norm=5.246345043182373, loss=1.3310788869857788
I0202 17:02:52.228554 139923852027648 logging_writer.py:48] [160600] global_step=160600, grad_norm=5.701812267303467, loss=3.5882012844085693
I0202 17:03:39.039349 139923868813056 logging_writer.py:48] [160700] global_step=160700, grad_norm=5.021765232086182, loss=1.5847371816635132
I0202 17:04:25.935023 139923852027648 logging_writer.py:48] [160800] global_step=160800, grad_norm=5.106627941131592, loss=3.2684884071350098
I0202 17:05:12.727731 139923868813056 logging_writer.py:48] [160900] global_step=160900, grad_norm=5.226999282836914, loss=1.8021341562271118
I0202 17:05:34.384903 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:05:45.202247 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:06:10.506979 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:06:12.161929 140085747812160 submission_runner.py:408] Time since start: 81287.49s, 	Step: 160948, 	{'train/accuracy': 0.8216406106948853, 'train/loss': 0.6819668412208557, 'validation/accuracy': 0.7469599843025208, 'validation/loss': 1.0042126178741455, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.608596682548523, 'test/num_examples': 10000, 'score': 74401.07062625885, 'total_duration': 81287.48914146423, 'accumulated_submission_time': 74401.07062625885, 'accumulated_eval_time': 6869.350848913193, 'accumulated_logging_time': 8.031014919281006}
I0202 17:06:12.204611 139923852027648 logging_writer.py:48] [160948] accumulated_eval_time=6869.350849, accumulated_logging_time=8.031015, accumulated_submission_time=74401.070626, global_step=160948, preemption_count=0, score=74401.070626, test/accuracy=0.623900, test/loss=1.608597, test/num_examples=10000, total_duration=81287.489141, train/accuracy=0.821641, train/loss=0.681967, validation/accuracy=0.746960, validation/loss=1.004213, validation/num_examples=50000
I0202 17:06:32.987648 139923868813056 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.899248123168945, loss=1.9066516160964966
I0202 17:07:18.573290 139923852027648 logging_writer.py:48] [161100] global_step=161100, grad_norm=5.149387359619141, loss=1.3769774436950684
I0202 17:08:05.574984 139923868813056 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.933233737945557, loss=1.5423192977905273
I0202 17:08:52.209341 139923852027648 logging_writer.py:48] [161300] global_step=161300, grad_norm=5.089076519012451, loss=1.5664178133010864
I0202 17:09:39.271212 139923868813056 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.638969898223877, loss=2.1690704822540283
I0202 17:10:25.956477 139923852027648 logging_writer.py:48] [161500] global_step=161500, grad_norm=5.173884391784668, loss=2.775942802429199
I0202 17:11:12.884261 139923868813056 logging_writer.py:48] [161600] global_step=161600, grad_norm=5.310803413391113, loss=1.5061911344528198
I0202 17:11:59.584473 139923852027648 logging_writer.py:48] [161700] global_step=161700, grad_norm=5.720707893371582, loss=1.5509307384490967
I0202 17:12:46.716342 139923868813056 logging_writer.py:48] [161800] global_step=161800, grad_norm=5.024232387542725, loss=1.3490740060806274
I0202 17:13:12.239118 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:13:22.615482 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:13:52.959703 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:13:54.611793 140085747812160 submission_runner.py:408] Time since start: 81749.94s, 	Step: 161856, 	{'train/accuracy': 0.8240429759025574, 'train/loss': 0.6735599637031555, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 0.9978360533714294, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.5975457429885864, 'test/num_examples': 10000, 'score': 74821.04405879974, 'total_duration': 81749.93900322914, 'accumulated_submission_time': 74821.04405879974, 'accumulated_eval_time': 6911.723528146744, 'accumulated_logging_time': 8.083473920822144}
I0202 17:13:54.656060 139923852027648 logging_writer.py:48] [161856] accumulated_eval_time=6911.723528, accumulated_logging_time=8.083474, accumulated_submission_time=74821.044059, global_step=161856, preemption_count=0, score=74821.044059, test/accuracy=0.632000, test/loss=1.597546, test/num_examples=10000, total_duration=81749.939003, train/accuracy=0.824043, train/loss=0.673560, validation/accuracy=0.750660, validation/loss=0.997836, validation/num_examples=50000
I0202 17:14:12.287592 139923868813056 logging_writer.py:48] [161900] global_step=161900, grad_norm=5.543312072753906, loss=3.5584917068481445
I0202 17:14:57.428451 139923852027648 logging_writer.py:48] [162000] global_step=162000, grad_norm=5.5760064125061035, loss=1.6285650730133057
I0202 17:15:44.428722 139923868813056 logging_writer.py:48] [162100] global_step=162100, grad_norm=6.393251419067383, loss=3.8693699836730957
I0202 17:16:31.001089 139923852027648 logging_writer.py:48] [162200] global_step=162200, grad_norm=5.119869232177734, loss=1.5673915147781372
I0202 17:17:17.724245 139923868813056 logging_writer.py:48] [162300] global_step=162300, grad_norm=6.869680881500244, loss=1.4612154960632324
I0202 17:18:04.427791 139923852027648 logging_writer.py:48] [162400] global_step=162400, grad_norm=5.21877908706665, loss=1.4363130331039429
I0202 17:18:51.300152 139923868813056 logging_writer.py:48] [162500] global_step=162500, grad_norm=5.651163578033447, loss=1.5801310539245605
I0202 17:19:38.109498 139923852027648 logging_writer.py:48] [162600] global_step=162600, grad_norm=5.088383674621582, loss=1.4819082021713257
I0202 17:20:25.180982 139923868813056 logging_writer.py:48] [162700] global_step=162700, grad_norm=5.5883026123046875, loss=1.4385885000228882
I0202 17:20:54.654813 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:21:05.617051 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:21:38.242072 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:21:39.889132 140085747812160 submission_runner.py:408] Time since start: 82215.22s, 	Step: 162765, 	{'train/accuracy': 0.8299023509025574, 'train/loss': 0.6468636989593506, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 0.9937317967414856, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.6002475023269653, 'test/num_examples': 10000, 'score': 75240.97953391075, 'total_duration': 82215.21632957458, 'accumulated_submission_time': 75240.97953391075, 'accumulated_eval_time': 6956.957853317261, 'accumulated_logging_time': 8.137528657913208}
I0202 17:21:39.931547 139923852027648 logging_writer.py:48] [162765] accumulated_eval_time=6956.957853, accumulated_logging_time=8.137529, accumulated_submission_time=75240.979534, global_step=162765, preemption_count=0, score=75240.979534, test/accuracy=0.631200, test/loss=1.600248, test/num_examples=10000, total_duration=82215.216330, train/accuracy=0.829902, train/loss=0.646864, validation/accuracy=0.751120, validation/loss=0.993732, validation/num_examples=50000
I0202 17:21:54.038229 139923868813056 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.729829788208008, loss=3.538600206375122
I0202 17:22:38.546315 139923852027648 logging_writer.py:48] [162900] global_step=162900, grad_norm=5.092990398406982, loss=3.0751824378967285
I0202 17:23:25.494445 139923868813056 logging_writer.py:48] [163000] global_step=163000, grad_norm=5.660496234893799, loss=1.5584015846252441
I0202 17:24:12.217727 139923852027648 logging_writer.py:48] [163100] global_step=163100, grad_norm=5.245113372802734, loss=2.3880927562713623
I0202 17:24:58.825578 139923868813056 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.985846996307373, loss=1.3981751203536987
I0202 17:25:45.427120 139923852027648 logging_writer.py:48] [163300] global_step=163300, grad_norm=5.616800785064697, loss=1.6031231880187988
I0202 17:26:32.375834 139923868813056 logging_writer.py:48] [163400] global_step=163400, grad_norm=5.117428779602051, loss=1.581622838973999
I0202 17:27:19.172082 139923852027648 logging_writer.py:48] [163500] global_step=163500, grad_norm=5.50277853012085, loss=3.0232996940612793
I0202 17:28:05.983151 139923868813056 logging_writer.py:48] [163600] global_step=163600, grad_norm=6.4644551277160645, loss=3.429638385772705
I0202 17:28:40.273840 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:28:50.906144 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:29:17.449950 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:29:19.088266 140085747812160 submission_runner.py:408] Time since start: 82674.42s, 	Step: 163675, 	{'train/accuracy': 0.8234374523162842, 'train/loss': 0.6833599209785461, 'validation/accuracy': 0.7499200105667114, 'validation/loss': 0.9993503093719482, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.6094584465026855, 'test/num_examples': 10000, 'score': 75661.26156401634, 'total_duration': 82674.41547107697, 'accumulated_submission_time': 75661.26156401634, 'accumulated_eval_time': 6995.772277355194, 'accumulated_logging_time': 8.188964128494263}
I0202 17:29:19.135387 139923852027648 logging_writer.py:48] [163675] accumulated_eval_time=6995.772277, accumulated_logging_time=8.188964, accumulated_submission_time=75661.261564, global_step=163675, preemption_count=0, score=75661.261564, test/accuracy=0.628100, test/loss=1.609458, test/num_examples=10000, total_duration=82674.415471, train/accuracy=0.823437, train/loss=0.683360, validation/accuracy=0.749920, validation/loss=0.999350, validation/num_examples=50000
I0202 17:29:29.328757 139923868813056 logging_writer.py:48] [163700] global_step=163700, grad_norm=5.612812519073486, loss=2.468658685684204
I0202 17:30:12.959292 139923852027648 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.495387077331543, loss=1.5202432870864868
I0202 17:31:00.119745 139923868813056 logging_writer.py:48] [163900] global_step=163900, grad_norm=5.9497270584106445, loss=1.5296252965927124
I0202 17:31:47.317759 139923852027648 logging_writer.py:48] [164000] global_step=164000, grad_norm=5.519686698913574, loss=2.98335337638855
I0202 17:32:34.080100 139923868813056 logging_writer.py:48] [164100] global_step=164100, grad_norm=6.584926605224609, loss=3.9122655391693115
I0202 17:33:20.755757 139923852027648 logging_writer.py:48] [164200] global_step=164200, grad_norm=6.314698696136475, loss=3.8085451126098633
I0202 17:34:07.522348 139923868813056 logging_writer.py:48] [164300] global_step=164300, grad_norm=5.564034461975098, loss=3.460062265396118
I0202 17:34:54.422011 139923852027648 logging_writer.py:48] [164400] global_step=164400, grad_norm=5.334045886993408, loss=1.5988978147506714
I0202 17:35:41.137993 139923868813056 logging_writer.py:48] [164500] global_step=164500, grad_norm=5.090824127197266, loss=1.346940040588379
I0202 17:36:19.137302 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:36:29.977790 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:36:58.343258 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:36:59.976185 140085747812160 submission_runner.py:408] Time since start: 83135.30s, 	Step: 164583, 	{'train/accuracy': 0.8306640386581421, 'train/loss': 0.6563959121704102, 'validation/accuracy': 0.7524600028991699, 'validation/loss': 0.9888281226158142, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.5833725929260254, 'test/num_examples': 10000, 'score': 76081.20295095444, 'total_duration': 83135.30335402489, 'accumulated_submission_time': 76081.20295095444, 'accumulated_eval_time': 7036.61114192009, 'accumulated_logging_time': 8.245900869369507}
I0202 17:37:00.022839 139923852027648 logging_writer.py:48] [164583] accumulated_eval_time=7036.611142, accumulated_logging_time=8.245901, accumulated_submission_time=76081.202951, global_step=164583, preemption_count=0, score=76081.202951, test/accuracy=0.632400, test/loss=1.583373, test/num_examples=10000, total_duration=83135.303354, train/accuracy=0.830664, train/loss=0.656396, validation/accuracy=0.752460, validation/loss=0.988828, validation/num_examples=50000
I0202 17:37:07.083853 139923868813056 logging_writer.py:48] [164600] global_step=164600, grad_norm=5.596439838409424, loss=1.4760687351226807
I0202 17:37:50.247602 139923852027648 logging_writer.py:48] [164700] global_step=164700, grad_norm=5.7252068519592285, loss=3.145538330078125
I0202 17:38:37.131546 139923868813056 logging_writer.py:48] [164800] global_step=164800, grad_norm=5.2399115562438965, loss=2.0841968059539795
I0202 17:39:23.904859 139923852027648 logging_writer.py:48] [164900] global_step=164900, grad_norm=5.2792840003967285, loss=1.733847975730896
I0202 17:40:10.816684 139923868813056 logging_writer.py:48] [165000] global_step=165000, grad_norm=6.496845722198486, loss=1.546105146408081
I0202 17:40:57.531409 139923852027648 logging_writer.py:48] [165100] global_step=165100, grad_norm=5.556843280792236, loss=2.9703760147094727
I0202 17:41:44.333374 139923868813056 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.451091289520264, loss=1.618666410446167
I0202 17:42:31.206827 139923852027648 logging_writer.py:48] [165300] global_step=165300, grad_norm=5.599234104156494, loss=1.7513478994369507
I0202 17:43:17.903682 139923868813056 logging_writer.py:48] [165400] global_step=165400, grad_norm=5.334179401397705, loss=1.4686086177825928
I0202 17:44:00.415058 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:44:11.026492 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:44:37.856777 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:44:39.497876 140085747812160 submission_runner.py:408] Time since start: 83594.83s, 	Step: 165493, 	{'train/accuracy': 0.8374218344688416, 'train/loss': 0.6225919723510742, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 0.9747494459152222, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.5695737600326538, 'test/num_examples': 10000, 'score': 76501.53271389008, 'total_duration': 83594.82507920265, 'accumulated_submission_time': 76501.53271389008, 'accumulated_eval_time': 7075.693947792053, 'accumulated_logging_time': 8.304190874099731}
I0202 17:44:39.542262 139923852027648 logging_writer.py:48] [165493] accumulated_eval_time=7075.693948, accumulated_logging_time=8.304191, accumulated_submission_time=76501.532714, global_step=165493, preemption_count=0, score=76501.532714, test/accuracy=0.630500, test/loss=1.569574, test/num_examples=10000, total_duration=83594.825079, train/accuracy=0.837422, train/loss=0.622592, validation/accuracy=0.754460, validation/loss=0.974749, validation/num_examples=50000
I0202 17:44:42.685018 139923868813056 logging_writer.py:48] [165500] global_step=165500, grad_norm=5.165279865264893, loss=2.6495344638824463
I0202 17:45:24.921288 139923852027648 logging_writer.py:48] [165600] global_step=165600, grad_norm=6.552411079406738, loss=3.7652196884155273
I0202 17:46:11.755531 139923868813056 logging_writer.py:48] [165700] global_step=165700, grad_norm=6.0760650634765625, loss=1.4906249046325684
I0202 17:46:58.626628 139923852027648 logging_writer.py:48] [165800] global_step=165800, grad_norm=5.221029758453369, loss=1.4358527660369873
I0202 17:47:45.320947 139923868813056 logging_writer.py:48] [165900] global_step=165900, grad_norm=5.818967342376709, loss=1.9488078355789185
I0202 17:48:32.098855 139923852027648 logging_writer.py:48] [166000] global_step=166000, grad_norm=5.292177200317383, loss=1.6176745891571045
I0202 17:49:18.762981 139923868813056 logging_writer.py:48] [166100] global_step=166100, grad_norm=5.43256139755249, loss=2.0321171283721924
I0202 17:50:05.587493 139923852027648 logging_writer.py:48] [166200] global_step=166200, grad_norm=6.200411319732666, loss=1.7413640022277832
I0202 17:50:52.238457 139923868813056 logging_writer.py:48] [166300] global_step=166300, grad_norm=5.573840618133545, loss=1.6479778289794922
I0202 17:51:39.314655 139923852027648 logging_writer.py:48] [166400] global_step=166400, grad_norm=5.197275161743164, loss=2.0376410484313965
I0202 17:51:39.972264 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:51:50.473224 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:52:20.819061 140085747812160 spec.py:349] Evaluating on the test split.
I0202 17:52:22.473006 140085747812160 submission_runner.py:408] Time since start: 84057.80s, 	Step: 166403, 	{'train/accuracy': 0.8303124904632568, 'train/loss': 0.6463891267776489, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 0.9667149782180786, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.5555449724197388, 'test/num_examples': 10000, 'score': 76921.90054345131, 'total_duration': 84057.80018854141, 'accumulated_submission_time': 76921.90054345131, 'accumulated_eval_time': 7118.194668292999, 'accumulated_logging_time': 8.359277486801147}
I0202 17:52:22.515081 139923868813056 logging_writer.py:48] [166403] accumulated_eval_time=7118.194668, accumulated_logging_time=8.359277, accumulated_submission_time=76921.900543, global_step=166403, preemption_count=0, score=76921.900543, test/accuracy=0.634900, test/loss=1.555545, test/num_examples=10000, total_duration=84057.800189, train/accuracy=0.830312, train/loss=0.646389, validation/accuracy=0.755640, validation/loss=0.966715, validation/num_examples=50000
I0202 17:53:03.365852 139923852027648 logging_writer.py:48] [166500] global_step=166500, grad_norm=5.813924789428711, loss=1.365457534790039
I0202 17:53:49.870283 139923868813056 logging_writer.py:48] [166600] global_step=166600, grad_norm=5.20200777053833, loss=2.874788284301758
I0202 17:54:36.847348 139923852027648 logging_writer.py:48] [166700] global_step=166700, grad_norm=5.4936676025390625, loss=1.8426344394683838
I0202 17:55:23.941343 139923868813056 logging_writer.py:48] [166800] global_step=166800, grad_norm=5.772555828094482, loss=1.4812406301498413
I0202 17:56:10.714291 139923852027648 logging_writer.py:48] [166900] global_step=166900, grad_norm=5.958669662475586, loss=3.422407627105713
I0202 17:56:57.869914 139923868813056 logging_writer.py:48] [167000] global_step=167000, grad_norm=6.308681964874268, loss=3.2181451320648193
I0202 17:57:44.907102 139923852027648 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.532712936401367, loss=1.4931565523147583
I0202 17:58:31.813383 139923868813056 logging_writer.py:48] [167200] global_step=167200, grad_norm=6.2978901863098145, loss=2.1704816818237305
I0202 17:59:18.549131 139923852027648 logging_writer.py:48] [167300] global_step=167300, grad_norm=5.52045202255249, loss=1.619339942932129
I0202 17:59:22.840404 140085747812160 spec.py:321] Evaluating on the training split.
I0202 17:59:33.135304 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 17:59:58.903281 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:00:00.542509 140085747812160 submission_runner.py:408] Time since start: 84515.87s, 	Step: 167311, 	{'train/accuracy': 0.8325781226158142, 'train/loss': 0.6565069556236267, 'validation/accuracy': 0.7567799687385559, 'validation/loss': 0.9810318350791931, 'validation/num_examples': 50000, 'test/accuracy': 0.6397000551223755, 'test/loss': 1.5751323699951172, 'test/num_examples': 10000, 'score': 77342.16409659386, 'total_duration': 84515.86970067024, 'accumulated_submission_time': 77342.16409659386, 'accumulated_eval_time': 7155.896743297577, 'accumulated_logging_time': 8.412142515182495}
I0202 18:00:00.584643 139923868813056 logging_writer.py:48] [167311] accumulated_eval_time=7155.896743, accumulated_logging_time=8.412143, accumulated_submission_time=77342.164097, global_step=167311, preemption_count=0, score=77342.164097, test/accuracy=0.639700, test/loss=1.575132, test/num_examples=10000, total_duration=84515.869701, train/accuracy=0.832578, train/loss=0.656507, validation/accuracy=0.756780, validation/loss=0.981032, validation/num_examples=50000
I0202 18:00:37.481228 139923852027648 logging_writer.py:48] [167400] global_step=167400, grad_norm=5.641220569610596, loss=3.171011447906494
I0202 18:01:24.298655 139923868813056 logging_writer.py:48] [167500] global_step=167500, grad_norm=6.36765718460083, loss=1.4503276348114014
I0202 18:02:11.369155 139923852027648 logging_writer.py:48] [167600] global_step=167600, grad_norm=5.8975911140441895, loss=1.4307128190994263
I0202 18:02:58.529170 139923868813056 logging_writer.py:48] [167700] global_step=167700, grad_norm=6.076535701751709, loss=3.2180261611938477
I0202 18:02:58.601521 139923852027648 logging_writer.py:48] [167701] global_step=167701, preemption_count=0, score=77520.092836
I0202 18:02:59.071214 140085747812160 checkpoints.py:490] Saving checkpoint at step: 167701
I0202 18:03:00.400813 140085747812160 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3/checkpoint_167701
I0202 18:03:00.420995 140085747812160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_3/checkpoint_167701.
I0202 18:03:01.598906 140085747812160 submission_runner.py:583] Tuning trial 3/5
I0202 18:03:01.599117 140085747812160 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0202 18:03:01.609471 140085747812160 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008398437057621777, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 35.492053747177124, 'total_duration': 62.411550998687744, 'accumulated_submission_time': 35.492053747177124, 'accumulated_eval_time': 26.919355869293213, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (857, {'train/accuracy': 0.012304686941206455, 'train/loss': 6.458219051361084, 'validation/accuracy': 0.0127399992197752, 'validation/loss': 6.469983100891113, 'validation/num_examples': 50000, 'test/accuracy': 0.009000000543892384, 'test/loss': 6.5088372230529785, 'test/num_examples': 10000, 'score': 455.7455909252167, 'total_duration': 519.2673494815826, 'accumulated_submission_time': 455.7455909252167, 'accumulated_eval_time': 63.456319093704224, 'accumulated_logging_time': 0.017473220825195312, 'global_step': 857, 'preemption_count': 0}), (1766, {'train/accuracy': 0.036210935562849045, 'train/loss': 5.856694221496582, 'validation/accuracy': 0.034940000623464584, 'validation/loss': 5.884265422821045, 'validation/num_examples': 50000, 'test/accuracy': 0.03189999982714653, 'test/loss': 5.995128154754639, 'test/num_examples': 10000, 'score': 875.765299320221, 'total_duration': 978.044549703598, 'accumulated_submission_time': 875.765299320221, 'accumulated_eval_time': 102.13130497932434, 'accumulated_logging_time': 0.047756195068359375, 'global_step': 1766, 'preemption_count': 0}), (2677, {'train/accuracy': 0.06332030892372131, 'train/loss': 5.436893939971924, 'validation/accuracy': 0.06027999892830849, 'validation/loss': 5.480632305145264, 'validation/num_examples': 50000, 'test/accuracy': 0.04780000075697899, 'test/loss': 5.662439346313477, 'test/num_examples': 10000, 'score': 1296.1715610027313, 'total_duration': 1437.1218678951263, 'accumulated_submission_time': 1296.1715610027313, 'accumulated_eval_time': 140.7256236076355, 'accumulated_logging_time': 0.07358026504516602, 'global_step': 2677, 'preemption_count': 0}), (3587, {'train/accuracy': 0.09263671934604645, 'train/loss': 5.093984603881836, 'validation/accuracy': 0.08709999918937683, 'validation/loss': 5.130605220794678, 'validation/num_examples': 50000, 'test/accuracy': 0.06840000301599503, 'test/loss': 5.372107982635498, 'test/num_examples': 10000, 'score': 1716.1118450164795, 'total_duration': 1895.8947789669037, 'accumulated_submission_time': 1716.1118450164795, 'accumulated_eval_time': 179.481369972229, 'accumulated_logging_time': 0.09964895248413086, 'global_step': 3587, 'preemption_count': 0}), (4496, {'train/accuracy': 0.12886717915534973, 'train/loss': 4.745182514190674, 'validation/accuracy': 0.12067999690771103, 'validation/loss': 4.8043293952941895, 'validation/num_examples': 50000, 'test/accuracy': 0.09400000423192978, 'test/loss': 5.104016304016113, 'test/num_examples': 10000, 'score': 2136.0499653816223, 'total_duration': 2354.783221721649, 'accumulated_submission_time': 2136.0499653816223, 'accumulated_eval_time': 218.35417985916138, 'accumulated_logging_time': 0.12586569786071777, 'global_step': 4496, 'preemption_count': 0}), (5405, {'train/accuracy': 0.16783203184604645, 'train/loss': 4.406538486480713, 'validation/accuracy': 0.15267999470233917, 'validation/loss': 4.4869513511657715, 'validation/num_examples': 50000, 'test/accuracy': 0.11730000376701355, 'test/loss': 4.828131675720215, 'test/num_examples': 10000, 'score': 2555.9860396385193, 'total_duration': 2814.194491147995, 'accumulated_submission_time': 2555.9860396385193, 'accumulated_eval_time': 257.7483706474304, 'accumulated_logging_time': 0.15589284896850586, 'global_step': 5405, 'preemption_count': 0}), (6314, {'train/accuracy': 0.2112695276737213, 'train/loss': 4.0574235916137695, 'validation/accuracy': 0.19563999772071838, 'validation/loss': 4.138259410858154, 'validation/num_examples': 50000, 'test/accuracy': 0.14390000700950623, 'test/loss': 4.525965213775635, 'test/num_examples': 10000, 'score': 2976.0876848697662, 'total_duration': 3268.1993370056152, 'accumulated_submission_time': 2976.0876848697662, 'accumulated_eval_time': 291.5754690170288, 'accumulated_logging_time': 0.18158912658691406, 'global_step': 6314, 'preemption_count': 0}), (7227, {'train/accuracy': 0.2490820288658142, 'train/loss': 3.757694959640503, 'validation/accuracy': 0.23157998919487, 'validation/loss': 3.856245756149292, 'validation/num_examples': 50000, 'test/accuracy': 0.1714000105857849, 'test/loss': 4.288461685180664, 'test/num_examples': 10000, 'score': 3396.280373096466, 'total_duration': 3726.625602722168, 'accumulated_submission_time': 3396.280373096466, 'accumulated_eval_time': 329.72999024391174, 'accumulated_logging_time': 0.2092578411102295, 'global_step': 7227, 'preemption_count': 0}), (8138, {'train/accuracy': 0.2967578172683716, 'train/loss': 3.435223340988159, 'validation/accuracy': 0.2703399956226349, 'validation/loss': 3.561870574951172, 'validation/num_examples': 50000, 'test/accuracy': 0.20630000531673431, 'test/loss': 4.033924579620361, 'test/num_examples': 10000, 'score': 3816.497978210449, 'total_duration': 4183.641686677933, 'accumulated_submission_time': 3816.497978210449, 'accumulated_eval_time': 366.4525589942932, 'accumulated_logging_time': 0.23423242568969727, 'global_step': 8138, 'preemption_count': 0}), (9048, {'train/accuracy': 0.34144529700279236, 'train/loss': 3.161966562271118, 'validation/accuracy': 0.30653998255729675, 'validation/loss': 3.330437660217285, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.8473939895629883, 'test/num_examples': 10000, 'score': 4236.516438961029, 'total_duration': 4642.813379764557, 'accumulated_submission_time': 4236.516438961029, 'accumulated_eval_time': 405.5273621082306, 'accumulated_logging_time': 0.2614433765411377, 'global_step': 9048, 'preemption_count': 0}), (9958, {'train/accuracy': 0.3555664122104645, 'train/loss': 3.0396194458007812, 'validation/accuracy': 0.33122000098228455, 'validation/loss': 3.1730129718780518, 'validation/num_examples': 50000, 'test/accuracy': 0.25140002369880676, 'test/loss': 3.7302799224853516, 'test/num_examples': 10000, 'score': 4656.703306913376, 'total_duration': 5099.041333913803, 'accumulated_submission_time': 4656.703306913376, 'accumulated_eval_time': 441.491233587265, 'accumulated_logging_time': 0.2879626750946045, 'global_step': 9958, 'preemption_count': 0}), (10870, {'train/accuracy': 0.3751562535762787, 'train/loss': 2.954991340637207, 'validation/accuracy': 0.34797999262809753, 'validation/loss': 3.092005491256714, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.626384973526001, 'test/num_examples': 10000, 'score': 5076.962865829468, 'total_duration': 5555.052897453308, 'accumulated_submission_time': 5076.962865829468, 'accumulated_eval_time': 477.1607825756073, 'accumulated_logging_time': 0.3193974494934082, 'global_step': 10870, 'preemption_count': 0}), (11782, {'train/accuracy': 0.4147265553474426, 'train/loss': 2.7238945960998535, 'validation/accuracy': 0.37109997868537903, 'validation/loss': 2.9334120750427246, 'validation/num_examples': 50000, 'test/accuracy': 0.28710001707077026, 'test/loss': 3.4888088703155518, 'test/num_examples': 10000, 'score': 5497.338998794556, 'total_duration': 6014.403256416321, 'accumulated_submission_time': 5497.338998794556, 'accumulated_eval_time': 516.0527064800262, 'accumulated_logging_time': 0.3488492965698242, 'global_step': 11782, 'preemption_count': 0}), (12691, {'train/accuracy': 0.4253320097923279, 'train/loss': 2.623037099838257, 'validation/accuracy': 0.3962000012397766, 'validation/loss': 2.782031536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.30320000648498535, 'test/loss': 3.3758649826049805, 'test/num_examples': 10000, 'score': 5917.761694431305, 'total_duration': 6470.8246874809265, 'accumulated_submission_time': 5917.761694431305, 'accumulated_eval_time': 551.9732129573822, 'accumulated_logging_time': 0.3765082359313965, 'global_step': 12691, 'preemption_count': 0}), (13601, {'train/accuracy': 0.43861326575279236, 'train/loss': 2.5734517574310303, 'validation/accuracy': 0.40821999311447144, 'validation/loss': 2.7287282943725586, 'validation/num_examples': 50000, 'test/accuracy': 0.3160000145435333, 'test/loss': 3.3127293586730957, 'test/num_examples': 10000, 'score': 6337.694573402405, 'total_duration': 6926.301305532455, 'accumulated_submission_time': 6337.694573402405, 'accumulated_eval_time': 587.4360723495483, 'accumulated_logging_time': 0.40647292137145996, 'global_step': 13601, 'preemption_count': 0}), (14512, {'train/accuracy': 0.46240234375, 'train/loss': 2.4503259658813477, 'validation/accuracy': 0.4207199811935425, 'validation/loss': 2.656090021133423, 'validation/num_examples': 50000, 'test/accuracy': 0.3279000222682953, 'test/loss': 3.24593186378479, 'test/num_examples': 10000, 'score': 6757.871794700623, 'total_duration': 7381.980210542679, 'accumulated_submission_time': 6757.871794700623, 'accumulated_eval_time': 622.858304977417, 'accumulated_logging_time': 0.43531012535095215, 'global_step': 14512, 'preemption_count': 0}), (15422, {'train/accuracy': 0.45710936188697815, 'train/loss': 2.456397771835327, 'validation/accuracy': 0.4252599775791168, 'validation/loss': 2.6171255111694336, 'validation/num_examples': 50000, 'test/accuracy': 0.32710000872612, 'test/loss': 3.216073989868164, 'test/num_examples': 10000, 'score': 7177.840661525726, 'total_duration': 7840.064656257629, 'accumulated_submission_time': 7177.840661525726, 'accumulated_eval_time': 660.8948771953583, 'accumulated_logging_time': 0.4631009101867676, 'global_step': 15422, 'preemption_count': 0}), (16333, {'train/accuracy': 0.4747656285762787, 'train/loss': 2.3626480102539062, 'validation/accuracy': 0.43915998935699463, 'validation/loss': 2.550366163253784, 'validation/num_examples': 50000, 'test/accuracy': 0.34140002727508545, 'test/loss': 3.161466598510742, 'test/num_examples': 10000, 'score': 7597.769626379013, 'total_duration': 8299.500366926193, 'accumulated_submission_time': 7597.769626379013, 'accumulated_eval_time': 700.3218250274658, 'accumulated_logging_time': 0.4916057586669922, 'global_step': 16333, 'preemption_count': 0}), (17241, {'train/accuracy': 0.4889843761920929, 'train/loss': 2.295726776123047, 'validation/accuracy': 0.4437599778175354, 'validation/loss': 2.507998466491699, 'validation/num_examples': 50000, 'test/accuracy': 0.344400018453598, 'test/loss': 3.1372451782226562, 'test/num_examples': 10000, 'score': 8018.110145807266, 'total_duration': 8758.87922167778, 'accumulated_submission_time': 8018.110145807266, 'accumulated_eval_time': 739.2799081802368, 'accumulated_logging_time': 0.520301103591919, 'global_step': 17241, 'preemption_count': 0}), (18151, {'train/accuracy': 0.48261716961860657, 'train/loss': 2.3039026260375977, 'validation/accuracy': 0.44777998328208923, 'validation/loss': 2.470780372619629, 'validation/num_examples': 50000, 'test/accuracy': 0.35030001401901245, 'test/loss': 3.0864689350128174, 'test/num_examples': 10000, 'score': 8438.437040090561, 'total_duration': 9221.031891822815, 'accumulated_submission_time': 8438.437040090561, 'accumulated_eval_time': 781.0245745182037, 'accumulated_logging_time': 0.5498020648956299, 'global_step': 18151, 'preemption_count': 0}), (19061, {'train/accuracy': 0.49947264790534973, 'train/loss': 2.22053599357605, 'validation/accuracy': 0.462039977312088, 'validation/loss': 2.4049911499023438, 'validation/num_examples': 50000, 'test/accuracy': 0.35850000381469727, 'test/loss': 3.022629976272583, 'test/num_examples': 10000, 'score': 8858.829084157944, 'total_duration': 9682.15875673294, 'accumulated_submission_time': 8858.829084157944, 'accumulated_eval_time': 821.6779868602753, 'accumulated_logging_time': 0.579658031463623, 'global_step': 19061, 'preemption_count': 0}), (19970, {'train/accuracy': 0.5083202719688416, 'train/loss': 2.1640000343322754, 'validation/accuracy': 0.4681599736213684, 'validation/loss': 2.3670108318328857, 'validation/num_examples': 50000, 'test/accuracy': 0.35830003023147583, 'test/loss': 3.0018560886383057, 'test/num_examples': 10000, 'score': 9278.822909593582, 'total_duration': 10140.743117809296, 'accumulated_submission_time': 9278.822909593582, 'accumulated_eval_time': 860.1880419254303, 'accumulated_logging_time': 0.6096100807189941, 'global_step': 19970, 'preemption_count': 0}), (20882, {'train/accuracy': 0.5099218487739563, 'train/loss': 2.1630897521972656, 'validation/accuracy': 0.4744599759578705, 'validation/loss': 2.3403162956237793, 'validation/num_examples': 50000, 'test/accuracy': 0.37070003151893616, 'test/loss': 2.948025941848755, 'test/num_examples': 10000, 'score': 9699.075863838196, 'total_duration': 10596.444150209427, 'accumulated_submission_time': 9699.075863838196, 'accumulated_eval_time': 895.5525405406952, 'accumulated_logging_time': 0.6403217315673828, 'global_step': 20882, 'preemption_count': 0}), (21793, {'train/accuracy': 0.5142382979393005, 'train/loss': 2.1343255043029785, 'validation/accuracy': 0.4792400002479553, 'validation/loss': 2.311788320541382, 'validation/num_examples': 50000, 'test/accuracy': 0.37470000982284546, 'test/loss': 2.934434413909912, 'test/num_examples': 10000, 'score': 10119.355597019196, 'total_duration': 11053.265462636948, 'accumulated_submission_time': 10119.355597019196, 'accumulated_eval_time': 932.0105512142181, 'accumulated_logging_time': 0.6730847358703613, 'global_step': 21793, 'preemption_count': 0}), (22703, {'train/accuracy': 0.5282812118530273, 'train/loss': 2.0714240074157715, 'validation/accuracy': 0.48555999994277954, 'validation/loss': 2.2779319286346436, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.8995566368103027, 'test/num_examples': 10000, 'score': 10539.649500846863, 'total_duration': 11509.611449241638, 'accumulated_submission_time': 10539.649500846863, 'accumulated_eval_time': 967.9775204658508, 'accumulated_logging_time': 0.7074997425079346, 'global_step': 22703, 'preemption_count': 0}), (23613, {'train/accuracy': 0.5564648509025574, 'train/loss': 1.9199002981185913, 'validation/accuracy': 0.4958999752998352, 'validation/loss': 2.209649085998535, 'validation/num_examples': 50000, 'test/accuracy': 0.3823000192642212, 'test/loss': 2.859225273132324, 'test/num_examples': 10000, 'score': 10959.790457248688, 'total_duration': 11969.157633304596, 'accumulated_submission_time': 10959.790457248688, 'accumulated_eval_time': 1007.3027441501617, 'accumulated_logging_time': 0.7358355522155762, 'global_step': 23613, 'preemption_count': 0}), (24523, {'train/accuracy': 0.5373241901397705, 'train/loss': 2.020864248275757, 'validation/accuracy': 0.5016999840736389, 'validation/loss': 2.1879594326019287, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8223278522491455, 'test/num_examples': 10000, 'score': 11379.97221159935, 'total_duration': 12429.073031425476, 'accumulated_submission_time': 11379.97221159935, 'accumulated_eval_time': 1046.9547855854034, 'accumulated_logging_time': 0.7669098377227783, 'global_step': 24523, 'preemption_count': 0}), (25434, {'train/accuracy': 0.5425586104393005, 'train/loss': 1.9881305694580078, 'validation/accuracy': 0.5017399787902832, 'validation/loss': 2.190019130706787, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.8353707790374756, 'test/num_examples': 10000, 'score': 11800.332841157913, 'total_duration': 12885.237624645233, 'accumulated_submission_time': 11800.332841157913, 'accumulated_eval_time': 1082.676115512848, 'accumulated_logging_time': 0.7968857288360596, 'global_step': 25434, 'preemption_count': 0}), (26343, {'train/accuracy': 0.573046863079071, 'train/loss': 1.8160487413406372, 'validation/accuracy': 0.5158599615097046, 'validation/loss': 2.1130800247192383, 'validation/num_examples': 50000, 'test/accuracy': 0.40790000557899475, 'test/loss': 2.7394766807556152, 'test/num_examples': 10000, 'score': 12220.630915403366, 'total_duration': 13341.120736837387, 'accumulated_submission_time': 12220.630915403366, 'accumulated_eval_time': 1118.181747674942, 'accumulated_logging_time': 0.8260171413421631, 'global_step': 26343, 'preemption_count': 0}), (27254, {'train/accuracy': 0.5526952743530273, 'train/loss': 1.9304035902023315, 'validation/accuracy': 0.5169399976730347, 'validation/loss': 2.108940362930298, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.7597310543060303, 'test/num_examples': 10000, 'score': 12640.863479614258, 'total_duration': 13797.866409778595, 'accumulated_submission_time': 12640.863479614258, 'accumulated_eval_time': 1154.6123263835907, 'accumulated_logging_time': 0.8571968078613281, 'global_step': 27254, 'preemption_count': 0}), (28164, {'train/accuracy': 0.5553905963897705, 'train/loss': 1.9481656551361084, 'validation/accuracy': 0.51528000831604, 'validation/loss': 2.139265298843384, 'validation/num_examples': 50000, 'test/accuracy': 0.4034000337123871, 'test/loss': 2.77888560295105, 'test/num_examples': 10000, 'score': 13061.040511369705, 'total_duration': 14252.119184017181, 'accumulated_submission_time': 13061.040511369705, 'accumulated_eval_time': 1188.6037278175354, 'accumulated_logging_time': 0.8905489444732666, 'global_step': 28164, 'preemption_count': 0}), (29074, {'train/accuracy': 0.576171875, 'train/loss': 1.8145115375518799, 'validation/accuracy': 0.5225200057029724, 'validation/loss': 2.0690627098083496, 'validation/num_examples': 50000, 'test/accuracy': 0.4146000146865845, 'test/loss': 2.6903810501098633, 'test/num_examples': 10000, 'score': 13481.007174015045, 'total_duration': 14706.609322786331, 'accumulated_submission_time': 13481.007174015045, 'accumulated_eval_time': 1223.0425362586975, 'accumulated_logging_time': 0.9243690967559814, 'global_step': 29074, 'preemption_count': 0}), (29985, {'train/accuracy': 0.5616992115974426, 'train/loss': 1.8721139430999756, 'validation/accuracy': 0.5232599973678589, 'validation/loss': 2.0561981201171875, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.7167224884033203, 'test/num_examples': 10000, 'score': 13901.04438996315, 'total_duration': 15160.329586267471, 'accumulated_submission_time': 13901.04438996315, 'accumulated_eval_time': 1256.6402442455292, 'accumulated_logging_time': 0.9586703777313232, 'global_step': 29985, 'preemption_count': 0}), (30896, {'train/accuracy': 0.5642382502555847, 'train/loss': 1.8705335855484009, 'validation/accuracy': 0.519819974899292, 'validation/loss': 2.0736615657806396, 'validation/num_examples': 50000, 'test/accuracy': 0.4124000072479248, 'test/loss': 2.723940849304199, 'test/num_examples': 10000, 'score': 14321.219009399414, 'total_duration': 15618.991045713425, 'accumulated_submission_time': 14321.219009399414, 'accumulated_eval_time': 1295.0405583381653, 'accumulated_logging_time': 0.9944887161254883, 'global_step': 30896, 'preemption_count': 0}), (31807, {'train/accuracy': 0.5814062356948853, 'train/loss': 1.8077421188354492, 'validation/accuracy': 0.5366799831390381, 'validation/loss': 2.019350290298462, 'validation/num_examples': 50000, 'test/accuracy': 0.4239000082015991, 'test/loss': 2.674529790878296, 'test/num_examples': 10000, 'score': 14741.544480085373, 'total_duration': 16073.455638170242, 'accumulated_submission_time': 14741.544480085373, 'accumulated_eval_time': 1329.0953433513641, 'accumulated_logging_time': 1.027569055557251, 'global_step': 31807, 'preemption_count': 0}), (32718, {'train/accuracy': 0.5716406106948853, 'train/loss': 1.8151863813400269, 'validation/accuracy': 0.5365599989891052, 'validation/loss': 1.983070731163025, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.6434519290924072, 'test/num_examples': 10000, 'score': 15161.686965703964, 'total_duration': 16532.786767959595, 'accumulated_submission_time': 15161.686965703964, 'accumulated_eval_time': 1368.2029874324799, 'accumulated_logging_time': 1.0579285621643066, 'global_step': 32718, 'preemption_count': 0}), (33628, {'train/accuracy': 0.5784569978713989, 'train/loss': 1.8230026960372925, 'validation/accuracy': 0.5387799739837646, 'validation/loss': 2.005711555480957, 'validation/num_examples': 50000, 'test/accuracy': 0.42920002341270447, 'test/loss': 2.636946439743042, 'test/num_examples': 10000, 'score': 15581.740196228027, 'total_duration': 16989.820979833603, 'accumulated_submission_time': 15581.740196228027, 'accumulated_eval_time': 1405.1028938293457, 'accumulated_logging_time': 1.087789535522461, 'global_step': 33628, 'preemption_count': 0}), (34539, {'train/accuracy': 0.5900781154632568, 'train/loss': 1.7298932075500488, 'validation/accuracy': 0.5426799654960632, 'validation/loss': 1.9463582038879395, 'validation/num_examples': 50000, 'test/accuracy': 0.42510002851486206, 'test/loss': 2.602484941482544, 'test/num_examples': 10000, 'score': 16001.988677024841, 'total_duration': 17444.537808418274, 'accumulated_submission_time': 16001.988677024841, 'accumulated_eval_time': 1439.4901642799377, 'accumulated_logging_time': 1.1178858280181885, 'global_step': 34539, 'preemption_count': 0}), (35449, {'train/accuracy': 0.5878710746765137, 'train/loss': 1.7523179054260254, 'validation/accuracy': 0.5478000044822693, 'validation/loss': 1.9458662271499634, 'validation/num_examples': 50000, 'test/accuracy': 0.43310001492500305, 'test/loss': 2.59553599357605, 'test/num_examples': 10000, 'score': 16422.16807770729, 'total_duration': 17901.24688887596, 'accumulated_submission_time': 16422.16807770729, 'accumulated_eval_time': 1475.937474489212, 'accumulated_logging_time': 1.149817943572998, 'global_step': 35449, 'preemption_count': 0}), (36359, {'train/accuracy': 0.5913280844688416, 'train/loss': 1.7497471570968628, 'validation/accuracy': 0.5470399856567383, 'validation/loss': 1.9463144540786743, 'validation/num_examples': 50000, 'test/accuracy': 0.4359000325202942, 'test/loss': 2.588318109512329, 'test/num_examples': 10000, 'score': 16842.540033340454, 'total_duration': 18359.965654611588, 'accumulated_submission_time': 16842.540033340454, 'accumulated_eval_time': 1514.2002153396606, 'accumulated_logging_time': 1.1828999519348145, 'global_step': 36359, 'preemption_count': 0}), (37270, {'train/accuracy': 0.5938476324081421, 'train/loss': 1.7227494716644287, 'validation/accuracy': 0.5518400073051453, 'validation/loss': 1.9233359098434448, 'validation/num_examples': 50000, 'test/accuracy': 0.43300002813339233, 'test/loss': 2.5716423988342285, 'test/num_examples': 10000, 'score': 17262.59283065796, 'total_duration': 18819.49461555481, 'accumulated_submission_time': 17262.59283065796, 'accumulated_eval_time': 1553.5912311077118, 'accumulated_logging_time': 1.2173235416412354, 'global_step': 37270, 'preemption_count': 0}), (38182, {'train/accuracy': 0.6218359470367432, 'train/loss': 1.614800214767456, 'validation/accuracy': 0.5530799627304077, 'validation/loss': 1.9338055849075317, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.5792202949523926, 'test/num_examples': 10000, 'score': 17682.696738004684, 'total_duration': 19273.491097211838, 'accumulated_submission_time': 17682.696738004684, 'accumulated_eval_time': 1587.3950009346008, 'accumulated_logging_time': 1.255021095275879, 'global_step': 38182, 'preemption_count': 0}), (39093, {'train/accuracy': 0.5911718606948853, 'train/loss': 1.776065468788147, 'validation/accuracy': 0.5468999743461609, 'validation/loss': 1.9785503149032593, 'validation/num_examples': 50000, 'test/accuracy': 0.4311000108718872, 'test/loss': 2.6091208457946777, 'test/num_examples': 10000, 'score': 18102.84414291382, 'total_duration': 19728.996298074722, 'accumulated_submission_time': 18102.84414291382, 'accumulated_eval_time': 1622.6679162979126, 'accumulated_logging_time': 1.2887091636657715, 'global_step': 39093, 'preemption_count': 0}), (40001, {'train/accuracy': 0.6050390601158142, 'train/loss': 1.6630072593688965, 'validation/accuracy': 0.5593199729919434, 'validation/loss': 1.883844017982483, 'validation/num_examples': 50000, 'test/accuracy': 0.43560001254081726, 'test/loss': 2.5480105876922607, 'test/num_examples': 10000, 'score': 18522.978043079376, 'total_duration': 20184.561008930206, 'accumulated_submission_time': 18522.978043079376, 'accumulated_eval_time': 1658.0126931667328, 'accumulated_logging_time': 1.3234169483184814, 'global_step': 40001, 'preemption_count': 0}), (40914, {'train/accuracy': 0.6123827695846558, 'train/loss': 1.6324337720870972, 'validation/accuracy': 0.5535199642181396, 'validation/loss': 1.9175604581832886, 'validation/num_examples': 50000, 'test/accuracy': 0.43890002369880676, 'test/loss': 2.5651254653930664, 'test/num_examples': 10000, 'score': 18943.064480304718, 'total_duration': 20642.259303569794, 'accumulated_submission_time': 18943.064480304718, 'accumulated_eval_time': 1695.5409202575684, 'accumulated_logging_time': 1.355790376663208, 'global_step': 40914, 'preemption_count': 0}), (41821, {'train/accuracy': 0.5993554592132568, 'train/loss': 1.6984341144561768, 'validation/accuracy': 0.5589599609375, 'validation/loss': 1.893854022026062, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.5530781745910645, 'test/num_examples': 10000, 'score': 19363.120608329773, 'total_duration': 21099.808168172836, 'accumulated_submission_time': 19363.120608329773, 'accumulated_eval_time': 1732.9484958648682, 'accumulated_logging_time': 1.3894102573394775, 'global_step': 41821, 'preemption_count': 0}), (42732, {'train/accuracy': 0.6033788919448853, 'train/loss': 1.6613901853561401, 'validation/accuracy': 0.5579400062561035, 'validation/loss': 1.870342493057251, 'validation/num_examples': 50000, 'test/accuracy': 0.4466000199317932, 'test/loss': 2.5125553607940674, 'test/num_examples': 10000, 'score': 19783.279280662537, 'total_duration': 21556.965598344803, 'accumulated_submission_time': 19783.279280662537, 'accumulated_eval_time': 1769.8629813194275, 'accumulated_logging_time': 1.42185640335083, 'global_step': 42732, 'preemption_count': 0}), (43642, {'train/accuracy': 0.6136132478713989, 'train/loss': 1.6449350118637085, 'validation/accuracy': 0.5623199939727783, 'validation/loss': 1.889671802520752, 'validation/num_examples': 50000, 'test/accuracy': 0.44210001826286316, 'test/loss': 2.542107105255127, 'test/num_examples': 10000, 'score': 20203.238243579865, 'total_duration': 22012.131967306137, 'accumulated_submission_time': 20203.238243579865, 'accumulated_eval_time': 1804.9868636131287, 'accumulated_logging_time': 1.4544131755828857, 'global_step': 43642, 'preemption_count': 0}), (44550, {'train/accuracy': 0.6031249761581421, 'train/loss': 1.6811929941177368, 'validation/accuracy': 0.5647599697113037, 'validation/loss': 1.8677431344985962, 'validation/num_examples': 50000, 'test/accuracy': 0.4440000355243683, 'test/loss': 2.530787944793701, 'test/num_examples': 10000, 'score': 20623.362012386322, 'total_duration': 22466.73867583275, 'accumulated_submission_time': 20623.362012386322, 'accumulated_eval_time': 1839.3811695575714, 'accumulated_logging_time': 1.4913108348846436, 'global_step': 44550, 'preemption_count': 0}), (45461, {'train/accuracy': 0.6108788847923279, 'train/loss': 1.6417521238327026, 'validation/accuracy': 0.567579984664917, 'validation/loss': 1.847375750541687, 'validation/num_examples': 50000, 'test/accuracy': 0.4482000172138214, 'test/loss': 2.4963107109069824, 'test/num_examples': 10000, 'score': 21043.66229391098, 'total_duration': 22921.24355506897, 'accumulated_submission_time': 21043.66229391098, 'accumulated_eval_time': 1873.5013628005981, 'accumulated_logging_time': 1.5242390632629395, 'global_step': 45461, 'preemption_count': 0}), (46372, {'train/accuracy': 0.6156249642372131, 'train/loss': 1.6239218711853027, 'validation/accuracy': 0.5699399709701538, 'validation/loss': 1.8485853672027588, 'validation/num_examples': 50000, 'test/accuracy': 0.44610002636909485, 'test/loss': 2.515526056289673, 'test/num_examples': 10000, 'score': 21463.673819065094, 'total_duration': 23379.889157772064, 'accumulated_submission_time': 21463.673819065094, 'accumulated_eval_time': 1912.0473115444183, 'accumulated_logging_time': 1.5611541271209717, 'global_step': 46372, 'preemption_count': 0}), (47284, {'train/accuracy': 0.6090039014816284, 'train/loss': 1.6575889587402344, 'validation/accuracy': 0.5688999891281128, 'validation/loss': 1.8510668277740479, 'validation/num_examples': 50000, 'test/accuracy': 0.44610002636909485, 'test/loss': 2.4953420162200928, 'test/num_examples': 10000, 'score': 21883.85037612915, 'total_duration': 23834.19975042343, 'accumulated_submission_time': 21883.85037612915, 'accumulated_eval_time': 1946.0973546504974, 'accumulated_logging_time': 1.593156099319458, 'global_step': 47284, 'preemption_count': 0}), (48197, {'train/accuracy': 0.611621081829071, 'train/loss': 1.6552009582519531, 'validation/accuracy': 0.5723199844360352, 'validation/loss': 1.836769938468933, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.478607177734375, 'test/num_examples': 10000, 'score': 22304.195620536804, 'total_duration': 24291.240940332413, 'accumulated_submission_time': 22304.195620536804, 'accumulated_eval_time': 1982.7054772377014, 'accumulated_logging_time': 1.6298718452453613, 'global_step': 48197, 'preemption_count': 0}), (49108, {'train/accuracy': 0.6163281202316284, 'train/loss': 1.6063055992126465, 'validation/accuracy': 0.5687999725341797, 'validation/loss': 1.8405550718307495, 'validation/num_examples': 50000, 'test/accuracy': 0.4492000341415405, 'test/loss': 2.4828150272369385, 'test/num_examples': 10000, 'score': 22724.199105739594, 'total_duration': 24752.58571910858, 'accumulated_submission_time': 22724.199105739594, 'accumulated_eval_time': 2023.9590499401093, 'accumulated_logging_time': 1.666623592376709, 'global_step': 49108, 'preemption_count': 0}), (50019, {'train/accuracy': 0.6085546612739563, 'train/loss': 1.6156573295593262, 'validation/accuracy': 0.5709399580955505, 'validation/loss': 1.8096331357955933, 'validation/num_examples': 50000, 'test/accuracy': 0.45420002937316895, 'test/loss': 2.4651927947998047, 'test/num_examples': 10000, 'score': 23144.242423772812, 'total_duration': 25207.855845689774, 'accumulated_submission_time': 23144.242423772812, 'accumulated_eval_time': 2059.097989797592, 'accumulated_logging_time': 1.7026152610778809, 'global_step': 50019, 'preemption_count': 0}), (50931, {'train/accuracy': 0.6145898103713989, 'train/loss': 1.6268950700759888, 'validation/accuracy': 0.5734800100326538, 'validation/loss': 1.8265553712844849, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.4841372966766357, 'test/num_examples': 10000, 'score': 23564.4644010067, 'total_duration': 25661.495080709457, 'accumulated_submission_time': 23564.4644010067, 'accumulated_eval_time': 2092.429440975189, 'accumulated_logging_time': 1.737779140472412, 'global_step': 50931, 'preemption_count': 0}), (51842, {'train/accuracy': 0.619140625, 'train/loss': 1.6129734516143799, 'validation/accuracy': 0.5745399594306946, 'validation/loss': 1.8239986896514893, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.465365409851074, 'test/num_examples': 10000, 'score': 23984.826218366623, 'total_duration': 26122.27624464035, 'accumulated_submission_time': 23984.826218366623, 'accumulated_eval_time': 2132.76446890831, 'accumulated_logging_time': 1.7713980674743652, 'global_step': 51842, 'preemption_count': 0}), (52752, {'train/accuracy': 0.6456249952316284, 'train/loss': 1.4840998649597168, 'validation/accuracy': 0.5798799991607666, 'validation/loss': 1.7855687141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4369454383850098, 'test/num_examples': 10000, 'score': 24404.973826408386, 'total_duration': 26583.041292905807, 'accumulated_submission_time': 24404.973826408386, 'accumulated_eval_time': 2173.295962333679, 'accumulated_logging_time': 1.8063812255859375, 'global_step': 52752, 'preemption_count': 0}), (53663, {'train/accuracy': 0.6150586009025574, 'train/loss': 1.6262010335922241, 'validation/accuracy': 0.5754199624061584, 'validation/loss': 1.8177956342697144, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.470672130584717, 'test/num_examples': 10000, 'score': 24825.06359386444, 'total_duration': 27037.99173426628, 'accumulated_submission_time': 24825.06359386444, 'accumulated_eval_time': 2208.069890022278, 'accumulated_logging_time': 1.8420429229736328, 'global_step': 53663, 'preemption_count': 0}), (54573, {'train/accuracy': 0.6263281106948853, 'train/loss': 1.5781675577163696, 'validation/accuracy': 0.5829600095748901, 'validation/loss': 1.770058512687683, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.430996894836426, 'test/num_examples': 10000, 'score': 25245.0795879364, 'total_duration': 27497.753811597824, 'accumulated_submission_time': 25245.0795879364, 'accumulated_eval_time': 2247.7225930690765, 'accumulated_logging_time': 1.88374662399292, 'global_step': 54573, 'preemption_count': 0}), (55484, {'train/accuracy': 0.6351562142372131, 'train/loss': 1.5078805685043335, 'validation/accuracy': 0.5780400037765503, 'validation/loss': 1.778692603111267, 'validation/num_examples': 50000, 'test/accuracy': 0.4611000120639801, 'test/loss': 2.4479312896728516, 'test/num_examples': 10000, 'score': 25665.398136615753, 'total_duration': 27958.751446008682, 'accumulated_submission_time': 25665.398136615753, 'accumulated_eval_time': 2288.314208507538, 'accumulated_logging_time': 1.9204604625701904, 'global_step': 55484, 'preemption_count': 0}), (56394, {'train/accuracy': 0.6251562237739563, 'train/loss': 1.5547682046890259, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 1.764073371887207, 'validation/num_examples': 50000, 'test/accuracy': 0.4609000086784363, 'test/loss': 2.4148685932159424, 'test/num_examples': 10000, 'score': 26085.56778717041, 'total_duration': 28418.657384634018, 'accumulated_submission_time': 26085.56778717041, 'accumulated_eval_time': 2327.961247444153, 'accumulated_logging_time': 1.9579541683197021, 'global_step': 56394, 'preemption_count': 0}), (57304, {'train/accuracy': 0.6250194907188416, 'train/loss': 1.5971978902816772, 'validation/accuracy': 0.5831599831581116, 'validation/loss': 1.794812798500061, 'validation/num_examples': 50000, 'test/accuracy': 0.46240001916885376, 'test/loss': 2.4551639556884766, 'test/num_examples': 10000, 'score': 26505.91107916832, 'total_duration': 28875.283742427826, 'accumulated_submission_time': 26505.91107916832, 'accumulated_eval_time': 2364.1540179252625, 'accumulated_logging_time': 1.9971649646759033, 'global_step': 57304, 'preemption_count': 0}), (58212, {'train/accuracy': 0.6395898461341858, 'train/loss': 1.5047837495803833, 'validation/accuracy': 0.5877599716186523, 'validation/loss': 1.7553316354751587, 'validation/num_examples': 50000, 'test/accuracy': 0.4683000147342682, 'test/loss': 2.4041175842285156, 'test/num_examples': 10000, 'score': 26926.0757522583, 'total_duration': 29336.025983572006, 'accumulated_submission_time': 26926.0757522583, 'accumulated_eval_time': 2404.646691799164, 'accumulated_logging_time': 2.0316531658172607, 'global_step': 58212, 'preemption_count': 0}), (59122, {'train/accuracy': 0.6267773509025574, 'train/loss': 1.5606385469436646, 'validation/accuracy': 0.5853599905967712, 'validation/loss': 1.7512341737747192, 'validation/num_examples': 50000, 'test/accuracy': 0.4666000306606293, 'test/loss': 2.407226085662842, 'test/num_examples': 10000, 'score': 27346.409598112106, 'total_duration': 29798.761336803436, 'accumulated_submission_time': 27346.409598112106, 'accumulated_eval_time': 2446.9639501571655, 'accumulated_logging_time': 2.0650646686553955, 'global_step': 59122, 'preemption_count': 0}), (60032, {'train/accuracy': 0.6316601634025574, 'train/loss': 1.5331214666366577, 'validation/accuracy': 0.5899999737739563, 'validation/loss': 1.7361007928848267, 'validation/num_examples': 50000, 'test/accuracy': 0.47520002722740173, 'test/loss': 2.382453680038452, 'test/num_examples': 10000, 'score': 27766.347306251526, 'total_duration': 30256.77499818802, 'accumulated_submission_time': 27766.347306251526, 'accumulated_eval_time': 2484.952459335327, 'accumulated_logging_time': 2.1022024154663086, 'global_step': 60032, 'preemption_count': 0}), (60941, {'train/accuracy': 0.6475585699081421, 'train/loss': 1.4857375621795654, 'validation/accuracy': 0.5921599864959717, 'validation/loss': 1.733931541442871, 'validation/num_examples': 50000, 'test/accuracy': 0.4711000323295593, 'test/loss': 2.3912408351898193, 'test/num_examples': 10000, 'score': 28186.30168414116, 'total_duration': 30716.703328847885, 'accumulated_submission_time': 28186.30168414116, 'accumulated_eval_time': 2524.839050769806, 'accumulated_logging_time': 2.1383426189422607, 'global_step': 60941, 'preemption_count': 0}), (61851, {'train/accuracy': 0.6359570026397705, 'train/loss': 1.5239770412445068, 'validation/accuracy': 0.5936599969863892, 'validation/loss': 1.7153202295303345, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.3749277591705322, 'test/num_examples': 10000, 'score': 28606.5775911808, 'total_duration': 31177.174296855927, 'accumulated_submission_time': 28606.5775911808, 'accumulated_eval_time': 2564.9455330371857, 'accumulated_logging_time': 2.1757090091705322, 'global_step': 61851, 'preemption_count': 0}), (62761, {'train/accuracy': 0.6364062428474426, 'train/loss': 1.5226380825042725, 'validation/accuracy': 0.592960000038147, 'validation/loss': 1.7290226221084595, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.3883395195007324, 'test/num_examples': 10000, 'score': 29026.926725625992, 'total_duration': 31640.543329954147, 'accumulated_submission_time': 29026.926725625992, 'accumulated_eval_time': 2607.8761575222015, 'accumulated_logging_time': 2.214055299758911, 'global_step': 62761, 'preemption_count': 0}), (63670, {'train/accuracy': 0.6468554735183716, 'train/loss': 1.4551808834075928, 'validation/accuracy': 0.5985000133514404, 'validation/loss': 1.690281867980957, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.3399128913879395, 'test/num_examples': 10000, 'score': 29447.01801109314, 'total_duration': 32096.44493150711, 'accumulated_submission_time': 29447.01801109314, 'accumulated_eval_time': 2643.5948944091797, 'accumulated_logging_time': 2.2544357776641846, 'global_step': 63670, 'preemption_count': 0}), (64580, {'train/accuracy': 0.6409569978713989, 'train/loss': 1.5243161916732788, 'validation/accuracy': 0.5908799767494202, 'validation/loss': 1.7485175132751465, 'validation/num_examples': 50000, 'test/accuracy': 0.47040003538131714, 'test/loss': 2.3861141204833984, 'test/num_examples': 10000, 'score': 29867.20395731926, 'total_duration': 32556.95909023285, 'accumulated_submission_time': 29867.20395731926, 'accumulated_eval_time': 2683.8333218097687, 'accumulated_logging_time': 2.2939212322235107, 'global_step': 64580, 'preemption_count': 0}), (65489, {'train/accuracy': 0.63232421875, 'train/loss': 1.5296992063522339, 'validation/accuracy': 0.5950599908828735, 'validation/loss': 1.7079485654830933, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.3629984855651855, 'test/num_examples': 10000, 'score': 30287.157656669617, 'total_duration': 33017.315880060196, 'accumulated_submission_time': 30287.157656669617, 'accumulated_eval_time': 2724.1434757709503, 'accumulated_logging_time': 2.335206985473633, 'global_step': 65489, 'preemption_count': 0}), (66399, {'train/accuracy': 0.6452734470367432, 'train/loss': 1.4815714359283447, 'validation/accuracy': 0.5956999659538269, 'validation/loss': 1.7074819803237915, 'validation/num_examples': 50000, 'test/accuracy': 0.4799000322818756, 'test/loss': 2.360117197036743, 'test/num_examples': 10000, 'score': 30707.103160381317, 'total_duration': 33475.26572585106, 'accumulated_submission_time': 30707.103160381317, 'accumulated_eval_time': 2762.0575156211853, 'accumulated_logging_time': 2.3746211528778076, 'global_step': 66399, 'preemption_count': 0}), (67309, {'train/accuracy': 0.6687890291213989, 'train/loss': 1.3784257173538208, 'validation/accuracy': 0.5956799983978271, 'validation/loss': 1.7057280540466309, 'validation/num_examples': 50000, 'test/accuracy': 0.47280001640319824, 'test/loss': 2.3525753021240234, 'test/num_examples': 10000, 'score': 31127.178329706192, 'total_duration': 33937.002141714096, 'accumulated_submission_time': 31127.178329706192, 'accumulated_eval_time': 2803.6267223358154, 'accumulated_logging_time': 2.4154183864593506, 'global_step': 67309, 'preemption_count': 0}), (68219, {'train/accuracy': 0.6403710842132568, 'train/loss': 1.5155787467956543, 'validation/accuracy': 0.5939199924468994, 'validation/loss': 1.7145018577575684, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.364424467086792, 'test/num_examples': 10000, 'score': 31547.497469186783, 'total_duration': 34399.2928917408, 'accumulated_submission_time': 31547.497469186783, 'accumulated_eval_time': 2845.501267194748, 'accumulated_logging_time': 2.461395263671875, 'global_step': 68219, 'preemption_count': 0}), (69130, {'train/accuracy': 0.6488866806030273, 'train/loss': 1.4620481729507446, 'validation/accuracy': 0.6020599603652954, 'validation/loss': 1.6721429824829102, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.311363458633423, 'test/num_examples': 10000, 'score': 31967.57464170456, 'total_duration': 34859.705701351166, 'accumulated_submission_time': 31967.57464170456, 'accumulated_eval_time': 2885.74742937088, 'accumulated_logging_time': 2.4994466304779053, 'global_step': 69130, 'preemption_count': 0}), (70042, {'train/accuracy': 0.6694726347923279, 'train/loss': 1.3626357316970825, 'validation/accuracy': 0.6027399897575378, 'validation/loss': 1.665413737297058, 'validation/num_examples': 50000, 'test/accuracy': 0.4807000160217285, 'test/loss': 2.3237833976745605, 'test/num_examples': 10000, 'score': 32387.686763048172, 'total_duration': 35320.11451506615, 'accumulated_submission_time': 32387.686763048172, 'accumulated_eval_time': 2925.9560775756836, 'accumulated_logging_time': 2.5358903408050537, 'global_step': 70042, 'preemption_count': 0}), (70951, {'train/accuracy': 0.6404492259025574, 'train/loss': 1.498771071434021, 'validation/accuracy': 0.6004399657249451, 'validation/loss': 1.6853318214416504, 'validation/num_examples': 50000, 'test/accuracy': 0.48360002040863037, 'test/loss': 2.3453047275543213, 'test/num_examples': 10000, 'score': 32807.90108847618, 'total_duration': 35780.31765007973, 'accumulated_submission_time': 32807.90108847618, 'accumulated_eval_time': 2965.8574674129486, 'accumulated_logging_time': 2.5720467567443848, 'global_step': 70951, 'preemption_count': 0}), (71860, {'train/accuracy': 0.6525781154632568, 'train/loss': 1.4528508186340332, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.674795389175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.3266289234161377, 'test/num_examples': 10000, 'score': 33228.07022809982, 'total_duration': 36241.46924185753, 'accumulated_submission_time': 33228.07022809982, 'accumulated_eval_time': 3006.7480852603912, 'accumulated_logging_time': 2.6135470867156982, 'global_step': 71860, 'preemption_count': 0}), (72770, {'train/accuracy': 0.6627148389816284, 'train/loss': 1.4005804061889648, 'validation/accuracy': 0.6051599979400635, 'validation/loss': 1.6614387035369873, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.316112756729126, 'test/num_examples': 10000, 'score': 33648.16423654556, 'total_duration': 36698.63916397095, 'accumulated_submission_time': 33648.16423654556, 'accumulated_eval_time': 3043.734041452408, 'accumulated_logging_time': 2.652384042739868, 'global_step': 72770, 'preemption_count': 0}), (73682, {'train/accuracy': 0.6526562571525574, 'train/loss': 1.441790223121643, 'validation/accuracy': 0.6084200143814087, 'validation/loss': 1.6414271593093872, 'validation/num_examples': 50000, 'test/accuracy': 0.4877000153064728, 'test/loss': 2.2946367263793945, 'test/num_examples': 10000, 'score': 34068.218707084656, 'total_duration': 37159.420274972916, 'accumulated_submission_time': 34068.218707084656, 'accumulated_eval_time': 3084.372179746628, 'accumulated_logging_time': 2.68951416015625, 'global_step': 73682, 'preemption_count': 0}), (74591, {'train/accuracy': 0.6527929306030273, 'train/loss': 1.446118950843811, 'validation/accuracy': 0.6092000007629395, 'validation/loss': 1.6501826047897339, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.3193719387054443, 'test/num_examples': 10000, 'score': 34488.36898994446, 'total_duration': 37617.76039338112, 'accumulated_submission_time': 34488.36898994446, 'accumulated_eval_time': 3122.4718701839447, 'accumulated_logging_time': 2.7291154861450195, 'global_step': 74591, 'preemption_count': 0}), (75503, {'train/accuracy': 0.6614648103713989, 'train/loss': 1.3912386894226074, 'validation/accuracy': 0.606719970703125, 'validation/loss': 1.6329096555709839, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2832729816436768, 'test/num_examples': 10000, 'score': 34908.69756317139, 'total_duration': 38077.84191131592, 'accumulated_submission_time': 34908.69756317139, 'accumulated_eval_time': 3162.1323087215424, 'accumulated_logging_time': 2.7706854343414307, 'global_step': 75503, 'preemption_count': 0}), (76412, {'train/accuracy': 0.6544336080551147, 'train/loss': 1.4055346250534058, 'validation/accuracy': 0.6148200035095215, 'validation/loss': 1.6018518209457397, 'validation/num_examples': 50000, 'test/accuracy': 0.4888000190258026, 'test/loss': 2.2715227603912354, 'test/num_examples': 10000, 'score': 35328.6748585701, 'total_duration': 38533.69681978226, 'accumulated_submission_time': 35328.6748585701, 'accumulated_eval_time': 3197.922151327133, 'accumulated_logging_time': 2.8070130348205566, 'global_step': 76412, 'preemption_count': 0}), (77323, {'train/accuracy': 0.6539453268051147, 'train/loss': 1.4418593645095825, 'validation/accuracy': 0.6126799583435059, 'validation/loss': 1.63576340675354, 'validation/num_examples': 50000, 'test/accuracy': 0.48580002784729004, 'test/loss': 2.295750617980957, 'test/num_examples': 10000, 'score': 35748.62918996811, 'total_duration': 38993.28647398949, 'accumulated_submission_time': 35748.62918996811, 'accumulated_eval_time': 3237.468738079071, 'accumulated_logging_time': 2.8454105854034424, 'global_step': 77323, 'preemption_count': 0}), (78235, {'train/accuracy': 0.665722668170929, 'train/loss': 1.3743427991867065, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.6258063316345215, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.2690629959106445, 'test/num_examples': 10000, 'score': 36168.83649253845, 'total_duration': 39449.87701368332, 'accumulated_submission_time': 36168.83649253845, 'accumulated_eval_time': 3273.759335756302, 'accumulated_logging_time': 2.88728928565979, 'global_step': 78235, 'preemption_count': 0}), (79146, {'train/accuracy': 0.6544336080551147, 'train/loss': 1.4474139213562012, 'validation/accuracy': 0.6101999878883362, 'validation/loss': 1.6504663228988647, 'validation/num_examples': 50000, 'test/accuracy': 0.4942000210285187, 'test/loss': 2.2950570583343506, 'test/num_examples': 10000, 'score': 36589.174001932144, 'total_duration': 39909.596652030945, 'accumulated_submission_time': 36589.174001932144, 'accumulated_eval_time': 3313.0534660816193, 'accumulated_logging_time': 2.9241793155670166, 'global_step': 79146, 'preemption_count': 0}), (80055, {'train/accuracy': 0.6633593440055847, 'train/loss': 1.3997061252593994, 'validation/accuracy': 0.6201199889183044, 'validation/loss': 1.590294361114502, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.22688364982605, 'test/num_examples': 10000, 'score': 37009.086544275284, 'total_duration': 40366.136556625366, 'accumulated_submission_time': 37009.086544275284, 'accumulated_eval_time': 3349.5856976509094, 'accumulated_logging_time': 2.9685537815093994, 'global_step': 80055, 'preemption_count': 0}), (80968, {'train/accuracy': 0.6633203029632568, 'train/loss': 1.383533239364624, 'validation/accuracy': 0.6125400066375732, 'validation/loss': 1.6144942045211792, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.279834032058716, 'test/num_examples': 10000, 'score': 37429.40017914772, 'total_duration': 40821.89309167862, 'accumulated_submission_time': 37429.40017914772, 'accumulated_eval_time': 3384.9342653751373, 'accumulated_logging_time': 3.0103461742401123, 'global_step': 80968, 'preemption_count': 0}), (81878, {'train/accuracy': 0.6926171779632568, 'train/loss': 1.2652881145477295, 'validation/accuracy': 0.6225599646568298, 'validation/loss': 1.5752804279327393, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2313928604125977, 'test/num_examples': 10000, 'score': 37849.333032131195, 'total_duration': 41281.0299179554, 'accumulated_submission_time': 37849.333032131195, 'accumulated_eval_time': 3424.0457706451416, 'accumulated_logging_time': 3.0522875785827637, 'global_step': 81878, 'preemption_count': 0}), (82788, {'train/accuracy': 0.6661523580551147, 'train/loss': 1.387743592262268, 'validation/accuracy': 0.6181600093841553, 'validation/loss': 1.5939165353775024, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.2245521545410156, 'test/num_examples': 10000, 'score': 38269.31256151199, 'total_duration': 41738.09440588951, 'accumulated_submission_time': 38269.31256151199, 'accumulated_eval_time': 3461.0371272563934, 'accumulated_logging_time': 3.0954391956329346, 'global_step': 82788, 'preemption_count': 0}), (83698, {'train/accuracy': 0.6722851395606995, 'train/loss': 1.3354723453521729, 'validation/accuracy': 0.6222999691963196, 'validation/loss': 1.5551286935806274, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.2071619033813477, 'test/num_examples': 10000, 'score': 38689.46293449402, 'total_duration': 42197.38641667366, 'accumulated_submission_time': 38689.46293449402, 'accumulated_eval_time': 3500.0861682891846, 'accumulated_logging_time': 3.1364245414733887, 'global_step': 83698, 'preemption_count': 0}), (84609, {'train/accuracy': 0.6911913752555847, 'train/loss': 1.2636808156967163, 'validation/accuracy': 0.6240800023078918, 'validation/loss': 1.56609046459198, 'validation/num_examples': 50000, 'test/accuracy': 0.5009000301361084, 'test/loss': 2.2192747592926025, 'test/num_examples': 10000, 'score': 39109.49723100662, 'total_duration': 42656.90248131752, 'accumulated_submission_time': 39109.49723100662, 'accumulated_eval_time': 3539.4701313972473, 'accumulated_logging_time': 3.183518648147583, 'global_step': 84609, 'preemption_count': 0}), (85519, {'train/accuracy': 0.6674023270606995, 'train/loss': 1.393503189086914, 'validation/accuracy': 0.6214599609375, 'validation/loss': 1.5956655740737915, 'validation/num_examples': 50000, 'test/accuracy': 0.5010000467300415, 'test/loss': 2.2568812370300293, 'test/num_examples': 10000, 'score': 39529.7694542408, 'total_duration': 43119.98700237274, 'accumulated_submission_time': 39529.7694542408, 'accumulated_eval_time': 3582.189652442932, 'accumulated_logging_time': 3.224731683731079, 'global_step': 85519, 'preemption_count': 0}), (86427, {'train/accuracy': 0.6750780940055847, 'train/loss': 1.347376823425293, 'validation/accuracy': 0.6284599900245667, 'validation/loss': 1.5590183734893799, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.208587408065796, 'test/num_examples': 10000, 'score': 39950.09155750275, 'total_duration': 43578.21590304375, 'accumulated_submission_time': 39950.09155750275, 'accumulated_eval_time': 3620.007707118988, 'accumulated_logging_time': 3.262495994567871, 'global_step': 86427, 'preemption_count': 0}), (87338, {'train/accuracy': 0.6869726181030273, 'train/loss': 1.2832924127578735, 'validation/accuracy': 0.6260600090026855, 'validation/loss': 1.5575437545776367, 'validation/num_examples': 50000, 'test/accuracy': 0.503000020980835, 'test/loss': 2.1890664100646973, 'test/num_examples': 10000, 'score': 40370.41019010544, 'total_duration': 44038.112585783005, 'accumulated_submission_time': 40370.41019010544, 'accumulated_eval_time': 3659.488579750061, 'accumulated_logging_time': 3.3086326122283936, 'global_step': 87338, 'preemption_count': 0}), (88245, {'train/accuracy': 0.6767382621765137, 'train/loss': 1.3393456935882568, 'validation/accuracy': 0.6292399764060974, 'validation/loss': 1.5438076257705688, 'validation/num_examples': 50000, 'test/accuracy': 0.5083000063896179, 'test/loss': 2.1848161220550537, 'test/num_examples': 10000, 'score': 40790.58680820465, 'total_duration': 44496.99372458458, 'accumulated_submission_time': 40790.58680820465, 'accumulated_eval_time': 3698.1002700328827, 'accumulated_logging_time': 3.3508591651916504, 'global_step': 88245, 'preemption_count': 0}), (89155, {'train/accuracy': 0.6765820384025574, 'train/loss': 1.3568689823150635, 'validation/accuracy': 0.6251199841499329, 'validation/loss': 1.592879056930542, 'validation/num_examples': 50000, 'test/accuracy': 0.4978000223636627, 'test/loss': 2.2331268787384033, 'test/num_examples': 10000, 'score': 41210.69068980217, 'total_duration': 44953.37469482422, 'accumulated_submission_time': 41210.69068980217, 'accumulated_eval_time': 3734.2811844348907, 'accumulated_logging_time': 3.3964245319366455, 'global_step': 89155, 'preemption_count': 0}), (90066, {'train/accuracy': 0.6912695169448853, 'train/loss': 1.2604010105133057, 'validation/accuracy': 0.6340000033378601, 'validation/loss': 1.5193389654159546, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.1705994606018066, 'test/num_examples': 10000, 'score': 41631.16453623772, 'total_duration': 45414.1202609539, 'accumulated_submission_time': 41631.16453623772, 'accumulated_eval_time': 3774.4601967334747, 'accumulated_logging_time': 3.4379467964172363, 'global_step': 90066, 'preemption_count': 0}), (90976, {'train/accuracy': 0.6769921779632568, 'train/loss': 1.3207937479019165, 'validation/accuracy': 0.6307799816131592, 'validation/loss': 1.5352376699447632, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.1914098262786865, 'test/num_examples': 10000, 'score': 42051.34705209732, 'total_duration': 45868.677743434906, 'accumulated_submission_time': 42051.34705209732, 'accumulated_eval_time': 3808.74009847641, 'accumulated_logging_time': 3.4811456203460693, 'global_step': 90976, 'preemption_count': 0}), (91885, {'train/accuracy': 0.6859374642372131, 'train/loss': 1.277827262878418, 'validation/accuracy': 0.6394599676132202, 'validation/loss': 1.4927690029144287, 'validation/num_examples': 50000, 'test/accuracy': 0.5106000304222107, 'test/loss': 2.1332180500030518, 'test/num_examples': 10000, 'score': 42471.574355363846, 'total_duration': 46325.46682262421, 'accumulated_submission_time': 42471.574355363846, 'accumulated_eval_time': 3845.2059903144836, 'accumulated_logging_time': 3.5260469913482666, 'global_step': 91885, 'preemption_count': 0}), (92793, {'train/accuracy': 0.6937304735183716, 'train/loss': 1.268678903579712, 'validation/accuracy': 0.6373999714851379, 'validation/loss': 1.5291402339935303, 'validation/num_examples': 50000, 'test/accuracy': 0.5193000435829163, 'test/loss': 2.167919874191284, 'test/num_examples': 10000, 'score': 42891.8378636837, 'total_duration': 46784.998166799545, 'accumulated_submission_time': 42891.8378636837, 'accumulated_eval_time': 3884.377197265625, 'accumulated_logging_time': 3.5714974403381348, 'global_step': 92793, 'preemption_count': 0}), (93700, {'train/accuracy': 0.684374988079071, 'train/loss': 1.302620768547058, 'validation/accuracy': 0.6352800130844116, 'validation/loss': 1.5236302614212036, 'validation/num_examples': 50000, 'test/accuracy': 0.5074000358581543, 'test/loss': 2.174853563308716, 'test/num_examples': 10000, 'score': 43311.91257548332, 'total_duration': 47240.98172211647, 'accumulated_submission_time': 43311.91257548332, 'accumulated_eval_time': 3920.189643383026, 'accumulated_logging_time': 3.617255449295044, 'global_step': 93700, 'preemption_count': 0}), (94609, {'train/accuracy': 0.6906640529632568, 'train/loss': 1.2588465213775635, 'validation/accuracy': 0.6386399865150452, 'validation/loss': 1.485780119895935, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.115467071533203, 'test/num_examples': 10000, 'score': 43732.20620751381, 'total_duration': 47700.05508303642, 'accumulated_submission_time': 43732.20620751381, 'accumulated_eval_time': 3958.876034975052, 'accumulated_logging_time': 3.6599841117858887, 'global_step': 94609, 'preemption_count': 0}), (95519, {'train/accuracy': 0.6943945288658142, 'train/loss': 1.260308861732483, 'validation/accuracy': 0.6425999999046326, 'validation/loss': 1.499654769897461, 'validation/num_examples': 50000, 'test/accuracy': 0.516800045967102, 'test/loss': 2.149538278579712, 'test/num_examples': 10000, 'score': 44152.353734731674, 'total_duration': 48157.12686371803, 'accumulated_submission_time': 44152.353734731674, 'accumulated_eval_time': 3995.7055275440216, 'accumulated_logging_time': 3.703071117401123, 'global_step': 95519, 'preemption_count': 0}), (96429, {'train/accuracy': 0.7122656106948853, 'train/loss': 1.1745492219924927, 'validation/accuracy': 0.6404199600219727, 'validation/loss': 1.485430121421814, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.13046932220459, 'test/num_examples': 10000, 'score': 44572.31914424896, 'total_duration': 48614.920499801636, 'accumulated_submission_time': 44572.31914424896, 'accumulated_eval_time': 4033.443476676941, 'accumulated_logging_time': 3.7422258853912354, 'global_step': 96429, 'preemption_count': 0}), (97339, {'train/accuracy': 0.6898632645606995, 'train/loss': 1.2658751010894775, 'validation/accuracy': 0.6431399583816528, 'validation/loss': 1.4787806272506714, 'validation/num_examples': 50000, 'test/accuracy': 0.5144000053405762, 'test/loss': 2.1275625228881836, 'test/num_examples': 10000, 'score': 44992.565873384476, 'total_duration': 49074.64042210579, 'accumulated_submission_time': 44992.565873384476, 'accumulated_eval_time': 4072.8260283470154, 'accumulated_logging_time': 3.7807199954986572, 'global_step': 97339, 'preemption_count': 0}), (98248, {'train/accuracy': 0.6936132907867432, 'train/loss': 1.285739541053772, 'validation/accuracy': 0.6411199569702148, 'validation/loss': 1.5151735544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.1551709175109863, 'test/num_examples': 10000, 'score': 45412.90539312363, 'total_duration': 49531.960334300995, 'accumulated_submission_time': 45412.90539312363, 'accumulated_eval_time': 4109.709174156189, 'accumulated_logging_time': 3.8271048069000244, 'global_step': 98248, 'preemption_count': 0}), (99158, {'train/accuracy': 0.7100781202316284, 'train/loss': 1.1822881698608398, 'validation/accuracy': 0.6423599720001221, 'validation/loss': 1.4824674129486084, 'validation/num_examples': 50000, 'test/accuracy': 0.5199000239372253, 'test/loss': 2.1205554008483887, 'test/num_examples': 10000, 'score': 45832.94949889183, 'total_duration': 49992.294642686844, 'accumulated_submission_time': 45832.94949889183, 'accumulated_eval_time': 4149.903354167938, 'accumulated_logging_time': 3.872529983520508, 'global_step': 99158, 'preemption_count': 0}), (100068, {'train/accuracy': 0.69677734375, 'train/loss': 1.2403640747070312, 'validation/accuracy': 0.6484400033950806, 'validation/loss': 1.4505292177200317, 'validation/num_examples': 50000, 'test/accuracy': 0.5268000364303589, 'test/loss': 2.0834946632385254, 'test/num_examples': 10000, 'score': 46253.277426958084, 'total_duration': 50451.75859832764, 'accumulated_submission_time': 46253.277426958084, 'accumulated_eval_time': 4188.944055318832, 'accumulated_logging_time': 3.9171104431152344, 'global_step': 100068, 'preemption_count': 0}), (100978, {'train/accuracy': 0.7009961009025574, 'train/loss': 1.234709620475769, 'validation/accuracy': 0.6490600109100342, 'validation/loss': 1.4626942873001099, 'validation/num_examples': 50000, 'test/accuracy': 0.5200000405311584, 'test/loss': 2.1023662090301514, 'test/num_examples': 10000, 'score': 46673.61992907524, 'total_duration': 50909.56090068817, 'accumulated_submission_time': 46673.61992907524, 'accumulated_eval_time': 4226.306841373444, 'accumulated_logging_time': 3.9611897468566895, 'global_step': 100978, 'preemption_count': 0}), (101885, {'train/accuracy': 0.7134765386581421, 'train/loss': 1.1484699249267578, 'validation/accuracy': 0.6495400071144104, 'validation/loss': 1.434063196182251, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.0676376819610596, 'test/num_examples': 10000, 'score': 47093.20279479027, 'total_duration': 51370.565054893494, 'accumulated_submission_time': 47093.20279479027, 'accumulated_eval_time': 4267.212629556656, 'accumulated_logging_time': 4.426010370254517, 'global_step': 101885, 'preemption_count': 0}), (102794, {'train/accuracy': 0.69691401720047, 'train/loss': 1.2173900604248047, 'validation/accuracy': 0.6519399881362915, 'validation/loss': 1.4335988759994507, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.074958086013794, 'test/num_examples': 10000, 'score': 47513.41642546654, 'total_duration': 51827.06900215149, 'accumulated_submission_time': 47513.41642546654, 'accumulated_eval_time': 4303.410413980484, 'accumulated_logging_time': 4.468097686767578, 'global_step': 102794, 'preemption_count': 0}), (103704, {'train/accuracy': 0.7031054496765137, 'train/loss': 1.2242692708969116, 'validation/accuracy': 0.6475200057029724, 'validation/loss': 1.4630873203277588, 'validation/num_examples': 50000, 'test/accuracy': 0.5266000032424927, 'test/loss': 2.095916271209717, 'test/num_examples': 10000, 'score': 47933.50048518181, 'total_duration': 52290.1813583374, 'accumulated_submission_time': 47933.50048518181, 'accumulated_eval_time': 4346.339977025986, 'accumulated_logging_time': 4.515044212341309, 'global_step': 103704, 'preemption_count': 0}), (104614, {'train/accuracy': 0.7122460603713989, 'train/loss': 1.1754224300384521, 'validation/accuracy': 0.6545599699020386, 'validation/loss': 1.444462776184082, 'validation/num_examples': 50000, 'test/accuracy': 0.5293000340461731, 'test/loss': 2.080199718475342, 'test/num_examples': 10000, 'score': 48353.644204854965, 'total_duration': 52754.61422896385, 'accumulated_submission_time': 48353.644204854965, 'accumulated_eval_time': 4390.538145542145, 'accumulated_logging_time': 4.555092096328735, 'global_step': 104614, 'preemption_count': 0}), (105522, {'train/accuracy': 0.7056835889816284, 'train/loss': 1.2042386531829834, 'validation/accuracy': 0.6565399765968323, 'validation/loss': 1.425550937652588, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.077850341796875, 'test/num_examples': 10000, 'score': 48773.55797100067, 'total_duration': 53216.09664773941, 'accumulated_submission_time': 48773.55797100067, 'accumulated_eval_time': 4432.014835357666, 'accumulated_logging_time': 4.596234560012817, 'global_step': 105522, 'preemption_count': 0}), (106430, {'train/accuracy': 0.7126562595367432, 'train/loss': 1.1635273694992065, 'validation/accuracy': 0.6616799831390381, 'validation/loss': 1.3938157558441162, 'validation/num_examples': 50000, 'test/accuracy': 0.539400041103363, 'test/loss': 2.0336849689483643, 'test/num_examples': 10000, 'score': 49193.51614046097, 'total_duration': 53679.70312547684, 'accumulated_submission_time': 49193.51614046097, 'accumulated_eval_time': 4475.56670665741, 'accumulated_logging_time': 4.640961647033691, 'global_step': 106430, 'preemption_count': 0}), (107337, {'train/accuracy': 0.717968761920929, 'train/loss': 1.1395562887191772, 'validation/accuracy': 0.6581799983978271, 'validation/loss': 1.4131308794021606, 'validation/num_examples': 50000, 'test/accuracy': 0.532200038433075, 'test/loss': 2.072439670562744, 'test/num_examples': 10000, 'score': 49613.643409490585, 'total_duration': 54139.6849834919, 'accumulated_submission_time': 49613.643409490585, 'accumulated_eval_time': 4515.323534250259, 'accumulated_logging_time': 4.687347173690796, 'global_step': 107337, 'preemption_count': 0}), (108246, {'train/accuracy': 0.7106640338897705, 'train/loss': 1.1932311058044434, 'validation/accuracy': 0.6593999862670898, 'validation/loss': 1.424089789390564, 'validation/num_examples': 50000, 'test/accuracy': 0.5309000015258789, 'test/loss': 2.076124429702759, 'test/num_examples': 10000, 'score': 50033.96087384224, 'total_duration': 54598.303564071655, 'accumulated_submission_time': 50033.96087384224, 'accumulated_eval_time': 4553.532001495361, 'accumulated_logging_time': 4.729357481002808, 'global_step': 108246, 'preemption_count': 0}), (109155, {'train/accuracy': 0.7136523127555847, 'train/loss': 1.1612012386322021, 'validation/accuracy': 0.6612799763679504, 'validation/loss': 1.3903865814208984, 'validation/num_examples': 50000, 'test/accuracy': 0.5367000102996826, 'test/loss': 2.0257108211517334, 'test/num_examples': 10000, 'score': 50454.120992183685, 'total_duration': 55059.87839961052, 'accumulated_submission_time': 50454.120992183685, 'accumulated_eval_time': 4594.851831197739, 'accumulated_logging_time': 4.773675441741943, 'global_step': 109155, 'preemption_count': 0}), (110063, {'train/accuracy': 0.724804699420929, 'train/loss': 1.1234993934631348, 'validation/accuracy': 0.6626799702644348, 'validation/loss': 1.3978769779205322, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.0300371646881104, 'test/num_examples': 10000, 'score': 50874.046491622925, 'total_duration': 55520.72982406616, 'accumulated_submission_time': 50874.046491622925, 'accumulated_eval_time': 4635.680241346359, 'accumulated_logging_time': 4.820109605789185, 'global_step': 110063, 'preemption_count': 0}), (110973, {'train/accuracy': 0.7303124666213989, 'train/loss': 1.0987309217453003, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.3945322036743164, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.026243209838867, 'test/num_examples': 10000, 'score': 51293.97769546509, 'total_duration': 55978.31710648537, 'accumulated_submission_time': 51293.97769546509, 'accumulated_eval_time': 4673.234929800034, 'accumulated_logging_time': 4.870450496673584, 'global_step': 110973, 'preemption_count': 0}), (111884, {'train/accuracy': 0.7140820026397705, 'train/loss': 1.1694566011428833, 'validation/accuracy': 0.6618799567222595, 'validation/loss': 1.3972446918487549, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.036576271057129, 'test/num_examples': 10000, 'score': 51714.3005130291, 'total_duration': 56440.18104696274, 'accumulated_submission_time': 51714.3005130291, 'accumulated_eval_time': 4714.676728963852, 'accumulated_logging_time': 4.919151782989502, 'global_step': 111884, 'preemption_count': 0}), (112793, {'train/accuracy': 0.7234960794448853, 'train/loss': 1.1330419778823853, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.3698722124099731, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0242462158203125, 'test/num_examples': 10000, 'score': 52134.38846230507, 'total_duration': 56899.53965449333, 'accumulated_submission_time': 52134.38846230507, 'accumulated_eval_time': 4753.851475954056, 'accumulated_logging_time': 4.964050054550171, 'global_step': 112793, 'preemption_count': 0}), (113701, {'train/accuracy': 0.74085932970047, 'train/loss': 1.0626214742660522, 'validation/accuracy': 0.6674599647521973, 'validation/loss': 1.3853868246078491, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.020291805267334, 'test/num_examples': 10000, 'score': 52555.08496952057, 'total_duration': 57360.21570634842, 'accumulated_submission_time': 52555.08496952057, 'accumulated_eval_time': 4793.7336666584015, 'accumulated_logging_time': 5.01096510887146, 'global_step': 113701, 'preemption_count': 0}), (114607, {'train/accuracy': 0.7202929258346558, 'train/loss': 1.1391555070877075, 'validation/accuracy': 0.6705399751663208, 'validation/loss': 1.3693618774414062, 'validation/num_examples': 50000, 'test/accuracy': 0.5461000204086304, 'test/loss': 2.0146656036376953, 'test/num_examples': 10000, 'score': 52975.2583398819, 'total_duration': 57822.64588141441, 'accumulated_submission_time': 52975.2583398819, 'accumulated_eval_time': 4835.897415399551, 'accumulated_logging_time': 5.053654193878174, 'global_step': 114607, 'preemption_count': 0}), (115514, {'train/accuracy': 0.7275585532188416, 'train/loss': 1.0852470397949219, 'validation/accuracy': 0.6701200008392334, 'validation/loss': 1.3525265455245972, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 1.9930286407470703, 'test/num_examples': 10000, 'score': 53395.23126530647, 'total_duration': 58283.81888151169, 'accumulated_submission_time': 53395.23126530647, 'accumulated_eval_time': 4877.004498958588, 'accumulated_logging_time': 5.096050024032593, 'global_step': 115514, 'preemption_count': 0}), (116421, {'train/accuracy': 0.7390038967132568, 'train/loss': 1.067587971687317, 'validation/accuracy': 0.6695599555969238, 'validation/loss': 1.372552514076233, 'validation/num_examples': 50000, 'test/accuracy': 0.5443000197410583, 'test/loss': 2.023348093032837, 'test/num_examples': 10000, 'score': 53815.36892461777, 'total_duration': 58746.47779226303, 'accumulated_submission_time': 53815.36892461777, 'accumulated_eval_time': 4919.428336620331, 'accumulated_logging_time': 5.142207384109497, 'global_step': 116421, 'preemption_count': 0}), (117327, {'train/accuracy': 0.7241405844688416, 'train/loss': 1.1141245365142822, 'validation/accuracy': 0.6716200113296509, 'validation/loss': 1.3542362451553345, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 1.9819422960281372, 'test/num_examples': 10000, 'score': 54235.277552843094, 'total_duration': 59207.736137628555, 'accumulated_submission_time': 54235.277552843094, 'accumulated_eval_time': 4960.681174516678, 'accumulated_logging_time': 5.188116788864136, 'global_step': 117327, 'preemption_count': 0}), (118236, {'train/accuracy': 0.7311132550239563, 'train/loss': 1.0923360586166382, 'validation/accuracy': 0.674079954624176, 'validation/loss': 1.3469245433807373, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 1.9686278104782104, 'test/num_examples': 10000, 'score': 54655.425716876984, 'total_duration': 59665.05784249306, 'accumulated_submission_time': 54655.425716876984, 'accumulated_eval_time': 4997.757047891617, 'accumulated_logging_time': 5.235001087188721, 'global_step': 118236, 'preemption_count': 0}), (119146, {'train/accuracy': 0.7421093583106995, 'train/loss': 1.022621989250183, 'validation/accuracy': 0.6779999732971191, 'validation/loss': 1.3137280941009521, 'validation/num_examples': 50000, 'test/accuracy': 0.5534000396728516, 'test/loss': 1.954906702041626, 'test/num_examples': 10000, 'score': 55075.748200416565, 'total_duration': 60127.634479761124, 'accumulated_submission_time': 55075.748200416565, 'accumulated_eval_time': 5039.916709423065, 'accumulated_logging_time': 5.278695106506348, 'global_step': 119146, 'preemption_count': 0}), (120055, {'train/accuracy': 0.7331249713897705, 'train/loss': 1.0569225549697876, 'validation/accuracy': 0.6812399625778198, 'validation/loss': 1.2919137477874756, 'validation/num_examples': 50000, 'test/accuracy': 0.5569000244140625, 'test/loss': 1.9266986846923828, 'test/num_examples': 10000, 'score': 55495.910600185394, 'total_duration': 60586.19439768791, 'accumulated_submission_time': 55495.910600185394, 'accumulated_eval_time': 5078.214918136597, 'accumulated_logging_time': 5.3276238441467285, 'global_step': 120055, 'preemption_count': 0}), (120963, {'train/accuracy': 0.7388867139816284, 'train/loss': 1.049467921257019, 'validation/accuracy': 0.6840800046920776, 'validation/loss': 1.2940298318862915, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 1.9345511198043823, 'test/num_examples': 10000, 'score': 55916.225331544876, 'total_duration': 61047.686688899994, 'accumulated_submission_time': 55916.225331544876, 'accumulated_eval_time': 5119.290406227112, 'accumulated_logging_time': 5.378458261489868, 'global_step': 120963, 'preemption_count': 0}), (121870, {'train/accuracy': 0.7439062595367432, 'train/loss': 1.030897855758667, 'validation/accuracy': 0.6851599812507629, 'validation/loss': 1.2949132919311523, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 1.9299076795578003, 'test/num_examples': 10000, 'score': 56336.53861951828, 'total_duration': 61505.68466210365, 'accumulated_submission_time': 56336.53861951828, 'accumulated_eval_time': 5156.877597808838, 'accumulated_logging_time': 5.425642251968384, 'global_step': 121870, 'preemption_count': 0}), (122779, {'train/accuracy': 0.7379491925239563, 'train/loss': 1.0452035665512085, 'validation/accuracy': 0.685979962348938, 'validation/loss': 1.2754040956497192, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 1.8941129446029663, 'test/num_examples': 10000, 'score': 56756.696388721466, 'total_duration': 61964.73351264, 'accumulated_submission_time': 56756.696388721466, 'accumulated_eval_time': 5195.670909404755, 'accumulated_logging_time': 5.473036527633667, 'global_step': 122779, 'preemption_count': 0}), (123689, {'train/accuracy': 0.7442382574081421, 'train/loss': 1.0326616764068604, 'validation/accuracy': 0.6847800016403198, 'validation/loss': 1.2847895622253418, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 1.901072382926941, 'test/num_examples': 10000, 'score': 57176.67815852165, 'total_duration': 62428.31165552139, 'accumulated_submission_time': 57176.67815852165, 'accumulated_eval_time': 5239.167343139648, 'accumulated_logging_time': 5.522738933563232, 'global_step': 123689, 'preemption_count': 0}), (124598, {'train/accuracy': 0.7477929592132568, 'train/loss': 1.0206478834152222, 'validation/accuracy': 0.6859599947929382, 'validation/loss': 1.2963595390319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9213535785675049, 'test/num_examples': 10000, 'score': 57596.78480887413, 'total_duration': 62886.074068784714, 'accumulated_submission_time': 57596.78480887413, 'accumulated_eval_time': 5276.72448348999, 'accumulated_logging_time': 5.570141315460205, 'global_step': 124598, 'preemption_count': 0}), (125508, {'train/accuracy': 0.7463671565055847, 'train/loss': 1.0201128721237183, 'validation/accuracy': 0.6872999668121338, 'validation/loss': 1.2751214504241943, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 1.9046622514724731, 'test/num_examples': 10000, 'score': 58016.77628803253, 'total_duration': 63345.201642513275, 'accumulated_submission_time': 58016.77628803253, 'accumulated_eval_time': 5315.758058547974, 'accumulated_logging_time': 5.622182130813599, 'global_step': 125508, 'preemption_count': 0}), (126415, {'train/accuracy': 0.7504687309265137, 'train/loss': 0.9982839822769165, 'validation/accuracy': 0.6909799575805664, 'validation/loss': 1.2527966499328613, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.8792650699615479, 'test/num_examples': 10000, 'score': 58437.0435795784, 'total_duration': 63803.568054914474, 'accumulated_submission_time': 58437.0435795784, 'accumulated_eval_time': 5353.759917020798, 'accumulated_logging_time': 5.668635845184326, 'global_step': 126415, 'preemption_count': 0}), (127324, {'train/accuracy': 0.7506445050239563, 'train/loss': 0.9825835824012756, 'validation/accuracy': 0.6895599961280823, 'validation/loss': 1.252158761024475, 'validation/num_examples': 50000, 'test/accuracy': 0.567300021648407, 'test/loss': 1.8777422904968262, 'test/num_examples': 10000, 'score': 58857.124438762665, 'total_duration': 64261.78774547577, 'accumulated_submission_time': 58857.124438762665, 'accumulated_eval_time': 5391.798512220383, 'accumulated_logging_time': 5.717786550521851, 'global_step': 127324, 'preemption_count': 0}), (128236, {'train/accuracy': 0.7712695002555847, 'train/loss': 0.9164721369743347, 'validation/accuracy': 0.6928399801254272, 'validation/loss': 1.2563698291778564, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 1.882158637046814, 'test/num_examples': 10000, 'score': 59277.34511780739, 'total_duration': 64719.820784807205, 'accumulated_submission_time': 59277.34511780739, 'accumulated_eval_time': 5429.511273860931, 'accumulated_logging_time': 5.765959978103638, 'global_step': 128236, 'preemption_count': 0}), (129143, {'train/accuracy': 0.753222644329071, 'train/loss': 1.0042492151260376, 'validation/accuracy': 0.6917200088500977, 'validation/loss': 1.2587978839874268, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 1.8835062980651855, 'test/num_examples': 10000, 'score': 59697.71044564247, 'total_duration': 65181.916934490204, 'accumulated_submission_time': 59697.71044564247, 'accumulated_eval_time': 5471.143674373627, 'accumulated_logging_time': 5.814197540283203, 'global_step': 129143, 'preemption_count': 0}), (130052, {'train/accuracy': 0.7607616782188416, 'train/loss': 0.9540216326713562, 'validation/accuracy': 0.6956999897956848, 'validation/loss': 1.2329747676849365, 'validation/num_examples': 50000, 'test/accuracy': 0.5714000463485718, 'test/loss': 1.8770899772644043, 'test/num_examples': 10000, 'score': 60117.83626127243, 'total_duration': 65637.22610259056, 'accumulated_submission_time': 60117.83626127243, 'accumulated_eval_time': 5506.229387283325, 'accumulated_logging_time': 5.861069679260254, 'global_step': 130052, 'preemption_count': 0}), (130962, {'train/accuracy': 0.7698827981948853, 'train/loss': 0.9147235155105591, 'validation/accuracy': 0.6962400078773499, 'validation/loss': 1.2309191226959229, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.8582838773727417, 'test/num_examples': 10000, 'score': 60537.820308446884, 'total_duration': 66098.19182014465, 'accumulated_submission_time': 60537.820308446884, 'accumulated_eval_time': 5547.110866785049, 'accumulated_logging_time': 5.910343647003174, 'global_step': 130962, 'preemption_count': 0}), (131870, {'train/accuracy': 0.7575585842132568, 'train/loss': 0.9641546607017517, 'validation/accuracy': 0.7005599737167358, 'validation/loss': 1.2190051078796387, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.8357348442077637, 'test/num_examples': 10000, 'score': 60957.97349905968, 'total_duration': 66560.34111618996, 'accumulated_submission_time': 60957.97349905968, 'accumulated_eval_time': 5589.005449295044, 'accumulated_logging_time': 5.961735725402832, 'global_step': 131870, 'preemption_count': 0}), (132778, {'train/accuracy': 0.7624413967132568, 'train/loss': 0.9442371129989624, 'validation/accuracy': 0.6990199685096741, 'validation/loss': 1.2210444211959839, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.856716513633728, 'test/num_examples': 10000, 'score': 61378.235782146454, 'total_duration': 67020.0600142479, 'accumulated_submission_time': 61378.235782146454, 'accumulated_eval_time': 5628.361680984497, 'accumulated_logging_time': 6.011536359786987, 'global_step': 132778, 'preemption_count': 0}), (133688, {'train/accuracy': 0.7743359208106995, 'train/loss': 0.8995473384857178, 'validation/accuracy': 0.7016800045967102, 'validation/loss': 1.211005687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.57750004529953, 'test/loss': 1.8353841304779053, 'test/num_examples': 10000, 'score': 61798.403445243835, 'total_duration': 67477.51272082329, 'accumulated_submission_time': 61798.403445243835, 'accumulated_eval_time': 5665.5401656627655, 'accumulated_logging_time': 6.066869735717773, 'global_step': 133688, 'preemption_count': 0}), (134599, {'train/accuracy': 0.76185542345047, 'train/loss': 0.9467484951019287, 'validation/accuracy': 0.7027999758720398, 'validation/loss': 1.2054047584533691, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 1.8349767923355103, 'test/num_examples': 10000, 'score': 62218.46045303345, 'total_duration': 67939.94962263107, 'accumulated_submission_time': 62218.46045303345, 'accumulated_eval_time': 5707.8211581707, 'accumulated_logging_time': 6.115529537200928, 'global_step': 134599, 'preemption_count': 0}), (135507, {'train/accuracy': 0.7659569978713989, 'train/loss': 0.9456735253334045, 'validation/accuracy': 0.7011799812316895, 'validation/loss': 1.2311320304870605, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.8514093160629272, 'test/num_examples': 10000, 'score': 62638.41057395935, 'total_duration': 68400.22089409828, 'accumulated_submission_time': 62638.41057395935, 'accumulated_eval_time': 5748.042767763138, 'accumulated_logging_time': 6.164484024047852, 'global_step': 135507, 'preemption_count': 0}), (136414, {'train/accuracy': 0.7757421731948853, 'train/loss': 0.9015170931816101, 'validation/accuracy': 0.704539954662323, 'validation/loss': 1.1993683576583862, 'validation/num_examples': 50000, 'test/accuracy': 0.5824000239372253, 'test/loss': 1.8206918239593506, 'test/num_examples': 10000, 'score': 63058.551265239716, 'total_duration': 68856.67860889435, 'accumulated_submission_time': 63058.551265239716, 'accumulated_eval_time': 5784.257081747055, 'accumulated_logging_time': 6.215606927871704, 'global_step': 136414, 'preemption_count': 0}), (137322, {'train/accuracy': 0.7714648246765137, 'train/loss': 0.9188026189804077, 'validation/accuracy': 0.7074999809265137, 'validation/loss': 1.1961042881011963, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 1.8227862119674683, 'test/num_examples': 10000, 'score': 63478.52360081673, 'total_duration': 69317.36960935593, 'accumulated_submission_time': 63478.52360081673, 'accumulated_eval_time': 5824.872405529022, 'accumulated_logging_time': 6.267154216766357, 'global_step': 137322, 'preemption_count': 0}), (138229, {'train/accuracy': 0.777636706829071, 'train/loss': 0.8658644556999207, 'validation/accuracy': 0.7101799845695496, 'validation/loss': 1.1642639636993408, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 1.7642264366149902, 'test/num_examples': 10000, 'score': 63898.665610551834, 'total_duration': 69776.55884242058, 'accumulated_submission_time': 63898.665610551834, 'accumulated_eval_time': 5863.8218557834625, 'accumulated_logging_time': 6.314066648483276, 'global_step': 138229, 'preemption_count': 0}), (139141, {'train/accuracy': 0.7803515195846558, 'train/loss': 0.868783712387085, 'validation/accuracy': 0.7113199830055237, 'validation/loss': 1.1666555404663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5849000215530396, 'test/loss': 1.7911489009857178, 'test/num_examples': 10000, 'score': 64318.93400526047, 'total_duration': 70236.60565376282, 'accumulated_submission_time': 64318.93400526047, 'accumulated_eval_time': 5903.501656532288, 'accumulated_logging_time': 6.361291170120239, 'global_step': 139141, 'preemption_count': 0}), (140050, {'train/accuracy': 0.7785937190055847, 'train/loss': 0.8846719861030579, 'validation/accuracy': 0.7123799920082092, 'validation/loss': 1.1717824935913086, 'validation/num_examples': 50000, 'test/accuracy': 0.5907000303268433, 'test/loss': 1.7813829183578491, 'test/num_examples': 10000, 'score': 64738.83408522606, 'total_duration': 70698.14928531647, 'accumulated_submission_time': 64738.83408522606, 'accumulated_eval_time': 5945.039888620377, 'accumulated_logging_time': 6.41525673866272, 'global_step': 140050, 'preemption_count': 0}), (140959, {'train/accuracy': 0.775390625, 'train/loss': 0.8824504017829895, 'validation/accuracy': 0.7128199934959412, 'validation/loss': 1.150181770324707, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.7759381532669067, 'test/num_examples': 10000, 'score': 65158.86788415909, 'total_duration': 71154.27746748924, 'accumulated_submission_time': 65158.86788415909, 'accumulated_eval_time': 5981.03254365921, 'accumulated_logging_time': 6.46516752243042, 'global_step': 140959, 'preemption_count': 0}), (141867, {'train/accuracy': 0.7857617139816284, 'train/loss': 0.8551717400550842, 'validation/accuracy': 0.7181199789047241, 'validation/loss': 1.148403525352478, 'validation/num_examples': 50000, 'test/accuracy': 0.5962000489234924, 'test/loss': 1.753408432006836, 'test/num_examples': 10000, 'score': 65579.00881290436, 'total_duration': 71614.71821403503, 'accumulated_submission_time': 65579.00881290436, 'accumulated_eval_time': 6021.227711677551, 'accumulated_logging_time': 6.5184853076934814, 'global_step': 141867, 'preemption_count': 0}), (142775, {'train/accuracy': 0.7930663824081421, 'train/loss': 0.8114429116249084, 'validation/accuracy': 0.7167999744415283, 'validation/loss': 1.1399496793746948, 'validation/num_examples': 50000, 'test/accuracy': 0.5949000120162964, 'test/loss': 1.7673008441925049, 'test/num_examples': 10000, 'score': 65999.13509559631, 'total_duration': 72077.39252829552, 'accumulated_submission_time': 65999.13509559631, 'accumulated_eval_time': 6063.670241594315, 'accumulated_logging_time': 6.572426080703735, 'global_step': 142775, 'preemption_count': 0}), (143682, {'train/accuracy': 0.7851952910423279, 'train/loss': 0.8426395058631897, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.118364691734314, 'validation/num_examples': 50000, 'test/accuracy': 0.6010000109672546, 'test/loss': 1.724225640296936, 'test/num_examples': 10000, 'score': 66419.16562604904, 'total_duration': 72537.31822061539, 'accumulated_submission_time': 66419.16562604904, 'accumulated_eval_time': 6103.460964918137, 'accumulated_logging_time': 6.625840425491333, 'global_step': 143682, 'preemption_count': 0}), (144588, {'train/accuracy': 0.7878710627555847, 'train/loss': 0.8365593552589417, 'validation/accuracy': 0.7193599939346313, 'validation/loss': 1.1317459344863892, 'validation/num_examples': 50000, 'test/accuracy': 0.5928000211715698, 'test/loss': 1.7537528276443481, 'test/num_examples': 10000, 'score': 66839.12981963158, 'total_duration': 72996.18788266182, 'accumulated_submission_time': 66839.12981963158, 'accumulated_eval_time': 6141.8354551792145, 'accumulated_logging_time': 7.105395555496216, 'global_step': 144588, 'preemption_count': 0}), (145495, {'train/accuracy': 0.7982421517372131, 'train/loss': 0.8103423714637756, 'validation/accuracy': 0.7236799597740173, 'validation/loss': 1.131292462348938, 'validation/num_examples': 50000, 'test/accuracy': 0.6044000387191772, 'test/loss': 1.7381134033203125, 'test/num_examples': 10000, 'score': 67259.35423231125, 'total_duration': 73458.10443782806, 'accumulated_submission_time': 67259.35423231125, 'accumulated_eval_time': 6183.4234120845795, 'accumulated_logging_time': 7.158540964126587, 'global_step': 145495, 'preemption_count': 0}), (146404, {'train/accuracy': 0.7885546684265137, 'train/loss': 0.8147965669631958, 'validation/accuracy': 0.7246999740600586, 'validation/loss': 1.102521538734436, 'validation/num_examples': 50000, 'test/accuracy': 0.6007000207901001, 'test/loss': 1.729142427444458, 'test/num_examples': 10000, 'score': 67679.5624153614, 'total_duration': 73919.75740528107, 'accumulated_submission_time': 67679.5624153614, 'accumulated_eval_time': 6224.7675149440765, 'accumulated_logging_time': 7.207941770553589, 'global_step': 146404, 'preemption_count': 0}), (147315, {'train/accuracy': 0.7919921875, 'train/loss': 0.8182185888290405, 'validation/accuracy': 0.7255399823188782, 'validation/loss': 1.1136888265609741, 'validation/num_examples': 50000, 'test/accuracy': 0.6016000509262085, 'test/loss': 1.728710412979126, 'test/num_examples': 10000, 'score': 68099.6663825512, 'total_duration': 74383.72610616684, 'accumulated_submission_time': 68099.6663825512, 'accumulated_eval_time': 6268.529381752014, 'accumulated_logging_time': 7.259737014770508, 'global_step': 147315, 'preemption_count': 0}), (148224, {'train/accuracy': 0.8016796708106995, 'train/loss': 0.7689365744590759, 'validation/accuracy': 0.7294999957084656, 'validation/loss': 1.0898467302322388, 'validation/num_examples': 50000, 'test/accuracy': 0.6040000319480896, 'test/loss': 1.6969178915023804, 'test/num_examples': 10000, 'score': 68519.65882515907, 'total_duration': 74840.09953451157, 'accumulated_submission_time': 68519.65882515907, 'accumulated_eval_time': 6304.80890417099, 'accumulated_logging_time': 7.309703350067139, 'global_step': 148224, 'preemption_count': 0}), (149132, {'train/accuracy': 0.7948632836341858, 'train/loss': 0.8024986982345581, 'validation/accuracy': 0.7267000079154968, 'validation/loss': 1.0969750881195068, 'validation/num_examples': 50000, 'test/accuracy': 0.6046000123023987, 'test/loss': 1.711686372756958, 'test/num_examples': 10000, 'score': 68939.72495675087, 'total_duration': 75302.07324838638, 'accumulated_submission_time': 68939.72495675087, 'accumulated_eval_time': 6346.617129325867, 'accumulated_logging_time': 7.357788801193237, 'global_step': 149132, 'preemption_count': 0}), (150039, {'train/accuracy': 0.7957812547683716, 'train/loss': 0.812127411365509, 'validation/accuracy': 0.7278800010681152, 'validation/loss': 1.094635009765625, 'validation/num_examples': 50000, 'test/accuracy': 0.6082000136375427, 'test/loss': 1.716665267944336, 'test/num_examples': 10000, 'score': 69359.74498486519, 'total_duration': 75761.56579613686, 'accumulated_submission_time': 69359.74498486519, 'accumulated_eval_time': 6385.988118648529, 'accumulated_logging_time': 7.408616781234741, 'global_step': 150039, 'preemption_count': 0}), (150949, {'train/accuracy': 0.8050976395606995, 'train/loss': 0.7490441799163818, 'validation/accuracy': 0.7310199737548828, 'validation/loss': 1.0718308687210083, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.6876506805419922, 'test/num_examples': 10000, 'score': 69780.05288887024, 'total_duration': 76225.19110178947, 'accumulated_submission_time': 69780.05288887024, 'accumulated_eval_time': 6429.201789140701, 'accumulated_logging_time': 7.461533546447754, 'global_step': 150949, 'preemption_count': 0}), (151859, {'train/accuracy': 0.8020703196525574, 'train/loss': 0.7700572609901428, 'validation/accuracy': 0.7359799742698669, 'validation/loss': 1.066151738166809, 'validation/num_examples': 50000, 'test/accuracy': 0.6122000217437744, 'test/loss': 1.6726754903793335, 'test/num_examples': 10000, 'score': 70200.3424217701, 'total_duration': 76687.96187376976, 'accumulated_submission_time': 70200.3424217701, 'accumulated_eval_time': 6471.581538200378, 'accumulated_logging_time': 7.511557102203369, 'global_step': 151859, 'preemption_count': 0}), (152769, {'train/accuracy': 0.8058202862739563, 'train/loss': 0.7566875219345093, 'validation/accuracy': 0.7363399863243103, 'validation/loss': 1.0597726106643677, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.6564711332321167, 'test/num_examples': 10000, 'score': 70620.28472137451, 'total_duration': 77147.348580122, 'accumulated_submission_time': 70620.28472137451, 'accumulated_eval_time': 6510.923154830933, 'accumulated_logging_time': 7.5636162757873535, 'global_step': 152769, 'preemption_count': 0}), (153678, {'train/accuracy': 0.81298828125, 'train/loss': 0.7200682759284973, 'validation/accuracy': 0.7379800081253052, 'validation/loss': 1.0499589443206787, 'validation/num_examples': 50000, 'test/accuracy': 0.6166000366210938, 'test/loss': 1.6612064838409424, 'test/num_examples': 10000, 'score': 71040.23987174034, 'total_duration': 77604.85888195038, 'accumulated_submission_time': 71040.23987174034, 'accumulated_eval_time': 6548.3741619586945, 'accumulated_logging_time': 7.616273880004883, 'global_step': 153678, 'preemption_count': 0}), (154587, {'train/accuracy': 0.8049609065055847, 'train/loss': 0.746151864528656, 'validation/accuracy': 0.737339973449707, 'validation/loss': 1.0490286350250244, 'validation/num_examples': 50000, 'test/accuracy': 0.6158000230789185, 'test/loss': 1.6725051403045654, 'test/num_examples': 10000, 'score': 71460.52765202522, 'total_duration': 78063.63143539429, 'accumulated_submission_time': 71460.52765202522, 'accumulated_eval_time': 6586.756381750107, 'accumulated_logging_time': 7.667668581008911, 'global_step': 154587, 'preemption_count': 0}), (155497, {'train/accuracy': 0.810839831829071, 'train/loss': 0.723305344581604, 'validation/accuracy': 0.7399199604988098, 'validation/loss': 1.035240888595581, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.652098298072815, 'test/num_examples': 10000, 'score': 71880.68959569931, 'total_duration': 78524.88885855675, 'accumulated_submission_time': 71880.68959569931, 'accumulated_eval_time': 6627.746407985687, 'accumulated_logging_time': 7.721628189086914, 'global_step': 155497, 'preemption_count': 0}), (156406, {'train/accuracy': 0.8157812356948853, 'train/loss': 0.7162489295005798, 'validation/accuracy': 0.7403799891471863, 'validation/loss': 1.0414212942123413, 'validation/num_examples': 50000, 'test/accuracy': 0.62090003490448, 'test/loss': 1.653515338897705, 'test/num_examples': 10000, 'score': 72300.75505638123, 'total_duration': 78981.39456152916, 'accumulated_submission_time': 72300.75505638123, 'accumulated_eval_time': 6664.082603693008, 'accumulated_logging_time': 7.774799346923828, 'global_step': 156406, 'preemption_count': 0}), (157314, {'train/accuracy': 0.8150194883346558, 'train/loss': 0.7227945327758789, 'validation/accuracy': 0.7415800094604492, 'validation/loss': 1.0487637519836426, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.6517783403396606, 'test/num_examples': 10000, 'score': 72720.79807853699, 'total_duration': 79443.12803387642, 'accumulated_submission_time': 72720.79807853699, 'accumulated_eval_time': 6705.670013189316, 'accumulated_logging_time': 7.826829195022583, 'global_step': 157314, 'preemption_count': 0}), (158223, {'train/accuracy': 0.8199414014816284, 'train/loss': 0.7127540707588196, 'validation/accuracy': 0.7445399761199951, 'validation/loss': 1.0261250734329224, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.6282966136932373, 'test/num_examples': 10000, 'score': 73141.04412603378, 'total_duration': 79906.46152997017, 'accumulated_submission_time': 73141.04412603378, 'accumulated_eval_time': 6748.654703617096, 'accumulated_logging_time': 7.878124952316284, 'global_step': 158223, 'preemption_count': 0}), (159130, {'train/accuracy': 0.8240038752555847, 'train/loss': 0.6871118545532227, 'validation/accuracy': 0.747439980506897, 'validation/loss': 1.0210548639297485, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.622787356376648, 'test/num_examples': 10000, 'score': 73561.13456368446, 'total_duration': 80370.60028123856, 'accumulated_submission_time': 73561.13456368446, 'accumulated_eval_time': 6792.603569984436, 'accumulated_logging_time': 7.927542448043823, 'global_step': 159130, 'preemption_count': 0}), (160039, {'train/accuracy': 0.8271484375, 'train/loss': 0.6677748560905457, 'validation/accuracy': 0.7458199858665466, 'validation/loss': 1.0177862644195557, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.6190569400787354, 'test/num_examples': 10000, 'score': 73981.15388822556, 'total_duration': 80829.69215226173, 'accumulated_submission_time': 73981.15388822556, 'accumulated_eval_time': 6831.573830366135, 'accumulated_logging_time': 7.979724168777466, 'global_step': 160039, 'preemption_count': 0}), (160948, {'train/accuracy': 0.8216406106948853, 'train/loss': 0.6819668412208557, 'validation/accuracy': 0.7469599843025208, 'validation/loss': 1.0042126178741455, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.608596682548523, 'test/num_examples': 10000, 'score': 74401.07062625885, 'total_duration': 81287.48914146423, 'accumulated_submission_time': 74401.07062625885, 'accumulated_eval_time': 6869.350848913193, 'accumulated_logging_time': 8.031014919281006, 'global_step': 160948, 'preemption_count': 0}), (161856, {'train/accuracy': 0.8240429759025574, 'train/loss': 0.6735599637031555, 'validation/accuracy': 0.7506600022315979, 'validation/loss': 0.9978360533714294, 'validation/num_examples': 50000, 'test/accuracy': 0.6320000290870667, 'test/loss': 1.5975457429885864, 'test/num_examples': 10000, 'score': 74821.04405879974, 'total_duration': 81749.93900322914, 'accumulated_submission_time': 74821.04405879974, 'accumulated_eval_time': 6911.723528146744, 'accumulated_logging_time': 8.083473920822144, 'global_step': 161856, 'preemption_count': 0}), (162765, {'train/accuracy': 0.8299023509025574, 'train/loss': 0.6468636989593506, 'validation/accuracy': 0.7511199712753296, 'validation/loss': 0.9937317967414856, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.6002475023269653, 'test/num_examples': 10000, 'score': 75240.97953391075, 'total_duration': 82215.21632957458, 'accumulated_submission_time': 75240.97953391075, 'accumulated_eval_time': 6956.957853317261, 'accumulated_logging_time': 8.137528657913208, 'global_step': 162765, 'preemption_count': 0}), (163675, {'train/accuracy': 0.8234374523162842, 'train/loss': 0.6833599209785461, 'validation/accuracy': 0.7499200105667114, 'validation/loss': 0.9993503093719482, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.6094584465026855, 'test/num_examples': 10000, 'score': 75661.26156401634, 'total_duration': 82674.41547107697, 'accumulated_submission_time': 75661.26156401634, 'accumulated_eval_time': 6995.772277355194, 'accumulated_logging_time': 8.188964128494263, 'global_step': 163675, 'preemption_count': 0}), (164583, {'train/accuracy': 0.8306640386581421, 'train/loss': 0.6563959121704102, 'validation/accuracy': 0.7524600028991699, 'validation/loss': 0.9888281226158142, 'validation/num_examples': 50000, 'test/accuracy': 0.6324000358581543, 'test/loss': 1.5833725929260254, 'test/num_examples': 10000, 'score': 76081.20295095444, 'total_duration': 83135.30335402489, 'accumulated_submission_time': 76081.20295095444, 'accumulated_eval_time': 7036.61114192009, 'accumulated_logging_time': 8.245900869369507, 'global_step': 164583, 'preemption_count': 0}), (165493, {'train/accuracy': 0.8374218344688416, 'train/loss': 0.6225919723510742, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 0.9747494459152222, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.5695737600326538, 'test/num_examples': 10000, 'score': 76501.53271389008, 'total_duration': 83594.82507920265, 'accumulated_submission_time': 76501.53271389008, 'accumulated_eval_time': 7075.693947792053, 'accumulated_logging_time': 8.304190874099731, 'global_step': 165493, 'preemption_count': 0}), (166403, {'train/accuracy': 0.8303124904632568, 'train/loss': 0.6463891267776489, 'validation/accuracy': 0.7556399703025818, 'validation/loss': 0.9667149782180786, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.5555449724197388, 'test/num_examples': 10000, 'score': 76921.90054345131, 'total_duration': 84057.80018854141, 'accumulated_submission_time': 76921.90054345131, 'accumulated_eval_time': 7118.194668292999, 'accumulated_logging_time': 8.359277486801147, 'global_step': 166403, 'preemption_count': 0}), (167311, {'train/accuracy': 0.8325781226158142, 'train/loss': 0.6565069556236267, 'validation/accuracy': 0.7567799687385559, 'validation/loss': 0.9810318350791931, 'validation/num_examples': 50000, 'test/accuracy': 0.6397000551223755, 'test/loss': 1.5751323699951172, 'test/num_examples': 10000, 'score': 77342.16409659386, 'total_duration': 84515.86970067024, 'accumulated_submission_time': 77342.16409659386, 'accumulated_eval_time': 7155.896743297577, 'accumulated_logging_time': 8.412142515182495, 'global_step': 167311, 'preemption_count': 0})], 'global_step': 167701}
I0202 18:03:01.610460 140085747812160 submission_runner.py:586] Timing: 77520.09283590317
I0202 18:03:01.610538 140085747812160 submission_runner.py:588] Total number of evals: 185
I0202 18:03:01.610581 140085747812160 submission_runner.py:589] ====================
I0202 18:03:01.610627 140085747812160 submission_runner.py:542] Using RNG seed 1800903789
I0202 18:03:01.612174 140085747812160 submission_runner.py:551] --- Tuning run 4/5 ---
I0202 18:03:01.612279 140085747812160 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4.
I0202 18:03:01.613864 140085747812160 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4/hparams.json.
I0202 18:03:01.614739 140085747812160 submission_runner.py:206] Initializing dataset.
I0202 18:03:01.625489 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0202 18:03:01.636462 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0202 18:03:01.823268 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0202 18:03:06.260270 140085747812160 submission_runner.py:213] Initializing model.
I0202 18:03:12.999610 140085747812160 submission_runner.py:255] Initializing optimizer.
I0202 18:03:13.486500 140085747812160 submission_runner.py:262] Initializing metrics bundle.
I0202 18:03:13.486668 140085747812160 submission_runner.py:280] Initializing checkpoint and logger.
I0202 18:03:13.500736 140085747812160 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4 with prefix checkpoint_
I0202 18:03:13.500874 140085747812160 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0202 18:03:30.025782 140085747812160 logger_utils.py:220] Unable to record git information. Continuing without it.
I0202 18:03:46.291254 140085747812160 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4/flags_0.json.
I0202 18:03:46.295916 140085747812160 submission_runner.py:314] Starting training loop.
I0202 18:04:22.683453 139923826849536 logging_writer.py:48] [0] global_step=0, grad_norm=0.3661307394504547, loss=6.907756328582764
I0202 18:04:22.700389 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:04:31.081939 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:04:49.776424 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:04:51.422291 140085747812160 submission_runner.py:408] Time since start: 65.13s, 	Step: 1, 	{'train/accuracy': 0.0012109375093132257, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.40435767173767, 'total_duration': 65.12631821632385, 'accumulated_submission_time': 36.40435767173767, 'accumulated_eval_time': 28.721853494644165, 'accumulated_logging_time': 0}
I0202 18:04:51.430700 139923835242240 logging_writer.py:48] [1] accumulated_eval_time=28.721853, accumulated_logging_time=0, accumulated_submission_time=36.404358, global_step=1, preemption_count=0, score=36.404358, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=65.126318, train/accuracy=0.001211, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0202 18:05:57.165821 139923868813056 logging_writer.py:48] [100] global_step=100, grad_norm=0.5026673078536987, loss=6.864692687988281
I0202 18:06:43.744731 139923852027648 logging_writer.py:48] [200] global_step=200, grad_norm=0.8778049349784851, loss=6.722440242767334
I0202 18:07:31.390658 139923868813056 logging_writer.py:48] [300] global_step=300, grad_norm=0.9597464203834534, loss=6.565675258636475
I0202 18:08:18.994696 139923852027648 logging_writer.py:48] [400] global_step=400, grad_norm=1.1293039321899414, loss=6.452789306640625
I0202 18:09:06.488027 139923868813056 logging_writer.py:48] [500] global_step=500, grad_norm=0.8986033201217651, loss=6.482828140258789
I0202 18:09:53.885181 139923852027648 logging_writer.py:48] [600] global_step=600, grad_norm=1.3880693912506104, loss=6.357982158660889
I0202 18:10:41.230610 139923868813056 logging_writer.py:48] [700] global_step=700, grad_norm=0.9549171328544617, loss=6.8025712966918945
I0202 18:11:28.852863 139923852027648 logging_writer.py:48] [800] global_step=800, grad_norm=0.7636935114860535, loss=6.327052116394043
I0202 18:11:51.681802 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:12:02.701178 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:12:30.348132 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:12:31.995666 140085747812160 submission_runner.py:408] Time since start: 525.70s, 	Step: 850, 	{'train/accuracy': 0.03794921934604645, 'train/loss': 5.836334705352783, 'validation/accuracy': 0.037379998713731766, 'validation/loss': 5.868350028991699, 'validation/num_examples': 50000, 'test/accuracy': 0.030400000512599945, 'test/loss': 6.007944583892822, 'test/num_examples': 10000, 'score': 456.5975193977356, 'total_duration': 525.6996972560883, 'accumulated_submission_time': 456.5975193977356, 'accumulated_eval_time': 69.03572416305542, 'accumulated_logging_time': 0.01876664161682129}
I0202 18:12:32.015531 139923868813056 logging_writer.py:48] [850] accumulated_eval_time=69.035724, accumulated_logging_time=0.018767, accumulated_submission_time=456.597519, global_step=850, preemption_count=0, score=456.597519, test/accuracy=0.030400, test/loss=6.007945, test/num_examples=10000, total_duration=525.699697, train/accuracy=0.037949, train/loss=5.836335, validation/accuracy=0.037380, validation/loss=5.868350, validation/num_examples=50000
I0202 18:12:52.004498 139923852027648 logging_writer.py:48] [900] global_step=900, grad_norm=0.7932831645011902, loss=6.156170845031738
I0202 18:13:37.844667 139923868813056 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.7723461985588074, loss=6.176821231842041
I0202 18:14:25.202871 139923852027648 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7683088779449463, loss=6.087345600128174
I0202 18:15:12.296094 139923868813056 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7133385539054871, loss=5.969473838806152
I0202 18:15:59.502411 139923852027648 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6383228898048401, loss=6.043000221252441
I0202 18:16:46.429444 139923868813056 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.625784158706665, loss=6.123153209686279
I0202 18:17:33.603175 139923852027648 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5890242457389832, loss=6.128995895385742
I0202 18:18:20.779819 139923868813056 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6393488645553589, loss=5.899905204772949
I0202 18:19:08.012853 139923852027648 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5683315396308899, loss=6.471216201782227
I0202 18:19:32.054980 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:19:42.855715 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:20:11.509558 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:20:13.168163 140085747812160 submission_runner.py:408] Time since start: 986.87s, 	Step: 1753, 	{'train/accuracy': 0.06109374761581421, 'train/loss': 5.500517845153809, 'validation/accuracy': 0.05495999753475189, 'validation/loss': 5.583978176116943, 'validation/num_examples': 50000, 'test/accuracy': 0.04190000146627426, 'test/loss': 5.780269145965576, 'test/num_examples': 10000, 'score': 876.5749049186707, 'total_duration': 986.8721923828125, 'accumulated_submission_time': 876.5749049186707, 'accumulated_eval_time': 110.14891505241394, 'accumulated_logging_time': 0.05063366889953613}
I0202 18:20:13.184137 139923868813056 logging_writer.py:48] [1753] accumulated_eval_time=110.148915, accumulated_logging_time=0.050634, accumulated_submission_time=876.574905, global_step=1753, preemption_count=0, score=876.574905, test/accuracy=0.041900, test/loss=5.780269, test/num_examples=10000, total_duration=986.872192, train/accuracy=0.061094, train/loss=5.500518, validation/accuracy=0.054960, validation/loss=5.583978, validation/num_examples=50000
I0202 18:20:32.028571 139923852027648 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5575941205024719, loss=6.359683513641357
I0202 18:21:17.325318 139923868813056 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4940650463104248, loss=6.646168231964111
I0202 18:22:04.405455 139923852027648 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.4952942430973053, loss=5.761574745178223
I0202 18:22:51.331177 139923868813056 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.6837221384048462, loss=5.777388572692871
I0202 18:23:38.270615 139923852027648 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.4735353887081146, loss=5.654081344604492
I0202 18:24:25.317906 139923868813056 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5392570495605469, loss=5.683590888977051
I0202 18:25:12.183926 139923852027648 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5378907322883606, loss=5.6139912605285645
I0202 18:25:59.273854 139923868813056 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.46459707617759705, loss=5.623080730438232
I0202 18:26:46.381161 139923852027648 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.513308048248291, loss=5.538085460662842
I0202 18:27:13.286784 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:27:24.226481 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:27:55.193509 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:27:56.842690 140085747812160 submission_runner.py:408] Time since start: 1450.55s, 	Step: 2659, 	{'train/accuracy': 0.10499999672174454, 'train/loss': 4.955112934112549, 'validation/accuracy': 0.095660001039505, 'validation/loss': 5.036952495574951, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 5.3251471519470215, 'test/num_examples': 10000, 'score': 1296.6166756153107, 'total_duration': 1450.5467166900635, 'accumulated_submission_time': 1296.6166756153107, 'accumulated_eval_time': 153.70484042167664, 'accumulated_logging_time': 0.07620692253112793}
I0202 18:27:56.860656 139923868813056 logging_writer.py:48] [2659] accumulated_eval_time=153.704840, accumulated_logging_time=0.076207, accumulated_submission_time=1296.616676, global_step=2659, preemption_count=0, score=1296.616676, test/accuracy=0.076700, test/loss=5.325147, test/num_examples=10000, total_duration=1450.546717, train/accuracy=0.105000, train/loss=4.955113, validation/accuracy=0.095660, validation/loss=5.036952, validation/num_examples=50000
I0202 18:28:13.363634 139923852027648 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.4798101782798767, loss=6.358532905578613
I0202 18:28:58.293147 139923868813056 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.45399776101112366, loss=5.3402509689331055
I0202 18:29:45.666914 139923852027648 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5613093972206116, loss=5.510897159576416
I0202 18:30:32.604436 139923868813056 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4519186317920685, loss=5.567215919494629
I0202 18:31:19.355389 139923852027648 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.44126641750335693, loss=5.334941387176514
I0202 18:32:06.328705 139923868813056 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5332409739494324, loss=6.370218753814697
I0202 18:32:53.395814 139923852027648 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9054765701293945, loss=5.3647074699401855
I0202 18:33:40.320801 139923868813056 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.4649534821510315, loss=5.35170316696167
I0202 18:34:27.415143 139923852027648 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.48612287640571594, loss=5.696015357971191
I0202 18:34:57.241729 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:35:07.999222 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:35:33.763731 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:35:35.403856 140085747812160 submission_runner.py:408] Time since start: 1909.11s, 	Step: 3565, 	{'train/accuracy': 0.1318359375, 'train/loss': 4.715084075927734, 'validation/accuracy': 0.12212000042200089, 'validation/loss': 4.792366981506348, 'validation/num_examples': 50000, 'test/accuracy': 0.09670000523328781, 'test/loss': 5.13610315322876, 'test/num_examples': 10000, 'score': 1716.937399148941, 'total_duration': 1909.1078877449036, 'accumulated_submission_time': 1716.937399148941, 'accumulated_eval_time': 191.86698031425476, 'accumulated_logging_time': 0.1044766902923584}
I0202 18:35:35.420477 139923868813056 logging_writer.py:48] [3565] accumulated_eval_time=191.866980, accumulated_logging_time=0.104477, accumulated_submission_time=1716.937399, global_step=3565, preemption_count=0, score=1716.937399, test/accuracy=0.096700, test/loss=5.136103, test/num_examples=10000, total_duration=1909.107888, train/accuracy=0.131836, train/loss=4.715084, validation/accuracy=0.122120, validation/loss=4.792367, validation/num_examples=50000
I0202 18:35:49.548663 139923852027648 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6632004976272583, loss=6.245645523071289
I0202 18:36:34.150533 139923868813056 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7527672052383423, loss=5.251349925994873
I0202 18:37:21.351275 139923852027648 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.6764078736305237, loss=5.307116508483887
I0202 18:38:08.532914 139923868813056 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.6365540027618408, loss=5.386167049407959
I0202 18:38:55.413185 139923852027648 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5921555161476135, loss=5.228376865386963
I0202 18:39:42.310877 139923868813056 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7115057110786438, loss=5.280693531036377
I0202 18:40:29.459382 139923852027648 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6527381539344788, loss=5.0810227394104
I0202 18:41:16.683500 139923868813056 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.8480277061462402, loss=5.13173770904541
I0202 18:42:03.383780 139923852027648 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7156683802604675, loss=6.261007308959961
I0202 18:42:35.636469 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:42:46.596077 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:43:13.060747 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:43:14.703328 140085747812160 submission_runner.py:408] Time since start: 2368.41s, 	Step: 4470, 	{'train/accuracy': 0.16396483778953552, 'train/loss': 4.44169282913208, 'validation/accuracy': 0.14983999729156494, 'validation/loss': 4.542600631713867, 'validation/num_examples': 50000, 'test/accuracy': 0.11570000648498535, 'test/loss': 4.900367259979248, 'test/num_examples': 10000, 'score': 2137.0898151397705, 'total_duration': 2368.407334804535, 'accumulated_submission_time': 2137.0898151397705, 'accumulated_eval_time': 230.93383693695068, 'accumulated_logging_time': 0.13391804695129395}
I0202 18:43:14.719783 139923868813056 logging_writer.py:48] [4470] accumulated_eval_time=230.933837, accumulated_logging_time=0.133918, accumulated_submission_time=2137.089815, global_step=4470, preemption_count=0, score=2137.089815, test/accuracy=0.115700, test/loss=4.900367, test/num_examples=10000, total_duration=2368.407335, train/accuracy=0.163965, train/loss=4.441693, validation/accuracy=0.149840, validation/loss=4.542601, validation/num_examples=50000
I0202 18:43:26.882275 139923852027648 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.6979068517684937, loss=5.084432125091553
I0202 18:44:11.091261 139923868813056 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.6151471734046936, loss=4.984427452087402
I0202 18:44:58.169360 139923852027648 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6488642692565918, loss=4.945119380950928
I0202 18:45:45.148079 139923868813056 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6944697499275208, loss=6.347020149230957
I0202 18:46:32.014749 139923852027648 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.6758379340171814, loss=4.977977275848389
I0202 18:47:19.043607 139923868813056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6250451803207397, loss=5.156716823577881
I0202 18:48:05.920602 139923852027648 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8277101516723633, loss=4.997973918914795
I0202 18:48:53.124984 139923868813056 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8945666551589966, loss=4.86848258972168
I0202 18:49:40.469549 139923852027648 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.664236307144165, loss=6.20596170425415
I0202 18:50:15.004088 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:50:25.677140 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:50:58.784424 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:51:00.421423 140085747812160 submission_runner.py:408] Time since start: 2834.13s, 	Step: 5375, 	{'train/accuracy': 0.19007812440395355, 'train/loss': 4.193375587463379, 'validation/accuracy': 0.17573998868465424, 'validation/loss': 4.2860283851623535, 'validation/num_examples': 50000, 'test/accuracy': 0.13700000941753387, 'test/loss': 4.693284034729004, 'test/num_examples': 10000, 'score': 2557.3137817382812, 'total_duration': 2834.1254436969757, 'accumulated_submission_time': 2557.3137817382812, 'accumulated_eval_time': 276.35117983818054, 'accumulated_logging_time': 0.16036367416381836}
I0202 18:51:00.438431 139923868813056 logging_writer.py:48] [5375] accumulated_eval_time=276.351180, accumulated_logging_time=0.160364, accumulated_submission_time=2557.313782, global_step=5375, preemption_count=0, score=2557.313782, test/accuracy=0.137000, test/loss=4.693284, test/num_examples=10000, total_duration=2834.125444, train/accuracy=0.190078, train/loss=4.193376, validation/accuracy=0.175740, validation/loss=4.286028, validation/num_examples=50000
I0202 18:51:10.637360 139923852027648 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.913306713104248, loss=6.429203987121582
I0202 18:51:54.095427 139923868813056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9006121754646301, loss=5.485767364501953
I0202 18:52:41.441672 139923852027648 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9013866782188416, loss=4.922328948974609
I0202 18:53:28.368537 139923868813056 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7712225914001465, loss=5.209306240081787
I0202 18:54:15.275103 139923852027648 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6053320169448853, loss=5.572266101837158
I0202 18:55:02.143718 139923868813056 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8212783336639404, loss=5.016424179077148
I0202 18:55:49.217868 139923852027648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8971454501152039, loss=5.3091816902160645
I0202 18:56:36.230636 139923868813056 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.7844105362892151, loss=4.726542949676514
I0202 18:57:23.386895 139923852027648 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.697371780872345, loss=5.365482807159424
I0202 18:58:00.489317 140085747812160 spec.py:321] Evaluating on the training split.
I0202 18:58:11.191787 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 18:58:42.433947 140085747812160 spec.py:349] Evaluating on the test split.
I0202 18:58:44.077381 140085747812160 submission_runner.py:408] Time since start: 3297.78s, 	Step: 6280, 	{'train/accuracy': 0.21013671159744263, 'train/loss': 4.0672478675842285, 'validation/accuracy': 0.19367998838424683, 'validation/loss': 4.1517534255981445, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 4.536256790161133, 'test/num_examples': 10000, 'score': 2977.3031663894653, 'total_duration': 3297.7814099788666, 'accumulated_submission_time': 2977.3031663894653, 'accumulated_eval_time': 319.9392533302307, 'accumulated_logging_time': 0.1883072853088379}
I0202 18:58:44.097031 139923868813056 logging_writer.py:48] [6280] accumulated_eval_time=319.939253, accumulated_logging_time=0.188307, accumulated_submission_time=2977.303166, global_step=6280, preemption_count=0, score=2977.303166, test/accuracy=0.152200, test/loss=4.536257, test/num_examples=10000, total_duration=3297.781410, train/accuracy=0.210137, train/loss=4.067248, validation/accuracy=0.193680, validation/loss=4.151753, validation/num_examples=50000
I0202 18:58:52.344200 139923852027648 logging_writer.py:48] [6300] global_step=6300, grad_norm=1.0037486553192139, loss=4.865729331970215
I0202 18:59:35.857161 139923868813056 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7763963341712952, loss=4.970251083374023
I0202 19:00:22.812785 139923852027648 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9319231510162354, loss=5.094143867492676
I0202 19:01:09.759557 139923868813056 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.792540967464447, loss=5.343972206115723
I0202 19:01:56.604547 139923852027648 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.6548689007759094, loss=6.297595024108887
I0202 19:02:43.753543 139923868813056 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7227131724357605, loss=5.057936668395996
I0202 19:03:30.645652 139923852027648 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.7641245126724243, loss=4.599417686462402
I0202 19:04:17.600109 139923868813056 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7192781567573547, loss=6.371228218078613
I0202 19:05:04.543512 139923852027648 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8461049795150757, loss=6.311277866363525
I0202 19:05:44.471335 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:05:55.528964 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:06:27.483660 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:06:29.127561 140085747812160 submission_runner.py:408] Time since start: 3762.83s, 	Step: 7187, 	{'train/accuracy': 0.2340429574251175, 'train/loss': 3.8879802227020264, 'validation/accuracy': 0.21383999288082123, 'validation/loss': 4.022542476654053, 'validation/num_examples': 50000, 'test/accuracy': 0.1632000058889389, 'test/loss': 4.448765754699707, 'test/num_examples': 10000, 'score': 3397.616981983185, 'total_duration': 3762.831591129303, 'accumulated_submission_time': 3397.616981983185, 'accumulated_eval_time': 364.59548354148865, 'accumulated_logging_time': 0.21813154220581055}
I0202 19:06:29.144591 139923868813056 logging_writer.py:48] [7187] accumulated_eval_time=364.595484, accumulated_logging_time=0.218132, accumulated_submission_time=3397.616982, global_step=7187, preemption_count=0, score=3397.616982, test/accuracy=0.163200, test/loss=4.448766, test/num_examples=10000, total_duration=3762.831591, train/accuracy=0.234043, train/loss=3.887980, validation/accuracy=0.213840, validation/loss=4.022542, validation/num_examples=50000
I0202 19:06:34.638983 139923852027648 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.617676317691803, loss=6.156827926635742
I0202 19:07:17.778365 139923868813056 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6328698992729187, loss=6.137658596038818
I0202 19:08:04.692960 139923852027648 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.08595871925354, loss=4.6488423347473145
I0202 19:08:51.760134 139923868813056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7515660524368286, loss=5.3527045249938965
I0202 19:09:38.457336 139923852027648 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.769987165927887, loss=4.546300411224365
I0202 19:10:25.308767 139923868813056 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6544151306152344, loss=4.634648323059082
I0202 19:11:12.165718 139923852027648 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.8754724860191345, loss=4.635442733764648
I0202 19:11:58.892253 139923868813056 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.5972992181777954, loss=5.523309230804443
I0202 19:12:46.157597 139923852027648 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9660841822624207, loss=4.555749893188477
I0202 19:13:29.383279 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:13:39.754047 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:14:07.916874 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:14:09.560889 140085747812160 submission_runner.py:408] Time since start: 4223.26s, 	Step: 8094, 	{'train/accuracy': 0.24208983778953552, 'train/loss': 3.8262104988098145, 'validation/accuracy': 0.22335998713970184, 'validation/loss': 3.9299376010894775, 'validation/num_examples': 50000, 'test/accuracy': 0.16870000958442688, 'test/loss': 4.404850482940674, 'test/num_examples': 10000, 'score': 3817.7951107025146, 'total_duration': 4223.264901161194, 'accumulated_submission_time': 3817.7951107025146, 'accumulated_eval_time': 404.7730875015259, 'accumulated_logging_time': 0.24477744102478027}
I0202 19:14:09.577831 139923868813056 logging_writer.py:48] [8094] accumulated_eval_time=404.773088, accumulated_logging_time=0.244777, accumulated_submission_time=3817.795111, global_step=8094, preemption_count=0, score=3817.795111, test/accuracy=0.168700, test/loss=4.404850, test/num_examples=10000, total_duration=4223.264901, train/accuracy=0.242090, train/loss=3.826210, validation/accuracy=0.223360, validation/loss=3.929938, validation/num_examples=50000
I0202 19:14:12.334927 139923852027648 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.8514585494995117, loss=4.489136695861816
I0202 19:14:54.640478 139923868813056 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5838727355003357, loss=6.168939590454102
I0202 19:15:41.522366 139923852027648 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8301379680633545, loss=4.673093318939209
I0202 19:16:28.318805 139923868813056 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.7921016812324524, loss=4.473904132843018
I0202 19:17:15.153169 139923852027648 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8367611169815063, loss=4.4517669677734375
I0202 19:18:01.997676 139923868813056 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.7842967510223389, loss=4.673301696777344
I0202 19:18:49.000077 139923852027648 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8155746459960938, loss=5.052262306213379
I0202 19:19:35.900952 139923868813056 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.0183418989181519, loss=4.537609577178955
I0202 19:20:22.814296 139923852027648 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6237260103225708, loss=5.602407932281494
I0202 19:21:09.760076 139923868813056 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.8491847515106201, loss=4.538715362548828
I0202 19:21:09.779178 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:21:20.854738 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:21:51.937808 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:21:53.586609 140085747812160 submission_runner.py:408] Time since start: 4687.29s, 	Step: 9001, 	{'train/accuracy': 0.27150389552116394, 'train/loss': 3.5727624893188477, 'validation/accuracy': 0.24845999479293823, 'validation/loss': 3.701709032058716, 'validation/num_examples': 50000, 'test/accuracy': 0.19030000269412994, 'test/loss': 4.200378894805908, 'test/num_examples': 10000, 'score': 4237.933657169342, 'total_duration': 4687.290641784668, 'accumulated_submission_time': 4237.933657169342, 'accumulated_eval_time': 448.5805284976959, 'accumulated_logging_time': 0.27434587478637695}
I0202 19:21:53.602675 139923852027648 logging_writer.py:48] [9001] accumulated_eval_time=448.580528, accumulated_logging_time=0.274346, accumulated_submission_time=4237.933657, global_step=9001, preemption_count=0, score=4237.933657, test/accuracy=0.190300, test/loss=4.200379, test/num_examples=10000, total_duration=4687.290642, train/accuracy=0.271504, train/loss=3.572762, validation/accuracy=0.248460, validation/loss=3.701709, validation/num_examples=50000
I0202 19:22:35.558222 139923868813056 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7528808116912842, loss=6.230942726135254
I0202 19:23:22.545758 139923852027648 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7241116762161255, loss=6.1556172370910645
I0202 19:24:09.462848 139923868813056 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5652289986610413, loss=5.911603927612305
I0202 19:24:56.169154 139923852027648 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.7638935446739197, loss=6.2479376792907715
I0202 19:25:43.046384 139923868813056 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6310640573501587, loss=6.253528118133545
I0202 19:26:29.902790 139923852027648 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8982667326927185, loss=4.67203426361084
I0202 19:27:16.651598 139923868813056 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.8671069145202637, loss=4.594932556152344
I0202 19:28:03.851970 139923852027648 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9878972172737122, loss=4.496833801269531
I0202 19:28:50.979719 139923868813056 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9646971225738525, loss=4.507008075714111
I0202 19:28:54.017272 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:29:05.215801 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:29:34.474957 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:29:36.123392 140085747812160 submission_runner.py:408] Time since start: 5149.83s, 	Step: 9908, 	{'train/accuracy': 0.2799023389816284, 'train/loss': 3.503488063812256, 'validation/accuracy': 0.2557999789714813, 'validation/loss': 3.6524834632873535, 'validation/num_examples': 50000, 'test/accuracy': 0.2045000046491623, 'test/loss': 4.137157917022705, 'test/num_examples': 10000, 'score': 4658.288609981537, 'total_duration': 5149.827425003052, 'accumulated_submission_time': 4658.288609981537, 'accumulated_eval_time': 490.68664383888245, 'accumulated_logging_time': 0.2993443012237549}
I0202 19:29:36.141626 139923852027648 logging_writer.py:48] [9908] accumulated_eval_time=490.686644, accumulated_logging_time=0.299344, accumulated_submission_time=4658.288610, global_step=9908, preemption_count=0, score=4658.288610, test/accuracy=0.204500, test/loss=4.137158, test/num_examples=10000, total_duration=5149.827425, train/accuracy=0.279902, train/loss=3.503488, validation/accuracy=0.255800, validation/loss=3.652483, validation/num_examples=50000
I0202 19:30:15.728600 139923868813056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7439323663711548, loss=4.435141086578369
I0202 19:31:02.080755 139923852027648 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.666938304901123, loss=5.299210548400879
I0202 19:31:49.856040 139923868813056 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6267430782318115, loss=5.6868815422058105
I0202 19:32:37.189162 139923852027648 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.7999900579452515, loss=4.363849639892578
I0202 19:33:24.529540 139923868813056 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9393424391746521, loss=4.780535697937012
I0202 19:34:11.843179 139923852027648 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8165744543075562, loss=4.379303455352783
I0202 19:34:58.804343 139923868813056 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8542630672454834, loss=4.717287063598633
I0202 19:35:46.060522 139923852027648 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.7721536159515381, loss=4.99440336227417
I0202 19:36:33.388312 139923868813056 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.801071047782898, loss=6.2392191886901855
I0202 19:36:36.377220 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:36:46.940831 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:37:13.892865 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:37:15.539212 140085747812160 submission_runner.py:408] Time since start: 5609.24s, 	Step: 10808, 	{'train/accuracy': 0.2586914002895355, 'train/loss': 3.687410593032837, 'validation/accuracy': 0.2386999875307083, 'validation/loss': 3.8200554847717285, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.281069278717041, 'test/num_examples': 10000, 'score': 5077.988896608353, 'total_duration': 5609.243244171143, 'accumulated_submission_time': 5077.988896608353, 'accumulated_eval_time': 529.848639011383, 'accumulated_logging_time': 0.8035287857055664}
I0202 19:37:15.556325 139923852027648 logging_writer.py:48] [10808] accumulated_eval_time=529.848639, accumulated_logging_time=0.803529, accumulated_submission_time=5077.988897, global_step=10808, preemption_count=0, score=5077.988897, test/accuracy=0.183800, test/loss=4.281069, test/num_examples=10000, total_duration=5609.243244, train/accuracy=0.258691, train/loss=3.687411, validation/accuracy=0.238700, validation/loss=3.820055, validation/num_examples=50000
I0202 19:37:54.323601 139923868813056 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.851192057132721, loss=4.456387042999268
I0202 19:38:41.661809 139923852027648 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.746942937374115, loss=4.689990520477295
I0202 19:39:28.760267 139923868813056 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.850064218044281, loss=4.41528844833374
I0202 19:40:15.744480 139923852027648 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6849278211593628, loss=5.982004642486572
I0202 19:41:02.674083 139923868813056 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0889766216278076, loss=4.641672611236572
I0202 19:41:49.426275 139923852027648 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.5850856304168701, loss=5.905966758728027
I0202 19:42:36.627765 139923868813056 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9213523864746094, loss=4.34758186340332
I0202 19:43:23.745057 139923852027648 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.7876962423324585, loss=4.280149936676025
I0202 19:44:10.850962 139923868813056 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.0696557760238647, loss=4.408746719360352
I0202 19:44:15.775702 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:44:26.382285 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:44:59.067760 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:45:00.719760 140085747812160 submission_runner.py:408] Time since start: 6074.42s, 	Step: 11712, 	{'train/accuracy': 0.29048827290534973, 'train/loss': 3.440134286880493, 'validation/accuracy': 0.27008000016212463, 'validation/loss': 3.5581839084625244, 'validation/num_examples': 50000, 'test/accuracy': 0.20440000295639038, 'test/loss': 4.096872806549072, 'test/num_examples': 10000, 'score': 5498.148392438889, 'total_duration': 6074.42378115654, 'accumulated_submission_time': 5498.148392438889, 'accumulated_eval_time': 574.7927012443542, 'accumulated_logging_time': 0.8307521343231201}
I0202 19:45:00.741024 139923852027648 logging_writer.py:48] [11712] accumulated_eval_time=574.792701, accumulated_logging_time=0.830752, accumulated_submission_time=5498.148392, global_step=11712, preemption_count=0, score=5498.148392, test/accuracy=0.204400, test/loss=4.096873, test/num_examples=10000, total_duration=6074.423781, train/accuracy=0.290488, train/loss=3.440134, validation/accuracy=0.270080, validation/loss=3.558184, validation/num_examples=50000
I0202 19:45:37.466700 139923868813056 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9459999203681946, loss=4.50623893737793
I0202 19:46:24.554295 139923852027648 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.2871575355529785, loss=4.34904670715332
I0202 19:47:11.612113 139923868813056 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9702642560005188, loss=4.382848262786865
I0202 19:47:58.771817 139923852027648 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.1354142427444458, loss=4.407188415527344
I0202 19:48:45.950162 139923868813056 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8700795769691467, loss=6.053472518920898
I0202 19:49:33.208772 139923852027648 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.8712408542633057, loss=5.500260353088379
I0202 19:50:20.457534 139923868813056 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5934176445007324, loss=6.0205841064453125
I0202 19:51:07.587369 139923852027648 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.0783069133758545, loss=4.4243268966674805
I0202 19:51:54.764737 139923868813056 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.7718560695648193, loss=6.152782440185547
I0202 19:52:00.990788 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:52:11.493425 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 19:52:42.485811 140085747812160 spec.py:349] Evaluating on the test split.
I0202 19:52:44.115226 140085747812160 submission_runner.py:408] Time since start: 6537.82s, 	Step: 12615, 	{'train/accuracy': 0.2845117151737213, 'train/loss': 3.4956023693084717, 'validation/accuracy': 0.262939989566803, 'validation/loss': 3.6273386478424072, 'validation/num_examples': 50000, 'test/accuracy': 0.20280000567436218, 'test/loss': 4.120352745056152, 'test/num_examples': 10000, 'score': 5918.338252067566, 'total_duration': 6537.819251060486, 'accumulated_submission_time': 5918.338252067566, 'accumulated_eval_time': 617.9171347618103, 'accumulated_logging_time': 0.8616728782653809}
I0202 19:52:44.137727 139923852027648 logging_writer.py:48] [12615] accumulated_eval_time=617.917135, accumulated_logging_time=0.861673, accumulated_submission_time=5918.338252, global_step=12615, preemption_count=0, score=5918.338252, test/accuracy=0.202800, test/loss=4.120353, test/num_examples=10000, total_duration=6537.819251, train/accuracy=0.284512, train/loss=3.495602, validation/accuracy=0.262940, validation/loss=3.627339, validation/num_examples=50000
I0202 19:53:19.592529 139923868813056 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.8616753816604614, loss=4.46310567855835
I0202 19:54:06.445641 139923852027648 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9672055244445801, loss=4.342987060546875
I0202 19:54:53.509965 139923868813056 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.8563319444656372, loss=4.199678897857666
I0202 19:55:40.464905 139923852027648 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6699334979057312, loss=6.141406536102295
I0202 19:56:27.312802 139923868813056 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.8575717210769653, loss=4.4777326583862305
I0202 19:57:14.213220 139923852027648 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.6282678842544556, loss=4.490849494934082
I0202 19:58:01.394304 139923868813056 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.91841721534729, loss=4.220906734466553
I0202 19:58:48.225513 139923852027648 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.8529114723205566, loss=4.475508689880371
I0202 19:59:35.316398 139923868813056 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7884413003921509, loss=5.348583221435547
I0202 19:59:44.421274 140085747812160 spec.py:321] Evaluating on the training split.
I0202 19:59:55.536238 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:00:23.283364 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:00:24.924646 140085747812160 submission_runner.py:408] Time since start: 6998.63s, 	Step: 13521, 	{'train/accuracy': 0.33222654461860657, 'train/loss': 3.20548152923584, 'validation/accuracy': 0.2886999845504761, 'validation/loss': 3.4614808559417725, 'validation/num_examples': 50000, 'test/accuracy': 0.2183000147342682, 'test/loss': 3.9869000911712646, 'test/num_examples': 10000, 'score': 6338.559593200684, 'total_duration': 6998.628676176071, 'accumulated_submission_time': 6338.559593200684, 'accumulated_eval_time': 658.4205052852631, 'accumulated_logging_time': 0.8963735103607178}
I0202 20:00:24.944046 139923852027648 logging_writer.py:48] [13521] accumulated_eval_time=658.420505, accumulated_logging_time=0.896374, accumulated_submission_time=6338.559593, global_step=13521, preemption_count=0, score=6338.559593, test/accuracy=0.218300, test/loss=3.986900, test/num_examples=10000, total_duration=6998.628676, train/accuracy=0.332227, train/loss=3.205482, validation/accuracy=0.288700, validation/loss=3.461481, validation/num_examples=50000
I0202 20:00:57.528900 139923868813056 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9596022367477417, loss=4.234277725219727
I0202 20:01:44.301873 139923852027648 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9073269963264465, loss=4.247072219848633
I0202 20:02:31.401108 139923868813056 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.8328937292098999, loss=4.2370686531066895
I0202 20:03:18.322863 139923852027648 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.7811647653579712, loss=5.010660171508789
I0202 20:04:05.253061 139923868813056 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7207399010658264, loss=4.501763343811035
I0202 20:04:52.252722 139923852027648 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.0163160562515259, loss=4.290489673614502
I0202 20:05:39.119425 139923868813056 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9744483828544617, loss=4.256887435913086
I0202 20:06:26.209701 139923852027648 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.7348254919052124, loss=4.331753253936768
I0202 20:07:12.948080 139923868813056 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.755321741104126, loss=5.723023891448975
I0202 20:07:25.342459 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:07:36.121601 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:08:03.283406 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:08:04.930145 140085747812160 submission_runner.py:408] Time since start: 7458.63s, 	Step: 14428, 	{'train/accuracy': 0.30156248807907104, 'train/loss': 3.388521671295166, 'validation/accuracy': 0.28001999855041504, 'validation/loss': 3.510899305343628, 'validation/num_examples': 50000, 'test/accuracy': 0.21120001375675201, 'test/loss': 4.033987522125244, 'test/num_examples': 10000, 'score': 6758.897345304489, 'total_duration': 7458.634178161621, 'accumulated_submission_time': 6758.897345304489, 'accumulated_eval_time': 698.0081906318665, 'accumulated_logging_time': 0.9258718490600586}
I0202 20:08:04.948650 139923852027648 logging_writer.py:48] [14428] accumulated_eval_time=698.008191, accumulated_logging_time=0.925872, accumulated_submission_time=6758.897345, global_step=14428, preemption_count=0, score=6758.897345, test/accuracy=0.211200, test/loss=4.033988, test/num_examples=10000, total_duration=7458.634178, train/accuracy=0.301562, train/loss=3.388522, validation/accuracy=0.280020, validation/loss=3.510899, validation/num_examples=50000
I0202 20:08:34.245202 139923868813056 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.3661305904388428, loss=4.217580795288086
I0202 20:09:20.965519 139923852027648 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.9862430691719055, loss=4.254521369934082
I0202 20:10:07.832708 139923868813056 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.9103894829750061, loss=5.9654927253723145
I0202 20:10:54.653258 139923852027648 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0785431861877441, loss=4.172929286956787
I0202 20:11:41.378659 139923868813056 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6937627792358398, loss=5.857125759124756
I0202 20:12:28.580464 139923852027648 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6779666543006897, loss=6.086543083190918
I0202 20:13:15.578406 139923868813056 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0349057912826538, loss=4.2200751304626465
I0202 20:14:02.604386 139923852027648 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.1155248880386353, loss=4.19309139251709
I0202 20:14:49.788827 139923868813056 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9337533116340637, loss=4.4711833000183105
I0202 20:15:04.936276 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:15:15.597606 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:15:47.975642 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:15:49.631926 140085747812160 submission_runner.py:408] Time since start: 7923.34s, 	Step: 15334, 	{'train/accuracy': 0.31974607706069946, 'train/loss': 3.255380868911743, 'validation/accuracy': 0.29853999614715576, 'validation/loss': 3.3957231044769287, 'validation/num_examples': 50000, 'test/accuracy': 0.2208000123500824, 'test/loss': 3.9570510387420654, 'test/num_examples': 10000, 'score': 7178.821423768997, 'total_duration': 7923.335958003998, 'accumulated_submission_time': 7178.821423768997, 'accumulated_eval_time': 742.7038342952728, 'accumulated_logging_time': 0.957528829574585}
I0202 20:15:49.650991 139923852027648 logging_writer.py:48] [15334] accumulated_eval_time=742.703834, accumulated_logging_time=0.957529, accumulated_submission_time=7178.821424, global_step=15334, preemption_count=0, score=7178.821424, test/accuracy=0.220800, test/loss=3.957051, test/num_examples=10000, total_duration=7923.335958, train/accuracy=0.319746, train/loss=3.255381, validation/accuracy=0.298540, validation/loss=3.395723, validation/num_examples=50000
I0202 20:16:15.946370 139923868813056 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.8436702489852905, loss=4.197474479675293
I0202 20:17:02.432590 139923852027648 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8260343670845032, loss=4.415424346923828
I0202 20:17:49.413459 139923868813056 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.9659349918365479, loss=4.154888153076172
I0202 20:18:36.145538 139923852027648 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9489647150039673, loss=4.500919342041016
I0202 20:19:22.827328 139923868813056 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7582674026489258, loss=5.442158222198486
I0202 20:20:09.966588 139923852027648 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.1784043312072754, loss=4.16875696182251
I0202 20:20:56.776228 139923868813056 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0756639242172241, loss=4.161572456359863
I0202 20:21:43.567975 139923852027648 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.8657564520835876, loss=4.257573127746582
I0202 20:22:30.658320 139923868813056 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0099247694015503, loss=4.765454292297363
I0202 20:22:49.670666 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:23:00.378331 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:23:32.124227 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:23:33.761246 140085747812160 submission_runner.py:408] Time since start: 8387.47s, 	Step: 16242, 	{'train/accuracy': 0.3459570109844208, 'train/loss': 3.120103120803833, 'validation/accuracy': 0.29927998781204224, 'validation/loss': 3.3753139972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 3.943704128265381, 'test/num_examples': 10000, 'score': 7598.78031373024, 'total_duration': 8387.46527838707, 'accumulated_submission_time': 7598.78031373024, 'accumulated_eval_time': 786.7944359779358, 'accumulated_logging_time': 0.9867620468139648}
I0202 20:23:33.779452 139923852027648 logging_writer.py:48] [16242] accumulated_eval_time=786.794436, accumulated_logging_time=0.986762, accumulated_submission_time=7598.780314, global_step=16242, preemption_count=0, score=7598.780314, test/accuracy=0.225100, test/loss=3.943704, test/num_examples=10000, total_duration=8387.465278, train/accuracy=0.345957, train/loss=3.120103, validation/accuracy=0.299280, validation/loss=3.375314, validation/num_examples=50000
I0202 20:23:56.952133 139923868813056 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.8750512003898621, loss=4.244743347167969
I0202 20:24:42.915365 139923852027648 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.1025292873382568, loss=4.393198013305664
I0202 20:25:29.897954 139923868813056 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7658950090408325, loss=4.518777847290039
I0202 20:26:17.030194 139923852027648 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0000967979431152, loss=4.197617530822754
I0202 20:27:04.046344 139923868813056 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9275785684585571, loss=4.597532749176025
I0202 20:27:50.901475 139923852027648 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.807206928730011, loss=4.722330093383789
I0202 20:28:37.979545 139923868813056 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.8133741021156311, loss=4.141787528991699
I0202 20:29:24.880431 139923852027648 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7676934003829956, loss=4.936509609222412
I0202 20:30:12.023016 139923868813056 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9469828605651855, loss=4.120988845825195
I0202 20:30:34.078302 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:30:45.705931 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:31:09.486553 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:31:11.131389 140085747812160 submission_runner.py:408] Time since start: 8844.84s, 	Step: 17149, 	{'train/accuracy': 0.33298826217651367, 'train/loss': 3.1438841819763184, 'validation/accuracy': 0.3075200021266937, 'validation/loss': 3.2924156188964844, 'validation/num_examples': 50000, 'test/accuracy': 0.23760001361370087, 'test/loss': 3.8583736419677734, 'test/num_examples': 10000, 'score': 8019.018817424774, 'total_duration': 8844.835416793823, 'accumulated_submission_time': 8019.018817424774, 'accumulated_eval_time': 823.8475241661072, 'accumulated_logging_time': 1.0148942470550537}
I0202 20:31:11.152149 139923852027648 logging_writer.py:48] [17149] accumulated_eval_time=823.847524, accumulated_logging_time=1.014894, accumulated_submission_time=8019.018817, global_step=17149, preemption_count=0, score=8019.018817, test/accuracy=0.237600, test/loss=3.858374, test/num_examples=10000, total_duration=8844.835417, train/accuracy=0.332988, train/loss=3.143884, validation/accuracy=0.307520, validation/loss=3.292416, validation/num_examples=50000
I0202 20:31:31.570765 139923868813056 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9223108291625977, loss=4.181518077850342
I0202 20:32:17.432229 139923852027648 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.9709132313728333, loss=4.223994255065918
I0202 20:33:05.053831 139923868813056 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.9788287281990051, loss=4.26393461227417
I0202 20:33:51.824802 139923852027648 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7111505270004272, loss=4.929941654205322
I0202 20:34:38.768847 139923868813056 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9057176113128662, loss=5.634645462036133
I0202 20:35:25.822176 139923852027648 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8708754777908325, loss=4.217662811279297
I0202 20:36:12.866383 139923868813056 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.8970999717712402, loss=4.229166507720947
I0202 20:37:00.016844 139923852027648 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7170459032058716, loss=6.117969512939453
I0202 20:37:47.052618 139923868813056 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.934047520160675, loss=4.233334541320801
I0202 20:38:11.169461 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:38:21.588072 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:38:51.309840 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:38:52.949351 140085747812160 submission_runner.py:408] Time since start: 9306.65s, 	Step: 18053, 	{'train/accuracy': 0.3213476538658142, 'train/loss': 3.2263472080230713, 'validation/accuracy': 0.2976999878883362, 'validation/loss': 3.3574440479278564, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 3.882843255996704, 'test/num_examples': 10000, 'score': 8438.975415468216, 'total_duration': 9306.65334200859, 'accumulated_submission_time': 8438.975415468216, 'accumulated_eval_time': 865.6273958683014, 'accumulated_logging_time': 1.0453910827636719}
I0202 20:38:52.968866 139923852027648 logging_writer.py:48] [18053] accumulated_eval_time=865.627396, accumulated_logging_time=1.045391, accumulated_submission_time=8438.975415, global_step=18053, preemption_count=0, score=8438.975415, test/accuracy=0.234600, test/loss=3.882843, test/num_examples=10000, total_duration=9306.653342, train/accuracy=0.321348, train/loss=3.226347, validation/accuracy=0.297700, validation/loss=3.357444, validation/num_examples=50000
I0202 20:39:11.801420 139923868813056 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.8465736508369446, loss=4.108053207397461
I0202 20:39:57.401323 139923852027648 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.2131792306900024, loss=4.2611894607543945
I0202 20:40:44.304046 139923868813056 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.8956283926963806, loss=4.173303604125977
I0202 20:41:30.911998 139923852027648 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1357977390289307, loss=4.087900161743164
I0202 20:42:17.905381 139923868813056 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7622826099395752, loss=4.2775139808654785
I0202 20:43:05.018922 139923852027648 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8490252494812012, loss=4.863816261291504
I0202 20:43:51.994741 139923868813056 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.7023904919624329, loss=6.020734786987305
I0202 20:44:38.772979 139923852027648 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.098463773727417, loss=4.131613254547119
I0202 20:45:25.695644 139923868813056 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.932525634765625, loss=4.425482273101807
I0202 20:45:53.169641 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:46:03.834488 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:46:35.998117 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:46:37.639570 140085747812160 submission_runner.py:408] Time since start: 9771.34s, 	Step: 18960, 	{'train/accuracy': 0.34953123331069946, 'train/loss': 3.1147797107696533, 'validation/accuracy': 0.30761998891830444, 'validation/loss': 3.3317410945892334, 'validation/num_examples': 50000, 'test/accuracy': 0.2371000051498413, 'test/loss': 3.893336057662964, 'test/num_examples': 10000, 'score': 8859.113800764084, 'total_duration': 9771.343579053879, 'accumulated_submission_time': 8859.113800764084, 'accumulated_eval_time': 910.0973136425018, 'accumulated_logging_time': 1.0767600536346436}
I0202 20:46:37.660314 139923852027648 logging_writer.py:48] [18960] accumulated_eval_time=910.097314, accumulated_logging_time=1.076760, accumulated_submission_time=8859.113801, global_step=18960, preemption_count=0, score=8859.113801, test/accuracy=0.237100, test/loss=3.893336, test/num_examples=10000, total_duration=9771.343579, train/accuracy=0.349531, train/loss=3.114780, validation/accuracy=0.307620, validation/loss=3.331741, validation/num_examples=50000
I0202 20:46:53.775365 139923868813056 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.1658953428268433, loss=4.118202209472656
I0202 20:47:38.754437 139923852027648 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.094839334487915, loss=4.101457118988037
I0202 20:48:25.663866 139923868813056 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.7495793104171753, loss=4.802416801452637
I0202 20:49:12.592324 139923852027648 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0469399690628052, loss=4.399057865142822
I0202 20:49:59.298735 139923868813056 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.8827124834060669, loss=4.357794284820557
I0202 20:50:46.469182 139923852027648 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9801931381225586, loss=4.192881107330322
I0202 20:51:33.311487 139923868813056 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9396905303001404, loss=4.153128623962402
I0202 20:52:20.212156 139923852027648 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0063588619232178, loss=4.8305840492248535
I0202 20:53:07.447672 139923868813056 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0108097791671753, loss=4.135188102722168
I0202 20:53:38.095430 140085747812160 spec.py:321] Evaluating on the training split.
I0202 20:53:48.938329 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 20:54:20.507076 140085747812160 spec.py:349] Evaluating on the test split.
I0202 20:54:22.148036 140085747812160 submission_runner.py:408] Time since start: 10235.85s, 	Step: 19867, 	{'train/accuracy': 0.3369531035423279, 'train/loss': 3.1348671913146973, 'validation/accuracy': 0.31793999671936035, 'validation/loss': 3.2607951164245605, 'validation/num_examples': 50000, 'test/accuracy': 0.2427000105381012, 'test/loss': 3.7915139198303223, 'test/num_examples': 10000, 'score': 9279.488339185715, 'total_duration': 10235.852065563202, 'accumulated_submission_time': 9279.488339185715, 'accumulated_eval_time': 954.1499533653259, 'accumulated_logging_time': 1.10762619972229}
I0202 20:54:22.168049 139923852027648 logging_writer.py:48] [19867] accumulated_eval_time=954.149953, accumulated_logging_time=1.107626, accumulated_submission_time=9279.488339, global_step=19867, preemption_count=0, score=9279.488339, test/accuracy=0.242700, test/loss=3.791514, test/num_examples=10000, total_duration=10235.852066, train/accuracy=0.336953, train/loss=3.134867, validation/accuracy=0.317940, validation/loss=3.260795, validation/num_examples=50000
I0202 20:54:35.522563 139923868813056 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0138498544692993, loss=4.104090213775635
I0202 20:55:19.842869 139923852027648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5715134739875793, loss=5.755465984344482
I0202 20:56:07.028182 139923868813056 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.9864421486854553, loss=4.2013115882873535
I0202 20:56:54.069663 139923852027648 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.7365614771842957, loss=5.40753698348999
I0202 20:57:41.102416 139923868813056 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.8639586567878723, loss=4.091579914093018
I0202 20:58:28.167677 139923852027648 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.8658311367034912, loss=4.162763595581055
I0202 20:59:15.376319 139923868813056 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9190977811813354, loss=3.9689688682556152
I0202 21:00:02.520617 139923852027648 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.8458260297775269, loss=4.293968200683594
I0202 21:00:49.645282 139923868813056 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.957926869392395, loss=6.115808963775635
I0202 21:01:22.343174 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:01:33.064035 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:02:04.003813 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:02:05.643130 140085747812160 submission_runner.py:408] Time since start: 10699.35s, 	Step: 20771, 	{'train/accuracy': 0.3226367235183716, 'train/loss': 3.2174956798553467, 'validation/accuracy': 0.2995399832725525, 'validation/loss': 3.355835199356079, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 3.92555832862854, 'test/num_examples': 10000, 'score': 9699.60234951973, 'total_duration': 10699.347157001495, 'accumulated_submission_time': 9699.60234951973, 'accumulated_eval_time': 997.4499151706696, 'accumulated_logging_time': 1.1384267807006836}
I0202 21:02:05.662672 139923852027648 logging_writer.py:48] [20771] accumulated_eval_time=997.449915, accumulated_logging_time=1.138427, accumulated_submission_time=9699.602350, global_step=20771, preemption_count=0, score=9699.602350, test/accuracy=0.229100, test/loss=3.925558, test/num_examples=10000, total_duration=10699.347157, train/accuracy=0.322637, train/loss=3.217496, validation/accuracy=0.299540, validation/loss=3.355835, validation/num_examples=50000
I0202 21:02:17.439121 139923868813056 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.5161876678466797, loss=5.892219543457031
I0202 21:03:01.983690 139923852027648 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.8637160658836365, loss=5.9777092933654785
I0202 21:03:49.306553 139923868813056 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8471542596817017, loss=5.008267879486084
I0202 21:04:36.367160 139923852027648 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.737349271774292, loss=5.8980231285095215
I0202 21:05:23.246293 139923868813056 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.7986677289009094, loss=4.188283920288086
I0202 21:06:10.214118 139923852027648 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.886464536190033, loss=4.013671398162842
I0202 21:06:57.123440 139923868813056 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9271022081375122, loss=5.503596305847168
I0202 21:07:44.153151 139923852027648 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.084656000137329, loss=4.1306962966918945
I0202 21:08:31.216118 139923868813056 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7864415645599365, loss=5.888020038604736
I0202 21:09:05.746148 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:09:16.525680 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:09:50.307977 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:09:51.939378 140085747812160 submission_runner.py:408] Time since start: 11165.64s, 	Step: 21675, 	{'train/accuracy': 0.3468359410762787, 'train/loss': 3.1015145778656006, 'validation/accuracy': 0.31501999497413635, 'validation/loss': 3.295905113220215, 'validation/num_examples': 50000, 'test/accuracy': 0.2485000044107437, 'test/loss': 3.8394834995269775, 'test/num_examples': 10000, 'score': 10119.622790336609, 'total_duration': 11165.643399238586, 'accumulated_submission_time': 10119.622790336609, 'accumulated_eval_time': 1043.6431443691254, 'accumulated_logging_time': 1.169325590133667}
I0202 21:09:51.959751 139923852027648 logging_writer.py:48] [21675] accumulated_eval_time=1043.643144, accumulated_logging_time=1.169326, accumulated_submission_time=10119.622790, global_step=21675, preemption_count=0, score=10119.622790, test/accuracy=0.248500, test/loss=3.839483, test/num_examples=10000, total_duration=11165.643399, train/accuracy=0.346836, train/loss=3.101515, validation/accuracy=0.315020, validation/loss=3.295905, validation/num_examples=50000
I0202 21:10:02.170135 139923868813056 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8720943331718445, loss=5.011794090270996
I0202 21:10:46.091645 139923852027648 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8648127913475037, loss=4.318512439727783
I0202 21:11:33.192194 139923868813056 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9239456653594971, loss=3.9542431831359863
I0202 21:12:20.302223 139923852027648 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8149366974830627, loss=5.146123886108398
I0202 21:13:07.355168 139923868813056 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.8641859292984009, loss=4.023528099060059
I0202 21:13:54.284995 139923852027648 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2726237773895264, loss=4.150646209716797
I0202 21:14:41.241923 139923868813056 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9533600211143494, loss=4.024147987365723
I0202 21:15:28.235783 139923852027648 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.8154482841491699, loss=5.980690956115723
I0202 21:16:15.125379 139923868813056 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6626675128936768, loss=5.979860305786133
I0202 21:16:52.321194 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:17:03.102110 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:17:35.265786 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:17:36.905406 140085747812160 submission_runner.py:408] Time since start: 11630.61s, 	Step: 22581, 	{'train/accuracy': 0.34675779938697815, 'train/loss': 3.073084592819214, 'validation/accuracy': 0.3256799876689911, 'validation/loss': 3.1851108074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.24720001220703125, 'test/loss': 3.7654662132263184, 'test/num_examples': 10000, 'score': 10539.923010349274, 'total_duration': 11630.60943365097, 'accumulated_submission_time': 10539.923010349274, 'accumulated_eval_time': 1088.2273619174957, 'accumulated_logging_time': 1.2005112171173096}
I0202 21:17:36.928354 139923852027648 logging_writer.py:48] [22581] accumulated_eval_time=1088.227362, accumulated_logging_time=1.200511, accumulated_submission_time=10539.923010, global_step=22581, preemption_count=0, score=10539.923010, test/accuracy=0.247200, test/loss=3.765466, test/num_examples=10000, total_duration=11630.609434, train/accuracy=0.346758, train/loss=3.073085, validation/accuracy=0.325680, validation/loss=3.185111, validation/num_examples=50000
I0202 21:17:44.777181 139923868813056 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.067954421043396, loss=4.7491021156311035
I0202 21:18:28.186823 139923852027648 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9629398584365845, loss=4.325220108032227
I0202 21:19:15.099685 139923868813056 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.8213071227073669, loss=4.3386030197143555
I0202 21:20:02.129619 139923852027648 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8175140023231506, loss=4.037875175476074
I0202 21:20:49.239646 139923868813056 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9976953864097595, loss=6.078519821166992
I0202 21:21:36.221663 139923852027648 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.0401875972747803, loss=3.957711696624756
I0202 21:22:23.136353 139923868813056 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.7974132895469666, loss=5.607370376586914
I0202 21:23:10.295739 139923852027648 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8652331233024597, loss=4.072364807128906
I0202 21:23:57.321822 139923868813056 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.1219569444656372, loss=4.499826431274414
I0202 21:24:36.959207 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:24:47.674218 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:25:21.949038 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:25:23.584143 140085747812160 submission_runner.py:408] Time since start: 12097.29s, 	Step: 23486, 	{'train/accuracy': 0.35316404700279236, 'train/loss': 3.0341224670410156, 'validation/accuracy': 0.33142000436782837, 'validation/loss': 3.17514705657959, 'validation/num_examples': 50000, 'test/accuracy': 0.2502000033855438, 'test/loss': 3.758253812789917, 'test/num_examples': 10000, 'score': 10959.892746925354, 'total_duration': 12097.288171768188, 'accumulated_submission_time': 10959.892746925354, 'accumulated_eval_time': 1134.8523106575012, 'accumulated_logging_time': 1.2339890003204346}
I0202 21:25:23.604669 139923852027648 logging_writer.py:48] [23486] accumulated_eval_time=1134.852311, accumulated_logging_time=1.233989, accumulated_submission_time=10959.892747, global_step=23486, preemption_count=0, score=10959.892747, test/accuracy=0.250200, test/loss=3.758254, test/num_examples=10000, total_duration=12097.288172, train/accuracy=0.353164, train/loss=3.034122, validation/accuracy=0.331420, validation/loss=3.175147, validation/num_examples=50000
I0202 21:25:29.494135 139923868813056 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8967546224594116, loss=4.047504901885986
I0202 21:26:12.781963 139923852027648 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.990895688533783, loss=4.057023525238037
I0202 21:26:59.784749 139923868813056 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.8737084269523621, loss=4.46782922744751
I0202 21:27:46.939062 139923852027648 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.732172429561615, loss=4.432346343994141
I0202 21:28:33.749297 139923868813056 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.8230865597724915, loss=6.056262969970703
I0202 21:29:20.944675 139923852027648 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9263190031051636, loss=4.649256229400635
I0202 21:30:07.795256 139923868813056 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.9305346012115479, loss=4.066893577575684
I0202 21:30:54.920369 139923852027648 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.8476685285568237, loss=5.968585014343262
I0202 21:31:42.166778 139923868813056 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.8388769030570984, loss=4.839404582977295
I0202 21:32:23.803506 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:32:34.582231 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:33:08.163200 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:33:09.800593 140085747812160 submission_runner.py:408] Time since start: 12563.50s, 	Step: 24390, 	{'train/accuracy': 0.37041014432907104, 'train/loss': 2.9094772338867188, 'validation/accuracy': 0.3361800014972687, 'validation/loss': 3.098283052444458, 'validation/num_examples': 50000, 'test/accuracy': 0.26080000400543213, 'test/loss': 3.6874732971191406, 'test/num_examples': 10000, 'score': 11380.030760765076, 'total_duration': 12563.504616975784, 'accumulated_submission_time': 11380.030760765076, 'accumulated_eval_time': 1180.8493909835815, 'accumulated_logging_time': 1.2654187679290771}
I0202 21:33:09.823803 139923852027648 logging_writer.py:48] [24390] accumulated_eval_time=1180.849391, accumulated_logging_time=1.265419, accumulated_submission_time=11380.030761, global_step=24390, preemption_count=0, score=11380.030761, test/accuracy=0.260800, test/loss=3.687473, test/num_examples=10000, total_duration=12563.504617, train/accuracy=0.370410, train/loss=2.909477, validation/accuracy=0.336180, validation/loss=3.098283, validation/num_examples=50000
I0202 21:33:14.137735 139923868813056 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8342240452766418, loss=5.178412914276123
I0202 21:33:56.802603 139923852027648 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8878768086433411, loss=4.029943466186523
I0202 21:34:43.958920 139923868813056 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.014495611190796, loss=4.0384979248046875
I0202 21:35:30.978086 139923852027648 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7230238318443298, loss=6.031301021575928
I0202 21:36:18.141342 139923868813056 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.019425392150879, loss=3.8753819465637207
I0202 21:37:04.898019 139923852027648 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.9446151852607727, loss=4.043076038360596
I0202 21:37:51.920310 139923868813056 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.79771488904953, loss=5.621114253997803
I0202 21:38:38.725659 139923852027648 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7238211631774902, loss=5.366373062133789
I0202 21:39:25.551934 139923868813056 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.076248049736023, loss=4.17984676361084
I0202 21:40:09.977820 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:40:20.558882 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:40:53.850919 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:40:55.491654 140085747812160 submission_runner.py:408] Time since start: 13029.20s, 	Step: 25296, 	{'train/accuracy': 0.3532421886920929, 'train/loss': 3.0145912170410156, 'validation/accuracy': 0.33399999141693115, 'validation/loss': 3.1400809288024902, 'validation/num_examples': 50000, 'test/accuracy': 0.25270000100135803, 'test/loss': 3.7258048057556152, 'test/num_examples': 10000, 'score': 11800.12370634079, 'total_duration': 13029.195685625076, 'accumulated_submission_time': 11800.12370634079, 'accumulated_eval_time': 1226.3632354736328, 'accumulated_logging_time': 1.2985377311706543}
I0202 21:40:55.512309 139923852027648 logging_writer.py:48] [25296] accumulated_eval_time=1226.363235, accumulated_logging_time=1.298538, accumulated_submission_time=11800.123706, global_step=25296, preemption_count=0, score=11800.123706, test/accuracy=0.252700, test/loss=3.725805, test/num_examples=10000, total_duration=13029.195686, train/accuracy=0.353242, train/loss=3.014591, validation/accuracy=0.334000, validation/loss=3.140081, validation/num_examples=50000
I0202 21:40:57.478157 139923868813056 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.0087569952011108, loss=4.073501110076904
I0202 21:41:39.958008 139923852027648 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.4674184322357178, loss=4.160226821899414
I0202 21:42:26.998682 139923868813056 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.9332013726234436, loss=4.690338134765625
I0202 21:43:13.946298 139923852027648 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.6633552312850952, loss=6.042229652404785
I0202 21:44:00.900308 139923868813056 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.9823881387710571, loss=4.2073187828063965
I0202 21:44:47.795557 139923852027648 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9023749828338623, loss=3.9195432662963867
I0202 21:45:34.758920 139923868813056 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.9538682103157043, loss=4.141796112060547
I0202 21:46:21.475510 139923852027648 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.78230220079422, loss=4.933389186859131
I0202 21:47:08.478280 139923868813056 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0862798690795898, loss=3.9265103340148926
I0202 21:47:55.466889 139923852027648 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1162612438201904, loss=4.050051689147949
I0202 21:47:56.137577 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:48:07.192167 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:48:39.917952 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:48:41.556967 140085747812160 submission_runner.py:408] Time since start: 13495.26s, 	Step: 26202, 	{'train/accuracy': 0.3569726347923279, 'train/loss': 3.0196990966796875, 'validation/accuracy': 0.3306399881839752, 'validation/loss': 3.152756690979004, 'validation/num_examples': 50000, 'test/accuracy': 0.2502000033855438, 'test/loss': 3.755117416381836, 'test/num_examples': 10000, 'score': 12220.688608169556, 'total_duration': 13495.26099729538, 'accumulated_submission_time': 12220.688608169556, 'accumulated_eval_time': 1271.7826147079468, 'accumulated_logging_time': 1.3286545276641846}
I0202 21:48:41.576203 139923868813056 logging_writer.py:48] [26202] accumulated_eval_time=1271.782615, accumulated_logging_time=1.328655, accumulated_submission_time=12220.688608, global_step=26202, preemption_count=0, score=12220.688608, test/accuracy=0.250200, test/loss=3.755117, test/num_examples=10000, total_duration=13495.260997, train/accuracy=0.356973, train/loss=3.019699, validation/accuracy=0.330640, validation/loss=3.152757, validation/num_examples=50000
I0202 21:49:23.126833 139923852027648 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.7731097340583801, loss=5.101512908935547
I0202 21:50:09.501745 139923868813056 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.8973048329353333, loss=4.990372657775879
I0202 21:50:56.663104 139923852027648 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0380470752716064, loss=4.064359664916992
I0202 21:51:43.487802 139923868813056 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0236772298812866, loss=4.008741855621338
I0202 21:52:30.689824 139923852027648 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.9489609599113464, loss=4.794493675231934
I0202 21:53:17.763513 139923868813056 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.146494746208191, loss=4.415168285369873
I0202 21:54:04.818075 139923852027648 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.9540621638298035, loss=3.878495454788208
I0202 21:54:51.491214 139923868813056 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.8630989789962769, loss=5.514673233032227
I0202 21:55:38.393538 139923852027648 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.7787084579467773, loss=5.316946983337402
I0202 21:55:41.895657 140085747812160 spec.py:321] Evaluating on the training split.
I0202 21:55:52.454704 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 21:56:27.700748 140085747812160 spec.py:349] Evaluating on the test split.
I0202 21:56:29.333688 140085747812160 submission_runner.py:408] Time since start: 13963.04s, 	Step: 27109, 	{'train/accuracy': 0.3746679723262787, 'train/loss': 2.8854188919067383, 'validation/accuracy': 0.3447200059890747, 'validation/loss': 3.0645339488983154, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.661961078643799, 'test/num_examples': 10000, 'score': 12640.947337388992, 'total_duration': 13963.037720918655, 'accumulated_submission_time': 12640.947337388992, 'accumulated_eval_time': 1319.2206687927246, 'accumulated_logging_time': 1.3581278324127197}
I0202 21:56:29.354858 139923868813056 logging_writer.py:48] [27109] accumulated_eval_time=1319.220669, accumulated_logging_time=1.358128, accumulated_submission_time=12640.947337, global_step=27109, preemption_count=0, score=12640.947337, test/accuracy=0.260600, test/loss=3.661961, test/num_examples=10000, total_duration=13963.037721, train/accuracy=0.374668, train/loss=2.885419, validation/accuracy=0.344720, validation/loss=3.064534, validation/num_examples=50000
I0202 21:57:07.793396 139923852027648 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.7845210433006287, loss=5.326094627380371
I0202 21:57:54.600269 139923868813056 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.8839861154556274, loss=3.8560311794281006
I0202 21:58:41.774400 139923852027648 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.7745388746261597, loss=5.575793266296387
I0202 21:59:28.831798 139923868813056 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.8056470155715942, loss=4.291279315948486
I0202 22:00:15.722117 139923852027648 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0058841705322266, loss=4.316660404205322
I0202 22:01:02.699059 139923868813056 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9650371670722961, loss=3.879101276397705
I0202 22:01:49.539724 139923852027648 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.9058525562286377, loss=3.950498342514038
I0202 22:02:36.554840 139923868813056 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.9057227373123169, loss=4.031497001647949
I0202 22:03:23.415506 139923852027648 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7005460858345032, loss=5.69795036315918
I0202 22:03:29.371996 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:03:39.995331 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:04:08.225750 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:04:09.864947 140085747812160 submission_runner.py:408] Time since start: 14423.57s, 	Step: 28014, 	{'train/accuracy': 0.36474609375, 'train/loss': 2.9812686443328857, 'validation/accuracy': 0.3385799825191498, 'validation/loss': 3.11944842338562, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.7341465950012207, 'test/num_examples': 10000, 'score': 13060.903878450394, 'total_duration': 14423.568979501724, 'accumulated_submission_time': 13060.903878450394, 'accumulated_eval_time': 1359.713604927063, 'accumulated_logging_time': 1.3895189762115479}
I0202 22:04:09.884602 139923868813056 logging_writer.py:48] [28014] accumulated_eval_time=1359.713605, accumulated_logging_time=1.389519, accumulated_submission_time=13060.903878, global_step=28014, preemption_count=0, score=13060.903878, test/accuracy=0.255600, test/loss=3.734147, test/num_examples=10000, total_duration=14423.568980, train/accuracy=0.364746, train/loss=2.981269, validation/accuracy=0.338580, validation/loss=3.119448, validation/num_examples=50000
I0202 22:04:45.853192 139923852027648 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1190640926361084, loss=3.9213733673095703
I0202 22:05:32.586157 139923868813056 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.9644594192504883, loss=3.9952034950256348
I0202 22:06:19.686245 139923852027648 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.025618314743042, loss=3.9329802989959717
I0202 22:07:06.372600 139923868813056 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.065575122833252, loss=4.3930816650390625
I0202 22:07:53.295823 139923852027648 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8386179208755493, loss=5.56565523147583
I0202 22:08:40.167463 139923868813056 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0211564302444458, loss=3.9351069927215576
I0202 22:09:27.198839 139923852027648 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9456438422203064, loss=3.988649845123291
I0202 22:10:14.273538 139923868813056 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0661029815673828, loss=4.01814603805542
I0202 22:11:01.051306 139923852027648 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.9932886958122253, loss=4.103701114654541
I0202 22:11:10.068898 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:11:20.721378 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:11:52.751487 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:11:54.392322 140085747812160 submission_runner.py:408] Time since start: 14888.10s, 	Step: 28921, 	{'train/accuracy': 0.3594140410423279, 'train/loss': 3.011570692062378, 'validation/accuracy': 0.33535999059677124, 'validation/loss': 3.157099723815918, 'validation/num_examples': 50000, 'test/accuracy': 0.26030001044273376, 'test/loss': 3.740095853805542, 'test/num_examples': 10000, 'score': 13481.02701640129, 'total_duration': 14888.096348762512, 'accumulated_submission_time': 13481.02701640129, 'accumulated_eval_time': 1404.037055015564, 'accumulated_logging_time': 1.4189238548278809}
I0202 22:11:54.416139 139923868813056 logging_writer.py:48] [28921] accumulated_eval_time=1404.037055, accumulated_logging_time=1.418924, accumulated_submission_time=13481.027016, global_step=28921, preemption_count=0, score=13481.027016, test/accuracy=0.260300, test/loss=3.740096, test/num_examples=10000, total_duration=14888.096349, train/accuracy=0.359414, train/loss=3.011571, validation/accuracy=0.335360, validation/loss=3.157100, validation/num_examples=50000
I0202 22:12:26.822388 139923852027648 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7860892415046692, loss=4.927190780639648
I0202 22:13:13.784130 139923868813056 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8204841017723083, loss=6.067852973937988
I0202 22:14:00.816170 139923852027648 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.0538934469223022, loss=3.870896339416504
I0202 22:14:47.824006 139923868813056 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.6290278434753418, loss=6.067741394042969
I0202 22:15:34.827871 139923852027648 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.8360273241996765, loss=4.706640243530273
I0202 22:16:21.798249 139923868813056 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8900874257087708, loss=4.587953567504883
I0202 22:17:08.642236 139923852027648 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.6728017926216125, loss=5.3896002769470215
I0202 22:17:55.444610 139923868813056 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.692183792591095, loss=6.037601470947266
I0202 22:18:42.651065 139923852027648 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.020196795463562, loss=3.8898818492889404
I0202 22:18:54.593855 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:19:05.374013 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:19:37.123662 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:19:38.768289 140085747812160 submission_runner.py:408] Time since start: 15352.47s, 	Step: 29827, 	{'train/accuracy': 0.3867773413658142, 'train/loss': 2.8524653911590576, 'validation/accuracy': 0.35624000430107117, 'validation/loss': 3.020305633544922, 'validation/num_examples': 50000, 'test/accuracy': 0.2722000181674957, 'test/loss': 3.626361608505249, 'test/num_examples': 10000, 'score': 13901.145128250122, 'total_duration': 15352.472299575806, 'accumulated_submission_time': 13901.145128250122, 'accumulated_eval_time': 1448.2114634513855, 'accumulated_logging_time': 1.4525518417358398}
I0202 22:19:38.796638 139923868813056 logging_writer.py:48] [29827] accumulated_eval_time=1448.211463, accumulated_logging_time=1.452552, accumulated_submission_time=13901.145128, global_step=29827, preemption_count=0, score=13901.145128, test/accuracy=0.272200, test/loss=3.626362, test/num_examples=10000, total_duration=15352.472300, train/accuracy=0.386777, train/loss=2.852465, validation/accuracy=0.356240, validation/loss=3.020306, validation/num_examples=50000
I0202 22:20:08.661460 139923852027648 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.968779444694519, loss=3.9799411296844482
I0202 22:20:55.422760 139923868813056 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.8207970857620239, loss=6.046365737915039
I0202 22:21:42.461767 139923852027648 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.8785218596458435, loss=3.97550892829895
I0202 22:22:29.445772 139923868813056 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.024369478225708, loss=4.5116071701049805
I0202 22:23:16.498838 139923852027648 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.0567857027053833, loss=3.9175901412963867
I0202 22:24:03.508975 139923868813056 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0005327463150024, loss=4.05667781829834
I0202 22:24:50.505643 139923852027648 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.2555034160614014, loss=4.005374908447266
I0202 22:25:37.388679 139923868813056 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.8221959471702576, loss=6.055506706237793
I0202 22:26:24.338394 139923852027648 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.8651386499404907, loss=3.7365219593048096
I0202 22:26:39.092753 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:26:49.733024 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:27:21.682683 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:27:23.325045 140085747812160 submission_runner.py:408] Time since start: 15817.03s, 	Step: 30733, 	{'train/accuracy': 0.40380859375, 'train/loss': 2.765493631362915, 'validation/accuracy': 0.35071998834609985, 'validation/loss': 3.034952402114868, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.6251919269561768, 'test/num_examples': 10000, 'score': 14321.3808157444, 'total_duration': 15817.029076099396, 'accumulated_submission_time': 14321.3808157444, 'accumulated_eval_time': 1492.4437718391418, 'accumulated_logging_time': 1.489807367324829}
I0202 22:27:23.346399 139923868813056 logging_writer.py:48] [30733] accumulated_eval_time=1492.443772, accumulated_logging_time=1.489807, accumulated_submission_time=14321.380816, global_step=30733, preemption_count=0, score=14321.380816, test/accuracy=0.269900, test/loss=3.625192, test/num_examples=10000, total_duration=15817.029076, train/accuracy=0.403809, train/loss=2.765494, validation/accuracy=0.350720, validation/loss=3.034952, validation/num_examples=50000
I0202 22:27:50.394242 139923852027648 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.9013754725456238, loss=3.8193020820617676
I0202 22:28:37.166969 139923868813056 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.117284893989563, loss=3.986337661743164
I0202 22:29:24.444880 139923852027648 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0403138399124146, loss=3.868501663208008
I0202 22:30:11.447382 139923868813056 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.0837093591690063, loss=3.8200154304504395
I0202 22:30:58.371419 139923852027648 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.007721185684204, loss=4.265631198883057
I0202 22:31:45.369586 139923868813056 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.8842687606811523, loss=4.1585822105407715
I0202 22:32:32.605431 139923852027648 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.835774838924408, loss=4.693437099456787
I0202 22:33:19.463687 139923868813056 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.061192512512207, loss=3.7613697052001953
I0202 22:34:06.468871 139923852027648 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.0432357788085938, loss=3.8795032501220703
I0202 22:34:23.596331 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:34:34.246206 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:35:06.358583 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:35:07.991358 140085747812160 submission_runner.py:408] Time since start: 16281.70s, 	Step: 31638, 	{'train/accuracy': 0.372871071100235, 'train/loss': 2.894007921218872, 'validation/accuracy': 0.3455199897289276, 'validation/loss': 3.053222179412842, 'validation/num_examples': 50000, 'test/accuracy': 0.26579999923706055, 'test/loss': 3.6384551525115967, 'test/num_examples': 10000, 'score': 14741.569679737091, 'total_duration': 16281.695380210876, 'accumulated_submission_time': 14741.569679737091, 'accumulated_eval_time': 1536.8387801647186, 'accumulated_logging_time': 1.5209715366363525}
I0202 22:35:08.014412 139923868813056 logging_writer.py:48] [31638] accumulated_eval_time=1536.838780, accumulated_logging_time=1.520972, accumulated_submission_time=14741.569680, global_step=31638, preemption_count=0, score=14741.569680, test/accuracy=0.265800, test/loss=3.638455, test/num_examples=10000, total_duration=16281.695380, train/accuracy=0.372871, train/loss=2.894008, validation/accuracy=0.345520, validation/loss=3.053222, validation/num_examples=50000
I0202 22:35:32.879362 139923852027648 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.9508322477340698, loss=3.8132729530334473
I0202 22:36:19.776214 139923868813056 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.8113701939582825, loss=3.832615375518799
I0202 22:37:07.001262 139923852027648 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9735880494117737, loss=3.8458003997802734
I0202 22:37:54.080344 139923868813056 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8175981640815735, loss=4.8224945068359375
I0202 22:38:40.968646 139923852027648 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0051625967025757, loss=3.8111391067504883
I0202 22:39:28.198083 139923868813056 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0333861112594604, loss=3.82468843460083
I0202 22:40:15.376208 139923852027648 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0216604471206665, loss=3.873575210571289
I0202 22:41:02.615132 139923868813056 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.9544930458068848, loss=4.392436981201172
I0202 22:41:49.606007 139923852027648 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.9626883864402771, loss=3.9720077514648438
I0202 22:42:08.232382 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:42:18.784053 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:42:50.354148 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:42:51.997969 140085747812160 submission_runner.py:408] Time since start: 16745.70s, 	Step: 32541, 	{'train/accuracy': 0.3836914002895355, 'train/loss': 2.8515477180480957, 'validation/accuracy': 0.35627999901771545, 'validation/loss': 3.018123149871826, 'validation/num_examples': 50000, 'test/accuracy': 0.27890002727508545, 'test/loss': 3.585503578186035, 'test/num_examples': 10000, 'score': 15161.727267503738, 'total_duration': 16745.701989650726, 'accumulated_submission_time': 15161.727267503738, 'accumulated_eval_time': 1580.6043591499329, 'accumulated_logging_time': 1.5537612438201904}
I0202 22:42:52.023290 139923868813056 logging_writer.py:48] [32541] accumulated_eval_time=1580.604359, accumulated_logging_time=1.553761, accumulated_submission_time=15161.727268, global_step=32541, preemption_count=0, score=15161.727268, test/accuracy=0.278900, test/loss=3.585504, test/num_examples=10000, total_duration=16745.701990, train/accuracy=0.383691, train/loss=2.851548, validation/accuracy=0.356280, validation/loss=3.018123, validation/num_examples=50000
I0202 22:43:15.579952 139923852027648 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.8093788623809814, loss=4.22210168838501
I0202 22:44:01.971604 139923868813056 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.8714665770530701, loss=4.289055824279785
I0202 22:44:49.362754 139923852027648 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0732828378677368, loss=4.064153671264648
I0202 22:45:36.334374 139923868813056 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.1179577112197876, loss=3.954890251159668
I0202 22:46:23.146690 139923852027648 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.0328119993209839, loss=3.855897903442383
I0202 22:47:10.316108 139923868813056 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.8859893083572388, loss=3.913235664367676
I0202 22:47:57.405569 139923852027648 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0894242525100708, loss=4.7134246826171875
I0202 22:48:44.373846 139923868813056 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.2985588312149048, loss=3.8836565017700195
I0202 22:49:31.421974 139923852027648 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.8654270172119141, loss=4.636870384216309
I0202 22:49:52.054905 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:50:03.041342 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:50:32.393804 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:50:34.036597 140085747812160 submission_runner.py:408] Time since start: 17207.74s, 	Step: 33446, 	{'train/accuracy': 0.4034960865974426, 'train/loss': 2.7900664806365967, 'validation/accuracy': 0.3520599901676178, 'validation/loss': 3.0779330730438232, 'validation/num_examples': 50000, 'test/accuracy': 0.2671000063419342, 'test/loss': 3.6647629737854004, 'test/num_examples': 10000, 'score': 15581.699176073074, 'total_duration': 17207.740622758865, 'accumulated_submission_time': 15581.699176073074, 'accumulated_eval_time': 1622.586046218872, 'accumulated_logging_time': 1.5887606143951416}
I0202 22:50:34.060528 139923868813056 logging_writer.py:48] [33446] accumulated_eval_time=1622.586046, accumulated_logging_time=1.588761, accumulated_submission_time=15581.699176, global_step=33446, preemption_count=0, score=15581.699176, test/accuracy=0.267100, test/loss=3.664763, test/num_examples=10000, total_duration=17207.740623, train/accuracy=0.403496, train/loss=2.790066, validation/accuracy=0.352060, validation/loss=3.077933, validation/num_examples=50000
I0202 22:50:55.649804 139923852027648 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.097102403640747, loss=3.967345714569092
I0202 22:51:41.696151 139923868813056 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.9092609286308289, loss=4.293970108032227
I0202 22:52:29.112781 139923852027648 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0450102090835571, loss=3.987672805786133
I0202 22:53:16.312983 139923868813056 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.8712443709373474, loss=3.8722476959228516
I0202 22:54:03.341175 139923852027648 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9055079817771912, loss=5.41352653503418
I0202 22:54:50.263032 139923868813056 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.2108855247497559, loss=3.7716002464294434
I0202 22:55:37.367042 139923852027648 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.7897270917892456, loss=5.736479759216309
I0202 22:56:24.314294 139923868813056 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8330646753311157, loss=6.027695655822754
I0202 22:57:11.679536 139923852027648 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.1664786338806152, loss=3.7982001304626465
I0202 22:57:34.195913 140085747812160 spec.py:321] Evaluating on the training split.
I0202 22:57:44.632982 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 22:58:14.273404 140085747812160 spec.py:349] Evaluating on the test split.
I0202 22:58:15.914221 140085747812160 submission_runner.py:408] Time since start: 17669.62s, 	Step: 34350, 	{'train/accuracy': 0.37398436665534973, 'train/loss': 2.903052568435669, 'validation/accuracy': 0.3515399992465973, 'validation/loss': 3.0420234203338623, 'validation/num_examples': 50000, 'test/accuracy': 0.2727999985218048, 'test/loss': 3.618873119354248, 'test/num_examples': 10000, 'score': 16001.772747516632, 'total_duration': 17669.6182513237, 'accumulated_submission_time': 16001.772747516632, 'accumulated_eval_time': 1664.3043756484985, 'accumulated_logging_time': 1.623687982559204}
I0202 22:58:15.935922 139923868813056 logging_writer.py:48] [34350] accumulated_eval_time=1664.304376, accumulated_logging_time=1.623688, accumulated_submission_time=16001.772748, global_step=34350, preemption_count=0, score=16001.772748, test/accuracy=0.272800, test/loss=3.618873, test/num_examples=10000, total_duration=17669.618251, train/accuracy=0.373984, train/loss=2.903053, validation/accuracy=0.351540, validation/loss=3.042023, validation/num_examples=50000
I0202 22:58:35.957732 139923852027648 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.9425028562545776, loss=4.334894180297852
I0202 22:59:21.651560 139923868813056 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7803127765655518, loss=4.263714790344238
I0202 23:00:08.754679 139923852027648 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.065366268157959, loss=4.065796852111816
I0202 23:00:55.801097 139923868813056 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.034497618675232, loss=3.9185280799865723
I0202 23:01:42.789920 139923852027648 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.9308786988258362, loss=3.7883317470550537
I0202 23:02:29.766583 139923868813056 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.8798320293426514, loss=5.831545352935791
I0202 23:03:16.740886 139923852027648 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.0430490970611572, loss=3.802905321121216
I0202 23:04:03.475242 139923868813056 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1721112728118896, loss=3.8750200271606445
I0202 23:04:50.283260 139923852027648 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.6893065571784973, loss=5.934709548950195
I0202 23:05:16.360141 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:05:26.997965 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:05:54.261555 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:05:55.898508 140085747812160 submission_runner.py:408] Time since start: 18129.60s, 	Step: 35257, 	{'train/accuracy': 0.3941406309604645, 'train/loss': 2.7809207439422607, 'validation/accuracy': 0.3646000027656555, 'validation/loss': 2.946488380432129, 'validation/num_examples': 50000, 'test/accuracy': 0.281000018119812, 'test/loss': 3.570101261138916, 'test/num_examples': 10000, 'score': 16422.13542985916, 'total_duration': 18129.602539777756, 'accumulated_submission_time': 16422.13542985916, 'accumulated_eval_time': 1703.8427374362946, 'accumulated_logging_time': 1.655987024307251}
I0202 23:05:55.920121 139923868813056 logging_writer.py:48] [35257] accumulated_eval_time=1703.842737, accumulated_logging_time=1.655987, accumulated_submission_time=16422.135430, global_step=35257, preemption_count=0, score=16422.135430, test/accuracy=0.281000, test/loss=3.570101, test/num_examples=10000, total_duration=18129.602540, train/accuracy=0.394141, train/loss=2.780921, validation/accuracy=0.364600, validation/loss=2.946488, validation/num_examples=50000
I0202 23:06:13.198308 139923852027648 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.7353336215019226, loss=5.822007656097412
I0202 23:06:58.438558 139923868813056 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.9827942848205566, loss=4.346022129058838
I0202 23:07:45.704948 139923852027648 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.0220168828964233, loss=3.8051962852478027
I0202 23:08:32.852838 139923868813056 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1686375141143799, loss=4.069986820220947
I0202 23:09:19.933407 139923852027648 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0908738374710083, loss=3.759190320968628
I0202 23:10:06.975982 139923868813056 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.6840513348579407, loss=5.055809020996094
I0202 23:10:54.255116 139923852027648 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.9108584523200989, loss=4.087629795074463
I0202 23:11:41.467975 139923868813056 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.86606365442276, loss=4.632920265197754
I0202 23:12:29.089381 139923852027648 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.1037616729736328, loss=3.7944841384887695
I0202 23:12:56.360663 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:13:07.142217 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:13:41.076656 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:13:42.714583 140085747812160 submission_runner.py:408] Time since start: 18596.42s, 	Step: 36160, 	{'train/accuracy': 0.4143359363079071, 'train/loss': 2.687903881072998, 'validation/accuracy': 0.37469998002052307, 'validation/loss': 2.915180206298828, 'validation/num_examples': 50000, 'test/accuracy': 0.289000004529953, 'test/loss': 3.515514612197876, 'test/num_examples': 10000, 'score': 16842.51434326172, 'total_duration': 18596.418609380722, 'accumulated_submission_time': 16842.51434326172, 'accumulated_eval_time': 1750.1966817378998, 'accumulated_logging_time': 1.688964605331421}
I0202 23:13:42.738292 139923868813056 logging_writer.py:48] [36160] accumulated_eval_time=1750.196682, accumulated_logging_time=1.688965, accumulated_submission_time=16842.514343, global_step=36160, preemption_count=0, score=16842.514343, test/accuracy=0.289000, test/loss=3.515515, test/num_examples=10000, total_duration=18596.418609, train/accuracy=0.414336, train/loss=2.687904, validation/accuracy=0.374700, validation/loss=2.915180, validation/num_examples=50000
I0202 23:13:58.821263 139923852027648 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.9990479350090027, loss=3.741405963897705
I0202 23:14:43.678617 139923868813056 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.7194544672966003, loss=5.278351306915283
I0202 23:15:30.245104 139923852027648 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.208693027496338, loss=3.8712234497070312
I0202 23:16:17.818353 139923868813056 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0504915714263916, loss=4.00588846206665
I0202 23:17:04.417944 139923852027648 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.8823675513267517, loss=3.795402765274048
I0202 23:17:51.497565 139923868813056 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0461605787277222, loss=3.649057388305664
I0202 23:18:38.352225 139923852027648 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0732002258300781, loss=3.90317964553833
I0202 23:19:25.337235 139923868813056 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1591997146606445, loss=4.077624797821045
I0202 23:20:12.145080 139923852027648 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7727171778678894, loss=5.4671854972839355
I0202 23:20:42.884833 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:20:53.435334 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:21:25.478368 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:21:27.116848 140085747812160 submission_runner.py:408] Time since start: 19060.82s, 	Step: 37067, 	{'train/accuracy': 0.3991992175579071, 'train/loss': 2.74881911277771, 'validation/accuracy': 0.3721599876880646, 'validation/loss': 2.8969502449035645, 'validation/num_examples': 50000, 'test/accuracy': 0.296500027179718, 'test/loss': 3.5063552856445312, 'test/num_examples': 10000, 'score': 17262.60005879402, 'total_duration': 19060.820858955383, 'accumulated_submission_time': 17262.60005879402, 'accumulated_eval_time': 1794.428700685501, 'accumulated_logging_time': 1.7227368354797363}
I0202 23:21:27.139115 139923868813056 logging_writer.py:48] [37067] accumulated_eval_time=1794.428701, accumulated_logging_time=1.722737, accumulated_submission_time=17262.600059, global_step=37067, preemption_count=0, score=17262.600059, test/accuracy=0.296500, test/loss=3.506355, test/num_examples=10000, total_duration=19060.820859, train/accuracy=0.399199, train/loss=2.748819, validation/accuracy=0.372160, validation/loss=2.896950, validation/num_examples=50000
I0202 23:21:40.493289 139923852027648 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1435819864273071, loss=3.89786434173584
I0202 23:22:24.674150 139923852027648 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.8186269402503967, loss=5.376845836639404
I0202 23:23:11.768000 139923868813056 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.080243706703186, loss=4.031866073608398
I0202 23:23:58.667791 139923852027648 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0048363208770752, loss=3.658440351486206
I0202 23:24:45.584660 139923868813056 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8823984861373901, loss=3.6704607009887695
I0202 23:25:32.338855 139923852027648 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0620458126068115, loss=5.14499568939209
I0202 23:26:19.266157 139923868813056 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.9790976643562317, loss=3.7947518825531006
I0202 23:27:06.203209 139923852027648 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.0254580974578857, loss=3.8397464752197266
I0202 23:27:53.001772 139923868813056 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7479708194732666, loss=5.784243106842041
I0202 23:28:27.258115 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:28:37.909197 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:29:06.449607 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:29:08.087470 140085747812160 submission_runner.py:408] Time since start: 19521.79s, 	Step: 37975, 	{'train/accuracy': 0.40013670921325684, 'train/loss': 2.7493491172790527, 'validation/accuracy': 0.3765600025653839, 'validation/loss': 2.8872478008270264, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.4961540699005127, 'test/num_examples': 10000, 'score': 17682.65801167488, 'total_duration': 19521.791479349136, 'accumulated_submission_time': 17682.65801167488, 'accumulated_eval_time': 1835.2586352825165, 'accumulated_logging_time': 1.7550582885742188}
I0202 23:29:08.115883 139923852027648 logging_writer.py:48] [37975] accumulated_eval_time=1835.258635, accumulated_logging_time=1.755058, accumulated_submission_time=17682.658012, global_step=37975, preemption_count=0, score=17682.658012, test/accuracy=0.286500, test/loss=3.496154, test/num_examples=10000, total_duration=19521.791479, train/accuracy=0.400137, train/loss=2.749349, validation/accuracy=0.376560, validation/loss=2.887248, validation/num_examples=50000
I0202 23:29:18.322040 139923868813056 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9232344031333923, loss=4.115962982177734
I0202 23:30:02.154373 139923852027648 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.0475839376449585, loss=3.8243398666381836
I0202 23:30:49.151080 139923868813056 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.0254359245300293, loss=3.919187545776367
I0202 23:31:36.204666 139923852027648 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.068450927734375, loss=6.089232444763184
I0202 23:32:23.072893 139923868813056 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.0365902185440063, loss=3.836378574371338
I0202 23:33:10.214742 139923852027648 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.8967190384864807, loss=3.8466787338256836
I0202 23:33:57.185315 139923868813056 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.6823713779449463, loss=5.640323638916016
I0202 23:34:44.233314 139923852027648 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.0171502828598022, loss=4.115528106689453
I0202 23:35:31.305455 139923868813056 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.7571263313293457, loss=5.071295261383057
I0202 23:36:08.467815 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:36:19.330060 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:36:53.245825 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:36:54.884276 140085747812160 submission_runner.py:408] Time since start: 19988.59s, 	Step: 38881, 	{'train/accuracy': 0.4105273485183716, 'train/loss': 2.71467924118042, 'validation/accuracy': 0.3744199872016907, 'validation/loss': 2.916626453399658, 'validation/num_examples': 50000, 'test/accuracy': 0.2857000231742859, 'test/loss': 3.532196283340454, 'test/num_examples': 10000, 'score': 18102.947584867477, 'total_duration': 19988.588298797607, 'accumulated_submission_time': 18102.947584867477, 'accumulated_eval_time': 1881.6751172542572, 'accumulated_logging_time': 1.7953834533691406}
I0202 23:36:54.908555 139923852027648 logging_writer.py:48] [38881] accumulated_eval_time=1881.675117, accumulated_logging_time=1.795383, accumulated_submission_time=18102.947585, global_step=38881, preemption_count=0, score=18102.947585, test/accuracy=0.285700, test/loss=3.532196, test/num_examples=10000, total_duration=19988.588299, train/accuracy=0.410527, train/loss=2.714679, validation/accuracy=0.374420, validation/loss=2.916626, validation/num_examples=50000
I0202 23:37:02.761668 139923868813056 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.7998068928718567, loss=5.331781387329102
I0202 23:37:45.748889 139923852027648 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.2248858213424683, loss=3.704867362976074
I0202 23:38:32.753094 139923868813056 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.8330526351928711, loss=4.49024772644043
I0202 23:39:19.639692 139923852027648 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.8463671803474426, loss=6.003499507904053
I0202 23:40:06.633841 139923868813056 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9131735563278198, loss=4.00850772857666
I0202 23:40:53.522536 139923852027648 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.7169103026390076, loss=5.912732124328613
I0202 23:41:40.375090 139923868813056 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.8273310661315918, loss=5.9850172996521
I0202 23:42:27.531334 139923852027648 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.124144196510315, loss=3.8121256828308105
I0202 23:43:14.604683 139923868813056 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.0243699550628662, loss=3.6770215034484863
I0202 23:43:54.892115 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:44:06.934314 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:44:37.483894 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:44:39.119159 140085747812160 submission_runner.py:408] Time since start: 20452.82s, 	Step: 39787, 	{'train/accuracy': 0.3961132764816284, 'train/loss': 2.812042713165283, 'validation/accuracy': 0.3680199980735779, 'validation/loss': 2.944639205932617, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.5363407135009766, 'test/num_examples': 10000, 'score': 18522.871568918228, 'total_duration': 20452.823182106018, 'accumulated_submission_time': 18522.871568918228, 'accumulated_eval_time': 1925.902155160904, 'accumulated_logging_time': 1.8292152881622314}
I0202 23:44:39.143372 139923852027648 logging_writer.py:48] [39787] accumulated_eval_time=1925.902155, accumulated_logging_time=1.829215, accumulated_submission_time=18522.871569, global_step=39787, preemption_count=0, score=18522.871569, test/accuracy=0.286500, test/loss=3.536341, test/num_examples=10000, total_duration=20452.823182, train/accuracy=0.396113, train/loss=2.812043, validation/accuracy=0.368020, validation/loss=2.944639, validation/num_examples=50000
I0202 23:44:44.643253 139923868813056 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.7544796466827393, loss=5.197244644165039
I0202 23:45:27.460898 139923852027648 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0440089702606201, loss=4.471582412719727
I0202 23:46:14.515125 139923868813056 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.021364450454712, loss=3.9048452377319336
I0202 23:47:01.667347 139923852027648 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.0518016815185547, loss=3.9140737056732178
I0202 23:47:48.419064 139923868813056 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.7018989324569702, loss=5.969816207885742
I0202 23:48:35.411460 139923852027648 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.0777369737625122, loss=3.8249351978302
I0202 23:49:22.372189 139923868813056 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.9582539200782776, loss=3.7339305877685547
I0202 23:50:09.466870 139923852027648 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0673748254776, loss=3.980198383331299
I0202 23:50:56.483293 139923868813056 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.9168899059295654, loss=3.7755050659179688
I0202 23:51:39.211760 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:51:50.053191 140085747812160 spec.py:333] Evaluating on the validation split.
I0202 23:52:21.697818 140085747812160 spec.py:349] Evaluating on the test split.
I0202 23:52:23.338964 140085747812160 submission_runner.py:408] Time since start: 20917.04s, 	Step: 40692, 	{'train/accuracy': 0.3843359351158142, 'train/loss': 2.8809280395507812, 'validation/accuracy': 0.35760000348091125, 'validation/loss': 3.0336527824401855, 'validation/num_examples': 50000, 'test/accuracy': 0.27740001678466797, 'test/loss': 3.6219842433929443, 'test/num_examples': 10000, 'score': 18942.88044810295, 'total_duration': 20917.042991399765, 'accumulated_submission_time': 18942.88044810295, 'accumulated_eval_time': 1970.02938914299, 'accumulated_logging_time': 1.8625555038452148}
I0202 23:52:23.366919 139923852027648 logging_writer.py:48] [40692] accumulated_eval_time=1970.029389, accumulated_logging_time=1.862556, accumulated_submission_time=18942.880448, global_step=40692, preemption_count=0, score=18942.880448, test/accuracy=0.277400, test/loss=3.621984, test/num_examples=10000, total_duration=20917.042991, train/accuracy=0.384336, train/loss=2.880928, validation/accuracy=0.357600, validation/loss=3.033653, validation/num_examples=50000
I0202 23:52:26.904346 139923868813056 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.9150030016899109, loss=4.587262153625488
I0202 23:53:09.538082 139923852027648 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.980343759059906, loss=3.7651753425598145
I0202 23:53:56.322207 139923868813056 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9757590293884277, loss=3.8592939376831055
I0202 23:54:43.322511 139923852027648 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.090734839439392, loss=3.8632898330688477
I0202 23:55:30.552389 139923868813056 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0752148628234863, loss=3.865760326385498
I0202 23:56:17.564370 139923852027648 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.248673677444458, loss=4.124380588531494
I0202 23:57:04.514323 139923868813056 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.7436906099319458, loss=5.817233085632324
I0202 23:57:51.337342 139923852027648 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.9822512269020081, loss=3.6063976287841797
I0202 23:58:38.425353 139923868813056 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.0198755264282227, loss=5.431554317474365
I0202 23:59:23.599895 140085747812160 spec.py:321] Evaluating on the training split.
I0202 23:59:34.315368 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:00:04.086252 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:00:05.724250 140085747812160 submission_runner.py:408] Time since start: 21379.43s, 	Step: 41598, 	{'train/accuracy': 0.41939452290534973, 'train/loss': 2.614825487136841, 'validation/accuracy': 0.38731998205184937, 'validation/loss': 2.794647216796875, 'validation/num_examples': 50000, 'test/accuracy': 0.30250000953674316, 'test/loss': 3.3992068767547607, 'test/num_examples': 10000, 'score': 19363.052728176117, 'total_duration': 21379.428280830383, 'accumulated_submission_time': 19363.052728176117, 'accumulated_eval_time': 2012.1537518501282, 'accumulated_logging_time': 1.9005413055419922}
I0203 00:00:05.747493 139923852027648 logging_writer.py:48] [41598] accumulated_eval_time=2012.153752, accumulated_logging_time=1.900541, accumulated_submission_time=19363.052728, global_step=41598, preemption_count=0, score=19363.052728, test/accuracy=0.302500, test/loss=3.399207, test/num_examples=10000, total_duration=21379.428281, train/accuracy=0.419395, train/loss=2.614825, validation/accuracy=0.387320, validation/loss=2.794647, validation/num_examples=50000
I0203 00:00:06.932146 139923868813056 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.056054949760437, loss=3.8559374809265137
I0203 00:00:49.171274 139923852027648 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.8605054020881653, loss=4.9504804611206055
I0203 00:01:36.170772 139923868813056 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.283605694770813, loss=3.9661240577697754
I0203 00:02:23.162935 139923852027648 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.9424171447753906, loss=3.979642868041992
I0203 00:03:10.519127 139923868813056 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.0980844497680664, loss=3.743333101272583
I0203 00:03:57.429168 139923852027648 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1092817783355713, loss=3.9498536586761475
I0203 00:04:44.356698 139923868813056 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.0073455572128296, loss=3.8983027935028076
I0203 00:05:31.658455 139923852027648 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.7294126749038696, loss=5.5118489265441895
I0203 00:06:19.017493 139923868813056 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.888756513595581, loss=4.41221809387207
I0203 00:07:06.109744 139923852027648 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.1554319858551025, loss=3.679741621017456
I0203 00:07:06.124062 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:07:16.975016 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:07:50.722867 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:07:52.360321 140085747812160 submission_runner.py:408] Time since start: 21846.06s, 	Step: 42501, 	{'train/accuracy': 0.41039061546325684, 'train/loss': 2.7060678005218506, 'validation/accuracy': 0.38383999466896057, 'validation/loss': 2.8514671325683594, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.468750238418579, 'test/num_examples': 10000, 'score': 19783.368386268616, 'total_duration': 21846.06435275078, 'accumulated_submission_time': 19783.368386268616, 'accumulated_eval_time': 2058.3900072574615, 'accumulated_logging_time': 1.9348745346069336}
I0203 00:07:52.383268 139923868813056 logging_writer.py:48] [42501] accumulated_eval_time=2058.390007, accumulated_logging_time=1.934875, accumulated_submission_time=19783.368386, global_step=42501, preemption_count=0, score=19783.368386, test/accuracy=0.296200, test/loss=3.468750, test/num_examples=10000, total_duration=21846.064353, train/accuracy=0.410391, train/loss=2.706068, validation/accuracy=0.383840, validation/loss=2.851467, validation/num_examples=50000
I0203 00:08:34.037672 139923852027648 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.9753872752189636, loss=3.743234872817993
I0203 00:09:20.643494 139923868813056 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.7521688342094421, loss=5.024840831756592
I0203 00:10:07.838876 139923852027648 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.811038613319397, loss=5.207062244415283
I0203 00:10:54.481529 139923868813056 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.17763090133667, loss=3.8161346912384033
I0203 00:11:41.448395 139923852027648 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9990271329879761, loss=3.6401519775390625
I0203 00:12:28.610693 139923868813056 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.0281928777694702, loss=3.642641305923462
I0203 00:13:15.677591 139923852027648 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.9180110692977905, loss=4.20529317855835
I0203 00:14:02.556377 139923868813056 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.097144603729248, loss=3.796823501586914
I0203 00:14:49.555358 139923852027648 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.9283714890480042, loss=3.6941254138946533
I0203 00:14:52.475816 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:15:02.869218 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:15:30.899136 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:15:32.542250 140085747812160 submission_runner.py:408] Time since start: 22306.25s, 	Step: 43408, 	{'train/accuracy': 0.4212304651737213, 'train/loss': 2.6352226734161377, 'validation/accuracy': 0.39013999700546265, 'validation/loss': 2.7732999324798584, 'validation/num_examples': 50000, 'test/accuracy': 0.3060000240802765, 'test/loss': 3.412951707839966, 'test/num_examples': 10000, 'score': 20203.399605989456, 'total_duration': 22306.24627304077, 'accumulated_submission_time': 20203.399605989456, 'accumulated_eval_time': 2098.456430912018, 'accumulated_logging_time': 1.9667832851409912}
I0203 00:15:32.565414 139923868813056 logging_writer.py:48] [43408] accumulated_eval_time=2098.456431, accumulated_logging_time=1.966783, accumulated_submission_time=20203.399606, global_step=43408, preemption_count=0, score=20203.399606, test/accuracy=0.306000, test/loss=3.412952, test/num_examples=10000, total_duration=22306.246273, train/accuracy=0.421230, train/loss=2.635223, validation/accuracy=0.390140, validation/loss=2.773300, validation/num_examples=50000
I0203 00:16:11.250930 139923852027648 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.038325309753418, loss=3.743684768676758
I0203 00:16:58.007413 139923868813056 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.093738079071045, loss=3.8247318267822266
I0203 00:17:44.869687 139923852027648 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.7257153987884521, loss=5.389970779418945
I0203 00:18:31.979390 139923868813056 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.0150609016418457, loss=3.7796144485473633
I0203 00:19:18.673673 139923852027648 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1967861652374268, loss=3.7384111881256104
I0203 00:20:05.662021 139923868813056 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1489253044128418, loss=3.754690647125244
I0203 00:20:52.674901 139923852027648 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.8602141737937927, loss=5.326541423797607
I0203 00:21:39.653221 139923868813056 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.0642175674438477, loss=3.700812339782715
I0203 00:22:26.916520 139923852027648 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.8934915065765381, loss=3.7511098384857178
I0203 00:22:32.868814 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:22:43.491712 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:23:12.994983 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:23:14.629027 140085747812160 submission_runner.py:408] Time since start: 22768.33s, 	Step: 44314, 	{'train/accuracy': 0.424628883600235, 'train/loss': 2.5954785346984863, 'validation/accuracy': 0.3944399952888489, 'validation/loss': 2.772815465927124, 'validation/num_examples': 50000, 'test/accuracy': 0.30400002002716064, 'test/loss': 3.4005684852600098, 'test/num_examples': 10000, 'score': 20623.64171051979, 'total_duration': 22768.33304667473, 'accumulated_submission_time': 20623.64171051979, 'accumulated_eval_time': 2140.2166497707367, 'accumulated_logging_time': 1.9997265338897705}
I0203 00:23:14.656635 139923868813056 logging_writer.py:48] [44314] accumulated_eval_time=2140.216650, accumulated_logging_time=1.999727, accumulated_submission_time=20623.641711, global_step=44314, preemption_count=0, score=20623.641711, test/accuracy=0.304000, test/loss=3.400568, test/num_examples=10000, total_duration=22768.333047, train/accuracy=0.424629, train/loss=2.595479, validation/accuracy=0.394440, validation/loss=2.772815, validation/num_examples=50000
I0203 00:23:50.432690 139923852027648 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.902387797832489, loss=4.906636714935303
I0203 00:24:37.273422 139923868813056 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.231024146080017, loss=3.726228952407837
I0203 00:25:24.400898 139923852027648 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.1705474853515625, loss=3.8955297470092773
I0203 00:26:11.445942 139923868813056 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1986056566238403, loss=4.275908946990967
I0203 00:26:58.441743 139923852027648 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.819081723690033, loss=5.195307731628418
I0203 00:27:45.585068 139923868813056 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.2825102806091309, loss=3.8835859298706055
I0203 00:28:32.502677 139923852027648 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.753921627998352, loss=5.516638278961182
I0203 00:29:19.140226 139923868813056 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.7309911847114563, loss=5.213420391082764
I0203 00:30:06.489090 139923852027648 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.8481608033180237, loss=6.014302730560303
I0203 00:30:14.865931 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:30:25.542435 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:30:59.922792 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:31:01.560631 140085747812160 submission_runner.py:408] Time since start: 23235.26s, 	Step: 45220, 	{'train/accuracy': 0.4167773425579071, 'train/loss': 2.6995503902435303, 'validation/accuracy': 0.39111998677253723, 'validation/loss': 2.838123083114624, 'validation/num_examples': 50000, 'test/accuracy': 0.30580002069473267, 'test/loss': 3.4516892433166504, 'test/num_examples': 10000, 'score': 21043.791278362274, 'total_duration': 23235.264661312103, 'accumulated_submission_time': 21043.791278362274, 'accumulated_eval_time': 2186.9113490581512, 'accumulated_logging_time': 2.0372612476348877}
I0203 00:31:01.585774 139923868813056 logging_writer.py:48] [45220] accumulated_eval_time=2186.911349, accumulated_logging_time=2.037261, accumulated_submission_time=21043.791278, global_step=45220, preemption_count=0, score=21043.791278, test/accuracy=0.305800, test/loss=3.451689, test/num_examples=10000, total_duration=23235.264661, train/accuracy=0.416777, train/loss=2.699550, validation/accuracy=0.391120, validation/loss=2.838123, validation/num_examples=50000
I0203 00:31:34.352045 139923852027648 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.9503034949302673, loss=3.7706403732299805
I0203 00:32:21.260658 139923868813056 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.9693563580513, loss=3.7664990425109863
I0203 00:33:08.640938 139923852027648 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.1166605949401855, loss=3.6542837619781494
I0203 00:33:55.713680 139923868813056 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.9917832016944885, loss=3.663926601409912
I0203 00:34:42.495391 139923852027648 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.7095108032226562, loss=5.760042667388916
I0203 00:35:29.471988 139923868813056 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.759785532951355, loss=5.411740779876709
I0203 00:36:16.504026 139923852027648 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.9762630462646484, loss=3.8102970123291016
I0203 00:37:03.637278 139923868813056 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.9144870638847351, loss=5.646112442016602
I0203 00:37:50.708460 139923852027648 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.0206106901168823, loss=3.6861045360565186
I0203 00:38:01.751776 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:38:12.178931 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:38:44.024457 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:38:45.669445 140085747812160 submission_runner.py:408] Time since start: 23699.37s, 	Step: 46125, 	{'train/accuracy': 0.4143945276737213, 'train/loss': 2.6916167736053467, 'validation/accuracy': 0.3907199800014496, 'validation/loss': 2.808105945587158, 'validation/num_examples': 50000, 'test/accuracy': 0.29090002179145813, 'test/loss': 3.441751718521118, 'test/num_examples': 10000, 'score': 21463.8963804245, 'total_duration': 23699.373474121094, 'accumulated_submission_time': 21463.8963804245, 'accumulated_eval_time': 2230.8290145397186, 'accumulated_logging_time': 2.0722317695617676}
I0203 00:38:45.694065 139923868813056 logging_writer.py:48] [46125] accumulated_eval_time=2230.829015, accumulated_logging_time=2.072232, accumulated_submission_time=21463.896380, global_step=46125, preemption_count=0, score=21463.896380, test/accuracy=0.290900, test/loss=3.441752, test/num_examples=10000, total_duration=23699.373474, train/accuracy=0.414395, train/loss=2.691617, validation/accuracy=0.390720, validation/loss=2.808106, validation/num_examples=50000
I0203 00:39:16.399254 139923852027648 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.0558539628982544, loss=3.7520065307617188
I0203 00:40:03.197585 139923868813056 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.8404944539070129, loss=4.336890697479248
I0203 00:40:50.213946 139923852027648 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.9696661829948425, loss=3.6160740852355957
I0203 00:41:37.415221 139923868813056 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.9523534774780273, loss=3.833787679672241
I0203 00:42:24.619200 139923852027648 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1028152704238892, loss=3.5284857749938965
I0203 00:43:11.869453 139923868813056 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.9718429446220398, loss=3.801762819290161
I0203 00:43:59.086044 139923852027648 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.7922236919403076, loss=5.753168106079102
I0203 00:44:46.139502 139923868813056 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8319671750068665, loss=4.96056604385376
I0203 00:45:33.178038 139923852027648 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.7549249529838562, loss=4.97100305557251
I0203 00:45:45.944066 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:45:56.680347 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:46:30.028306 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:46:31.674768 140085747812160 submission_runner.py:408] Time since start: 24165.38s, 	Step: 47029, 	{'train/accuracy': 0.4289257824420929, 'train/loss': 2.5802981853485107, 'validation/accuracy': 0.39791998267173767, 'validation/loss': 2.7644102573394775, 'validation/num_examples': 50000, 'test/accuracy': 0.30260002613067627, 'test/loss': 3.3919708728790283, 'test/num_examples': 10000, 'score': 21884.086091279984, 'total_duration': 24165.378796577454, 'accumulated_submission_time': 21884.086091279984, 'accumulated_eval_time': 2276.559750556946, 'accumulated_logging_time': 2.1068155765533447}
I0203 00:46:31.699966 139923868813056 logging_writer.py:48] [47029] accumulated_eval_time=2276.559751, accumulated_logging_time=2.106816, accumulated_submission_time=21884.086091, global_step=47029, preemption_count=0, score=21884.086091, test/accuracy=0.302600, test/loss=3.391971, test/num_examples=10000, total_duration=24165.378797, train/accuracy=0.428926, train/loss=2.580298, validation/accuracy=0.397920, validation/loss=2.764410, validation/num_examples=50000
I0203 00:47:00.510118 139923852027648 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.0234400033950806, loss=3.7506186962127686
I0203 00:47:47.249207 139923868813056 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.9667769074440002, loss=3.5424845218658447
I0203 00:48:34.507373 139923852027648 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.861470639705658, loss=5.353689193725586
I0203 00:49:21.625954 139923868813056 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.6546250581741333, loss=5.872891902923584
I0203 00:50:08.700973 139923852027648 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.0504766702651978, loss=3.9181599617004395
I0203 00:50:55.695591 139923868813056 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.4160504341125488, loss=3.818593978881836
I0203 00:51:42.720499 139923852027648 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.9114254713058472, loss=4.6664862632751465
I0203 00:52:29.868917 139923868813056 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.9469828009605408, loss=4.422358512878418
I0203 00:53:16.706418 139923852027648 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0211029052734375, loss=3.7175092697143555
I0203 00:53:31.881620 140085747812160 spec.py:321] Evaluating on the training split.
I0203 00:53:42.394583 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 00:54:13.828576 140085747812160 spec.py:349] Evaluating on the test split.
I0203 00:54:15.478671 140085747812160 submission_runner.py:408] Time since start: 24629.18s, 	Step: 47934, 	{'train/accuracy': 0.4466210901737213, 'train/loss': 2.509028196334839, 'validation/accuracy': 0.40153998136520386, 'validation/loss': 2.7524359226226807, 'validation/num_examples': 50000, 'test/accuracy': 0.31370002031326294, 'test/loss': 3.3566060066223145, 'test/num_examples': 10000, 'score': 22304.20796895027, 'total_duration': 24629.18269968033, 'accumulated_submission_time': 22304.20796895027, 'accumulated_eval_time': 2320.1568138599396, 'accumulated_logging_time': 2.1419427394866943}
I0203 00:54:15.504010 139923868813056 logging_writer.py:48] [47934] accumulated_eval_time=2320.156814, accumulated_logging_time=2.141943, accumulated_submission_time=22304.207969, global_step=47934, preemption_count=0, score=22304.207969, test/accuracy=0.313700, test/loss=3.356606, test/num_examples=10000, total_duration=24629.182700, train/accuracy=0.446621, train/loss=2.509028, validation/accuracy=0.401540, validation/loss=2.752436, validation/num_examples=50000
I0203 00:54:42.186138 139923852027648 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.9855096340179443, loss=4.459684371948242
I0203 00:55:28.720026 139923868813056 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.1470483541488647, loss=3.694300651550293
I0203 00:56:15.651938 139923852027648 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.9930771589279175, loss=3.845207691192627
I0203 00:57:02.637605 139923868813056 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.134354829788208, loss=3.819084405899048
I0203 00:57:49.581523 139923852027648 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.0564088821411133, loss=3.6857733726501465
I0203 00:58:36.811566 139923868813056 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.1605955362319946, loss=3.4496939182281494
I0203 00:59:24.034619 139923852027648 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.0910289287567139, loss=3.6792590618133545
I0203 01:00:11.032294 139923868813056 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.314210295677185, loss=3.80139422416687
I0203 01:00:57.938016 139923852027648 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.9468855261802673, loss=3.6226701736450195
I0203 01:01:15.841624 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:01:26.465538 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:01:57.244935 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:01:58.881295 140085747812160 submission_runner.py:408] Time since start: 25092.59s, 	Step: 48840, 	{'train/accuracy': 0.42265623807907104, 'train/loss': 2.647350788116455, 'validation/accuracy': 0.39475998282432556, 'validation/loss': 2.8125829696655273, 'validation/num_examples': 50000, 'test/accuracy': 0.3037000000476837, 'test/loss': 3.428317070007324, 'test/num_examples': 10000, 'score': 22724.48389363289, 'total_duration': 25092.585326194763, 'accumulated_submission_time': 22724.48389363289, 'accumulated_eval_time': 2363.196498155594, 'accumulated_logging_time': 2.1787829399108887}
I0203 01:01:58.904896 139923868813056 logging_writer.py:48] [48840] accumulated_eval_time=2363.196498, accumulated_logging_time=2.178783, accumulated_submission_time=22724.483894, global_step=48840, preemption_count=0, score=22724.483894, test/accuracy=0.303700, test/loss=3.428317, test/num_examples=10000, total_duration=25092.585326, train/accuracy=0.422656, train/loss=2.647351, validation/accuracy=0.394760, validation/loss=2.812583, validation/num_examples=50000
I0203 01:02:22.852649 139923852027648 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0718967914581299, loss=4.366800308227539
I0203 01:03:09.435071 139923868813056 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8829202055931091, loss=4.855128765106201
I0203 01:03:56.493150 139923852027648 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2032662630081177, loss=3.649583578109741
I0203 01:04:43.199025 139923868813056 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.9596657156944275, loss=3.60113263130188
I0203 01:05:30.136155 139923852027648 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.8322063088417053, loss=4.949375629425049
I0203 01:06:16.914389 139923868813056 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.053928017616272, loss=3.6763391494750977
I0203 01:07:03.896761 139923852027648 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.9727555513381958, loss=3.549771308898926
I0203 01:07:50.728605 139923868813056 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.6027408838272095, loss=5.81544303894043
I0203 01:08:37.873309 139923852027648 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.0540589094161987, loss=3.681229829788208
I0203 01:08:58.949341 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:09:09.400646 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:09:42.255146 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:09:43.894281 140085747812160 submission_runner.py:408] Time since start: 25557.60s, 	Step: 49746, 	{'train/accuracy': 0.4342578053474426, 'train/loss': 2.547525405883789, 'validation/accuracy': 0.39907997846603394, 'validation/loss': 2.7272603511810303, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.355839252471924, 'test/num_examples': 10000, 'score': 23144.46818780899, 'total_duration': 25557.598313570023, 'accumulated_submission_time': 23144.46818780899, 'accumulated_eval_time': 2408.141443490982, 'accumulated_logging_time': 2.212597131729126}
I0203 01:09:43.920635 139923868813056 logging_writer.py:48] [49746] accumulated_eval_time=2408.141443, accumulated_logging_time=2.212597, accumulated_submission_time=23144.468188, global_step=49746, preemption_count=0, score=23144.468188, test/accuracy=0.309900, test/loss=3.355839, test/num_examples=10000, total_duration=25557.598314, train/accuracy=0.434258, train/loss=2.547525, validation/accuracy=0.399080, validation/loss=2.727260, validation/num_examples=50000
I0203 01:10:05.517345 139923852027648 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.9966066479682922, loss=3.5555548667907715
I0203 01:10:51.279330 139923868813056 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.972767174243927, loss=3.8070363998413086
I0203 01:11:38.237826 139923852027648 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.9975298047065735, loss=3.596992254257202
I0203 01:12:24.861277 139923868813056 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.6894824504852295, loss=5.789690971374512
I0203 01:13:11.791449 139923852027648 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.0194671154022217, loss=3.5672595500946045
I0203 01:13:58.768655 139923868813056 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.9250715970993042, loss=3.969784736633301
I0203 01:14:45.677374 139923852027648 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.014699935913086, loss=6.005576133728027
I0203 01:15:32.746126 139923868813056 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.1007468700408936, loss=3.832171678543091
I0203 01:16:19.560325 139923852027648 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.0277862548828125, loss=3.720034599304199
I0203 01:16:44.128084 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:16:54.759510 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:17:28.636567 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:17:30.276608 140085747812160 submission_runner.py:408] Time since start: 26023.98s, 	Step: 50654, 	{'train/accuracy': 0.46113279461860657, 'train/loss': 2.4386072158813477, 'validation/accuracy': 0.4008199870586395, 'validation/loss': 2.7578697204589844, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.3625118732452393, 'test/num_examples': 10000, 'score': 23564.613875627518, 'total_duration': 26023.98063802719, 'accumulated_submission_time': 23564.613875627518, 'accumulated_eval_time': 2454.2899844646454, 'accumulated_logging_time': 2.249370574951172}
I0203 01:17:30.303854 139923868813056 logging_writer.py:48] [50654] accumulated_eval_time=2454.289984, accumulated_logging_time=2.249371, accumulated_submission_time=23564.613876, global_step=50654, preemption_count=0, score=23564.613876, test/accuracy=0.313400, test/loss=3.362512, test/num_examples=10000, total_duration=26023.980638, train/accuracy=0.461133, train/loss=2.438607, validation/accuracy=0.400820, validation/loss=2.757870, validation/num_examples=50000
I0203 01:17:48.750101 139923852027648 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.0131359100341797, loss=4.29282808303833
I0203 01:18:33.950483 139923868813056 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.0822703838348389, loss=3.560223340988159
I0203 01:19:20.826569 139923852027648 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.0724220275878906, loss=3.6268258094787598
I0203 01:20:07.911211 139923868813056 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.023350715637207, loss=3.66353702545166
I0203 01:20:55.013923 139923852027648 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.3110554218292236, loss=3.598876476287842
I0203 01:21:42.205543 139923868813056 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.8646849989891052, loss=5.365623474121094
I0203 01:22:29.321967 139923852027648 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.169430136680603, loss=3.6540536880493164
I0203 01:23:16.238891 139923868813056 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.9296032786369324, loss=3.622304916381836
I0203 01:24:03.284635 139923852027648 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.9969613552093506, loss=3.5202643871307373
I0203 01:24:30.689882 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:24:41.112238 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:25:12.553928 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:25:14.203558 140085747812160 submission_runner.py:408] Time since start: 26487.91s, 	Step: 51560, 	{'train/accuracy': 0.4273632764816284, 'train/loss': 2.59151029586792, 'validation/accuracy': 0.4017999768257141, 'validation/loss': 2.755169630050659, 'validation/num_examples': 50000, 'test/accuracy': 0.314300000667572, 'test/loss': 3.3453259468078613, 'test/num_examples': 10000, 'score': 23984.93890619278, 'total_duration': 26487.907591342926, 'accumulated_submission_time': 23984.93890619278, 'accumulated_eval_time': 2497.8036675453186, 'accumulated_logging_time': 2.286559820175171}
I0203 01:25:14.230095 139923868813056 logging_writer.py:48] [51560] accumulated_eval_time=2497.803668, accumulated_logging_time=2.286560, accumulated_submission_time=23984.938906, global_step=51560, preemption_count=0, score=23984.938906, test/accuracy=0.314300, test/loss=3.345326, test/num_examples=10000, total_duration=26487.907591, train/accuracy=0.427363, train/loss=2.591510, validation/accuracy=0.401800, validation/loss=2.755170, validation/num_examples=50000
I0203 01:25:30.333857 139923852027648 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.7541986703872681, loss=5.88131856918335
I0203 01:26:15.026630 139923868813056 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.9525408744812012, loss=4.355597496032715
I0203 01:27:02.324136 139923852027648 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.8469765186309814, loss=5.663669109344482
I0203 01:27:49.490073 139923868813056 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.056820034980774, loss=3.4692323207855225
I0203 01:28:36.431359 139923852027648 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2690938711166382, loss=4.14229679107666
I0203 01:29:23.553570 139923868813056 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.7899045944213867, loss=5.44113826751709
I0203 01:30:10.413487 139923852027648 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.0051132440567017, loss=3.560586452484131
I0203 01:30:57.469400 139923868813056 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.9451294541358948, loss=3.578702449798584
I0203 01:31:44.650116 139923852027648 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.9449689984321594, loss=4.555225849151611
I0203 01:32:14.336732 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:32:24.875977 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:32:55.698044 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:32:57.348751 140085747812160 submission_runner.py:408] Time since start: 26951.05s, 	Step: 52465, 	{'train/accuracy': 0.43562498688697815, 'train/loss': 2.548950672149658, 'validation/accuracy': 0.4038800001144409, 'validation/loss': 2.7284858226776123, 'validation/num_examples': 50000, 'test/accuracy': 0.30960002541542053, 'test/loss': 3.3761465549468994, 'test/num_examples': 10000, 'score': 24404.984939336777, 'total_duration': 26951.052780389786, 'accumulated_submission_time': 24404.984939336777, 'accumulated_eval_time': 2540.815685033798, 'accumulated_logging_time': 2.323159694671631}
I0203 01:32:57.375813 139923868813056 logging_writer.py:48] [52465] accumulated_eval_time=2540.815685, accumulated_logging_time=2.323160, accumulated_submission_time=24404.984939, global_step=52465, preemption_count=0, score=24404.984939, test/accuracy=0.309600, test/loss=3.376147, test/num_examples=10000, total_duration=26951.052780, train/accuracy=0.435625, train/loss=2.548951, validation/accuracy=0.403880, validation/loss=2.728486, validation/num_examples=50000
I0203 01:33:11.513885 139923852027648 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.9732774496078491, loss=3.9474644660949707
I0203 01:33:56.055221 139923868813056 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.8892608284950256, loss=5.546914100646973
I0203 01:34:42.780526 139923852027648 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.6359025835990906, loss=5.8265509605407715
I0203 01:35:29.738258 139923868813056 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.922597348690033, loss=3.961348533630371
I0203 01:36:16.456719 139923852027648 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.1603988409042358, loss=4.59020471572876
I0203 01:37:03.522269 139923868813056 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.0314100980758667, loss=3.722027540206909
I0203 01:37:51.452602 139923852027648 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1311726570129395, loss=3.7731857299804688
I0203 01:38:38.542446 139923868813056 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.8787946105003357, loss=3.7187271118164062
I0203 01:39:25.341494 139923852027648 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.977105438709259, loss=4.078141212463379
I0203 01:39:57.531833 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:40:08.435798 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:40:40.733021 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:40:42.378589 140085747812160 submission_runner.py:408] Time since start: 27416.08s, 	Step: 53370, 	{'train/accuracy': 0.4490039050579071, 'train/loss': 2.4784021377563477, 'validation/accuracy': 0.4007599949836731, 'validation/loss': 2.747389316558838, 'validation/num_examples': 50000, 'test/accuracy': 0.3069000244140625, 'test/loss': 3.367358684539795, 'test/num_examples': 10000, 'score': 24825.079872131348, 'total_duration': 27416.082621097565, 'accumulated_submission_time': 24825.079872131348, 'accumulated_eval_time': 2585.6624703407288, 'accumulated_logging_time': 2.3613462448120117}
I0203 01:40:42.404063 139923868813056 logging_writer.py:48] [53370] accumulated_eval_time=2585.662470, accumulated_logging_time=2.361346, accumulated_submission_time=24825.079872, global_step=53370, preemption_count=0, score=24825.079872, test/accuracy=0.306900, test/loss=3.367359, test/num_examples=10000, total_duration=27416.082621, train/accuracy=0.449004, train/loss=2.478402, validation/accuracy=0.400760, validation/loss=2.747389, validation/num_examples=50000
I0203 01:40:54.578318 139923852027648 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.998245358467102, loss=3.9323770999908447
I0203 01:41:38.597498 139923868813056 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8657196760177612, loss=5.957644939422607
I0203 01:42:25.834024 139923852027648 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.8457475900650024, loss=4.900521755218506
I0203 01:43:12.951419 139923868813056 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.9869147539138794, loss=3.572505235671997
I0203 01:43:59.989066 139923852027648 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.8779947757720947, loss=5.752632141113281
I0203 01:44:46.817664 139923868813056 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.7183250784873962, loss=5.38100528717041
I0203 01:45:34.055883 139923852027648 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9695944786071777, loss=4.57958459854126
I0203 01:46:20.999760 139923868813056 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2081413269042969, loss=3.5920398235321045
I0203 01:47:07.832316 139923852027648 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.0670742988586426, loss=3.5947537422180176
I0203 01:47:42.550634 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:47:53.219554 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:48:23.901632 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:48:25.540162 140085747812160 submission_runner.py:408] Time since start: 27879.24s, 	Step: 54276, 	{'train/accuracy': 0.4378320276737213, 'train/loss': 2.539738655090332, 'validation/accuracy': 0.4072999954223633, 'validation/loss': 2.691128969192505, 'validation/num_examples': 50000, 'test/accuracy': 0.3165000081062317, 'test/loss': 3.3001091480255127, 'test/num_examples': 10000, 'score': 25245.164329767227, 'total_duration': 27879.24419236183, 'accumulated_submission_time': 25245.164329767227, 'accumulated_eval_time': 2628.6520042419434, 'accumulated_logging_time': 2.3979926109313965}
I0203 01:48:25.565596 139923868813056 logging_writer.py:48] [54276] accumulated_eval_time=2628.652004, accumulated_logging_time=2.397993, accumulated_submission_time=25245.164330, global_step=54276, preemption_count=0, score=25245.164330, test/accuracy=0.316500, test/loss=3.300109, test/num_examples=10000, total_duration=27879.244192, train/accuracy=0.437832, train/loss=2.539739, validation/accuracy=0.407300, validation/loss=2.691129, validation/num_examples=50000
I0203 01:48:35.380735 139923852027648 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.8645098805427551, loss=3.956136703491211
I0203 01:49:18.784970 139923868813056 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1916441917419434, loss=3.778623580932617
I0203 01:50:05.669356 139923852027648 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.103230357170105, loss=3.434368133544922
I0203 01:50:52.280424 139923868813056 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.158875584602356, loss=3.624021053314209
I0203 01:51:39.442001 139923852027648 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.1761984825134277, loss=3.80599045753479
I0203 01:52:26.359433 139923868813056 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.8623476624488831, loss=4.249125003814697
I0203 01:53:13.448280 139923852027648 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.0334973335266113, loss=3.547945022583008
I0203 01:54:00.408027 139923868813056 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.9697397947311401, loss=3.461495876312256
I0203 01:54:47.532709 139923852027648 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.8548281192779541, loss=4.431231498718262
I0203 01:55:25.605219 140085747812160 spec.py:321] Evaluating on the training split.
I0203 01:55:36.243633 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 01:56:09.301599 140085747812160 spec.py:349] Evaluating on the test split.
I0203 01:56:10.948967 140085747812160 submission_runner.py:408] Time since start: 28344.65s, 	Step: 55183, 	{'train/accuracy': 0.4406445324420929, 'train/loss': 2.527313470840454, 'validation/accuracy': 0.4094799757003784, 'validation/loss': 2.68900203704834, 'validation/num_examples': 50000, 'test/accuracy': 0.3144000172615051, 'test/loss': 3.315009593963623, 'test/num_examples': 10000, 'score': 25665.14268875122, 'total_duration': 28344.652994155884, 'accumulated_submission_time': 25665.14268875122, 'accumulated_eval_time': 2673.9957478046417, 'accumulated_logging_time': 2.4337828159332275}
I0203 01:56:10.977421 139923868813056 logging_writer.py:48] [55183] accumulated_eval_time=2673.995748, accumulated_logging_time=2.433783, accumulated_submission_time=25665.142689, global_step=55183, preemption_count=0, score=25665.142689, test/accuracy=0.314400, test/loss=3.315010, test/num_examples=10000, total_duration=28344.652994, train/accuracy=0.440645, train/loss=2.527313, validation/accuracy=0.409480, validation/loss=2.689002, validation/num_examples=50000
I0203 01:56:18.036145 139923852027648 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.0323783159255981, loss=3.6012256145477295
I0203 01:57:01.161095 139923868813056 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.8923698663711548, loss=3.598005771636963
I0203 01:57:48.100076 139923852027648 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.9765655994415283, loss=3.430605888366699
I0203 01:58:35.255298 139923868813056 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.6943747997283936, loss=5.835256576538086
I0203 01:59:22.139073 139923852027648 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0337698459625244, loss=3.714132785797119
I0203 02:00:09.248667 139923868813056 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.873480498790741, loss=4.791635990142822
I0203 02:00:55.889637 139923852027648 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.8957836031913757, loss=4.900696277618408
I0203 02:01:42.905496 139923868813056 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.9852514863014221, loss=3.556934356689453
I0203 02:02:29.941273 139923852027648 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.7637875080108643, loss=5.692889213562012
I0203 02:03:11.224371 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:03:21.780920 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:03:55.442129 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:03:57.078975 140085747812160 submission_runner.py:408] Time since start: 28810.78s, 	Step: 56089, 	{'train/accuracy': 0.4562695324420929, 'train/loss': 2.470243453979492, 'validation/accuracy': 0.4152199923992157, 'validation/loss': 2.69305157661438, 'validation/num_examples': 50000, 'test/accuracy': 0.3237000107765198, 'test/loss': 3.3011837005615234, 'test/num_examples': 10000, 'score': 26085.327237844467, 'total_duration': 28810.783006429672, 'accumulated_submission_time': 26085.327237844467, 'accumulated_eval_time': 2719.850363969803, 'accumulated_logging_time': 2.47387957572937}
I0203 02:03:57.106086 139923868813056 logging_writer.py:48] [56089] accumulated_eval_time=2719.850364, accumulated_logging_time=2.473880, accumulated_submission_time=26085.327238, global_step=56089, preemption_count=0, score=26085.327238, test/accuracy=0.323700, test/loss=3.301184, test/num_examples=10000, total_duration=28810.783006, train/accuracy=0.456270, train/loss=2.470243, validation/accuracy=0.415220, validation/loss=2.693052, validation/num_examples=50000
I0203 02:04:01.819048 139923852027648 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0691691637039185, loss=3.7596468925476074
I0203 02:04:44.560849 139923868813056 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.9857457280158997, loss=3.6353025436401367
I0203 02:05:31.515331 139923852027648 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.8556556701660156, loss=4.649612903594971
I0203 02:06:18.409806 139923868813056 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.0517481565475464, loss=3.6762449741363525
I0203 02:07:05.287330 139923852027648 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.8996129631996155, loss=4.085015296936035
I0203 02:07:52.448991 139923868813056 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.1262356042861938, loss=3.486795425415039
I0203 02:08:39.275056 139923852027648 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0648844242095947, loss=3.6075551509857178
I0203 02:09:26.260990 139923868813056 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.182407021522522, loss=3.6343071460723877
I0203 02:10:13.145319 139923852027648 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.0937861204147339, loss=3.489570379257202
I0203 02:10:57.175573 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:11:07.815882 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:11:41.373591 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:11:43.011220 140085747812160 submission_runner.py:408] Time since start: 29276.72s, 	Step: 56995, 	{'train/accuracy': 0.4382421672344208, 'train/loss': 2.575744867324829, 'validation/accuracy': 0.41349998116493225, 'validation/loss': 2.7167928218841553, 'validation/num_examples': 50000, 'test/accuracy': 0.3207000195980072, 'test/loss': 3.299652099609375, 'test/num_examples': 10000, 'score': 26505.334926843643, 'total_duration': 29276.715250492096, 'accumulated_submission_time': 26505.334926843643, 'accumulated_eval_time': 2765.686028242111, 'accumulated_logging_time': 2.5117812156677246}
I0203 02:11:43.038015 139923868813056 logging_writer.py:48] [56995] accumulated_eval_time=2765.686028, accumulated_logging_time=2.511781, accumulated_submission_time=26505.334927, global_step=56995, preemption_count=0, score=26505.334927, test/accuracy=0.320700, test/loss=3.299652, test/num_examples=10000, total_duration=29276.715250, train/accuracy=0.438242, train/loss=2.575745, validation/accuracy=0.413500, validation/loss=2.716793, validation/num_examples=50000
I0203 02:11:45.397047 139923852027648 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1985176801681519, loss=3.6011698246002197
I0203 02:12:27.520156 139923868813056 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2988409996032715, loss=3.6624338626861572
I0203 02:13:14.441180 139923852027648 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.9135745167732239, loss=3.9268438816070557
I0203 02:14:01.254963 139923868813056 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.0180820226669312, loss=3.7281384468078613
I0203 02:14:48.371268 139923852027648 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.0241422653198242, loss=3.859551429748535
I0203 02:15:35.461416 139923868813056 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.9553121328353882, loss=3.360097885131836
I0203 02:16:22.197163 139923852027648 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.7955825924873352, loss=4.843896865844727
I0203 02:17:09.005726 139923868813056 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.9720582962036133, loss=3.8312807083129883
I0203 02:17:55.853762 139923852027648 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.0710325241088867, loss=3.7160511016845703
I0203 02:18:43.222322 139923868813056 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2126684188842773, loss=3.558701992034912
I0203 02:18:43.238677 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:18:54.293972 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:19:26.222600 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:19:27.863116 140085747812160 submission_runner.py:408] Time since start: 29741.57s, 	Step: 57901, 	{'train/accuracy': 0.44578123092651367, 'train/loss': 2.49980092048645, 'validation/accuracy': 0.41985997557640076, 'validation/loss': 2.653440475463867, 'validation/num_examples': 50000, 'test/accuracy': 0.32110002636909485, 'test/loss': 3.273627281188965, 'test/num_examples': 10000, 'score': 26925.474437713623, 'total_duration': 29741.56713962555, 'accumulated_submission_time': 26925.474437713623, 'accumulated_eval_time': 2810.3104593753815, 'accumulated_logging_time': 2.5485615730285645}
I0203 02:19:27.889875 139923852027648 logging_writer.py:48] [57901] accumulated_eval_time=2810.310459, accumulated_logging_time=2.548562, accumulated_submission_time=26925.474438, global_step=57901, preemption_count=0, score=26925.474438, test/accuracy=0.321100, test/loss=3.273627, test/num_examples=10000, total_duration=29741.567140, train/accuracy=0.445781, train/loss=2.499801, validation/accuracy=0.419860, validation/loss=2.653440, validation/num_examples=50000
I0203 02:20:09.659003 139923868813056 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7236829996109009, loss=5.760568618774414
I0203 02:20:56.313490 139923852027648 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.7242757081985474, loss=5.8540263175964355
I0203 02:21:43.489890 139923868813056 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.1422821283340454, loss=3.531803846359253
I0203 02:22:30.548089 139923852027648 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.068100094795227, loss=3.608973979949951
I0203 02:23:17.396207 139923868813056 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.9391204714775085, loss=5.313470840454102
I0203 02:24:04.065539 139923852027648 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.0248440504074097, loss=3.620344877243042
I0203 02:24:51.075443 139923868813056 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.868804931640625, loss=5.51956033706665
I0203 02:25:37.985630 139923852027648 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.9232465028762817, loss=3.5560319423675537
I0203 02:26:24.841849 139923868813056 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.7785975933074951, loss=5.481609344482422
I0203 02:26:28.286996 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:26:38.829712 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:27:10.742542 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:27:12.384692 140085747812160 submission_runner.py:408] Time since start: 30206.09s, 	Step: 58809, 	{'train/accuracy': 0.4603320062160492, 'train/loss': 2.395381212234497, 'validation/accuracy': 0.4214800000190735, 'validation/loss': 2.604405164718628, 'validation/num_examples': 50000, 'test/accuracy': 0.33000001311302185, 'test/loss': 3.225630521774292, 'test/num_examples': 10000, 'score': 27345.811821460724, 'total_duration': 30206.088725328445, 'accumulated_submission_time': 27345.811821460724, 'accumulated_eval_time': 2854.4081478118896, 'accumulated_logging_time': 2.5851545333862305}
I0203 02:27:12.410872 139923852027648 logging_writer.py:48] [58809] accumulated_eval_time=2854.408148, accumulated_logging_time=2.585155, accumulated_submission_time=27345.811821, global_step=58809, preemption_count=0, score=27345.811821, test/accuracy=0.330000, test/loss=3.225631, test/num_examples=10000, total_duration=30206.088725, train/accuracy=0.460332, train/loss=2.395381, validation/accuracy=0.421480, validation/loss=2.604405, validation/num_examples=50000
I0203 02:27:50.445436 139923868813056 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.6368253231048584, loss=5.639192581176758
I0203 02:28:37.061872 139923852027648 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.1511245965957642, loss=3.640012264251709
I0203 02:29:24.194590 139923868813056 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0735899209976196, loss=3.455437660217285
I0203 02:30:11.193093 139923852027648 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.9648370146751404, loss=3.583014726638794
I0203 02:30:58.078957 139923868813056 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.7678216099739075, loss=5.116943836212158
I0203 02:31:45.181842 139923852027648 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.031896948814392, loss=3.6077778339385986
I0203 02:32:32.431395 139923868813056 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.988297700881958, loss=4.057981014251709
I0203 02:33:19.529305 139923852027648 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.3244140148162842, loss=3.598031759262085
I0203 02:34:06.906714 139923868813056 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.1573731899261475, loss=3.7239990234375
I0203 02:34:12.650103 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:34:23.119886 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:34:54.009800 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:34:55.648965 140085747812160 submission_runner.py:408] Time since start: 30669.35s, 	Step: 59714, 	{'train/accuracy': 0.4366796910762787, 'train/loss': 2.612844705581665, 'validation/accuracy': 0.4098999798297882, 'validation/loss': 2.748467445373535, 'validation/num_examples': 50000, 'test/accuracy': 0.32100000977516174, 'test/loss': 3.3422865867614746, 'test/num_examples': 10000, 'score': 27765.990793704987, 'total_duration': 30669.352987527847, 'accumulated_submission_time': 27765.990793704987, 'accumulated_eval_time': 2897.407002687454, 'accumulated_logging_time': 2.621112585067749}
I0203 02:34:55.674615 139923852027648 logging_writer.py:48] [59714] accumulated_eval_time=2897.407003, accumulated_logging_time=2.621113, accumulated_submission_time=27765.990794, global_step=59714, preemption_count=0, score=27765.990794, test/accuracy=0.321000, test/loss=3.342287, test/num_examples=10000, total_duration=30669.352988, train/accuracy=0.436680, train/loss=2.612845, validation/accuracy=0.409900, validation/loss=2.748467, validation/num_examples=50000
I0203 02:35:31.594775 139923868813056 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.8638205528259277, loss=5.742663383483887
I0203 02:36:18.618810 139923852027648 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2167452573776245, loss=3.6592679023742676
I0203 02:37:05.736567 139923868813056 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.757766604423523, loss=5.012404441833496
I0203 02:37:52.627197 139923852027648 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.9802379608154297, loss=3.583181619644165
I0203 02:38:39.643788 139923868813056 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.9779505729675293, loss=3.75697660446167
I0203 02:39:26.784141 139923852027648 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1107369661331177, loss=3.4334099292755127
I0203 02:40:13.959041 139923868813056 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.9497982263565063, loss=3.637139320373535
I0203 02:41:00.878496 139923852027648 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.9262863397598267, loss=4.208537578582764
I0203 02:41:47.797259 139923868813056 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.9755058884620667, loss=3.6248133182525635
I0203 02:41:55.956515 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:42:06.651487 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:42:42.737489 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:42:44.386286 140085747812160 submission_runner.py:408] Time since start: 31138.09s, 	Step: 60619, 	{'train/accuracy': 0.4499804675579071, 'train/loss': 2.467857837677002, 'validation/accuracy': 0.4191199839115143, 'validation/loss': 2.639002799987793, 'validation/num_examples': 50000, 'test/accuracy': 0.3256000280380249, 'test/loss': 3.250809669494629, 'test/num_examples': 10000, 'score': 28186.21174645424, 'total_duration': 31138.090316534042, 'accumulated_submission_time': 28186.21174645424, 'accumulated_eval_time': 2945.836772441864, 'accumulated_logging_time': 2.6567232608795166}
I0203 02:42:44.414778 139923852027648 logging_writer.py:48] [60619] accumulated_eval_time=2945.836772, accumulated_logging_time=2.656723, accumulated_submission_time=28186.211746, global_step=60619, preemption_count=0, score=28186.211746, test/accuracy=0.325600, test/loss=3.250810, test/num_examples=10000, total_duration=31138.090317, train/accuracy=0.449980, train/loss=2.467858, validation/accuracy=0.419120, validation/loss=2.639003, validation/num_examples=50000
I0203 02:43:17.963422 139923868813056 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.9939458966255188, loss=4.576383113861084
I0203 02:44:04.819849 139923852027648 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.0837295055389404, loss=3.5794239044189453
I0203 02:44:51.765253 139923868813056 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.7894731163978577, loss=5.761018753051758
I0203 02:45:38.443224 139923852027648 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.9789616465568542, loss=4.07513427734375
I0203 02:46:25.302786 139923868813056 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.102981448173523, loss=3.402815580368042
I0203 02:47:11.994095 139923852027648 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.1973931789398193, loss=3.547372341156006
I0203 02:47:58.833393 139923868813056 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.815987765789032, loss=5.4211039543151855
I0203 02:48:45.560050 139923852027648 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.2488958835601807, loss=3.69551944732666
I0203 02:49:32.247791 139923868813056 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.1605902910232544, loss=3.5867300033569336
I0203 02:49:44.634678 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:49:55.015285 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:50:27.229965 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:50:28.892914 140085747812160 submission_runner.py:408] Time since start: 31602.60s, 	Step: 61528, 	{'train/accuracy': 0.4624609351158142, 'train/loss': 2.3810806274414062, 'validation/accuracy': 0.4293999969959259, 'validation/loss': 2.5703067779541016, 'validation/num_examples': 50000, 'test/accuracy': 0.33500000834465027, 'test/loss': 3.217402696609497, 'test/num_examples': 10000, 'score': 28606.36927008629, 'total_duration': 31602.596896648407, 'accumulated_submission_time': 28606.36927008629, 'accumulated_eval_time': 2990.0949623584747, 'accumulated_logging_time': 2.695803642272949}
I0203 02:50:28.927317 139923852027648 logging_writer.py:48] [61528] accumulated_eval_time=2990.094962, accumulated_logging_time=2.695804, accumulated_submission_time=28606.369270, global_step=61528, preemption_count=0, score=28606.369270, test/accuracy=0.335000, test/loss=3.217403, test/num_examples=10000, total_duration=31602.596897, train/accuracy=0.462461, train/loss=2.381081, validation/accuracy=0.429400, validation/loss=2.570307, validation/num_examples=50000
I0203 02:50:57.849214 139923868813056 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.9054036736488342, loss=5.048986434936523
I0203 02:51:44.617338 139923852027648 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.1819541454315186, loss=3.5872907638549805
I0203 02:52:31.990115 139923868813056 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.1662144660949707, loss=3.5520787239074707
I0203 02:53:18.978012 139923852027648 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.1766074895858765, loss=3.792100667953491
I0203 02:54:05.656467 139923868813056 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.8757917284965515, loss=5.10455846786499
I0203 02:54:52.620388 139923852027648 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.0551425218582153, loss=3.53371524810791
I0203 02:55:39.572589 139923868813056 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.8902193903923035, loss=5.367014408111572
I0203 02:56:26.600353 139923852027648 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.1508606672286987, loss=3.8203132152557373
I0203 02:57:13.553543 139923868813056 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.0280948877334595, loss=3.6610312461853027
I0203 02:57:29.250991 140085747812160 spec.py:321] Evaluating on the training split.
I0203 02:57:40.691361 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 02:58:09.425356 140085747812160 spec.py:349] Evaluating on the test split.
I0203 02:58:11.073584 140085747812160 submission_runner.py:408] Time since start: 32064.78s, 	Step: 62435, 	{'train/accuracy': 0.46302732825279236, 'train/loss': 2.4418976306915283, 'validation/accuracy': 0.43140000104904175, 'validation/loss': 2.5994136333465576, 'validation/num_examples': 50000, 'test/accuracy': 0.33820000290870667, 'test/loss': 3.2195005416870117, 'test/num_examples': 10000, 'score': 29026.63190627098, 'total_duration': 32064.77761387825, 'accumulated_submission_time': 29026.63190627098, 'accumulated_eval_time': 3031.917558193207, 'accumulated_logging_time': 2.7409818172454834}
I0203 02:58:11.102753 139923852027648 logging_writer.py:48] [62435] accumulated_eval_time=3031.917558, accumulated_logging_time=2.740982, accumulated_submission_time=29026.631906, global_step=62435, preemption_count=0, score=29026.631906, test/accuracy=0.338200, test/loss=3.219501, test/num_examples=10000, total_duration=32064.777614, train/accuracy=0.463027, train/loss=2.441898, validation/accuracy=0.431400, validation/loss=2.599414, validation/num_examples=50000
I0203 02:58:36.993823 139923868813056 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.919352650642395, loss=4.226321220397949
I0203 02:59:23.312921 139923852027648 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.7903499007225037, loss=5.028443336486816
I0203 03:00:10.155556 139923868813056 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.1531777381896973, loss=3.6604905128479004
I0203 03:00:57.229619 139923852027648 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.0241252183914185, loss=5.839406490325928
I0203 03:01:44.436501 139923868813056 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.2371739149093628, loss=3.398287296295166
I0203 03:02:31.687422 139923852027648 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1665891408920288, loss=3.526488780975342
I0203 03:03:18.750813 139923868813056 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.1615651845932007, loss=3.521329879760742
I0203 03:04:05.676756 139923852027648 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.8246839046478271, loss=5.19212532043457
I0203 03:04:52.547641 139923868813056 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.8871682286262512, loss=5.680353164672852
I0203 03:05:11.511799 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:05:22.188751 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:05:53.723900 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:05:55.372065 140085747812160 submission_runner.py:408] Time since start: 32529.08s, 	Step: 63342, 	{'train/accuracy': 0.46730467677116394, 'train/loss': 2.354846954345703, 'validation/accuracy': 0.4369799792766571, 'validation/loss': 2.523207664489746, 'validation/num_examples': 50000, 'test/accuracy': 0.34130001068115234, 'test/loss': 3.1512272357940674, 'test/num_examples': 10000, 'score': 29446.979049682617, 'total_duration': 32529.076095342636, 'accumulated_submission_time': 29446.979049682617, 'accumulated_eval_time': 3075.777815580368, 'accumulated_logging_time': 2.7796471118927}
I0203 03:05:55.400870 139923852027648 logging_writer.py:48] [63342] accumulated_eval_time=3075.777816, accumulated_logging_time=2.779647, accumulated_submission_time=29446.979050, global_step=63342, preemption_count=0, score=29446.979050, test/accuracy=0.341300, test/loss=3.151227, test/num_examples=10000, total_duration=32529.076095, train/accuracy=0.467305, train/loss=2.354847, validation/accuracy=0.436980, validation/loss=2.523208, validation/num_examples=50000
I0203 03:06:18.535934 139923868813056 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.8708097338676453, loss=5.815146446228027
I0203 03:07:04.931714 139923852027648 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.1448367834091187, loss=3.4382150173187256
I0203 03:07:51.712535 139923868813056 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.9150621294975281, loss=5.216172218322754
I0203 03:08:38.644972 139923852027648 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.0591981410980225, loss=3.611192226409912
I0203 03:09:25.684620 139923868813056 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.0685791969299316, loss=3.568056583404541
I0203 03:10:12.605582 139923852027648 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.8599528074264526, loss=4.221593856811523
I0203 03:10:59.522287 139923868813056 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2639355659484863, loss=3.499056100845337
I0203 03:11:46.538548 139923852027648 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.1095627546310425, loss=3.553419589996338
I0203 03:12:34.122705 139923868813056 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.9221939444541931, loss=4.582883834838867
I0203 03:12:55.770465 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:13:06.532094 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:13:39.292746 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:13:40.934781 140085747812160 submission_runner.py:408] Time since start: 32994.64s, 	Step: 64248, 	{'train/accuracy': 0.4647851586341858, 'train/loss': 2.3934576511383057, 'validation/accuracy': 0.4303799867630005, 'validation/loss': 2.5851118564605713, 'validation/num_examples': 50000, 'test/accuracy': 0.3419000208377838, 'test/loss': 3.1868813037872314, 'test/num_examples': 10000, 'score': 29867.28816127777, 'total_duration': 32994.63881659508, 'accumulated_submission_time': 29867.28816127777, 'accumulated_eval_time': 3120.9421343803406, 'accumulated_logging_time': 2.8182005882263184}
I0203 03:13:40.965689 139923852027648 logging_writer.py:48] [64248] accumulated_eval_time=3120.942134, accumulated_logging_time=2.818201, accumulated_submission_time=29867.288161, global_step=64248, preemption_count=0, score=29867.288161, test/accuracy=0.341900, test/loss=3.186881, test/num_examples=10000, total_duration=32994.638817, train/accuracy=0.464785, train/loss=2.393458, validation/accuracy=0.430380, validation/loss=2.585112, validation/num_examples=50000
I0203 03:14:01.747634 139923868813056 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.3743478059768677, loss=3.3597190380096436
I0203 03:14:47.375481 139923852027648 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.9814425110816956, loss=3.454176187515259
I0203 03:15:34.361401 139923868813056 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.023628830909729, loss=3.3647072315216064
I0203 03:16:21.254812 139923852027648 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.9990653991699219, loss=3.6177618503570557
I0203 03:17:08.259645 139923868813056 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.074763298034668, loss=5.705967903137207
I0203 03:17:55.143059 139923852027648 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.199615240097046, loss=3.490551471710205
I0203 03:18:42.211864 139923868813056 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.9227879047393799, loss=3.9739108085632324
I0203 03:19:29.314260 139923852027648 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.9670937061309814, loss=3.586623191833496
I0203 03:20:16.203288 139923868813056 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.1545978784561157, loss=3.5152461528778076
I0203 03:20:41.224256 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:20:51.771789 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:21:19.455958 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:21:21.091639 140085747812160 submission_runner.py:408] Time since start: 33454.80s, 	Step: 65155, 	{'train/accuracy': 0.4700976312160492, 'train/loss': 2.4589695930480957, 'validation/accuracy': 0.4227599799633026, 'validation/loss': 2.704218626022339, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.3210971355438232, 'test/num_examples': 10000, 'score': 30287.485833883286, 'total_duration': 33454.79567170143, 'accumulated_submission_time': 30287.485833883286, 'accumulated_eval_time': 3160.80952334404, 'accumulated_logging_time': 2.8595807552337646}
I0203 03:21:21.118141 139923852027648 logging_writer.py:48] [65155] accumulated_eval_time=3160.809523, accumulated_logging_time=2.859581, accumulated_submission_time=30287.485834, global_step=65155, preemption_count=0, score=30287.485834, test/accuracy=0.325700, test/loss=3.321097, test/num_examples=10000, total_duration=33454.795672, train/accuracy=0.470098, train/loss=2.458970, validation/accuracy=0.422760, validation/loss=2.704219, validation/num_examples=50000
I0203 03:21:39.164124 139923868813056 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.9885073900222778, loss=3.4665257930755615
I0203 03:22:24.247709 139923852027648 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.1335583925247192, loss=3.4166412353515625
I0203 03:23:11.702705 139923868813056 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.1375852823257446, loss=3.5314321517944336
I0203 03:23:58.586406 139923852027648 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2039035558700562, loss=3.639410972595215
I0203 03:24:45.794763 139923868813056 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.0365667343139648, loss=4.425997734069824
I0203 03:25:32.842895 139923852027648 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.1054779291152954, loss=3.7927584648132324
I0203 03:26:20.167625 139923868813056 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.092423677444458, loss=3.3734469413757324
I0203 03:27:07.184825 139923852027648 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.8164733052253723, loss=5.832394123077393
I0203 03:27:54.252021 139923868813056 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.029905080795288, loss=3.839048385620117
I0203 03:28:21.212862 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:28:31.837036 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:29:02.422332 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:29:04.067299 140085747812160 submission_runner.py:408] Time since start: 33917.77s, 	Step: 66059, 	{'train/accuracy': 0.4700585901737213, 'train/loss': 2.350682258605957, 'validation/accuracy': 0.4400999844074249, 'validation/loss': 2.517786741256714, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.131437301635742, 'test/num_examples': 10000, 'score': 30707.519748210907, 'total_duration': 33917.77133059502, 'accumulated_submission_time': 30707.519748210907, 'accumulated_eval_time': 3203.6639914512634, 'accumulated_logging_time': 2.895918846130371}
I0203 03:29:04.096424 139923852027648 logging_writer.py:48] [66059] accumulated_eval_time=3203.663991, accumulated_logging_time=2.895919, accumulated_submission_time=30707.519748, global_step=66059, preemption_count=0, score=30707.519748, test/accuracy=0.346100, test/loss=3.131437, test/num_examples=10000, total_duration=33917.771331, train/accuracy=0.470059, train/loss=2.350682, validation/accuracy=0.440100, validation/loss=2.517787, validation/num_examples=50000
I0203 03:29:20.581766 139923868813056 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.0553089380264282, loss=5.584874153137207
I0203 03:30:05.547942 139923852027648 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9580076336860657, loss=5.103705883026123
I0203 03:30:52.481535 139923868813056 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2166005373001099, loss=4.319575786590576
I0203 03:31:39.763339 139923852027648 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.7654335498809814, loss=5.142305374145508
I0203 03:32:26.963847 139923868813056 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9115604162216187, loss=5.7365403175354
I0203 03:33:14.139721 139923852027648 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.1480262279510498, loss=3.468212127685547
I0203 03:34:01.209277 139923868813056 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.2017383575439453, loss=3.4978983402252197
I0203 03:34:48.506428 139923852027648 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.1728121042251587, loss=3.5930776596069336
I0203 03:35:35.593147 139923868813056 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1130588054656982, loss=3.4463977813720703
I0203 03:36:04.163995 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:36:14.702808 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:36:48.585544 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:36:50.230551 140085747812160 submission_runner.py:408] Time since start: 34383.93s, 	Step: 66962, 	{'train/accuracy': 0.46162107586860657, 'train/loss': 2.3884122371673584, 'validation/accuracy': 0.4339599907398224, 'validation/loss': 2.554633617401123, 'validation/num_examples': 50000, 'test/accuracy': 0.3354000151157379, 'test/loss': 3.2074503898620605, 'test/num_examples': 10000, 'score': 31127.525916337967, 'total_duration': 34383.9345805645, 'accumulated_submission_time': 31127.525916337967, 'accumulated_eval_time': 3249.730548620224, 'accumulated_logging_time': 2.936368942260742}
I0203 03:36:50.259314 139923852027648 logging_writer.py:48] [66962] accumulated_eval_time=3249.730549, accumulated_logging_time=2.936369, accumulated_submission_time=31127.525916, global_step=66962, preemption_count=0, score=31127.525916, test/accuracy=0.335400, test/loss=3.207450, test/num_examples=10000, total_duration=34383.934581, train/accuracy=0.461621, train/loss=2.388412, validation/accuracy=0.433960, validation/loss=2.554634, validation/num_examples=50000
I0203 03:37:05.560697 139923868813056 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.0916975736618042, loss=3.4732277393341064
I0203 03:37:49.820761 139923852027648 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.9964564442634583, loss=3.702134370803833
I0203 03:38:36.941624 139923868813056 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.053739070892334, loss=3.647437334060669
I0203 03:39:23.939442 139923852027648 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1021794080734253, loss=3.593914031982422
I0203 03:40:11.021800 139923868813056 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.8528602719306946, loss=5.159543514251709
I0203 03:40:57.964516 139923852027648 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.0119917392730713, loss=3.4752590656280518
I0203 03:41:44.831128 139923868813056 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9565812945365906, loss=4.571341037750244
I0203 03:42:32.177744 139923852027648 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.0561760663986206, loss=3.5218303203582764
I0203 03:43:19.144824 139923868813056 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2867511510849, loss=3.4601049423217773
I0203 03:43:50.350406 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:44:00.899384 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:44:33.174957 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:44:34.823074 140085747812160 submission_runner.py:408] Time since start: 34848.53s, 	Step: 67868, 	{'train/accuracy': 0.5029687285423279, 'train/loss': 2.1818504333496094, 'validation/accuracy': 0.4394199848175049, 'validation/loss': 2.515748977661133, 'validation/num_examples': 50000, 'test/accuracy': 0.3392000198364258, 'test/loss': 3.1619865894317627, 'test/num_examples': 10000, 'score': 31547.554701805115, 'total_duration': 34848.527092695236, 'accumulated_submission_time': 31547.554701805115, 'accumulated_eval_time': 3294.203216075897, 'accumulated_logging_time': 2.976597309112549}
I0203 03:44:34.854449 139923852027648 logging_writer.py:48] [67868] accumulated_eval_time=3294.203216, accumulated_logging_time=2.976597, accumulated_submission_time=31547.554702, global_step=67868, preemption_count=0, score=31547.554702, test/accuracy=0.339200, test/loss=3.161987, test/num_examples=10000, total_duration=34848.527093, train/accuracy=0.502969, train/loss=2.181850, validation/accuracy=0.439420, validation/loss=2.515749, validation/num_examples=50000
I0203 03:44:47.818809 139923868813056 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.9676417708396912, loss=4.5272135734558105
I0203 03:45:31.871381 139923852027648 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.1799962520599365, loss=3.6952059268951416
I0203 03:46:18.951749 139923868813056 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0865243673324585, loss=3.571284055709839
I0203 03:47:05.886125 139923852027648 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.1682488918304443, loss=3.5021891593933105
I0203 03:47:52.791986 139923868813056 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.8365252614021301, loss=4.710020542144775
I0203 03:48:39.831604 139923852027648 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.1055588722229004, loss=3.4174911975860596
I0203 03:49:26.850605 139923868813056 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.8749938011169434, loss=4.998300075531006
I0203 03:50:13.890022 139923852027648 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.9644507169723511, loss=5.767691612243652
I0203 03:51:00.660102 139923868813056 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.9788950085639954, loss=4.068686485290527
I0203 03:51:35.122676 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:51:45.400205 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 03:52:16.542484 140085747812160 spec.py:349] Evaluating on the test split.
I0203 03:52:18.184488 140085747812160 submission_runner.py:408] Time since start: 35311.89s, 	Step: 68775, 	{'train/accuracy': 0.4720703065395355, 'train/loss': 2.326566219329834, 'validation/accuracy': 0.4456599950790405, 'validation/loss': 2.470954656600952, 'validation/num_examples': 50000, 'test/accuracy': 0.34380000829696655, 'test/loss': 3.1224875450134277, 'test/num_examples': 10000, 'score': 31967.763231039047, 'total_duration': 35311.88851070404, 'accumulated_submission_time': 31967.763231039047, 'accumulated_eval_time': 3337.2650215625763, 'accumulated_logging_time': 3.0178487300872803}
I0203 03:52:18.213110 139923852027648 logging_writer.py:48] [68775] accumulated_eval_time=3337.265022, accumulated_logging_time=3.017849, accumulated_submission_time=31967.763231, global_step=68775, preemption_count=0, score=31967.763231, test/accuracy=0.343800, test/loss=3.122488, test/num_examples=10000, total_duration=35311.888511, train/accuracy=0.472070, train/loss=2.326566, validation/accuracy=0.445660, validation/loss=2.470955, validation/num_examples=50000
I0203 03:52:28.419462 139923868813056 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.157137393951416, loss=3.365612030029297
I0203 03:53:12.043171 139923852027648 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.9622442722320557, loss=3.5425095558166504
I0203 03:53:58.758833 139923868813056 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.9055372476577759, loss=4.26348876953125
I0203 03:54:45.942531 139923852027648 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.8559698462486267, loss=5.195758819580078
I0203 03:55:32.716228 139923868813056 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.8753299117088318, loss=4.67543363571167
I0203 03:56:19.560329 139923852027648 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.8978736996650696, loss=3.8036742210388184
I0203 03:57:06.604060 139923868813056 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.7676186561584473, loss=5.160801887512207
I0203 03:57:53.448887 139923852027648 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0438350439071655, loss=3.7975263595581055
I0203 03:58:40.570949 139923868813056 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.0324490070343018, loss=4.163356304168701
I0203 03:59:18.390591 140085747812160 spec.py:321] Evaluating on the training split.
I0203 03:59:28.902388 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:00:01.277409 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:00:02.926407 140085747812160 submission_runner.py:408] Time since start: 35776.63s, 	Step: 69682, 	{'train/accuracy': 0.4788476526737213, 'train/loss': 2.295114278793335, 'validation/accuracy': 0.4430199861526489, 'validation/loss': 2.4969727993011475, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.122164487838745, 'test/num_examples': 10000, 'score': 32387.879041671753, 'total_duration': 35776.63043308258, 'accumulated_submission_time': 32387.879041671753, 'accumulated_eval_time': 3381.8008399009705, 'accumulated_logging_time': 3.0573742389678955}
I0203 04:00:02.958475 139923852027648 logging_writer.py:48] [69682] accumulated_eval_time=3381.800840, accumulated_logging_time=3.057374, accumulated_submission_time=32387.879042, global_step=69682, preemption_count=0, score=32387.879042, test/accuracy=0.346200, test/loss=3.122164, test/num_examples=10000, total_duration=35776.630433, train/accuracy=0.478848, train/loss=2.295114, validation/accuracy=0.443020, validation/loss=2.496973, validation/num_examples=50000
I0203 04:00:10.416472 139923868813056 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.127211332321167, loss=3.565135955810547
I0203 04:00:53.702205 139923852027648 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.9774087071418762, loss=3.2708075046539307
I0203 04:01:40.731391 139923868813056 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.0775936841964722, loss=3.337679862976074
I0203 04:02:27.695580 139923852027648 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.0809831619262695, loss=4.306649684906006
I0203 04:03:14.849818 139923868813056 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.051539421081543, loss=3.4215900897979736
I0203 04:04:01.608525 139923852027648 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.0129040479660034, loss=4.046898365020752
I0203 04:04:48.388581 139923868813056 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.134951114654541, loss=3.2999470233917236
I0203 04:05:35.386037 139923852027648 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.9127085208892822, loss=4.773910045623779
I0203 04:06:22.418104 139923868813056 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.0431171655654907, loss=5.701478481292725
I0203 04:07:03.028703 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:07:13.328879 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:07:42.405408 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:07:44.043938 140085747812160 submission_runner.py:408] Time since start: 36237.75s, 	Step: 70588, 	{'train/accuracy': 0.4978906214237213, 'train/loss': 2.208686113357544, 'validation/accuracy': 0.44947999715805054, 'validation/loss': 2.4724645614624023, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.092029333114624, 'test/num_examples': 10000, 'score': 32807.88691544533, 'total_duration': 36237.747968912125, 'accumulated_submission_time': 32807.88691544533, 'accumulated_eval_time': 3422.816102027893, 'accumulated_logging_time': 3.100841999053955}
I0203 04:07:44.074023 139923852027648 logging_writer.py:48] [70588] accumulated_eval_time=3422.816102, accumulated_logging_time=3.100842, accumulated_submission_time=32807.886915, global_step=70588, preemption_count=0, score=32807.886915, test/accuracy=0.346200, test/loss=3.092029, test/num_examples=10000, total_duration=36237.747969, train/accuracy=0.497891, train/loss=2.208686, validation/accuracy=0.449480, validation/loss=2.472465, validation/num_examples=50000
I0203 04:07:49.174158 139923868813056 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.0743435621261597, loss=3.4325156211853027
I0203 04:08:32.108666 139923852027648 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.1175827980041504, loss=3.4884989261627197
I0203 04:09:18.927105 139923868813056 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.2389819622039795, loss=3.3929498195648193
I0203 04:10:05.666367 139923852027648 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.8618866801261902, loss=4.808103084564209
I0203 04:10:52.682327 139923868813056 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3910247087478638, loss=3.773005723953247
I0203 04:11:39.471228 139923852027648 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.9630075693130493, loss=5.682150363922119
I0203 04:12:26.441881 139923868813056 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.1915892362594604, loss=3.5266847610473633
I0203 04:13:13.557402 139923852027648 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.8037040829658508, loss=4.681423187255859
I0203 04:14:00.274121 139923868813056 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.0456122159957886, loss=3.395219564437866
I0203 04:14:44.467049 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:14:54.964047 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:15:28.115530 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:15:29.745987 140085747812160 submission_runner.py:408] Time since start: 36703.45s, 	Step: 71496, 	{'train/accuracy': 0.47214841842651367, 'train/loss': 2.3273391723632812, 'validation/accuracy': 0.4462999999523163, 'validation/loss': 2.475956678390503, 'validation/num_examples': 50000, 'test/accuracy': 0.34060001373291016, 'test/loss': 3.128873109817505, 'test/num_examples': 10000, 'score': 33228.21760249138, 'total_duration': 36703.45001864433, 'accumulated_submission_time': 33228.21760249138, 'accumulated_eval_time': 3468.0950469970703, 'accumulated_logging_time': 3.1422665119171143}
I0203 04:15:29.776175 139923852027648 logging_writer.py:48] [71496] accumulated_eval_time=3468.095047, accumulated_logging_time=3.142267, accumulated_submission_time=33228.217602, global_step=71496, preemption_count=0, score=33228.217602, test/accuracy=0.340600, test/loss=3.128873, test/num_examples=10000, total_duration=36703.450019, train/accuracy=0.472148, train/loss=2.327339, validation/accuracy=0.446300, validation/loss=2.475957, validation/num_examples=50000
I0203 04:15:31.749663 139923868813056 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1168502569198608, loss=3.409149646759033
I0203 04:16:13.772244 139923852027648 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.9474678039550781, loss=4.413517475128174
I0203 04:17:00.843331 139923868813056 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.026689887046814, loss=3.340740203857422
I0203 04:17:47.947374 139923852027648 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.0191532373428345, loss=4.232570171356201
I0203 04:18:34.802919 139923868813056 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.0844606161117554, loss=3.7062740325927734
I0203 04:19:21.687254 139923852027648 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.127246618270874, loss=4.030569076538086
I0203 04:20:08.501492 139923868813056 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.9546287059783936, loss=3.9140737056732178
I0203 04:20:55.308706 139923852027648 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.178017020225525, loss=3.6507956981658936
I0203 04:21:42.193192 139923868813056 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.1814866065979004, loss=3.450321912765503
I0203 04:22:29.240976 139923852027648 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.0675829648971558, loss=3.5360355377197266
I0203 04:22:29.830188 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:22:40.414619 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:23:11.881673 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:23:13.526737 140085747812160 submission_runner.py:408] Time since start: 37167.23s, 	Step: 72403, 	{'train/accuracy': 0.4733007848262787, 'train/loss': 2.381632089614868, 'validation/accuracy': 0.44369998574256897, 'validation/loss': 2.5497748851776123, 'validation/num_examples': 50000, 'test/accuracy': 0.34870001673698425, 'test/loss': 3.151477575302124, 'test/num_examples': 10000, 'score': 33648.21060991287, 'total_duration': 37167.230764865875, 'accumulated_submission_time': 33648.21060991287, 'accumulated_eval_time': 3511.7916102409363, 'accumulated_logging_time': 3.1832900047302246}
I0203 04:23:13.553986 139923868813056 logging_writer.py:48] [72403] accumulated_eval_time=3511.791610, accumulated_logging_time=3.183290, accumulated_submission_time=33648.210610, global_step=72403, preemption_count=0, score=33648.210610, test/accuracy=0.348700, test/loss=3.151478, test/num_examples=10000, total_duration=37167.230765, train/accuracy=0.473301, train/loss=2.381632, validation/accuracy=0.443700, validation/loss=2.549775, validation/num_examples=50000
I0203 04:23:54.342599 139923852027648 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.8899319171905518, loss=5.521338939666748
I0203 04:24:41.191036 139923868813056 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.0266964435577393, loss=3.249171495437622
I0203 04:25:28.020150 139923852027648 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.7973517775535583, loss=5.498483657836914
I0203 04:26:15.069350 139923868813056 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.065502405166626, loss=3.411688804626465
I0203 04:27:02.117272 139923852027648 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.9580736756324768, loss=5.206876277923584
I0203 04:27:49.030510 139923868813056 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.9027365446090698, loss=4.039363861083984
I0203 04:28:35.996359 139923852027648 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.1833726167678833, loss=3.2913742065429688
I0203 04:29:23.106305 139923868813056 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.0611610412597656, loss=3.376883029937744
I0203 04:30:09.875695 139923852027648 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.0980796813964844, loss=3.309713125228882
I0203 04:30:13.825363 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:30:24.112609 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:30:54.241456 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:30:55.872305 140085747812160 submission_runner.py:408] Time since start: 37629.58s, 	Step: 73310, 	{'train/accuracy': 0.48337888717651367, 'train/loss': 2.329761266708374, 'validation/accuracy': 0.4386399984359741, 'validation/loss': 2.551053047180176, 'validation/num_examples': 50000, 'test/accuracy': 0.3387000262737274, 'test/loss': 3.1976158618927, 'test/num_examples': 10000, 'score': 34068.42133665085, 'total_duration': 37629.576330661774, 'accumulated_submission_time': 34068.42133665085, 'accumulated_eval_time': 3553.8385372161865, 'accumulated_logging_time': 3.2199816703796387}
I0203 04:30:55.899312 139923868813056 logging_writer.py:48] [73310] accumulated_eval_time=3553.838537, accumulated_logging_time=3.219982, accumulated_submission_time=34068.421337, global_step=73310, preemption_count=0, score=34068.421337, test/accuracy=0.338700, test/loss=3.197616, test/num_examples=10000, total_duration=37629.576331, train/accuracy=0.483379, train/loss=2.329761, validation/accuracy=0.438640, validation/loss=2.551053, validation/num_examples=50000
I0203 04:31:33.627369 139923852027648 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.0545889139175415, loss=3.3033156394958496
I0203 04:32:20.543991 139923868813056 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.9186665415763855, loss=3.608675479888916
I0203 04:33:07.701956 139923852027648 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.0833148956298828, loss=3.3468871116638184
I0203 04:33:54.367663 139923868813056 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.0706212520599365, loss=3.3363678455352783
I0203 04:34:41.352448 139923852027648 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.1460165977478027, loss=3.4076216220855713
I0203 04:35:28.294504 139923868813056 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.7410843968391418, loss=5.194472312927246
I0203 04:36:15.016705 139923852027648 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.9720411896705627, loss=4.0945634841918945
I0203 04:37:02.251872 139923868813056 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.098225474357605, loss=3.6265709400177
I0203 04:37:49.256464 139923852027648 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.8930702209472656, loss=5.78515625
I0203 04:37:55.999170 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:38:06.784323 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:38:39.531710 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:38:41.175995 140085747812160 submission_runner.py:408] Time since start: 38094.88s, 	Step: 74216, 	{'train/accuracy': 0.48607420921325684, 'train/loss': 2.254650354385376, 'validation/accuracy': 0.45413997769355774, 'validation/loss': 2.4290952682495117, 'validation/num_examples': 50000, 'test/accuracy': 0.3589000105857849, 'test/loss': 3.0651798248291016, 'test/num_examples': 10000, 'score': 34488.45884680748, 'total_duration': 38094.87999844551, 'accumulated_submission_time': 34488.45884680748, 'accumulated_eval_time': 3599.0153257846832, 'accumulated_logging_time': 3.2576658725738525}
I0203 04:38:41.203823 139923868813056 logging_writer.py:48] [74216] accumulated_eval_time=3599.015326, accumulated_logging_time=3.257666, accumulated_submission_time=34488.458847, global_step=74216, preemption_count=0, score=34488.458847, test/accuracy=0.358900, test/loss=3.065180, test/num_examples=10000, total_duration=38094.879998, train/accuracy=0.486074, train/loss=2.254650, validation/accuracy=0.454140, validation/loss=2.429095, validation/num_examples=50000
I0203 04:39:16.049669 139923852027648 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.0534520149230957, loss=3.279299259185791
I0203 04:40:02.697796 139923868813056 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.042847990989685, loss=3.4062538146972656
I0203 04:40:49.893218 139923852027648 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.0041180849075317, loss=3.452077865600586
I0203 04:41:36.822589 139923868813056 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.1705996990203857, loss=3.5401504039764404
I0203 04:42:23.695934 139923852027648 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.0283890962600708, loss=3.434706687927246
I0203 04:43:10.903864 139923868813056 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.1141269207000732, loss=3.239360809326172
I0203 04:43:58.019328 139923852027648 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1434513330459595, loss=4.1871747970581055
I0203 04:44:45.004264 139923868813056 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.1299952268600464, loss=3.1918716430664062
I0203 04:45:31.861778 139923852027648 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.9624274969100952, loss=3.34419846534729
I0203 04:45:41.284318 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:45:51.532065 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:46:26.324347 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:46:27.974910 140085747812160 submission_runner.py:408] Time since start: 38561.68s, 	Step: 75122, 	{'train/accuracy': 0.4874609410762787, 'train/loss': 2.3284049034118652, 'validation/accuracy': 0.4536999762058258, 'validation/loss': 2.506525754928589, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.143037796020508, 'test/num_examples': 10000, 'score': 34908.47983670235, 'total_duration': 38561.67894077301, 'accumulated_submission_time': 34908.47983670235, 'accumulated_eval_time': 3645.7059197425842, 'accumulated_logging_time': 3.2945048809051514}
I0203 04:46:28.004307 139923868813056 logging_writer.py:48] [75122] accumulated_eval_time=3645.705920, accumulated_logging_time=3.294505, accumulated_submission_time=34908.479837, global_step=75122, preemption_count=0, score=34908.479837, test/accuracy=0.350500, test/loss=3.143038, test/num_examples=10000, total_duration=38561.678941, train/accuracy=0.487461, train/loss=2.328405, validation/accuracy=0.453700, validation/loss=2.506526, validation/num_examples=50000
I0203 04:46:59.725439 139923852027648 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.067653775215149, loss=3.6463074684143066
I0203 04:47:46.441318 139923868813056 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.7334498167037964, loss=5.265437602996826
I0203 04:48:33.652540 139923852027648 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.0214836597442627, loss=3.5888609886169434
I0203 04:49:20.435363 139923868813056 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.101561188697815, loss=3.226140022277832
I0203 04:50:07.199703 139923852027648 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.0823705196380615, loss=3.295693874359131
I0203 04:50:53.977770 139923868813056 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.854925274848938, loss=5.719028472900391
I0203 04:51:40.749991 139923852027648 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.0857923030853271, loss=3.6131184101104736
I0203 04:52:27.627829 139923868813056 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.7654585242271423, loss=5.111366271972656
I0203 04:53:14.528488 139923852027648 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.0026588439941406, loss=3.236879587173462
I0203 04:53:28.282137 140085747812160 spec.py:321] Evaluating on the training split.
I0203 04:53:38.942215 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 04:54:12.771770 140085747812160 spec.py:349] Evaluating on the test split.
I0203 04:54:14.417769 140085747812160 submission_runner.py:408] Time since start: 39028.12s, 	Step: 76031, 	{'train/accuracy': 0.49755859375, 'train/loss': 2.247817039489746, 'validation/accuracy': 0.4552599787712097, 'validation/loss': 2.4645187854766846, 'validation/num_examples': 50000, 'test/accuracy': 0.35430002212524414, 'test/loss': 3.110901117324829, 'test/num_examples': 10000, 'score': 35328.69760990143, 'total_duration': 39028.121799230576, 'accumulated_submission_time': 35328.69760990143, 'accumulated_eval_time': 3691.8415746688843, 'accumulated_logging_time': 3.3326914310455322}
I0203 04:54:14.447984 139923868813056 logging_writer.py:48] [76031] accumulated_eval_time=3691.841575, accumulated_logging_time=3.332691, accumulated_submission_time=35328.697610, global_step=76031, preemption_count=0, score=35328.697610, test/accuracy=0.354300, test/loss=3.110901, test/num_examples=10000, total_duration=39028.121799, train/accuracy=0.497559, train/loss=2.247817, validation/accuracy=0.455260, validation/loss=2.464519, validation/num_examples=50000
I0203 04:54:42.214557 139923852027648 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.9957967400550842, loss=3.4168834686279297
I0203 04:55:28.975003 139923868813056 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.48485267162323, loss=3.3594112396240234
I0203 04:56:15.667106 139923852027648 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.011171579360962, loss=3.3318026065826416
I0203 04:57:02.328530 139923868813056 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.0677952766418457, loss=3.137991189956665
I0203 04:57:49.249303 139923852027648 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.264412522315979, loss=3.299968719482422
I0203 04:58:36.154169 139923868813056 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.1983963251113892, loss=3.495460271835327
I0203 04:59:22.886139 139923852027648 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.0607342720031738, loss=3.286426544189453
I0203 05:00:09.837900 139923868813056 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.9040036797523499, loss=5.625459671020508
I0203 05:00:56.649814 139923852027648 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.1958849430084229, loss=3.366199493408203
I0203 05:01:14.530438 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:01:25.014619 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:01:53.913439 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:01:55.563299 140085747812160 submission_runner.py:408] Time since start: 39489.27s, 	Step: 76940, 	{'train/accuracy': 0.48652341961860657, 'train/loss': 2.275959014892578, 'validation/accuracy': 0.4581599831581116, 'validation/loss': 2.4188859462738037, 'validation/num_examples': 50000, 'test/accuracy': 0.35830003023147583, 'test/loss': 3.0840256214141846, 'test/num_examples': 10000, 'score': 35748.71830582619, 'total_duration': 39489.267315626144, 'accumulated_submission_time': 35748.71830582619, 'accumulated_eval_time': 3732.8744130134583, 'accumulated_logging_time': 3.374640941619873}
I0203 05:01:55.594388 139923868813056 logging_writer.py:48] [76940] accumulated_eval_time=3732.874413, accumulated_logging_time=3.374641, accumulated_submission_time=35748.718306, global_step=76940, preemption_count=0, score=35748.718306, test/accuracy=0.358300, test/loss=3.084026, test/num_examples=10000, total_duration=39489.267316, train/accuracy=0.486523, train/loss=2.275959, validation/accuracy=0.458160, validation/loss=2.418886, validation/num_examples=50000
I0203 05:02:19.544467 139923852027648 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.9716464877128601, loss=5.782038688659668
I0203 05:03:06.190136 139923868813056 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.047157883644104, loss=5.1571269035339355
I0203 05:03:53.451214 139923852027648 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.9532615542411804, loss=5.558619022369385
I0203 05:04:40.297060 139923868813056 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.9701294898986816, loss=4.869062423706055
I0203 05:05:27.426000 139923852027648 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.1006603240966797, loss=3.2700600624084473
I0203 05:06:14.086889 139923868813056 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.9618105292320251, loss=5.212625503540039
I0203 05:07:00.817936 139923852027648 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.9698190689086914, loss=4.003352165222168
I0203 05:07:47.735288 139923868813056 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.0945744514465332, loss=3.3358960151672363
I0203 05:08:34.447516 139923852027648 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.9510861039161682, loss=4.439090728759766
I0203 05:08:55.747262 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:09:06.196463 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:09:35.818021 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:09:37.451317 140085747812160 submission_runner.py:408] Time since start: 39951.16s, 	Step: 77847, 	{'train/accuracy': 0.5015038847923279, 'train/loss': 2.2032933235168457, 'validation/accuracy': 0.46413999795913696, 'validation/loss': 2.393378496170044, 'validation/num_examples': 50000, 'test/accuracy': 0.36670002341270447, 'test/loss': 3.028707265853882, 'test/num_examples': 10000, 'score': 36168.80930709839, 'total_duration': 39951.15533995628, 'accumulated_submission_time': 36168.80930709839, 'accumulated_eval_time': 3774.5784554481506, 'accumulated_logging_time': 3.415534734725952}
I0203 05:09:37.480719 139923868813056 logging_writer.py:48] [77847] accumulated_eval_time=3774.578455, accumulated_logging_time=3.415535, accumulated_submission_time=36168.809307, global_step=77847, preemption_count=0, score=36168.809307, test/accuracy=0.366700, test/loss=3.028707, test/num_examples=10000, total_duration=39951.155340, train/accuracy=0.501504, train/loss=2.203293, validation/accuracy=0.464140, validation/loss=2.393378, validation/num_examples=50000
I0203 05:09:58.678221 139923852027648 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.166500210762024, loss=3.3830673694610596
I0203 05:10:44.243557 139923868813056 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.0444082021713257, loss=3.212111473083496
I0203 05:11:31.407963 139923852027648 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.1246702671051025, loss=3.3058948516845703
I0203 05:12:17.953758 139923868813056 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.0518627166748047, loss=3.4299824237823486
I0203 05:13:05.290023 139923852027648 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.962107241153717, loss=5.214364528656006
I0203 05:13:51.934539 139923868813056 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.0934560298919678, loss=3.1519484519958496
I0203 05:14:38.936283 139923852027648 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.9619108438491821, loss=5.491694450378418
I0203 05:15:25.631571 139923868813056 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.240900993347168, loss=3.390395402908325
I0203 05:16:12.661883 139923852027648 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.9958433508872986, loss=3.4755592346191406
I0203 05:16:37.485359 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:16:48.026227 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:17:23.006169 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:17:24.650876 140085747812160 submission_runner.py:408] Time since start: 40418.35s, 	Step: 78755, 	{'train/accuracy': 0.5059765577316284, 'train/loss': 2.1536612510681152, 'validation/accuracy': 0.46639999747276306, 'validation/loss': 2.365823268890381, 'validation/num_examples': 50000, 'test/accuracy': 0.3612000048160553, 'test/loss': 3.025886297225952, 'test/num_examples': 10000, 'score': 36588.75086402893, 'total_duration': 40418.35488009453, 'accumulated_submission_time': 36588.75086402893, 'accumulated_eval_time': 3821.7439455986023, 'accumulated_logging_time': 3.457021474838257}
I0203 05:17:24.680341 139923868813056 logging_writer.py:48] [78755] accumulated_eval_time=3821.743946, accumulated_logging_time=3.457021, accumulated_submission_time=36588.750864, global_step=78755, preemption_count=0, score=36588.750864, test/accuracy=0.361200, test/loss=3.025886, test/num_examples=10000, total_duration=40418.354880, train/accuracy=0.505977, train/loss=2.153661, validation/accuracy=0.466400, validation/loss=2.365823, validation/num_examples=50000
I0203 05:17:42.728973 139923852027648 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.2012338638305664, loss=3.2886013984680176
I0203 05:18:27.824760 139923868813056 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3306831121444702, loss=3.3848886489868164
I0203 05:19:14.649636 139923852027648 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.1426153182983398, loss=3.65462064743042
I0203 05:20:01.505506 139923868813056 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.1242140531539917, loss=3.3311195373535156
I0203 05:20:48.449975 139923852027648 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.0593005418777466, loss=3.4621479511260986
I0203 05:21:35.511788 139923868813056 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.973521888256073, loss=4.520823955535889
I0203 05:22:22.679399 139923852027648 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.078407883644104, loss=3.571215867996216
I0203 05:23:09.813327 139923868813056 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.1508780717849731, loss=3.9934234619140625
I0203 05:23:56.712250 139923852027648 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.0108871459960938, loss=3.624293327331543
I0203 05:24:25.036999 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:24:35.638104 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:25:04.454802 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:25:06.094066 140085747812160 submission_runner.py:408] Time since start: 40879.80s, 	Step: 79662, 	{'train/accuracy': 0.5085351467132568, 'train/loss': 2.1568405628204346, 'validation/accuracy': 0.4750399887561798, 'validation/loss': 2.333594560623169, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.014451026916504, 'test/num_examples': 10000, 'score': 37009.046035289764, 'total_duration': 40879.798095703125, 'accumulated_submission_time': 37009.046035289764, 'accumulated_eval_time': 3862.8010079860687, 'accumulated_logging_time': 3.495908260345459}
I0203 05:25:06.123040 139923868813056 logging_writer.py:48] [79662] accumulated_eval_time=3862.801008, accumulated_logging_time=3.495908, accumulated_submission_time=37009.046035, global_step=79662, preemption_count=0, score=37009.046035, test/accuracy=0.366400, test/loss=3.014451, test/num_examples=10000, total_duration=40879.798096, train/accuracy=0.508535, train/loss=2.156841, validation/accuracy=0.475040, validation/loss=2.333595, validation/num_examples=50000
I0203 05:25:21.439001 139923852027648 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.0676350593566895, loss=3.2534852027893066
I0203 05:26:06.042880 139923868813056 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.3285303115844727, loss=3.2963855266571045
I0203 05:26:53.042455 139923852027648 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.1684342622756958, loss=3.2524399757385254
I0203 05:27:39.819245 139923868813056 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.9182970523834229, loss=4.373422622680664
I0203 05:28:26.619377 139923852027648 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.275148868560791, loss=3.445629119873047
I0203 05:29:13.478140 139923868813056 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.0088492631912231, loss=5.130305290222168
I0203 05:30:00.237448 139923852027648 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.9992506504058838, loss=3.5073118209838867
I0203 05:30:47.278626 139923868813056 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.11640465259552, loss=3.415131092071533
I0203 05:31:34.108298 139923852027648 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.289548635482788, loss=3.3459041118621826
I0203 05:32:06.474915 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:32:17.146490 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:32:49.089012 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:32:50.728911 140085747812160 submission_runner.py:408] Time since start: 41344.43s, 	Step: 80571, 	{'train/accuracy': 0.49183592200279236, 'train/loss': 2.2679104804992676, 'validation/accuracy': 0.4589399993419647, 'validation/loss': 2.433488130569458, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.056978940963745, 'test/num_examples': 10000, 'score': 37429.33586239815, 'total_duration': 41344.432941913605, 'accumulated_submission_time': 37429.33586239815, 'accumulated_eval_time': 3907.055018186569, 'accumulated_logging_time': 3.535282850265503}
I0203 05:32:50.758312 139923868813056 logging_writer.py:48] [80571] accumulated_eval_time=3907.055018, accumulated_logging_time=3.535283, accumulated_submission_time=37429.335862, global_step=80571, preemption_count=0, score=37429.335862, test/accuracy=0.358100, test/loss=3.056979, test/num_examples=10000, total_duration=41344.432942, train/accuracy=0.491836, train/loss=2.267910, validation/accuracy=0.458940, validation/loss=2.433488, validation/num_examples=50000
I0203 05:33:02.541475 139923852027648 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.241166353225708, loss=3.2228572368621826
I0203 05:33:46.376580 139923868813056 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.0830646753311157, loss=3.398906707763672
I0203 05:34:33.560290 139923852027648 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.242448091506958, loss=3.3874101638793945
I0203 05:35:20.229737 139923868813056 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.9594129323959351, loss=3.1961026191711426
I0203 05:36:07.330648 139923852027648 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.0998321771621704, loss=3.3136065006256104
I0203 05:36:54.336837 139923868813056 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.9263935089111328, loss=3.7970378398895264
I0203 05:37:41.381755 139923852027648 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.2247061729431152, loss=3.2130484580993652
I0203 05:38:28.114650 139923868813056 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.144609808921814, loss=3.255558490753174
I0203 05:39:14.968171 139923852027648 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.1103228330612183, loss=3.296483278274536
I0203 05:39:51.141364 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:40:01.742095 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:40:34.149580 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:40:35.791373 140085747812160 submission_runner.py:408] Time since start: 41809.50s, 	Step: 81479, 	{'train/accuracy': 0.5015624761581421, 'train/loss': 2.221553087234497, 'validation/accuracy': 0.46647998690605164, 'validation/loss': 2.406338691711426, 'validation/num_examples': 50000, 'test/accuracy': 0.3653000295162201, 'test/loss': 3.051608085632324, 'test/num_examples': 10000, 'score': 37849.65655899048, 'total_duration': 41809.49540543556, 'accumulated_submission_time': 37849.65655899048, 'accumulated_eval_time': 3951.7050552368164, 'accumulated_logging_time': 3.575528621673584}
I0203 05:40:35.825412 139923868813056 logging_writer.py:48] [81479] accumulated_eval_time=3951.705055, accumulated_logging_time=3.575529, accumulated_submission_time=37849.656559, global_step=81479, preemption_count=0, score=37849.656559, test/accuracy=0.365300, test/loss=3.051608, test/num_examples=10000, total_duration=41809.495405, train/accuracy=0.501562, train/loss=2.221553, validation/accuracy=0.466480, validation/loss=2.406339, validation/num_examples=50000
I0203 05:40:44.459194 139923852027648 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.1034305095672607, loss=3.233484983444214
I0203 05:41:28.092400 139923868813056 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.1336396932601929, loss=3.5981569290161133
I0203 05:42:14.724793 139923852027648 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.924078106880188, loss=5.599006652832031
I0203 05:43:01.736344 139923868813056 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.1219191551208496, loss=3.139806032180786
I0203 05:43:48.644722 139923852027648 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.2310997247695923, loss=3.3580803871154785
I0203 05:44:35.812230 139923868813056 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.056631326675415, loss=3.9775686264038086
I0203 05:45:22.690454 139923852027648 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.2740092277526855, loss=3.2753448486328125
I0203 05:46:09.720437 139923868813056 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.8774272203445435, loss=4.753035068511963
I0203 05:46:56.473823 139923852027648 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.029167652130127, loss=3.1686038970947266
I0203 05:47:36.109310 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:47:46.916057 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:48:14.016494 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:48:15.656154 140085747812160 submission_runner.py:408] Time since start: 42269.36s, 	Step: 82386, 	{'train/accuracy': 0.5391015410423279, 'train/loss': 2.0318500995635986, 'validation/accuracy': 0.4758799970149994, 'validation/loss': 2.33320951461792, 'validation/num_examples': 50000, 'test/accuracy': 0.37640002369880676, 'test/loss': 2.9651641845703125, 'test/num_examples': 10000, 'score': 38269.87842488289, 'total_duration': 42269.360181331635, 'accumulated_submission_time': 38269.87842488289, 'accumulated_eval_time': 3991.2518994808197, 'accumulated_logging_time': 3.620238780975342}
I0203 05:48:15.687833 139923868813056 logging_writer.py:48] [82386] accumulated_eval_time=3991.251899, accumulated_logging_time=3.620239, accumulated_submission_time=38269.878425, global_step=82386, preemption_count=0, score=38269.878425, test/accuracy=0.376400, test/loss=2.965164, test/num_examples=10000, total_duration=42269.360181, train/accuracy=0.539102, train/loss=2.031850, validation/accuracy=0.475880, validation/loss=2.333210, validation/num_examples=50000
I0203 05:48:21.579972 139923852027648 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.0685293674468994, loss=3.2342848777770996
I0203 05:49:04.402573 139923868813056 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.9214801788330078, loss=5.615224361419678
I0203 05:49:51.327451 139923852027648 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.1335246562957764, loss=3.2981367111206055
I0203 05:50:38.383130 139923868813056 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.245016098022461, loss=3.265421152114868
I0203 05:51:25.223516 139923852027648 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.9809167981147766, loss=3.1089396476745605
I0203 05:52:12.281634 139923868813056 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.9435194134712219, loss=4.1334004402160645
I0203 05:52:59.206742 139923852027648 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.5431967973709106, loss=3.3429064750671387
I0203 05:53:46.356711 139923868813056 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.0191974639892578, loss=5.186293125152588
I0203 05:54:33.430349 139923852027648 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.9098010063171387, loss=4.738862991333008
I0203 05:55:15.867900 140085747812160 spec.py:321] Evaluating on the training split.
I0203 05:55:26.420828 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 05:55:58.146673 140085747812160 spec.py:349] Evaluating on the test split.
I0203 05:55:59.789656 140085747812160 submission_runner.py:408] Time since start: 42733.49s, 	Step: 83292, 	{'train/accuracy': 0.509472668170929, 'train/loss': 2.1737000942230225, 'validation/accuracy': 0.47259998321533203, 'validation/loss': 2.343513250350952, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 2.987058639526367, 'test/num_examples': 10000, 'score': 38689.997086048126, 'total_duration': 42733.49368548393, 'accumulated_submission_time': 38689.997086048126, 'accumulated_eval_time': 4035.173714160919, 'accumulated_logging_time': 3.6620676517486572}
I0203 05:55:59.822341 139923868813056 logging_writer.py:48] [83292] accumulated_eval_time=4035.173714, accumulated_logging_time=3.662068, accumulated_submission_time=38689.997086, global_step=83292, preemption_count=0, score=38689.997086, test/accuracy=0.370600, test/loss=2.987059, test/num_examples=10000, total_duration=42733.493685, train/accuracy=0.509473, train/loss=2.173700, validation/accuracy=0.472600, validation/loss=2.343513, validation/num_examples=50000
I0203 05:56:03.398232 139923852027648 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.0394247770309448, loss=5.1030964851379395
I0203 05:56:46.035717 139923868813056 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.0532280206680298, loss=3.609266996383667
I0203 05:57:32.986079 139923852027648 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.1060014963150024, loss=4.1101155281066895
I0203 05:58:20.131469 139923868813056 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.8889949321746826, loss=4.894029140472412
I0203 05:59:07.257375 139923852027648 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.1406587362289429, loss=3.2299838066101074
I0203 05:59:54.157116 139923868813056 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.224426031112671, loss=3.1541264057159424
I0203 06:00:40.974203 139923852027648 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.8342769145965576, loss=4.698348522186279
I0203 06:01:27.742711 139923868813056 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.1041843891143799, loss=3.1365044116973877
I0203 06:02:14.513089 139923852027648 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.053193211555481, loss=3.7555768489837646
I0203 06:03:00.000762 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:03:10.829567 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:03:43.968979 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:03:45.612495 140085747812160 submission_runner.py:408] Time since start: 43199.32s, 	Step: 84198, 	{'train/accuracy': 0.5048437118530273, 'train/loss': 2.2214019298553467, 'validation/accuracy': 0.46671998500823975, 'validation/loss': 2.4125983715057373, 'validation/num_examples': 50000, 'test/accuracy': 0.3580000102519989, 'test/loss': 3.0713653564453125, 'test/num_examples': 10000, 'score': 39110.11424565315, 'total_duration': 43199.31651544571, 'accumulated_submission_time': 39110.11424565315, 'accumulated_eval_time': 4080.7854421138763, 'accumulated_logging_time': 3.705613374710083}
I0203 06:03:45.642888 139923868813056 logging_writer.py:48] [84198] accumulated_eval_time=4080.785442, accumulated_logging_time=3.705613, accumulated_submission_time=39110.114246, global_step=84198, preemption_count=0, score=39110.114246, test/accuracy=0.358000, test/loss=3.071365, test/num_examples=10000, total_duration=43199.316515, train/accuracy=0.504844, train/loss=2.221402, validation/accuracy=0.466720, validation/loss=2.412598, validation/num_examples=50000
I0203 06:03:46.823711 139923852027648 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.0149174928665161, loss=3.9252736568450928
I0203 06:04:28.854830 139923868813056 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.059467077255249, loss=3.2194926738739014
I0203 06:05:15.757936 139923852027648 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.0935248136520386, loss=3.1799776554107666
I0203 06:06:02.814511 139923868813056 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.0186841487884521, loss=3.5648622512817383
I0203 06:06:49.698308 139923852027648 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.9632734656333923, loss=4.951681137084961
I0203 06:07:36.352964 139923868813056 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.0477352142333984, loss=4.781257152557373
I0203 06:08:23.138285 139923852027648 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.312542200088501, loss=3.4371249675750732
I0203 06:09:10.055306 139923868813056 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.2117401361465454, loss=3.240452289581299
I0203 06:09:57.118873 139923852027648 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.9785526990890503, loss=4.7450079917907715
I0203 06:10:44.115803 139923868813056 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.8891278505325317, loss=4.15056848526001
I0203 06:10:45.658868 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:10:57.738125 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:11:23.428300 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:11:25.078006 140085747812160 submission_runner.py:408] Time since start: 43658.78s, 	Step: 85105, 	{'train/accuracy': 0.5434765219688416, 'train/loss': 1.9751157760620117, 'validation/accuracy': 0.48027998208999634, 'validation/loss': 2.3029189109802246, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.939318895339966, 'test/num_examples': 10000, 'score': 39530.067018032074, 'total_duration': 43658.78203487396, 'accumulated_submission_time': 39530.067018032074, 'accumulated_eval_time': 4120.204581737518, 'accumulated_logging_time': 3.74727201461792}
I0203 06:11:25.107784 139923852027648 logging_writer.py:48] [85105] accumulated_eval_time=4120.204582, accumulated_logging_time=3.747272, accumulated_submission_time=39530.067018, global_step=85105, preemption_count=0, score=39530.067018, test/accuracy=0.382700, test/loss=2.939319, test/num_examples=10000, total_duration=43658.782035, train/accuracy=0.543477, train/loss=1.975116, validation/accuracy=0.480280, validation/loss=2.302919, validation/num_examples=50000
I0203 06:12:04.963622 139923868813056 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.9744012951850891, loss=5.598935604095459
I0203 06:12:51.844926 139923852027648 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.9675718545913696, loss=3.556401252746582
I0203 06:13:38.931063 139923868813056 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.1859440803527832, loss=3.278109073638916
I0203 06:14:25.836331 139923852027648 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.1997240781784058, loss=3.505122661590576
I0203 06:15:12.887468 139923868813056 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.989898145198822, loss=3.301422119140625
I0203 06:15:59.685353 139923852027648 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.014386773109436, loss=3.374340534210205
I0203 06:16:46.512199 139923868813056 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.9874710440635681, loss=3.811735153198242
I0203 06:17:33.498680 139923852027648 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.0644750595092773, loss=3.397393226623535
I0203 06:18:20.469395 139923868813056 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.8150460124015808, loss=5.187838554382324
I0203 06:18:25.381995 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:18:35.942198 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:19:07.501903 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:19:09.141272 140085747812160 submission_runner.py:408] Time since start: 44122.85s, 	Step: 86012, 	{'train/accuracy': 0.5088672041893005, 'train/loss': 2.1853909492492676, 'validation/accuracy': 0.4745599925518036, 'validation/loss': 2.349569082260132, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.0009925365448, 'test/num_examples': 10000, 'score': 39950.27555394173, 'total_duration': 44122.84530091286, 'accumulated_submission_time': 39950.27555394173, 'accumulated_eval_time': 4163.963869810104, 'accumulated_logging_time': 3.7868733406066895}
I0203 06:19:09.172008 139923852027648 logging_writer.py:48] [86012] accumulated_eval_time=4163.963870, accumulated_logging_time=3.786873, accumulated_submission_time=39950.275554, global_step=86012, preemption_count=0, score=39950.275554, test/accuracy=0.366400, test/loss=3.000993, test/num_examples=10000, total_duration=44122.845301, train/accuracy=0.508867, train/loss=2.185391, validation/accuracy=0.474560, validation/loss=2.349569, validation/num_examples=50000
I0203 06:19:45.896319 139923868813056 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.1325973272323608, loss=3.2347893714904785
I0203 06:20:32.671928 139923852027648 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.0868234634399414, loss=3.7138166427612305
I0203 06:21:19.676096 139923868813056 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.2239353656768799, loss=3.3342881202697754
I0203 06:22:06.612390 139923852027648 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.9933539032936096, loss=4.059820652008057
I0203 06:22:53.745641 139923868813056 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.0207277536392212, loss=3.751593828201294
I0203 06:23:40.759453 139923852027648 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.202001929283142, loss=3.2733874320983887
I0203 06:24:27.765342 139923868813056 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.9319084882736206, loss=4.110006332397461
I0203 06:25:14.702138 139923852027648 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.1062958240509033, loss=3.3065500259399414
I0203 06:26:01.587504 139923868813056 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.0530146360397339, loss=5.719543933868408
I0203 06:26:09.278188 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:26:20.003899 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:26:53.202640 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:26:54.840179 140085747812160 submission_runner.py:408] Time since start: 44588.54s, 	Step: 86918, 	{'train/accuracy': 0.5166406035423279, 'train/loss': 2.1085727214813232, 'validation/accuracy': 0.4803199768066406, 'validation/loss': 2.2921464443206787, 'validation/num_examples': 50000, 'test/accuracy': 0.37160003185272217, 'test/loss': 2.941880702972412, 'test/num_examples': 10000, 'score': 40370.320439338684, 'total_duration': 44588.54420852661, 'accumulated_submission_time': 40370.320439338684, 'accumulated_eval_time': 4209.52586555481, 'accumulated_logging_time': 3.8280630111694336}
I0203 06:26:54.872447 139923852027648 logging_writer.py:48] [86918] accumulated_eval_time=4209.525866, accumulated_logging_time=3.828063, accumulated_submission_time=40370.320439, global_step=86918, preemption_count=0, score=40370.320439, test/accuracy=0.371600, test/loss=2.941881, test/num_examples=10000, total_duration=44588.544209, train/accuracy=0.516641, train/loss=2.108573, validation/accuracy=0.480320, validation/loss=2.292146, validation/num_examples=50000
I0203 06:27:28.832257 139923868813056 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.120164155960083, loss=3.1130480766296387
I0203 06:28:15.478679 139923852027648 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.2139240503311157, loss=5.675073146820068
I0203 06:29:02.616739 139923868813056 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.2190567255020142, loss=3.4149117469787598
I0203 06:29:49.636396 139923852027648 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.9638692140579224, loss=5.240410804748535
I0203 06:30:36.949903 139923868813056 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.2616322040557861, loss=3.2205939292907715
I0203 06:31:24.016075 139923852027648 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.1035642623901367, loss=3.667541265487671
I0203 06:32:11.220093 139923868813056 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.150329828262329, loss=3.617776393890381
I0203 06:32:58.305480 139923852027648 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.2249752283096313, loss=3.091928482055664
I0203 06:33:45.454998 139923868813056 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.073123574256897, loss=3.1956429481506348
I0203 06:33:55.024961 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:34:05.665636 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:34:38.574091 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:34:40.217454 140085747812160 submission_runner.py:408] Time since start: 45053.92s, 	Step: 87822, 	{'train/accuracy': 0.5345312356948853, 'train/loss': 2.0101184844970703, 'validation/accuracy': 0.48437997698783875, 'validation/loss': 2.270280122756958, 'validation/num_examples': 50000, 'test/accuracy': 0.3785000145435333, 'test/loss': 2.930760622024536, 'test/num_examples': 10000, 'score': 40790.413435697556, 'total_duration': 45053.92148447037, 'accumulated_submission_time': 40790.413435697556, 'accumulated_eval_time': 4254.7183582782745, 'accumulated_logging_time': 3.8700315952301025}
I0203 06:34:40.247843 139923852027648 logging_writer.py:48] [87822] accumulated_eval_time=4254.718358, accumulated_logging_time=3.870032, accumulated_submission_time=40790.413436, global_step=87822, preemption_count=0, score=40790.413436, test/accuracy=0.378500, test/loss=2.930761, test/num_examples=10000, total_duration=45053.921484, train/accuracy=0.534531, train/loss=2.010118, validation/accuracy=0.484380, validation/loss=2.270280, validation/num_examples=50000
I0203 06:35:12.315301 139923868813056 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.044723629951477, loss=3.0731868743896484
I0203 06:35:58.967320 139923852027648 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3024358749389648, loss=3.201390027999878
I0203 06:36:46.030926 139923868813056 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.194691777229309, loss=4.169560432434082
I0203 06:37:33.098739 139923852027648 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.1199086904525757, loss=3.689528703689575
I0203 06:38:19.948518 139923868813056 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.964202344417572, loss=3.681210517883301
I0203 06:39:06.883371 139923852027648 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.9275376796722412, loss=4.297127723693848
I0203 06:39:53.892725 139923868813056 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.1419533491134644, loss=3.0894460678100586
I0203 06:40:41.159379 139923852027648 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.244223952293396, loss=3.1949989795684814
I0203 06:41:28.135287 139923868813056 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.029847502708435, loss=4.964164733886719
I0203 06:41:40.509545 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:41:51.055554 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:42:24.818437 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:42:26.461909 140085747812160 submission_runner.py:408] Time since start: 45520.17s, 	Step: 88728, 	{'train/accuracy': 0.5253124833106995, 'train/loss': 2.1078312397003174, 'validation/accuracy': 0.4916599988937378, 'validation/loss': 2.2708756923675537, 'validation/num_examples': 50000, 'test/accuracy': 0.38440001010894775, 'test/loss': 2.910942554473877, 'test/num_examples': 10000, 'score': 41210.61529803276, 'total_duration': 45520.16591215134, 'accumulated_submission_time': 41210.61529803276, 'accumulated_eval_time': 4300.670683383942, 'accumulated_logging_time': 3.910280704498291}
I0203 06:42:26.496204 139923852027648 logging_writer.py:48] [88728] accumulated_eval_time=4300.670683, accumulated_logging_time=3.910281, accumulated_submission_time=41210.615298, global_step=88728, preemption_count=0, score=41210.615298, test/accuracy=0.384400, test/loss=2.910943, test/num_examples=10000, total_duration=45520.165912, train/accuracy=0.525312, train/loss=2.107831, validation/accuracy=0.491660, validation/loss=2.270876, validation/num_examples=50000
I0203 06:42:55.694208 139923868813056 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.2629977464675903, loss=3.127171754837036
I0203 06:43:42.472436 139923852027648 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.1172454357147217, loss=3.1811916828155518
I0203 06:44:29.358567 139923868813056 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.9762281179428101, loss=3.9231746196746826
I0203 06:45:16.331308 139923852027648 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.1646453142166138, loss=3.977945327758789
I0203 06:46:03.268360 139923868813056 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.055681586265564, loss=3.1341071128845215
I0203 06:46:49.963262 139923852027648 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.0593876838684082, loss=3.344438314437866
I0203 06:47:37.113620 139923868813056 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.8581181168556213, loss=4.462020397186279
I0203 06:48:24.072143 139923852027648 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.375481128692627, loss=3.1792778968811035
I0203 06:49:11.146072 139923868813056 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.0281144380569458, loss=5.645810604095459
I0203 06:49:26.837176 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:49:37.342466 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:50:08.251654 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:50:09.895836 140085747812160 submission_runner.py:408] Time since start: 45983.60s, 	Step: 89635, 	{'train/accuracy': 0.5248632431030273, 'train/loss': 2.0753753185272217, 'validation/accuracy': 0.48985999822616577, 'validation/loss': 2.2578797340393066, 'validation/num_examples': 50000, 'test/accuracy': 0.38280001282691956, 'test/loss': 2.9128763675689697, 'test/num_examples': 10000, 'score': 41630.89320707321, 'total_duration': 45983.5998442173, 'accumulated_submission_time': 41630.89320707321, 'accumulated_eval_time': 4343.729335069656, 'accumulated_logging_time': 3.955927610397339}
I0203 06:50:09.927628 139923852027648 logging_writer.py:48] [89635] accumulated_eval_time=4343.729335, accumulated_logging_time=3.955928, accumulated_submission_time=41630.893207, global_step=89635, preemption_count=0, score=41630.893207, test/accuracy=0.382800, test/loss=2.912876, test/num_examples=10000, total_duration=45983.599844, train/accuracy=0.524863, train/loss=2.075375, validation/accuracy=0.489860, validation/loss=2.257880, validation/num_examples=50000
I0203 06:50:35.931397 139923868813056 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.099372386932373, loss=4.058164596557617
I0203 06:51:22.876249 139923852027648 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.1688826084136963, loss=3.4880318641662598
I0203 06:52:10.008220 139923868813056 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.3002198934555054, loss=3.099686861038208
I0203 06:52:57.230353 139923852027648 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.1999245882034302, loss=3.2146389484405518
I0203 06:53:44.551335 139923868813056 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.02567458152771, loss=3.494255542755127
I0203 06:54:31.450554 139923852027648 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.0578651428222656, loss=3.4991893768310547
I0203 06:55:18.495720 139923868813056 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.100412368774414, loss=3.1931896209716797
I0203 06:56:05.717541 139923852027648 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.986549973487854, loss=4.220606803894043
I0203 06:56:52.796552 139923868813056 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.2998311519622803, loss=3.228264570236206
I0203 06:57:10.065899 140085747812160 spec.py:321] Evaluating on the training split.
I0203 06:57:20.762521 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 06:57:53.175271 140085747812160 spec.py:349] Evaluating on the test split.
I0203 06:57:54.810638 140085747812160 submission_runner.py:408] Time since start: 46448.51s, 	Step: 90538, 	{'train/accuracy': 0.5314648151397705, 'train/loss': 2.07725191116333, 'validation/accuracy': 0.48589998483657837, 'validation/loss': 2.3086907863616943, 'validation/num_examples': 50000, 'test/accuracy': 0.37540000677108765, 'test/loss': 2.9723060131073, 'test/num_examples': 10000, 'score': 42050.968324422836, 'total_duration': 46448.51464676857, 'accumulated_submission_time': 42050.968324422836, 'accumulated_eval_time': 4388.4740562438965, 'accumulated_logging_time': 4.000329256057739}
I0203 06:57:54.843800 139923852027648 logging_writer.py:48] [90538] accumulated_eval_time=4388.474056, accumulated_logging_time=4.000329, accumulated_submission_time=42050.968324, global_step=90538, preemption_count=0, score=42050.968324, test/accuracy=0.375400, test/loss=2.972306, test/num_examples=10000, total_duration=46448.514647, train/accuracy=0.531465, train/loss=2.077252, validation/accuracy=0.485900, validation/loss=2.308691, validation/num_examples=50000
I0203 06:58:19.552033 139923868813056 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.1406159400939941, loss=4.57158088684082
I0203 06:59:06.288376 139923852027648 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.9334535002708435, loss=4.7481303215026855
I0203 06:59:53.534438 139923868813056 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.1422163248062134, loss=3.02085542678833
I0203 07:00:40.526194 139923852027648 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.0727404356002808, loss=3.1012167930603027
I0203 07:01:27.542265 139923868813056 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.1657785177230835, loss=4.045657157897949
I0203 07:02:14.844754 139923852027648 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.8988543152809143, loss=4.210963249206543
I0203 07:03:01.919946 139923868813056 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.0470194816589355, loss=4.998910903930664
I0203 07:03:49.073803 139923852027648 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.1809040307998657, loss=3.3330559730529785
I0203 07:04:36.129521 139923868813056 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.8953604698181152, loss=5.1511993408203125
I0203 07:04:55.143129 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:05:06.194244 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:05:38.703271 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:05:40.337968 140085747812160 submission_runner.py:408] Time since start: 46914.04s, 	Step: 91442, 	{'train/accuracy': 0.5350781083106995, 'train/loss': 2.0236992835998535, 'validation/accuracy': 0.5030400156974792, 'validation/loss': 2.1932621002197266, 'validation/num_examples': 50000, 'test/accuracy': 0.394400030374527, 'test/loss': 2.8569884300231934, 'test/num_examples': 10000, 'score': 42471.20595598221, 'total_duration': 46914.04198694229, 'accumulated_submission_time': 42471.20595598221, 'accumulated_eval_time': 4433.668882369995, 'accumulated_logging_time': 4.045256614685059}
I0203 07:05:40.370360 139923852027648 logging_writer.py:48] [91442] accumulated_eval_time=4433.668882, accumulated_logging_time=4.045257, accumulated_submission_time=42471.205956, global_step=91442, preemption_count=0, score=42471.205956, test/accuracy=0.394400, test/loss=2.856988, test/num_examples=10000, total_duration=46914.041987, train/accuracy=0.535078, train/loss=2.023699, validation/accuracy=0.503040, validation/loss=2.193262, validation/num_examples=50000
I0203 07:06:03.522854 139923868813056 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.8465010523796082, loss=4.866838455200195
I0203 07:06:49.886263 139923852027648 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.1137950420379639, loss=3.2636702060699463
I0203 07:07:36.683412 139923868813056 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.1952825784683228, loss=3.184683084487915
I0203 07:08:23.589500 139923852027648 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.1718634366989136, loss=3.0205960273742676
I0203 07:09:10.377680 139923868813056 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.9537169337272644, loss=4.568017959594727
I0203 07:09:57.342390 139923852027648 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.2783632278442383, loss=3.1169185638427734
I0203 07:10:44.083939 139923868813056 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.058906078338623, loss=3.3838067054748535
I0203 07:11:31.011402 139923852027648 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.9743975400924683, loss=5.393852233886719
I0203 07:12:17.987395 139923868813056 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.2102948427200317, loss=3.0721640586853027
I0203 07:12:40.383787 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:12:51.040376 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:13:20.250551 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:13:21.894054 140085747812160 submission_runner.py:408] Time since start: 47375.60s, 	Step: 92349, 	{'train/accuracy': 0.5308398604393005, 'train/loss': 2.097571849822998, 'validation/accuracy': 0.4921799898147583, 'validation/loss': 2.277677059173584, 'validation/num_examples': 50000, 'test/accuracy': 0.3882000148296356, 'test/loss': 2.938272476196289, 'test/num_examples': 10000, 'score': 42891.15717124939, 'total_duration': 47375.598071336746, 'accumulated_submission_time': 42891.15717124939, 'accumulated_eval_time': 4475.179137229919, 'accumulated_logging_time': 4.08874249458313}
I0203 07:13:21.926685 139923852027648 logging_writer.py:48] [92349] accumulated_eval_time=4475.179137, accumulated_logging_time=4.088742, accumulated_submission_time=42891.157171, global_step=92349, preemption_count=0, score=42891.157171, test/accuracy=0.388200, test/loss=2.938272, test/num_examples=10000, total_duration=47375.598071, train/accuracy=0.530840, train/loss=2.097572, validation/accuracy=0.492180, validation/loss=2.277677, validation/num_examples=50000
I0203 07:13:42.341533 139923868813056 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.077194333076477, loss=3.225581407546997
I0203 07:14:27.911457 139923852027648 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.1618945598602295, loss=2.9818923473358154
I0203 07:15:15.108187 139923868813056 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.0741419792175293, loss=3.012845516204834
I0203 07:16:02.257723 139923852027648 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.9723449945449829, loss=5.313183784484863
I0203 07:16:49.186558 139923868813056 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.0412992238998413, loss=3.4609899520874023
I0203 07:17:35.982181 139923852027648 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.0701903104782104, loss=3.0997519493103027
I0203 07:18:23.309025 139923868813056 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.3549282550811768, loss=5.676144123077393
I0203 07:19:10.332389 139923852027648 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.2998709678649902, loss=3.1594901084899902
I0203 07:19:57.512201 139923868813056 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.3281844854354858, loss=3.4176621437072754
I0203 07:20:22.125808 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:20:32.685656 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:21:03.879911 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:21:05.518412 140085747812160 submission_runner.py:408] Time since start: 47839.22s, 	Step: 93254, 	{'train/accuracy': 0.5414257645606995, 'train/loss': 2.011777639389038, 'validation/accuracy': 0.4967799782752991, 'validation/loss': 2.2263717651367188, 'validation/num_examples': 50000, 'test/accuracy': 0.3873000144958496, 'test/loss': 2.8787736892700195, 'test/num_examples': 10000, 'score': 43311.2950565815, 'total_duration': 47839.22241187096, 'accumulated_submission_time': 43311.2950565815, 'accumulated_eval_time': 4518.571710586548, 'accumulated_logging_time': 4.132167816162109}
I0203 07:21:05.549696 139923852027648 logging_writer.py:48] [93254] accumulated_eval_time=4518.571711, accumulated_logging_time=4.132168, accumulated_submission_time=43311.295057, global_step=93254, preemption_count=0, score=43311.295057, test/accuracy=0.387300, test/loss=2.878774, test/num_examples=10000, total_duration=47839.222412, train/accuracy=0.541426, train/loss=2.011778, validation/accuracy=0.496780, validation/loss=2.226372, validation/num_examples=50000
I0203 07:21:23.996357 139923868813056 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.1129006147384644, loss=5.0543293952941895
I0203 07:22:09.161211 139923852027648 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.329411506652832, loss=3.0332226753234863
I0203 07:22:56.612374 139923868813056 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.0956717729568481, loss=3.1250507831573486
I0203 07:23:43.867472 139923852027648 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.9969008564949036, loss=3.8468992710113525
I0203 07:24:31.225787 139923868813056 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.0728034973144531, loss=3.0817513465881348
I0203 07:25:18.113389 139923852027648 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.9622945785522461, loss=5.114046573638916
I0203 07:26:05.061219 139923868813056 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.9632841944694519, loss=4.286900520324707
I0203 07:26:51.925306 139923852027648 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.1981151103973389, loss=2.9752771854400635
I0203 07:27:39.055263 139923868813056 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.152140736579895, loss=3.113719940185547
I0203 07:28:05.637378 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:28:16.110356 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:28:49.499874 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:28:51.149836 140085747812160 submission_runner.py:408] Time since start: 48304.85s, 	Step: 94158, 	{'train/accuracy': 0.5350976586341858, 'train/loss': 2.0232887268066406, 'validation/accuracy': 0.5008000135421753, 'validation/loss': 2.1791908740997314, 'validation/num_examples': 50000, 'test/accuracy': 0.395300030708313, 'test/loss': 2.828667640686035, 'test/num_examples': 10000, 'score': 43731.321390390396, 'total_duration': 48304.853842020035, 'accumulated_submission_time': 43731.321390390396, 'accumulated_eval_time': 4564.084159851074, 'accumulated_logging_time': 4.174811124801636}
I0203 07:28:51.187475 139923852027648 logging_writer.py:48] [94158] accumulated_eval_time=4564.084160, accumulated_logging_time=4.174811, accumulated_submission_time=43731.321390, global_step=94158, preemption_count=0, score=43731.321390, test/accuracy=0.395300, test/loss=2.828668, test/num_examples=10000, total_duration=48304.853842, train/accuracy=0.535098, train/loss=2.023289, validation/accuracy=0.500800, validation/loss=2.179191, validation/num_examples=50000
I0203 07:29:08.067559 139923868813056 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.1451414823532104, loss=3.1121559143066406
I0203 07:29:52.805556 139923852027648 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.1836788654327393, loss=3.1939198970794678
I0203 07:30:39.995084 139923868813056 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.0491000413894653, loss=3.6333084106445312
I0203 07:31:26.952801 139923852027648 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.1786768436431885, loss=3.109804153442383
I0203 07:32:13.993806 139923868813056 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.3148226737976074, loss=3.042424201965332
I0203 07:33:00.820532 139923852027648 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.8905209302902222, loss=5.391406536102295
I0203 07:33:47.651816 139923868813056 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.1466535329818726, loss=3.1215717792510986
I0203 07:34:34.557131 139923852027648 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.0577075481414795, loss=3.079169750213623
I0203 07:35:21.410502 139923868813056 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.122552752494812, loss=3.2147634029388428
I0203 07:35:51.211958 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:36:01.548517 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:36:35.021538 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:36:36.661284 140085747812160 submission_runner.py:408] Time since start: 48770.37s, 	Step: 95065, 	{'train/accuracy': 0.5455663800239563, 'train/loss': 1.9697304964065552, 'validation/accuracy': 0.5080199837684631, 'validation/loss': 2.1573288440704346, 'validation/num_examples': 50000, 'test/accuracy': 0.4004000127315521, 'test/loss': 2.8348753452301025, 'test/num_examples': 10000, 'score': 44151.28519535065, 'total_duration': 48770.36529827118, 'accumulated_submission_time': 44151.28519535065, 'accumulated_eval_time': 4609.533478021622, 'accumulated_logging_time': 4.222652435302734}
I0203 07:36:36.692575 139923852027648 logging_writer.py:48] [95065] accumulated_eval_time=4609.533478, accumulated_logging_time=4.222652, accumulated_submission_time=44151.285195, global_step=95065, preemption_count=0, score=44151.285195, test/accuracy=0.400400, test/loss=2.834875, test/num_examples=10000, total_duration=48770.365298, train/accuracy=0.545566, train/loss=1.969730, validation/accuracy=0.508020, validation/loss=2.157329, validation/num_examples=50000
I0203 07:36:51.053437 139923868813056 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.301293134689331, loss=2.978713035583496
I0203 07:37:35.156656 139923852027648 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.1767034530639648, loss=3.196317434310913
I0203 07:38:22.064349 139923868813056 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.8561568260192871, loss=5.519673824310303
I0203 07:39:09.087675 139923852027648 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.8673763275146484, loss=4.807101726531982
I0203 07:39:55.853553 139923868813056 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.0208964347839355, loss=3.9275689125061035
I0203 07:40:42.991577 139923852027648 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.107832670211792, loss=2.943779468536377
I0203 07:41:29.817451 139923868813056 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.1801447868347168, loss=2.9732518196105957
I0203 07:42:16.888556 139923852027648 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.1833724975585938, loss=3.3278610706329346
I0203 07:43:03.863835 139923868813056 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.9443326592445374, loss=5.406753063201904
I0203 07:43:36.894557 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:43:47.560405 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:44:15.994135 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:44:17.632096 140085747812160 submission_runner.py:408] Time since start: 49231.34s, 	Step: 95972, 	{'train/accuracy': 0.5479297041893005, 'train/loss': 1.936432957649231, 'validation/accuracy': 0.5101799964904785, 'validation/loss': 2.144627571105957, 'validation/num_examples': 50000, 'test/accuracy': 0.39100003242492676, 'test/loss': 2.8307206630706787, 'test/num_examples': 10000, 'score': 44571.42405152321, 'total_duration': 49231.33609867096, 'accumulated_submission_time': 44571.42405152321, 'accumulated_eval_time': 4650.271003246307, 'accumulated_logging_time': 4.266034364700317}
I0203 07:44:17.665174 139923852027648 logging_writer.py:48] [95972] accumulated_eval_time=4650.271003, accumulated_logging_time=4.266034, accumulated_submission_time=44571.424052, global_step=95972, preemption_count=0, score=44571.424052, test/accuracy=0.391000, test/loss=2.830721, test/num_examples=10000, total_duration=49231.336099, train/accuracy=0.547930, train/loss=1.936433, validation/accuracy=0.510180, validation/loss=2.144628, validation/num_examples=50000
I0203 07:44:29.459680 139923868813056 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.8414430618286133, loss=4.817403316497803
I0203 07:45:13.678445 139923852027648 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.1582822799682617, loss=2.9949963092803955
I0203 07:46:00.711117 139923868813056 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.0801575183868408, loss=3.361809015274048
I0203 07:46:48.080294 139923852027648 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.2995593547821045, loss=3.2280995845794678
I0203 07:47:35.301447 139923868813056 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.8806531429290771, loss=5.360872268676758
I0203 07:48:22.634432 139923852027648 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.2224427461624146, loss=2.9544572830200195
I0203 07:49:09.832016 139923868813056 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.105749249458313, loss=4.582583427429199
I0203 07:49:56.913093 139923852027648 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.357689619064331, loss=3.106027364730835
I0203 07:50:44.045981 139923868813056 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.9357442855834961, loss=4.358255386352539
I0203 07:51:17.776130 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:51:28.534732 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:51:57.622370 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:51:59.264679 140085747812160 submission_runner.py:408] Time since start: 49692.97s, 	Step: 96873, 	{'train/accuracy': 0.5483007431030273, 'train/loss': 1.9771265983581543, 'validation/accuracy': 0.5109999775886536, 'validation/loss': 2.1710920333862305, 'validation/num_examples': 50000, 'test/accuracy': 0.4003000259399414, 'test/loss': 2.800229549407959, 'test/num_examples': 10000, 'score': 44991.071533203125, 'total_duration': 49692.96869182587, 'accumulated_submission_time': 44991.071533203125, 'accumulated_eval_time': 4691.759567499161, 'accumulated_logging_time': 4.712217807769775}
I0203 07:51:59.298590 139923852027648 logging_writer.py:48] [96873] accumulated_eval_time=4691.759567, accumulated_logging_time=4.712218, accumulated_submission_time=44991.071533, global_step=96873, preemption_count=0, score=44991.071533, test/accuracy=0.400300, test/loss=2.800230, test/num_examples=10000, total_duration=49692.968692, train/accuracy=0.548301, train/loss=1.977127, validation/accuracy=0.511000, validation/loss=2.171092, validation/num_examples=50000
I0203 07:52:10.300012 139923868813056 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.2942646741867065, loss=3.100959539413452
I0203 07:52:54.485007 139923852027648 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.1241917610168457, loss=3.1366469860076904
I0203 07:53:41.884728 139923868813056 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.0170202255249023, loss=4.03768253326416
I0203 07:54:28.958796 139923852027648 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.9770100712776184, loss=4.566437244415283
I0203 07:55:15.938595 139923868813056 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.3683360815048218, loss=2.9894819259643555
I0203 07:56:02.880644 139923852027648 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.2530462741851807, loss=2.9837286472320557
I0203 07:56:49.896674 139923868813056 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.360353708267212, loss=3.095231533050537
I0203 07:57:36.988553 139923852027648 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.2985397577285767, loss=3.101299524307251
I0203 07:58:23.698118 139923868813056 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.303165316581726, loss=3.0512959957122803
I0203 07:58:59.680091 140085747812160 spec.py:321] Evaluating on the training split.
I0203 07:59:10.410467 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 07:59:44.373042 140085747812160 spec.py:349] Evaluating on the test split.
I0203 07:59:46.016785 140085747812160 submission_runner.py:408] Time since start: 50159.72s, 	Step: 97778, 	{'train/accuracy': 0.550976574420929, 'train/loss': 1.9703459739685059, 'validation/accuracy': 0.5161399841308594, 'validation/loss': 2.1413543224334717, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.8006556034088135, 'test/num_examples': 10000, 'score': 45411.392758369446, 'total_duration': 50159.720798015594, 'accumulated_submission_time': 45411.392758369446, 'accumulated_eval_time': 4738.096249103546, 'accumulated_logging_time': 4.756515741348267}
I0203 07:59:46.051003 139923852027648 logging_writer.py:48] [97778] accumulated_eval_time=4738.096249, accumulated_logging_time=4.756516, accumulated_submission_time=45411.392758, global_step=97778, preemption_count=0, score=45411.392758, test/accuracy=0.410400, test/loss=2.800656, test/num_examples=10000, total_duration=50159.720798, train/accuracy=0.550977, train/loss=1.970346, validation/accuracy=0.516140, validation/loss=2.141354, validation/num_examples=50000
I0203 07:59:55.072821 139923868813056 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.2793488502502441, loss=3.105772018432617
I0203 08:00:38.530954 139923852027648 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.3331918716430664, loss=3.0353102684020996
I0203 08:01:25.404541 139923868813056 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.1807513236999512, loss=2.8897974491119385
I0203 08:02:12.230096 139923852027648 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.2445656061172485, loss=3.1537792682647705
I0203 08:02:59.254997 139923868813056 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.9802982807159424, loss=4.863606929779053
I0203 08:03:46.486836 139923852027648 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.1355359554290771, loss=3.3336966037750244
I0203 08:04:33.399464 139923868813056 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.3159286975860596, loss=2.994751214981079
I0203 08:05:20.467937 139923852027648 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.296710729598999, loss=2.8794894218444824
I0203 08:06:07.514783 139923868813056 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.9468603730201721, loss=5.366540431976318
I0203 08:06:46.336611 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:06:56.749881 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:07:29.285821 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:07:30.922725 140085747812160 submission_runner.py:408] Time since start: 50624.63s, 	Step: 98684, 	{'train/accuracy': 0.5575000047683716, 'train/loss': 1.9146215915679932, 'validation/accuracy': 0.5131999850273132, 'validation/loss': 2.1295666694641113, 'validation/num_examples': 50000, 'test/accuracy': 0.40220001339912415, 'test/loss': 2.7948946952819824, 'test/num_examples': 10000, 'score': 45831.61665439606, 'total_duration': 50624.62672114372, 'accumulated_submission_time': 45831.61665439606, 'accumulated_eval_time': 4782.682325363159, 'accumulated_logging_time': 4.801445245742798}
I0203 08:07:30.964196 139923852027648 logging_writer.py:48] [98684] accumulated_eval_time=4782.682325, accumulated_logging_time=4.801445, accumulated_submission_time=45831.616654, global_step=98684, preemption_count=0, score=45831.616654, test/accuracy=0.402200, test/loss=2.794895, test/num_examples=10000, total_duration=50624.626721, train/accuracy=0.557500, train/loss=1.914622, validation/accuracy=0.513200, validation/loss=2.129567, validation/num_examples=50000
I0203 08:07:37.644745 139923868813056 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.2380244731903076, loss=2.874636650085449
I0203 08:08:20.773718 139923852027648 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.1351686716079712, loss=3.037132501602173
I0203 08:09:07.666287 139923868813056 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.1613249778747559, loss=2.9704232215881348
I0203 08:09:54.658600 139923852027648 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.1375596523284912, loss=3.0129408836364746
I0203 08:10:41.520458 139923868813056 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.9808607697486877, loss=4.538001537322998
I0203 08:11:28.410293 139923852027648 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.977627158164978, loss=4.062522888183594
I0203 08:12:15.203225 139923868813056 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.2995662689208984, loss=3.1161386966705322
I0203 08:13:02.245515 139923852027648 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.0006777048110962, loss=3.7892568111419678
I0203 08:13:49.240296 139923868813056 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.0207226276397705, loss=5.570605278015137
I0203 08:14:31.192737 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:14:41.868010 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:15:12.973246 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:15:14.609245 140085747812160 submission_runner.py:408] Time since start: 51088.31s, 	Step: 99591, 	{'train/accuracy': 0.587890625, 'train/loss': 1.753287434577942, 'validation/accuracy': 0.5281199812889099, 'validation/loss': 2.0528218746185303, 'validation/num_examples': 50000, 'test/accuracy': 0.41030001640319824, 'test/loss': 2.728025436401367, 'test/num_examples': 10000, 'score': 46251.78324842453, 'total_duration': 51088.31325173378, 'accumulated_submission_time': 46251.78324842453, 'accumulated_eval_time': 4826.09882068634, 'accumulated_logging_time': 4.854480981826782}
I0203 08:15:14.641668 139923852027648 logging_writer.py:48] [99591] accumulated_eval_time=4826.098821, accumulated_logging_time=4.854481, accumulated_submission_time=46251.783248, global_step=99591, preemption_count=0, score=46251.783248, test/accuracy=0.410300, test/loss=2.728025, test/num_examples=10000, total_duration=51088.313252, train/accuracy=0.587891, train/loss=1.753287, validation/accuracy=0.528120, validation/loss=2.052822, validation/num_examples=50000
I0203 08:15:18.570722 139923868813056 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.0862451791763306, loss=3.07375431060791
I0203 08:16:01.263232 139923852027648 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.168705701828003, loss=3.3811511993408203
I0203 08:16:48.013068 139923868813056 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.1973600387573242, loss=3.1038568019866943
I0203 08:17:35.250234 139923852027648 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.1740347146987915, loss=3.0950942039489746
I0203 08:18:22.339003 139923868813056 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.1714385747909546, loss=3.0225467681884766
I0203 08:19:09.401201 139923852027648 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.2142521142959595, loss=3.1286673545837402
I0203 08:19:56.364700 139923868813056 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.0135061740875244, loss=4.0862321853637695
I0203 08:20:43.211876 139923852027648 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4212968349456787, loss=3.1219217777252197
I0203 08:21:30.212639 139923868813056 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.1451411247253418, loss=3.037209987640381
I0203 08:22:14.975284 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:22:25.594646 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:23:01.779430 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:23:03.421854 140085747812160 submission_runner.py:408] Time since start: 51557.13s, 	Step: 100497, 	{'train/accuracy': 0.5584765672683716, 'train/loss': 1.894142508506775, 'validation/accuracy': 0.5205199718475342, 'validation/loss': 2.0861923694610596, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.745382308959961, 'test/num_examples': 10000, 'score': 46672.05391287804, 'total_duration': 51557.12585926056, 'accumulated_submission_time': 46672.05391287804, 'accumulated_eval_time': 4874.545390844345, 'accumulated_logging_time': 4.899670839309692}
I0203 08:23:03.456355 139923852027648 logging_writer.py:48] [100497] accumulated_eval_time=4874.545391, accumulated_logging_time=4.899671, accumulated_submission_time=46672.053913, global_step=100497, preemption_count=0, score=46672.053913, test/accuracy=0.411500, test/loss=2.745382, test/num_examples=10000, total_duration=51557.125859, train/accuracy=0.558477, train/loss=1.894143, validation/accuracy=0.520520, validation/loss=2.086192, validation/num_examples=50000
I0203 08:23:05.031471 139923868813056 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.9912286996841431, loss=4.658590316772461
I0203 08:23:47.190784 139923852027648 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.9649662375450134, loss=3.9264373779296875
I0203 08:24:34.006053 139923868813056 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.1900360584259033, loss=3.9898478984832764
I0203 08:25:21.065714 139923852027648 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.1266225576400757, loss=4.027621746063232
I0203 08:26:08.090595 139923868813056 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.9359093904495239, loss=4.362175941467285
I0203 08:26:55.001170 139923852027648 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.1303884983062744, loss=2.9996588230133057
I0203 08:27:42.164435 139923868813056 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.1658427715301514, loss=3.097595453262329
I0203 08:28:29.318345 139923852027648 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.2741528749465942, loss=2.9625306129455566
I0203 08:29:16.337275 139923868813056 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.0549334287643433, loss=3.2064244747161865
I0203 08:30:03.850716 139923852027648 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.2372803688049316, loss=2.891221046447754
I0203 08:30:03.866088 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:30:14.356127 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:30:45.211006 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:30:46.851781 140085747812160 submission_runner.py:408] Time since start: 52020.56s, 	Step: 101401, 	{'train/accuracy': 0.5635937452316284, 'train/loss': 1.8744672536849976, 'validation/accuracy': 0.5206999778747559, 'validation/loss': 2.0845861434936523, 'validation/num_examples': 50000, 'test/accuracy': 0.41590002179145813, 'test/loss': 2.720703601837158, 'test/num_examples': 10000, 'score': 47092.40252113342, 'total_duration': 52020.555797576904, 'accumulated_submission_time': 47092.40252113342, 'accumulated_eval_time': 4917.531066417694, 'accumulated_logging_time': 4.94485330581665}
I0203 08:30:46.885813 139923868813056 logging_writer.py:48] [101401] accumulated_eval_time=4917.531066, accumulated_logging_time=4.944853, accumulated_submission_time=47092.402521, global_step=101401, preemption_count=0, score=47092.402521, test/accuracy=0.415900, test/loss=2.720704, test/num_examples=10000, total_duration=52020.555798, train/accuracy=0.563594, train/loss=1.874467, validation/accuracy=0.520700, validation/loss=2.084586, validation/num_examples=50000
I0203 08:31:28.596610 139923852027648 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.1682201623916626, loss=3.229307174682617
I0203 08:32:15.414347 139923868813056 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.1733871698379517, loss=3.216804027557373
I0203 08:33:02.833631 139923852027648 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.0530824661254883, loss=3.5372536182403564
I0203 08:33:49.675849 139923868813056 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.0125787258148193, loss=5.4754838943481445
I0203 08:34:36.635254 139923852027648 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.0499104261398315, loss=5.171903610229492
I0203 08:35:23.658574 139923868813056 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.307712435722351, loss=3.2239878177642822
I0203 08:36:10.883819 139923852027648 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.3658335208892822, loss=2.926870346069336
I0203 08:36:57.771080 139923868813056 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.335852026939392, loss=3.0366950035095215
I0203 08:37:44.934988 139923852027648 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.2303913831710815, loss=3.022921085357666
I0203 08:37:47.033541 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:37:57.757907 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:38:28.858436 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:38:30.502534 140085747812160 submission_runner.py:408] Time since start: 52484.21s, 	Step: 102306, 	{'train/accuracy': 0.6005273461341858, 'train/loss': 1.6896134614944458, 'validation/accuracy': 0.5290799736976624, 'validation/loss': 2.0353610515594482, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.692728281021118, 'test/num_examples': 10000, 'score': 47512.490429639816, 'total_duration': 52484.20654010773, 'accumulated_submission_time': 47512.490429639816, 'accumulated_eval_time': 4961.000032663345, 'accumulated_logging_time': 4.988691568374634}
I0203 08:38:30.534206 139923868813056 logging_writer.py:48] [102306] accumulated_eval_time=4961.000033, accumulated_logging_time=4.988692, accumulated_submission_time=47512.490430, global_step=102306, preemption_count=0, score=47512.490430, test/accuracy=0.417900, test/loss=2.692728, test/num_examples=10000, total_duration=52484.206540, train/accuracy=0.600527, train/loss=1.689613, validation/accuracy=0.529080, validation/loss=2.035361, validation/num_examples=50000
I0203 08:39:10.052823 139923852027648 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.3929554224014282, loss=2.84053897857666
I0203 08:39:56.843167 139923868813056 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.0037286281585693, loss=5.28737735748291
I0203 08:40:43.990924 139923852027648 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.1377547979354858, loss=2.9048922061920166
I0203 08:41:30.768676 139923868813056 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.0392706394195557, loss=4.572703838348389
I0203 08:42:17.620892 139923852027648 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.0134378671646118, loss=5.39853048324585
I0203 08:43:04.676235 139923868813056 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.120887041091919, loss=3.7849905490875244
I0203 08:43:51.442186 139923852027648 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.9614669680595398, loss=5.428716659545898
I0203 08:44:38.496018 139923868813056 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.2193334102630615, loss=2.948457717895508
I0203 08:45:25.291510 139923852027648 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.156335473060608, loss=3.7681620121002197
I0203 08:45:30.727309 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:45:41.200655 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:46:09.341292 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:46:10.994951 140085747812160 submission_runner.py:408] Time since start: 52944.70s, 	Step: 103213, 	{'train/accuracy': 0.5564843416213989, 'train/loss': 1.9020169973373413, 'validation/accuracy': 0.5239399671554565, 'validation/loss': 2.0747263431549072, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.7468771934509277, 'test/num_examples': 10000, 'score': 47932.62344169617, 'total_duration': 52944.69896483421, 'accumulated_submission_time': 47932.62344169617, 'accumulated_eval_time': 5001.267645597458, 'accumulated_logging_time': 5.030147075653076}
I0203 08:46:11.028470 139923868813056 logging_writer.py:48] [103213] accumulated_eval_time=5001.267646, accumulated_logging_time=5.030147, accumulated_submission_time=47932.623442, global_step=103213, preemption_count=0, score=47932.623442, test/accuracy=0.407000, test/loss=2.746877, test/num_examples=10000, total_duration=52944.698965, train/accuracy=0.556484, train/loss=1.902017, validation/accuracy=0.523940, validation/loss=2.074726, validation/num_examples=50000
I0203 08:46:47.270263 139923852027648 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.206028938293457, loss=2.7939300537109375
I0203 08:47:33.930910 139923868813056 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.2749296426773071, loss=2.896667242050171
I0203 08:48:21.034383 139923852027648 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.2201541662216187, loss=2.9894814491271973
I0203 08:49:08.274628 139923868813056 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.0812588930130005, loss=4.138482570648193
I0203 08:49:55.003019 139923852027648 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.3044110536575317, loss=2.769172191619873
I0203 08:50:41.781581 139923868813056 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.2525864839553833, loss=2.9446194171905518
I0203 08:51:28.774166 139923852027648 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.102937936782837, loss=3.004034996032715
I0203 08:52:15.711169 139923868813056 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.9053611755371094, loss=4.976848125457764
I0203 08:53:02.914285 139923852027648 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.278786063194275, loss=2.979644775390625
I0203 08:53:11.421948 140085747812160 spec.py:321] Evaluating on the training split.
I0203 08:53:22.247416 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 08:53:54.657183 140085747812160 spec.py:349] Evaluating on the test split.
I0203 08:53:56.296837 140085747812160 submission_runner.py:408] Time since start: 53410.00s, 	Step: 104120, 	{'train/accuracy': 0.5759570002555847, 'train/loss': 1.8427623510360718, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.0636205673217773, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.700232744216919, 'test/num_examples': 10000, 'score': 48352.95508027077, 'total_duration': 53410.000838279724, 'accumulated_submission_time': 48352.95508027077, 'accumulated_eval_time': 5046.1425104141235, 'accumulated_logging_time': 5.07442831993103}
I0203 08:53:56.328994 139923868813056 logging_writer.py:48] [104120] accumulated_eval_time=5046.142510, accumulated_logging_time=5.074428, accumulated_submission_time=48352.955080, global_step=104120, preemption_count=0, score=48352.955080, test/accuracy=0.418800, test/loss=2.700233, test/num_examples=10000, total_duration=53410.000838, train/accuracy=0.575957, train/loss=1.842762, validation/accuracy=0.529780, validation/loss=2.063621, validation/num_examples=50000
I0203 08:54:29.278422 139923852027648 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.196183204650879, loss=3.1847035884857178
I0203 08:55:15.899716 139923868813056 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.3841276168823242, loss=2.881772994995117
I0203 08:56:03.078271 139923852027648 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.2213577032089233, loss=5.014340877532959
I0203 08:56:49.780782 139923868813056 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.1943283081054688, loss=2.945110559463501
I0203 08:57:36.710308 139923852027648 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.1395976543426514, loss=3.019591808319092
I0203 08:58:23.652547 139923868813056 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.1576508283615112, loss=2.9183850288391113
I0203 08:59:10.483805 139923852027648 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.0227208137512207, loss=3.627873182296753
I0203 08:59:57.256849 139923868813056 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.4395703077316284, loss=2.9542369842529297
I0203 09:00:44.389839 139923852027648 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.153624176979065, loss=2.936544895172119
I0203 09:00:56.759927 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:01:07.535099 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:01:40.058055 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:01:41.704207 140085747812160 submission_runner.py:408] Time since start: 53875.41s, 	Step: 105028, 	{'train/accuracy': 0.5927929282188416, 'train/loss': 1.7482671737670898, 'validation/accuracy': 0.5322999954223633, 'validation/loss': 2.0392508506774902, 'validation/num_examples': 50000, 'test/accuracy': 0.4181000292301178, 'test/loss': 2.695244312286377, 'test/num_examples': 10000, 'score': 48773.32455801964, 'total_duration': 53875.408218860626, 'accumulated_submission_time': 48773.32455801964, 'accumulated_eval_time': 5091.086785554886, 'accumulated_logging_time': 5.116716623306274}
I0203 09:01:41.739094 139923868813056 logging_writer.py:48] [105028] accumulated_eval_time=5091.086786, accumulated_logging_time=5.116717, accumulated_submission_time=48773.324558, global_step=105028, preemption_count=0, score=48773.324558, test/accuracy=0.418100, test/loss=2.695244, test/num_examples=10000, total_duration=53875.408219, train/accuracy=0.592793, train/loss=1.748267, validation/accuracy=0.532300, validation/loss=2.039251, validation/num_examples=50000
I0203 09:02:10.852156 139923852027648 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.2210452556610107, loss=2.935086250305176
I0203 09:02:57.355476 139923868813056 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.95772784948349, loss=5.022068500518799
I0203 09:03:44.463020 139923852027648 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.0976420640945435, loss=3.2436771392822266
I0203 09:04:31.347060 139923868813056 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.0661845207214355, loss=3.3989415168762207
I0203 09:05:18.287973 139923852027648 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.2436636686325073, loss=2.8547346591949463
I0203 09:06:05.221462 139923868813056 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.500960350036621, loss=2.9244298934936523
I0203 09:06:51.963647 139923852027648 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.98979252576828, loss=4.048906326293945
I0203 09:07:38.937476 139923868813056 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.3315653800964355, loss=3.0278677940368652
I0203 09:08:25.829224 139923852027648 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.1028162240982056, loss=4.6195759773254395
I0203 09:08:41.822698 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:08:52.680447 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:09:24.371339 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:09:26.027992 140085747812160 submission_runner.py:408] Time since start: 54339.73s, 	Step: 105936, 	{'train/accuracy': 0.571972668170929, 'train/loss': 1.8334988355636597, 'validation/accuracy': 0.5362200140953064, 'validation/loss': 2.0098884105682373, 'validation/num_examples': 50000, 'test/accuracy': 0.42240002751350403, 'test/loss': 2.671083688735962, 'test/num_examples': 10000, 'score': 49193.346816301346, 'total_duration': 54339.73200273514, 'accumulated_submission_time': 49193.346816301346, 'accumulated_eval_time': 5135.2920553684235, 'accumulated_logging_time': 5.161932468414307}
I0203 09:09:26.065035 139923868813056 logging_writer.py:48] [105936] accumulated_eval_time=5135.292055, accumulated_logging_time=5.161932, accumulated_submission_time=49193.346816, global_step=105936, preemption_count=0, score=49193.346816, test/accuracy=0.422400, test/loss=2.671084, test/num_examples=10000, total_duration=54339.732003, train/accuracy=0.571973, train/loss=1.833499, validation/accuracy=0.536220, validation/loss=2.009888, validation/num_examples=50000
I0203 09:09:51.584068 139923852027648 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.9716297388076782, loss=4.740813255310059
I0203 09:10:38.082566 139923868813056 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.2287516593933105, loss=2.9089457988739014
I0203 09:11:25.450081 139923852027648 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.1426888704299927, loss=3.0678839683532715
I0203 09:12:12.432180 139923868813056 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.2104365825653076, loss=5.447898864746094
I0203 09:12:59.736541 139923852027648 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.306402325630188, loss=3.0032057762145996
I0203 09:13:46.766831 139923868813056 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.149816870689392, loss=3.324936628341675
I0203 09:14:33.796565 139923852027648 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.333461046218872, loss=3.021841526031494
I0203 09:15:20.890398 139923868813056 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.330111026763916, loss=2.8721601963043213
I0203 09:16:07.822196 139923852027648 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.255942463874817, loss=3.116865634918213
I0203 09:16:26.470939 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:16:37.375701 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:17:06.281653 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:17:07.924510 140085747812160 submission_runner.py:408] Time since start: 54801.63s, 	Step: 106841, 	{'train/accuracy': 0.5798632502555847, 'train/loss': 1.801438570022583, 'validation/accuracy': 0.5390200018882751, 'validation/loss': 2.0081238746643066, 'validation/num_examples': 50000, 'test/accuracy': 0.4261000156402588, 'test/loss': 2.6580498218536377, 'test/num_examples': 10000, 'score': 49613.69072461128, 'total_duration': 54801.62851881981, 'accumulated_submission_time': 49613.69072461128, 'accumulated_eval_time': 5176.745602607727, 'accumulated_logging_time': 5.209371089935303}
I0203 09:17:07.961595 139923868813056 logging_writer.py:48] [106841] accumulated_eval_time=5176.745603, accumulated_logging_time=5.209371, accumulated_submission_time=49613.690725, global_step=106841, preemption_count=0, score=49613.690725, test/accuracy=0.426100, test/loss=2.658050, test/num_examples=10000, total_duration=54801.628519, train/accuracy=0.579863, train/loss=1.801439, validation/accuracy=0.539020, validation/loss=2.008124, validation/num_examples=50000
I0203 09:17:31.530288 139923852027648 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.1477402448654175, loss=3.7752790451049805
I0203 09:18:18.192426 139923868813056 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.1575175523757935, loss=3.2297561168670654
I0203 09:19:05.517054 139923852027648 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.1873551607131958, loss=2.9838452339172363
I0203 09:19:52.620641 139923868813056 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.3115509748458862, loss=2.934861660003662
I0203 09:20:39.572571 139923852027648 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.2364447116851807, loss=2.942206859588623
I0203 09:21:26.787867 139923868813056 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.2387464046478271, loss=3.1682374477386475
I0203 09:22:13.893212 139923852027648 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.1962441205978394, loss=2.7983198165893555
I0203 09:23:01.372350 139923868813056 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.4243121147155762, loss=2.8621277809143066
I0203 09:23:48.456457 139923852027648 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.2878602743148804, loss=3.0239689350128174
I0203 09:24:08.342785 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:24:19.844422 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:24:54.013082 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:24:55.655567 140085747812160 submission_runner.py:408] Time since start: 55269.36s, 	Step: 107744, 	{'train/accuracy': 0.5798437595367432, 'train/loss': 1.8626606464385986, 'validation/accuracy': 0.5293999910354614, 'validation/loss': 2.1074492931365967, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.7498693466186523, 'test/num_examples': 10000, 'score': 50034.01110982895, 'total_duration': 55269.35957503319, 'accumulated_submission_time': 50034.01110982895, 'accumulated_eval_time': 5224.058357954025, 'accumulated_logging_time': 5.257244348526001}
I0203 09:24:55.690901 139923868813056 logging_writer.py:48] [107744] accumulated_eval_time=5224.058358, accumulated_logging_time=5.257244, accumulated_submission_time=50034.011110, global_step=107744, preemption_count=0, score=50034.011110, test/accuracy=0.417800, test/loss=2.749869, test/num_examples=10000, total_duration=55269.359575, train/accuracy=0.579844, train/loss=1.862661, validation/accuracy=0.529400, validation/loss=2.107449, validation/num_examples=50000
I0203 09:25:18.075665 139923852027648 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.3335071802139282, loss=2.9619970321655273
I0203 09:26:04.103470 139923868813056 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.1021254062652588, loss=4.108643531799316
I0203 09:26:51.248359 139923852027648 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.2542470693588257, loss=2.9115042686462402
I0203 09:27:38.164931 139923868813056 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.1859930753707886, loss=3.4512367248535156
I0203 09:28:25.251981 139923852027648 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.9308708310127258, loss=5.336424827575684
I0203 09:29:12.124584 139923868813056 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.2802832126617432, loss=2.8457109928131104
I0203 09:29:58.841300 139923852027648 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.3083769083023071, loss=2.8654425144195557
I0203 09:30:45.647464 139923868813056 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.9930512309074402, loss=5.228651523590088
I0203 09:31:32.734797 139923852027648 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.065312147140503, loss=4.025424003601074
I0203 09:31:56.030848 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:32:06.703145 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:32:35.141261 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:32:36.783751 140085747812160 submission_runner.py:408] Time since start: 55730.49s, 	Step: 108651, 	{'train/accuracy': 0.5818749666213989, 'train/loss': 1.7912267446517944, 'validation/accuracy': 0.5414400100708008, 'validation/loss': 1.9961179494857788, 'validation/num_examples': 50000, 'test/accuracy': 0.4240000247955322, 'test/loss': 2.6577701568603516, 'test/num_examples': 10000, 'score': 50454.28623723984, 'total_duration': 55730.48776316643, 'accumulated_submission_time': 50454.28623723984, 'accumulated_eval_time': 5264.811240434647, 'accumulated_logging_time': 5.306406021118164}
I0203 09:32:36.818268 139923868813056 logging_writer.py:48] [108651] accumulated_eval_time=5264.811240, accumulated_logging_time=5.306406, accumulated_submission_time=50454.286237, global_step=108651, preemption_count=0, score=50454.286237, test/accuracy=0.424000, test/loss=2.657770, test/num_examples=10000, total_duration=55730.487763, train/accuracy=0.581875, train/loss=1.791227, validation/accuracy=0.541440, validation/loss=1.996118, validation/num_examples=50000
I0203 09:32:56.453397 139923852027648 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.2125883102416992, loss=2.745732307434082
I0203 09:33:41.882827 139923868813056 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.3395105600357056, loss=2.885141611099243
I0203 09:34:29.116034 139923852027648 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.2962744235992432, loss=2.9194202423095703
I0203 09:35:16.178110 139923868813056 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.3361915349960327, loss=3.1170990467071533
I0203 09:36:03.044952 139923852027648 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.2218762636184692, loss=2.9141829013824463
I0203 09:36:50.005552 139923868813056 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.2485600709915161, loss=3.2043280601501465
I0203 09:37:37.117314 139923852027648 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.0482618808746338, loss=3.856431007385254
I0203 09:38:24.177762 139923868813056 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.456287145614624, loss=2.848093032836914
I0203 09:39:11.216760 139923852027648 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.1690999269485474, loss=4.094583034515381
I0203 09:39:37.177146 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:39:47.756549 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:40:15.090427 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:40:16.733867 140085747812160 submission_runner.py:408] Time since start: 56190.44s, 	Step: 109557, 	{'train/accuracy': 0.5807812213897705, 'train/loss': 1.7843594551086426, 'validation/accuracy': 0.5475599765777588, 'validation/loss': 1.9676730632781982, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.6383376121520996, 'test/num_examples': 10000, 'score': 50874.582459926605, 'total_duration': 56190.437881946564, 'accumulated_submission_time': 50874.582459926605, 'accumulated_eval_time': 5304.367951393127, 'accumulated_logging_time': 5.35333776473999}
I0203 09:40:16.768801 139923868813056 logging_writer.py:48] [109557] accumulated_eval_time=5304.367951, accumulated_logging_time=5.353338, accumulated_submission_time=50874.582460, global_step=109557, preemption_count=0, score=50874.582460, test/accuracy=0.429400, test/loss=2.638338, test/num_examples=10000, total_duration=56190.437882, train/accuracy=0.580781, train/loss=1.784359, validation/accuracy=0.547560, validation/loss=1.967673, validation/num_examples=50000
I0203 09:40:34.052255 139923852027648 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.2314894199371338, loss=2.801795482635498
I0203 09:41:19.144807 139923868813056 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.0124905109405518, loss=3.653876781463623
I0203 09:42:05.962343 139923852027648 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.242144227027893, loss=2.845578670501709
I0203 09:42:52.954369 139923868813056 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.2728602886199951, loss=2.8956568241119385
I0203 09:43:39.951027 139923852027648 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.1725101470947266, loss=3.537768602371216
I0203 09:44:27.054601 139923868813056 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.9670069217681885, loss=4.872932434082031
I0203 09:45:13.933653 139923852027648 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.3899688720703125, loss=2.8431389331817627
I0203 09:46:00.725436 139923868813056 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.9724439382553101, loss=4.933617115020752
I0203 09:46:47.685641 139923852027648 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.9187029600143433, loss=4.930253982543945
I0203 09:47:16.852193 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:47:27.637737 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:47:59.984280 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:48:01.618300 140085747812160 submission_runner.py:408] Time since start: 56655.32s, 	Step: 110464, 	{'train/accuracy': 0.5948437452316284, 'train/loss': 1.731536626815796, 'validation/accuracy': 0.5445600152015686, 'validation/loss': 1.9790481328964233, 'validation/num_examples': 50000, 'test/accuracy': 0.43150001764297485, 'test/loss': 2.643044948577881, 'test/num_examples': 10000, 'score': 51294.605360507965, 'total_duration': 56655.322309970856, 'accumulated_submission_time': 51294.605360507965, 'accumulated_eval_time': 5349.13404250145, 'accumulated_logging_time': 5.398436546325684}
I0203 09:48:01.653936 139923868813056 logging_writer.py:48] [110464] accumulated_eval_time=5349.134043, accumulated_logging_time=5.398437, accumulated_submission_time=51294.605361, global_step=110464, preemption_count=0, score=51294.605361, test/accuracy=0.431500, test/loss=2.643045, test/num_examples=10000, total_duration=56655.322310, train/accuracy=0.594844, train/loss=1.731537, validation/accuracy=0.544560, validation/loss=1.979048, validation/num_examples=50000
I0203 09:48:16.177987 139923852027648 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.1051745414733887, loss=4.558575630187988
I0203 09:49:00.702702 139923868813056 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.2760529518127441, loss=2.666851043701172
I0203 09:49:47.756938 139923852027648 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.2833216190338135, loss=2.808948040008545
I0203 09:50:34.503808 139923868813056 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.2675138711929321, loss=2.7777702808380127
I0203 09:51:21.177206 139923852027648 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.2918602228164673, loss=3.165752649307251
I0203 09:52:08.011629 139923868813056 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.1684266328811646, loss=2.8937783241271973
I0203 09:52:55.216673 139923852027648 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.0482581853866577, loss=5.365304946899414
I0203 09:53:42.171968 139923868813056 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.133861780166626, loss=5.301604270935059
I0203 09:54:29.225870 139923852027648 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.1453200578689575, loss=3.3131189346313477
I0203 09:55:01.889505 140085747812160 spec.py:321] Evaluating on the training split.
I0203 09:55:12.366998 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 09:55:39.895439 140085747812160 spec.py:349] Evaluating on the test split.
I0203 09:55:41.539557 140085747812160 submission_runner.py:408] Time since start: 57115.24s, 	Step: 111371, 	{'train/accuracy': 0.5899804830551147, 'train/loss': 1.7508074045181274, 'validation/accuracy': 0.555079996585846, 'validation/loss': 1.9349216222763062, 'validation/num_examples': 50000, 'test/accuracy': 0.4382000267505646, 'test/loss': 2.608139991760254, 'test/num_examples': 10000, 'score': 51714.772094249725, 'total_duration': 57115.24357366562, 'accumulated_submission_time': 51714.772094249725, 'accumulated_eval_time': 5388.784100055695, 'accumulated_logging_time': 5.452826261520386}
I0203 09:55:41.576739 139923868813056 logging_writer.py:48] [111371] accumulated_eval_time=5388.784100, accumulated_logging_time=5.452826, accumulated_submission_time=51714.772094, global_step=111371, preemption_count=0, score=51714.772094, test/accuracy=0.438200, test/loss=2.608140, test/num_examples=10000, total_duration=57115.243574, train/accuracy=0.589980, train/loss=1.750807, validation/accuracy=0.555080, validation/loss=1.934922, validation/num_examples=50000
I0203 09:55:53.357524 139923852027648 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.1033916473388672, loss=3.62300968170166
I0203 09:56:37.192093 139923868813056 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.420056939125061, loss=2.8196821212768555
I0203 09:57:24.110576 139923852027648 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.1664074659347534, loss=4.025982856750488
I0203 09:58:10.735528 139923868813056 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.2521506547927856, loss=3.092421531677246
I0203 09:58:57.393029 139923852027648 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.0132540464401245, loss=5.396026611328125
I0203 09:59:44.075310 139923868813056 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.0850954055786133, loss=4.022007465362549
I0203 10:00:31.023175 139923852027648 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.1940388679504395, loss=2.7930030822753906
I0203 10:01:17.900255 139923868813056 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.1214048862457275, loss=4.133942604064941
I0203 10:02:04.673585 139923852027648 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.9857859015464783, loss=5.224875450134277
I0203 10:02:41.901620 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:02:52.406664 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:03:24.856600 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:03:26.504377 140085747812160 submission_runner.py:408] Time since start: 57580.21s, 	Step: 112281, 	{'train/accuracy': 0.5955273509025574, 'train/loss': 1.7364590167999268, 'validation/accuracy': 0.5569800138473511, 'validation/loss': 1.9330426454544067, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.598820447921753, 'test/num_examples': 10000, 'score': 52135.0346596241, 'total_duration': 57580.20839238167, 'accumulated_submission_time': 52135.0346596241, 'accumulated_eval_time': 5433.386849164963, 'accumulated_logging_time': 5.500851154327393}
I0203 10:03:26.541131 139923868813056 logging_writer.py:48] [112281] accumulated_eval_time=5433.386849, accumulated_logging_time=5.500851, accumulated_submission_time=52135.034660, global_step=112281, preemption_count=0, score=52135.034660, test/accuracy=0.441300, test/loss=2.598820, test/num_examples=10000, total_duration=57580.208392, train/accuracy=0.595527, train/loss=1.736459, validation/accuracy=0.556980, validation/loss=1.933043, validation/num_examples=50000
I0203 10:03:34.390450 139923852027648 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.3385417461395264, loss=3.5503766536712646
I0203 10:04:17.607718 139923868813056 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.257657527923584, loss=2.9661927223205566
I0203 10:05:04.475653 139923852027648 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.4788185358047485, loss=2.8267569541931152
I0203 10:05:51.284664 139923868813056 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.0200843811035156, loss=4.07078218460083
I0203 10:06:38.239077 139923852027648 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.462519645690918, loss=2.7821848392486572
I0203 10:07:25.224273 139923868813056 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.051138162612915, loss=4.413267612457275
I0203 10:08:12.477846 139923852027648 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.3042744398117065, loss=2.7990431785583496
I0203 10:08:59.517471 139923868813056 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.003803014755249, loss=4.812699317932129
I0203 10:09:46.666071 139923852027648 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.2751190662384033, loss=2.8728723526000977
I0203 10:10:26.752499 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:10:37.026069 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:11:11.573611 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:11:13.231034 140085747812160 submission_runner.py:408] Time since start: 58046.94s, 	Step: 113187, 	{'train/accuracy': 0.6030077934265137, 'train/loss': 1.6891416311264038, 'validation/accuracy': 0.5582199692726135, 'validation/loss': 1.9116791486740112, 'validation/num_examples': 50000, 'test/accuracy': 0.44210001826286316, 'test/loss': 2.5687875747680664, 'test/num_examples': 10000, 'score': 52555.18597626686, 'total_duration': 58046.9350438118, 'accumulated_submission_time': 52555.18597626686, 'accumulated_eval_time': 5479.865357398987, 'accumulated_logging_time': 5.547860145568848}
I0203 10:11:13.265483 139923868813056 logging_writer.py:48] [113187] accumulated_eval_time=5479.865357, accumulated_logging_time=5.547860, accumulated_submission_time=52555.185976, global_step=113187, preemption_count=0, score=52555.185976, test/accuracy=0.442100, test/loss=2.568788, test/num_examples=10000, total_duration=58046.935044, train/accuracy=0.603008, train/loss=1.689142, validation/accuracy=0.558220, validation/loss=1.911679, validation/num_examples=50000
I0203 10:11:18.765286 139923852027648 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.306863784790039, loss=4.104532241821289
I0203 10:12:01.553814 139923868813056 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.2455990314483643, loss=2.8337090015411377
I0203 10:12:48.750587 139923852027648 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.3650190830230713, loss=2.7659497261047363
I0203 10:13:35.678959 139923868813056 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.2064927816390991, loss=2.9576358795166016
I0203 10:14:22.561071 139923852027648 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.3697930574417114, loss=2.7394278049468994
I0203 10:15:09.398365 139923868813056 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.3690383434295654, loss=2.770901679992676
I0203 10:15:56.335425 139923852027648 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.2377086877822876, loss=2.7480626106262207
I0203 10:16:43.404828 139923868813056 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.1681759357452393, loss=4.763914108276367
I0203 10:17:30.476696 139923852027648 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.1919552087783813, loss=4.892055034637451
I0203 10:18:13.652668 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:18:24.116605 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:18:58.710295 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:19:00.350273 140085747812160 submission_runner.py:408] Time since start: 58514.05s, 	Step: 114093, 	{'train/accuracy': 0.5959765315055847, 'train/loss': 1.7394243478775024, 'validation/accuracy': 0.5590400099754333, 'validation/loss': 1.9202549457550049, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.6083571910858154, 'test/num_examples': 10000, 'score': 52975.51297545433, 'total_duration': 58514.05428671837, 'accumulated_submission_time': 52975.51297545433, 'accumulated_eval_time': 5526.562952518463, 'accumulated_logging_time': 5.59241247177124}
I0203 10:19:00.387651 139923868813056 logging_writer.py:48] [114093] accumulated_eval_time=5526.562953, accumulated_logging_time=5.592412, accumulated_submission_time=52975.512975, global_step=114093, preemption_count=0, score=52975.512975, test/accuracy=0.440900, test/loss=2.608357, test/num_examples=10000, total_duration=58514.054287, train/accuracy=0.595977, train/loss=1.739424, validation/accuracy=0.559040, validation/loss=1.920255, validation/num_examples=50000
I0203 10:19:03.545378 139923852027648 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.2114734649658203, loss=2.8250718116760254
I0203 10:19:46.097097 139923868813056 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.1449170112609863, loss=3.6006362438201904
I0203 10:20:32.942942 139923852027648 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.1083236932754517, loss=4.405704021453857
I0203 10:21:20.071697 139923868813056 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.3273259401321411, loss=2.9078176021575928
I0203 10:22:07.064957 139923852027648 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.1984621286392212, loss=3.385287284851074
I0203 10:22:54.036908 139923868813056 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.5699799060821533, loss=2.882798671722412
I0203 10:23:41.185862 139923852027648 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.2804495096206665, loss=2.82460355758667
I0203 10:24:28.085105 139923868813056 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.1081712245941162, loss=4.215219974517822
I0203 10:25:14.895710 139923852027648 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.3719695806503296, loss=2.732618808746338
I0203 10:26:00.797765 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:26:11.607324 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:26:44.826310 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:26:46.480894 140085747812160 submission_runner.py:408] Time since start: 58980.18s, 	Step: 114999, 	{'train/accuracy': 0.5999413728713989, 'train/loss': 1.698953628540039, 'validation/accuracy': 0.5589599609375, 'validation/loss': 1.9015834331512451, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.568891763687134, 'test/num_examples': 10000, 'score': 53395.86148428917, 'total_duration': 58980.18489718437, 'accumulated_submission_time': 53395.86148428917, 'accumulated_eval_time': 5572.246104717255, 'accumulated_logging_time': 5.641361236572266}
I0203 10:26:46.518985 139923868813056 logging_writer.py:48] [114999] accumulated_eval_time=5572.246105, accumulated_logging_time=5.641361, accumulated_submission_time=53395.861484, global_step=114999, preemption_count=0, score=53395.861484, test/accuracy=0.443800, test/loss=2.568892, test/num_examples=10000, total_duration=58980.184897, train/accuracy=0.599941, train/loss=1.698954, validation/accuracy=0.558960, validation/loss=1.901583, validation/num_examples=50000
I0203 10:26:47.306806 139923852027648 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.1734211444854736, loss=5.092742443084717
I0203 10:27:29.476112 139923868813056 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.4669119119644165, loss=2.7900986671447754
I0203 10:28:16.332506 139923852027648 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.4028255939483643, loss=2.9383130073547363
I0203 10:29:03.522902 139923868813056 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.3141100406646729, loss=2.7349050045013428
I0203 10:29:50.473840 139923852027648 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.3686708211898804, loss=3.0065441131591797
I0203 10:30:37.512669 139923868813056 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.1925610303878784, loss=3.509369373321533
I0203 10:31:24.696934 139923852027648 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.042630910873413, loss=4.576953887939453
I0203 10:32:11.707401 139923868813056 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.3546332120895386, loss=2.7408173084259033
I0203 10:32:58.986126 139923852027648 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.2712812423706055, loss=3.2543630599975586
I0203 10:33:45.997367 139923868813056 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.432787299156189, loss=2.611768960952759
I0203 10:33:46.611697 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:33:57.347633 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:34:27.849075 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:34:29.490885 140085747812160 submission_runner.py:408] Time since start: 59443.19s, 	Step: 115903, 	{'train/accuracy': 0.6122460961341858, 'train/loss': 1.6250638961791992, 'validation/accuracy': 0.5637800097465515, 'validation/loss': 1.8601083755493164, 'validation/num_examples': 50000, 'test/accuracy': 0.44770002365112305, 'test/loss': 2.5321247577667236, 'test/num_examples': 10000, 'score': 53815.892790317535, 'total_duration': 59443.194900512695, 'accumulated_submission_time': 53815.892790317535, 'accumulated_eval_time': 5615.125262737274, 'accumulated_logging_time': 5.690145254135132}
I0203 10:34:29.523840 139923852027648 logging_writer.py:48] [115903] accumulated_eval_time=5615.125263, accumulated_logging_time=5.690145, accumulated_submission_time=53815.892790, global_step=115903, preemption_count=0, score=53815.892790, test/accuracy=0.447700, test/loss=2.532125, test/num_examples=10000, total_duration=59443.194901, train/accuracy=0.612246, train/loss=1.625064, validation/accuracy=0.563780, validation/loss=1.860108, validation/num_examples=50000
I0203 10:35:10.524361 139923868813056 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.3847527503967285, loss=2.758909225463867
I0203 10:35:57.214976 139923852027648 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.3049739599227905, loss=2.739166021347046
I0203 10:36:44.177582 139923868813056 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.4468539953231812, loss=2.880324602127075
I0203 10:37:31.064831 139923852027648 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.2830276489257812, loss=2.5675768852233887
I0203 10:38:18.114754 139923868813056 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.42097806930542, loss=2.761451482772827
I0203 10:39:05.254430 139923852027648 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.4470117092132568, loss=2.9365525245666504
I0203 10:39:52.190480 139923868813056 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.3574955463409424, loss=2.872232437133789
I0203 10:40:39.312493 139923852027648 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.3170198202133179, loss=3.052093029022217
I0203 10:41:26.512689 139923868813056 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.528908610343933, loss=2.8547403812408447
I0203 10:41:29.912082 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:41:40.762309 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:42:10.376196 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:42:12.006780 140085747812160 submission_runner.py:408] Time since start: 59905.71s, 	Step: 116809, 	{'train/accuracy': 0.6307226419448853, 'train/loss': 1.5751163959503174, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8742843866348267, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.542642116546631, 'test/num_examples': 10000, 'score': 54236.21877121925, 'total_duration': 59905.71079039574, 'accumulated_submission_time': 54236.21877121925, 'accumulated_eval_time': 5657.219936609268, 'accumulated_logging_time': 5.732917547225952}
I0203 10:42:12.043484 139923852027648 logging_writer.py:48] [116809] accumulated_eval_time=5657.219937, accumulated_logging_time=5.732918, accumulated_submission_time=54236.218771, global_step=116809, preemption_count=0, score=54236.218771, test/accuracy=0.442900, test/loss=2.542642, test/num_examples=10000, total_duration=59905.710790, train/accuracy=0.630723, train/loss=1.575116, validation/accuracy=0.565280, validation/loss=1.874284, validation/num_examples=50000
I0203 10:42:50.182211 139923868813056 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.350150227546692, loss=2.8666861057281494
I0203 10:43:37.130640 139923852027648 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.0664210319519043, loss=4.665670871734619
I0203 10:44:24.157036 139923868813056 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.3098721504211426, loss=2.7133429050445557
I0203 10:45:10.991588 139923852027648 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.2732195854187012, loss=2.9826841354370117
I0203 10:45:58.005788 139923868813056 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.301316499710083, loss=2.790140390396118
I0203 10:46:45.186738 139923852027648 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.2336173057556152, loss=3.4318063259124756
I0203 10:47:31.957581 139923868813056 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.1020456552505493, loss=4.78695011138916
I0203 10:48:19.328895 139923852027648 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.3389643430709839, loss=2.848313570022583
I0203 10:49:06.144409 139923868813056 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.1450530290603638, loss=5.201503753662109
I0203 10:49:12.359789 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:49:22.944064 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:49:53.346903 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:49:54.980127 140085747812160 submission_runner.py:408] Time since start: 60368.68s, 	Step: 117715, 	{'train/accuracy': 0.606738269329071, 'train/loss': 1.6873126029968262, 'validation/accuracy': 0.5666399598121643, 'validation/loss': 1.881292462348938, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.5546884536743164, 'test/num_examples': 10000, 'score': 54656.47298908234, 'total_duration': 60368.68414545059, 'accumulated_submission_time': 54656.47298908234, 'accumulated_eval_time': 5699.84024477005, 'accumulated_logging_time': 5.780933141708374}
I0203 10:49:55.014199 139923852027648 logging_writer.py:48] [117715] accumulated_eval_time=5699.840245, accumulated_logging_time=5.780933, accumulated_submission_time=54656.472989, global_step=117715, preemption_count=0, score=54656.472989, test/accuracy=0.444200, test/loss=2.554688, test/num_examples=10000, total_duration=60368.684145, train/accuracy=0.606738, train/loss=1.687313, validation/accuracy=0.566640, validation/loss=1.881292, validation/num_examples=50000
I0203 10:50:30.137696 139923868813056 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.3239699602127075, loss=2.731194257736206
I0203 10:51:16.937236 139923852027648 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.262926697731018, loss=3.0165061950683594
I0203 10:52:04.275883 139923868813056 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.3305925130844116, loss=2.6992273330688477
I0203 10:52:51.472252 139923852027648 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.3425456285476685, loss=5.260294437408447
I0203 10:53:38.292279 139923868813056 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.139493465423584, loss=4.423386096954346
I0203 10:54:25.122979 139923852027648 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.2148061990737915, loss=3.631989002227783
I0203 10:55:12.242591 139923868813056 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.427643060684204, loss=2.6939215660095215
I0203 10:55:59.046328 139923852027648 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.189888596534729, loss=4.733373165130615
I0203 10:56:46.148998 139923868813056 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.2464261054992676, loss=3.4166579246520996
I0203 10:56:55.374851 140085747812160 spec.py:321] Evaluating on the training split.
I0203 10:57:06.036725 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 10:57:37.015923 140085747812160 spec.py:349] Evaluating on the test split.
I0203 10:57:38.659038 140085747812160 submission_runner.py:408] Time since start: 60832.36s, 	Step: 118621, 	{'train/accuracy': 0.6252343654632568, 'train/loss': 1.57450532913208, 'validation/accuracy': 0.5776000022888184, 'validation/loss': 1.8012522459030151, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.4840798377990723, 'test/num_examples': 10000, 'score': 55076.772084236145, 'total_duration': 60832.36305522919, 'accumulated_submission_time': 55076.772084236145, 'accumulated_eval_time': 5743.124422311783, 'accumulated_logging_time': 5.825575351715088}
I0203 10:57:38.695879 139923852027648 logging_writer.py:48] [118621] accumulated_eval_time=5743.124422, accumulated_logging_time=5.825575, accumulated_submission_time=55076.772084, global_step=118621, preemption_count=0, score=55076.772084, test/accuracy=0.459500, test/loss=2.484080, test/num_examples=10000, total_duration=60832.363055, train/accuracy=0.625234, train/loss=1.574505, validation/accuracy=0.577600, validation/loss=1.801252, validation/num_examples=50000
I0203 10:58:11.116297 139923868813056 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.2710802555084229, loss=2.740025281906128
I0203 10:58:57.823270 139923852027648 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.1671271324157715, loss=3.892031669616699
I0203 10:59:45.185748 139923868813056 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.3645373582839966, loss=2.7200868129730225
I0203 11:00:32.326685 139923852027648 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.3298159837722778, loss=2.6459248065948486
I0203 11:01:19.487391 139923868813056 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.1769542694091797, loss=5.252553939819336
I0203 11:02:06.873298 139923852027648 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.4879916906356812, loss=2.679105281829834
I0203 11:02:54.241494 139923868813056 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.2205498218536377, loss=3.189847946166992
I0203 11:03:41.474844 139923852027648 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.4652378559112549, loss=3.0931248664855957
I0203 11:04:28.532249 139923868813056 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.3287676572799683, loss=5.088377475738525
I0203 11:04:38.677787 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:04:49.074164 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:05:22.065801 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:05:23.703443 140085747812160 submission_runner.py:408] Time since start: 61297.41s, 	Step: 119523, 	{'train/accuracy': 0.6426367163658142, 'train/loss': 1.501293420791626, 'validation/accuracy': 0.5740599632263184, 'validation/loss': 1.8339369297027588, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.4974396228790283, 'test/num_examples': 10000, 'score': 55496.693836927414, 'total_duration': 61297.40745139122, 'accumulated_submission_time': 55496.693836927414, 'accumulated_eval_time': 5788.15007519722, 'accumulated_logging_time': 5.872597932815552}
I0203 11:05:23.740555 139923852027648 logging_writer.py:48] [119523] accumulated_eval_time=5788.150075, accumulated_logging_time=5.872598, accumulated_submission_time=55496.693837, global_step=119523, preemption_count=0, score=55496.693837, test/accuracy=0.456700, test/loss=2.497440, test/num_examples=10000, total_duration=61297.407451, train/accuracy=0.642637, train/loss=1.501293, validation/accuracy=0.574060, validation/loss=1.833937, validation/num_examples=50000
I0203 11:05:55.516434 139923868813056 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.5011497735977173, loss=2.6652607917785645
I0203 11:06:42.367159 139923852027648 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.5666451454162598, loss=2.636443853378296
I0203 11:07:29.669661 139923868813056 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.399330973625183, loss=2.527200222015381
I0203 11:08:16.678191 139923852027648 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.3648725748062134, loss=2.559738874435425
I0203 11:09:03.596008 139923868813056 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.3541171550750732, loss=2.7114219665527344
I0203 11:09:50.757840 139923852027648 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.2543867826461792, loss=2.851797580718994
I0203 11:10:37.751328 139923868813056 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.4515622854232788, loss=2.5664796829223633
I0203 11:11:25.012973 139923852027648 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.484315037727356, loss=2.5668797492980957
I0203 11:12:11.942282 139923868813056 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.6929117441177368, loss=2.9694485664367676
I0203 11:12:23.892381 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:12:34.756785 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:13:07.010859 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:13:08.654979 140085747812160 submission_runner.py:408] Time since start: 61762.36s, 	Step: 120427, 	{'train/accuracy': 0.6160937547683716, 'train/loss': 1.6148782968521118, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.8061535358428955, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.485478162765503, 'test/num_examples': 10000, 'score': 55916.784044504166, 'total_duration': 61762.35899710655, 'accumulated_submission_time': 55916.784044504166, 'accumulated_eval_time': 5832.912647247314, 'accumulated_logging_time': 5.921623706817627}
I0203 11:13:08.691975 139923852027648 logging_writer.py:48] [120427] accumulated_eval_time=5832.912647, accumulated_logging_time=5.921624, accumulated_submission_time=55916.784045, global_step=120427, preemption_count=0, score=55916.784045, test/accuracy=0.456100, test/loss=2.485478, test/num_examples=10000, total_duration=61762.358997, train/accuracy=0.616094, train/loss=1.614878, validation/accuracy=0.580680, validation/loss=1.806154, validation/num_examples=50000
I0203 11:13:38.517303 139923868813056 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.3420085906982422, loss=2.8163390159606934
I0203 11:14:25.164876 139923852027648 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.2468159198760986, loss=3.3800506591796875
I0203 11:15:12.376515 139923868813056 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.0873019695281982, loss=4.04875373840332
I0203 11:15:59.212998 139923852027648 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.3563339710235596, loss=2.785950183868408
I0203 11:16:46.020072 139923868813056 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.208522915840149, loss=3.140077829360962
I0203 11:17:33.118335 139923852027648 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.2167344093322754, loss=5.16544771194458
I0203 11:18:20.186789 139923868813056 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.3875683546066284, loss=3.876951217651367
I0203 11:19:07.131336 139923852027648 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.0456312894821167, loss=5.05264949798584
I0203 11:19:54.127277 139923868813056 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.49380624294281, loss=5.379195690155029
I0203 11:20:08.795033 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:20:19.276325 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:20:55.516441 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:20:57.185099 140085747812160 submission_runner.py:408] Time since start: 62230.89s, 	Step: 121333, 	{'train/accuracy': 0.6248632669448853, 'train/loss': 1.5781731605529785, 'validation/accuracy': 0.5776399970054626, 'validation/loss': 1.8077296018600464, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.4872207641601562, 'test/num_examples': 10000, 'score': 56336.82581615448, 'total_duration': 62230.889113903046, 'accumulated_submission_time': 56336.82581615448, 'accumulated_eval_time': 5881.302688598633, 'accumulated_logging_time': 5.969411849975586}
I0203 11:20:57.224137 139923852027648 logging_writer.py:48] [121333] accumulated_eval_time=5881.302689, accumulated_logging_time=5.969412, accumulated_submission_time=56336.825816, global_step=121333, preemption_count=0, score=56336.825816, test/accuracy=0.458400, test/loss=2.487221, test/num_examples=10000, total_duration=62230.889114, train/accuracy=0.624863, train/loss=1.578173, validation/accuracy=0.577640, validation/loss=1.807730, validation/num_examples=50000
I0203 11:21:23.886703 139923868813056 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.07309889793396, loss=5.01838493347168
I0203 11:22:10.774750 139923852027648 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.2568904161453247, loss=3.7108962535858154
I0203 11:22:58.361089 139923868813056 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.052080512046814, loss=5.159797668457031
I0203 11:23:45.501519 139923852027648 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.5293612480163574, loss=2.615372896194458
I0203 11:24:32.679580 139923868813056 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.234922170639038, loss=4.349253177642822
I0203 11:25:19.754217 139923852027648 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.5394264459609985, loss=2.4250032901763916
I0203 11:26:06.865293 139923868813056 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.364808201789856, loss=2.8575122356414795
I0203 11:26:53.708639 139923852027648 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.2263143062591553, loss=3.838568687438965
I0203 11:27:40.787855 139923868813056 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.351088285446167, loss=2.5694539546966553
I0203 11:27:57.371432 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:28:07.874664 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:28:40.597100 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:28:42.247385 140085747812160 submission_runner.py:408] Time since start: 62695.95s, 	Step: 122237, 	{'train/accuracy': 0.6359570026397705, 'train/loss': 1.5680468082427979, 'validation/accuracy': 0.5774999856948853, 'validation/loss': 1.843104600906372, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.5103447437286377, 'test/num_examples': 10000, 'score': 56756.91334247589, 'total_duration': 62695.95139026642, 'accumulated_submission_time': 56756.91334247589, 'accumulated_eval_time': 5926.17863202095, 'accumulated_logging_time': 6.0188164710998535}
I0203 11:28:42.285568 139923852027648 logging_writer.py:48] [122237] accumulated_eval_time=5926.178632, accumulated_logging_time=6.018816, accumulated_submission_time=56756.913342, global_step=122237, preemption_count=0, score=56756.913342, test/accuracy=0.458400, test/loss=2.510345, test/num_examples=10000, total_duration=62695.951390, train/accuracy=0.635957, train/loss=1.568047, validation/accuracy=0.577500, validation/loss=1.843105, validation/num_examples=50000
I0203 11:29:07.400362 139923868813056 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.282090187072754, loss=5.213524341583252
I0203 11:29:53.946533 139923852027648 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.3401217460632324, loss=2.834591865539551
I0203 11:30:41.284248 139923868813056 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.2309274673461914, loss=3.674513339996338
I0203 11:31:28.237548 139923852027648 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.304451584815979, loss=2.6066768169403076
I0203 11:32:15.034337 139923868813056 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.3356105089187622, loss=3.600006580352783
I0203 11:33:02.085054 139923852027648 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.2113392353057861, loss=4.270450115203857
I0203 11:33:49.109445 139923868813056 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.372159719467163, loss=2.6682448387145996
I0203 11:34:35.954129 139923852027648 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.463659405708313, loss=2.594406843185425
I0203 11:35:22.806865 139923868813056 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.2538447380065918, loss=4.674903392791748
I0203 11:35:42.561319 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:35:53.213131 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:36:22.694087 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:36:24.336174 140085747812160 submission_runner.py:408] Time since start: 63158.04s, 	Step: 123144, 	{'train/accuracy': 0.6289648413658142, 'train/loss': 1.563175916671753, 'validation/accuracy': 0.5860599875450134, 'validation/loss': 1.769154667854309, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.4270832538604736, 'test/num_examples': 10000, 'score': 57177.12469792366, 'total_duration': 63158.04018783569, 'accumulated_submission_time': 57177.12469792366, 'accumulated_eval_time': 5967.953478097916, 'accumulated_logging_time': 6.071156740188599}
I0203 11:36:24.372388 139923852027648 logging_writer.py:48] [123144] accumulated_eval_time=5967.953478, accumulated_logging_time=6.071157, accumulated_submission_time=57177.124698, global_step=123144, preemption_count=0, score=57177.124698, test/accuracy=0.467700, test/loss=2.427083, test/num_examples=10000, total_duration=63158.040188, train/accuracy=0.628965, train/loss=1.563176, validation/accuracy=0.586060, validation/loss=1.769155, validation/num_examples=50000
I0203 11:36:46.754915 139923868813056 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.4676384925842285, loss=2.5846147537231445
I0203 11:37:33.113849 139923852027648 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.3118504285812378, loss=3.265700578689575
I0203 11:38:20.104223 139923868813056 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.4283947944641113, loss=2.4499728679656982
I0203 11:39:07.167857 139923852027648 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.4677289724349976, loss=2.5255160331726074
I0203 11:39:54.205749 139923868813056 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.2421761751174927, loss=4.3002095222473145
I0203 11:40:41.303099 139923852027648 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.5841695070266724, loss=2.705812454223633
I0203 11:41:28.351175 139923868813056 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.4127758741378784, loss=2.5870087146759033
I0203 11:42:15.261940 139923852027648 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.5668917894363403, loss=2.661555767059326
I0203 11:43:02.478132 139923868813056 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.2679182291030884, loss=3.6372787952423096
I0203 11:43:24.664273 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:43:35.108064 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:44:09.185072 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:44:10.825768 140085747812160 submission_runner.py:408] Time since start: 63624.53s, 	Step: 124049, 	{'train/accuracy': 0.640429675579071, 'train/loss': 1.5160832405090332, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.7468619346618652, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4021661281585693, 'test/num_examples': 10000, 'score': 57597.355749607086, 'total_duration': 63624.529782772064, 'accumulated_submission_time': 57597.355749607086, 'accumulated_eval_time': 6014.114964962006, 'accumulated_logging_time': 6.117965459823608}
I0203 11:44:10.864469 139923852027648 logging_writer.py:48] [124049] accumulated_eval_time=6014.114965, accumulated_logging_time=6.117965, accumulated_submission_time=57597.355750, global_step=124049, preemption_count=0, score=57597.355750, test/accuracy=0.470500, test/loss=2.402166, test/num_examples=10000, total_duration=63624.529783, train/accuracy=0.640430, train/loss=1.516083, validation/accuracy=0.591860, validation/loss=1.746862, validation/num_examples=50000
I0203 11:44:31.273619 139923868813056 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.501605749130249, loss=3.6855568885803223
I0203 11:45:17.063198 139923852027648 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.432332158088684, loss=2.8475518226623535
I0203 11:46:04.126202 139923868813056 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.4375979900360107, loss=2.638983964920044
I0203 11:46:51.077071 139923852027648 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.3752758502960205, loss=2.5423007011413574
I0203 11:47:38.129136 139923868813056 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.2595409154891968, loss=4.107485294342041
I0203 11:48:25.246518 139923852027648 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.3596197366714478, loss=3.066861152648926
I0203 11:49:11.961750 139923868813056 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.260309100151062, loss=5.154341220855713
I0203 11:49:59.261893 139923852027648 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.4486916065216064, loss=2.57975435256958
I0203 11:50:46.331384 139923868813056 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.67020583152771, loss=2.721292495727539
I0203 11:51:11.091706 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:51:21.703589 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:51:55.616286 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:51:57.254682 140085747812160 submission_runner.py:408] Time since start: 64090.96s, 	Step: 124954, 	{'train/accuracy': 0.6468749642372131, 'train/loss': 1.47525954246521, 'validation/accuracy': 0.5937199592590332, 'validation/loss': 1.733345627784729, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.402538537979126, 'test/num_examples': 10000, 'score': 58017.52013874054, 'total_duration': 64090.95868706703, 'accumulated_submission_time': 58017.52013874054, 'accumulated_eval_time': 6060.277943134308, 'accumulated_logging_time': 6.1684510707855225}
I0203 11:51:57.290698 139923852027648 logging_writer.py:48] [124954] accumulated_eval_time=6060.277943, accumulated_logging_time=6.168451, accumulated_submission_time=58017.520139, global_step=124954, preemption_count=0, score=58017.520139, test/accuracy=0.467300, test/loss=2.402539, test/num_examples=10000, total_duration=64090.958687, train/accuracy=0.646875, train/loss=1.475260, validation/accuracy=0.593720, validation/loss=1.733346, validation/num_examples=50000
I0203 11:52:15.745231 139923868813056 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.2087843418121338, loss=4.276552200317383
I0203 11:53:01.545314 139923852027648 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.2612433433532715, loss=3.211564064025879
I0203 11:53:48.468800 139923868813056 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.461881399154663, loss=2.546767473220825
I0203 11:54:35.393152 139923852027648 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.5854949951171875, loss=2.5503268241882324
I0203 11:55:22.531738 139923868813056 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.4881693124771118, loss=4.23919677734375
I0203 11:56:09.648471 139923852027648 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.53132164478302, loss=2.5103583335876465
I0203 11:56:56.661491 139923868813056 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.5873626470565796, loss=2.7241244316101074
I0203 11:57:43.734924 139923852027648 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.46841299533844, loss=2.6261754035949707
I0203 11:58:30.830609 139923868813056 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.2366124391555786, loss=2.8202929496765137
I0203 11:58:57.363550 140085747812160 spec.py:321] Evaluating on the training split.
I0203 11:59:08.173966 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 11:59:39.690460 140085747812160 spec.py:349] Evaluating on the test split.
I0203 11:59:41.337802 140085747812160 submission_runner.py:408] Time since start: 64555.04s, 	Step: 125858, 	{'train/accuracy': 0.6384179592132568, 'train/loss': 1.4969202280044556, 'validation/accuracy': 0.5963000059127808, 'validation/loss': 1.702694058418274, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.3764123916625977, 'test/num_examples': 10000, 'score': 58437.53057861328, 'total_duration': 64555.04180955887, 'accumulated_submission_time': 58437.53057861328, 'accumulated_eval_time': 6104.252175807953, 'accumulated_logging_time': 6.2157580852508545}
I0203 11:59:41.375662 139923852027648 logging_writer.py:48] [125858] accumulated_eval_time=6104.252176, accumulated_logging_time=6.215758, accumulated_submission_time=58437.530579, global_step=125858, preemption_count=0, score=58437.530579, test/accuracy=0.475400, test/loss=2.376412, test/num_examples=10000, total_duration=64555.041810, train/accuracy=0.638418, train/loss=1.496920, validation/accuracy=0.596300, validation/loss=1.702694, validation/num_examples=50000
I0203 11:59:58.255632 139923868813056 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.6031255722045898, loss=2.515446424484253
I0203 12:00:43.341345 139923852027648 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.465774655342102, loss=2.4489450454711914
I0203 12:01:30.385780 139923868813056 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.322658896446228, loss=3.4001612663269043
I0203 12:02:17.437259 139923852027648 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.2442396879196167, loss=4.634633541107178
I0203 12:03:04.680138 139923868813056 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.520176887512207, loss=2.5658013820648193
I0203 12:03:51.748710 139923852027648 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.2183575630187988, loss=4.664999961853027
I0203 12:04:38.883875 139923868813056 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.4460251331329346, loss=2.743865966796875
I0203 12:05:25.981241 139923852027648 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.51939058303833, loss=2.602412700653076
I0203 12:06:12.936040 139923868813056 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.35759437084198, loss=5.183424949645996
I0203 12:06:41.746057 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:06:52.468871 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:07:25.068468 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:07:26.708412 140085747812160 submission_runner.py:408] Time since start: 65020.41s, 	Step: 126763, 	{'train/accuracy': 0.6460155844688416, 'train/loss': 1.478101372718811, 'validation/accuracy': 0.6011399626731873, 'validation/loss': 1.703931212425232, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.3670125007629395, 'test/num_examples': 10000, 'score': 58857.83850765228, 'total_duration': 65020.412427186966, 'accumulated_submission_time': 58857.83850765228, 'accumulated_eval_time': 6149.2145137786865, 'accumulated_logging_time': 6.2652997970581055}
I0203 12:07:26.745857 139923852027648 logging_writer.py:48] [126763] accumulated_eval_time=6149.214514, accumulated_logging_time=6.265300, accumulated_submission_time=58857.838508, global_step=126763, preemption_count=0, score=58857.838508, test/accuracy=0.479100, test/loss=2.367013, test/num_examples=10000, total_duration=65020.412427, train/accuracy=0.646016, train/loss=1.478101, validation/accuracy=0.601140, validation/loss=1.703931, validation/num_examples=50000
I0203 12:07:41.656206 139923868813056 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.7009276151657104, loss=2.78623366355896
I0203 12:08:26.552158 139923852027648 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.452901840209961, loss=2.475862979888916
I0203 12:09:13.361246 139923868813056 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.3229731321334839, loss=5.01559591293335
I0203 12:10:00.329324 139923852027648 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.472360372543335, loss=2.6904854774475098
I0203 12:10:47.265288 139923868813056 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.541404366493225, loss=2.669339179992676
I0203 12:11:34.250975 139923852027648 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.557120680809021, loss=2.470357656478882
I0203 12:12:21.279817 139923868813056 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.5366119146347046, loss=2.4852864742279053
I0203 12:13:08.388234 139923852027648 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.5917688608169556, loss=5.199087142944336
I0203 12:13:55.408783 139923868813056 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.5204826593399048, loss=2.558715343475342
I0203 12:14:26.989975 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:14:38.022823 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:15:13.080166 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:15:14.712757 140085747812160 submission_runner.py:408] Time since start: 65488.42s, 	Step: 127669, 	{'train/accuracy': 0.6504492163658142, 'train/loss': 1.4836300611495972, 'validation/accuracy': 0.5961999893188477, 'validation/loss': 1.7273157835006714, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.3896336555480957, 'test/num_examples': 10000, 'score': 59278.021849155426, 'total_duration': 65488.41676783562, 'accumulated_submission_time': 59278.021849155426, 'accumulated_eval_time': 6196.937285423279, 'accumulated_logging_time': 6.313244581222534}
I0203 12:15:14.754397 139923852027648 logging_writer.py:48] [127669] accumulated_eval_time=6196.937285, accumulated_logging_time=6.313245, accumulated_submission_time=59278.021849, global_step=127669, preemption_count=0, score=59278.021849, test/accuracy=0.478200, test/loss=2.389634, test/num_examples=10000, total_duration=65488.416768, train/accuracy=0.650449, train/loss=1.483630, validation/accuracy=0.596200, validation/loss=1.727316, validation/num_examples=50000
I0203 12:15:27.319043 139923868813056 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.2248185873031616, loss=4.211808204650879
I0203 12:16:11.151708 139923852027648 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.426584243774414, loss=4.577611923217773
I0203 12:16:58.103859 139923868813056 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.1870230436325073, loss=4.273611545562744
I0203 12:17:45.089098 139923852027648 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.4664994478225708, loss=2.575429677963257
I0203 12:18:32.270304 139923868813056 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.1690868139266968, loss=4.186881065368652
I0203 12:19:18.959325 139923852027648 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.271049976348877, loss=4.449048042297363
I0203 12:20:06.049148 139923868813056 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.3578871488571167, loss=4.527561187744141
I0203 12:20:52.973626 139923852027648 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.7279930114746094, loss=2.627608299255371
I0203 12:21:39.831928 139923868813056 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.4648672342300415, loss=2.7804887294769287
I0203 12:22:14.996222 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:22:25.757627 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:22:57.701555 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:22:59.339832 140085747812160 submission_runner.py:408] Time since start: 65953.04s, 	Step: 128576, 	{'train/accuracy': 0.6483983993530273, 'train/loss': 1.4797208309173584, 'validation/accuracy': 0.6062399744987488, 'validation/loss': 1.6872668266296387, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3387036323547363, 'test/num_examples': 10000, 'score': 59698.199717760086, 'total_duration': 65953.0438401699, 'accumulated_submission_time': 59698.199717760086, 'accumulated_eval_time': 6241.280877828598, 'accumulated_logging_time': 6.368543863296509}
I0203 12:22:59.377738 139923852027648 logging_writer.py:48] [128576] accumulated_eval_time=6241.280878, accumulated_logging_time=6.368544, accumulated_submission_time=59698.199718, global_step=128576, preemption_count=0, score=59698.199718, test/accuracy=0.487000, test/loss=2.338704, test/num_examples=10000, total_duration=65953.043840, train/accuracy=0.648398, train/loss=1.479721, validation/accuracy=0.606240, validation/loss=1.687267, validation/num_examples=50000
I0203 12:23:09.192270 139923868813056 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.4945040941238403, loss=2.540778160095215
I0203 12:23:53.177596 139923852027648 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.461855173110962, loss=2.5084805488586426
I0203 12:24:40.254338 139923868813056 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.6698647737503052, loss=2.5241127014160156
I0203 12:25:27.309807 139923852027648 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.3132386207580566, loss=3.828885793685913
I0203 12:26:14.364493 139923868813056 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.4942381381988525, loss=2.883479356765747
I0203 12:27:01.392845 139923852027648 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.5209965705871582, loss=2.480050563812256
I0203 12:27:48.542067 139923868813056 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.2395637035369873, loss=3.6153314113616943
I0203 12:28:35.676267 139923852027648 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.5204789638519287, loss=2.418952465057373
I0203 12:29:22.845302 139923868813056 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.3732647895812988, loss=4.634779453277588
I0203 12:29:59.777602 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:30:10.661614 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:30:44.605952 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:30:46.251227 140085747812160 submission_runner.py:408] Time since start: 66419.96s, 	Step: 129480, 	{'train/accuracy': 0.6539648175239563, 'train/loss': 1.4522738456726074, 'validation/accuracy': 0.605679988861084, 'validation/loss': 1.6788485050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.354218006134033, 'test/num_examples': 10000, 'score': 60118.538821697235, 'total_duration': 66419.9552268982, 'accumulated_submission_time': 60118.538821697235, 'accumulated_eval_time': 6287.754509687424, 'accumulated_logging_time': 6.416506052017212}
I0203 12:30:46.290921 139923852027648 logging_writer.py:48] [129480] accumulated_eval_time=6287.754510, accumulated_logging_time=6.416506, accumulated_submission_time=60118.538822, global_step=129480, preemption_count=0, score=60118.538822, test/accuracy=0.484100, test/loss=2.354218, test/num_examples=10000, total_duration=66419.955227, train/accuracy=0.653965, train/loss=1.452274, validation/accuracy=0.605680, validation/loss=1.678849, validation/num_examples=50000
I0203 12:30:54.542482 139923868813056 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.3418655395507812, loss=4.2701945304870605
I0203 12:31:37.936132 139923852027648 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.2711122035980225, loss=4.98604679107666
I0203 12:32:24.997116 139923868813056 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.504964828491211, loss=2.5324487686157227
I0203 12:33:11.993245 139923852027648 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.2800722122192383, loss=5.077081203460693
I0203 12:33:59.055603 139923868813056 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.694315791130066, loss=2.5816426277160645
I0203 12:34:45.779649 139923852027648 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.4345918893814087, loss=2.8324573040008545
I0203 12:35:32.830612 139923868813056 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.6159734725952148, loss=2.471219301223755
I0203 12:36:19.859237 139923852027648 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.7577698230743408, loss=2.423574924468994
I0203 12:37:06.663597 139923868813056 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.345862627029419, loss=3.462266683578491
I0203 12:37:46.256567 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:37:57.776294 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:38:30.157014 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:38:31.798643 140085747812160 submission_runner.py:408] Time since start: 66885.50s, 	Step: 130386, 	{'train/accuracy': 0.6702734231948853, 'train/loss': 1.4016005992889404, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.6603667736053467, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.3320703506469727, 'test/num_examples': 10000, 'score': 60538.44319176674, 'total_duration': 66885.50264811516, 'accumulated_submission_time': 60538.44319176674, 'accumulated_eval_time': 6333.296562671661, 'accumulated_logging_time': 6.466786623001099}
I0203 12:38:31.835780 139923852027648 logging_writer.py:48] [130386] accumulated_eval_time=6333.296563, accumulated_logging_time=6.466787, accumulated_submission_time=60538.443192, global_step=130386, preemption_count=0, score=60538.443192, test/accuracy=0.492200, test/loss=2.332070, test/num_examples=10000, total_duration=66885.502648, train/accuracy=0.670273, train/loss=1.401601, validation/accuracy=0.613620, validation/loss=1.660367, validation/num_examples=50000
I0203 12:38:37.723307 139923868813056 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.2545294761657715, loss=3.0890088081359863
I0203 12:39:20.501865 139923852027648 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.491538643836975, loss=2.5785489082336426
I0203 12:40:07.407926 139923868813056 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.6139670610427856, loss=2.403954029083252
I0203 12:40:54.247248 139923852027648 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.5396878719329834, loss=2.819878339767456
I0203 12:41:41.287857 139923868813056 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.3542578220367432, loss=5.0638651847839355
I0203 12:42:28.335905 139923852027648 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.3147481679916382, loss=4.099301815032959
I0203 12:43:15.474496 139923868813056 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.509482979774475, loss=2.8525497913360596
I0203 12:44:02.292412 139923852027648 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.44280207157135, loss=2.5965678691864014
I0203 12:44:49.136412 139923868813056 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.268876314163208, loss=4.387027740478516
I0203 12:45:32.108647 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:45:42.577233 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:46:11.759159 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:46:13.412458 140085747812160 submission_runner.py:408] Time since start: 67347.12s, 	Step: 131293, 	{'train/accuracy': 0.6552929282188416, 'train/loss': 1.4327030181884766, 'validation/accuracy': 0.6084200143814087, 'validation/loss': 1.6539702415466309, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.317131996154785, 'test/num_examples': 10000, 'score': 60958.65554857254, 'total_duration': 67347.1164739132, 'accumulated_submission_time': 60958.65554857254, 'accumulated_eval_time': 6374.600378513336, 'accumulated_logging_time': 6.51424241065979}
I0203 12:46:13.449917 139923852027648 logging_writer.py:48] [131293] accumulated_eval_time=6374.600379, accumulated_logging_time=6.514242, accumulated_submission_time=60958.655549, global_step=131293, preemption_count=0, score=60958.655549, test/accuracy=0.490700, test/loss=2.317132, test/num_examples=10000, total_duration=67347.116474, train/accuracy=0.655293, train/loss=1.432703, validation/accuracy=0.608420, validation/loss=1.653970, validation/num_examples=50000
I0203 12:46:16.596872 139923868813056 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6469825506210327, loss=2.3540327548980713
I0203 12:46:59.363591 139923852027648 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.5572983026504517, loss=3.0509464740753174
I0203 12:47:45.936366 139923868813056 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.6432301998138428, loss=2.4581940174102783
I0203 12:48:33.112514 139923852027648 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.6288440227508545, loss=2.5369770526885986
I0203 12:49:20.272923 139923868813056 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.6181936264038086, loss=2.510122776031494
I0203 12:50:07.153881 139923852027648 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.3948875665664673, loss=2.528346538543701
I0203 12:50:54.203088 139923868813056 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.5821313858032227, loss=2.5829193592071533
I0203 12:51:41.142833 139923852027648 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.653125524520874, loss=2.431873321533203
I0203 12:52:28.246214 139923868813056 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.3873883485794067, loss=4.616615295410156
I0203 12:53:13.507318 140085747812160 spec.py:321] Evaluating on the training split.
I0203 12:53:24.286602 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 12:53:57.870359 140085747812160 spec.py:349] Evaluating on the test split.
I0203 12:53:59.519913 140085747812160 submission_runner.py:408] Time since start: 67813.22s, 	Step: 132198, 	{'train/accuracy': 0.6626366972923279, 'train/loss': 1.4006588459014893, 'validation/accuracy': 0.6137999892234802, 'validation/loss': 1.6258078813552856, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3008532524108887, 'test/num_examples': 10000, 'score': 61378.65146899223, 'total_duration': 67813.22391462326, 'accumulated_submission_time': 61378.65146899223, 'accumulated_eval_time': 6420.612956047058, 'accumulated_logging_time': 6.562076568603516}
I0203 12:53:59.557021 139923852027648 logging_writer.py:48] [132198] accumulated_eval_time=6420.612956, accumulated_logging_time=6.562077, accumulated_submission_time=61378.651469, global_step=132198, preemption_count=0, score=61378.651469, test/accuracy=0.491700, test/loss=2.300853, test/num_examples=10000, total_duration=67813.223915, train/accuracy=0.662637, train/loss=1.400659, validation/accuracy=0.613800, validation/loss=1.625808, validation/num_examples=50000
I0203 12:54:00.737415 139923868813056 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.7042453289031982, loss=2.458735227584839
I0203 12:54:42.761017 139923852027648 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.3421307802200317, loss=4.584251403808594
I0203 12:55:29.454965 139923868813056 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.6774952411651611, loss=2.294429302215576
I0203 12:56:16.568998 139923852027648 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.3143621683120728, loss=4.642396450042725
I0203 12:57:03.423965 139923868813056 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.5575629472732544, loss=2.4131247997283936
I0203 12:57:50.467807 139923852027648 logging_writer.py:48] [132700] global_step=132700, grad_norm=1.484787940979004, loss=2.525266647338867
I0203 12:58:37.574557 139923868813056 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.6664340496063232, loss=2.560396194458008
I0203 12:59:24.517288 139923852027648 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.5426114797592163, loss=2.5117483139038086
I0203 13:00:11.444398 139923868813056 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.4113212823867798, loss=3.4555625915527344
I0203 13:00:58.505385 139923852027648 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.5779931545257568, loss=2.616607904434204
I0203 13:00:59.609292 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:01:10.466199 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:01:43.037923 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:01:44.677554 140085747812160 submission_runner.py:408] Time since start: 68278.38s, 	Step: 133104, 	{'train/accuracy': 0.6655077934265137, 'train/loss': 1.4070724248886108, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.652784824371338, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3164613246917725, 'test/num_examples': 10000, 'score': 61798.642517089844, 'total_duration': 68278.38157367706, 'accumulated_submission_time': 61798.642517089844, 'accumulated_eval_time': 6465.681235074997, 'accumulated_logging_time': 6.609776496887207}
I0203 13:01:44.714668 139923868813056 logging_writer.py:48] [133104] accumulated_eval_time=6465.681235, accumulated_logging_time=6.609776, accumulated_submission_time=61798.642517, global_step=133104, preemption_count=0, score=61798.642517, test/accuracy=0.490100, test/loss=2.316461, test/num_examples=10000, total_duration=68278.381574, train/accuracy=0.665508, train/loss=1.407072, validation/accuracy=0.614260, validation/loss=1.652785, validation/num_examples=50000
I0203 13:02:25.284069 139923852027648 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.351878046989441, loss=5.0517497062683105
I0203 13:03:12.357076 139923868813056 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.6262198686599731, loss=2.3689725399017334
I0203 13:03:59.309611 139923852027648 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.3094817399978638, loss=4.222201347351074
I0203 13:04:46.362004 139923868813056 logging_writer.py:48] [133500] global_step=133500, grad_norm=1.7219083309173584, loss=2.54194974899292
I0203 13:05:33.312302 139923852027648 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.5854958295822144, loss=2.3993325233459473
I0203 13:06:20.366648 139923868813056 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.3975794315338135, loss=4.318769454956055
I0203 13:07:07.671811 139923852027648 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.7304973602294922, loss=2.3787784576416016
I0203 13:07:54.732960 139923868813056 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.5291551351547241, loss=2.397153377532959
I0203 13:08:41.668739 139923852027648 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.4478296041488647, loss=2.4906680583953857
I0203 13:08:44.681633 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:08:55.095108 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:09:27.127640 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:09:28.769937 140085747812160 submission_runner.py:408] Time since start: 68742.47s, 	Step: 134008, 	{'train/accuracy': 0.6812499761581421, 'train/loss': 1.345516324043274, 'validation/accuracy': 0.6206799745559692, 'validation/loss': 1.6100428104400635, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.2720818519592285, 'test/num_examples': 10000, 'score': 62218.546484708786, 'total_duration': 68742.47394442558, 'accumulated_submission_time': 62218.546484708786, 'accumulated_eval_time': 6509.769501447678, 'accumulated_logging_time': 6.65800929069519}
I0203 13:09:28.807250 139923868813056 logging_writer.py:48] [134008] accumulated_eval_time=6509.769501, accumulated_logging_time=6.658009, accumulated_submission_time=62218.546485, global_step=134008, preemption_count=0, score=62218.546485, test/accuracy=0.497700, test/loss=2.272082, test/num_examples=10000, total_duration=68742.473944, train/accuracy=0.681250, train/loss=1.345516, validation/accuracy=0.620680, validation/loss=1.610043, validation/num_examples=50000
I0203 13:10:07.392086 139923852027648 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.5989941358566284, loss=2.547168731689453
I0203 13:10:54.153167 139923868813056 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.6895726919174194, loss=2.336357593536377
I0203 13:11:41.383422 139923852027648 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.5430465936660767, loss=2.4928722381591797
I0203 13:12:28.497059 139923868813056 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.5502564907073975, loss=2.8106000423431396
I0203 13:13:15.617317 139923852027648 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.6087757349014282, loss=2.3651115894317627
I0203 13:14:02.476294 139923868813056 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.429256796836853, loss=4.885476112365723
I0203 13:14:49.588251 139923852027648 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.5515345335006714, loss=2.66367506980896
I0203 13:15:36.707370 139923868813056 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.3714931011199951, loss=3.3429458141326904
I0203 13:16:23.940660 139923852027648 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.6449345350265503, loss=2.2962543964385986
I0203 13:16:29.200572 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:16:39.702137 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:17:13.951653 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:17:15.606680 140085747812160 submission_runner.py:408] Time since start: 69209.31s, 	Step: 134913, 	{'train/accuracy': 0.6734570264816284, 'train/loss': 1.3531352281570435, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.563097596168518, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.2082858085632324, 'test/num_examples': 10000, 'score': 62638.877262592316, 'total_duration': 69209.31068348885, 'accumulated_submission_time': 62638.877262592316, 'accumulated_eval_time': 6556.1755702495575, 'accumulated_logging_time': 6.708175182342529}
I0203 13:17:15.644666 139923868813056 logging_writer.py:48] [134913] accumulated_eval_time=6556.175570, accumulated_logging_time=6.708175, accumulated_submission_time=62638.877263, global_step=134913, preemption_count=0, score=62638.877263, test/accuracy=0.507800, test/loss=2.208286, test/num_examples=10000, total_duration=69209.310683, train/accuracy=0.673457, train/loss=1.353135, validation/accuracy=0.626680, validation/loss=1.563098, validation/num_examples=50000
I0203 13:17:52.028937 139923852027648 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.6159569025039673, loss=2.6278133392333984
I0203 13:18:39.109105 139923868813056 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.4306087493896484, loss=4.524462699890137
I0203 13:19:26.622138 139923852027648 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.6277765035629272, loss=3.499467372894287
I0203 13:20:13.484098 139923868813056 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.5110856294631958, loss=4.877870559692383
I0203 13:21:00.602174 139923852027648 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.474822998046875, loss=2.8735504150390625
I0203 13:21:47.570486 139923868813056 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.6188362836837769, loss=2.360321283340454
I0203 13:22:34.690805 139923852027648 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.6133992671966553, loss=2.3159453868865967
I0203 13:23:21.582004 139923868813056 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.559408187866211, loss=2.4251503944396973
I0203 13:24:08.560904 139923852027648 logging_writer.py:48] [135800] global_step=135800, grad_norm=1.3787950277328491, loss=3.8476665019989014
I0203 13:24:15.997003 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:24:26.531889 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:24:56.781536 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:24:58.426413 140085747812160 submission_runner.py:408] Time since start: 69672.13s, 	Step: 135817, 	{'train/accuracy': 0.6738085746765137, 'train/loss': 1.3599903583526611, 'validation/accuracy': 0.6240599751472473, 'validation/loss': 1.5829092264175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.244506359100342, 'test/num_examples': 10000, 'score': 63059.16834306717, 'total_duration': 69672.13042116165, 'accumulated_submission_time': 63059.16834306717, 'accumulated_eval_time': 6598.604954004288, 'accumulated_logging_time': 6.757187128067017}
I0203 13:24:58.463054 139923868813056 logging_writer.py:48] [135817] accumulated_eval_time=6598.604954, accumulated_logging_time=6.757187, accumulated_submission_time=63059.168343, global_step=135817, preemption_count=0, score=63059.168343, test/accuracy=0.503100, test/loss=2.244506, test/num_examples=10000, total_duration=69672.130421, train/accuracy=0.673809, train/loss=1.359990, validation/accuracy=0.624060, validation/loss=1.582909, validation/num_examples=50000
I0203 13:25:32.719640 139923852027648 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.492891550064087, loss=2.7670624256134033
I0203 13:26:19.361314 139923868813056 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.4562509059906006, loss=3.918431282043457
I0203 13:27:06.594213 139923852027648 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.4568067789077759, loss=2.8707187175750732
I0203 13:27:53.438961 139923868813056 logging_writer.py:48] [136200] global_step=136200, grad_norm=1.7229849100112915, loss=2.3387537002563477
I0203 13:28:40.647421 139923852027648 logging_writer.py:48] [136300] global_step=136300, grad_norm=1.629709005355835, loss=2.45536732673645
I0203 13:29:27.799463 139923868813056 logging_writer.py:48] [136400] global_step=136400, grad_norm=1.783829689025879, loss=2.4479846954345703
I0203 13:30:14.859764 139923852027648 logging_writer.py:48] [136500] global_step=136500, grad_norm=1.7152457237243652, loss=2.265523910522461
I0203 13:31:01.988260 139923868813056 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.3318147659301758, loss=4.191486835479736
I0203 13:31:49.008251 139923852027648 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.6101224422454834, loss=2.2512710094451904
I0203 13:31:58.659334 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:32:09.630077 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:32:42.931843 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:32:44.573702 140085747812160 submission_runner.py:408] Time since start: 70138.28s, 	Step: 136722, 	{'train/accuracy': 0.7079687118530273, 'train/loss': 1.2369790077209473, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.5897619724273682, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.246837615966797, 'test/num_examples': 10000, 'score': 63479.30086803436, 'total_duration': 70138.27771234512, 'accumulated_submission_time': 63479.30086803436, 'accumulated_eval_time': 6644.519300699234, 'accumulated_logging_time': 6.807169198989868}
I0203 13:32:44.612772 139923868813056 logging_writer.py:48] [136722] accumulated_eval_time=6644.519301, accumulated_logging_time=6.807169, accumulated_submission_time=63479.300868, global_step=136722, preemption_count=0, score=63479.300868, test/accuracy=0.502000, test/loss=2.246838, test/num_examples=10000, total_duration=70138.277712, train/accuracy=0.707969, train/loss=1.236979, validation/accuracy=0.627200, validation/loss=1.589762, validation/num_examples=50000
I0203 13:33:16.494183 139923852027648 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.6705659627914429, loss=2.470865488052368
I0203 13:34:03.139652 139923868813056 logging_writer.py:48] [136900] global_step=136900, grad_norm=1.578125238418579, loss=2.693906307220459
I0203 13:34:50.161968 139923852027648 logging_writer.py:48] [137000] global_step=137000, grad_norm=1.5038305521011353, loss=2.3140735626220703
I0203 13:35:37.179486 139923868813056 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.4358816146850586, loss=3.7253668308258057
I0203 13:36:23.862426 139923852027648 logging_writer.py:48] [137200] global_step=137200, grad_norm=1.8432568311691284, loss=2.3427789211273193
I0203 13:37:10.729607 139923868813056 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.4692944288253784, loss=3.4711923599243164
I0203 13:37:57.427536 139923852027648 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.7007890939712524, loss=2.284270763397217
I0203 13:38:44.362824 139923868813056 logging_writer.py:48] [137500] global_step=137500, grad_norm=1.6936722993850708, loss=2.2652840614318848
I0203 13:39:31.298465 139923852027648 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.7389975786209106, loss=2.462519645690918
I0203 13:39:44.829114 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:39:55.147100 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:40:23.793067 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:40:25.428695 140085747812160 submission_runner.py:408] Time since start: 70599.13s, 	Step: 137630, 	{'train/accuracy': 0.675585925579071, 'train/loss': 1.340919017791748, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.5627877712249756, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.199458122253418, 'test/num_examples': 10000, 'score': 63899.45568084717, 'total_duration': 70599.13270401955, 'accumulated_submission_time': 63899.45568084717, 'accumulated_eval_time': 6685.11887216568, 'accumulated_logging_time': 6.857578992843628}
I0203 13:40:25.465782 139923868813056 logging_writer.py:48] [137630] accumulated_eval_time=6685.118872, accumulated_logging_time=6.857579, accumulated_submission_time=63899.455681, global_step=137630, preemption_count=0, score=63899.455681, test/accuracy=0.509900, test/loss=2.199458, test/num_examples=10000, total_duration=70599.132704, train/accuracy=0.675586, train/loss=1.340919, validation/accuracy=0.632760, validation/loss=1.562788, validation/num_examples=50000
I0203 13:40:53.696630 139923852027648 logging_writer.py:48] [137700] global_step=137700, grad_norm=1.7027807235717773, loss=4.3868021965026855
I0203 13:41:40.640992 139923868813056 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.6136678457260132, loss=2.4283218383789062
I0203 13:42:27.989596 139923852027648 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.4984980821609497, loss=3.692502498626709
I0203 13:43:15.121215 139923868813056 logging_writer.py:48] [138000] global_step=138000, grad_norm=1.6742557287216187, loss=2.707080364227295
I0203 13:44:02.168654 139923852027648 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.5269862413406372, loss=3.308086395263672
I0203 13:44:49.238098 139923868813056 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.4356017112731934, loss=4.4137444496154785
I0203 13:45:36.310676 139923852027648 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.766168236732483, loss=2.3225271701812744
I0203 13:46:23.518029 139923868813056 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.4564099311828613, loss=4.54666805267334
I0203 13:47:10.739986 139923852027648 logging_writer.py:48] [138500] global_step=138500, grad_norm=1.5592228174209595, loss=4.905935764312744
I0203 13:47:25.554619 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:47:36.082985 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:48:07.341571 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:48:08.978163 140085747812160 submission_runner.py:408] Time since start: 71062.68s, 	Step: 138533, 	{'train/accuracy': 0.6859960556030273, 'train/loss': 1.2904889583587646, 'validation/accuracy': 0.6377399563789368, 'validation/loss': 1.5166523456573486, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.1723082065582275, 'test/num_examples': 10000, 'score': 64319.47856760025, 'total_duration': 71062.68217468262, 'accumulated_submission_time': 64319.47856760025, 'accumulated_eval_time': 6728.542403936386, 'accumulated_logging_time': 6.904958009719849}
I0203 13:48:09.017121 139923868813056 logging_writer.py:48] [138533] accumulated_eval_time=6728.542404, accumulated_logging_time=6.904958, accumulated_submission_time=64319.478568, global_step=138533, preemption_count=0, score=64319.478568, test/accuracy=0.512000, test/loss=2.172308, test/num_examples=10000, total_duration=71062.682175, train/accuracy=0.685996, train/loss=1.290489, validation/accuracy=0.637740, validation/loss=1.516652, validation/num_examples=50000
I0203 13:48:36.006919 139923852027648 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.5678704977035522, loss=2.756443500518799
I0203 13:49:23.066357 139923868813056 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.5579564571380615, loss=2.9532368183135986
I0203 13:50:10.195846 139923852027648 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.5843538045883179, loss=2.7793705463409424
I0203 13:50:58.567080 139923868813056 logging_writer.py:48] [138900] global_step=138900, grad_norm=1.723676085472107, loss=2.3780510425567627
I0203 13:51:45.350997 139923852027648 logging_writer.py:48] [139000] global_step=139000, grad_norm=1.4917445182800293, loss=4.173192501068115
I0203 13:52:32.531629 139923868813056 logging_writer.py:48] [139100] global_step=139100, grad_norm=1.6980863809585571, loss=2.3394627571105957
I0203 13:53:19.556997 139923852027648 logging_writer.py:48] [139200] global_step=139200, grad_norm=1.708577036857605, loss=2.3803131580352783
I0203 13:54:06.433887 139923868813056 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.7951939105987549, loss=2.550053834915161
I0203 13:54:53.647321 139923852027648 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.7521628141403198, loss=4.61236572265625
I0203 13:55:09.301673 140085747812160 spec.py:321] Evaluating on the training split.
I0203 13:55:20.017309 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 13:55:51.924517 140085747812160 spec.py:349] Evaluating on the test split.
I0203 13:55:53.557372 140085747812160 submission_runner.py:408] Time since start: 71527.26s, 	Step: 139435, 	{'train/accuracy': 0.6988281011581421, 'train/loss': 1.2646912336349487, 'validation/accuracy': 0.6321799755096436, 'validation/loss': 1.5676641464233398, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2396981716156006, 'test/num_examples': 10000, 'score': 64739.70188331604, 'total_duration': 71527.2613837719, 'accumulated_submission_time': 64739.70188331604, 'accumulated_eval_time': 6772.7980988025665, 'accumulated_logging_time': 6.95450758934021}
I0203 13:55:53.594625 139923868813056 logging_writer.py:48] [139435] accumulated_eval_time=6772.798099, accumulated_logging_time=6.954508, accumulated_submission_time=64739.701883, global_step=139435, preemption_count=0, score=64739.701883, test/accuracy=0.506300, test/loss=2.239698, test/num_examples=10000, total_duration=71527.261384, train/accuracy=0.698828, train/loss=1.264691, validation/accuracy=0.632180, validation/loss=1.567664, validation/num_examples=50000
I0203 13:56:19.498139 139923852027648 logging_writer.py:48] [139500] global_step=139500, grad_norm=1.6314339637756348, loss=4.069459915161133
I0203 13:57:06.085310 139923868813056 logging_writer.py:48] [139600] global_step=139600, grad_norm=1.974861741065979, loss=2.2888684272766113
I0203 13:57:53.083965 139923852027648 logging_writer.py:48] [139700] global_step=139700, grad_norm=1.7190027236938477, loss=2.3955013751983643
I0203 13:58:40.044513 139923868813056 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.745062232017517, loss=2.320096492767334
I0203 13:59:26.884175 139923852027648 logging_writer.py:48] [139900] global_step=139900, grad_norm=1.7149816751480103, loss=2.144474983215332
I0203 14:00:13.653125 139923868813056 logging_writer.py:48] [140000] global_step=140000, grad_norm=1.6663011312484741, loss=2.1913278102874756
I0203 14:01:00.816679 139923852027648 logging_writer.py:48] [140100] global_step=140100, grad_norm=1.6371489763259888, loss=2.6890177726745605
I0203 14:01:47.636977 139923868813056 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.6236823797225952, loss=3.016374111175537
I0203 14:02:34.577552 139923852027648 logging_writer.py:48] [140300] global_step=140300, grad_norm=1.9195990562438965, loss=2.3107850551605225
I0203 14:02:53.584078 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:03:03.946189 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:03:36.356005 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:03:38.003618 140085747812160 submission_runner.py:408] Time since start: 71991.71s, 	Step: 140342, 	{'train/accuracy': 0.6906836032867432, 'train/loss': 1.2769279479980469, 'validation/accuracy': 0.6407399773597717, 'validation/loss': 1.5057283639907837, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.154827356338501, 'test/num_examples': 10000, 'score': 65159.62976360321, 'total_duration': 71991.70762300491, 'accumulated_submission_time': 65159.62976360321, 'accumulated_eval_time': 6817.2175986766815, 'accumulated_logging_time': 7.002084493637085}
I0203 14:03:38.045939 139923868813056 logging_writer.py:48] [140342] accumulated_eval_time=6817.217599, accumulated_logging_time=7.002084, accumulated_submission_time=65159.629764, global_step=140342, preemption_count=0, score=65159.629764, test/accuracy=0.520100, test/loss=2.154827, test/num_examples=10000, total_duration=71991.707623, train/accuracy=0.690684, train/loss=1.276928, validation/accuracy=0.640740, validation/loss=1.505728, validation/num_examples=50000
I0203 14:04:01.202410 139923852027648 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.64336359500885, loss=2.796537399291992
I0203 14:04:47.538731 139923868813056 logging_writer.py:48] [140500] global_step=140500, grad_norm=1.5997591018676758, loss=3.82765531539917
I0203 14:05:34.677813 139923852027648 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.8543082475662231, loss=2.20820689201355
I0203 14:06:21.833448 139923868813056 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.6939815282821655, loss=2.1828560829162598
I0203 14:07:08.791975 139923852027648 logging_writer.py:48] [140800] global_step=140800, grad_norm=1.8017113208770752, loss=2.221385955810547
I0203 14:07:55.771481 139923868813056 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.6729103326797485, loss=2.258854389190674
I0203 14:08:43.072780 139923852027648 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.7404298782348633, loss=2.6116161346435547
I0203 14:09:30.029845 139923868813056 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.6365286111831665, loss=4.61353063583374
I0203 14:10:17.246502 139923852027648 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.5356764793395996, loss=3.5683014392852783
I0203 14:10:38.444543 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:10:49.290124 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:11:21.880314 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:11:23.515935 140085747812160 submission_runner.py:408] Time since start: 72457.22s, 	Step: 141247, 	{'train/accuracy': 0.69837886095047, 'train/loss': 1.2515182495117188, 'validation/accuracy': 0.6464599967002869, 'validation/loss': 1.4908294677734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1413466930389404, 'test/num_examples': 10000, 'score': 65579.96733903885, 'total_duration': 72457.2199523449, 'accumulated_submission_time': 65579.96733903885, 'accumulated_eval_time': 6862.2890038490295, 'accumulated_logging_time': 7.054901123046875}
I0203 14:11:23.556038 139923868813056 logging_writer.py:48] [141247] accumulated_eval_time=6862.289004, accumulated_logging_time=7.054901, accumulated_submission_time=65579.967339, global_step=141247, preemption_count=0, score=65579.967339, test/accuracy=0.523400, test/loss=2.141347, test/num_examples=10000, total_duration=72457.219952, train/accuracy=0.698379, train/loss=1.251518, validation/accuracy=0.646460, validation/loss=1.490829, validation/num_examples=50000
I0203 14:11:44.752467 139923852027648 logging_writer.py:48] [141300] global_step=141300, grad_norm=1.5743361711502075, loss=4.386912822723389
I0203 14:12:31.067214 139923868813056 logging_writer.py:48] [141400] global_step=141400, grad_norm=1.7123545408248901, loss=2.981485366821289
I0203 14:13:18.052108 139923852027648 logging_writer.py:48] [141500] global_step=141500, grad_norm=1.610379934310913, loss=2.5933778285980225
I0203 14:14:05.298855 139923868813056 logging_writer.py:48] [141600] global_step=141600, grad_norm=1.6122530698776245, loss=4.21672248840332
I0203 14:14:52.348508 139923852027648 logging_writer.py:48] [141700] global_step=141700, grad_norm=1.5538338422775269, loss=4.319579601287842
I0203 14:15:39.426800 139923868813056 logging_writer.py:48] [141800] global_step=141800, grad_norm=1.9320731163024902, loss=2.154416084289551
I0203 14:16:26.411950 139923852027648 logging_writer.py:48] [141900] global_step=141900, grad_norm=1.5895824432373047, loss=4.900028705596924
I0203 14:17:13.690456 139923868813056 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.051281452178955, loss=2.378154754638672
I0203 14:18:00.680637 139923852027648 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.6542394161224365, loss=3.260213851928711
I0203 14:18:23.854623 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:18:34.352370 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:19:07.308112 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:19:08.945044 140085747812160 submission_runner.py:408] Time since start: 72922.65s, 	Step: 142151, 	{'train/accuracy': 0.7085741758346558, 'train/loss': 1.1992639303207397, 'validation/accuracy': 0.6479600071907043, 'validation/loss': 1.4763096570968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.128354549407959, 'test/num_examples': 10000, 'score': 66000.20277547836, 'total_duration': 72922.64905381203, 'accumulated_submission_time': 66000.20277547836, 'accumulated_eval_time': 6907.3794157505035, 'accumulated_logging_time': 7.107923269271851}
I0203 14:19:08.984697 139923868813056 logging_writer.py:48] [142151] accumulated_eval_time=6907.379416, accumulated_logging_time=7.107923, accumulated_submission_time=66000.202775, global_step=142151, preemption_count=0, score=66000.202775, test/accuracy=0.521000, test/loss=2.128355, test/num_examples=10000, total_duration=72922.649054, train/accuracy=0.708574, train/loss=1.199264, validation/accuracy=0.647960, validation/loss=1.476310, validation/num_examples=50000
I0203 14:19:28.625565 139923852027648 logging_writer.py:48] [142200] global_step=142200, grad_norm=1.8508206605911255, loss=2.2674560546875
I0203 14:20:14.045414 139923868813056 logging_writer.py:48] [142300] global_step=142300, grad_norm=1.5889962911605835, loss=3.5520739555358887
I0203 14:21:01.345430 139923852027648 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.695192813873291, loss=2.3404488563537598
I0203 14:21:48.564949 139923868813056 logging_writer.py:48] [142500] global_step=142500, grad_norm=1.8367056846618652, loss=2.058549165725708
I0203 14:22:35.617400 139923852027648 logging_writer.py:48] [142600] global_step=142600, grad_norm=1.7894885540008545, loss=2.1742897033691406
I0203 14:23:22.681291 139923868813056 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.7260022163391113, loss=2.6446337699890137
I0203 14:24:09.633110 139923852027648 logging_writer.py:48] [142800] global_step=142800, grad_norm=1.5979700088500977, loss=3.147362470626831
I0203 14:24:56.719364 139923868813056 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.8307898044586182, loss=2.2327709197998047
I0203 14:25:43.836413 139923852027648 logging_writer.py:48] [143000] global_step=143000, grad_norm=1.7663525342941284, loss=4.480695724487305
I0203 14:26:08.977177 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:26:19.560545 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:26:55.403217 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:26:57.049163 140085747812160 submission_runner.py:408] Time since start: 73390.75s, 	Step: 143055, 	{'train/accuracy': 0.7023242115974426, 'train/loss': 1.2293777465820312, 'validation/accuracy': 0.6479600071907043, 'validation/loss': 1.4754472970962524, 'validation/num_examples': 50000, 'test/accuracy': 0.5277000069618225, 'test/loss': 2.1308555603027344, 'test/num_examples': 10000, 'score': 66420.13485479355, 'total_duration': 73390.75317406654, 'accumulated_submission_time': 66420.13485479355, 'accumulated_eval_time': 6955.451377868652, 'accumulated_logging_time': 7.158038854598999}
I0203 14:26:57.088582 139923868813056 logging_writer.py:48] [143055] accumulated_eval_time=6955.451378, accumulated_logging_time=7.158039, accumulated_submission_time=66420.134855, global_step=143055, preemption_count=0, score=66420.134855, test/accuracy=0.527700, test/loss=2.130856, test/num_examples=10000, total_duration=73390.753174, train/accuracy=0.702324, train/loss=1.229378, validation/accuracy=0.647960, validation/loss=1.475447, validation/num_examples=50000
I0203 14:27:15.142703 139923852027648 logging_writer.py:48] [143100] global_step=143100, grad_norm=1.7793439626693726, loss=2.163889169692993
I0203 14:28:00.385403 139923868813056 logging_writer.py:48] [143200] global_step=143200, grad_norm=1.9201916456222534, loss=2.247300863265991
I0203 14:28:47.545638 139923852027648 logging_writer.py:48] [143300] global_step=143300, grad_norm=1.9373968839645386, loss=4.767539024353027
I0203 14:29:34.456491 139923868813056 logging_writer.py:48] [143400] global_step=143400, grad_norm=1.8254433870315552, loss=2.334348678588867
I0203 14:30:21.340341 139923852027648 logging_writer.py:48] [143500] global_step=143500, grad_norm=1.9260131120681763, loss=2.33331298828125
I0203 14:31:08.346236 139923868813056 logging_writer.py:48] [143600] global_step=143600, grad_norm=1.6254304647445679, loss=2.7591707706451416
I0203 14:31:55.201880 139923852027648 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.5389679670333862, loss=3.315337896347046
I0203 14:32:42.268240 139923868813056 logging_writer.py:48] [143800] global_step=143800, grad_norm=1.780592679977417, loss=2.240366220474243
I0203 14:33:29.334108 139923852027648 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.589460015296936, loss=3.391056776046753
I0203 14:33:57.077868 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:34:07.816996 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:34:40.987000 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:34:42.628499 140085747812160 submission_runner.py:408] Time since start: 73856.33s, 	Step: 143961, 	{'train/accuracy': 0.70570307970047, 'train/loss': 1.21626615524292, 'validation/accuracy': 0.6527599692344666, 'validation/loss': 1.4616146087646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.108567237854004, 'test/num_examples': 10000, 'score': 66840.06156110764, 'total_duration': 73856.33251214027, 'accumulated_submission_time': 66840.06156110764, 'accumulated_eval_time': 7001.00201010704, 'accumulated_logging_time': 7.209303379058838}
I0203 14:34:42.669183 139923868813056 logging_writer.py:48] [143961] accumulated_eval_time=7001.002010, accumulated_logging_time=7.209303, accumulated_submission_time=66840.061561, global_step=143961, preemption_count=0, score=66840.061561, test/accuracy=0.530100, test/loss=2.108567, test/num_examples=10000, total_duration=73856.332512, train/accuracy=0.705703, train/loss=1.216266, validation/accuracy=0.652760, validation/loss=1.461615, validation/num_examples=50000
I0203 14:34:58.361947 139923852027648 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.0984482765197754, loss=2.1778886318206787
I0203 14:35:42.918136 139923868813056 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.6029822826385498, loss=4.442468166351318
I0203 14:36:30.116995 139923852027648 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.0191733837127686, loss=2.2677338123321533
I0203 14:37:17.185957 139923868813056 logging_writer.py:48] [144300] global_step=144300, grad_norm=1.9894856214523315, loss=2.2088053226470947
I0203 14:38:04.470990 139923852027648 logging_writer.py:48] [144400] global_step=144400, grad_norm=1.889042615890503, loss=2.214991569519043
I0203 14:38:51.376848 139923868813056 logging_writer.py:48] [144500] global_step=144500, grad_norm=1.6850301027297974, loss=2.213127851486206
I0203 14:39:38.215824 139923852027648 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.192858934402466, loss=2.223043203353882
I0203 14:40:25.203468 139923868813056 logging_writer.py:48] [144700] global_step=144700, grad_norm=1.8415724039077759, loss=2.2294912338256836
I0203 14:41:12.131302 139923852027648 logging_writer.py:48] [144800] global_step=144800, grad_norm=1.7813063859939575, loss=2.6597204208374023
I0203 14:41:42.728506 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:41:53.548417 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:42:26.393971 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:42:28.034466 140085747812160 submission_runner.py:408] Time since start: 74321.74s, 	Step: 144867, 	{'train/accuracy': 0.7190039157867432, 'train/loss': 1.1549161672592163, 'validation/accuracy': 0.6559000015258789, 'validation/loss': 1.4345026016235352, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.0833964347839355, 'test/num_examples': 10000, 'score': 67260.06018710136, 'total_duration': 74321.73847436905, 'accumulated_submission_time': 67260.06018710136, 'accumulated_eval_time': 7046.307956695557, 'accumulated_logging_time': 7.260730981826782}
I0203 14:42:28.075252 139923868813056 logging_writer.py:48] [144867] accumulated_eval_time=7046.307957, accumulated_logging_time=7.260731, accumulated_submission_time=67260.060187, global_step=144867, preemption_count=0, score=67260.060187, test/accuracy=0.534500, test/loss=2.083396, test/num_examples=10000, total_duration=74321.738474, train/accuracy=0.719004, train/loss=1.154916, validation/accuracy=0.655900, validation/loss=1.434503, validation/num_examples=50000
I0203 14:42:41.410493 139923852027648 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.0027847290039062, loss=2.145395517349243
I0203 14:43:25.893100 139923868813056 logging_writer.py:48] [145000] global_step=145000, grad_norm=1.6523700952529907, loss=3.112819194793701
I0203 14:44:12.658499 139923852027648 logging_writer.py:48] [145100] global_step=145100, grad_norm=1.8784235715866089, loss=2.3322219848632812
I0203 14:44:59.715653 139923868813056 logging_writer.py:48] [145200] global_step=145200, grad_norm=1.9163670539855957, loss=2.2286880016326904
I0203 14:45:46.495513 139923852027648 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.8295502662658691, loss=2.072423219680786
I0203 14:46:33.678480 139923868813056 logging_writer.py:48] [145400] global_step=145400, grad_norm=1.8648958206176758, loss=2.1546528339385986
I0203 14:47:20.538308 139923852027648 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.7532579898834229, loss=2.1201107501983643
I0203 14:48:07.514959 139923868813056 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.000098705291748, loss=4.4164042472839355
I0203 14:48:54.524905 139923852027648 logging_writer.py:48] [145700] global_step=145700, grad_norm=1.9022927284240723, loss=1.9741129875183105
I0203 14:49:28.487223 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:49:39.049557 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:50:12.623389 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:50:14.273165 140085747812160 submission_runner.py:408] Time since start: 74787.98s, 	Step: 145774, 	{'train/accuracy': 0.7044140696525574, 'train/loss': 1.2267926931381226, 'validation/accuracy': 0.6544199585914612, 'validation/loss': 1.4555846452713013, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.109943389892578, 'test/num_examples': 10000, 'score': 67680.40997552872, 'total_duration': 74787.97717380524, 'accumulated_submission_time': 67680.40997552872, 'accumulated_eval_time': 7092.093881845474, 'accumulated_logging_time': 7.313398361206055}
I0203 14:50:14.312675 139923868813056 logging_writer.py:48] [145774] accumulated_eval_time=7092.093882, accumulated_logging_time=7.313398, accumulated_submission_time=67680.409976, global_step=145774, preemption_count=0, score=67680.409976, test/accuracy=0.531200, test/loss=2.109943, test/num_examples=10000, total_duration=74787.977174, train/accuracy=0.704414, train/loss=1.226793, validation/accuracy=0.654420, validation/loss=1.455585, validation/num_examples=50000
I0203 14:50:24.906415 139923852027648 logging_writer.py:48] [145800] global_step=145800, grad_norm=1.643825650215149, loss=3.460515022277832
I0203 14:51:08.722794 139923868813056 logging_writer.py:48] [145900] global_step=145900, grad_norm=1.5342791080474854, loss=3.958641290664673
I0203 14:51:55.687919 139923852027648 logging_writer.py:48] [146000] global_step=146000, grad_norm=1.8674182891845703, loss=2.320186138153076
I0203 14:52:42.852306 139923868813056 logging_writer.py:48] [146100] global_step=146100, grad_norm=1.9608418941497803, loss=2.1347451210021973
I0203 14:53:29.922167 139923852027648 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.2672407627105713, loss=2.1952695846557617
I0203 14:54:16.819274 139923868813056 logging_writer.py:48] [146300] global_step=146300, grad_norm=1.8667757511138916, loss=2.4374687671661377
I0203 14:55:03.867638 139923852027648 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.6703643798828125, loss=2.8971004486083984
I0203 14:55:50.667480 139923868813056 logging_writer.py:48] [146500] global_step=146500, grad_norm=1.8146599531173706, loss=4.749688625335693
I0203 14:56:37.575380 139923852027648 logging_writer.py:48] [146600] global_step=146600, grad_norm=1.8581571578979492, loss=2.177431344985962
I0203 14:57:14.309715 140085747812160 spec.py:321] Evaluating on the training split.
I0203 14:57:24.927875 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 14:58:01.414325 140085747812160 spec.py:349] Evaluating on the test split.
I0203 14:58:03.054397 140085747812160 submission_runner.py:408] Time since start: 75256.76s, 	Step: 146680, 	{'train/accuracy': 0.7185156345367432, 'train/loss': 1.158596158027649, 'validation/accuracy': 0.6607599854469299, 'validation/loss': 1.4224454164505005, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.0639452934265137, 'test/num_examples': 10000, 'score': 68100.34601187706, 'total_duration': 75256.75840878487, 'accumulated_submission_time': 68100.34601187706, 'accumulated_eval_time': 7140.838541984558, 'accumulated_logging_time': 7.363656282424927}
I0203 14:58:03.095038 139923868813056 logging_writer.py:48] [146680] accumulated_eval_time=7140.838542, accumulated_logging_time=7.363656, accumulated_submission_time=68100.346012, global_step=146680, preemption_count=0, score=68100.346012, test/accuracy=0.537100, test/loss=2.063945, test/num_examples=10000, total_duration=75256.758409, train/accuracy=0.718516, train/loss=1.158596, validation/accuracy=0.660760, validation/loss=1.422445, validation/num_examples=50000
I0203 14:58:11.340739 139923852027648 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.3555943965911865, loss=2.12968111038208
I0203 14:58:54.608945 139923868813056 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.0399553775787354, loss=4.7650861740112305
I0203 14:59:41.553753 139923852027648 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.056957960128784, loss=2.531934976577759
I0203 15:00:28.620157 139923868813056 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.012479305267334, loss=2.1197240352630615
I0203 15:01:15.644112 139923852027648 logging_writer.py:48] [147100] global_step=147100, grad_norm=1.8773527145385742, loss=2.2253100872039795
I0203 15:02:02.615399 139923868813056 logging_writer.py:48] [147200] global_step=147200, grad_norm=1.6699748039245605, loss=3.714916706085205
I0203 15:02:49.705964 139923852027648 logging_writer.py:48] [147300] global_step=147300, grad_norm=1.9994128942489624, loss=2.1220543384552
I0203 15:03:36.925311 139923868813056 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.7524325847625732, loss=3.47334623336792
I0203 15:04:23.896439 139923852027648 logging_writer.py:48] [147500] global_step=147500, grad_norm=1.8297518491744995, loss=2.9908511638641357
I0203 15:05:03.171543 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:05:13.821459 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:05:42.724465 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:05:44.360910 140085747812160 submission_runner.py:408] Time since start: 75718.06s, 	Step: 147585, 	{'train/accuracy': 0.7215625047683716, 'train/loss': 1.1436489820480347, 'validation/accuracy': 0.6613399982452393, 'validation/loss': 1.4184284210205078, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0514132976531982, 'test/num_examples': 10000, 'score': 68520.35908174515, 'total_duration': 75718.0649137497, 'accumulated_submission_time': 68520.35908174515, 'accumulated_eval_time': 7182.027892827988, 'accumulated_logging_time': 7.416749477386475}
I0203 15:05:44.402081 139923868813056 logging_writer.py:48] [147585] accumulated_eval_time=7182.027893, accumulated_logging_time=7.416749, accumulated_submission_time=68520.359082, global_step=147585, preemption_count=0, score=68520.359082, test/accuracy=0.541700, test/loss=2.051413, test/num_examples=10000, total_duration=75718.064914, train/accuracy=0.721563, train/loss=1.143649, validation/accuracy=0.661340, validation/loss=1.418428, validation/num_examples=50000
I0203 15:05:50.692696 139923852027648 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.0714638233184814, loss=2.1391658782958984
I0203 15:06:33.621077 139923868813056 logging_writer.py:48] [147700] global_step=147700, grad_norm=1.8533729314804077, loss=2.4457285404205322
I0203 15:07:20.083955 139923852027648 logging_writer.py:48] [147800] global_step=147800, grad_norm=1.9769718647003174, loss=2.127115488052368
I0203 15:08:07.054456 139923868813056 logging_writer.py:48] [147900] global_step=147900, grad_norm=1.968821406364441, loss=2.0709850788116455
I0203 15:08:53.785722 139923852027648 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.01094651222229, loss=2.1882801055908203
I0203 15:09:40.743677 139923868813056 logging_writer.py:48] [148100] global_step=148100, grad_norm=1.8502110242843628, loss=2.4095029830932617
I0203 15:10:27.659805 139923852027648 logging_writer.py:48] [148200] global_step=148200, grad_norm=1.8686397075653076, loss=2.2102222442626953
I0203 15:11:14.583513 139923868813056 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.0105063915252686, loss=2.4508509635925293
I0203 15:12:01.483527 139923852027648 logging_writer.py:48] [148400] global_step=148400, grad_norm=1.8251876831054688, loss=3.4424965381622314
I0203 15:12:44.524714 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:12:55.174856 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:13:28.021491 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:13:29.655661 140085747812160 submission_runner.py:408] Time since start: 76183.36s, 	Step: 148493, 	{'train/accuracy': 0.7193359136581421, 'train/loss': 1.1469899415969849, 'validation/accuracy': 0.667140007019043, 'validation/loss': 1.39079749584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5446000099182129, 'test/loss': 2.0252130031585693, 'test/num_examples': 10000, 'score': 68940.4190402031, 'total_duration': 76183.35967731476, 'accumulated_submission_time': 68940.4190402031, 'accumulated_eval_time': 7227.158820390701, 'accumulated_logging_time': 7.469902276992798}
I0203 15:13:29.697402 139923868813056 logging_writer.py:48] [148493] accumulated_eval_time=7227.158820, accumulated_logging_time=7.469902, accumulated_submission_time=68940.419040, global_step=148493, preemption_count=0, score=68940.419040, test/accuracy=0.544600, test/loss=2.025213, test/num_examples=10000, total_duration=76183.359677, train/accuracy=0.719336, train/loss=1.146990, validation/accuracy=0.667140, validation/loss=1.390797, validation/num_examples=50000
I0203 15:13:32.840936 139923852027648 logging_writer.py:48] [148500] global_step=148500, grad_norm=1.8340827226638794, loss=4.025883197784424
I0203 15:14:15.512859 139923868813056 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.0090560913085938, loss=2.1074821949005127
I0203 15:15:02.281075 139923852027648 logging_writer.py:48] [148700] global_step=148700, grad_norm=1.9866968393325806, loss=1.9598007202148438
I0203 15:15:49.396508 139923868813056 logging_writer.py:48] [148800] global_step=148800, grad_norm=1.9638923406600952, loss=4.51984167098999
I0203 15:16:36.430304 139923852027648 logging_writer.py:48] [148900] global_step=148900, grad_norm=1.8755533695220947, loss=2.2569658756256104
I0203 15:17:23.170356 139923868813056 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.226325273513794, loss=2.014620780944824
I0203 15:18:10.087677 139923852027648 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.24247407913208, loss=2.062413215637207
I0203 15:18:57.030389 139923868813056 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.141659736633301, loss=2.045095205307007
I0203 15:19:43.870702 139923852027648 logging_writer.py:48] [149300] global_step=149300, grad_norm=1.8916993141174316, loss=2.6169252395629883
I0203 15:20:30.015580 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:20:40.542790 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:21:14.025813 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:21:15.670574 140085747812160 submission_runner.py:408] Time since start: 76649.37s, 	Step: 149400, 	{'train/accuracy': 0.7261328101158142, 'train/loss': 1.1292051076889038, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3793786764144897, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.033937692642212, 'test/num_examples': 10000, 'score': 69360.67560601234, 'total_duration': 76649.3745880127, 'accumulated_submission_time': 69360.67560601234, 'accumulated_eval_time': 7272.813814640045, 'accumulated_logging_time': 7.522792100906372}
I0203 15:21:15.715819 139923868813056 logging_writer.py:48] [149400] accumulated_eval_time=7272.813815, accumulated_logging_time=7.522792, accumulated_submission_time=69360.675606, global_step=149400, preemption_count=0, score=69360.675606, test/accuracy=0.542900, test/loss=2.033938, test/num_examples=10000, total_duration=76649.374588, train/accuracy=0.726133, train/loss=1.129205, validation/accuracy=0.667720, validation/loss=1.379379, validation/num_examples=50000
I0203 15:21:16.114256 139923852027648 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.0343594551086426, loss=4.5558977127075195
I0203 15:21:57.821525 139923868813056 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.083130121231079, loss=1.9708508253097534
I0203 15:22:44.829161 139923852027648 logging_writer.py:48] [149600] global_step=149600, grad_norm=1.8359096050262451, loss=2.9015278816223145
I0203 15:23:32.069800 139923868813056 logging_writer.py:48] [149700] global_step=149700, grad_norm=1.8524372577667236, loss=3.3014824390411377
I0203 15:24:19.019941 139923852027648 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.1033217906951904, loss=2.1901659965515137
I0203 15:25:06.785803 139923868813056 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.0839147567749023, loss=2.134265422821045
I0203 15:25:53.286843 139923852027648 logging_writer.py:48] [150000] global_step=150000, grad_norm=1.9911918640136719, loss=2.1641433238983154
I0203 15:26:40.358269 139923868813056 logging_writer.py:48] [150100] global_step=150100, grad_norm=1.838916540145874, loss=3.269829511642456
I0203 15:27:27.193572 139923852027648 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.0457518100738525, loss=2.084965229034424
I0203 15:28:13.942877 139923868813056 logging_writer.py:48] [150300] global_step=150300, grad_norm=1.8989770412445068, loss=4.151627063751221
I0203 15:28:16.088360 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:28:26.644760 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:28:55.762906 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:28:57.400280 140085747812160 submission_runner.py:408] Time since start: 77111.10s, 	Step: 150306, 	{'train/accuracy': 0.7345898151397705, 'train/loss': 1.0911519527435303, 'validation/accuracy': 0.6721599698066711, 'validation/loss': 1.3719079494476318, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0280535221099854, 'test/num_examples': 10000, 'score': 69780.98648524284, 'total_duration': 77111.10426402092, 'accumulated_submission_time': 69780.98648524284, 'accumulated_eval_time': 7314.125680685043, 'accumulated_logging_time': 7.57967209815979}
I0203 15:28:57.441135 139923852027648 logging_writer.py:48] [150306] accumulated_eval_time=7314.125681, accumulated_logging_time=7.579672, accumulated_submission_time=69780.986485, global_step=150306, preemption_count=0, score=69780.986485, test/accuracy=0.543200, test/loss=2.028054, test/num_examples=10000, total_duration=77111.104264, train/accuracy=0.734590, train/loss=1.091152, validation/accuracy=0.672160, validation/loss=1.371908, validation/num_examples=50000
I0203 15:29:36.791132 139923868813056 logging_writer.py:48] [150400] global_step=150400, grad_norm=1.866049885749817, loss=3.5191662311553955
I0203 15:30:23.852560 139923852027648 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.157120704650879, loss=2.062398910522461
I0203 15:31:11.060391 139923868813056 logging_writer.py:48] [150600] global_step=150600, grad_norm=1.8985674381256104, loss=2.149735689163208
I0203 15:31:57.946047 139923852027648 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.3095011711120605, loss=4.376264572143555
I0203 15:32:45.281591 139923868813056 logging_writer.py:48] [150800] global_step=150800, grad_norm=1.842516303062439, loss=2.905662775039673
I0203 15:33:32.318865 139923852027648 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.1052939891815186, loss=2.0240392684936523
I0203 15:34:19.449206 139923868813056 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.0570068359375, loss=1.8995944261550903
I0203 15:35:06.586122 139923852027648 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.0053319931030273, loss=2.125307083129883
I0203 15:35:53.616175 139923868813056 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.3278563022613525, loss=2.042922258377075
I0203 15:35:57.606649 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:36:08.149022 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:36:42.252633 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:36:43.893313 140085747812160 submission_runner.py:408] Time since start: 77577.60s, 	Step: 151210, 	{'train/accuracy': 0.7315624952316284, 'train/loss': 1.0995270013809204, 'validation/accuracy': 0.6723600029945374, 'validation/loss': 1.3750596046447754, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.0155558586120605, 'test/num_examples': 10000, 'score': 70201.0912911892, 'total_duration': 77577.59732437134, 'accumulated_submission_time': 70201.0912911892, 'accumulated_eval_time': 7360.41232419014, 'accumulated_logging_time': 7.6300084590911865}
I0203 15:36:43.934708 139923852027648 logging_writer.py:48] [151210] accumulated_eval_time=7360.412324, accumulated_logging_time=7.630008, accumulated_submission_time=70201.091291, global_step=151210, preemption_count=0, score=70201.091291, test/accuracy=0.547900, test/loss=2.015556, test/num_examples=10000, total_duration=77577.597324, train/accuracy=0.731562, train/loss=1.099527, validation/accuracy=0.672360, validation/loss=1.375060, validation/num_examples=50000
I0203 15:37:21.673263 139923868813056 logging_writer.py:48] [151300] global_step=151300, grad_norm=1.9472932815551758, loss=4.261951923370361
I0203 15:38:08.345369 139923852027648 logging_writer.py:48] [151400] global_step=151400, grad_norm=1.9322391748428345, loss=3.8089425563812256
I0203 15:38:55.436272 139923868813056 logging_writer.py:48] [151500] global_step=151500, grad_norm=1.9815152883529663, loss=3.0557382106781006
I0203 15:39:42.225748 139923852027648 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.1451051235198975, loss=2.1062734127044678
I0203 15:40:29.224793 139923868813056 logging_writer.py:48] [151700] global_step=151700, grad_norm=1.993378758430481, loss=2.023002862930298
I0203 15:41:16.041753 139923852027648 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.098982334136963, loss=2.599052906036377
I0203 15:42:03.118353 139923868813056 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.1481776237487793, loss=1.9661561250686646
I0203 15:42:50.189779 139923852027648 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.129817008972168, loss=2.030174493789673
I0203 15:43:37.143090 139923868813056 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.112708806991577, loss=2.090264320373535
I0203 15:43:43.904748 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:43:54.548938 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:44:27.828264 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:44:29.467163 140085747812160 submission_runner.py:408] Time since start: 78043.17s, 	Step: 152116, 	{'train/accuracy': 0.7346875071525574, 'train/loss': 1.0947026014328003, 'validation/accuracy': 0.6771000027656555, 'validation/loss': 1.352385401725769, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 1.9908647537231445, 'test/num_examples': 10000, 'score': 70620.99999403954, 'total_duration': 78043.17117524147, 'accumulated_submission_time': 70620.99999403954, 'accumulated_eval_time': 7405.974725008011, 'accumulated_logging_time': 7.682317733764648}
I0203 15:44:29.506731 139923852027648 logging_writer.py:48] [152116] accumulated_eval_time=7405.974725, accumulated_logging_time=7.682318, accumulated_submission_time=70620.999994, global_step=152116, preemption_count=0, score=70620.999994, test/accuracy=0.556100, test/loss=1.990865, test/num_examples=10000, total_duration=78043.171175, train/accuracy=0.734688, train/loss=1.094703, validation/accuracy=0.677100, validation/loss=1.352385, validation/num_examples=50000
I0203 15:45:04.409825 139923868813056 logging_writer.py:48] [152200] global_step=152200, grad_norm=1.943927526473999, loss=1.9969985485076904
I0203 15:45:51.036237 139923852027648 logging_writer.py:48] [152300] global_step=152300, grad_norm=1.8502551317214966, loss=3.510056972503662
I0203 15:46:38.310793 139923868813056 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.1277599334716797, loss=2.155409812927246
I0203 15:47:25.387260 139923852027648 logging_writer.py:48] [152500] global_step=152500, grad_norm=1.970207691192627, loss=2.2882089614868164
I0203 15:48:12.398331 139923868813056 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.0116946697235107, loss=2.3958892822265625
I0203 15:48:59.630328 139923852027648 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.1144626140594482, loss=1.953721046447754
I0203 15:49:46.994644 139923868813056 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.320131778717041, loss=2.1414413452148438
I0203 15:50:34.157963 139923852027648 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.1290385723114014, loss=4.174520492553711
I0203 15:51:21.296192 139923868813056 logging_writer.py:48] [153000] global_step=153000, grad_norm=1.9458329677581787, loss=3.280094861984253
I0203 15:51:29.860556 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:51:41.513733 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:52:09.911197 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:52:11.553438 140085747812160 submission_runner.py:408] Time since start: 78505.26s, 	Step: 153020, 	{'train/accuracy': 0.7451757788658142, 'train/loss': 1.0271271467208862, 'validation/accuracy': 0.6819199919700623, 'validation/loss': 1.3150537014007568, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 1.9496073722839355, 'test/num_examples': 10000, 'score': 71041.29298949242, 'total_duration': 78505.25744843483, 'accumulated_submission_time': 71041.29298949242, 'accumulated_eval_time': 7447.667580366135, 'accumulated_logging_time': 7.731944561004639}
I0203 15:52:11.592470 139923852027648 logging_writer.py:48] [153020] accumulated_eval_time=7447.667580, accumulated_logging_time=7.731945, accumulated_submission_time=71041.292989, global_step=153020, preemption_count=0, score=71041.292989, test/accuracy=0.558600, test/loss=1.949607, test/num_examples=10000, total_duration=78505.257448, train/accuracy=0.745176, train/loss=1.027127, validation/accuracy=0.681920, validation/loss=1.315054, validation/num_examples=50000
I0203 15:52:44.649019 139923868813056 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.1892917156219482, loss=3.9009628295898438
I0203 15:53:31.731330 139923852027648 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.1370952129364014, loss=2.161597728729248
I0203 15:54:19.129331 139923868813056 logging_writer.py:48] [153300] global_step=153300, grad_norm=1.8926503658294678, loss=2.507941722869873
I0203 15:55:06.212365 139923852027648 logging_writer.py:48] [153400] global_step=153400, grad_norm=1.9909366369247437, loss=1.9191341400146484
I0203 15:55:53.218994 139923868813056 logging_writer.py:48] [153500] global_step=153500, grad_norm=1.7971338033676147, loss=3.02666974067688
I0203 15:56:40.105026 139923852027648 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.1906888484954834, loss=2.03068470954895
I0203 15:57:27.720369 139923868813056 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.367372512817383, loss=2.078390121459961
I0203 15:58:15.009262 139923852027648 logging_writer.py:48] [153800] global_step=153800, grad_norm=1.9565359354019165, loss=2.953044891357422
I0203 15:59:02.405933 139923868813056 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.2288761138916016, loss=1.9575003385543823
I0203 15:59:11.949489 140085747812160 spec.py:321] Evaluating on the training split.
I0203 15:59:22.225998 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 15:59:52.866605 140085747812160 spec.py:349] Evaluating on the test split.
I0203 15:59:54.504068 140085747812160 submission_runner.py:408] Time since start: 78968.21s, 	Step: 153922, 	{'train/accuracy': 0.758496105670929, 'train/loss': 0.9787788391113281, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.3084824085235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 1.9502512216567993, 'test/num_examples': 10000, 'score': 71461.58991360664, 'total_duration': 78968.2080783844, 'accumulated_submission_time': 71461.58991360664, 'accumulated_eval_time': 7490.222132205963, 'accumulated_logging_time': 7.7806432247161865}
I0203 15:59:54.543985 139923852027648 logging_writer.py:48] [153922] accumulated_eval_time=7490.222132, accumulated_logging_time=7.780643, accumulated_submission_time=71461.589914, global_step=153922, preemption_count=0, score=71461.589914, test/accuracy=0.554300, test/loss=1.950251, test/num_examples=10000, total_duration=78968.208078, train/accuracy=0.758496, train/loss=0.978779, validation/accuracy=0.684560, validation/loss=1.308482, validation/num_examples=50000
I0203 16:00:26.485261 139923868813056 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.262061834335327, loss=1.9675710201263428
I0203 16:01:13.225141 139923852027648 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.4488608837127686, loss=4.516363620758057
I0203 16:02:00.340379 139923868813056 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.3470020294189453, loss=2.1703875064849854
I0203 16:02:47.335700 139923852027648 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.12271785736084, loss=2.2583441734313965
I0203 16:03:34.196669 139923868813056 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.2198376655578613, loss=1.8997063636779785
I0203 16:04:21.059041 139923852027648 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.193446397781372, loss=2.7132439613342285
I0203 16:05:07.823487 139923868813056 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.3278889656066895, loss=1.9755960702896118
I0203 16:05:54.764125 139923852027648 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.3691775798797607, loss=2.032339096069336
I0203 16:06:41.826922 139923868813056 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.045309543609619, loss=2.1701176166534424
I0203 16:06:54.687439 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:07:05.639970 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:07:40.480682 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:07:42.119394 140085747812160 submission_runner.py:408] Time since start: 79435.82s, 	Step: 154829, 	{'train/accuracy': 0.7406249642372131, 'train/loss': 1.0503544807434082, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.2965481281280518, 'validation/num_examples': 50000, 'test/accuracy': 0.5617000460624695, 'test/loss': 1.9382882118225098, 'test/num_examples': 10000, 'score': 71881.67134642601, 'total_duration': 79435.82341265678, 'accumulated_submission_time': 71881.67134642601, 'accumulated_eval_time': 7537.654074668884, 'accumulated_logging_time': 7.830984830856323}
I0203 16:07:42.162029 139923852027648 logging_writer.py:48] [154829] accumulated_eval_time=7537.654075, accumulated_logging_time=7.830985, accumulated_submission_time=71881.671346, global_step=154829, preemption_count=0, score=71881.671346, test/accuracy=0.561700, test/loss=1.938288, test/num_examples=10000, total_duration=79435.823413, train/accuracy=0.740625, train/loss=1.050354, validation/accuracy=0.687380, validation/loss=1.296548, validation/num_examples=50000
I0203 16:08:10.683017 139923868813056 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.199780225753784, loss=2.0314199924468994
I0203 16:08:57.377213 139923852027648 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.0279879570007324, loss=2.531181812286377
I0203 16:09:44.580324 139923868813056 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.2632203102111816, loss=2.0827629566192627
I0203 16:10:31.475657 139923852027648 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.3080520629882812, loss=1.986965298652649
I0203 16:11:18.636319 139923868813056 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.0642201900482178, loss=2.4782142639160156
I0203 16:12:05.448404 139923852027648 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.3414368629455566, loss=2.0085792541503906
I0203 16:12:52.408641 139923868813056 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.5021753311157227, loss=4.511270046234131
I0203 16:13:39.403367 139923852027648 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.132657051086426, loss=1.9973275661468506
I0203 16:14:26.392615 139923868813056 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.0843732357025146, loss=2.6983277797698975
I0203 16:14:42.505807 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:14:52.953300 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:15:25.780825 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:15:27.420225 140085747812160 submission_runner.py:408] Time since start: 79901.12s, 	Step: 155736, 	{'train/accuracy': 0.7502343654632568, 'train/loss': 1.0065470933914185, 'validation/accuracy': 0.687559962272644, 'validation/loss': 1.2907850742340088, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.922777771949768, 'test/num_examples': 10000, 'score': 72301.95320534706, 'total_duration': 79901.12423586845, 'accumulated_submission_time': 72301.95320534706, 'accumulated_eval_time': 7582.568460941315, 'accumulated_logging_time': 7.88397216796875}
I0203 16:15:27.471703 139923852027648 logging_writer.py:48] [155736] accumulated_eval_time=7582.568461, accumulated_logging_time=7.883972, accumulated_submission_time=72301.953205, global_step=155736, preemption_count=0, score=72301.953205, test/accuracy=0.561000, test/loss=1.922778, test/num_examples=10000, total_duration=79901.124236, train/accuracy=0.750234, train/loss=1.006547, validation/accuracy=0.687560, validation/loss=1.290785, validation/num_examples=50000
I0203 16:15:53.086398 139923868813056 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.381948471069336, loss=1.972349762916565
I0203 16:16:39.705860 139923852027648 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.3863608837127686, loss=2.395927906036377
I0203 16:17:26.880745 139923868813056 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.4170925617218018, loss=2.1124701499938965
I0203 16:18:13.670048 139923852027648 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.2755377292633057, loss=2.145179271697998
I0203 16:19:00.660531 139923868813056 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.489264965057373, loss=1.9801602363586426
I0203 16:19:47.678923 139923852027648 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.1744396686553955, loss=3.7199254035949707
I0203 16:20:34.815199 139923868813056 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.6275651454925537, loss=4.5000081062316895
I0203 16:21:21.689425 139923852027648 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.3697144985198975, loss=1.98062002658844
I0203 16:22:08.821985 139923868813056 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.409379243850708, loss=2.096640110015869
I0203 16:22:27.499408 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:22:38.226496 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:23:10.754974 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:23:12.408675 140085747812160 submission_runner.py:408] Time since start: 80366.11s, 	Step: 156641, 	{'train/accuracy': 0.76318359375, 'train/loss': 0.952807605266571, 'validation/accuracy': 0.6926800012588501, 'validation/loss': 1.2713115215301514, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 1.9105957746505737, 'test/num_examples': 10000, 'score': 72721.92038750648, 'total_duration': 80366.11267876625, 'accumulated_submission_time': 72721.92038750648, 'accumulated_eval_time': 7627.477694272995, 'accumulated_logging_time': 7.9458537101745605}
I0203 16:23:12.452424 139923852027648 logging_writer.py:48] [156641] accumulated_eval_time=7627.477694, accumulated_logging_time=7.945854, accumulated_submission_time=72721.920388, global_step=156641, preemption_count=0, score=72721.920388, test/accuracy=0.564800, test/loss=1.910596, test/num_examples=10000, total_duration=80366.112679, train/accuracy=0.763184, train/loss=0.952808, validation/accuracy=0.692680, validation/loss=1.271312, validation/num_examples=50000
I0203 16:23:35.993404 139923868813056 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.392155885696411, loss=1.933695673942566
I0203 16:24:22.471050 139923852027648 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.205691337585449, loss=1.8857009410858154
I0203 16:25:09.697563 139923868813056 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.256242513656616, loss=2.058995485305786
I0203 16:25:56.515599 139923852027648 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.547252655029297, loss=1.8277020454406738
I0203 16:26:43.415779 139923868813056 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.460763454437256, loss=1.879995346069336
I0203 16:27:30.152707 139923852027648 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.038752317428589, loss=3.343707323074341
I0203 16:28:17.172732 139923868813056 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.3922905921936035, loss=1.9671604633331299
I0203 16:29:04.581945 139923852027648 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.3033690452575684, loss=1.8711282014846802
I0203 16:29:51.223805 139923868813056 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.444636106491089, loss=1.8536580801010132
I0203 16:30:12.555014 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:30:22.978027 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:30:56.742822 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:30:58.388058 140085747812160 submission_runner.py:408] Time since start: 80832.09s, 	Step: 157547, 	{'train/accuracy': 0.7520703077316284, 'train/loss': 1.0017552375793457, 'validation/accuracy': 0.6958400011062622, 'validation/loss': 1.251082420349121, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.8889102935791016, 'test/num_examples': 10000, 'score': 73141.96051359177, 'total_duration': 80832.09207010269, 'accumulated_submission_time': 73141.96051359177, 'accumulated_eval_time': 7673.310721158981, 'accumulated_logging_time': 8.001603126525879}
I0203 16:30:58.432062 139923852027648 logging_writer.py:48] [157547] accumulated_eval_time=7673.310721, accumulated_logging_time=8.001603, accumulated_submission_time=73141.960514, global_step=157547, preemption_count=0, score=73141.960514, test/accuracy=0.572700, test/loss=1.888910, test/num_examples=10000, total_duration=80832.092070, train/accuracy=0.752070, train/loss=1.001755, validation/accuracy=0.695840, validation/loss=1.251082, validation/num_examples=50000
I0203 16:31:19.600119 139923868813056 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.3630847930908203, loss=1.8836860656738281
I0203 16:32:05.638669 139923852027648 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.509511709213257, loss=1.8894490003585815
I0203 16:32:52.864970 139923868813056 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.2158617973327637, loss=2.1624791622161865
I0203 16:33:40.120883 139923852027648 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.5911481380462646, loss=1.9570958614349365
I0203 16:34:27.089077 139923868813056 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.5646719932556152, loss=1.8890162706375122
I0203 16:35:14.168122 139923852027648 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.403470516204834, loss=1.8431847095489502
I0203 16:36:01.088194 139923868813056 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.468324661254883, loss=3.440126657485962
I0203 16:36:48.144864 139923852027648 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.569822311401367, loss=2.056002616882324
I0203 16:37:35.229537 139923868813056 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.509596347808838, loss=2.0217185020446777
I0203 16:37:58.821432 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:38:09.636387 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:38:37.642301 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:38:39.284141 140085747812160 submission_runner.py:408] Time since start: 81292.99s, 	Step: 158452, 	{'train/accuracy': 0.7565234303474426, 'train/loss': 0.9877767562866211, 'validation/accuracy': 0.6963799595832825, 'validation/loss': 1.252785563468933, 'validation/num_examples': 50000, 'test/accuracy': 0.5720000267028809, 'test/loss': 1.8924669027328491, 'test/num_examples': 10000, 'score': 73562.28885316849, 'total_duration': 81292.98815059662, 'accumulated_submission_time': 73562.28885316849, 'accumulated_eval_time': 7713.773415803909, 'accumulated_logging_time': 8.056279420852661}
I0203 16:38:39.329514 139923852027648 logging_writer.py:48] [158452] accumulated_eval_time=7713.773416, accumulated_logging_time=8.056279, accumulated_submission_time=73562.288853, global_step=158452, preemption_count=0, score=73562.288853, test/accuracy=0.572000, test/loss=1.892467, test/num_examples=10000, total_duration=81292.988151, train/accuracy=0.756523, train/loss=0.987777, validation/accuracy=0.696380, validation/loss=1.252786, validation/num_examples=50000
I0203 16:38:58.560607 139923868813056 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.3484439849853516, loss=2.1844139099121094
I0203 16:39:44.019265 139923852027648 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.6930577754974365, loss=4.113833904266357
I0203 16:40:31.329077 139923868813056 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.1753294467926025, loss=3.123640298843384
I0203 16:41:18.157038 139923852027648 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.4824042320251465, loss=3.985295534133911
I0203 16:42:05.384325 139923868813056 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.2219128608703613, loss=2.7826154232025146
I0203 16:42:52.234345 139923852027648 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.429480791091919, loss=1.8900279998779297
I0203 16:43:38.896677 139923868813056 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.2852203845977783, loss=2.3657875061035156
I0203 16:44:25.801506 139923852027648 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.281913995742798, loss=2.848285675048828
I0203 16:45:12.661768 139923868813056 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.4461894035339355, loss=1.968843698501587
I0203 16:45:39.577836 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:45:50.271257 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:46:24.037082 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:46:25.676092 140085747812160 submission_runner.py:408] Time since start: 81759.38s, 	Step: 159359, 	{'train/accuracy': 0.7693945169448853, 'train/loss': 0.939318060874939, 'validation/accuracy': 0.6983799934387207, 'validation/loss': 1.2409757375717163, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.8835759162902832, 'test/num_examples': 10000, 'score': 73982.47636890411, 'total_duration': 81759.38010597229, 'accumulated_submission_time': 73982.47636890411, 'accumulated_eval_time': 7759.871674776077, 'accumulated_logging_time': 8.111968994140625}
I0203 16:46:25.724132 139923852027648 logging_writer.py:48] [159359] accumulated_eval_time=7759.871675, accumulated_logging_time=8.111969, accumulated_submission_time=73982.476369, global_step=159359, preemption_count=0, score=73982.476369, test/accuracy=0.573200, test/loss=1.883576, test/num_examples=10000, total_duration=81759.380106, train/accuracy=0.769395, train/loss=0.939318, validation/accuracy=0.698380, validation/loss=1.240976, validation/num_examples=50000
I0203 16:46:42.202561 139923868813056 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.4793262481689453, loss=1.991166353225708
I0203 16:47:26.821300 139923852027648 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.4460391998291016, loss=1.8063137531280518
I0203 16:48:14.002167 139923868813056 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.364839792251587, loss=3.4550535678863525
I0203 16:49:00.991369 139923852027648 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.3751704692840576, loss=3.510303258895874
I0203 16:49:47.885834 139923868813056 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.3182263374328613, loss=1.8278515338897705
I0203 16:50:34.869571 139923852027648 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.3400111198425293, loss=3.096794843673706
I0203 16:51:22.020402 139923868813056 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.5275988578796387, loss=3.993285894393921
I0203 16:52:09.223523 139923852027648 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.3086252212524414, loss=3.345327138900757
I0203 16:52:56.537259 139923868813056 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.5455336570739746, loss=3.5792949199676514
I0203 16:53:26.002713 140085747812160 spec.py:321] Evaluating on the training split.
I0203 16:53:36.572257 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 16:54:08.607023 140085747812160 spec.py:349] Evaluating on the test split.
I0203 16:54:10.246914 140085747812160 submission_runner.py:408] Time since start: 82223.95s, 	Step: 160264, 	{'train/accuracy': 0.7602343559265137, 'train/loss': 0.9711340665817261, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.2386878728866577, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 1.869041085243225, 'test/num_examples': 10000, 'score': 74402.69127678871, 'total_duration': 82223.95091509819, 'accumulated_submission_time': 74402.69127678871, 'accumulated_eval_time': 7804.115849733353, 'accumulated_logging_time': 8.173494815826416}
I0203 16:54:10.290500 139923852027648 logging_writer.py:48] [160264] accumulated_eval_time=7804.115850, accumulated_logging_time=8.173495, accumulated_submission_time=74402.691277, global_step=160264, preemption_count=0, score=74402.691277, test/accuracy=0.581400, test/loss=1.869041, test/num_examples=10000, total_duration=82223.950915, train/accuracy=0.760234, train/loss=0.971134, validation/accuracy=0.699440, validation/loss=1.238688, validation/num_examples=50000
I0203 16:54:24.802016 139923868813056 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.610318899154663, loss=4.213128566741943
I0203 16:55:09.066310 139923852027648 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.531081199645996, loss=1.9626035690307617
I0203 16:55:56.585646 139923868813056 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.277061700820923, loss=1.6572068929672241
I0203 16:56:43.799567 139923852027648 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.489326000213623, loss=4.022976398468018
I0203 16:57:31.019636 139923868813056 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.7650370597839355, loss=1.9413154125213623
I0203 16:58:18.129699 139923852027648 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.334155797958374, loss=3.6080427169799805
I0203 16:59:05.179069 139923868813056 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.4887313842773438, loss=2.1036458015441895
I0203 16:59:52.176281 139923852027648 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.576215982437134, loss=2.3074212074279785
I0203 17:00:39.187753 139923868813056 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.687671184539795, loss=1.764114499092102
I0203 17:01:10.615154 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:01:21.256466 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:01:56.787462 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:01:58.424240 140085747812160 submission_runner.py:408] Time since start: 82692.13s, 	Step: 161168, 	{'train/accuracy': 0.7683203220367432, 'train/loss': 0.9366870522499084, 'validation/accuracy': 0.7050999999046326, 'validation/loss': 1.2145330905914307, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.8563323020935059, 'test/num_examples': 10000, 'score': 74822.95345377922, 'total_duration': 82692.12824940681, 'accumulated_submission_time': 74822.95345377922, 'accumulated_eval_time': 7851.924923419952, 'accumulated_logging_time': 8.229990243911743}
I0203 17:01:58.467801 139923852027648 logging_writer.py:48] [161168] accumulated_eval_time=7851.924923, accumulated_logging_time=8.229990, accumulated_submission_time=74822.953454, global_step=161168, preemption_count=0, score=74822.953454, test/accuracy=0.578300, test/loss=1.856332, test/num_examples=10000, total_duration=82692.128249, train/accuracy=0.768320, train/loss=0.936687, validation/accuracy=0.705100, validation/loss=1.214533, validation/num_examples=50000
I0203 17:02:11.399778 139923868813056 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.9265167713165283, loss=1.889960527420044
I0203 17:02:55.842618 139923852027648 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.5520613193511963, loss=1.9182515144348145
I0203 17:03:43.148791 139923868813056 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.341963052749634, loss=2.466602087020874
I0203 17:04:30.472916 139923852027648 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.382629871368408, loss=3.1073102951049805
I0203 17:05:17.611131 139923868813056 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.6345608234405518, loss=1.8946917057037354
I0203 17:06:04.608965 139923852027648 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.733189582824707, loss=1.8711211681365967
I0203 17:06:51.818802 139923868813056 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.728975772857666, loss=1.7034802436828613
I0203 17:07:39.139123 139923852027648 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.5534355640411377, loss=4.02837610244751
I0203 17:08:26.091233 139923868813056 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.6663646697998047, loss=1.871216893196106
I0203 17:08:58.496597 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:09:09.161421 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:09:40.908474 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:09:42.543429 140085747812160 submission_runner.py:408] Time since start: 83156.25s, 	Step: 162070, 	{'train/accuracy': 0.77783203125, 'train/loss': 0.8856746554374695, 'validation/accuracy': 0.7069199681282043, 'validation/loss': 1.2027339935302734, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.8382530212402344, 'test/num_examples': 10000, 'score': 75242.92199492455, 'total_duration': 83156.2474398613, 'accumulated_submission_time': 75242.92199492455, 'accumulated_eval_time': 7895.971745014191, 'accumulated_logging_time': 8.284348249435425}
I0203 17:09:42.587078 139923852027648 logging_writer.py:48] [162070] accumulated_eval_time=7895.971745, accumulated_logging_time=8.284348, accumulated_submission_time=75242.921995, global_step=162070, preemption_count=0, score=75242.921995, test/accuracy=0.585100, test/loss=1.838253, test/num_examples=10000, total_duration=83156.247440, train/accuracy=0.777832, train/loss=0.885675, validation/accuracy=0.706920, validation/loss=1.202734, validation/num_examples=50000
I0203 17:09:54.743322 139923868813056 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.8882243633270264, loss=4.336593151092529
I0203 17:10:38.798087 139923852027648 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.67051100730896, loss=1.8819133043289185
I0203 17:11:25.779550 139923868813056 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.7303593158721924, loss=1.8065745830535889
I0203 17:12:12.761813 139923852027648 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.584967613220215, loss=1.7790021896362305
I0203 17:12:59.789109 139923868813056 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.5731585025787354, loss=1.8785359859466553
I0203 17:13:46.755094 139923852027648 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.7612085342407227, loss=1.8191982507705688
I0203 17:14:34.056496 139923868813056 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.540430784225464, loss=1.7514762878417969
I0203 17:15:21.132653 139923852027648 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.6765477657318115, loss=3.9716625213623047
I0203 17:16:08.119143 139923868813056 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.494663953781128, loss=3.427302837371826
I0203 17:16:42.940890 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:16:53.609732 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:17:22.330211 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:17:23.972259 140085747812160 submission_runner.py:408] Time since start: 83617.68s, 	Step: 162976, 	{'train/accuracy': 0.7721288800239563, 'train/loss': 0.9069911241531372, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1784546375274658, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.7933886051177979, 'test/num_examples': 10000, 'score': 75663.21282863617, 'total_duration': 83617.6762702465, 'accumulated_submission_time': 75663.21282863617, 'accumulated_eval_time': 7937.003118753433, 'accumulated_logging_time': 8.34020447731018}
I0203 17:17:24.018731 139923852027648 logging_writer.py:48] [162976] accumulated_eval_time=7937.003119, accumulated_logging_time=8.340204, accumulated_submission_time=75663.212829, global_step=162976, preemption_count=0, score=75663.212829, test/accuracy=0.586800, test/loss=1.793389, test/num_examples=10000, total_duration=83617.676270, train/accuracy=0.772129, train/loss=0.906991, validation/accuracy=0.711880, validation/loss=1.178455, validation/num_examples=50000
I0203 17:17:33.835315 139923868813056 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.870659828186035, loss=1.8088189363479614
I0203 17:18:17.592795 139923852027648 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.697803497314453, loss=2.7380118370056152
I0203 17:19:04.436445 139923868813056 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.4937245845794678, loss=1.7370195388793945
I0203 17:19:51.420513 139923852027648 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.7081964015960693, loss=2.003157138824463
I0203 17:20:38.469355 139923868813056 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.547630786895752, loss=1.8508634567260742
I0203 17:21:25.291196 139923852027648 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.592609167098999, loss=3.407747507095337
I0203 17:22:12.299673 139923868813056 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.920665979385376, loss=3.9102988243103027
I0203 17:22:59.472349 139923852027648 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.8483994007110596, loss=2.7827234268188477
I0203 17:23:46.339190 139923868813056 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.64754056930542, loss=1.8310842514038086
I0203 17:24:24.686904 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:24:35.061250 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:25:10.161882 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:25:11.808475 140085747812160 submission_runner.py:408] Time since start: 84085.51s, 	Step: 163883, 	{'train/accuracy': 0.7769335508346558, 'train/loss': 0.8996948599815369, 'validation/accuracy': 0.7105799913406372, 'validation/loss': 1.1845804452896118, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.8173900842666626, 'test/num_examples': 10000, 'score': 76083.81787419319, 'total_duration': 84085.51246571541, 'accumulated_submission_time': 76083.81787419319, 'accumulated_eval_time': 7984.124661684036, 'accumulated_logging_time': 8.398981094360352}
I0203 17:25:11.854647 139923852027648 logging_writer.py:48] [163883] accumulated_eval_time=7984.124662, accumulated_logging_time=8.398981, accumulated_submission_time=76083.817874, global_step=163883, preemption_count=0, score=76083.817874, test/accuracy=0.590000, test/loss=1.817390, test/num_examples=10000, total_duration=84085.512466, train/accuracy=0.776934, train/loss=0.899695, validation/accuracy=0.710580, validation/loss=1.184580, validation/num_examples=50000
I0203 17:25:18.917044 139923868813056 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.8552863597869873, loss=1.8406269550323486
I0203 17:26:02.217715 139923852027648 logging_writer.py:48] [164000] global_step=164000, grad_norm=3.0531599521636963, loss=3.352407455444336
I0203 17:26:48.817979 139923868813056 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.837933301925659, loss=4.3436808586120605
I0203 17:27:35.989999 139923852027648 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.7627804279327393, loss=4.215620994567871
I0203 17:28:22.792597 139923868813056 logging_writer.py:48] [164300] global_step=164300, grad_norm=3.0051591396331787, loss=3.8463058471679688
I0203 17:29:09.755299 139923852027648 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.7317769527435303, loss=1.9358041286468506
I0203 17:29:56.740837 139923868813056 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.527087926864624, loss=1.70517897605896
I0203 17:30:43.650752 139923852027648 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.7797489166259766, loss=1.758015513420105
I0203 17:31:30.503623 139923868813056 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.7189133167266846, loss=3.5137171745300293
I0203 17:32:12.290367 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:32:23.027115 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:32:56.868419 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:32:58.508577 140085747812160 submission_runner.py:408] Time since start: 84552.21s, 	Step: 164791, 	{'train/accuracy': 0.7859765291213989, 'train/loss': 0.87266606092453, 'validation/accuracy': 0.7135399580001831, 'validation/loss': 1.186859130859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.812342643737793, 'test/num_examples': 10000, 'score': 76504.19021439552, 'total_duration': 84552.21259260178, 'accumulated_submission_time': 76504.19021439552, 'accumulated_eval_time': 8030.3428745269775, 'accumulated_logging_time': 8.457646608352661}
I0203 17:32:58.552778 139923852027648 logging_writer.py:48] [164791] accumulated_eval_time=8030.342875, accumulated_logging_time=8.457647, accumulated_submission_time=76504.190214, global_step=164791, preemption_count=0, score=76504.190214, test/accuracy=0.586800, test/loss=1.812343, test/num_examples=10000, total_duration=84552.212593, train/accuracy=0.785977, train/loss=0.872666, validation/accuracy=0.713540, validation/loss=1.186859, validation/num_examples=50000
I0203 17:33:02.478187 139923868813056 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.590388774871826, loss=2.38151216506958
I0203 17:33:44.877548 139923852027648 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.754819869995117, loss=2.0976181030273438
I0203 17:34:31.779996 139923868813056 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.6656599044799805, loss=1.8158845901489258
I0203 17:35:18.755724 139923852027648 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.458388090133667, loss=3.3241701126098633
I0203 17:36:06.053375 139923868813056 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.774425506591797, loss=1.915055274963379
I0203 17:36:53.069896 139923852027648 logging_writer.py:48] [165300] global_step=165300, grad_norm=2.8716113567352295, loss=2.0963621139526367
I0203 17:37:39.945997 139923868813056 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.85337495803833, loss=1.829551339149475
I0203 17:38:27.102982 139923852027648 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6576876640319824, loss=2.9232683181762695
I0203 17:39:14.142408 139923868813056 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.3543055057525635, loss=4.139017105102539
I0203 17:39:58.580129 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:40:09.292648 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:40:41.758350 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:40:43.394108 140085747812160 submission_runner.py:408] Time since start: 85017.10s, 	Step: 165696, 	{'train/accuracy': 0.7836523056030273, 'train/loss': 0.8662102222442627, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.161203145980835, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.7810871601104736, 'test/num_examples': 10000, 'score': 76924.15655565262, 'total_duration': 85017.09811878204, 'accumulated_submission_time': 76924.15655565262, 'accumulated_eval_time': 8075.156850099564, 'accumulated_logging_time': 8.51274585723877}
I0203 17:40:43.439046 139923852027648 logging_writer.py:48] [165696] accumulated_eval_time=8075.156850, accumulated_logging_time=8.512746, accumulated_submission_time=76924.156556, global_step=165696, preemption_count=0, score=76924.156556, test/accuracy=0.591400, test/loss=1.781087, test/num_examples=10000, total_duration=85017.098119, train/accuracy=0.783652, train/loss=0.866210, validation/accuracy=0.715480, validation/loss=1.161203, validation/num_examples=50000
I0203 17:40:45.410186 139923868813056 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.8973875045776367, loss=1.7660471200942993
I0203 17:41:27.792736 139923852027648 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.817545175552368, loss=1.7661943435668945
I0203 17:42:14.521229 139923868813056 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.8632571697235107, loss=2.2299726009368896
I0203 17:43:01.500152 139923852027648 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.839693546295166, loss=2.0053982734680176
I0203 17:43:48.340544 139923868813056 logging_writer.py:48] [166100] global_step=166100, grad_norm=2.666550874710083, loss=2.296064853668213
I0203 17:44:35.290474 139923852027648 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.602447271347046, loss=1.9467107057571411
I0203 17:45:22.365944 139923868813056 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.7956020832061768, loss=1.9061906337738037
I0203 17:46:09.554933 139923852027648 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.718287944793701, loss=2.320903778076172
I0203 17:46:56.329792 139923868813056 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.8330302238464355, loss=1.7461256980895996
I0203 17:47:43.236140 139923852027648 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.0425944328308105, loss=3.1557440757751465
I0203 17:47:43.431742 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:47:53.991023 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:48:28.736573 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:48:30.382439 140085747812160 submission_runner.py:408] Time since start: 85484.09s, 	Step: 166602, 	{'train/accuracy': 0.7826757431030273, 'train/loss': 0.8744816184043884, 'validation/accuracy': 0.7170400023460388, 'validation/loss': 1.1657946109771729, 'validation/num_examples': 50000, 'test/accuracy': 0.595300018787384, 'test/loss': 1.7856409549713135, 'test/num_examples': 10000, 'score': 77344.08736562729, 'total_duration': 85484.08644890785, 'accumulated_submission_time': 77344.08736562729, 'accumulated_eval_time': 8122.107517242432, 'accumulated_logging_time': 8.569658279418945}
I0203 17:48:30.424271 139923868813056 logging_writer.py:48] [166602] accumulated_eval_time=8122.107517, accumulated_logging_time=8.569658, accumulated_submission_time=77344.087366, global_step=166602, preemption_count=0, score=77344.087366, test/accuracy=0.595300, test/loss=1.785641, test/num_examples=10000, total_duration=85484.086449, train/accuracy=0.782676, train/loss=0.874482, validation/accuracy=0.717040, validation/loss=1.165795, validation/num_examples=50000
I0203 17:49:11.930466 139923852027648 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.6548609733581543, loss=2.1126954555511475
I0203 17:49:58.813881 139923868813056 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.858391284942627, loss=1.838632583618164
I0203 17:50:45.923426 139923852027648 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.925684690475464, loss=3.723068952560425
I0203 17:51:26.587142 139923868813056 logging_writer.py:48] [166988] global_step=166988, preemption_count=0, score=77520.157780
I0203 17:51:27.297465 140085747812160 checkpoints.py:490] Saving checkpoint at step: 166988
I0203 17:51:28.780945 140085747812160 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4/checkpoint_166988
I0203 17:51:28.808422 140085747812160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_4/checkpoint_166988.
I0203 17:51:29.897488 140085747812160 submission_runner.py:583] Tuning trial 4/5
I0203 17:51:29.897717 140085747812160 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0203 17:51:29.908980 140085747812160 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012109375093132257, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 36.40435767173767, 'total_duration': 65.12631821632385, 'accumulated_submission_time': 36.40435767173767, 'accumulated_eval_time': 28.721853494644165, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (850, {'train/accuracy': 0.03794921934604645, 'train/loss': 5.836334705352783, 'validation/accuracy': 0.037379998713731766, 'validation/loss': 5.868350028991699, 'validation/num_examples': 50000, 'test/accuracy': 0.030400000512599945, 'test/loss': 6.007944583892822, 'test/num_examples': 10000, 'score': 456.5975193977356, 'total_duration': 525.6996972560883, 'accumulated_submission_time': 456.5975193977356, 'accumulated_eval_time': 69.03572416305542, 'accumulated_logging_time': 0.01876664161682129, 'global_step': 850, 'preemption_count': 0}), (1753, {'train/accuracy': 0.06109374761581421, 'train/loss': 5.500517845153809, 'validation/accuracy': 0.05495999753475189, 'validation/loss': 5.583978176116943, 'validation/num_examples': 50000, 'test/accuracy': 0.04190000146627426, 'test/loss': 5.780269145965576, 'test/num_examples': 10000, 'score': 876.5749049186707, 'total_duration': 986.8721923828125, 'accumulated_submission_time': 876.5749049186707, 'accumulated_eval_time': 110.14891505241394, 'accumulated_logging_time': 0.05063366889953613, 'global_step': 1753, 'preemption_count': 0}), (2659, {'train/accuracy': 0.10499999672174454, 'train/loss': 4.955112934112549, 'validation/accuracy': 0.095660001039505, 'validation/loss': 5.036952495574951, 'validation/num_examples': 50000, 'test/accuracy': 0.07670000195503235, 'test/loss': 5.3251471519470215, 'test/num_examples': 10000, 'score': 1296.6166756153107, 'total_duration': 1450.5467166900635, 'accumulated_submission_time': 1296.6166756153107, 'accumulated_eval_time': 153.70484042167664, 'accumulated_logging_time': 0.07620692253112793, 'global_step': 2659, 'preemption_count': 0}), (3565, {'train/accuracy': 0.1318359375, 'train/loss': 4.715084075927734, 'validation/accuracy': 0.12212000042200089, 'validation/loss': 4.792366981506348, 'validation/num_examples': 50000, 'test/accuracy': 0.09670000523328781, 'test/loss': 5.13610315322876, 'test/num_examples': 10000, 'score': 1716.937399148941, 'total_duration': 1909.1078877449036, 'accumulated_submission_time': 1716.937399148941, 'accumulated_eval_time': 191.86698031425476, 'accumulated_logging_time': 0.1044766902923584, 'global_step': 3565, 'preemption_count': 0}), (4470, {'train/accuracy': 0.16396483778953552, 'train/loss': 4.44169282913208, 'validation/accuracy': 0.14983999729156494, 'validation/loss': 4.542600631713867, 'validation/num_examples': 50000, 'test/accuracy': 0.11570000648498535, 'test/loss': 4.900367259979248, 'test/num_examples': 10000, 'score': 2137.0898151397705, 'total_duration': 2368.407334804535, 'accumulated_submission_time': 2137.0898151397705, 'accumulated_eval_time': 230.93383693695068, 'accumulated_logging_time': 0.13391804695129395, 'global_step': 4470, 'preemption_count': 0}), (5375, {'train/accuracy': 0.19007812440395355, 'train/loss': 4.193375587463379, 'validation/accuracy': 0.17573998868465424, 'validation/loss': 4.2860283851623535, 'validation/num_examples': 50000, 'test/accuracy': 0.13700000941753387, 'test/loss': 4.693284034729004, 'test/num_examples': 10000, 'score': 2557.3137817382812, 'total_duration': 2834.1254436969757, 'accumulated_submission_time': 2557.3137817382812, 'accumulated_eval_time': 276.35117983818054, 'accumulated_logging_time': 0.16036367416381836, 'global_step': 5375, 'preemption_count': 0}), (6280, {'train/accuracy': 0.21013671159744263, 'train/loss': 4.0672478675842285, 'validation/accuracy': 0.19367998838424683, 'validation/loss': 4.1517534255981445, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 4.536256790161133, 'test/num_examples': 10000, 'score': 2977.3031663894653, 'total_duration': 3297.7814099788666, 'accumulated_submission_time': 2977.3031663894653, 'accumulated_eval_time': 319.9392533302307, 'accumulated_logging_time': 0.1883072853088379, 'global_step': 6280, 'preemption_count': 0}), (7187, {'train/accuracy': 0.2340429574251175, 'train/loss': 3.8879802227020264, 'validation/accuracy': 0.21383999288082123, 'validation/loss': 4.022542476654053, 'validation/num_examples': 50000, 'test/accuracy': 0.1632000058889389, 'test/loss': 4.448765754699707, 'test/num_examples': 10000, 'score': 3397.616981983185, 'total_duration': 3762.831591129303, 'accumulated_submission_time': 3397.616981983185, 'accumulated_eval_time': 364.59548354148865, 'accumulated_logging_time': 0.21813154220581055, 'global_step': 7187, 'preemption_count': 0}), (8094, {'train/accuracy': 0.24208983778953552, 'train/loss': 3.8262104988098145, 'validation/accuracy': 0.22335998713970184, 'validation/loss': 3.9299376010894775, 'validation/num_examples': 50000, 'test/accuracy': 0.16870000958442688, 'test/loss': 4.404850482940674, 'test/num_examples': 10000, 'score': 3817.7951107025146, 'total_duration': 4223.264901161194, 'accumulated_submission_time': 3817.7951107025146, 'accumulated_eval_time': 404.7730875015259, 'accumulated_logging_time': 0.24477744102478027, 'global_step': 8094, 'preemption_count': 0}), (9001, {'train/accuracy': 0.27150389552116394, 'train/loss': 3.5727624893188477, 'validation/accuracy': 0.24845999479293823, 'validation/loss': 3.701709032058716, 'validation/num_examples': 50000, 'test/accuracy': 0.19030000269412994, 'test/loss': 4.200378894805908, 'test/num_examples': 10000, 'score': 4237.933657169342, 'total_duration': 4687.290641784668, 'accumulated_submission_time': 4237.933657169342, 'accumulated_eval_time': 448.5805284976959, 'accumulated_logging_time': 0.27434587478637695, 'global_step': 9001, 'preemption_count': 0}), (9908, {'train/accuracy': 0.2799023389816284, 'train/loss': 3.503488063812256, 'validation/accuracy': 0.2557999789714813, 'validation/loss': 3.6524834632873535, 'validation/num_examples': 50000, 'test/accuracy': 0.2045000046491623, 'test/loss': 4.137157917022705, 'test/num_examples': 10000, 'score': 4658.288609981537, 'total_duration': 5149.827425003052, 'accumulated_submission_time': 4658.288609981537, 'accumulated_eval_time': 490.68664383888245, 'accumulated_logging_time': 0.2993443012237549, 'global_step': 9908, 'preemption_count': 0}), (10808, {'train/accuracy': 0.2586914002895355, 'train/loss': 3.687410593032837, 'validation/accuracy': 0.2386999875307083, 'validation/loss': 3.8200554847717285, 'validation/num_examples': 50000, 'test/accuracy': 0.18380001187324524, 'test/loss': 4.281069278717041, 'test/num_examples': 10000, 'score': 5077.988896608353, 'total_duration': 5609.243244171143, 'accumulated_submission_time': 5077.988896608353, 'accumulated_eval_time': 529.848639011383, 'accumulated_logging_time': 0.8035287857055664, 'global_step': 10808, 'preemption_count': 0}), (11712, {'train/accuracy': 0.29048827290534973, 'train/loss': 3.440134286880493, 'validation/accuracy': 0.27008000016212463, 'validation/loss': 3.5581839084625244, 'validation/num_examples': 50000, 'test/accuracy': 0.20440000295639038, 'test/loss': 4.096872806549072, 'test/num_examples': 10000, 'score': 5498.148392438889, 'total_duration': 6074.42378115654, 'accumulated_submission_time': 5498.148392438889, 'accumulated_eval_time': 574.7927012443542, 'accumulated_logging_time': 0.8307521343231201, 'global_step': 11712, 'preemption_count': 0}), (12615, {'train/accuracy': 0.2845117151737213, 'train/loss': 3.4956023693084717, 'validation/accuracy': 0.262939989566803, 'validation/loss': 3.6273386478424072, 'validation/num_examples': 50000, 'test/accuracy': 0.20280000567436218, 'test/loss': 4.120352745056152, 'test/num_examples': 10000, 'score': 5918.338252067566, 'total_duration': 6537.819251060486, 'accumulated_submission_time': 5918.338252067566, 'accumulated_eval_time': 617.9171347618103, 'accumulated_logging_time': 0.8616728782653809, 'global_step': 12615, 'preemption_count': 0}), (13521, {'train/accuracy': 0.33222654461860657, 'train/loss': 3.20548152923584, 'validation/accuracy': 0.2886999845504761, 'validation/loss': 3.4614808559417725, 'validation/num_examples': 50000, 'test/accuracy': 0.2183000147342682, 'test/loss': 3.9869000911712646, 'test/num_examples': 10000, 'score': 6338.559593200684, 'total_duration': 6998.628676176071, 'accumulated_submission_time': 6338.559593200684, 'accumulated_eval_time': 658.4205052852631, 'accumulated_logging_time': 0.8963735103607178, 'global_step': 13521, 'preemption_count': 0}), (14428, {'train/accuracy': 0.30156248807907104, 'train/loss': 3.388521671295166, 'validation/accuracy': 0.28001999855041504, 'validation/loss': 3.510899305343628, 'validation/num_examples': 50000, 'test/accuracy': 0.21120001375675201, 'test/loss': 4.033987522125244, 'test/num_examples': 10000, 'score': 6758.897345304489, 'total_duration': 7458.634178161621, 'accumulated_submission_time': 6758.897345304489, 'accumulated_eval_time': 698.0081906318665, 'accumulated_logging_time': 0.9258718490600586, 'global_step': 14428, 'preemption_count': 0}), (15334, {'train/accuracy': 0.31974607706069946, 'train/loss': 3.255380868911743, 'validation/accuracy': 0.29853999614715576, 'validation/loss': 3.3957231044769287, 'validation/num_examples': 50000, 'test/accuracy': 0.2208000123500824, 'test/loss': 3.9570510387420654, 'test/num_examples': 10000, 'score': 7178.821423768997, 'total_duration': 7923.335958003998, 'accumulated_submission_time': 7178.821423768997, 'accumulated_eval_time': 742.7038342952728, 'accumulated_logging_time': 0.957528829574585, 'global_step': 15334, 'preemption_count': 0}), (16242, {'train/accuracy': 0.3459570109844208, 'train/loss': 3.120103120803833, 'validation/accuracy': 0.29927998781204224, 'validation/loss': 3.3753139972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.22510001063346863, 'test/loss': 3.943704128265381, 'test/num_examples': 10000, 'score': 7598.78031373024, 'total_duration': 8387.46527838707, 'accumulated_submission_time': 7598.78031373024, 'accumulated_eval_time': 786.7944359779358, 'accumulated_logging_time': 0.9867620468139648, 'global_step': 16242, 'preemption_count': 0}), (17149, {'train/accuracy': 0.33298826217651367, 'train/loss': 3.1438841819763184, 'validation/accuracy': 0.3075200021266937, 'validation/loss': 3.2924156188964844, 'validation/num_examples': 50000, 'test/accuracy': 0.23760001361370087, 'test/loss': 3.8583736419677734, 'test/num_examples': 10000, 'score': 8019.018817424774, 'total_duration': 8844.835416793823, 'accumulated_submission_time': 8019.018817424774, 'accumulated_eval_time': 823.8475241661072, 'accumulated_logging_time': 1.0148942470550537, 'global_step': 17149, 'preemption_count': 0}), (18053, {'train/accuracy': 0.3213476538658142, 'train/loss': 3.2263472080230713, 'validation/accuracy': 0.2976999878883362, 'validation/loss': 3.3574440479278564, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 3.882843255996704, 'test/num_examples': 10000, 'score': 8438.975415468216, 'total_duration': 9306.65334200859, 'accumulated_submission_time': 8438.975415468216, 'accumulated_eval_time': 865.6273958683014, 'accumulated_logging_time': 1.0453910827636719, 'global_step': 18053, 'preemption_count': 0}), (18960, {'train/accuracy': 0.34953123331069946, 'train/loss': 3.1147797107696533, 'validation/accuracy': 0.30761998891830444, 'validation/loss': 3.3317410945892334, 'validation/num_examples': 50000, 'test/accuracy': 0.2371000051498413, 'test/loss': 3.893336057662964, 'test/num_examples': 10000, 'score': 8859.113800764084, 'total_duration': 9771.343579053879, 'accumulated_submission_time': 8859.113800764084, 'accumulated_eval_time': 910.0973136425018, 'accumulated_logging_time': 1.0767600536346436, 'global_step': 18960, 'preemption_count': 0}), (19867, {'train/accuracy': 0.3369531035423279, 'train/loss': 3.1348671913146973, 'validation/accuracy': 0.31793999671936035, 'validation/loss': 3.2607951164245605, 'validation/num_examples': 50000, 'test/accuracy': 0.2427000105381012, 'test/loss': 3.7915139198303223, 'test/num_examples': 10000, 'score': 9279.488339185715, 'total_duration': 10235.852065563202, 'accumulated_submission_time': 9279.488339185715, 'accumulated_eval_time': 954.1499533653259, 'accumulated_logging_time': 1.10762619972229, 'global_step': 19867, 'preemption_count': 0}), (20771, {'train/accuracy': 0.3226367235183716, 'train/loss': 3.2174956798553467, 'validation/accuracy': 0.2995399832725525, 'validation/loss': 3.355835199356079, 'validation/num_examples': 50000, 'test/accuracy': 0.22910000383853912, 'test/loss': 3.92555832862854, 'test/num_examples': 10000, 'score': 9699.60234951973, 'total_duration': 10699.347157001495, 'accumulated_submission_time': 9699.60234951973, 'accumulated_eval_time': 997.4499151706696, 'accumulated_logging_time': 1.1384267807006836, 'global_step': 20771, 'preemption_count': 0}), (21675, {'train/accuracy': 0.3468359410762787, 'train/loss': 3.1015145778656006, 'validation/accuracy': 0.31501999497413635, 'validation/loss': 3.295905113220215, 'validation/num_examples': 50000, 'test/accuracy': 0.2485000044107437, 'test/loss': 3.8394834995269775, 'test/num_examples': 10000, 'score': 10119.622790336609, 'total_duration': 11165.643399238586, 'accumulated_submission_time': 10119.622790336609, 'accumulated_eval_time': 1043.6431443691254, 'accumulated_logging_time': 1.169325590133667, 'global_step': 21675, 'preemption_count': 0}), (22581, {'train/accuracy': 0.34675779938697815, 'train/loss': 3.073084592819214, 'validation/accuracy': 0.3256799876689911, 'validation/loss': 3.1851108074188232, 'validation/num_examples': 50000, 'test/accuracy': 0.24720001220703125, 'test/loss': 3.7654662132263184, 'test/num_examples': 10000, 'score': 10539.923010349274, 'total_duration': 11630.60943365097, 'accumulated_submission_time': 10539.923010349274, 'accumulated_eval_time': 1088.2273619174957, 'accumulated_logging_time': 1.2005112171173096, 'global_step': 22581, 'preemption_count': 0}), (23486, {'train/accuracy': 0.35316404700279236, 'train/loss': 3.0341224670410156, 'validation/accuracy': 0.33142000436782837, 'validation/loss': 3.17514705657959, 'validation/num_examples': 50000, 'test/accuracy': 0.2502000033855438, 'test/loss': 3.758253812789917, 'test/num_examples': 10000, 'score': 10959.892746925354, 'total_duration': 12097.288171768188, 'accumulated_submission_time': 10959.892746925354, 'accumulated_eval_time': 1134.8523106575012, 'accumulated_logging_time': 1.2339890003204346, 'global_step': 23486, 'preemption_count': 0}), (24390, {'train/accuracy': 0.37041014432907104, 'train/loss': 2.9094772338867188, 'validation/accuracy': 0.3361800014972687, 'validation/loss': 3.098283052444458, 'validation/num_examples': 50000, 'test/accuracy': 0.26080000400543213, 'test/loss': 3.6874732971191406, 'test/num_examples': 10000, 'score': 11380.030760765076, 'total_duration': 12563.504616975784, 'accumulated_submission_time': 11380.030760765076, 'accumulated_eval_time': 1180.8493909835815, 'accumulated_logging_time': 1.2654187679290771, 'global_step': 24390, 'preemption_count': 0}), (25296, {'train/accuracy': 0.3532421886920929, 'train/loss': 3.0145912170410156, 'validation/accuracy': 0.33399999141693115, 'validation/loss': 3.1400809288024902, 'validation/num_examples': 50000, 'test/accuracy': 0.25270000100135803, 'test/loss': 3.7258048057556152, 'test/num_examples': 10000, 'score': 11800.12370634079, 'total_duration': 13029.195685625076, 'accumulated_submission_time': 11800.12370634079, 'accumulated_eval_time': 1226.3632354736328, 'accumulated_logging_time': 1.2985377311706543, 'global_step': 25296, 'preemption_count': 0}), (26202, {'train/accuracy': 0.3569726347923279, 'train/loss': 3.0196990966796875, 'validation/accuracy': 0.3306399881839752, 'validation/loss': 3.152756690979004, 'validation/num_examples': 50000, 'test/accuracy': 0.2502000033855438, 'test/loss': 3.755117416381836, 'test/num_examples': 10000, 'score': 12220.688608169556, 'total_duration': 13495.26099729538, 'accumulated_submission_time': 12220.688608169556, 'accumulated_eval_time': 1271.7826147079468, 'accumulated_logging_time': 1.3286545276641846, 'global_step': 26202, 'preemption_count': 0}), (27109, {'train/accuracy': 0.3746679723262787, 'train/loss': 2.8854188919067383, 'validation/accuracy': 0.3447200059890747, 'validation/loss': 3.0645339488983154, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.661961078643799, 'test/num_examples': 10000, 'score': 12640.947337388992, 'total_duration': 13963.037720918655, 'accumulated_submission_time': 12640.947337388992, 'accumulated_eval_time': 1319.2206687927246, 'accumulated_logging_time': 1.3581278324127197, 'global_step': 27109, 'preemption_count': 0}), (28014, {'train/accuracy': 0.36474609375, 'train/loss': 2.9812686443328857, 'validation/accuracy': 0.3385799825191498, 'validation/loss': 3.11944842338562, 'validation/num_examples': 50000, 'test/accuracy': 0.2556000053882599, 'test/loss': 3.7341465950012207, 'test/num_examples': 10000, 'score': 13060.903878450394, 'total_duration': 14423.568979501724, 'accumulated_submission_time': 13060.903878450394, 'accumulated_eval_time': 1359.713604927063, 'accumulated_logging_time': 1.3895189762115479, 'global_step': 28014, 'preemption_count': 0}), (28921, {'train/accuracy': 0.3594140410423279, 'train/loss': 3.011570692062378, 'validation/accuracy': 0.33535999059677124, 'validation/loss': 3.157099723815918, 'validation/num_examples': 50000, 'test/accuracy': 0.26030001044273376, 'test/loss': 3.740095853805542, 'test/num_examples': 10000, 'score': 13481.02701640129, 'total_duration': 14888.096348762512, 'accumulated_submission_time': 13481.02701640129, 'accumulated_eval_time': 1404.037055015564, 'accumulated_logging_time': 1.4189238548278809, 'global_step': 28921, 'preemption_count': 0}), (29827, {'train/accuracy': 0.3867773413658142, 'train/loss': 2.8524653911590576, 'validation/accuracy': 0.35624000430107117, 'validation/loss': 3.020305633544922, 'validation/num_examples': 50000, 'test/accuracy': 0.2722000181674957, 'test/loss': 3.626361608505249, 'test/num_examples': 10000, 'score': 13901.145128250122, 'total_duration': 15352.472299575806, 'accumulated_submission_time': 13901.145128250122, 'accumulated_eval_time': 1448.2114634513855, 'accumulated_logging_time': 1.4525518417358398, 'global_step': 29827, 'preemption_count': 0}), (30733, {'train/accuracy': 0.40380859375, 'train/loss': 2.765493631362915, 'validation/accuracy': 0.35071998834609985, 'validation/loss': 3.034952402114868, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.6251919269561768, 'test/num_examples': 10000, 'score': 14321.3808157444, 'total_duration': 15817.029076099396, 'accumulated_submission_time': 14321.3808157444, 'accumulated_eval_time': 1492.4437718391418, 'accumulated_logging_time': 1.489807367324829, 'global_step': 30733, 'preemption_count': 0}), (31638, {'train/accuracy': 0.372871071100235, 'train/loss': 2.894007921218872, 'validation/accuracy': 0.3455199897289276, 'validation/loss': 3.053222179412842, 'validation/num_examples': 50000, 'test/accuracy': 0.26579999923706055, 'test/loss': 3.6384551525115967, 'test/num_examples': 10000, 'score': 14741.569679737091, 'total_duration': 16281.695380210876, 'accumulated_submission_time': 14741.569679737091, 'accumulated_eval_time': 1536.8387801647186, 'accumulated_logging_time': 1.5209715366363525, 'global_step': 31638, 'preemption_count': 0}), (32541, {'train/accuracy': 0.3836914002895355, 'train/loss': 2.8515477180480957, 'validation/accuracy': 0.35627999901771545, 'validation/loss': 3.018123149871826, 'validation/num_examples': 50000, 'test/accuracy': 0.27890002727508545, 'test/loss': 3.585503578186035, 'test/num_examples': 10000, 'score': 15161.727267503738, 'total_duration': 16745.701989650726, 'accumulated_submission_time': 15161.727267503738, 'accumulated_eval_time': 1580.6043591499329, 'accumulated_logging_time': 1.5537612438201904, 'global_step': 32541, 'preemption_count': 0}), (33446, {'train/accuracy': 0.4034960865974426, 'train/loss': 2.7900664806365967, 'validation/accuracy': 0.3520599901676178, 'validation/loss': 3.0779330730438232, 'validation/num_examples': 50000, 'test/accuracy': 0.2671000063419342, 'test/loss': 3.6647629737854004, 'test/num_examples': 10000, 'score': 15581.699176073074, 'total_duration': 17207.740622758865, 'accumulated_submission_time': 15581.699176073074, 'accumulated_eval_time': 1622.586046218872, 'accumulated_logging_time': 1.5887606143951416, 'global_step': 33446, 'preemption_count': 0}), (34350, {'train/accuracy': 0.37398436665534973, 'train/loss': 2.903052568435669, 'validation/accuracy': 0.3515399992465973, 'validation/loss': 3.0420234203338623, 'validation/num_examples': 50000, 'test/accuracy': 0.2727999985218048, 'test/loss': 3.618873119354248, 'test/num_examples': 10000, 'score': 16001.772747516632, 'total_duration': 17669.6182513237, 'accumulated_submission_time': 16001.772747516632, 'accumulated_eval_time': 1664.3043756484985, 'accumulated_logging_time': 1.623687982559204, 'global_step': 34350, 'preemption_count': 0}), (35257, {'train/accuracy': 0.3941406309604645, 'train/loss': 2.7809207439422607, 'validation/accuracy': 0.3646000027656555, 'validation/loss': 2.946488380432129, 'validation/num_examples': 50000, 'test/accuracy': 0.281000018119812, 'test/loss': 3.570101261138916, 'test/num_examples': 10000, 'score': 16422.13542985916, 'total_duration': 18129.602539777756, 'accumulated_submission_time': 16422.13542985916, 'accumulated_eval_time': 1703.8427374362946, 'accumulated_logging_time': 1.655987024307251, 'global_step': 35257, 'preemption_count': 0}), (36160, {'train/accuracy': 0.4143359363079071, 'train/loss': 2.687903881072998, 'validation/accuracy': 0.37469998002052307, 'validation/loss': 2.915180206298828, 'validation/num_examples': 50000, 'test/accuracy': 0.289000004529953, 'test/loss': 3.515514612197876, 'test/num_examples': 10000, 'score': 16842.51434326172, 'total_duration': 18596.418609380722, 'accumulated_submission_time': 16842.51434326172, 'accumulated_eval_time': 1750.1966817378998, 'accumulated_logging_time': 1.688964605331421, 'global_step': 36160, 'preemption_count': 0}), (37067, {'train/accuracy': 0.3991992175579071, 'train/loss': 2.74881911277771, 'validation/accuracy': 0.3721599876880646, 'validation/loss': 2.8969502449035645, 'validation/num_examples': 50000, 'test/accuracy': 0.296500027179718, 'test/loss': 3.5063552856445312, 'test/num_examples': 10000, 'score': 17262.60005879402, 'total_duration': 19060.820858955383, 'accumulated_submission_time': 17262.60005879402, 'accumulated_eval_time': 1794.428700685501, 'accumulated_logging_time': 1.7227368354797363, 'global_step': 37067, 'preemption_count': 0}), (37975, {'train/accuracy': 0.40013670921325684, 'train/loss': 2.7493491172790527, 'validation/accuracy': 0.3765600025653839, 'validation/loss': 2.8872478008270264, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.4961540699005127, 'test/num_examples': 10000, 'score': 17682.65801167488, 'total_duration': 19521.791479349136, 'accumulated_submission_time': 17682.65801167488, 'accumulated_eval_time': 1835.2586352825165, 'accumulated_logging_time': 1.7550582885742188, 'global_step': 37975, 'preemption_count': 0}), (38881, {'train/accuracy': 0.4105273485183716, 'train/loss': 2.71467924118042, 'validation/accuracy': 0.3744199872016907, 'validation/loss': 2.916626453399658, 'validation/num_examples': 50000, 'test/accuracy': 0.2857000231742859, 'test/loss': 3.532196283340454, 'test/num_examples': 10000, 'score': 18102.947584867477, 'total_duration': 19988.588298797607, 'accumulated_submission_time': 18102.947584867477, 'accumulated_eval_time': 1881.6751172542572, 'accumulated_logging_time': 1.7953834533691406, 'global_step': 38881, 'preemption_count': 0}), (39787, {'train/accuracy': 0.3961132764816284, 'train/loss': 2.812042713165283, 'validation/accuracy': 0.3680199980735779, 'validation/loss': 2.944639205932617, 'validation/num_examples': 50000, 'test/accuracy': 0.2865000069141388, 'test/loss': 3.5363407135009766, 'test/num_examples': 10000, 'score': 18522.871568918228, 'total_duration': 20452.823182106018, 'accumulated_submission_time': 18522.871568918228, 'accumulated_eval_time': 1925.902155160904, 'accumulated_logging_time': 1.8292152881622314, 'global_step': 39787, 'preemption_count': 0}), (40692, {'train/accuracy': 0.3843359351158142, 'train/loss': 2.8809280395507812, 'validation/accuracy': 0.35760000348091125, 'validation/loss': 3.0336527824401855, 'validation/num_examples': 50000, 'test/accuracy': 0.27740001678466797, 'test/loss': 3.6219842433929443, 'test/num_examples': 10000, 'score': 18942.88044810295, 'total_duration': 20917.042991399765, 'accumulated_submission_time': 18942.88044810295, 'accumulated_eval_time': 1970.02938914299, 'accumulated_logging_time': 1.8625555038452148, 'global_step': 40692, 'preemption_count': 0}), (41598, {'train/accuracy': 0.41939452290534973, 'train/loss': 2.614825487136841, 'validation/accuracy': 0.38731998205184937, 'validation/loss': 2.794647216796875, 'validation/num_examples': 50000, 'test/accuracy': 0.30250000953674316, 'test/loss': 3.3992068767547607, 'test/num_examples': 10000, 'score': 19363.052728176117, 'total_duration': 21379.428280830383, 'accumulated_submission_time': 19363.052728176117, 'accumulated_eval_time': 2012.1537518501282, 'accumulated_logging_time': 1.9005413055419922, 'global_step': 41598, 'preemption_count': 0}), (42501, {'train/accuracy': 0.41039061546325684, 'train/loss': 2.7060678005218506, 'validation/accuracy': 0.38383999466896057, 'validation/loss': 2.8514671325683594, 'validation/num_examples': 50000, 'test/accuracy': 0.2962000072002411, 'test/loss': 3.468750238418579, 'test/num_examples': 10000, 'score': 19783.368386268616, 'total_duration': 21846.06435275078, 'accumulated_submission_time': 19783.368386268616, 'accumulated_eval_time': 2058.3900072574615, 'accumulated_logging_time': 1.9348745346069336, 'global_step': 42501, 'preemption_count': 0}), (43408, {'train/accuracy': 0.4212304651737213, 'train/loss': 2.6352226734161377, 'validation/accuracy': 0.39013999700546265, 'validation/loss': 2.7732999324798584, 'validation/num_examples': 50000, 'test/accuracy': 0.3060000240802765, 'test/loss': 3.412951707839966, 'test/num_examples': 10000, 'score': 20203.399605989456, 'total_duration': 22306.24627304077, 'accumulated_submission_time': 20203.399605989456, 'accumulated_eval_time': 2098.456430912018, 'accumulated_logging_time': 1.9667832851409912, 'global_step': 43408, 'preemption_count': 0}), (44314, {'train/accuracy': 0.424628883600235, 'train/loss': 2.5954785346984863, 'validation/accuracy': 0.3944399952888489, 'validation/loss': 2.772815465927124, 'validation/num_examples': 50000, 'test/accuracy': 0.30400002002716064, 'test/loss': 3.4005684852600098, 'test/num_examples': 10000, 'score': 20623.64171051979, 'total_duration': 22768.33304667473, 'accumulated_submission_time': 20623.64171051979, 'accumulated_eval_time': 2140.2166497707367, 'accumulated_logging_time': 1.9997265338897705, 'global_step': 44314, 'preemption_count': 0}), (45220, {'train/accuracy': 0.4167773425579071, 'train/loss': 2.6995503902435303, 'validation/accuracy': 0.39111998677253723, 'validation/loss': 2.838123083114624, 'validation/num_examples': 50000, 'test/accuracy': 0.30580002069473267, 'test/loss': 3.4516892433166504, 'test/num_examples': 10000, 'score': 21043.791278362274, 'total_duration': 23235.264661312103, 'accumulated_submission_time': 21043.791278362274, 'accumulated_eval_time': 2186.9113490581512, 'accumulated_logging_time': 2.0372612476348877, 'global_step': 45220, 'preemption_count': 0}), (46125, {'train/accuracy': 0.4143945276737213, 'train/loss': 2.6916167736053467, 'validation/accuracy': 0.3907199800014496, 'validation/loss': 2.808105945587158, 'validation/num_examples': 50000, 'test/accuracy': 0.29090002179145813, 'test/loss': 3.441751718521118, 'test/num_examples': 10000, 'score': 21463.8963804245, 'total_duration': 23699.373474121094, 'accumulated_submission_time': 21463.8963804245, 'accumulated_eval_time': 2230.8290145397186, 'accumulated_logging_time': 2.0722317695617676, 'global_step': 46125, 'preemption_count': 0}), (47029, {'train/accuracy': 0.4289257824420929, 'train/loss': 2.5802981853485107, 'validation/accuracy': 0.39791998267173767, 'validation/loss': 2.7644102573394775, 'validation/num_examples': 50000, 'test/accuracy': 0.30260002613067627, 'test/loss': 3.3919708728790283, 'test/num_examples': 10000, 'score': 21884.086091279984, 'total_duration': 24165.378796577454, 'accumulated_submission_time': 21884.086091279984, 'accumulated_eval_time': 2276.559750556946, 'accumulated_logging_time': 2.1068155765533447, 'global_step': 47029, 'preemption_count': 0}), (47934, {'train/accuracy': 0.4466210901737213, 'train/loss': 2.509028196334839, 'validation/accuracy': 0.40153998136520386, 'validation/loss': 2.7524359226226807, 'validation/num_examples': 50000, 'test/accuracy': 0.31370002031326294, 'test/loss': 3.3566060066223145, 'test/num_examples': 10000, 'score': 22304.20796895027, 'total_duration': 24629.18269968033, 'accumulated_submission_time': 22304.20796895027, 'accumulated_eval_time': 2320.1568138599396, 'accumulated_logging_time': 2.1419427394866943, 'global_step': 47934, 'preemption_count': 0}), (48840, {'train/accuracy': 0.42265623807907104, 'train/loss': 2.647350788116455, 'validation/accuracy': 0.39475998282432556, 'validation/loss': 2.8125829696655273, 'validation/num_examples': 50000, 'test/accuracy': 0.3037000000476837, 'test/loss': 3.428317070007324, 'test/num_examples': 10000, 'score': 22724.48389363289, 'total_duration': 25092.585326194763, 'accumulated_submission_time': 22724.48389363289, 'accumulated_eval_time': 2363.196498155594, 'accumulated_logging_time': 2.1787829399108887, 'global_step': 48840, 'preemption_count': 0}), (49746, {'train/accuracy': 0.4342578053474426, 'train/loss': 2.547525405883789, 'validation/accuracy': 0.39907997846603394, 'validation/loss': 2.7272603511810303, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.355839252471924, 'test/num_examples': 10000, 'score': 23144.46818780899, 'total_duration': 25557.598313570023, 'accumulated_submission_time': 23144.46818780899, 'accumulated_eval_time': 2408.141443490982, 'accumulated_logging_time': 2.212597131729126, 'global_step': 49746, 'preemption_count': 0}), (50654, {'train/accuracy': 0.46113279461860657, 'train/loss': 2.4386072158813477, 'validation/accuracy': 0.4008199870586395, 'validation/loss': 2.7578697204589844, 'validation/num_examples': 50000, 'test/accuracy': 0.313400000333786, 'test/loss': 3.3625118732452393, 'test/num_examples': 10000, 'score': 23564.613875627518, 'total_duration': 26023.98063802719, 'accumulated_submission_time': 23564.613875627518, 'accumulated_eval_time': 2454.2899844646454, 'accumulated_logging_time': 2.249370574951172, 'global_step': 50654, 'preemption_count': 0}), (51560, {'train/accuracy': 0.4273632764816284, 'train/loss': 2.59151029586792, 'validation/accuracy': 0.4017999768257141, 'validation/loss': 2.755169630050659, 'validation/num_examples': 50000, 'test/accuracy': 0.314300000667572, 'test/loss': 3.3453259468078613, 'test/num_examples': 10000, 'score': 23984.93890619278, 'total_duration': 26487.907591342926, 'accumulated_submission_time': 23984.93890619278, 'accumulated_eval_time': 2497.8036675453186, 'accumulated_logging_time': 2.286559820175171, 'global_step': 51560, 'preemption_count': 0}), (52465, {'train/accuracy': 0.43562498688697815, 'train/loss': 2.548950672149658, 'validation/accuracy': 0.4038800001144409, 'validation/loss': 2.7284858226776123, 'validation/num_examples': 50000, 'test/accuracy': 0.30960002541542053, 'test/loss': 3.3761465549468994, 'test/num_examples': 10000, 'score': 24404.984939336777, 'total_duration': 26951.052780389786, 'accumulated_submission_time': 24404.984939336777, 'accumulated_eval_time': 2540.815685033798, 'accumulated_logging_time': 2.323159694671631, 'global_step': 52465, 'preemption_count': 0}), (53370, {'train/accuracy': 0.4490039050579071, 'train/loss': 2.4784021377563477, 'validation/accuracy': 0.4007599949836731, 'validation/loss': 2.747389316558838, 'validation/num_examples': 50000, 'test/accuracy': 0.3069000244140625, 'test/loss': 3.367358684539795, 'test/num_examples': 10000, 'score': 24825.079872131348, 'total_duration': 27416.082621097565, 'accumulated_submission_time': 24825.079872131348, 'accumulated_eval_time': 2585.6624703407288, 'accumulated_logging_time': 2.3613462448120117, 'global_step': 53370, 'preemption_count': 0}), (54276, {'train/accuracy': 0.4378320276737213, 'train/loss': 2.539738655090332, 'validation/accuracy': 0.4072999954223633, 'validation/loss': 2.691128969192505, 'validation/num_examples': 50000, 'test/accuracy': 0.3165000081062317, 'test/loss': 3.3001091480255127, 'test/num_examples': 10000, 'score': 25245.164329767227, 'total_duration': 27879.24419236183, 'accumulated_submission_time': 25245.164329767227, 'accumulated_eval_time': 2628.6520042419434, 'accumulated_logging_time': 2.3979926109313965, 'global_step': 54276, 'preemption_count': 0}), (55183, {'train/accuracy': 0.4406445324420929, 'train/loss': 2.527313470840454, 'validation/accuracy': 0.4094799757003784, 'validation/loss': 2.68900203704834, 'validation/num_examples': 50000, 'test/accuracy': 0.3144000172615051, 'test/loss': 3.315009593963623, 'test/num_examples': 10000, 'score': 25665.14268875122, 'total_duration': 28344.652994155884, 'accumulated_submission_time': 25665.14268875122, 'accumulated_eval_time': 2673.9957478046417, 'accumulated_logging_time': 2.4337828159332275, 'global_step': 55183, 'preemption_count': 0}), (56089, {'train/accuracy': 0.4562695324420929, 'train/loss': 2.470243453979492, 'validation/accuracy': 0.4152199923992157, 'validation/loss': 2.69305157661438, 'validation/num_examples': 50000, 'test/accuracy': 0.3237000107765198, 'test/loss': 3.3011837005615234, 'test/num_examples': 10000, 'score': 26085.327237844467, 'total_duration': 28810.783006429672, 'accumulated_submission_time': 26085.327237844467, 'accumulated_eval_time': 2719.850363969803, 'accumulated_logging_time': 2.47387957572937, 'global_step': 56089, 'preemption_count': 0}), (56995, {'train/accuracy': 0.4382421672344208, 'train/loss': 2.575744867324829, 'validation/accuracy': 0.41349998116493225, 'validation/loss': 2.7167928218841553, 'validation/num_examples': 50000, 'test/accuracy': 0.3207000195980072, 'test/loss': 3.299652099609375, 'test/num_examples': 10000, 'score': 26505.334926843643, 'total_duration': 29276.715250492096, 'accumulated_submission_time': 26505.334926843643, 'accumulated_eval_time': 2765.686028242111, 'accumulated_logging_time': 2.5117812156677246, 'global_step': 56995, 'preemption_count': 0}), (57901, {'train/accuracy': 0.44578123092651367, 'train/loss': 2.49980092048645, 'validation/accuracy': 0.41985997557640076, 'validation/loss': 2.653440475463867, 'validation/num_examples': 50000, 'test/accuracy': 0.32110002636909485, 'test/loss': 3.273627281188965, 'test/num_examples': 10000, 'score': 26925.474437713623, 'total_duration': 29741.56713962555, 'accumulated_submission_time': 26925.474437713623, 'accumulated_eval_time': 2810.3104593753815, 'accumulated_logging_time': 2.5485615730285645, 'global_step': 57901, 'preemption_count': 0}), (58809, {'train/accuracy': 0.4603320062160492, 'train/loss': 2.395381212234497, 'validation/accuracy': 0.4214800000190735, 'validation/loss': 2.604405164718628, 'validation/num_examples': 50000, 'test/accuracy': 0.33000001311302185, 'test/loss': 3.225630521774292, 'test/num_examples': 10000, 'score': 27345.811821460724, 'total_duration': 30206.088725328445, 'accumulated_submission_time': 27345.811821460724, 'accumulated_eval_time': 2854.4081478118896, 'accumulated_logging_time': 2.5851545333862305, 'global_step': 58809, 'preemption_count': 0}), (59714, {'train/accuracy': 0.4366796910762787, 'train/loss': 2.612844705581665, 'validation/accuracy': 0.4098999798297882, 'validation/loss': 2.748467445373535, 'validation/num_examples': 50000, 'test/accuracy': 0.32100000977516174, 'test/loss': 3.3422865867614746, 'test/num_examples': 10000, 'score': 27765.990793704987, 'total_duration': 30669.352987527847, 'accumulated_submission_time': 27765.990793704987, 'accumulated_eval_time': 2897.407002687454, 'accumulated_logging_time': 2.621112585067749, 'global_step': 59714, 'preemption_count': 0}), (60619, {'train/accuracy': 0.4499804675579071, 'train/loss': 2.467857837677002, 'validation/accuracy': 0.4191199839115143, 'validation/loss': 2.639002799987793, 'validation/num_examples': 50000, 'test/accuracy': 0.3256000280380249, 'test/loss': 3.250809669494629, 'test/num_examples': 10000, 'score': 28186.21174645424, 'total_duration': 31138.090316534042, 'accumulated_submission_time': 28186.21174645424, 'accumulated_eval_time': 2945.836772441864, 'accumulated_logging_time': 2.6567232608795166, 'global_step': 60619, 'preemption_count': 0}), (61528, {'train/accuracy': 0.4624609351158142, 'train/loss': 2.3810806274414062, 'validation/accuracy': 0.4293999969959259, 'validation/loss': 2.5703067779541016, 'validation/num_examples': 50000, 'test/accuracy': 0.33500000834465027, 'test/loss': 3.217402696609497, 'test/num_examples': 10000, 'score': 28606.36927008629, 'total_duration': 31602.596896648407, 'accumulated_submission_time': 28606.36927008629, 'accumulated_eval_time': 2990.0949623584747, 'accumulated_logging_time': 2.695803642272949, 'global_step': 61528, 'preemption_count': 0}), (62435, {'train/accuracy': 0.46302732825279236, 'train/loss': 2.4418976306915283, 'validation/accuracy': 0.43140000104904175, 'validation/loss': 2.5994136333465576, 'validation/num_examples': 50000, 'test/accuracy': 0.33820000290870667, 'test/loss': 3.2195005416870117, 'test/num_examples': 10000, 'score': 29026.63190627098, 'total_duration': 32064.77761387825, 'accumulated_submission_time': 29026.63190627098, 'accumulated_eval_time': 3031.917558193207, 'accumulated_logging_time': 2.7409818172454834, 'global_step': 62435, 'preemption_count': 0}), (63342, {'train/accuracy': 0.46730467677116394, 'train/loss': 2.354846954345703, 'validation/accuracy': 0.4369799792766571, 'validation/loss': 2.523207664489746, 'validation/num_examples': 50000, 'test/accuracy': 0.34130001068115234, 'test/loss': 3.1512272357940674, 'test/num_examples': 10000, 'score': 29446.979049682617, 'total_duration': 32529.076095342636, 'accumulated_submission_time': 29446.979049682617, 'accumulated_eval_time': 3075.777815580368, 'accumulated_logging_time': 2.7796471118927, 'global_step': 63342, 'preemption_count': 0}), (64248, {'train/accuracy': 0.4647851586341858, 'train/loss': 2.3934576511383057, 'validation/accuracy': 0.4303799867630005, 'validation/loss': 2.5851118564605713, 'validation/num_examples': 50000, 'test/accuracy': 0.3419000208377838, 'test/loss': 3.1868813037872314, 'test/num_examples': 10000, 'score': 29867.28816127777, 'total_duration': 32994.63881659508, 'accumulated_submission_time': 29867.28816127777, 'accumulated_eval_time': 3120.9421343803406, 'accumulated_logging_time': 2.8182005882263184, 'global_step': 64248, 'preemption_count': 0}), (65155, {'train/accuracy': 0.4700976312160492, 'train/loss': 2.4589695930480957, 'validation/accuracy': 0.4227599799633026, 'validation/loss': 2.704218626022339, 'validation/num_examples': 50000, 'test/accuracy': 0.3257000148296356, 'test/loss': 3.3210971355438232, 'test/num_examples': 10000, 'score': 30287.485833883286, 'total_duration': 33454.79567170143, 'accumulated_submission_time': 30287.485833883286, 'accumulated_eval_time': 3160.80952334404, 'accumulated_logging_time': 2.8595807552337646, 'global_step': 65155, 'preemption_count': 0}), (66059, {'train/accuracy': 0.4700585901737213, 'train/loss': 2.350682258605957, 'validation/accuracy': 0.4400999844074249, 'validation/loss': 2.517786741256714, 'validation/num_examples': 50000, 'test/accuracy': 0.34610000252723694, 'test/loss': 3.131437301635742, 'test/num_examples': 10000, 'score': 30707.519748210907, 'total_duration': 33917.77133059502, 'accumulated_submission_time': 30707.519748210907, 'accumulated_eval_time': 3203.6639914512634, 'accumulated_logging_time': 2.895918846130371, 'global_step': 66059, 'preemption_count': 0}), (66962, {'train/accuracy': 0.46162107586860657, 'train/loss': 2.3884122371673584, 'validation/accuracy': 0.4339599907398224, 'validation/loss': 2.554633617401123, 'validation/num_examples': 50000, 'test/accuracy': 0.3354000151157379, 'test/loss': 3.2074503898620605, 'test/num_examples': 10000, 'score': 31127.525916337967, 'total_duration': 34383.9345805645, 'accumulated_submission_time': 31127.525916337967, 'accumulated_eval_time': 3249.730548620224, 'accumulated_logging_time': 2.936368942260742, 'global_step': 66962, 'preemption_count': 0}), (67868, {'train/accuracy': 0.5029687285423279, 'train/loss': 2.1818504333496094, 'validation/accuracy': 0.4394199848175049, 'validation/loss': 2.515748977661133, 'validation/num_examples': 50000, 'test/accuracy': 0.3392000198364258, 'test/loss': 3.1619865894317627, 'test/num_examples': 10000, 'score': 31547.554701805115, 'total_duration': 34848.527092695236, 'accumulated_submission_time': 31547.554701805115, 'accumulated_eval_time': 3294.203216075897, 'accumulated_logging_time': 2.976597309112549, 'global_step': 67868, 'preemption_count': 0}), (68775, {'train/accuracy': 0.4720703065395355, 'train/loss': 2.326566219329834, 'validation/accuracy': 0.4456599950790405, 'validation/loss': 2.470954656600952, 'validation/num_examples': 50000, 'test/accuracy': 0.34380000829696655, 'test/loss': 3.1224875450134277, 'test/num_examples': 10000, 'score': 31967.763231039047, 'total_duration': 35311.88851070404, 'accumulated_submission_time': 31967.763231039047, 'accumulated_eval_time': 3337.2650215625763, 'accumulated_logging_time': 3.0178487300872803, 'global_step': 68775, 'preemption_count': 0}), (69682, {'train/accuracy': 0.4788476526737213, 'train/loss': 2.295114278793335, 'validation/accuracy': 0.4430199861526489, 'validation/loss': 2.4969727993011475, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.122164487838745, 'test/num_examples': 10000, 'score': 32387.879041671753, 'total_duration': 35776.63043308258, 'accumulated_submission_time': 32387.879041671753, 'accumulated_eval_time': 3381.8008399009705, 'accumulated_logging_time': 3.0573742389678955, 'global_step': 69682, 'preemption_count': 0}), (70588, {'train/accuracy': 0.4978906214237213, 'train/loss': 2.208686113357544, 'validation/accuracy': 0.44947999715805054, 'validation/loss': 2.4724645614624023, 'validation/num_examples': 50000, 'test/accuracy': 0.34620001912117004, 'test/loss': 3.092029333114624, 'test/num_examples': 10000, 'score': 32807.88691544533, 'total_duration': 36237.747968912125, 'accumulated_submission_time': 32807.88691544533, 'accumulated_eval_time': 3422.816102027893, 'accumulated_logging_time': 3.100841999053955, 'global_step': 70588, 'preemption_count': 0}), (71496, {'train/accuracy': 0.47214841842651367, 'train/loss': 2.3273391723632812, 'validation/accuracy': 0.4462999999523163, 'validation/loss': 2.475956678390503, 'validation/num_examples': 50000, 'test/accuracy': 0.34060001373291016, 'test/loss': 3.128873109817505, 'test/num_examples': 10000, 'score': 33228.21760249138, 'total_duration': 36703.45001864433, 'accumulated_submission_time': 33228.21760249138, 'accumulated_eval_time': 3468.0950469970703, 'accumulated_logging_time': 3.1422665119171143, 'global_step': 71496, 'preemption_count': 0}), (72403, {'train/accuracy': 0.4733007848262787, 'train/loss': 2.381632089614868, 'validation/accuracy': 0.44369998574256897, 'validation/loss': 2.5497748851776123, 'validation/num_examples': 50000, 'test/accuracy': 0.34870001673698425, 'test/loss': 3.151477575302124, 'test/num_examples': 10000, 'score': 33648.21060991287, 'total_duration': 37167.230764865875, 'accumulated_submission_time': 33648.21060991287, 'accumulated_eval_time': 3511.7916102409363, 'accumulated_logging_time': 3.1832900047302246, 'global_step': 72403, 'preemption_count': 0}), (73310, {'train/accuracy': 0.48337888717651367, 'train/loss': 2.329761266708374, 'validation/accuracy': 0.4386399984359741, 'validation/loss': 2.551053047180176, 'validation/num_examples': 50000, 'test/accuracy': 0.3387000262737274, 'test/loss': 3.1976158618927, 'test/num_examples': 10000, 'score': 34068.42133665085, 'total_duration': 37629.576330661774, 'accumulated_submission_time': 34068.42133665085, 'accumulated_eval_time': 3553.8385372161865, 'accumulated_logging_time': 3.2199816703796387, 'global_step': 73310, 'preemption_count': 0}), (74216, {'train/accuracy': 0.48607420921325684, 'train/loss': 2.254650354385376, 'validation/accuracy': 0.45413997769355774, 'validation/loss': 2.4290952682495117, 'validation/num_examples': 50000, 'test/accuracy': 0.3589000105857849, 'test/loss': 3.0651798248291016, 'test/num_examples': 10000, 'score': 34488.45884680748, 'total_duration': 38094.87999844551, 'accumulated_submission_time': 34488.45884680748, 'accumulated_eval_time': 3599.0153257846832, 'accumulated_logging_time': 3.2576658725738525, 'global_step': 74216, 'preemption_count': 0}), (75122, {'train/accuracy': 0.4874609410762787, 'train/loss': 2.3284049034118652, 'validation/accuracy': 0.4536999762058258, 'validation/loss': 2.506525754928589, 'validation/num_examples': 50000, 'test/accuracy': 0.3505000174045563, 'test/loss': 3.143037796020508, 'test/num_examples': 10000, 'score': 34908.47983670235, 'total_duration': 38561.67894077301, 'accumulated_submission_time': 34908.47983670235, 'accumulated_eval_time': 3645.7059197425842, 'accumulated_logging_time': 3.2945048809051514, 'global_step': 75122, 'preemption_count': 0}), (76031, {'train/accuracy': 0.49755859375, 'train/loss': 2.247817039489746, 'validation/accuracy': 0.4552599787712097, 'validation/loss': 2.4645187854766846, 'validation/num_examples': 50000, 'test/accuracy': 0.35430002212524414, 'test/loss': 3.110901117324829, 'test/num_examples': 10000, 'score': 35328.69760990143, 'total_duration': 39028.121799230576, 'accumulated_submission_time': 35328.69760990143, 'accumulated_eval_time': 3691.8415746688843, 'accumulated_logging_time': 3.3326914310455322, 'global_step': 76031, 'preemption_count': 0}), (76940, {'train/accuracy': 0.48652341961860657, 'train/loss': 2.275959014892578, 'validation/accuracy': 0.4581599831581116, 'validation/loss': 2.4188859462738037, 'validation/num_examples': 50000, 'test/accuracy': 0.35830003023147583, 'test/loss': 3.0840256214141846, 'test/num_examples': 10000, 'score': 35748.71830582619, 'total_duration': 39489.267315626144, 'accumulated_submission_time': 35748.71830582619, 'accumulated_eval_time': 3732.8744130134583, 'accumulated_logging_time': 3.374640941619873, 'global_step': 76940, 'preemption_count': 0}), (77847, {'train/accuracy': 0.5015038847923279, 'train/loss': 2.2032933235168457, 'validation/accuracy': 0.46413999795913696, 'validation/loss': 2.393378496170044, 'validation/num_examples': 50000, 'test/accuracy': 0.36670002341270447, 'test/loss': 3.028707265853882, 'test/num_examples': 10000, 'score': 36168.80930709839, 'total_duration': 39951.15533995628, 'accumulated_submission_time': 36168.80930709839, 'accumulated_eval_time': 3774.5784554481506, 'accumulated_logging_time': 3.415534734725952, 'global_step': 77847, 'preemption_count': 0}), (78755, {'train/accuracy': 0.5059765577316284, 'train/loss': 2.1536612510681152, 'validation/accuracy': 0.46639999747276306, 'validation/loss': 2.365823268890381, 'validation/num_examples': 50000, 'test/accuracy': 0.3612000048160553, 'test/loss': 3.025886297225952, 'test/num_examples': 10000, 'score': 36588.75086402893, 'total_duration': 40418.35488009453, 'accumulated_submission_time': 36588.75086402893, 'accumulated_eval_time': 3821.7439455986023, 'accumulated_logging_time': 3.457021474838257, 'global_step': 78755, 'preemption_count': 0}), (79662, {'train/accuracy': 0.5085351467132568, 'train/loss': 2.1568405628204346, 'validation/accuracy': 0.4750399887561798, 'validation/loss': 2.333594560623169, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.014451026916504, 'test/num_examples': 10000, 'score': 37009.046035289764, 'total_duration': 40879.798095703125, 'accumulated_submission_time': 37009.046035289764, 'accumulated_eval_time': 3862.8010079860687, 'accumulated_logging_time': 3.495908260345459, 'global_step': 79662, 'preemption_count': 0}), (80571, {'train/accuracy': 0.49183592200279236, 'train/loss': 2.2679104804992676, 'validation/accuracy': 0.4589399993419647, 'validation/loss': 2.433488130569458, 'validation/num_examples': 50000, 'test/accuracy': 0.358100026845932, 'test/loss': 3.056978940963745, 'test/num_examples': 10000, 'score': 37429.33586239815, 'total_duration': 41344.432941913605, 'accumulated_submission_time': 37429.33586239815, 'accumulated_eval_time': 3907.055018186569, 'accumulated_logging_time': 3.535282850265503, 'global_step': 80571, 'preemption_count': 0}), (81479, {'train/accuracy': 0.5015624761581421, 'train/loss': 2.221553087234497, 'validation/accuracy': 0.46647998690605164, 'validation/loss': 2.406338691711426, 'validation/num_examples': 50000, 'test/accuracy': 0.3653000295162201, 'test/loss': 3.051608085632324, 'test/num_examples': 10000, 'score': 37849.65655899048, 'total_duration': 41809.49540543556, 'accumulated_submission_time': 37849.65655899048, 'accumulated_eval_time': 3951.7050552368164, 'accumulated_logging_time': 3.575528621673584, 'global_step': 81479, 'preemption_count': 0}), (82386, {'train/accuracy': 0.5391015410423279, 'train/loss': 2.0318500995635986, 'validation/accuracy': 0.4758799970149994, 'validation/loss': 2.33320951461792, 'validation/num_examples': 50000, 'test/accuracy': 0.37640002369880676, 'test/loss': 2.9651641845703125, 'test/num_examples': 10000, 'score': 38269.87842488289, 'total_duration': 42269.360181331635, 'accumulated_submission_time': 38269.87842488289, 'accumulated_eval_time': 3991.2518994808197, 'accumulated_logging_time': 3.620238780975342, 'global_step': 82386, 'preemption_count': 0}), (83292, {'train/accuracy': 0.509472668170929, 'train/loss': 2.1737000942230225, 'validation/accuracy': 0.47259998321533203, 'validation/loss': 2.343513250350952, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 2.987058639526367, 'test/num_examples': 10000, 'score': 38689.997086048126, 'total_duration': 42733.49368548393, 'accumulated_submission_time': 38689.997086048126, 'accumulated_eval_time': 4035.173714160919, 'accumulated_logging_time': 3.6620676517486572, 'global_step': 83292, 'preemption_count': 0}), (84198, {'train/accuracy': 0.5048437118530273, 'train/loss': 2.2214019298553467, 'validation/accuracy': 0.46671998500823975, 'validation/loss': 2.4125983715057373, 'validation/num_examples': 50000, 'test/accuracy': 0.3580000102519989, 'test/loss': 3.0713653564453125, 'test/num_examples': 10000, 'score': 39110.11424565315, 'total_duration': 43199.31651544571, 'accumulated_submission_time': 39110.11424565315, 'accumulated_eval_time': 4080.7854421138763, 'accumulated_logging_time': 3.705613374710083, 'global_step': 84198, 'preemption_count': 0}), (85105, {'train/accuracy': 0.5434765219688416, 'train/loss': 1.9751157760620117, 'validation/accuracy': 0.48027998208999634, 'validation/loss': 2.3029189109802246, 'validation/num_examples': 50000, 'test/accuracy': 0.38270002603530884, 'test/loss': 2.939318895339966, 'test/num_examples': 10000, 'score': 39530.067018032074, 'total_duration': 43658.78203487396, 'accumulated_submission_time': 39530.067018032074, 'accumulated_eval_time': 4120.204581737518, 'accumulated_logging_time': 3.74727201461792, 'global_step': 85105, 'preemption_count': 0}), (86012, {'train/accuracy': 0.5088672041893005, 'train/loss': 2.1853909492492676, 'validation/accuracy': 0.4745599925518036, 'validation/loss': 2.349569082260132, 'validation/num_examples': 50000, 'test/accuracy': 0.36640000343322754, 'test/loss': 3.0009925365448, 'test/num_examples': 10000, 'score': 39950.27555394173, 'total_duration': 44122.84530091286, 'accumulated_submission_time': 39950.27555394173, 'accumulated_eval_time': 4163.963869810104, 'accumulated_logging_time': 3.7868733406066895, 'global_step': 86012, 'preemption_count': 0}), (86918, {'train/accuracy': 0.5166406035423279, 'train/loss': 2.1085727214813232, 'validation/accuracy': 0.4803199768066406, 'validation/loss': 2.2921464443206787, 'validation/num_examples': 50000, 'test/accuracy': 0.37160003185272217, 'test/loss': 2.941880702972412, 'test/num_examples': 10000, 'score': 40370.320439338684, 'total_duration': 44588.54420852661, 'accumulated_submission_time': 40370.320439338684, 'accumulated_eval_time': 4209.52586555481, 'accumulated_logging_time': 3.8280630111694336, 'global_step': 86918, 'preemption_count': 0}), (87822, {'train/accuracy': 0.5345312356948853, 'train/loss': 2.0101184844970703, 'validation/accuracy': 0.48437997698783875, 'validation/loss': 2.270280122756958, 'validation/num_examples': 50000, 'test/accuracy': 0.3785000145435333, 'test/loss': 2.930760622024536, 'test/num_examples': 10000, 'score': 40790.413435697556, 'total_duration': 45053.92148447037, 'accumulated_submission_time': 40790.413435697556, 'accumulated_eval_time': 4254.7183582782745, 'accumulated_logging_time': 3.8700315952301025, 'global_step': 87822, 'preemption_count': 0}), (88728, {'train/accuracy': 0.5253124833106995, 'train/loss': 2.1078312397003174, 'validation/accuracy': 0.4916599988937378, 'validation/loss': 2.2708756923675537, 'validation/num_examples': 50000, 'test/accuracy': 0.38440001010894775, 'test/loss': 2.910942554473877, 'test/num_examples': 10000, 'score': 41210.61529803276, 'total_duration': 45520.16591215134, 'accumulated_submission_time': 41210.61529803276, 'accumulated_eval_time': 4300.670683383942, 'accumulated_logging_time': 3.910280704498291, 'global_step': 88728, 'preemption_count': 0}), (89635, {'train/accuracy': 0.5248632431030273, 'train/loss': 2.0753753185272217, 'validation/accuracy': 0.48985999822616577, 'validation/loss': 2.2578797340393066, 'validation/num_examples': 50000, 'test/accuracy': 0.38280001282691956, 'test/loss': 2.9128763675689697, 'test/num_examples': 10000, 'score': 41630.89320707321, 'total_duration': 45983.5998442173, 'accumulated_submission_time': 41630.89320707321, 'accumulated_eval_time': 4343.729335069656, 'accumulated_logging_time': 3.955927610397339, 'global_step': 89635, 'preemption_count': 0}), (90538, {'train/accuracy': 0.5314648151397705, 'train/loss': 2.07725191116333, 'validation/accuracy': 0.48589998483657837, 'validation/loss': 2.3086907863616943, 'validation/num_examples': 50000, 'test/accuracy': 0.37540000677108765, 'test/loss': 2.9723060131073, 'test/num_examples': 10000, 'score': 42050.968324422836, 'total_duration': 46448.51464676857, 'accumulated_submission_time': 42050.968324422836, 'accumulated_eval_time': 4388.4740562438965, 'accumulated_logging_time': 4.000329256057739, 'global_step': 90538, 'preemption_count': 0}), (91442, {'train/accuracy': 0.5350781083106995, 'train/loss': 2.0236992835998535, 'validation/accuracy': 0.5030400156974792, 'validation/loss': 2.1932621002197266, 'validation/num_examples': 50000, 'test/accuracy': 0.394400030374527, 'test/loss': 2.8569884300231934, 'test/num_examples': 10000, 'score': 42471.20595598221, 'total_duration': 46914.04198694229, 'accumulated_submission_time': 42471.20595598221, 'accumulated_eval_time': 4433.668882369995, 'accumulated_logging_time': 4.045256614685059, 'global_step': 91442, 'preemption_count': 0}), (92349, {'train/accuracy': 0.5308398604393005, 'train/loss': 2.097571849822998, 'validation/accuracy': 0.4921799898147583, 'validation/loss': 2.277677059173584, 'validation/num_examples': 50000, 'test/accuracy': 0.3882000148296356, 'test/loss': 2.938272476196289, 'test/num_examples': 10000, 'score': 42891.15717124939, 'total_duration': 47375.598071336746, 'accumulated_submission_time': 42891.15717124939, 'accumulated_eval_time': 4475.179137229919, 'accumulated_logging_time': 4.08874249458313, 'global_step': 92349, 'preemption_count': 0}), (93254, {'train/accuracy': 0.5414257645606995, 'train/loss': 2.011777639389038, 'validation/accuracy': 0.4967799782752991, 'validation/loss': 2.2263717651367188, 'validation/num_examples': 50000, 'test/accuracy': 0.3873000144958496, 'test/loss': 2.8787736892700195, 'test/num_examples': 10000, 'score': 43311.2950565815, 'total_duration': 47839.22241187096, 'accumulated_submission_time': 43311.2950565815, 'accumulated_eval_time': 4518.571710586548, 'accumulated_logging_time': 4.132167816162109, 'global_step': 93254, 'preemption_count': 0}), (94158, {'train/accuracy': 0.5350976586341858, 'train/loss': 2.0232887268066406, 'validation/accuracy': 0.5008000135421753, 'validation/loss': 2.1791908740997314, 'validation/num_examples': 50000, 'test/accuracy': 0.395300030708313, 'test/loss': 2.828667640686035, 'test/num_examples': 10000, 'score': 43731.321390390396, 'total_duration': 48304.853842020035, 'accumulated_submission_time': 43731.321390390396, 'accumulated_eval_time': 4564.084159851074, 'accumulated_logging_time': 4.174811124801636, 'global_step': 94158, 'preemption_count': 0}), (95065, {'train/accuracy': 0.5455663800239563, 'train/loss': 1.9697304964065552, 'validation/accuracy': 0.5080199837684631, 'validation/loss': 2.1573288440704346, 'validation/num_examples': 50000, 'test/accuracy': 0.4004000127315521, 'test/loss': 2.8348753452301025, 'test/num_examples': 10000, 'score': 44151.28519535065, 'total_duration': 48770.36529827118, 'accumulated_submission_time': 44151.28519535065, 'accumulated_eval_time': 4609.533478021622, 'accumulated_logging_time': 4.222652435302734, 'global_step': 95065, 'preemption_count': 0}), (95972, {'train/accuracy': 0.5479297041893005, 'train/loss': 1.936432957649231, 'validation/accuracy': 0.5101799964904785, 'validation/loss': 2.144627571105957, 'validation/num_examples': 50000, 'test/accuracy': 0.39100003242492676, 'test/loss': 2.8307206630706787, 'test/num_examples': 10000, 'score': 44571.42405152321, 'total_duration': 49231.33609867096, 'accumulated_submission_time': 44571.42405152321, 'accumulated_eval_time': 4650.271003246307, 'accumulated_logging_time': 4.266034364700317, 'global_step': 95972, 'preemption_count': 0}), (96873, {'train/accuracy': 0.5483007431030273, 'train/loss': 1.9771265983581543, 'validation/accuracy': 0.5109999775886536, 'validation/loss': 2.1710920333862305, 'validation/num_examples': 50000, 'test/accuracy': 0.4003000259399414, 'test/loss': 2.800229549407959, 'test/num_examples': 10000, 'score': 44991.071533203125, 'total_duration': 49692.96869182587, 'accumulated_submission_time': 44991.071533203125, 'accumulated_eval_time': 4691.759567499161, 'accumulated_logging_time': 4.712217807769775, 'global_step': 96873, 'preemption_count': 0}), (97778, {'train/accuracy': 0.550976574420929, 'train/loss': 1.9703459739685059, 'validation/accuracy': 0.5161399841308594, 'validation/loss': 2.1413543224334717, 'validation/num_examples': 50000, 'test/accuracy': 0.41040003299713135, 'test/loss': 2.8006556034088135, 'test/num_examples': 10000, 'score': 45411.392758369446, 'total_duration': 50159.720798015594, 'accumulated_submission_time': 45411.392758369446, 'accumulated_eval_time': 4738.096249103546, 'accumulated_logging_time': 4.756515741348267, 'global_step': 97778, 'preemption_count': 0}), (98684, {'train/accuracy': 0.5575000047683716, 'train/loss': 1.9146215915679932, 'validation/accuracy': 0.5131999850273132, 'validation/loss': 2.1295666694641113, 'validation/num_examples': 50000, 'test/accuracy': 0.40220001339912415, 'test/loss': 2.7948946952819824, 'test/num_examples': 10000, 'score': 45831.61665439606, 'total_duration': 50624.62672114372, 'accumulated_submission_time': 45831.61665439606, 'accumulated_eval_time': 4782.682325363159, 'accumulated_logging_time': 4.801445245742798, 'global_step': 98684, 'preemption_count': 0}), (99591, {'train/accuracy': 0.587890625, 'train/loss': 1.753287434577942, 'validation/accuracy': 0.5281199812889099, 'validation/loss': 2.0528218746185303, 'validation/num_examples': 50000, 'test/accuracy': 0.41030001640319824, 'test/loss': 2.728025436401367, 'test/num_examples': 10000, 'score': 46251.78324842453, 'total_duration': 51088.31325173378, 'accumulated_submission_time': 46251.78324842453, 'accumulated_eval_time': 4826.09882068634, 'accumulated_logging_time': 4.854480981826782, 'global_step': 99591, 'preemption_count': 0}), (100497, {'train/accuracy': 0.5584765672683716, 'train/loss': 1.894142508506775, 'validation/accuracy': 0.5205199718475342, 'validation/loss': 2.0861923694610596, 'validation/num_examples': 50000, 'test/accuracy': 0.4115000069141388, 'test/loss': 2.745382308959961, 'test/num_examples': 10000, 'score': 46672.05391287804, 'total_duration': 51557.12585926056, 'accumulated_submission_time': 46672.05391287804, 'accumulated_eval_time': 4874.545390844345, 'accumulated_logging_time': 4.899670839309692, 'global_step': 100497, 'preemption_count': 0}), (101401, {'train/accuracy': 0.5635937452316284, 'train/loss': 1.8744672536849976, 'validation/accuracy': 0.5206999778747559, 'validation/loss': 2.0845861434936523, 'validation/num_examples': 50000, 'test/accuracy': 0.41590002179145813, 'test/loss': 2.720703601837158, 'test/num_examples': 10000, 'score': 47092.40252113342, 'total_duration': 52020.555797576904, 'accumulated_submission_time': 47092.40252113342, 'accumulated_eval_time': 4917.531066417694, 'accumulated_logging_time': 4.94485330581665, 'global_step': 101401, 'preemption_count': 0}), (102306, {'train/accuracy': 0.6005273461341858, 'train/loss': 1.6896134614944458, 'validation/accuracy': 0.5290799736976624, 'validation/loss': 2.0353610515594482, 'validation/num_examples': 50000, 'test/accuracy': 0.417900025844574, 'test/loss': 2.692728281021118, 'test/num_examples': 10000, 'score': 47512.490429639816, 'total_duration': 52484.20654010773, 'accumulated_submission_time': 47512.490429639816, 'accumulated_eval_time': 4961.000032663345, 'accumulated_logging_time': 4.988691568374634, 'global_step': 102306, 'preemption_count': 0}), (103213, {'train/accuracy': 0.5564843416213989, 'train/loss': 1.9020169973373413, 'validation/accuracy': 0.5239399671554565, 'validation/loss': 2.0747263431549072, 'validation/num_examples': 50000, 'test/accuracy': 0.40700000524520874, 'test/loss': 2.7468771934509277, 'test/num_examples': 10000, 'score': 47932.62344169617, 'total_duration': 52944.69896483421, 'accumulated_submission_time': 47932.62344169617, 'accumulated_eval_time': 5001.267645597458, 'accumulated_logging_time': 5.030147075653076, 'global_step': 103213, 'preemption_count': 0}), (104120, {'train/accuracy': 0.5759570002555847, 'train/loss': 1.8427623510360718, 'validation/accuracy': 0.5297799706459045, 'validation/loss': 2.0636205673217773, 'validation/num_examples': 50000, 'test/accuracy': 0.41880002617836, 'test/loss': 2.700232744216919, 'test/num_examples': 10000, 'score': 48352.95508027077, 'total_duration': 53410.000838279724, 'accumulated_submission_time': 48352.95508027077, 'accumulated_eval_time': 5046.1425104141235, 'accumulated_logging_time': 5.07442831993103, 'global_step': 104120, 'preemption_count': 0}), (105028, {'train/accuracy': 0.5927929282188416, 'train/loss': 1.7482671737670898, 'validation/accuracy': 0.5322999954223633, 'validation/loss': 2.0392508506774902, 'validation/num_examples': 50000, 'test/accuracy': 0.4181000292301178, 'test/loss': 2.695244312286377, 'test/num_examples': 10000, 'score': 48773.32455801964, 'total_duration': 53875.408218860626, 'accumulated_submission_time': 48773.32455801964, 'accumulated_eval_time': 5091.086785554886, 'accumulated_logging_time': 5.116716623306274, 'global_step': 105028, 'preemption_count': 0}), (105936, {'train/accuracy': 0.571972668170929, 'train/loss': 1.8334988355636597, 'validation/accuracy': 0.5362200140953064, 'validation/loss': 2.0098884105682373, 'validation/num_examples': 50000, 'test/accuracy': 0.42240002751350403, 'test/loss': 2.671083688735962, 'test/num_examples': 10000, 'score': 49193.346816301346, 'total_duration': 54339.73200273514, 'accumulated_submission_time': 49193.346816301346, 'accumulated_eval_time': 5135.2920553684235, 'accumulated_logging_time': 5.161932468414307, 'global_step': 105936, 'preemption_count': 0}), (106841, {'train/accuracy': 0.5798632502555847, 'train/loss': 1.801438570022583, 'validation/accuracy': 0.5390200018882751, 'validation/loss': 2.0081238746643066, 'validation/num_examples': 50000, 'test/accuracy': 0.4261000156402588, 'test/loss': 2.6580498218536377, 'test/num_examples': 10000, 'score': 49613.69072461128, 'total_duration': 54801.62851881981, 'accumulated_submission_time': 49613.69072461128, 'accumulated_eval_time': 5176.745602607727, 'accumulated_logging_time': 5.209371089935303, 'global_step': 106841, 'preemption_count': 0}), (107744, {'train/accuracy': 0.5798437595367432, 'train/loss': 1.8626606464385986, 'validation/accuracy': 0.5293999910354614, 'validation/loss': 2.1074492931365967, 'validation/num_examples': 50000, 'test/accuracy': 0.41780000925064087, 'test/loss': 2.7498693466186523, 'test/num_examples': 10000, 'score': 50034.01110982895, 'total_duration': 55269.35957503319, 'accumulated_submission_time': 50034.01110982895, 'accumulated_eval_time': 5224.058357954025, 'accumulated_logging_time': 5.257244348526001, 'global_step': 107744, 'preemption_count': 0}), (108651, {'train/accuracy': 0.5818749666213989, 'train/loss': 1.7912267446517944, 'validation/accuracy': 0.5414400100708008, 'validation/loss': 1.9961179494857788, 'validation/num_examples': 50000, 'test/accuracy': 0.4240000247955322, 'test/loss': 2.6577701568603516, 'test/num_examples': 10000, 'score': 50454.28623723984, 'total_duration': 55730.48776316643, 'accumulated_submission_time': 50454.28623723984, 'accumulated_eval_time': 5264.811240434647, 'accumulated_logging_time': 5.306406021118164, 'global_step': 108651, 'preemption_count': 0}), (109557, {'train/accuracy': 0.5807812213897705, 'train/loss': 1.7843594551086426, 'validation/accuracy': 0.5475599765777588, 'validation/loss': 1.9676730632781982, 'validation/num_examples': 50000, 'test/accuracy': 0.4294000267982483, 'test/loss': 2.6383376121520996, 'test/num_examples': 10000, 'score': 50874.582459926605, 'total_duration': 56190.437881946564, 'accumulated_submission_time': 50874.582459926605, 'accumulated_eval_time': 5304.367951393127, 'accumulated_logging_time': 5.35333776473999, 'global_step': 109557, 'preemption_count': 0}), (110464, {'train/accuracy': 0.5948437452316284, 'train/loss': 1.731536626815796, 'validation/accuracy': 0.5445600152015686, 'validation/loss': 1.9790481328964233, 'validation/num_examples': 50000, 'test/accuracy': 0.43150001764297485, 'test/loss': 2.643044948577881, 'test/num_examples': 10000, 'score': 51294.605360507965, 'total_duration': 56655.322309970856, 'accumulated_submission_time': 51294.605360507965, 'accumulated_eval_time': 5349.13404250145, 'accumulated_logging_time': 5.398436546325684, 'global_step': 110464, 'preemption_count': 0}), (111371, {'train/accuracy': 0.5899804830551147, 'train/loss': 1.7508074045181274, 'validation/accuracy': 0.555079996585846, 'validation/loss': 1.9349216222763062, 'validation/num_examples': 50000, 'test/accuracy': 0.4382000267505646, 'test/loss': 2.608139991760254, 'test/num_examples': 10000, 'score': 51714.772094249725, 'total_duration': 57115.24357366562, 'accumulated_submission_time': 51714.772094249725, 'accumulated_eval_time': 5388.784100055695, 'accumulated_logging_time': 5.452826261520386, 'global_step': 111371, 'preemption_count': 0}), (112281, {'train/accuracy': 0.5955273509025574, 'train/loss': 1.7364590167999268, 'validation/accuracy': 0.5569800138473511, 'validation/loss': 1.9330426454544067, 'validation/num_examples': 50000, 'test/accuracy': 0.44130003452301025, 'test/loss': 2.598820447921753, 'test/num_examples': 10000, 'score': 52135.0346596241, 'total_duration': 57580.20839238167, 'accumulated_submission_time': 52135.0346596241, 'accumulated_eval_time': 5433.386849164963, 'accumulated_logging_time': 5.500851154327393, 'global_step': 112281, 'preemption_count': 0}), (113187, {'train/accuracy': 0.6030077934265137, 'train/loss': 1.6891416311264038, 'validation/accuracy': 0.5582199692726135, 'validation/loss': 1.9116791486740112, 'validation/num_examples': 50000, 'test/accuracy': 0.44210001826286316, 'test/loss': 2.5687875747680664, 'test/num_examples': 10000, 'score': 52555.18597626686, 'total_duration': 58046.9350438118, 'accumulated_submission_time': 52555.18597626686, 'accumulated_eval_time': 5479.865357398987, 'accumulated_logging_time': 5.547860145568848, 'global_step': 113187, 'preemption_count': 0}), (114093, {'train/accuracy': 0.5959765315055847, 'train/loss': 1.7394243478775024, 'validation/accuracy': 0.5590400099754333, 'validation/loss': 1.9202549457550049, 'validation/num_examples': 50000, 'test/accuracy': 0.4409000277519226, 'test/loss': 2.6083571910858154, 'test/num_examples': 10000, 'score': 52975.51297545433, 'total_duration': 58514.05428671837, 'accumulated_submission_time': 52975.51297545433, 'accumulated_eval_time': 5526.562952518463, 'accumulated_logging_time': 5.59241247177124, 'global_step': 114093, 'preemption_count': 0}), (114999, {'train/accuracy': 0.5999413728713989, 'train/loss': 1.698953628540039, 'validation/accuracy': 0.5589599609375, 'validation/loss': 1.9015834331512451, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.568891763687134, 'test/num_examples': 10000, 'score': 53395.86148428917, 'total_duration': 58980.18489718437, 'accumulated_submission_time': 53395.86148428917, 'accumulated_eval_time': 5572.246104717255, 'accumulated_logging_time': 5.641361236572266, 'global_step': 114999, 'preemption_count': 0}), (115903, {'train/accuracy': 0.6122460961341858, 'train/loss': 1.6250638961791992, 'validation/accuracy': 0.5637800097465515, 'validation/loss': 1.8601083755493164, 'validation/num_examples': 50000, 'test/accuracy': 0.44770002365112305, 'test/loss': 2.5321247577667236, 'test/num_examples': 10000, 'score': 53815.892790317535, 'total_duration': 59443.194900512695, 'accumulated_submission_time': 53815.892790317535, 'accumulated_eval_time': 5615.125262737274, 'accumulated_logging_time': 5.690145254135132, 'global_step': 115903, 'preemption_count': 0}), (116809, {'train/accuracy': 0.6307226419448853, 'train/loss': 1.5751163959503174, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8742843866348267, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.542642116546631, 'test/num_examples': 10000, 'score': 54236.21877121925, 'total_duration': 59905.71079039574, 'accumulated_submission_time': 54236.21877121925, 'accumulated_eval_time': 5657.219936609268, 'accumulated_logging_time': 5.732917547225952, 'global_step': 116809, 'preemption_count': 0}), (117715, {'train/accuracy': 0.606738269329071, 'train/loss': 1.6873126029968262, 'validation/accuracy': 0.5666399598121643, 'validation/loss': 1.881292462348938, 'validation/num_examples': 50000, 'test/accuracy': 0.4442000091075897, 'test/loss': 2.5546884536743164, 'test/num_examples': 10000, 'score': 54656.47298908234, 'total_duration': 60368.68414545059, 'accumulated_submission_time': 54656.47298908234, 'accumulated_eval_time': 5699.84024477005, 'accumulated_logging_time': 5.780933141708374, 'global_step': 117715, 'preemption_count': 0}), (118621, {'train/accuracy': 0.6252343654632568, 'train/loss': 1.57450532913208, 'validation/accuracy': 0.5776000022888184, 'validation/loss': 1.8012522459030151, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.4840798377990723, 'test/num_examples': 10000, 'score': 55076.772084236145, 'total_duration': 60832.36305522919, 'accumulated_submission_time': 55076.772084236145, 'accumulated_eval_time': 5743.124422311783, 'accumulated_logging_time': 5.825575351715088, 'global_step': 118621, 'preemption_count': 0}), (119523, {'train/accuracy': 0.6426367163658142, 'train/loss': 1.501293420791626, 'validation/accuracy': 0.5740599632263184, 'validation/loss': 1.8339369297027588, 'validation/num_examples': 50000, 'test/accuracy': 0.45670002698898315, 'test/loss': 2.4974396228790283, 'test/num_examples': 10000, 'score': 55496.693836927414, 'total_duration': 61297.40745139122, 'accumulated_submission_time': 55496.693836927414, 'accumulated_eval_time': 5788.15007519722, 'accumulated_logging_time': 5.872597932815552, 'global_step': 119523, 'preemption_count': 0}), (120427, {'train/accuracy': 0.6160937547683716, 'train/loss': 1.6148782968521118, 'validation/accuracy': 0.5806800127029419, 'validation/loss': 1.8061535358428955, 'validation/num_examples': 50000, 'test/accuracy': 0.4561000168323517, 'test/loss': 2.485478162765503, 'test/num_examples': 10000, 'score': 55916.784044504166, 'total_duration': 61762.35899710655, 'accumulated_submission_time': 55916.784044504166, 'accumulated_eval_time': 5832.912647247314, 'accumulated_logging_time': 5.921623706817627, 'global_step': 120427, 'preemption_count': 0}), (121333, {'train/accuracy': 0.6248632669448853, 'train/loss': 1.5781731605529785, 'validation/accuracy': 0.5776399970054626, 'validation/loss': 1.8077296018600464, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.4872207641601562, 'test/num_examples': 10000, 'score': 56336.82581615448, 'total_duration': 62230.889113903046, 'accumulated_submission_time': 56336.82581615448, 'accumulated_eval_time': 5881.302688598633, 'accumulated_logging_time': 5.969411849975586, 'global_step': 121333, 'preemption_count': 0}), (122237, {'train/accuracy': 0.6359570026397705, 'train/loss': 1.5680468082427979, 'validation/accuracy': 0.5774999856948853, 'validation/loss': 1.843104600906372, 'validation/num_examples': 50000, 'test/accuracy': 0.45840001106262207, 'test/loss': 2.5103447437286377, 'test/num_examples': 10000, 'score': 56756.91334247589, 'total_duration': 62695.95139026642, 'accumulated_submission_time': 56756.91334247589, 'accumulated_eval_time': 5926.17863202095, 'accumulated_logging_time': 6.0188164710998535, 'global_step': 122237, 'preemption_count': 0}), (123144, {'train/accuracy': 0.6289648413658142, 'train/loss': 1.563175916671753, 'validation/accuracy': 0.5860599875450134, 'validation/loss': 1.769154667854309, 'validation/num_examples': 50000, 'test/accuracy': 0.4677000343799591, 'test/loss': 2.4270832538604736, 'test/num_examples': 10000, 'score': 57177.12469792366, 'total_duration': 63158.04018783569, 'accumulated_submission_time': 57177.12469792366, 'accumulated_eval_time': 5967.953478097916, 'accumulated_logging_time': 6.071156740188599, 'global_step': 123144, 'preemption_count': 0}), (124049, {'train/accuracy': 0.640429675579071, 'train/loss': 1.5160832405090332, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.7468619346618652, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4021661281585693, 'test/num_examples': 10000, 'score': 57597.355749607086, 'total_duration': 63624.529782772064, 'accumulated_submission_time': 57597.355749607086, 'accumulated_eval_time': 6014.114964962006, 'accumulated_logging_time': 6.117965459823608, 'global_step': 124049, 'preemption_count': 0}), (124954, {'train/accuracy': 0.6468749642372131, 'train/loss': 1.47525954246521, 'validation/accuracy': 0.5937199592590332, 'validation/loss': 1.733345627784729, 'validation/num_examples': 50000, 'test/accuracy': 0.46730002760887146, 'test/loss': 2.402538537979126, 'test/num_examples': 10000, 'score': 58017.52013874054, 'total_duration': 64090.95868706703, 'accumulated_submission_time': 58017.52013874054, 'accumulated_eval_time': 6060.277943134308, 'accumulated_logging_time': 6.1684510707855225, 'global_step': 124954, 'preemption_count': 0}), (125858, {'train/accuracy': 0.6384179592132568, 'train/loss': 1.4969202280044556, 'validation/accuracy': 0.5963000059127808, 'validation/loss': 1.702694058418274, 'validation/num_examples': 50000, 'test/accuracy': 0.47540003061294556, 'test/loss': 2.3764123916625977, 'test/num_examples': 10000, 'score': 58437.53057861328, 'total_duration': 64555.04180955887, 'accumulated_submission_time': 58437.53057861328, 'accumulated_eval_time': 6104.252175807953, 'accumulated_logging_time': 6.2157580852508545, 'global_step': 125858, 'preemption_count': 0}), (126763, {'train/accuracy': 0.6460155844688416, 'train/loss': 1.478101372718811, 'validation/accuracy': 0.6011399626731873, 'validation/loss': 1.703931212425232, 'validation/num_examples': 50000, 'test/accuracy': 0.4791000187397003, 'test/loss': 2.3670125007629395, 'test/num_examples': 10000, 'score': 58857.83850765228, 'total_duration': 65020.412427186966, 'accumulated_submission_time': 58857.83850765228, 'accumulated_eval_time': 6149.2145137786865, 'accumulated_logging_time': 6.2652997970581055, 'global_step': 126763, 'preemption_count': 0}), (127669, {'train/accuracy': 0.6504492163658142, 'train/loss': 1.4836300611495972, 'validation/accuracy': 0.5961999893188477, 'validation/loss': 1.7273157835006714, 'validation/num_examples': 50000, 'test/accuracy': 0.4782000184059143, 'test/loss': 2.3896336555480957, 'test/num_examples': 10000, 'score': 59278.021849155426, 'total_duration': 65488.41676783562, 'accumulated_submission_time': 59278.021849155426, 'accumulated_eval_time': 6196.937285423279, 'accumulated_logging_time': 6.313244581222534, 'global_step': 127669, 'preemption_count': 0}), (128576, {'train/accuracy': 0.6483983993530273, 'train/loss': 1.4797208309173584, 'validation/accuracy': 0.6062399744987488, 'validation/loss': 1.6872668266296387, 'validation/num_examples': 50000, 'test/accuracy': 0.4870000183582306, 'test/loss': 2.3387036323547363, 'test/num_examples': 10000, 'score': 59698.199717760086, 'total_duration': 65953.0438401699, 'accumulated_submission_time': 59698.199717760086, 'accumulated_eval_time': 6241.280877828598, 'accumulated_logging_time': 6.368543863296509, 'global_step': 128576, 'preemption_count': 0}), (129480, {'train/accuracy': 0.6539648175239563, 'train/loss': 1.4522738456726074, 'validation/accuracy': 0.605679988861084, 'validation/loss': 1.6788485050201416, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.354218006134033, 'test/num_examples': 10000, 'score': 60118.538821697235, 'total_duration': 66419.9552268982, 'accumulated_submission_time': 60118.538821697235, 'accumulated_eval_time': 6287.754509687424, 'accumulated_logging_time': 6.416506052017212, 'global_step': 129480, 'preemption_count': 0}), (130386, {'train/accuracy': 0.6702734231948853, 'train/loss': 1.4016005992889404, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.6603667736053467, 'validation/num_examples': 50000, 'test/accuracy': 0.49220001697540283, 'test/loss': 2.3320703506469727, 'test/num_examples': 10000, 'score': 60538.44319176674, 'total_duration': 66885.50264811516, 'accumulated_submission_time': 60538.44319176674, 'accumulated_eval_time': 6333.296562671661, 'accumulated_logging_time': 6.466786623001099, 'global_step': 130386, 'preemption_count': 0}), (131293, {'train/accuracy': 0.6552929282188416, 'train/loss': 1.4327030181884766, 'validation/accuracy': 0.6084200143814087, 'validation/loss': 1.6539702415466309, 'validation/num_examples': 50000, 'test/accuracy': 0.49070003628730774, 'test/loss': 2.317131996154785, 'test/num_examples': 10000, 'score': 60958.65554857254, 'total_duration': 67347.1164739132, 'accumulated_submission_time': 60958.65554857254, 'accumulated_eval_time': 6374.600378513336, 'accumulated_logging_time': 6.51424241065979, 'global_step': 131293, 'preemption_count': 0}), (132198, {'train/accuracy': 0.6626366972923279, 'train/loss': 1.4006588459014893, 'validation/accuracy': 0.6137999892234802, 'validation/loss': 1.6258078813552856, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3008532524108887, 'test/num_examples': 10000, 'score': 61378.65146899223, 'total_duration': 67813.22391462326, 'accumulated_submission_time': 61378.65146899223, 'accumulated_eval_time': 6420.612956047058, 'accumulated_logging_time': 6.562076568603516, 'global_step': 132198, 'preemption_count': 0}), (133104, {'train/accuracy': 0.6655077934265137, 'train/loss': 1.4070724248886108, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.652784824371338, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3164613246917725, 'test/num_examples': 10000, 'score': 61798.642517089844, 'total_duration': 68278.38157367706, 'accumulated_submission_time': 61798.642517089844, 'accumulated_eval_time': 6465.681235074997, 'accumulated_logging_time': 6.609776496887207, 'global_step': 133104, 'preemption_count': 0}), (134008, {'train/accuracy': 0.6812499761581421, 'train/loss': 1.345516324043274, 'validation/accuracy': 0.6206799745559692, 'validation/loss': 1.6100428104400635, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.2720818519592285, 'test/num_examples': 10000, 'score': 62218.546484708786, 'total_duration': 68742.47394442558, 'accumulated_submission_time': 62218.546484708786, 'accumulated_eval_time': 6509.769501447678, 'accumulated_logging_time': 6.65800929069519, 'global_step': 134008, 'preemption_count': 0}), (134913, {'train/accuracy': 0.6734570264816284, 'train/loss': 1.3531352281570435, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.563097596168518, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.2082858085632324, 'test/num_examples': 10000, 'score': 62638.877262592316, 'total_duration': 69209.31068348885, 'accumulated_submission_time': 62638.877262592316, 'accumulated_eval_time': 6556.1755702495575, 'accumulated_logging_time': 6.708175182342529, 'global_step': 134913, 'preemption_count': 0}), (135817, {'train/accuracy': 0.6738085746765137, 'train/loss': 1.3599903583526611, 'validation/accuracy': 0.6240599751472473, 'validation/loss': 1.5829092264175415, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.244506359100342, 'test/num_examples': 10000, 'score': 63059.16834306717, 'total_duration': 69672.13042116165, 'accumulated_submission_time': 63059.16834306717, 'accumulated_eval_time': 6598.604954004288, 'accumulated_logging_time': 6.757187128067017, 'global_step': 135817, 'preemption_count': 0}), (136722, {'train/accuracy': 0.7079687118530273, 'train/loss': 1.2369790077209473, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.5897619724273682, 'validation/num_examples': 50000, 'test/accuracy': 0.5020000338554382, 'test/loss': 2.246837615966797, 'test/num_examples': 10000, 'score': 63479.30086803436, 'total_duration': 70138.27771234512, 'accumulated_submission_time': 63479.30086803436, 'accumulated_eval_time': 6644.519300699234, 'accumulated_logging_time': 6.807169198989868, 'global_step': 136722, 'preemption_count': 0}), (137630, {'train/accuracy': 0.675585925579071, 'train/loss': 1.340919017791748, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.5627877712249756, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.199458122253418, 'test/num_examples': 10000, 'score': 63899.45568084717, 'total_duration': 70599.13270401955, 'accumulated_submission_time': 63899.45568084717, 'accumulated_eval_time': 6685.11887216568, 'accumulated_logging_time': 6.857578992843628, 'global_step': 137630, 'preemption_count': 0}), (138533, {'train/accuracy': 0.6859960556030273, 'train/loss': 1.2904889583587646, 'validation/accuracy': 0.6377399563789368, 'validation/loss': 1.5166523456573486, 'validation/num_examples': 50000, 'test/accuracy': 0.5120000243186951, 'test/loss': 2.1723082065582275, 'test/num_examples': 10000, 'score': 64319.47856760025, 'total_duration': 71062.68217468262, 'accumulated_submission_time': 64319.47856760025, 'accumulated_eval_time': 6728.542403936386, 'accumulated_logging_time': 6.904958009719849, 'global_step': 138533, 'preemption_count': 0}), (139435, {'train/accuracy': 0.6988281011581421, 'train/loss': 1.2646912336349487, 'validation/accuracy': 0.6321799755096436, 'validation/loss': 1.5676641464233398, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.2396981716156006, 'test/num_examples': 10000, 'score': 64739.70188331604, 'total_duration': 71527.2613837719, 'accumulated_submission_time': 64739.70188331604, 'accumulated_eval_time': 6772.7980988025665, 'accumulated_logging_time': 6.95450758934021, 'global_step': 139435, 'preemption_count': 0}), (140342, {'train/accuracy': 0.6906836032867432, 'train/loss': 1.2769279479980469, 'validation/accuracy': 0.6407399773597717, 'validation/loss': 1.5057283639907837, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.154827356338501, 'test/num_examples': 10000, 'score': 65159.62976360321, 'total_duration': 71991.70762300491, 'accumulated_submission_time': 65159.62976360321, 'accumulated_eval_time': 6817.2175986766815, 'accumulated_logging_time': 7.002084493637085, 'global_step': 140342, 'preemption_count': 0}), (141247, {'train/accuracy': 0.69837886095047, 'train/loss': 1.2515182495117188, 'validation/accuracy': 0.6464599967002869, 'validation/loss': 1.4908294677734375, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1413466930389404, 'test/num_examples': 10000, 'score': 65579.96733903885, 'total_duration': 72457.2199523449, 'accumulated_submission_time': 65579.96733903885, 'accumulated_eval_time': 6862.2890038490295, 'accumulated_logging_time': 7.054901123046875, 'global_step': 141247, 'preemption_count': 0}), (142151, {'train/accuracy': 0.7085741758346558, 'train/loss': 1.1992639303207397, 'validation/accuracy': 0.6479600071907043, 'validation/loss': 1.4763096570968628, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.128354549407959, 'test/num_examples': 10000, 'score': 66000.20277547836, 'total_duration': 72922.64905381203, 'accumulated_submission_time': 66000.20277547836, 'accumulated_eval_time': 6907.3794157505035, 'accumulated_logging_time': 7.107923269271851, 'global_step': 142151, 'preemption_count': 0}), (143055, {'train/accuracy': 0.7023242115974426, 'train/loss': 1.2293777465820312, 'validation/accuracy': 0.6479600071907043, 'validation/loss': 1.4754472970962524, 'validation/num_examples': 50000, 'test/accuracy': 0.5277000069618225, 'test/loss': 2.1308555603027344, 'test/num_examples': 10000, 'score': 66420.13485479355, 'total_duration': 73390.75317406654, 'accumulated_submission_time': 66420.13485479355, 'accumulated_eval_time': 6955.451377868652, 'accumulated_logging_time': 7.158038854598999, 'global_step': 143055, 'preemption_count': 0}), (143961, {'train/accuracy': 0.70570307970047, 'train/loss': 1.21626615524292, 'validation/accuracy': 0.6527599692344666, 'validation/loss': 1.4616146087646484, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.108567237854004, 'test/num_examples': 10000, 'score': 66840.06156110764, 'total_duration': 73856.33251214027, 'accumulated_submission_time': 66840.06156110764, 'accumulated_eval_time': 7001.00201010704, 'accumulated_logging_time': 7.209303379058838, 'global_step': 143961, 'preemption_count': 0}), (144867, {'train/accuracy': 0.7190039157867432, 'train/loss': 1.1549161672592163, 'validation/accuracy': 0.6559000015258789, 'validation/loss': 1.4345026016235352, 'validation/num_examples': 50000, 'test/accuracy': 0.534500002861023, 'test/loss': 2.0833964347839355, 'test/num_examples': 10000, 'score': 67260.06018710136, 'total_duration': 74321.73847436905, 'accumulated_submission_time': 67260.06018710136, 'accumulated_eval_time': 7046.307956695557, 'accumulated_logging_time': 7.260730981826782, 'global_step': 144867, 'preemption_count': 0}), (145774, {'train/accuracy': 0.7044140696525574, 'train/loss': 1.2267926931381226, 'validation/accuracy': 0.6544199585914612, 'validation/loss': 1.4555846452713013, 'validation/num_examples': 50000, 'test/accuracy': 0.5312000513076782, 'test/loss': 2.109943389892578, 'test/num_examples': 10000, 'score': 67680.40997552872, 'total_duration': 74787.97717380524, 'accumulated_submission_time': 67680.40997552872, 'accumulated_eval_time': 7092.093881845474, 'accumulated_logging_time': 7.313398361206055, 'global_step': 145774, 'preemption_count': 0}), (146680, {'train/accuracy': 0.7185156345367432, 'train/loss': 1.158596158027649, 'validation/accuracy': 0.6607599854469299, 'validation/loss': 1.4224454164505005, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.0639452934265137, 'test/num_examples': 10000, 'score': 68100.34601187706, 'total_duration': 75256.75840878487, 'accumulated_submission_time': 68100.34601187706, 'accumulated_eval_time': 7140.838541984558, 'accumulated_logging_time': 7.363656282424927, 'global_step': 146680, 'preemption_count': 0}), (147585, {'train/accuracy': 0.7215625047683716, 'train/loss': 1.1436489820480347, 'validation/accuracy': 0.6613399982452393, 'validation/loss': 1.4184284210205078, 'validation/num_examples': 50000, 'test/accuracy': 0.541700005531311, 'test/loss': 2.0514132976531982, 'test/num_examples': 10000, 'score': 68520.35908174515, 'total_duration': 75718.0649137497, 'accumulated_submission_time': 68520.35908174515, 'accumulated_eval_time': 7182.027892827988, 'accumulated_logging_time': 7.416749477386475, 'global_step': 147585, 'preemption_count': 0}), (148493, {'train/accuracy': 0.7193359136581421, 'train/loss': 1.1469899415969849, 'validation/accuracy': 0.667140007019043, 'validation/loss': 1.39079749584198, 'validation/num_examples': 50000, 'test/accuracy': 0.5446000099182129, 'test/loss': 2.0252130031585693, 'test/num_examples': 10000, 'score': 68940.4190402031, 'total_duration': 76183.35967731476, 'accumulated_submission_time': 68940.4190402031, 'accumulated_eval_time': 7227.158820390701, 'accumulated_logging_time': 7.469902276992798, 'global_step': 148493, 'preemption_count': 0}), (149400, {'train/accuracy': 0.7261328101158142, 'train/loss': 1.1292051076889038, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3793786764144897, 'validation/num_examples': 50000, 'test/accuracy': 0.542900025844574, 'test/loss': 2.033937692642212, 'test/num_examples': 10000, 'score': 69360.67560601234, 'total_duration': 76649.3745880127, 'accumulated_submission_time': 69360.67560601234, 'accumulated_eval_time': 7272.813814640045, 'accumulated_logging_time': 7.522792100906372, 'global_step': 149400, 'preemption_count': 0}), (150306, {'train/accuracy': 0.7345898151397705, 'train/loss': 1.0911519527435303, 'validation/accuracy': 0.6721599698066711, 'validation/loss': 1.3719079494476318, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0280535221099854, 'test/num_examples': 10000, 'score': 69780.98648524284, 'total_duration': 77111.10426402092, 'accumulated_submission_time': 69780.98648524284, 'accumulated_eval_time': 7314.125680685043, 'accumulated_logging_time': 7.57967209815979, 'global_step': 150306, 'preemption_count': 0}), (151210, {'train/accuracy': 0.7315624952316284, 'train/loss': 1.0995270013809204, 'validation/accuracy': 0.6723600029945374, 'validation/loss': 1.3750596046447754, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.0155558586120605, 'test/num_examples': 10000, 'score': 70201.0912911892, 'total_duration': 77577.59732437134, 'accumulated_submission_time': 70201.0912911892, 'accumulated_eval_time': 7360.41232419014, 'accumulated_logging_time': 7.6300084590911865, 'global_step': 151210, 'preemption_count': 0}), (152116, {'train/accuracy': 0.7346875071525574, 'train/loss': 1.0947026014328003, 'validation/accuracy': 0.6771000027656555, 'validation/loss': 1.352385401725769, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 1.9908647537231445, 'test/num_examples': 10000, 'score': 70620.99999403954, 'total_duration': 78043.17117524147, 'accumulated_submission_time': 70620.99999403954, 'accumulated_eval_time': 7405.974725008011, 'accumulated_logging_time': 7.682317733764648, 'global_step': 152116, 'preemption_count': 0}), (153020, {'train/accuracy': 0.7451757788658142, 'train/loss': 1.0271271467208862, 'validation/accuracy': 0.6819199919700623, 'validation/loss': 1.3150537014007568, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 1.9496073722839355, 'test/num_examples': 10000, 'score': 71041.29298949242, 'total_duration': 78505.25744843483, 'accumulated_submission_time': 71041.29298949242, 'accumulated_eval_time': 7447.667580366135, 'accumulated_logging_time': 7.731944561004639, 'global_step': 153020, 'preemption_count': 0}), (153922, {'train/accuracy': 0.758496105670929, 'train/loss': 0.9787788391113281, 'validation/accuracy': 0.6845600008964539, 'validation/loss': 1.3084824085235596, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 1.9502512216567993, 'test/num_examples': 10000, 'score': 71461.58991360664, 'total_duration': 78968.2080783844, 'accumulated_submission_time': 71461.58991360664, 'accumulated_eval_time': 7490.222132205963, 'accumulated_logging_time': 7.7806432247161865, 'global_step': 153922, 'preemption_count': 0}), (154829, {'train/accuracy': 0.7406249642372131, 'train/loss': 1.0503544807434082, 'validation/accuracy': 0.6873799562454224, 'validation/loss': 1.2965481281280518, 'validation/num_examples': 50000, 'test/accuracy': 0.5617000460624695, 'test/loss': 1.9382882118225098, 'test/num_examples': 10000, 'score': 71881.67134642601, 'total_duration': 79435.82341265678, 'accumulated_submission_time': 71881.67134642601, 'accumulated_eval_time': 7537.654074668884, 'accumulated_logging_time': 7.830984830856323, 'global_step': 154829, 'preemption_count': 0}), (155736, {'train/accuracy': 0.7502343654632568, 'train/loss': 1.0065470933914185, 'validation/accuracy': 0.687559962272644, 'validation/loss': 1.2907850742340088, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.922777771949768, 'test/num_examples': 10000, 'score': 72301.95320534706, 'total_duration': 79901.12423586845, 'accumulated_submission_time': 72301.95320534706, 'accumulated_eval_time': 7582.568460941315, 'accumulated_logging_time': 7.88397216796875, 'global_step': 155736, 'preemption_count': 0}), (156641, {'train/accuracy': 0.76318359375, 'train/loss': 0.952807605266571, 'validation/accuracy': 0.6926800012588501, 'validation/loss': 1.2713115215301514, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 1.9105957746505737, 'test/num_examples': 10000, 'score': 72721.92038750648, 'total_duration': 80366.11267876625, 'accumulated_submission_time': 72721.92038750648, 'accumulated_eval_time': 7627.477694272995, 'accumulated_logging_time': 7.9458537101745605, 'global_step': 156641, 'preemption_count': 0}), (157547, {'train/accuracy': 0.7520703077316284, 'train/loss': 1.0017552375793457, 'validation/accuracy': 0.6958400011062622, 'validation/loss': 1.251082420349121, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.8889102935791016, 'test/num_examples': 10000, 'score': 73141.96051359177, 'total_duration': 80832.09207010269, 'accumulated_submission_time': 73141.96051359177, 'accumulated_eval_time': 7673.310721158981, 'accumulated_logging_time': 8.001603126525879, 'global_step': 157547, 'preemption_count': 0}), (158452, {'train/accuracy': 0.7565234303474426, 'train/loss': 0.9877767562866211, 'validation/accuracy': 0.6963799595832825, 'validation/loss': 1.252785563468933, 'validation/num_examples': 50000, 'test/accuracy': 0.5720000267028809, 'test/loss': 1.8924669027328491, 'test/num_examples': 10000, 'score': 73562.28885316849, 'total_duration': 81292.98815059662, 'accumulated_submission_time': 73562.28885316849, 'accumulated_eval_time': 7713.773415803909, 'accumulated_logging_time': 8.056279420852661, 'global_step': 158452, 'preemption_count': 0}), (159359, {'train/accuracy': 0.7693945169448853, 'train/loss': 0.939318060874939, 'validation/accuracy': 0.6983799934387207, 'validation/loss': 1.2409757375717163, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.8835759162902832, 'test/num_examples': 10000, 'score': 73982.47636890411, 'total_duration': 81759.38010597229, 'accumulated_submission_time': 73982.47636890411, 'accumulated_eval_time': 7759.871674776077, 'accumulated_logging_time': 8.111968994140625, 'global_step': 159359, 'preemption_count': 0}), (160264, {'train/accuracy': 0.7602343559265137, 'train/loss': 0.9711340665817261, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.2386878728866577, 'validation/num_examples': 50000, 'test/accuracy': 0.5814000368118286, 'test/loss': 1.869041085243225, 'test/num_examples': 10000, 'score': 74402.69127678871, 'total_duration': 82223.95091509819, 'accumulated_submission_time': 74402.69127678871, 'accumulated_eval_time': 7804.115849733353, 'accumulated_logging_time': 8.173494815826416, 'global_step': 160264, 'preemption_count': 0}), (161168, {'train/accuracy': 0.7683203220367432, 'train/loss': 0.9366870522499084, 'validation/accuracy': 0.7050999999046326, 'validation/loss': 1.2145330905914307, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.8563323020935059, 'test/num_examples': 10000, 'score': 74822.95345377922, 'total_duration': 82692.12824940681, 'accumulated_submission_time': 74822.95345377922, 'accumulated_eval_time': 7851.924923419952, 'accumulated_logging_time': 8.229990243911743, 'global_step': 161168, 'preemption_count': 0}), (162070, {'train/accuracy': 0.77783203125, 'train/loss': 0.8856746554374695, 'validation/accuracy': 0.7069199681282043, 'validation/loss': 1.2027339935302734, 'validation/num_examples': 50000, 'test/accuracy': 0.5851000547409058, 'test/loss': 1.8382530212402344, 'test/num_examples': 10000, 'score': 75242.92199492455, 'total_duration': 83156.2474398613, 'accumulated_submission_time': 75242.92199492455, 'accumulated_eval_time': 7895.971745014191, 'accumulated_logging_time': 8.284348249435425, 'global_step': 162070, 'preemption_count': 0}), (162976, {'train/accuracy': 0.7721288800239563, 'train/loss': 0.9069911241531372, 'validation/accuracy': 0.7118799686431885, 'validation/loss': 1.1784546375274658, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.7933886051177979, 'test/num_examples': 10000, 'score': 75663.21282863617, 'total_duration': 83617.6762702465, 'accumulated_submission_time': 75663.21282863617, 'accumulated_eval_time': 7937.003118753433, 'accumulated_logging_time': 8.34020447731018, 'global_step': 162976, 'preemption_count': 0}), (163883, {'train/accuracy': 0.7769335508346558, 'train/loss': 0.8996948599815369, 'validation/accuracy': 0.7105799913406372, 'validation/loss': 1.1845804452896118, 'validation/num_examples': 50000, 'test/accuracy': 0.5900000333786011, 'test/loss': 1.8173900842666626, 'test/num_examples': 10000, 'score': 76083.81787419319, 'total_duration': 84085.51246571541, 'accumulated_submission_time': 76083.81787419319, 'accumulated_eval_time': 7984.124661684036, 'accumulated_logging_time': 8.398981094360352, 'global_step': 163883, 'preemption_count': 0}), (164791, {'train/accuracy': 0.7859765291213989, 'train/loss': 0.87266606092453, 'validation/accuracy': 0.7135399580001831, 'validation/loss': 1.186859130859375, 'validation/num_examples': 50000, 'test/accuracy': 0.5868000388145447, 'test/loss': 1.812342643737793, 'test/num_examples': 10000, 'score': 76504.19021439552, 'total_duration': 84552.21259260178, 'accumulated_submission_time': 76504.19021439552, 'accumulated_eval_time': 8030.3428745269775, 'accumulated_logging_time': 8.457646608352661, 'global_step': 164791, 'preemption_count': 0}), (165696, {'train/accuracy': 0.7836523056030273, 'train/loss': 0.8662102222442627, 'validation/accuracy': 0.7154799699783325, 'validation/loss': 1.161203145980835, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.7810871601104736, 'test/num_examples': 10000, 'score': 76924.15655565262, 'total_duration': 85017.09811878204, 'accumulated_submission_time': 76924.15655565262, 'accumulated_eval_time': 8075.156850099564, 'accumulated_logging_time': 8.51274585723877, 'global_step': 165696, 'preemption_count': 0}), (166602, {'train/accuracy': 0.7826757431030273, 'train/loss': 0.8744816184043884, 'validation/accuracy': 0.7170400023460388, 'validation/loss': 1.1657946109771729, 'validation/num_examples': 50000, 'test/accuracy': 0.595300018787384, 'test/loss': 1.7856409549713135, 'test/num_examples': 10000, 'score': 77344.08736562729, 'total_duration': 85484.08644890785, 'accumulated_submission_time': 77344.08736562729, 'accumulated_eval_time': 8122.107517242432, 'accumulated_logging_time': 8.569658279418945, 'global_step': 166602, 'preemption_count': 0})], 'global_step': 166988}
I0203 17:51:29.909973 140085747812160 submission_runner.py:586] Timing: 77520.1577796936
I0203 17:51:29.910100 140085747812160 submission_runner.py:588] Total number of evals: 185
I0203 17:51:29.910169 140085747812160 submission_runner.py:589] ====================
I0203 17:51:29.910237 140085747812160 submission_runner.py:542] Using RNG seed 1800903789
I0203 17:51:29.911913 140085747812160 submission_runner.py:551] --- Tuning run 5/5 ---
I0203 17:51:29.912038 140085747812160 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5.
I0203 17:51:29.915040 140085747812160 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5/hparams.json.
I0203 17:51:29.916021 140085747812160 submission_runner.py:206] Initializing dataset.
I0203 17:51:29.927062 140085747812160 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0203 17:51:29.942067 140085747812160 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0203 17:51:30.132050 140085747812160 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0203 17:51:34.460171 140085747812160 submission_runner.py:213] Initializing model.
I0203 17:51:41.199720 140085747812160 submission_runner.py:255] Initializing optimizer.
I0203 17:51:41.715226 140085747812160 submission_runner.py:262] Initializing metrics bundle.
I0203 17:51:41.715395 140085747812160 submission_runner.py:280] Initializing checkpoint and logger.
I0203 17:51:41.730411 140085747812160 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5 with prefix checkpoint_
I0203 17:51:41.730535 140085747812160 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0203 17:51:58.360848 140085747812160 logger_utils.py:220] Unable to record git information. Continuing without it.
I0203 17:52:14.715013 140085747812160 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5/flags_0.json.
I0203 17:52:14.719886 140085747812160 submission_runner.py:314] Starting training loop.
I0203 17:52:55.162908 139923826849536 logging_writer.py:48] [0] global_step=0, grad_norm=0.349168062210083, loss=6.907756328582764
I0203 17:52:55.181706 140085747812160 spec.py:321] Evaluating on the training split.
I0203 17:53:03.635927 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 17:53:22.411279 140085747812160 spec.py:349] Evaluating on the test split.
I0203 17:53:24.046674 140085747812160 submission_runner.py:408] Time since start: 69.33s, 	Step: 1, 	{'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.46163988113403, 'total_duration': 69.32669401168823, 'accumulated_submission_time': 40.46163988113403, 'accumulated_eval_time': 28.864896059036255, 'accumulated_logging_time': 0}
I0203 17:53:24.055327 139923835242240 logging_writer.py:48] [1] accumulated_eval_time=28.864896, accumulated_logging_time=0, accumulated_submission_time=40.461640, global_step=1, preemption_count=0, score=40.461640, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=69.326694, train/accuracy=0.000820, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0203 17:54:34.675690 139923868813056 logging_writer.py:48] [100] global_step=100, grad_norm=0.4436020255088806, loss=6.89186954498291
I0203 17:55:21.053541 139923852027648 logging_writer.py:48] [200] global_step=200, grad_norm=0.5918576717376709, loss=6.790581703186035
I0203 17:56:07.981463 139923868813056 logging_writer.py:48] [300] global_step=300, grad_norm=0.799533486366272, loss=6.713069438934326
I0203 17:56:55.002773 139923852027648 logging_writer.py:48] [400] global_step=400, grad_norm=1.0912278890609741, loss=6.5579352378845215
I0203 17:57:42.627146 139923868813056 logging_writer.py:48] [500] global_step=500, grad_norm=0.9816434979438782, loss=6.525278091430664
I0203 17:58:29.798650 139923852027648 logging_writer.py:48] [600] global_step=600, grad_norm=0.9752480387687683, loss=6.396842002868652
I0203 17:59:17.065986 139923868813056 logging_writer.py:48] [700] global_step=700, grad_norm=0.7811565399169922, loss=6.73986291885376
I0203 18:00:04.231297 139923852027648 logging_writer.py:48] [800] global_step=800, grad_norm=1.3334810733795166, loss=6.360804557800293
I0203 18:00:24.294137 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:00:34.973310 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:01:03.186336 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:01:04.829731 140085747812160 submission_runner.py:408] Time since start: 530.11s, 	Step: 844, 	{'train/accuracy': 0.03410156071186066, 'train/loss': 5.897230625152588, 'validation/accuracy': 0.029819998890161514, 'validation/loss': 5.968258380889893, 'validation/num_examples': 50000, 'test/accuracy': 0.021900001913309097, 'test/loss': 6.086627006530762, 'test/num_examples': 10000, 'score': 460.644246339798, 'total_duration': 530.1097981929779, 'accumulated_submission_time': 460.644246339798, 'accumulated_eval_time': 69.4005184173584, 'accumulated_logging_time': 0.017852783203125}
I0203 18:01:04.848894 139923868813056 logging_writer.py:48] [844] accumulated_eval_time=69.400518, accumulated_logging_time=0.017853, accumulated_submission_time=460.644246, global_step=844, preemption_count=0, score=460.644246, test/accuracy=0.021900, test/loss=6.086627, test/num_examples=10000, total_duration=530.109798, train/accuracy=0.034102, train/loss=5.897231, validation/accuracy=0.029820, validation/loss=5.968258, validation/num_examples=50000
I0203 18:01:28.222144 139923852027648 logging_writer.py:48] [900] global_step=900, grad_norm=1.3952487707138062, loss=6.132802486419678
I0203 18:02:13.879717 139923868813056 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2681667804718018, loss=6.15779972076416
I0203 18:03:01.385098 139923852027648 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.3139182329177856, loss=6.03328800201416
I0203 18:03:48.430038 139923868813056 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.2634848356246948, loss=5.9491987228393555
I0203 18:04:36.203007 139923852027648 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.0638866424560547, loss=6.035062313079834
I0203 18:05:23.412124 139923868813056 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.1219183206558228, loss=6.052868843078613
I0203 18:06:10.270839 139923852027648 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.097141146659851, loss=6.078745365142822
I0203 18:06:57.385098 139923868813056 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.1379021406173706, loss=5.817065715789795
I0203 18:07:44.445784 139923852027648 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7618756294250488, loss=6.421184539794922
I0203 18:08:04.937013 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:08:15.628799 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:08:46.364932 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:08:48.003575 140085747812160 submission_runner.py:408] Time since start: 993.28s, 	Step: 1745, 	{'train/accuracy': 0.07203125208616257, 'train/loss': 5.3030266761779785, 'validation/accuracy': 0.06712000072002411, 'validation/loss': 5.3553972244262695, 'validation/num_examples': 50000, 'test/accuracy': 0.05120000243186951, 'test/loss': 5.565213203430176, 'test/num_examples': 10000, 'score': 880.6717927455902, 'total_duration': 993.2836394309998, 'accumulated_submission_time': 880.6717927455902, 'accumulated_eval_time': 112.46707344055176, 'accumulated_logging_time': 0.04637718200683594}
I0203 18:08:48.020332 139923868813056 logging_writer.py:48] [1745] accumulated_eval_time=112.467073, accumulated_logging_time=0.046377, accumulated_submission_time=880.671793, global_step=1745, preemption_count=0, score=880.671793, test/accuracy=0.051200, test/loss=5.565213, test/num_examples=10000, total_duration=993.283639, train/accuracy=0.072031, train/loss=5.303027, validation/accuracy=0.067120, validation/loss=5.355397, validation/num_examples=50000
I0203 18:09:11.006212 139923852027648 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1030118465423584, loss=6.320637226104736
I0203 18:09:56.796329 139923868813056 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0349851846694946, loss=6.652490139007568
I0203 18:10:44.047296 139923852027648 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9835183620452881, loss=5.61485481262207
I0203 18:11:30.934013 139923868813056 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9788414835929871, loss=5.623391628265381
I0203 18:12:17.814708 139923852027648 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.259682536125183, loss=5.593453884124756
I0203 18:13:05.071428 139923868813056 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.0677398443222046, loss=5.496741771697998
I0203 18:13:51.883356 139923852027648 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3896489143371582, loss=5.500401973724365
I0203 18:14:39.065320 139923868813056 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.0889135599136353, loss=5.481827735900879
I0203 18:15:26.025690 139923852027648 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.029185175895691, loss=5.315786361694336
I0203 18:15:48.302680 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:15:58.602793 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:16:31.967081 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:16:33.608050 140085747812160 submission_runner.py:408] Time since start: 1458.89s, 	Step: 2649, 	{'train/accuracy': 0.1213671863079071, 'train/loss': 4.764578342437744, 'validation/accuracy': 0.11085999757051468, 'validation/loss': 4.83726167678833, 'validation/num_examples': 50000, 'test/accuracy': 0.08620000630617142, 'test/loss': 5.133504390716553, 'test/num_examples': 10000, 'score': 1300.8934905529022, 'total_duration': 1458.8881137371063, 'accumulated_submission_time': 1300.8934905529022, 'accumulated_eval_time': 157.77245998382568, 'accumulated_logging_time': 0.07286334037780762}
I0203 18:16:33.625246 139923868813056 logging_writer.py:48] [2649] accumulated_eval_time=157.772460, accumulated_logging_time=0.072863, accumulated_submission_time=1300.893491, global_step=2649, preemption_count=0, score=1300.893491, test/accuracy=0.086200, test/loss=5.133504, test/num_examples=10000, total_duration=1458.888114, train/accuracy=0.121367, train/loss=4.764578, validation/accuracy=0.110860, validation/loss=4.837262, validation/num_examples=50000
I0203 18:16:54.995420 139923852027648 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8529947996139526, loss=6.235999584197998
I0203 18:17:40.374246 139923868813056 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2885940074920654, loss=5.275402545928955
I0203 18:18:27.797567 139923852027648 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0545618534088135, loss=5.285558700561523
I0203 18:19:14.756054 139923868813056 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8948273062705994, loss=5.433722019195557
I0203 18:20:01.650562 139923852027648 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8316313624382019, loss=5.196928977966309
I0203 18:20:48.511947 139923868813056 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.200615406036377, loss=6.3683600425720215
I0203 18:21:35.763350 139923852027648 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.006656289100647, loss=5.062606334686279
I0203 18:22:22.611097 139923868813056 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.103651762008667, loss=5.173779487609863
I0203 18:23:09.917416 139923852027648 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.000361680984497, loss=5.554365634918213
I0203 18:23:33.703252 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:23:44.244125 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:24:17.021033 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:24:18.669341 140085747812160 submission_runner.py:408] Time since start: 1923.95s, 	Step: 3552, 	{'train/accuracy': 0.1814257800579071, 'train/loss': 4.194916248321533, 'validation/accuracy': 0.16040000319480896, 'validation/loss': 4.344925403594971, 'validation/num_examples': 50000, 'test/accuracy': 0.12460000813007355, 'test/loss': 4.737356662750244, 'test/num_examples': 10000, 'score': 1720.9098417758942, 'total_duration': 1923.9494013786316, 'accumulated_submission_time': 1720.9098417758942, 'accumulated_eval_time': 202.7385392189026, 'accumulated_logging_time': 0.10111761093139648}
I0203 18:24:18.685549 139923868813056 logging_writer.py:48] [3552] accumulated_eval_time=202.738539, accumulated_logging_time=0.101118, accumulated_submission_time=1720.909842, global_step=3552, preemption_count=0, score=1720.909842, test/accuracy=0.124600, test/loss=4.737357, test/num_examples=10000, total_duration=1923.949401, train/accuracy=0.181426, train/loss=4.194916, validation/accuracy=0.160400, validation/loss=4.344925, validation/num_examples=50000
I0203 18:24:38.816300 139923852027648 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6704508066177368, loss=6.058694362640381
I0203 18:25:23.907652 139923868813056 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.0349457263946533, loss=4.874147415161133
I0203 18:26:11.433982 139923852027648 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.955028772354126, loss=5.001726150512695
I0203 18:26:58.439956 139923868813056 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9160151481628418, loss=5.107234001159668
I0203 18:27:45.357345 139923852027648 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.7848473787307739, loss=4.921910762786865
I0203 18:28:32.570125 139923868813056 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.9148539900779724, loss=4.891805648803711
I0203 18:29:19.605808 139923852027648 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.8592373132705688, loss=4.704129219055176
I0203 18:30:06.618363 139923868813056 logging_writer.py:48] [4300] global_step=4300, grad_norm=1.0931739807128906, loss=4.728848934173584
I0203 18:30:53.301473 139923852027648 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6328603029251099, loss=6.080962657928467
I0203 18:31:18.890815 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:31:29.601125 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:31:59.960944 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:32:01.607028 140085747812160 submission_runner.py:408] Time since start: 2386.89s, 	Step: 4456, 	{'train/accuracy': 0.2257421761751175, 'train/loss': 3.8626036643981934, 'validation/accuracy': 0.20969998836517334, 'validation/loss': 3.9672608375549316, 'validation/num_examples': 50000, 'test/accuracy': 0.15650001168251038, 'test/loss': 4.419209003448486, 'test/num_examples': 10000, 'score': 2141.0544922351837, 'total_duration': 2386.887092113495, 'accumulated_submission_time': 2141.0544922351837, 'accumulated_eval_time': 245.45475339889526, 'accumulated_logging_time': 0.12727046012878418}
I0203 18:32:01.623842 139923868813056 logging_writer.py:48] [4456] accumulated_eval_time=245.454753, accumulated_logging_time=0.127270, accumulated_submission_time=2141.054492, global_step=4456, preemption_count=0, score=2141.054492, test/accuracy=0.156500, test/loss=4.419209, test/num_examples=10000, total_duration=2386.887092, train/accuracy=0.225742, train/loss=3.862604, validation/accuracy=0.209700, validation/loss=3.967261, validation/num_examples=50000
I0203 18:32:20.283806 139923852027648 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8487139344215393, loss=4.735202789306641
I0203 18:33:05.711633 139923868813056 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9934406876564026, loss=4.646955966949463
I0203 18:33:52.740119 139923852027648 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.8597787618637085, loss=4.650463104248047
I0203 18:34:39.643101 139923868813056 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6869457960128784, loss=6.228035926818848
I0203 18:35:26.462242 139923852027648 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.8101249933242798, loss=4.597196578979492
I0203 18:36:13.688073 139923868813056 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.810653805732727, loss=4.852878570556641
I0203 18:37:00.464940 139923852027648 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7945744395256042, loss=4.60246467590332
I0203 18:37:47.864556 139923868813056 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.1726746559143066, loss=4.450418949127197
I0203 18:38:34.708575 139923852027648 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.7723103165626526, loss=6.096297264099121
I0203 18:39:01.627510 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:39:12.339171 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:39:44.247321 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:39:45.897673 140085747812160 submission_runner.py:408] Time since start: 2851.18s, 	Step: 5359, 	{'train/accuracy': 0.2718164026737213, 'train/loss': 3.5856893062591553, 'validation/accuracy': 0.2515200078487396, 'validation/loss': 3.714376211166382, 'validation/num_examples': 50000, 'test/accuracy': 0.18800000846385956, 'test/loss': 4.178256511688232, 'test/num_examples': 10000, 'score': 2560.998118877411, 'total_duration': 2851.1777324676514, 'accumulated_submission_time': 2560.998118877411, 'accumulated_eval_time': 289.7249255180359, 'accumulated_logging_time': 0.15322399139404297}
I0203 18:39:45.914494 139923868813056 logging_writer.py:48] [5359] accumulated_eval_time=289.724926, accumulated_logging_time=0.153224, accumulated_submission_time=2560.998119, global_step=5359, preemption_count=0, score=2560.998119, test/accuracy=0.188000, test/loss=4.178257, test/num_examples=10000, total_duration=2851.177732, train/accuracy=0.271816, train/loss=3.585689, validation/accuracy=0.251520, validation/loss=3.714376, validation/num_examples=50000
I0203 18:40:03.160240 139923852027648 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.0658297538757324, loss=6.342489242553711
I0203 18:40:48.041602 139923868813056 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.7703801393508911, loss=5.104060173034668
I0203 18:41:35.321971 139923852027648 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9342070817947388, loss=4.361048221588135
I0203 18:42:22.255880 139923868813056 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.7967261075973511, loss=4.877899169921875
I0203 18:43:09.413974 139923852027648 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.6337283849716187, loss=5.321995735168457
I0203 18:43:56.306569 139923868813056 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.79392409324646, loss=4.508002281188965
I0203 18:44:45.580898 139923852027648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7832761406898499, loss=4.91316556930542
I0203 18:45:32.691775 139923868813056 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8977712392807007, loss=4.28477668762207
I0203 18:46:19.449106 139923852027648 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6607838869094849, loss=5.0129499435424805
I0203 18:46:45.933098 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:46:56.309503 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:47:26.545958 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:47:28.190201 140085747812160 submission_runner.py:408] Time since start: 3313.47s, 	Step: 6257, 	{'train/accuracy': 0.32466796040534973, 'train/loss': 3.229868173599243, 'validation/accuracy': 0.2886599898338318, 'validation/loss': 3.425157070159912, 'validation/num_examples': 50000, 'test/accuracy': 0.22220000624656677, 'test/loss': 3.962292432785034, 'test/num_examples': 10000, 'score': 2980.9549465179443, 'total_duration': 3313.470268011093, 'accumulated_submission_time': 2980.9549465179443, 'accumulated_eval_time': 331.98203206062317, 'accumulated_logging_time': 0.18113970756530762}
I0203 18:47:28.207141 139923868813056 logging_writer.py:48] [6257] accumulated_eval_time=331.982032, accumulated_logging_time=0.181140, accumulated_submission_time=2980.954947, global_step=6257, preemption_count=0, score=2980.954947, test/accuracy=0.222200, test/loss=3.962292, test/num_examples=10000, total_duration=3313.470268, train/accuracy=0.324668, train/loss=3.229868, validation/accuracy=0.288660, validation/loss=3.425157, validation/num_examples=50000
I0203 18:47:46.301887 139923852027648 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.7437795400619507, loss=4.203766822814941
I0203 18:48:30.963973 139923868813056 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.7821587920188904, loss=4.406228065490723
I0203 18:49:17.826308 139923852027648 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8029327392578125, loss=4.624624252319336
I0203 18:50:04.607374 139923868813056 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8083723783493042, loss=5.019370079040527
I0203 18:50:51.695752 139923852027648 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.561343789100647, loss=6.117231845855713
I0203 18:51:38.581090 139923868813056 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.7120822072029114, loss=4.608323097229004
I0203 18:52:25.357012 139923852027648 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.8466134071350098, loss=3.9985618591308594
I0203 18:53:12.328219 139923868813056 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7214173078536987, loss=6.153179168701172
I0203 18:53:59.053295 139923852027648 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.8199877142906189, loss=6.134425163269043
I0203 18:54:28.248406 140085747812160 spec.py:321] Evaluating on the training split.
I0203 18:54:39.036285 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 18:55:09.622553 140085747812160 spec.py:349] Evaluating on the test split.
I0203 18:55:11.271131 140085747812160 submission_runner.py:408] Time since start: 3776.55s, 	Step: 7164, 	{'train/accuracy': 0.3366992175579071, 'train/loss': 3.113884210586548, 'validation/accuracy': 0.3124600052833557, 'validation/loss': 3.242727041244507, 'validation/num_examples': 50000, 'test/accuracy': 0.23670001327991486, 'test/loss': 3.8153295516967773, 'test/num_examples': 10000, 'score': 3400.9355845451355, 'total_duration': 3776.551196575165, 'accumulated_submission_time': 3400.9355845451355, 'accumulated_eval_time': 375.00476264953613, 'accumulated_logging_time': 0.2081298828125}
I0203 18:55:11.289140 139923868813056 logging_writer.py:48] [7164] accumulated_eval_time=375.004763, accumulated_logging_time=0.208130, accumulated_submission_time=3400.935585, global_step=7164, preemption_count=0, score=3400.935585, test/accuracy=0.236700, test/loss=3.815330, test/num_examples=10000, total_duration=3776.551197, train/accuracy=0.336699, train/loss=3.113884, validation/accuracy=0.312460, validation/loss=3.242727, validation/num_examples=50000
I0203 18:55:26.492953 139923852027648 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7855049967765808, loss=6.070438385009766
I0203 18:56:10.930262 139923868813056 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.6133499145507812, loss=5.939591884613037
I0203 18:56:57.964248 139923852027648 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.0016989707946777, loss=3.9619126319885254
I0203 18:57:44.959948 139923868813056 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7850790023803711, loss=4.97589111328125
I0203 18:58:32.165780 139923852027648 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.7696900963783264, loss=4.012270450592041
I0203 18:59:19.267937 139923868813056 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.8232143521308899, loss=4.037154674530029
I0203 19:00:06.396083 139923852027648 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.0265274047851562, loss=4.050009250640869
I0203 19:00:53.287400 139923868813056 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.6170253157615662, loss=5.201284885406494
I0203 19:01:40.207012 139923852027648 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0765838623046875, loss=3.941787004470825
I0203 19:02:11.485730 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:02:22.055692 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:02:54.539986 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:02:56.180730 140085747812160 submission_runner.py:408] Time since start: 4241.46s, 	Step: 8068, 	{'train/accuracy': 0.37083983421325684, 'train/loss': 2.9305522441864014, 'validation/accuracy': 0.338919997215271, 'validation/loss': 3.110529899597168, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.680340051651001, 'test/num_examples': 10000, 'score': 3821.070417881012, 'total_duration': 4241.460793018341, 'accumulated_submission_time': 3821.070417881012, 'accumulated_eval_time': 419.69977498054504, 'accumulated_logging_time': 0.2372419834136963}
I0203 19:02:56.197739 139923868813056 logging_writer.py:48] [8068] accumulated_eval_time=419.699775, accumulated_logging_time=0.237242, accumulated_submission_time=3821.070418, global_step=8068, preemption_count=0, score=3821.070418, test/accuracy=0.257100, test/loss=3.680340, test/num_examples=10000, total_duration=4241.460793, train/accuracy=0.370840, train/loss=2.930552, validation/accuracy=0.338920, validation/loss=3.110530, validation/num_examples=50000
I0203 19:03:09.755397 139923852027648 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.839976966381073, loss=3.942037582397461
I0203 19:03:53.706593 139923868813056 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.5047917366027832, loss=5.954585552215576
I0203 19:04:40.711049 139923852027648 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.7928470969200134, loss=4.11512565612793
I0203 19:05:27.520128 139923868813056 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8633103370666504, loss=3.8870348930358887
I0203 19:06:14.658974 139923852027648 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8577751517295837, loss=3.835631847381592
I0203 19:07:01.602964 139923868813056 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9933428168296814, loss=4.065797805786133
I0203 19:07:48.440069 139923852027648 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.666358232498169, loss=4.603259086608887
I0203 19:08:35.310941 139923868813056 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.8531986474990845, loss=3.804631471633911
I0203 19:09:22.307803 139923852027648 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.7531530261039734, loss=5.306201457977295
I0203 19:09:56.189546 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:10:06.490384 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:10:33.241364 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:10:34.887467 140085747812160 submission_runner.py:408] Time since start: 4700.17s, 	Step: 8974, 	{'train/accuracy': 0.3893359303474426, 'train/loss': 2.798448085784912, 'validation/accuracy': 0.35621997714042664, 'validation/loss': 2.991145610809326, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.5983641147613525, 'test/num_examples': 10000, 'score': 4241.001017093658, 'total_duration': 4700.1675000190735, 'accumulated_submission_time': 4241.001017093658, 'accumulated_eval_time': 458.3977072238922, 'accumulated_logging_time': 0.26456356048583984}
I0203 19:10:34.905715 139923868813056 logging_writer.py:48] [8974] accumulated_eval_time=458.397707, accumulated_logging_time=0.264564, accumulated_submission_time=4241.001017, global_step=8974, preemption_count=0, score=4241.001017, test/accuracy=0.272500, test/loss=3.598364, test/num_examples=10000, total_duration=4700.167500, train/accuracy=0.389336, train/loss=2.798448, validation/accuracy=0.356220, validation/loss=2.991146, validation/num_examples=50000
I0203 19:10:46.011428 139923852027648 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.7440717816352844, loss=3.8578310012817383
I0203 19:11:30.059404 139923868813056 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.7013072967529297, loss=6.014195919036865
I0203 19:12:17.154671 139923852027648 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.680783748626709, loss=5.908111572265625
I0203 19:13:04.179154 139923868813056 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.6357032656669617, loss=5.633821487426758
I0203 19:13:51.044231 139923852027648 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.5640708804130554, loss=5.927367210388184
I0203 19:14:38.097634 139923868813056 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6875702738761902, loss=5.9891357421875
I0203 19:15:25.132194 139923852027648 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8009371161460876, loss=4.04371976852417
I0203 19:16:11.900269 139923868813056 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.3800530433654785, loss=3.8972115516662598
I0203 19:16:58.961970 139923852027648 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.0116877555847168, loss=3.8253395557403564
I0203 19:17:35.155842 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:17:45.777758 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:18:15.899358 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:18:17.556781 140085747812160 submission_runner.py:408] Time since start: 5162.84s, 	Step: 9879, 	{'train/accuracy': 0.40507811307907104, 'train/loss': 2.719622850418091, 'validation/accuracy': 0.37549999356269836, 'validation/loss': 2.876948118209839, 'validation/num_examples': 50000, 'test/accuracy': 0.29280000925064087, 'test/loss': 3.4608092308044434, 'test/num_examples': 10000, 'score': 4661.186886072159, 'total_duration': 5162.8368492126465, 'accumulated_submission_time': 4661.186886072159, 'accumulated_eval_time': 500.7986707687378, 'accumulated_logging_time': 0.29593586921691895}
I0203 19:18:17.577649 139923868813056 logging_writer.py:48] [9879] accumulated_eval_time=500.798671, accumulated_logging_time=0.295936, accumulated_submission_time=4661.186886, global_step=9879, preemption_count=0, score=4661.186886, test/accuracy=0.292800, test/loss=3.460809, test/num_examples=10000, total_duration=5162.836849, train/accuracy=0.405078, train/loss=2.719623, validation/accuracy=0.375500, validation/loss=2.876948, validation/num_examples=50000
I0203 19:18:26.628000 139923852027648 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9572053551673889, loss=3.7325034141540527
I0203 19:19:10.416702 139923868813056 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.8998319506645203, loss=3.7491753101348877
I0203 19:19:57.381184 139923852027648 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.2818279266357422, loss=4.935741424560547
I0203 19:20:44.460289 139923868813056 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.6643548607826233, loss=5.301745414733887
I0203 19:21:31.427894 139923852027648 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.868778645992279, loss=3.7220139503479004
I0203 19:22:18.530285 139923868813056 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9870091676712036, loss=4.103431224822998
I0203 19:23:05.860745 139923852027648 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9021738171577454, loss=3.6056530475616455
I0203 19:23:52.663090 139923868813056 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.8777005672454834, loss=4.1104888916015625
I0203 19:24:39.774585 139923852027648 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8297798037528992, loss=4.466280460357666
I0203 19:25:17.979228 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:25:28.377944 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:26:01.772860 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:26:03.421093 140085747812160 submission_runner.py:408] Time since start: 5628.70s, 	Step: 10783, 	{'train/accuracy': 0.4206640422344208, 'train/loss': 2.6827948093414307, 'validation/accuracy': 0.3864399790763855, 'validation/loss': 2.8488309383392334, 'validation/num_examples': 50000, 'test/accuracy': 0.3021000027656555, 'test/loss': 3.441816568374634, 'test/num_examples': 10000, 'score': 5081.527356147766, 'total_duration': 5628.701157808304, 'accumulated_submission_time': 5081.527356147766, 'accumulated_eval_time': 546.2405309677124, 'accumulated_logging_time': 0.3265047073364258}
I0203 19:26:03.442056 139923868813056 logging_writer.py:48] [10783] accumulated_eval_time=546.240531, accumulated_logging_time=0.326505, accumulated_submission_time=5081.527356, global_step=10783, preemption_count=0, score=5081.527356, test/accuracy=0.302100, test/loss=3.441817, test/num_examples=10000, total_duration=5628.701158, train/accuracy=0.420664, train/loss=2.682795, validation/accuracy=0.386440, validation/loss=2.848831, validation/num_examples=50000
I0203 19:26:10.836194 139923852027648 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.9657720327377319, loss=5.954132080078125
I0203 19:26:54.187264 139923868813056 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9642634987831116, loss=3.716291666030884
I0203 19:27:41.095548 139923852027648 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8698371648788452, loss=4.110707759857178
I0203 19:28:27.925536 139923868813056 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.1149483919143677, loss=3.7572362422943115
I0203 19:29:14.710622 139923852027648 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.6954955458641052, loss=5.655749797821045
I0203 19:30:01.674031 139923868813056 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.980006217956543, loss=3.9244933128356934
I0203 19:30:48.551213 139923852027648 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6388872265815735, loss=5.593733787536621
I0203 19:31:35.565210 139923868813056 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9718998074531555, loss=3.536085844039917
I0203 19:32:22.765297 139923852027648 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.9143521785736084, loss=3.5948891639709473
I0203 19:33:03.588957 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:33:14.321068 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:33:43.943718 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:33:45.587201 140085747812160 submission_runner.py:408] Time since start: 6090.87s, 	Step: 11688, 	{'train/accuracy': 0.4415038824081421, 'train/loss': 2.5091471672058105, 'validation/accuracy': 0.405379980802536, 'validation/loss': 2.7154462337493896, 'validation/num_examples': 50000, 'test/accuracy': 0.31060001254081726, 'test/loss': 3.323500633239746, 'test/num_examples': 10000, 'score': 5501.6132843494415, 'total_duration': 6090.867266654968, 'accumulated_submission_time': 5501.6132843494415, 'accumulated_eval_time': 588.2387778759003, 'accumulated_logging_time': 0.35753369331359863}
I0203 19:33:45.609597 139923868813056 logging_writer.py:48] [11688] accumulated_eval_time=588.238778, accumulated_logging_time=0.357534, accumulated_submission_time=5501.613284, global_step=11688, preemption_count=0, score=5501.613284, test/accuracy=0.310600, test/loss=3.323501, test/num_examples=10000, total_duration=6090.867267, train/accuracy=0.441504, train/loss=2.509147, validation/accuracy=0.405380, validation/loss=2.715446, validation/num_examples=50000
I0203 19:33:50.953003 139923852027648 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8492386937141418, loss=3.629384994506836
I0203 19:34:34.193029 139923868813056 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.9418330192565918, loss=3.720588445663452
I0203 19:35:21.334251 139923852027648 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9498398303985596, loss=3.5205514430999756
I0203 19:36:08.499160 139923868813056 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8507277369499207, loss=3.5355653762817383
I0203 19:36:55.671176 139923852027648 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9504980444908142, loss=3.4281117916107178
I0203 19:37:42.845521 139923868813056 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7386263608932495, loss=5.667008399963379
I0203 19:38:30.251208 139923852027648 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.6644914150238037, loss=4.9217209815979
I0203 19:39:17.293883 139923868813056 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.5985980033874512, loss=5.730570316314697
I0203 19:40:04.730105 139923852027648 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9413763880729675, loss=3.5176215171813965
I0203 19:40:46.067508 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:40:56.454530 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:41:31.177160 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:41:32.822529 140085747812160 submission_runner.py:408] Time since start: 6558.10s, 	Step: 12590, 	{'train/accuracy': 0.4447070360183716, 'train/loss': 2.521016836166382, 'validation/accuracy': 0.4148999750614166, 'validation/loss': 2.6761584281921387, 'validation/num_examples': 50000, 'test/accuracy': 0.3197000026702881, 'test/loss': 3.3049449920654297, 'test/num_examples': 10000, 'score': 5922.011283874512, 'total_duration': 6558.102586507797, 'accumulated_submission_time': 5922.011283874512, 'accumulated_eval_time': 634.9937858581543, 'accumulated_logging_time': 0.3894164562225342}
I0203 19:41:32.843807 139923868813056 logging_writer.py:48] [12590] accumulated_eval_time=634.993786, accumulated_logging_time=0.389416, accumulated_submission_time=5922.011284, global_step=12590, preemption_count=0, score=5922.011284, test/accuracy=0.319700, test/loss=3.304945, test/num_examples=10000, total_duration=6558.102587, train/accuracy=0.444707, train/loss=2.521017, validation/accuracy=0.414900, validation/loss=2.676158, validation/num_examples=50000
I0203 19:41:37.371321 139923852027648 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.8599210381507874, loss=5.855147361755371
I0203 19:42:20.432738 139923868813056 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0816737413406372, loss=3.6405534744262695
I0203 19:43:07.783434 139923852027648 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9285709261894226, loss=3.608886480331421
I0203 19:43:54.854904 139923868813056 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.0220224857330322, loss=3.4657368659973145
I0203 19:44:41.628752 139923852027648 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7112115621566772, loss=5.842996120452881
I0203 19:45:28.281625 139923868813056 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9940911531448364, loss=3.7548229694366455
I0203 19:46:15.053169 139923852027648 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.1378881931304932, loss=3.546069622039795
I0203 19:47:02.009062 139923868813056 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9257060289382935, loss=3.443185567855835
I0203 19:47:48.860590 139923852027648 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9392374157905579, loss=3.7132327556610107
I0203 19:48:33.140671 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:48:43.491340 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:49:11.805203 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:49:13.474629 140085747812160 submission_runner.py:408] Time since start: 7018.75s, 	Step: 13496, 	{'train/accuracy': 0.4694921672344208, 'train/loss': 2.356694221496582, 'validation/accuracy': 0.435479998588562, 'validation/loss': 2.5276846885681152, 'validation/num_examples': 50000, 'test/accuracy': 0.33570000529289246, 'test/loss': 3.1664600372314453, 'test/num_examples': 10000, 'score': 6342.245297193527, 'total_duration': 7018.754692077637, 'accumulated_submission_time': 6342.245297193527, 'accumulated_eval_time': 675.3277657032013, 'accumulated_logging_time': 0.4220564365386963}
I0203 19:49:13.495930 139923868813056 logging_writer.py:48] [13496] accumulated_eval_time=675.327766, accumulated_logging_time=0.422056, accumulated_submission_time=6342.245297, global_step=13496, preemption_count=0, score=6342.245297, test/accuracy=0.335700, test/loss=3.166460, test/num_examples=10000, total_duration=7018.754692, train/accuracy=0.469492, train/loss=2.356694, validation/accuracy=0.435480, validation/loss=2.527685, validation/num_examples=50000
I0203 19:49:15.588189 139923852027648 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8101092576980591, loss=4.843400478363037
I0203 19:49:58.288333 139923868813056 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.1599063873291016, loss=3.410365343093872
I0203 19:50:45.313951 139923852027648 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0510096549987793, loss=3.3908326625823975
I0203 19:51:32.585533 139923868813056 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.1210107803344727, loss=3.476775884628296
I0203 19:52:19.519995 139923852027648 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.8100934028625488, loss=4.456892013549805
I0203 19:53:06.528427 139923868813056 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.877461314201355, loss=3.8379337787628174
I0203 19:53:53.454912 139923852027648 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.9803209900856018, loss=3.3721683025360107
I0203 19:54:40.343906 139923868813056 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9247099161148071, loss=3.332674741744995
I0203 19:55:27.659010 139923852027648 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0608996152877808, loss=3.54518723487854
I0203 19:56:13.604702 140085747812160 spec.py:321] Evaluating on the training split.
I0203 19:56:24.374972 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 19:56:56.984298 140085747812160 spec.py:349] Evaluating on the test split.
I0203 19:56:58.621324 140085747812160 submission_runner.py:408] Time since start: 7483.90s, 	Step: 14400, 	{'train/accuracy': 0.47019529342651367, 'train/loss': 2.3852572441101074, 'validation/accuracy': 0.43121999502182007, 'validation/loss': 2.579622268676758, 'validation/num_examples': 50000, 'test/accuracy': 0.33650001883506775, 'test/loss': 3.197645664215088, 'test/num_examples': 10000, 'score': 6762.292719364166, 'total_duration': 7483.901390790939, 'accumulated_submission_time': 6762.292719364166, 'accumulated_eval_time': 720.3443946838379, 'accumulated_logging_time': 0.45426154136657715}
I0203 19:56:58.639462 139923868813056 logging_writer.py:48] [14400] accumulated_eval_time=720.344395, accumulated_logging_time=0.454262, accumulated_submission_time=6762.292719, global_step=14400, preemption_count=0, score=6762.292719, test/accuracy=0.336500, test/loss=3.197646, test/num_examples=10000, total_duration=7483.901391, train/accuracy=0.470195, train/loss=2.385257, validation/accuracy=0.431220, validation/loss=2.579622, validation/num_examples=50000
I0203 19:56:59.061672 139923852027648 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.6228326559066772, loss=5.284890651702881
I0203 19:57:41.498948 139923868813056 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0757193565368652, loss=3.3935256004333496
I0203 19:58:28.321664 139923852027648 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0069910287857056, loss=3.289104700088501
I0203 19:59:15.277626 139923868813056 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.8361435532569885, loss=5.512132167816162
I0203 20:00:02.063196 139923852027648 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9971668124198914, loss=3.2917532920837402
I0203 20:00:48.673517 139923868813056 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.6900709271430969, loss=5.389312744140625
I0203 20:01:35.611276 139923852027648 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8618313074111938, loss=5.754084587097168
I0203 20:02:22.639991 139923868813056 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9125648736953735, loss=3.27437686920166
I0203 20:03:09.730616 139923852027648 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0895284414291382, loss=3.1753311157226562
I0203 20:03:56.300887 139923868813056 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.9279077649116516, loss=3.6266889572143555
I0203 20:03:58.844033 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:04:09.708366 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:04:43.204308 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:04:44.842486 140085747812160 submission_runner.py:408] Time since start: 7950.12s, 	Step: 15307, 	{'train/accuracy': 0.4783593714237213, 'train/loss': 2.331885814666748, 'validation/accuracy': 0.4416399896144867, 'validation/loss': 2.534491539001465, 'validation/num_examples': 50000, 'test/accuracy': 0.34200000762939453, 'test/loss': 3.1562328338623047, 'test/num_examples': 10000, 'score': 7182.435878038406, 'total_duration': 7950.122545957565, 'accumulated_submission_time': 7182.435878038406, 'accumulated_eval_time': 766.3428432941437, 'accumulated_logging_time': 0.48224782943725586}
I0203 20:04:44.860188 139923852027648 logging_writer.py:48] [15307] accumulated_eval_time=766.342843, accumulated_logging_time=0.482248, accumulated_submission_time=7182.435878, global_step=15307, preemption_count=0, score=7182.435878, test/accuracy=0.342000, test/loss=3.156233, test/num_examples=10000, total_duration=7950.122546, train/accuracy=0.478359, train/loss=2.331886, validation/accuracy=0.441640, validation/loss=2.534492, validation/num_examples=50000
I0203 20:05:24.473827 139923868813056 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.1104040145874023, loss=3.428537607192993
I0203 20:06:10.922255 139923852027648 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.8245906233787537, loss=3.7627015113830566
I0203 20:06:57.991275 139923868813056 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0594912767410278, loss=3.2744340896606445
I0203 20:07:44.678813 139923852027648 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9886884093284607, loss=3.6879379749298096
I0203 20:08:31.451718 139923868813056 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.7171176075935364, loss=4.916171073913574
I0203 20:09:18.260139 139923852027648 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.053987979888916, loss=3.258183479309082
I0203 20:10:05.116304 139923868813056 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.1479265689849854, loss=3.2629473209381104
I0203 20:10:51.918751 139923852027648 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0534430742263794, loss=3.332882881164551
I0203 20:11:38.761404 139923868813056 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.8833312392234802, loss=3.9687886238098145
I0203 20:11:44.966500 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:11:55.566055 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:12:30.261580 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:12:31.895976 140085747812160 submission_runner.py:408] Time since start: 8417.18s, 	Step: 16215, 	{'train/accuracy': 0.4888867139816284, 'train/loss': 2.2850089073181152, 'validation/accuracy': 0.45423999428749084, 'validation/loss': 2.4670143127441406, 'validation/num_examples': 50000, 'test/accuracy': 0.3481000065803528, 'test/loss': 3.092832088470459, 'test/num_examples': 10000, 'score': 7602.47861289978, 'total_duration': 8417.176041603088, 'accumulated_submission_time': 7602.47861289978, 'accumulated_eval_time': 813.2723207473755, 'accumulated_logging_time': 0.5117042064666748}
I0203 20:12:31.917307 139923852027648 logging_writer.py:48] [16215] accumulated_eval_time=813.272321, accumulated_logging_time=0.511704, accumulated_submission_time=7602.478613, global_step=16215, preemption_count=0, score=7602.478613, test/accuracy=0.348100, test/loss=3.092832, test/num_examples=10000, total_duration=8417.176042, train/accuracy=0.488887, train/loss=2.285009, validation/accuracy=0.454240, validation/loss=2.467014, validation/num_examples=50000
I0203 20:13:08.078171 139923868813056 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9906780123710632, loss=3.3660686016082764
I0203 20:13:54.375544 139923852027648 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.2097371816635132, loss=3.5828514099121094
I0203 20:14:41.597346 139923868813056 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.998229444026947, loss=3.825226306915283
I0203 20:15:28.414177 139923852027648 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.0674129724502563, loss=3.24739933013916
I0203 20:16:15.321784 139923868813056 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9881711602210999, loss=3.741584062576294
I0203 20:17:02.177820 139923852027648 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9397366046905518, loss=4.001157760620117
I0203 20:17:49.084875 139923868813056 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0075244903564453, loss=3.2756283283233643
I0203 20:18:35.768749 139923852027648 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7977219820022583, loss=4.203371047973633
I0203 20:19:22.757660 139923868813056 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9891347885131836, loss=3.202399730682373
I0203 20:19:32.213482 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:19:42.996613 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:20:13.484837 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:20:15.121536 140085747812160 submission_runner.py:408] Time since start: 8880.40s, 	Step: 17122, 	{'train/accuracy': 0.49986326694488525, 'train/loss': 2.2155532836914062, 'validation/accuracy': 0.46083998680114746, 'validation/loss': 2.4244303703308105, 'validation/num_examples': 50000, 'test/accuracy': 0.3571000099182129, 'test/loss': 3.0820775032043457, 'test/num_examples': 10000, 'score': 8022.711845397949, 'total_duration': 8880.401599168777, 'accumulated_submission_time': 8022.711845397949, 'accumulated_eval_time': 856.1803793907166, 'accumulated_logging_time': 0.5442097187042236}
I0203 20:20:15.144059 139923852027648 logging_writer.py:48] [17122] accumulated_eval_time=856.180379, accumulated_logging_time=0.544210, accumulated_submission_time=8022.711845, global_step=17122, preemption_count=0, score=8022.711845, test/accuracy=0.357100, test/loss=3.082078, test/num_examples=10000, total_duration=8880.401599, train/accuracy=0.499863, train/loss=2.215553, validation/accuracy=0.460840, validation/loss=2.424430, validation/num_examples=50000
I0203 20:20:48.119824 139923868813056 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0935953855514526, loss=3.2803635597229004
I0203 20:21:34.925476 139923852027648 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0820215940475464, loss=3.316211223602295
I0203 20:22:22.103486 139923868813056 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.051218032836914, loss=3.3161613941192627
I0203 20:23:09.339247 139923852027648 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.8712337017059326, loss=4.3669233322143555
I0203 20:23:56.262382 139923868813056 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.7658118605613708, loss=5.083301544189453
I0203 20:24:43.133895 139923852027648 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0685523748397827, loss=3.311997413635254
I0203 20:25:30.041986 139923868813056 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.9320235252380371, loss=3.226019859313965
I0203 20:26:16.893832 139923852027648 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.7616969347000122, loss=5.705791473388672
I0203 20:27:03.958968 139923868813056 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.1108498573303223, loss=3.3594818115234375
I0203 20:27:15.330470 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:27:25.642116 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:28:01.384255 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:28:03.032598 140085747812160 submission_runner.py:408] Time since start: 9348.31s, 	Step: 18026, 	{'train/accuracy': 0.5294336080551147, 'train/loss': 2.071880340576172, 'validation/accuracy': 0.46521997451782227, 'validation/loss': 2.3781328201293945, 'validation/num_examples': 50000, 'test/accuracy': 0.35750001668930054, 'test/loss': 3.0405938625335693, 'test/num_examples': 10000, 'score': 8442.835622787476, 'total_duration': 9348.312663078308, 'accumulated_submission_time': 8442.835622787476, 'accumulated_eval_time': 903.8825159072876, 'accumulated_logging_time': 0.5782530307769775}
I0203 20:28:03.051757 139923852027648 logging_writer.py:48] [18026] accumulated_eval_time=903.882516, accumulated_logging_time=0.578253, accumulated_submission_time=8442.835623, global_step=18026, preemption_count=0, score=8442.835623, test/accuracy=0.357500, test/loss=3.040594, test/num_examples=10000, total_duration=9348.312663, train/accuracy=0.529434, train/loss=2.071880, validation/accuracy=0.465220, validation/loss=2.378133, validation/num_examples=50000
I0203 20:28:33.993251 139923868813056 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.0344172716140747, loss=3.2079930305480957
I0203 20:29:20.688362 139923852027648 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.1893489360809326, loss=3.1785473823547363
I0203 20:30:07.591360 139923868813056 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.9773808717727661, loss=3.3355183601379395
I0203 20:30:54.389110 139923852027648 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1728650331497192, loss=3.1009182929992676
I0203 20:31:41.168535 139923868813056 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0342683792114258, loss=3.367091655731201
I0203 20:32:28.115764 139923852027648 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.8834872841835022, loss=4.230808258056641
I0203 20:33:15.197260 139923868813056 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.8441663980484009, loss=5.584815979003906
I0203 20:34:02.231644 139923852027648 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0319018363952637, loss=3.1468794345855713
I0203 20:34:49.130122 139923868813056 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0231969356536865, loss=3.654075860977173
I0203 20:35:03.042618 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:35:13.641114 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:35:46.873981 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:35:48.513038 140085747812160 submission_runner.py:408] Time since start: 9813.79s, 	Step: 18931, 	{'train/accuracy': 0.5103710889816284, 'train/loss': 2.1674885749816895, 'validation/accuracy': 0.4759399890899658, 'validation/loss': 2.3467721939086914, 'validation/num_examples': 50000, 'test/accuracy': 0.36880001425743103, 'test/loss': 3.0019314289093018, 'test/num_examples': 10000, 'score': 8862.765635967255, 'total_duration': 9813.793103218079, 'accumulated_submission_time': 8862.765635967255, 'accumulated_eval_time': 949.3529365062714, 'accumulated_logging_time': 0.607285737991333}
I0203 20:35:48.533289 139923852027648 logging_writer.py:48] [18931] accumulated_eval_time=949.352937, accumulated_logging_time=0.607286, accumulated_submission_time=8862.765636, global_step=18931, preemption_count=0, score=8862.765636, test/accuracy=0.368800, test/loss=3.001931, test/num_examples=10000, total_duration=9813.793103, train/accuracy=0.510371, train/loss=2.167489, validation/accuracy=0.475940, validation/loss=2.346772, validation/num_examples=50000
I0203 20:36:17.277783 139923868813056 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.105384111404419, loss=3.158402919769287
I0203 20:37:03.958643 139923852027648 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.0128815174102783, loss=3.2241854667663574
I0203 20:37:51.005998 139923868813056 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9220881462097168, loss=4.117406368255615
I0203 20:38:37.827070 139923852027648 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2225792407989502, loss=3.6353297233581543
I0203 20:39:24.742285 139923868813056 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9188987016677856, loss=3.4645166397094727
I0203 20:40:11.539353 139923852027648 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0366880893707275, loss=3.330709457397461
I0203 20:40:58.419564 139923868813056 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.017317771911621, loss=3.1622419357299805
I0203 20:41:45.292498 139923852027648 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.9417047500610352, loss=4.120306015014648
I0203 20:42:32.231453 139923868813056 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.076828122138977, loss=3.188410997390747
I0203 20:42:48.852686 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:42:59.416039 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:43:34.295766 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:43:35.933629 140085747812160 submission_runner.py:408] Time since start: 10281.21s, 	Step: 19837, 	{'train/accuracy': 0.5275976657867432, 'train/loss': 2.005328416824341, 'validation/accuracy': 0.48749998211860657, 'validation/loss': 2.2312562465667725, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 2.899568557739258, 'test/num_examples': 10000, 'score': 9283.02355337143, 'total_duration': 10281.213695764542, 'accumulated_submission_time': 9283.02355337143, 'accumulated_eval_time': 996.4339108467102, 'accumulated_logging_time': 0.6371915340423584}
I0203 20:43:35.954336 139923852027648 logging_writer.py:48] [19837] accumulated_eval_time=996.433911, accumulated_logging_time=0.637192, accumulated_submission_time=9283.023553, global_step=19837, preemption_count=0, score=9283.023553, test/accuracy=0.378800, test/loss=2.899569, test/num_examples=10000, total_duration=10281.213696, train/accuracy=0.527598, train/loss=2.005328, validation/accuracy=0.487500, validation/loss=2.231256, validation/num_examples=50000
I0203 20:44:02.240785 139923868813056 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.0084505081176758, loss=3.1950511932373047
I0203 20:44:48.464685 139923852027648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7476268410682678, loss=5.289222717285156
I0203 20:45:35.938148 139923868813056 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.1414278745651245, loss=3.3018829822540283
I0203 20:46:22.788972 139923852027648 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.8254014849662781, loss=4.8165998458862305
I0203 20:47:09.436128 139923868813056 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0520532131195068, loss=3.1962192058563232
I0203 20:47:56.510213 139923852027648 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.154934048652649, loss=3.354219436645508
I0203 20:48:43.290101 139923868813056 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.1410731077194214, loss=3.1080710887908936
I0203 20:49:30.278465 139923852027648 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.031988263130188, loss=3.478734254837036
I0203 20:50:16.996042 139923868813056 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.8100566267967224, loss=5.5316643714904785
I0203 20:50:36.308939 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:50:47.045160 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:51:12.224801 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:51:13.873632 140085747812160 submission_runner.py:408] Time since start: 10739.15s, 	Step: 20743, 	{'train/accuracy': 0.5421093702316284, 'train/loss': 2.062912940979004, 'validation/accuracy': 0.4810599982738495, 'validation/loss': 2.356579303741455, 'validation/num_examples': 50000, 'test/accuracy': 0.37370002269744873, 'test/loss': 2.998039484024048, 'test/num_examples': 10000, 'score': 9703.317378759384, 'total_duration': 10739.15369963646, 'accumulated_submission_time': 9703.317378759384, 'accumulated_eval_time': 1033.9986152648926, 'accumulated_logging_time': 0.6677725315093994}
I0203 20:51:13.892735 139923852027648 logging_writer.py:48] [20743] accumulated_eval_time=1033.998615, accumulated_logging_time=0.667773, accumulated_submission_time=9703.317379, global_step=20743, preemption_count=0, score=9703.317379, test/accuracy=0.373700, test/loss=2.998039, test/num_examples=10000, total_duration=10739.153700, train/accuracy=0.542109, train/loss=2.062913, validation/accuracy=0.481060, validation/loss=2.356579, validation/num_examples=50000
I0203 20:51:37.731561 139923868813056 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.6601564884185791, loss=5.451167583465576
I0203 20:52:23.591752 139923852027648 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.956282377243042, loss=5.454737186431885
I0203 20:53:10.857807 139923868813056 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.9532594084739685, loss=4.351876735687256
I0203 20:53:57.549017 139923852027648 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.8877727389335632, loss=5.364401817321777
I0203 20:54:44.636322 139923868813056 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.0602126121520996, loss=3.4573376178741455
I0203 20:55:31.838644 139923852027648 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.037004828453064, loss=3.214878559112549
I0203 20:56:18.744768 139923868813056 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.8478764295578003, loss=4.924305438995361
I0203 20:57:05.999025 139923852027648 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0526739358901978, loss=3.0860352516174316
I0203 20:57:53.022905 139923868813056 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.7378961443901062, loss=5.416650772094727
I0203 20:58:14.020363 140085747812160 spec.py:321] Evaluating on the training split.
I0203 20:58:24.602573 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 20:58:59.573454 140085747812160 spec.py:349] Evaluating on the test split.
I0203 20:59:01.219451 140085747812160 submission_runner.py:408] Time since start: 11206.50s, 	Step: 21646, 	{'train/accuracy': 0.5312694907188416, 'train/loss': 2.0607404708862305, 'validation/accuracy': 0.4913399815559387, 'validation/loss': 2.2656807899475098, 'validation/num_examples': 50000, 'test/accuracy': 0.38190001249313354, 'test/loss': 2.9162533283233643, 'test/num_examples': 10000, 'score': 10123.384189367294, 'total_duration': 11206.499513149261, 'accumulated_submission_time': 10123.384189367294, 'accumulated_eval_time': 1081.1976935863495, 'accumulated_logging_time': 0.6967108249664307}
I0203 20:59:01.241129 139923852027648 logging_writer.py:48] [21646] accumulated_eval_time=1081.197694, accumulated_logging_time=0.696711, accumulated_submission_time=10123.384189, global_step=21646, preemption_count=0, score=10123.384189, test/accuracy=0.381900, test/loss=2.916253, test/num_examples=10000, total_duration=11206.499513, train/accuracy=0.531269, train/loss=2.060740, validation/accuracy=0.491340, validation/loss=2.265681, validation/num_examples=50000
I0203 20:59:23.834192 139923868813056 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.844781756401062, loss=4.364187717437744
I0203 21:00:09.588344 139923852027648 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.0919110774993896, loss=3.4266419410705566
I0203 21:00:56.729713 139923868813056 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.167722225189209, loss=3.0889945030212402
I0203 21:01:43.594920 139923852027648 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8447388410568237, loss=4.470295429229736
I0203 21:02:30.255661 139923868813056 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1444156169891357, loss=3.0518040657043457
I0203 21:03:17.177232 139923852027648 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.289015531539917, loss=3.1996660232543945
I0203 21:04:04.188035 139923868813056 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0949443578720093, loss=3.0853657722473145
I0203 21:04:50.792092 139923852027648 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9554743766784668, loss=5.434185028076172
I0203 21:05:37.583428 139923868813056 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6987645030021667, loss=5.457834720611572
I0203 21:06:01.488203 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:06:11.920043 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:06:44.670824 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:06:46.313753 140085747812160 submission_runner.py:408] Time since start: 11671.59s, 	Step: 22552, 	{'train/accuracy': 0.5475195050239563, 'train/loss': 1.9644417762756348, 'validation/accuracy': 0.5002999901771545, 'validation/loss': 2.18509578704834, 'validation/num_examples': 50000, 'test/accuracy': 0.39170002937316895, 'test/loss': 2.8608169555664062, 'test/num_examples': 10000, 'score': 10543.569134950638, 'total_duration': 11671.593814611435, 'accumulated_submission_time': 10543.569134950638, 'accumulated_eval_time': 1126.02326130867, 'accumulated_logging_time': 0.7283682823181152}
I0203 21:06:46.333914 139923852027648 logging_writer.py:48] [22552] accumulated_eval_time=1126.023261, accumulated_logging_time=0.728368, accumulated_submission_time=10543.569135, global_step=22552, preemption_count=0, score=10543.569135, test/accuracy=0.391700, test/loss=2.860817, test/num_examples=10000, total_duration=11671.593815, train/accuracy=0.547520, train/loss=1.964442, validation/accuracy=0.500300, validation/loss=2.185096, validation/num_examples=50000
I0203 21:07:06.465353 139923868813056 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.0195802450180054, loss=3.941890001296997
I0203 21:07:51.676224 139923852027648 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.988270103931427, loss=3.3890438079833984
I0203 21:08:38.707308 139923868813056 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.9742489457130432, loss=3.5433034896850586
I0203 21:09:25.512289 139923852027648 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.0455611944198608, loss=3.1134402751922607
I0203 21:10:12.554324 139923868813056 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8455005288124084, loss=5.530126571655273
I0203 21:10:59.282424 139923852027648 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.518622636795044, loss=3.0740840435028076
I0203 21:11:46.070573 139923868813056 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.8112956285476685, loss=5.087933540344238
I0203 21:12:33.023878 139923852027648 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.150155782699585, loss=3.1924476623535156
I0203 21:13:19.791985 139923868813056 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0037219524383545, loss=3.6973419189453125
I0203 21:13:46.593309 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:13:56.949525 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:14:33.488639 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:14:35.130857 140085747812160 submission_runner.py:408] Time since start: 12140.41s, 	Step: 23459, 	{'train/accuracy': 0.5622265338897705, 'train/loss': 1.8803318738937378, 'validation/accuracy': 0.5024799704551697, 'validation/loss': 2.188140630722046, 'validation/num_examples': 50000, 'test/accuracy': 0.3927000164985657, 'test/loss': 2.830965995788574, 'test/num_examples': 10000, 'score': 10963.765152215958, 'total_duration': 12140.410922527313, 'accumulated_submission_time': 10963.765152215958, 'accumulated_eval_time': 1174.560831785202, 'accumulated_logging_time': 0.7606453895568848}
I0203 21:14:35.151200 139923852027648 logging_writer.py:48] [23459] accumulated_eval_time=1174.560832, accumulated_logging_time=0.760645, accumulated_submission_time=10963.765152, global_step=23459, preemption_count=0, score=10963.765152, test/accuracy=0.392700, test/loss=2.830966, test/num_examples=10000, total_duration=12140.410923, train/accuracy=0.562227, train/loss=1.880332, validation/accuracy=0.502480, validation/loss=2.188141, validation/num_examples=50000
I0203 21:14:52.400168 139923868813056 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.0491739511489868, loss=3.0670666694641113
I0203 21:15:37.256241 139923852027648 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.358490228652954, loss=3.014765739440918
I0203 21:16:24.231302 139923868813056 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.2739545106887817, loss=3.7291812896728516
I0203 21:17:11.217221 139923852027648 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.9526060819625854, loss=3.7240214347839355
I0203 21:17:57.919890 139923868813056 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.781411349773407, loss=5.4839043617248535
I0203 21:18:44.866595 139923852027648 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9104976058006287, loss=3.891913652420044
I0203 21:19:31.690597 139923868813056 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.193504810333252, loss=3.0758097171783447
I0203 21:20:18.588919 139923852027648 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.067896842956543, loss=5.495454788208008
I0203 21:21:05.737095 139923868813056 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.9346799850463867, loss=4.143897533416748
I0203 21:21:35.450821 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:21:46.118209 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:22:13.625145 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:22:15.307614 140085747812160 submission_runner.py:408] Time since start: 12600.59s, 	Step: 24365, 	{'train/accuracy': 0.5406445264816284, 'train/loss': 2.031728982925415, 'validation/accuracy': 0.5039199590682983, 'validation/loss': 2.2272121906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.39340001344680786, 'test/loss': 2.869940996170044, 'test/num_examples': 10000, 'score': 11384.004220485687, 'total_duration': 12600.587675094604, 'accumulated_submission_time': 11384.004220485687, 'accumulated_eval_time': 1214.4176337718964, 'accumulated_logging_time': 0.7908592224121094}
I0203 21:22:15.330102 139923852027648 logging_writer.py:48] [24365] accumulated_eval_time=1214.417634, accumulated_logging_time=0.790859, accumulated_submission_time=11384.004220, global_step=24365, preemption_count=0, score=11384.004220, test/accuracy=0.393400, test/loss=2.869941, test/num_examples=10000, total_duration=12600.587675, train/accuracy=0.540645, train/loss=2.031729, validation/accuracy=0.503920, validation/loss=2.227212, validation/num_examples=50000
I0203 21:22:30.125154 139923868813056 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.8660580515861511, loss=4.497134685516357
I0203 21:23:14.730861 139923852027648 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.1278024911880493, loss=3.1113619804382324
I0203 21:24:01.753673 139923868813056 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.0932929515838623, loss=3.1699085235595703
I0203 21:24:48.544711 139923852027648 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.7267825603485107, loss=5.5116376876831055
I0203 21:25:35.427426 139923868813056 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.0825649499893188, loss=2.8673348426818848
I0203 21:26:22.203341 139923852027648 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.117635726928711, loss=3.091294288635254
I0203 21:27:09.131686 139923868813056 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7834157943725586, loss=5.0572052001953125
I0203 21:27:55.880542 139923852027648 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.7807733416557312, loss=4.726462364196777
I0203 21:28:42.586093 139923868813056 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.160384178161621, loss=3.0999600887298584
I0203 21:29:15.575386 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:29:26.346406 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:30:01.342621 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:30:02.995180 140085747812160 submission_runner.py:408] Time since start: 13068.28s, 	Step: 25272, 	{'train/accuracy': 0.5468554496765137, 'train/loss': 1.9566597938537598, 'validation/accuracy': 0.5080199837684631, 'validation/loss': 2.159663438796997, 'validation/num_examples': 50000, 'test/accuracy': 0.3937000334262848, 'test/loss': 2.8316471576690674, 'test/num_examples': 10000, 'score': 11804.188628435135, 'total_duration': 13068.275243759155, 'accumulated_submission_time': 11804.188628435135, 'accumulated_eval_time': 1261.837417602539, 'accumulated_logging_time': 0.822918176651001}
I0203 21:30:03.016510 139923852027648 logging_writer.py:48] [25272] accumulated_eval_time=1261.837418, accumulated_logging_time=0.822918, accumulated_submission_time=11804.188628, global_step=25272, preemption_count=0, score=11804.188628, test/accuracy=0.393700, test/loss=2.831647, test/num_examples=10000, total_duration=13068.275244, train/accuracy=0.546855, train/loss=1.956660, validation/accuracy=0.508020, validation/loss=2.159663, validation/num_examples=50000
I0203 21:30:15.075203 139923868813056 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1614841222763062, loss=3.047952890396118
I0203 21:30:59.079196 139923852027648 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.1687465906143188, loss=3.1492176055908203
I0203 21:31:45.945803 139923868813056 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8834936022758484, loss=3.9293298721313477
I0203 21:32:32.961225 139923852027648 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.8336392641067505, loss=5.52901554107666
I0203 21:33:19.824237 139923868813056 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.283260464668274, loss=3.1618497371673584
I0203 21:34:06.580896 139923852027648 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.2663158178329468, loss=2.9758687019348145
I0203 21:34:53.262763 139923868813056 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0308938026428223, loss=2.954714298248291
I0203 21:35:40.351587 139923852027648 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0357906818389893, loss=4.289324760437012
I0203 21:36:27.174442 139923868813056 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.1294012069702148, loss=2.9508132934570312
I0203 21:37:03.004410 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:37:13.569700 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:37:43.556279 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:37:45.197736 140085747812160 submission_runner.py:408] Time since start: 13530.48s, 	Step: 26178, 	{'train/accuracy': 0.5712695121765137, 'train/loss': 1.8304411172866821, 'validation/accuracy': 0.519760012626648, 'validation/loss': 2.1027395725250244, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7602477073669434, 'test/num_examples': 10000, 'score': 12224.113825082779, 'total_duration': 13530.477799415588, 'accumulated_submission_time': 12224.113825082779, 'accumulated_eval_time': 1304.0307462215424, 'accumulated_logging_time': 0.8551509380340576}
I0203 21:37:45.221738 139923852027648 logging_writer.py:48] [26178] accumulated_eval_time=1304.030746, accumulated_logging_time=0.855151, accumulated_submission_time=12224.113825, global_step=26178, preemption_count=0, score=12224.113825, test/accuracy=0.407500, test/loss=2.760248, test/num_examples=10000, total_duration=13530.477799, train/accuracy=0.571270, train/loss=1.830441, validation/accuracy=0.519760, validation/loss=2.102740, validation/num_examples=50000
I0203 21:37:54.665749 139923868813056 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1184077262878418, loss=2.989461898803711
I0203 21:38:38.614356 139923852027648 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.8741430640220642, loss=4.3720574378967285
I0203 21:39:25.173661 139923868813056 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9378818273544312, loss=4.221194267272949
I0203 21:40:12.162516 139923852027648 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.0550706386566162, loss=3.0561342239379883
I0203 21:40:59.238902 139923868813056 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0995985269546509, loss=3.1270482540130615
I0203 21:41:46.055773 139923852027648 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.954605221748352, loss=3.995286226272583
I0203 21:42:33.264722 139923868813056 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.9675384163856506, loss=3.4741454124450684
I0203 21:43:20.288247 139923852027648 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.3440741300582886, loss=2.903360605239868
I0203 21:44:07.127385 139923868813056 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.0032892227172852, loss=4.9214653968811035
I0203 21:44:45.302507 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:44:55.936531 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:45:28.369595 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:45:30.004039 140085747812160 submission_runner.py:408] Time since start: 13995.28s, 	Step: 27083, 	{'train/accuracy': 0.5553905963897705, 'train/loss': 1.8902047872543335, 'validation/accuracy': 0.5142999887466431, 'validation/loss': 2.0941996574401855, 'validation/num_examples': 50000, 'test/accuracy': 0.4067000150680542, 'test/loss': 2.7455155849456787, 'test/num_examples': 10000, 'score': 12644.132603883743, 'total_duration': 13995.284102916718, 'accumulated_submission_time': 12644.132603883743, 'accumulated_eval_time': 1348.732283115387, 'accumulated_logging_time': 0.8903157711029053}
I0203 21:45:30.029824 139923852027648 logging_writer.py:48] [27083] accumulated_eval_time=1348.732283, accumulated_logging_time=0.890316, accumulated_submission_time=12644.132604, global_step=27083, preemption_count=0, score=12644.132604, test/accuracy=0.406700, test/loss=2.745516, test/num_examples=10000, total_duration=13995.284103, train/accuracy=0.555391, train/loss=1.890205, validation/accuracy=0.514300, validation/loss=2.094200, validation/num_examples=50000
I0203 21:45:37.425395 139923868813056 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.8733201026916504, loss=4.596142292022705
I0203 21:46:21.214651 139923852027648 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.865302562713623, loss=4.673823356628418
I0203 21:47:08.119072 139923868813056 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0697965621948242, loss=2.795797109603882
I0203 21:47:55.007261 139923852027648 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.9433777928352356, loss=5.045332908630371
I0203 21:48:41.802609 139923868813056 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.176442265510559, loss=3.4702181816101074
I0203 21:49:28.670619 139923852027648 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1106247901916504, loss=3.365272283554077
I0203 21:50:15.426682 139923868813056 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.0302023887634277, loss=2.9051601886749268
I0203 21:51:02.229461 139923852027648 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.1900858879089355, loss=2.911691427230835
I0203 21:51:49.141798 139923868813056 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.177878975868225, loss=2.925332546234131
I0203 21:52:30.015861 140085747812160 spec.py:321] Evaluating on the training split.
I0203 21:52:40.700612 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 21:53:10.573151 140085747812160 spec.py:349] Evaluating on the test split.
I0203 21:53:12.215053 140085747812160 submission_runner.py:408] Time since start: 14457.50s, 	Step: 27989, 	{'train/accuracy': 0.5625976324081421, 'train/loss': 1.8604013919830322, 'validation/accuracy': 0.5207599997520447, 'validation/loss': 2.0770411491394043, 'validation/num_examples': 50000, 'test/accuracy': 0.4098000228404999, 'test/loss': 2.7530479431152344, 'test/num_examples': 10000, 'score': 13064.056718111038, 'total_duration': 14457.495115756989, 'accumulated_submission_time': 13064.056718111038, 'accumulated_eval_time': 1390.9314963817596, 'accumulated_logging_time': 0.926861047744751}
I0203 21:53:12.236369 139923852027648 logging_writer.py:48] [27989] accumulated_eval_time=1390.931496, accumulated_logging_time=0.926861, accumulated_submission_time=13064.056718, global_step=27989, preemption_count=0, score=13064.056718, test/accuracy=0.409800, test/loss=2.753048, test/num_examples=10000, total_duration=14457.495116, train/accuracy=0.562598, train/loss=1.860401, validation/accuracy=0.520760, validation/loss=2.077041, validation/num_examples=50000
I0203 21:53:17.172466 139923868813056 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.8254001140594482, loss=5.058073043823242
I0203 21:54:00.253026 139923852027648 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1532524824142456, loss=2.8572335243225098
I0203 21:54:47.014714 139923868813056 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.0750157833099365, loss=2.844921827316284
I0203 21:55:34.032371 139923852027648 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1086909770965576, loss=2.9422779083251953
I0203 21:56:20.814155 139923868813056 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.05992591381073, loss=3.4456045627593994
I0203 21:57:07.751636 139923852027648 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8638268113136292, loss=4.883448123931885
I0203 21:57:54.676686 139923868813056 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0728591680526733, loss=2.867764711380005
I0203 21:58:41.547519 139923852027648 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.0751843452453613, loss=2.9912452697753906
I0203 21:59:28.642931 139923868813056 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.323053002357483, loss=3.101193904876709
I0203 22:00:12.630590 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:00:23.123147 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:00:57.146683 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:00:58.779132 140085747812160 submission_runner.py:408] Time since start: 14924.06s, 	Step: 28896, 	{'train/accuracy': 0.5817968845367432, 'train/loss': 1.820831298828125, 'validation/accuracy': 0.5314399600028992, 'validation/loss': 2.074846029281616, 'validation/num_examples': 50000, 'test/accuracy': 0.4182000160217285, 'test/loss': 2.7303760051727295, 'test/num_examples': 10000, 'score': 13484.38296675682, 'total_duration': 14924.05919790268, 'accumulated_submission_time': 13484.38296675682, 'accumulated_eval_time': 1437.0800392627716, 'accumulated_logging_time': 0.9647171497344971}
I0203 22:00:58.801109 139923852027648 logging_writer.py:48] [28896] accumulated_eval_time=1437.080039, accumulated_logging_time=0.964717, accumulated_submission_time=13484.382967, global_step=28896, preemption_count=0, score=13484.382967, test/accuracy=0.418200, test/loss=2.730376, test/num_examples=10000, total_duration=14924.059198, train/accuracy=0.581797, train/loss=1.820831, validation/accuracy=0.531440, validation/loss=2.074846, validation/num_examples=50000
I0203 22:01:00.862599 139923868813056 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.049191951751709, loss=3.1593990325927734
I0203 22:01:43.556302 139923852027648 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8660135865211487, loss=4.168679714202881
I0203 22:02:30.562175 139923868813056 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.8983988165855408, loss=5.577221870422363
I0203 22:03:17.556373 139923852027648 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1402825117111206, loss=2.8613762855529785
I0203 22:04:04.222728 139923868813056 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.8723070025444031, loss=5.570550918579102
I0203 22:04:51.006160 139923852027648 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.0707213878631592, loss=4.0982232093811035
I0203 22:05:37.848073 139923868813056 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9413229823112488, loss=3.7477030754089355
I0203 22:06:24.695149 139923852027648 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.8626258373260498, loss=4.759968280792236
I0203 22:07:11.598954 139923868813056 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.8457533121109009, loss=5.502603530883789
I0203 22:07:58.361774 139923852027648 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.1323509216308594, loss=2.897963285446167
I0203 22:07:58.916298 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:08:09.158115 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:08:43.104525 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:08:44.748999 140085747812160 submission_runner.py:408] Time since start: 15390.03s, 	Step: 29803, 	{'train/accuracy': 0.5716015696525574, 'train/loss': 1.8176162242889404, 'validation/accuracy': 0.5288999676704407, 'validation/loss': 2.0264298915863037, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7066798210144043, 'test/num_examples': 10000, 'score': 13904.436289548874, 'total_duration': 15390.029060602188, 'accumulated_submission_time': 13904.436289548874, 'accumulated_eval_time': 1482.9127361774445, 'accumulated_logging_time': 0.9980545043945312}
I0203 22:08:44.769969 139923868813056 logging_writer.py:48] [29803] accumulated_eval_time=1482.912736, accumulated_logging_time=0.998055, accumulated_submission_time=13904.436290, global_step=29803, preemption_count=0, score=13904.436290, test/accuracy=0.415800, test/loss=2.706680, test/num_examples=10000, total_duration=15390.029061, train/accuracy=0.571602, train/loss=1.817616, validation/accuracy=0.528900, validation/loss=2.026430, validation/num_examples=50000
I0203 22:09:26.066500 139923852027648 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.0781242847442627, loss=2.9914705753326416
I0203 22:10:12.690091 139923868813056 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.0680862665176392, loss=5.538282871246338
I0203 22:10:59.937377 139923852027648 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.2156550884246826, loss=2.9537181854248047
I0203 22:11:46.608063 139923868813056 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.020755410194397, loss=3.597191333770752
I0203 22:12:33.630976 139923852027648 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3334418535232544, loss=2.9787445068359375
I0203 22:13:20.442500 139923868813056 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0502402782440186, loss=2.9847614765167236
I0203 22:14:07.362311 139923852027648 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.2488449811935425, loss=2.9266061782836914
I0203 22:14:54.182928 139923868813056 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.8325709104537964, loss=5.456134796142578
I0203 22:15:41.059664 139923852027648 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2308906316757202, loss=2.8110873699188232
I0203 22:15:44.906349 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:15:55.427522 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:16:30.431762 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:16:32.074643 140085747812160 submission_runner.py:408] Time since start: 15857.35s, 	Step: 30710, 	{'train/accuracy': 0.5737109184265137, 'train/loss': 1.8410923480987549, 'validation/accuracy': 0.5341199636459351, 'validation/loss': 2.0497286319732666, 'validation/num_examples': 50000, 'test/accuracy': 0.4155000150203705, 'test/loss': 2.7235465049743652, 'test/num_examples': 10000, 'score': 14324.511536359787, 'total_duration': 15857.354704141617, 'accumulated_submission_time': 14324.511536359787, 'accumulated_eval_time': 1530.0810143947601, 'accumulated_logging_time': 1.0294535160064697}
I0203 22:16:32.095555 139923868813056 logging_writer.py:48] [30710] accumulated_eval_time=1530.081014, accumulated_logging_time=1.029454, accumulated_submission_time=14324.511536, global_step=30710, preemption_count=0, score=14324.511536, test/accuracy=0.415500, test/loss=2.723547, test/num_examples=10000, total_duration=15857.354704, train/accuracy=0.573711, train/loss=1.841092, validation/accuracy=0.534120, validation/loss=2.049729, validation/num_examples=50000
I0203 22:17:10.575947 139923852027648 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.2396749258041382, loss=2.8767170906066895
I0203 22:17:57.235564 139923868813056 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2702021598815918, loss=2.9710943698883057
I0203 22:18:44.444537 139923852027648 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1708892583847046, loss=2.9203147888183594
I0203 22:19:31.411005 139923868813056 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1028467416763306, loss=2.8715152740478516
I0203 22:20:18.335575 139923852027648 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9868847131729126, loss=3.2717864513397217
I0203 22:21:05.724352 139923868813056 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.9871560335159302, loss=3.3478689193725586
I0203 22:21:52.655901 139923852027648 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.075508952140808, loss=3.934372663497925
I0203 22:22:39.907425 139923868813056 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1083214282989502, loss=2.7471041679382324
I0203 22:23:26.933205 139923852027648 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.126628041267395, loss=2.8952383995056152
I0203 22:23:32.209429 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:23:42.624072 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:24:16.895242 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:24:18.536123 140085747812160 submission_runner.py:408] Time since start: 16323.82s, 	Step: 31613, 	{'train/accuracy': 0.5854882597923279, 'train/loss': 1.7064871788024902, 'validation/accuracy': 0.5389999747276306, 'validation/loss': 1.964975118637085, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.6509931087493896, 'test/num_examples': 10000, 'score': 14744.565649032593, 'total_duration': 16323.816191673279, 'accumulated_submission_time': 14744.565649032593, 'accumulated_eval_time': 1576.4077117443085, 'accumulated_logging_time': 1.0594823360443115}
I0203 22:24:18.556943 139923868813056 logging_writer.py:48] [31613] accumulated_eval_time=1576.407712, accumulated_logging_time=1.059482, accumulated_submission_time=14744.565649, global_step=31613, preemption_count=0, score=14744.565649, test/accuracy=0.419100, test/loss=2.650993, test/num_examples=10000, total_duration=16323.816192, train/accuracy=0.585488, train/loss=1.706487, validation/accuracy=0.539000, validation/loss=1.964975, validation/num_examples=50000
I0203 22:24:55.714884 139923852027648 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1669238805770874, loss=2.816439151763916
I0203 22:25:42.390022 139923868813056 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.2254916429519653, loss=2.966130256652832
I0203 22:26:29.439580 139923852027648 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2922688722610474, loss=2.86194109916687
I0203 22:27:16.126153 139923868813056 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.9360394477844238, loss=4.132213115692139
I0203 22:28:03.017914 139923852027648 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.2556843757629395, loss=2.7875378131866455
I0203 22:28:50.014469 139923868813056 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.1616957187652588, loss=2.7510721683502197
I0203 22:29:36.802036 139923852027648 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.228829026222229, loss=2.8283679485321045
I0203 22:30:23.642016 139923868813056 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.0508930683135986, loss=3.5191707611083984
I0203 22:31:10.497619 139923852027648 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1613343954086304, loss=2.914989471435547
I0203 22:31:18.806890 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:31:29.062066 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:31:59.167016 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:32:00.805739 140085747812160 submission_runner.py:408] Time since start: 16786.09s, 	Step: 32519, 	{'train/accuracy': 0.5890039205551147, 'train/loss': 1.7607898712158203, 'validation/accuracy': 0.5417999625205994, 'validation/loss': 1.980234980583191, 'validation/num_examples': 50000, 'test/accuracy': 0.42820000648498535, 'test/loss': 2.6211376190185547, 'test/num_examples': 10000, 'score': 15164.755456447601, 'total_duration': 16786.085805416107, 'accumulated_submission_time': 15164.755456447601, 'accumulated_eval_time': 1618.4065613746643, 'accumulated_logging_time': 1.089357614517212}
I0203 22:32:00.826136 139923868813056 logging_writer.py:48] [32519] accumulated_eval_time=1618.406561, accumulated_logging_time=1.089358, accumulated_submission_time=15164.755456, global_step=32519, preemption_count=0, score=15164.755456, test/accuracy=0.428200, test/loss=2.621138, test/num_examples=10000, total_duration=16786.085805, train/accuracy=0.589004, train/loss=1.760790, validation/accuracy=0.541800, validation/loss=1.980235, validation/num_examples=50000
I0203 22:32:35.462265 139923852027648 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.1154097318649292, loss=3.283299207687378
I0203 22:33:21.851510 139923868813056 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.9693073034286499, loss=3.4807543754577637
I0203 22:34:08.894009 139923852027648 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.093877911567688, loss=3.0357189178466797
I0203 22:34:55.667819 139923868813056 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.545483112335205, loss=2.9225142002105713
I0203 22:35:42.510649 139923852027648 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.188429832458496, loss=2.877023935317993
I0203 22:36:29.315895 139923868813056 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1309335231781006, loss=2.9479520320892334
I0203 22:37:16.021172 139923852027648 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.8861622214317322, loss=3.972757339477539
I0203 22:38:03.012838 139923868813056 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1176187992095947, loss=2.8576159477233887
I0203 22:38:49.811823 139923852027648 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.9453057050704956, loss=3.8797967433929443
I0203 22:39:01.223651 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:39:11.822312 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:39:45.411303 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:39:47.052625 140085747812160 submission_runner.py:408] Time since start: 17252.33s, 	Step: 33426, 	{'train/accuracy': 0.5855468511581421, 'train/loss': 1.7376009225845337, 'validation/accuracy': 0.5393800139427185, 'validation/loss': 1.9697725772857666, 'validation/num_examples': 50000, 'test/accuracy': 0.42080003023147583, 'test/loss': 2.6821558475494385, 'test/num_examples': 10000, 'score': 15585.092749595642, 'total_duration': 17252.33269262314, 'accumulated_submission_time': 15585.092749595642, 'accumulated_eval_time': 1664.2355268001556, 'accumulated_logging_time': 1.1192798614501953}
I0203 22:39:47.075726 139923868813056 logging_writer.py:48] [33426] accumulated_eval_time=1664.235527, accumulated_logging_time=1.119280, accumulated_submission_time=15585.092750, global_step=33426, preemption_count=0, score=15585.092750, test/accuracy=0.420800, test/loss=2.682156, test/num_examples=10000, total_duration=17252.332693, train/accuracy=0.585547, train/loss=1.737601, validation/accuracy=0.539380, validation/loss=1.969773, validation/num_examples=50000
I0203 22:40:17.999490 139923852027648 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.1407803297042847, loss=2.9588782787323
I0203 22:41:04.483241 139923868813056 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.9723636507987976, loss=3.5065696239471436
I0203 22:41:51.651308 139923852027648 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.2370957136154175, loss=2.9705100059509277
I0203 22:42:38.887821 139923868813056 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1020839214324951, loss=2.900292158126831
I0203 22:43:25.791213 139923852027648 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.9176145195960999, loss=4.753078460693359
I0203 22:44:12.576903 139923868813056 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1166411638259888, loss=2.7166476249694824
I0203 22:44:59.440721 139923852027648 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.9434500336647034, loss=5.138866424560547
I0203 22:45:46.219006 139923868813056 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.8716714382171631, loss=5.384400367736816
I0203 22:46:33.207196 139923852027648 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.1273670196533203, loss=2.773341655731201
I0203 22:46:47.304558 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:46:57.952521 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:47:31.265019 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:47:32.908716 140085747812160 submission_runner.py:408] Time since start: 17718.19s, 	Step: 34332, 	{'train/accuracy': 0.5911523103713989, 'train/loss': 1.7255398035049438, 'validation/accuracy': 0.5455799698829651, 'validation/loss': 1.9740146398544312, 'validation/num_examples': 50000, 'test/accuracy': 0.42810001969337463, 'test/loss': 2.6390552520751953, 'test/num_examples': 10000, 'score': 16005.261153697968, 'total_duration': 17718.188776254654, 'accumulated_submission_time': 16005.261153697968, 'accumulated_eval_time': 1709.8396821022034, 'accumulated_logging_time': 1.1521375179290771}
I0203 22:47:32.934940 139923868813056 logging_writer.py:48] [34332] accumulated_eval_time=1709.839682, accumulated_logging_time=1.152138, accumulated_submission_time=16005.261154, global_step=34332, preemption_count=0, score=16005.261154, test/accuracy=0.428100, test/loss=2.639055, test/num_examples=10000, total_duration=17718.188776, train/accuracy=0.591152, train/loss=1.725540, validation/accuracy=0.545580, validation/loss=1.974015, validation/num_examples=50000
I0203 22:48:01.277278 139923852027648 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.0241934061050415, loss=3.5924062728881836
I0203 22:48:47.340267 139923868813056 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9845964312553406, loss=3.508065938949585
I0203 22:49:34.109438 139923852027648 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.0880481004714966, loss=3.1282172203063965
I0203 22:50:20.676909 139923868813056 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.1346796751022339, loss=2.842775821685791
I0203 22:51:07.870801 139923852027648 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.0381948947906494, loss=2.6954727172851562
I0203 22:51:54.561862 139923868813056 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2063789367675781, loss=5.278334617614746
I0203 22:52:41.547910 139923852027648 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.2040114402770996, loss=2.7247562408447266
I0203 22:53:28.574863 139923868813056 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.2047029733657837, loss=2.7990899085998535
I0203 22:54:15.313234 139923852027648 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.9322124123573303, loss=5.3792500495910645
I0203 22:54:33.294574 140085747812160 spec.py:321] Evaluating on the training split.
I0203 22:54:43.571019 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 22:55:17.713385 140085747812160 spec.py:349] Evaluating on the test split.
I0203 22:55:19.353090 140085747812160 submission_runner.py:408] Time since start: 18184.63s, 	Step: 35240, 	{'train/accuracy': 0.6228320002555847, 'train/loss': 1.5777928829193115, 'validation/accuracy': 0.555180013179779, 'validation/loss': 1.9033994674682617, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.584881544113159, 'test/num_examples': 10000, 'score': 16425.55720925331, 'total_duration': 18184.633157491684, 'accumulated_submission_time': 16425.55720925331, 'accumulated_eval_time': 1755.8981931209564, 'accumulated_logging_time': 1.190577507019043}
I0203 22:55:19.376238 139923868813056 logging_writer.py:48] [35240] accumulated_eval_time=1755.898193, accumulated_logging_time=1.190578, accumulated_submission_time=16425.557209, global_step=35240, preemption_count=0, score=16425.557209, test/accuracy=0.441100, test/loss=2.584882, test/num_examples=10000, total_duration=18184.633157, train/accuracy=0.622832, train/loss=1.577793, validation/accuracy=0.555180, validation/loss=1.903399, validation/num_examples=50000
I0203 22:55:44.423479 139923852027648 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.0205628871917725, loss=5.162714958190918
I0203 22:56:30.291735 139923868813056 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.0187309980392456, loss=3.4873154163360596
I0203 22:57:17.715457 139923852027648 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.1320608854293823, loss=2.84167218208313
I0203 22:58:04.554554 139923868813056 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1735605001449585, loss=2.948634147644043
I0203 22:58:51.351881 139923852027648 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0815824270248413, loss=2.755622386932373
I0203 22:59:38.198297 139923868813056 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.9154919981956482, loss=4.390539169311523
I0203 23:00:25.098167 139923852027648 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0886094570159912, loss=3.1893153190612793
I0203 23:01:12.058149 139923868813056 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0315687656402588, loss=3.82169508934021
I0203 23:01:59.263772 139923852027648 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.159450650215149, loss=2.7141172885894775
I0203 23:02:19.535422 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:02:30.397337 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:03:03.087507 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:03:04.733347 140085747812160 submission_runner.py:408] Time since start: 18650.01s, 	Step: 36145, 	{'train/accuracy': 0.5803906321525574, 'train/loss': 1.8397955894470215, 'validation/accuracy': 0.5388000011444092, 'validation/loss': 2.0403988361358643, 'validation/num_examples': 50000, 'test/accuracy': 0.4221000075340271, 'test/loss': 2.6815497875213623, 'test/num_examples': 10000, 'score': 16845.653829813004, 'total_duration': 18650.0134100914, 'accumulated_submission_time': 16845.653829813004, 'accumulated_eval_time': 1801.0961050987244, 'accumulated_logging_time': 1.2253484725952148}
I0203 23:03:04.759068 139923868813056 logging_writer.py:48] [36145] accumulated_eval_time=1801.096105, accumulated_logging_time=1.225348, accumulated_submission_time=16845.653830, global_step=36145, preemption_count=0, score=16845.653830, test/accuracy=0.422100, test/loss=2.681550, test/num_examples=10000, total_duration=18650.013410, train/accuracy=0.580391, train/loss=1.839796, validation/accuracy=0.538800, validation/loss=2.040399, validation/num_examples=50000
I0203 23:03:27.749845 139923852027648 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.215717077255249, loss=2.8188741207122803
I0203 23:04:13.779548 139923868813056 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.9819993376731873, loss=4.631789207458496
I0203 23:05:00.633495 139923852027648 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.102523684501648, loss=2.794097661972046
I0203 23:05:48.063658 139923868813056 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1909713745117188, loss=2.9299702644348145
I0203 23:06:34.707216 139923852027648 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.07830810546875, loss=2.735199213027954
I0203 23:07:21.600697 139923868813056 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.3136199712753296, loss=2.7985639572143555
I0203 23:08:08.585114 139923852027648 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.2421798706054688, loss=2.8854575157165527
I0203 23:08:55.346338 139923868813056 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1126350164413452, loss=2.9618735313415527
I0203 23:09:42.365528 139923852027648 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8716525435447693, loss=4.803797721862793
I0203 23:10:05.001030 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:10:15.703911 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:10:50.333405 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:10:51.976004 140085747812160 submission_runner.py:408] Time since start: 19117.26s, 	Step: 37050, 	{'train/accuracy': 0.6094335913658142, 'train/loss': 1.6258376836776733, 'validation/accuracy': 0.5605800151824951, 'validation/loss': 1.8679131269454956, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.5216729640960693, 'test/num_examples': 10000, 'score': 17265.834097862244, 'total_duration': 19117.25606751442, 'accumulated_submission_time': 17265.834097862244, 'accumulated_eval_time': 1848.0710878372192, 'accumulated_logging_time': 1.2622802257537842}
I0203 23:10:52.001312 139923868813056 logging_writer.py:48] [37050] accumulated_eval_time=1848.071088, accumulated_logging_time=1.262280, accumulated_submission_time=17265.834098, global_step=37050, preemption_count=0, score=17265.834098, test/accuracy=0.442900, test/loss=2.521673, test/num_examples=10000, total_duration=19117.256068, train/accuracy=0.609434, train/loss=1.625838, validation/accuracy=0.560580, validation/loss=1.867913, validation/num_examples=50000
I0203 23:11:12.947793 139923852027648 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.308344841003418, loss=2.9056906700134277
I0203 23:11:58.691345 139923868813056 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.9613719582557678, loss=4.697594165802002
I0203 23:12:45.892581 139923852027648 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.2081512212753296, loss=3.098735809326172
I0203 23:13:32.816602 139923868813056 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1657671928405762, loss=2.6637802124023438
I0203 23:14:19.928455 139923852027648 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.0354901552200317, loss=2.795084238052368
I0203 23:15:07.203662 139923868813056 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.9035497307777405, loss=4.4451775550842285
I0203 23:15:53.931412 139923852027648 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.1271098852157593, loss=2.7641193866729736
I0203 23:16:41.061088 139923868813056 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1293253898620605, loss=2.807250499725342
I0203 23:17:28.102308 139923852027648 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.7912033796310425, loss=5.083433151245117
I0203 23:17:52.187079 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:18:02.708157 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:18:32.441881 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:18:34.091732 140085747812160 submission_runner.py:408] Time since start: 19579.37s, 	Step: 37953, 	{'train/accuracy': 0.6267187595367432, 'train/loss': 1.5315265655517578, 'validation/accuracy': 0.5590599775314331, 'validation/loss': 1.8768879175186157, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.54549503326416, 'test/num_examples': 10000, 'score': 17685.957972049713, 'total_duration': 19579.371796131134, 'accumulated_submission_time': 17685.957972049713, 'accumulated_eval_time': 1889.9757521152496, 'accumulated_logging_time': 1.2984967231750488}
I0203 23:18:34.115593 139923868813056 logging_writer.py:48] [37953] accumulated_eval_time=1889.975752, accumulated_logging_time=1.298497, accumulated_submission_time=17685.957972, global_step=37953, preemption_count=0, score=17685.957972, test/accuracy=0.442300, test/loss=2.545495, test/num_examples=10000, total_duration=19579.371796, train/accuracy=0.626719, train/loss=1.531527, validation/accuracy=0.559060, validation/loss=1.876888, validation/num_examples=50000
I0203 23:18:53.846508 139923852027648 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1059916019439697, loss=3.3517394065856934
I0203 23:19:39.436699 139923868813056 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1944633722305298, loss=2.765127420425415
I0203 23:20:26.435557 139923852027648 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1162457466125488, loss=2.860853910446167
I0203 23:21:13.147561 139923868813056 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.3756150007247925, loss=5.488253116607666
I0203 23:21:59.903838 139923852027648 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.1278325319290161, loss=2.8006064891815186
I0203 23:22:46.787008 139923868813056 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.1249480247497559, loss=2.90385103225708
I0203 23:23:33.680597 139923852027648 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.8328524231910706, loss=4.995399475097656
I0203 23:24:20.721957 139923868813056 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.0326021909713745, loss=3.2346372604370117
I0203 23:25:07.846773 139923852027648 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.903437077999115, loss=4.426318645477295
I0203 23:25:34.146788 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:25:44.406330 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:26:20.683026 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:26:22.329799 140085747812160 submission_runner.py:408] Time since start: 20047.61s, 	Step: 38858, 	{'train/accuracy': 0.6036718487739563, 'train/loss': 1.6944724321365356, 'validation/accuracy': 0.5605999827384949, 'validation/loss': 1.9091485738754272, 'validation/num_examples': 50000, 'test/accuracy': 0.44440001249313354, 'test/loss': 2.581925630569458, 'test/num_examples': 10000, 'score': 18105.92478275299, 'total_duration': 20047.60985803604, 'accumulated_submission_time': 18105.92478275299, 'accumulated_eval_time': 1938.1587483882904, 'accumulated_logging_time': 1.3353419303894043}
I0203 23:26:22.354727 139923868813056 logging_writer.py:48] [38858] accumulated_eval_time=1938.158748, accumulated_logging_time=1.335342, accumulated_submission_time=18105.924783, global_step=38858, preemption_count=0, score=18105.924783, test/accuracy=0.444400, test/loss=2.581926, test/num_examples=10000, total_duration=20047.609858, train/accuracy=0.603672, train/loss=1.694472, validation/accuracy=0.560600, validation/loss=1.909149, validation/num_examples=50000
I0203 23:26:40.010119 139923852027648 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.097318410873413, loss=4.6615986824035645
I0203 23:27:24.747475 139923868813056 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.1573176383972168, loss=2.7364649772644043
I0203 23:28:11.709557 139923852027648 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.9740647077560425, loss=3.7417845726013184
I0203 23:28:58.332068 139923868813056 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.9898332953453064, loss=5.39146614074707
I0203 23:29:45.371389 139923852027648 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1207829713821411, loss=3.045626163482666
I0203 23:30:32.179064 139923868813056 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.002215027809143, loss=5.331161022186279
I0203 23:31:18.958688 139923852027648 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.9545083045959473, loss=5.432109355926514
I0203 23:32:05.741483 139923868813056 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1649186611175537, loss=2.837702989578247
I0203 23:32:52.557554 139923852027648 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.1722625494003296, loss=2.7746694087982178
I0203 23:33:22.774042 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:33:33.172249 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:34:08.283584 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:34:09.927556 140085747812160 submission_runner.py:408] Time since start: 20515.21s, 	Step: 39766, 	{'train/accuracy': 0.6093164086341858, 'train/loss': 1.628745675086975, 'validation/accuracy': 0.5579000115394592, 'validation/loss': 1.8780062198638916, 'validation/num_examples': 50000, 'test/accuracy': 0.4431000351905823, 'test/loss': 2.5480268001556396, 'test/num_examples': 10000, 'score': 18526.282481193542, 'total_duration': 20515.20761990547, 'accumulated_submission_time': 18526.282481193542, 'accumulated_eval_time': 1985.312269449234, 'accumulated_logging_time': 1.3708126544952393}
I0203 23:34:09.953284 139923868813056 logging_writer.py:48] [39766] accumulated_eval_time=1985.312269, accumulated_logging_time=1.370813, accumulated_submission_time=18526.282481, global_step=39766, preemption_count=0, score=18526.282481, test/accuracy=0.443100, test/loss=2.548027, test/num_examples=10000, total_duration=20515.207620, train/accuracy=0.609316, train/loss=1.628746, validation/accuracy=0.557900, validation/loss=1.878006, validation/num_examples=50000
I0203 23:34:24.330545 139923852027648 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.0963748693466187, loss=4.5430378913879395
I0203 23:35:08.631447 139923868813056 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.9903610348701477, loss=3.7441577911376953
I0203 23:35:55.772540 139923852027648 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2359850406646729, loss=3.0523102283477783
I0203 23:36:42.793824 139923868813056 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.4149497747421265, loss=2.936150312423706
I0203 23:37:29.522343 139923852027648 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0006144046783447, loss=5.367812633514404
I0203 23:38:16.652516 139923868813056 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.1807324886322021, loss=2.740938663482666
I0203 23:39:03.486168 139923852027648 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1955246925354004, loss=2.8084981441497803
I0203 23:39:50.283400 139923868813056 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0539799928665161, loss=3.193197727203369
I0203 23:40:37.250867 139923852027648 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1444895267486572, loss=2.7929255962371826
I0203 23:41:10.208285 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:41:20.665516 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:41:54.649821 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:41:56.303594 140085747812160 submission_runner.py:408] Time since start: 20981.58s, 	Step: 40672, 	{'train/accuracy': 0.6217382550239563, 'train/loss': 1.5823453664779663, 'validation/accuracy': 0.5575399994850159, 'validation/loss': 1.8897854089736938, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.577730178833008, 'test/num_examples': 10000, 'score': 18946.474817752838, 'total_duration': 20981.583657741547, 'accumulated_submission_time': 18946.474817752838, 'accumulated_eval_time': 2031.4075977802277, 'accumulated_logging_time': 1.408151388168335}
I0203 23:41:56.329866 139923868813056 logging_writer.py:48] [40672] accumulated_eval_time=2031.407598, accumulated_logging_time=1.408151, accumulated_submission_time=18946.474818, global_step=40672, preemption_count=0, score=18946.474818, test/accuracy=0.439700, test/loss=2.577730, test/num_examples=10000, total_duration=20981.583658, train/accuracy=0.621738, train/loss=1.582345, validation/accuracy=0.557540, validation/loss=1.889785, validation/num_examples=50000
I0203 23:42:08.237746 139923852027648 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1420328617095947, loss=3.811652183532715
I0203 23:42:52.631685 139923868813056 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1726479530334473, loss=2.6342389583587646
I0203 23:43:39.561516 139923852027648 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.1223338842391968, loss=2.9794976711273193
I0203 23:44:26.623821 139923868813056 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.2763859033584595, loss=2.7734851837158203
I0203 23:45:13.587209 139923852027648 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1225481033325195, loss=2.883821725845337
I0203 23:46:00.580820 139923868813056 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1589652299880981, loss=3.1725475788116455
I0203 23:46:47.691909 139923852027648 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.9101606607437134, loss=5.18435001373291
I0203 23:47:34.408658 139923868813056 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.143704891204834, loss=2.6584525108337402
I0203 23:48:21.400342 139923852027648 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9996550679206848, loss=4.742201805114746
I0203 23:48:56.602487 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:49:07.177197 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:49:43.533408 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:49:45.175039 140085747812160 submission_runner.py:408] Time since start: 21450.46s, 	Step: 41577, 	{'train/accuracy': 0.6031640768051147, 'train/loss': 1.6544686555862427, 'validation/accuracy': 0.5612999796867371, 'validation/loss': 1.8774502277374268, 'validation/num_examples': 50000, 'test/accuracy': 0.44270002841949463, 'test/loss': 2.5433125495910645, 'test/num_examples': 10000, 'score': 19366.68581557274, 'total_duration': 21450.455101966858, 'accumulated_submission_time': 19366.68581557274, 'accumulated_eval_time': 2079.980150461197, 'accumulated_logging_time': 1.4446847438812256}
I0203 23:49:45.199817 139923868813056 logging_writer.py:48] [41577] accumulated_eval_time=2079.980150, accumulated_logging_time=1.444685, accumulated_submission_time=19366.685816, global_step=41577, preemption_count=0, score=19366.685816, test/accuracy=0.442700, test/loss=2.543313, test/num_examples=10000, total_duration=21450.455102, train/accuracy=0.603164, train/loss=1.654469, validation/accuracy=0.561300, validation/loss=1.877450, validation/num_examples=50000
I0203 23:49:55.061077 139923852027648 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.1557955741882324, loss=2.9292798042297363
I0203 23:50:39.181439 139923868813056 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.9041277766227722, loss=4.254760265350342
I0203 23:51:26.243691 139923852027648 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.3429080247879028, loss=2.783906936645508
I0203 23:52:13.272986 139923868813056 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.0365033149719238, loss=3.130343198776245
I0203 23:53:00.327241 139923852027648 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.2455123662948608, loss=2.698615074157715
I0203 23:53:47.277035 139923868813056 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1844631433486938, loss=2.9964723587036133
I0203 23:54:33.990371 139923852027648 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1583117246627808, loss=2.812429189682007
I0203 23:55:20.915489 139923868813056 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.9156638979911804, loss=4.8078484535217285
I0203 23:56:07.992473 139923852027648 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.0858579874038696, loss=3.615079879760742
I0203 23:56:45.534751 140085747812160 spec.py:321] Evaluating on the training split.
I0203 23:56:55.892318 140085747812160 spec.py:333] Evaluating on the validation split.
I0203 23:57:26.662364 140085747812160 spec.py:349] Evaluating on the test split.
I0203 23:57:28.303386 140085747812160 submission_runner.py:408] Time since start: 21913.58s, 	Step: 42482, 	{'train/accuracy': 0.6094140410423279, 'train/loss': 1.6763795614242554, 'validation/accuracy': 0.5593799948692322, 'validation/loss': 1.911125898361206, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.589221715927124, 'test/num_examples': 10000, 'score': 19786.95946097374, 'total_duration': 21913.58345246315, 'accumulated_submission_time': 19786.95946097374, 'accumulated_eval_time': 2122.7487738132477, 'accumulated_logging_time': 1.4790616035461426}
I0203 23:57:28.327845 139923868813056 logging_writer.py:48] [42482] accumulated_eval_time=2122.748774, accumulated_logging_time=1.479062, accumulated_submission_time=19786.959461, global_step=42482, preemption_count=0, score=19786.959461, test/accuracy=0.441000, test/loss=2.589222, test/num_examples=10000, total_duration=21913.583452, train/accuracy=0.609414, train/loss=1.676380, validation/accuracy=0.559380, validation/loss=1.911126, validation/num_examples=50000
I0203 23:57:36.145191 139923852027648 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.059751272201538, loss=2.720621109008789
I0203 23:58:19.748690 139923868813056 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.2534431219100952, loss=2.7095329761505127
I0203 23:59:06.242922 139923852027648 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.9568240642547607, loss=4.287904262542725
I0203 23:59:53.404719 139923868813056 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.9838769435882568, loss=4.516574382781982
I0204 00:00:40.173584 139923852027648 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2063900232315063, loss=2.883328676223755
I0204 00:01:26.968739 139923868813056 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.1120764017105103, loss=2.659943103790283
I0204 00:02:14.177933 139923852027648 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.1933343410491943, loss=2.685265064239502
I0204 00:03:01.035013 139923868813056 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1380267143249512, loss=3.303154945373535
I0204 00:03:48.024247 139923852027648 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.188968300819397, loss=2.7982757091522217
I0204 00:04:28.512293 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:04:39.420373 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:05:12.490060 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:05:14.142057 140085747812160 submission_runner.py:408] Time since start: 22379.42s, 	Step: 43388, 	{'train/accuracy': 0.6217382550239563, 'train/loss': 1.639617919921875, 'validation/accuracy': 0.5581200122833252, 'validation/loss': 1.9320470094680786, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.585341215133667, 'test/num_examples': 10000, 'score': 20207.082400798798, 'total_duration': 22379.42211842537, 'accumulated_submission_time': 20207.082400798798, 'accumulated_eval_time': 2168.378532409668, 'accumulated_logging_time': 1.5133049488067627}
I0204 00:05:14.168010 139923868813056 logging_writer.py:48] [43388] accumulated_eval_time=2168.378532, accumulated_logging_time=1.513305, accumulated_submission_time=20207.082401, global_step=43388, preemption_count=0, score=20207.082401, test/accuracy=0.439700, test/loss=2.585341, test/num_examples=10000, total_duration=22379.422118, train/accuracy=0.621738, train/loss=1.639618, validation/accuracy=0.558120, validation/loss=1.932047, validation/num_examples=50000
I0204 00:05:19.508103 139923852027648 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1169706583023071, loss=2.730867385864258
I0204 00:06:03.024149 139923868813056 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.2399535179138184, loss=2.8102779388427734
I0204 00:06:49.735183 139923852027648 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2732174396514893, loss=2.8177709579467773
I0204 00:07:36.686151 139923868813056 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.843082070350647, loss=4.696987628936768
I0204 00:08:23.860026 139923852027648 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.2489006519317627, loss=2.756196975708008
I0204 00:09:10.652347 139923868813056 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.2098674774169922, loss=2.8124523162841797
I0204 00:09:57.403676 139923852027648 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.1879909038543701, loss=2.7358102798461914
I0204 00:10:44.237568 139923868813056 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.9857302308082581, loss=4.590329647064209
I0204 00:11:30.966624 139923852027648 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.217551350593567, loss=2.6802196502685547
I0204 00:12:14.415592 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:12:24.997999 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:13:00.984408 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:13:02.638476 140085747812160 submission_runner.py:408] Time since start: 22847.92s, 	Step: 44294, 	{'train/accuracy': 0.613964855670929, 'train/loss': 1.6308157444000244, 'validation/accuracy': 0.5718799829483032, 'validation/loss': 1.8435885906219482, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.4995086193084717, 'test/num_examples': 10000, 'score': 20627.26855278015, 'total_duration': 22847.91852927208, 'accumulated_submission_time': 20627.26855278015, 'accumulated_eval_time': 2216.601419687271, 'accumulated_logging_time': 1.5490844249725342}
I0204 00:13:02.661930 139923868813056 logging_writer.py:48] [44294] accumulated_eval_time=2216.601420, accumulated_logging_time=1.549084, accumulated_submission_time=20627.268553, global_step=44294, preemption_count=0, score=20627.268553, test/accuracy=0.451900, test/loss=2.499509, test/num_examples=10000, total_duration=22847.918529, train/accuracy=0.613965, train/loss=1.630816, validation/accuracy=0.571880, validation/loss=1.843589, validation/num_examples=50000
I0204 00:13:05.533326 139923852027648 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.2412210702896118, loss=2.7569286823272705
I0204 00:13:48.292723 139923868813056 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.9791772961616516, loss=4.2241973876953125
I0204 00:14:35.090208 139923852027648 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.2912490367889404, loss=2.6226890087127686
I0204 00:15:21.987787 139923868813056 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2126305103302002, loss=2.8122270107269287
I0204 00:16:08.869030 139923852027648 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.079296350479126, loss=3.4566636085510254
I0204 00:16:55.899266 139923868813056 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1211636066436768, loss=4.494994640350342
I0204 00:17:43.250459 139923852027648 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1733604669570923, loss=2.6608059406280518
I0204 00:18:30.063701 139923868813056 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.8862790465354919, loss=4.780420303344727
I0204 00:19:17.629612 139923852027648 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.051385760307312, loss=4.569965362548828
I0204 00:20:02.837429 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:20:13.371637 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:20:44.761234 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:20:46.399215 140085747812160 submission_runner.py:408] Time since start: 23311.68s, 	Step: 45197, 	{'train/accuracy': 0.6221679449081421, 'train/loss': 1.5743632316589355, 'validation/accuracy': 0.5730199813842773, 'validation/loss': 1.8135331869125366, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.4988009929656982, 'test/num_examples': 10000, 'score': 21047.384136915207, 'total_duration': 23311.679278612137, 'accumulated_submission_time': 21047.384136915207, 'accumulated_eval_time': 2260.163228034973, 'accumulated_logging_time': 1.5819377899169922}
I0204 00:20:46.424201 139923868813056 logging_writer.py:48] [45197] accumulated_eval_time=2260.163228, accumulated_logging_time=1.581938, accumulated_submission_time=21047.384137, global_step=45197, preemption_count=0, score=21047.384137, test/accuracy=0.456600, test/loss=2.498801, test/num_examples=10000, total_duration=23311.679279, train/accuracy=0.622168, train/loss=1.574363, validation/accuracy=0.573020, validation/loss=1.813533, validation/num_examples=50000
I0204 00:20:48.072151 139923852027648 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.0484106540679932, loss=5.391846656799316
I0204 00:21:30.804997 139923868813056 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.109544038772583, loss=2.8590950965881348
I0204 00:22:17.728229 139923852027648 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1362143754959106, loss=2.816653251647949
I0204 00:23:05.019210 139923868813056 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3282594680786133, loss=2.6758270263671875
I0204 00:23:52.203476 139923852027648 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.3938663005828857, loss=2.6683030128479004
I0204 00:24:39.263761 139923868813056 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.9683717489242554, loss=5.168943405151367
I0204 00:25:26.021864 139923852027648 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.022861361503601, loss=4.681596279144287
I0204 00:26:12.958001 139923868813056 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1716351509094238, loss=2.781205654144287
I0204 00:27:00.017771 139923852027648 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.0836825370788574, loss=4.871920108795166
I0204 00:27:47.119528 139923868813056 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2206790447235107, loss=2.6916303634643555
I0204 00:27:47.136152 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:27:57.597570 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:28:33.016462 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:28:34.657245 140085747812160 submission_runner.py:408] Time since start: 23779.94s, 	Step: 46101, 	{'train/accuracy': 0.6398242115974426, 'train/loss': 1.471780776977539, 'validation/accuracy': 0.5819199681282043, 'validation/loss': 1.7632266283035278, 'validation/num_examples': 50000, 'test/accuracy': 0.46000000834465027, 'test/loss': 2.4405603408813477, 'test/num_examples': 10000, 'score': 21468.033683538437, 'total_duration': 23779.937309980392, 'accumulated_submission_time': 21468.033683538437, 'accumulated_eval_time': 2307.6843173503876, 'accumulated_logging_time': 1.6177711486816406}
I0204 00:28:34.679827 139923852027648 logging_writer.py:48] [46101] accumulated_eval_time=2307.684317, accumulated_logging_time=1.617771, accumulated_submission_time=21468.033684, global_step=46101, preemption_count=0, score=21468.033684, test/accuracy=0.460000, test/loss=2.440560, test/num_examples=10000, total_duration=23779.937310, train/accuracy=0.639824, train/loss=1.471781, validation/accuracy=0.581920, validation/loss=1.763227, validation/num_examples=50000
I0204 00:29:17.434542 139923868813056 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.1277365684509277, loss=2.733853340148926
I0204 00:30:04.741865 139923852027648 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.0510179996490479, loss=3.566448450088501
I0204 00:30:51.940905 139923868813056 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.186795711517334, loss=2.661715269088745
I0204 00:31:38.949801 139923852027648 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0943734645843506, loss=2.8109099864959717
I0204 00:32:26.105071 139923868813056 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.1922059059143066, loss=2.5163652896881104
I0204 00:33:13.213805 139923852027648 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.2002525329589844, loss=2.7953715324401855
I0204 00:34:00.269044 139923868813056 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.8898353576660156, loss=5.1104583740234375
I0204 00:34:47.133042 139923852027648 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.0642106533050537, loss=4.335900783538818
I0204 00:35:34.105012 139923868813056 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.0363035202026367, loss=4.246463298797607
I0204 00:35:34.757843 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:35:45.327200 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:36:18.703331 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:36:20.340186 140085747812160 submission_runner.py:408] Time since start: 24245.62s, 	Step: 47003, 	{'train/accuracy': 0.6192382574081421, 'train/loss': 1.632784128189087, 'validation/accuracy': 0.5776799917221069, 'validation/loss': 1.838263750076294, 'validation/num_examples': 50000, 'test/accuracy': 0.45340001583099365, 'test/loss': 2.5132648944854736, 'test/num_examples': 10000, 'score': 21888.051579475403, 'total_duration': 24245.620250225067, 'accumulated_submission_time': 21888.051579475403, 'accumulated_eval_time': 2353.266669511795, 'accumulated_logging_time': 1.6493885517120361}
I0204 00:36:20.363071 139923852027648 logging_writer.py:48] [47003] accumulated_eval_time=2353.266670, accumulated_logging_time=1.649389, accumulated_submission_time=21888.051579, global_step=47003, preemption_count=0, score=21888.051579, test/accuracy=0.453400, test/loss=2.513265, test/num_examples=10000, total_duration=24245.620250, train/accuracy=0.619238, train/loss=1.632784, validation/accuracy=0.577680, validation/loss=1.838264, validation/num_examples=50000
I0204 00:37:02.397907 139923868813056 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.099549651145935, loss=2.6275880336761475
I0204 00:37:49.272521 139923852027648 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.5332804918289185, loss=2.6570627689361572
I0204 00:38:36.302229 139923868813056 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.9509449005126953, loss=4.672428607940674
I0204 00:39:23.496991 139923852027648 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.9072458148002625, loss=5.287782669067383
I0204 00:40:10.503562 139923868813056 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1219433546066284, loss=2.9193742275238037
I0204 00:40:57.476014 139923852027648 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.326072096824646, loss=2.7420740127563477
I0204 00:41:44.531029 139923868813056 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.020126223564148, loss=3.9620585441589355
I0204 00:42:31.613172 139923852027648 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.157254934310913, loss=3.670966148376465
I0204 00:43:18.767201 139923868813056 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.2597692012786865, loss=2.6539790630340576
I0204 00:43:20.388280 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:43:30.747061 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:44:07.343129 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:44:08.979756 140085747812160 submission_runner.py:408] Time since start: 24714.26s, 	Step: 47905, 	{'train/accuracy': 0.6240624785423279, 'train/loss': 1.5510444641113281, 'validation/accuracy': 0.5730800032615662, 'validation/loss': 1.7903846502304077, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.462394952774048, 'test/num_examples': 10000, 'score': 22308.016907691956, 'total_duration': 24714.259820222855, 'accumulated_submission_time': 22308.016907691956, 'accumulated_eval_time': 2401.858140230179, 'accumulated_logging_time': 1.6812567710876465}
I0204 00:44:09.005496 139923852027648 logging_writer.py:48] [47905] accumulated_eval_time=2401.858140, accumulated_logging_time=1.681257, accumulated_submission_time=22308.016908, global_step=47905, preemption_count=0, score=22308.016908, test/accuracy=0.457100, test/loss=2.462395, test/num_examples=10000, total_duration=24714.259820, train/accuracy=0.624062, train/loss=1.551044, validation/accuracy=0.573080, validation/loss=1.790385, validation/num_examples=50000
I0204 00:44:49.781114 139923868813056 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0258675813674927, loss=3.707043170928955
I0204 00:45:36.429055 139923852027648 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.2525321245193481, loss=2.7571582794189453
I0204 00:46:23.475645 139923868813056 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.1186050176620483, loss=2.9398036003112793
I0204 00:47:10.475456 139923852027648 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.163865327835083, loss=2.8351128101348877
I0204 00:47:57.209557 139923868813056 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.162304401397705, loss=2.644191026687622
I0204 00:48:44.439322 139923852027648 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2396475076675415, loss=2.570082902908325
I0204 00:49:31.250459 139923868813056 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.086046814918518, loss=2.6999359130859375
I0204 00:50:18.474682 139923852027648 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1989774703979492, loss=2.6312406063079834
I0204 00:51:05.630519 139923868813056 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.078037977218628, loss=2.5998079776763916
I0204 00:51:09.097715 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:51:19.571593 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:51:46.875759 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:51:48.522308 140085747812160 submission_runner.py:408] Time since start: 25173.80s, 	Step: 48809, 	{'train/accuracy': 0.6260156035423279, 'train/loss': 1.572181224822998, 'validation/accuracy': 0.5747199654579163, 'validation/loss': 1.8311009407043457, 'validation/num_examples': 50000, 'test/accuracy': 0.45680001378059387, 'test/loss': 2.4816720485687256, 'test/num_examples': 10000, 'score': 22728.048118829727, 'total_duration': 25173.80234694481, 'accumulated_submission_time': 22728.048118829727, 'accumulated_eval_time': 2441.2826936244965, 'accumulated_logging_time': 1.717482089996338}
I0204 00:51:48.547975 139923852027648 logging_writer.py:48] [48809] accumulated_eval_time=2441.282694, accumulated_logging_time=1.717482, accumulated_submission_time=22728.048119, global_step=48809, preemption_count=0, score=22728.048119, test/accuracy=0.456800, test/loss=2.481672, test/num_examples=10000, total_duration=25173.802347, train/accuracy=0.626016, train/loss=1.572181, validation/accuracy=0.574720, validation/loss=1.831101, validation/num_examples=50000
I0204 00:52:28.224884 139923868813056 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.0456821918487549, loss=3.5929689407348633
I0204 00:53:14.951323 139923852027648 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0634188652038574, loss=4.199345111846924
I0204 00:54:02.113726 139923868813056 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.1680238246917725, loss=2.643868923187256
I0204 00:54:49.176604 139923852027648 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.2890092134475708, loss=2.68641996383667
I0204 00:55:36.416540 139923868813056 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.9709727168083191, loss=4.200540542602539
I0204 00:56:23.428867 139923852027648 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.0976135730743408, loss=2.756344795227051
I0204 00:57:10.409625 139923868813056 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.2852177619934082, loss=2.5049684047698975
I0204 00:57:57.359011 139923852027648 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9101664423942566, loss=5.173407077789307
I0204 00:58:44.255095 139923868813056 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.1916937828063965, loss=2.6295604705810547
I0204 00:58:48.640608 140085747812160 spec.py:321] Evaluating on the training split.
I0204 00:58:59.200611 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 00:59:33.184787 140085747812160 spec.py:349] Evaluating on the test split.
I0204 00:59:34.833704 140085747812160 submission_runner.py:408] Time since start: 25640.11s, 	Step: 49711, 	{'train/accuracy': 0.6138671636581421, 'train/loss': 1.6917517185211182, 'validation/accuracy': 0.5685399770736694, 'validation/loss': 1.90058434009552, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.5759851932525635, 'test/num_examples': 10000, 'score': 23147.635646104813, 'total_duration': 25640.113765478134, 'accumulated_submission_time': 23147.635646104813, 'accumulated_eval_time': 2487.4757957458496, 'accumulated_logging_time': 2.1974527835845947}
I0204 00:59:34.861012 139923852027648 logging_writer.py:48] [49711] accumulated_eval_time=2487.475796, accumulated_logging_time=2.197453, accumulated_submission_time=23147.635646, global_step=49711, preemption_count=0, score=23147.635646, test/accuracy=0.446300, test/loss=2.575985, test/num_examples=10000, total_duration=25640.113765, train/accuracy=0.613867, train/loss=1.691752, validation/accuracy=0.568540, validation/loss=1.900584, validation/num_examples=50000
I0204 01:00:12.882914 139923868813056 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.2849825620651245, loss=2.509702444076538
I0204 01:00:59.630561 139923852027648 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.2960457801818848, loss=2.897862434387207
I0204 01:01:46.758605 139923868813056 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.2551600933074951, loss=2.7280845642089844
I0204 01:02:33.992968 139923852027648 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.913276195526123, loss=5.150545597076416
I0204 01:03:21.117595 139923868813056 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.1965481042861938, loss=2.6021671295166016
I0204 01:04:08.306418 139923852027648 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2300500869750977, loss=3.0529088973999023
I0204 01:04:55.143783 139923868813056 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2433003187179565, loss=5.318689346313477
I0204 01:05:42.180901 139923852027648 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.2420237064361572, loss=2.810631275177002
I0204 01:06:29.194936 139923868813056 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1574373245239258, loss=2.7206804752349854
I0204 01:06:35.018326 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:06:45.701183 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:07:17.379353 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:07:19.012946 140085747812160 submission_runner.py:408] Time since start: 26104.29s, 	Step: 50614, 	{'train/accuracy': 0.627246081829071, 'train/loss': 1.6077184677124023, 'validation/accuracy': 0.5752800107002258, 'validation/loss': 1.8396003246307373, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.4899773597717285, 'test/num_examples': 10000, 'score': 23567.732134580612, 'total_duration': 26104.293008327484, 'accumulated_submission_time': 23567.732134580612, 'accumulated_eval_time': 2531.470423936844, 'accumulated_logging_time': 2.233795166015625}
I0204 01:07:19.038822 139923852027648 logging_writer.py:48] [50614] accumulated_eval_time=2531.470424, accumulated_logging_time=2.233795, accumulated_submission_time=23567.732135, global_step=50614, preemption_count=0, score=23567.732135, test/accuracy=0.458800, test/loss=2.489977, test/num_examples=10000, total_duration=26104.293008, train/accuracy=0.627246, train/loss=1.607718, validation/accuracy=0.575280, validation/loss=1.839600, validation/num_examples=50000
I0204 01:07:55.400754 139923868813056 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.0636271238327026, loss=3.564755916595459
I0204 01:08:42.167819 139923852027648 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.202742338180542, loss=2.597332000732422
I0204 01:09:29.161223 139923868813056 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2899693250656128, loss=2.581543445587158
I0204 01:10:15.937406 139923852027648 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.3417818546295166, loss=2.7183165550231934
I0204 01:11:02.750030 139923868813056 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.1738280057907104, loss=2.5394394397735596
I0204 01:11:49.764333 139923852027648 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.0151077508926392, loss=4.655072212219238
I0204 01:12:37.230321 139923868813056 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.1580334901809692, loss=2.636248826980591
I0204 01:13:24.396495 139923852027648 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.4848358631134033, loss=2.7705202102661133
I0204 01:14:11.495449 139923868813056 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2506910562515259, loss=2.5304932594299316
I0204 01:14:19.245078 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:14:29.868497 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:15:03.475115 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:15:05.118676 140085747812160 submission_runner.py:408] Time since start: 26570.40s, 	Step: 51518, 	{'train/accuracy': 0.6355859041213989, 'train/loss': 1.5609021186828613, 'validation/accuracy': 0.5841400027275085, 'validation/loss': 1.812380313873291, 'validation/num_examples': 50000, 'test/accuracy': 0.4621000289916992, 'test/loss': 2.4717304706573486, 'test/num_examples': 10000, 'score': 23987.8773355484, 'total_duration': 26570.3987429142, 'accumulated_submission_time': 23987.8773355484, 'accumulated_eval_time': 2577.3440272808075, 'accumulated_logging_time': 2.2699053287506104}
I0204 01:15:05.142867 139923852027648 logging_writer.py:48] [51518] accumulated_eval_time=2577.344027, accumulated_logging_time=2.269905, accumulated_submission_time=23987.877336, global_step=51518, preemption_count=0, score=23987.877336, test/accuracy=0.462100, test/loss=2.471730, test/num_examples=10000, total_duration=26570.398743, train/accuracy=0.635586, train/loss=1.560902, validation/accuracy=0.584140, validation/loss=1.812380, validation/num_examples=50000
I0204 01:15:39.931859 139923868813056 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.9118602871894836, loss=5.24636173248291
I0204 01:16:26.601937 139923852027648 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.0778497457504272, loss=3.502687454223633
I0204 01:17:13.705707 139923868813056 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0695905685424805, loss=4.9271392822265625
I0204 01:18:00.733415 139923852027648 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.247044563293457, loss=2.505589723587036
I0204 01:18:47.706414 139923868813056 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.0436797142028809, loss=3.2306113243103027
I0204 01:19:34.636978 139923852027648 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.9432787299156189, loss=4.671414375305176
I0204 01:20:21.387244 139923868813056 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.4318222999572754, loss=2.5778560638427734
I0204 01:21:08.487855 139923852027648 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1405328512191772, loss=2.494755983352661
I0204 01:21:55.561445 139923868813056 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.0109792947769165, loss=3.8242692947387695
I0204 01:22:05.540575 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:22:16.029722 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:22:50.651120 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:22:52.294863 140085747812160 submission_runner.py:408] Time since start: 27037.57s, 	Step: 52423, 	{'train/accuracy': 0.6373242139816284, 'train/loss': 1.5606440305709839, 'validation/accuracy': 0.5805400013923645, 'validation/loss': 1.8203362226486206, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4787209033966064, 'test/num_examples': 10000, 'score': 24408.212697029114, 'total_duration': 27037.57492542267, 'accumulated_submission_time': 24408.212697029114, 'accumulated_eval_time': 2624.098313808441, 'accumulated_logging_time': 2.305191993713379}
I0204 01:22:52.321310 139923852027648 logging_writer.py:48] [52423] accumulated_eval_time=2624.098314, accumulated_logging_time=2.305192, accumulated_submission_time=24408.212697, global_step=52423, preemption_count=0, score=24408.212697, test/accuracy=0.463800, test/loss=2.478721, test/num_examples=10000, total_duration=27037.574925, train/accuracy=0.637324, train/loss=1.560644, validation/accuracy=0.580540, validation/loss=1.820336, validation/num_examples=50000
I0204 01:23:24.723565 139923868813056 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2000199556350708, loss=3.195751428604126
I0204 01:24:11.763244 139923852027648 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.1615419387817383, loss=4.8510332107543945
I0204 01:24:58.749903 139923868813056 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.9061331748962402, loss=5.116596221923828
I0204 01:25:45.545182 139923852027648 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.121158242225647, loss=3.008199453353882
I0204 01:26:32.288775 139923868813056 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.089858055114746, loss=3.7425405979156494
I0204 01:27:19.032580 139923852027648 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.1960179805755615, loss=2.7920401096343994
I0204 01:28:05.886954 139923868813056 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2210289239883423, loss=2.890159845352173
I0204 01:28:52.767668 139923852027648 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.1342822313308716, loss=2.8559110164642334
I0204 01:29:39.879073 139923868813056 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.0956164598464966, loss=3.205094814300537
I0204 01:29:52.534815 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:30:03.193709 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:30:34.668677 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:30:36.311721 140085747812160 submission_runner.py:408] Time since start: 27501.59s, 	Step: 53329, 	{'train/accuracy': 0.6320117115974426, 'train/loss': 1.5440112352371216, 'validation/accuracy': 0.586359977722168, 'validation/loss': 1.7737897634506226, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.432278871536255, 'test/num_examples': 10000, 'score': 24828.36497950554, 'total_duration': 27501.591789245605, 'accumulated_submission_time': 24828.36497950554, 'accumulated_eval_time': 2667.875230550766, 'accumulated_logging_time': 2.341886043548584}
I0204 01:30:36.336578 139923852027648 logging_writer.py:48] [53329] accumulated_eval_time=2667.875231, accumulated_logging_time=2.341886, accumulated_submission_time=24828.364980, global_step=53329, preemption_count=0, score=24828.364980, test/accuracy=0.470800, test/loss=2.432279, test/num_examples=10000, total_duration=27501.591789, train/accuracy=0.632012, train/loss=1.544011, validation/accuracy=0.586360, validation/loss=1.773790, validation/num_examples=50000
I0204 01:31:06.003569 139923868813056 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1949788331985474, loss=3.0410592555999756
I0204 01:31:52.619708 139923852027648 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.045505404472351, loss=5.286566257476807
I0204 01:32:40.006574 139923868813056 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.0587557554244995, loss=4.092230796813965
I0204 01:33:26.979848 139923852027648 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.163719892501831, loss=2.6368863582611084
I0204 01:34:14.090248 139923868813056 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.0779505968093872, loss=4.98563814163208
I0204 01:35:00.952373 139923852027648 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.1279112100601196, loss=4.615458965301514
I0204 01:35:48.216345 139923868813056 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.0548408031463623, loss=3.8317699432373047
I0204 01:36:34.958149 139923852027648 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2335573434829712, loss=2.559579610824585
I0204 01:37:22.020873 139923868813056 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.1714798212051392, loss=2.642939329147339
I0204 01:37:36.780235 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:37:47.502750 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:38:17.671814 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:38:19.304179 140085747812160 submission_runner.py:408] Time since start: 27964.58s, 	Step: 54233, 	{'train/accuracy': 0.6431054472923279, 'train/loss': 1.4797742366790771, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.7450494766235352, 'validation/num_examples': 50000, 'test/accuracy': 0.4805000126361847, 'test/loss': 2.3959012031555176, 'test/num_examples': 10000, 'score': 25248.749005794525, 'total_duration': 27964.58424758911, 'accumulated_submission_time': 25248.749005794525, 'accumulated_eval_time': 2710.3991668224335, 'accumulated_logging_time': 2.375976324081421}
I0204 01:38:19.332350 139923852027648 logging_writer.py:48] [54233] accumulated_eval_time=2710.399167, accumulated_logging_time=2.375976, accumulated_submission_time=25248.749006, global_step=54233, preemption_count=0, score=25248.749006, test/accuracy=0.480500, test/loss=2.395901, test/num_examples=10000, total_duration=27964.584248, train/accuracy=0.643105, train/loss=1.479774, validation/accuracy=0.591860, validation/loss=1.745049, validation/num_examples=50000
I0204 01:38:47.255874 139923868813056 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.0695297718048096, loss=3.094536781311035
I0204 01:39:33.883753 139923852027648 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.3822994232177734, loss=2.758330821990967
I0204 01:40:20.855766 139923868813056 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2863194942474365, loss=2.4857702255249023
I0204 01:41:07.467466 139923852027648 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.2946239709854126, loss=2.68638277053833
I0204 01:41:54.539343 139923868813056 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.180337905883789, loss=2.9660181999206543
I0204 01:42:41.914723 139923852027648 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1647831201553345, loss=3.4810476303100586
I0204 01:43:28.875837 139923868813056 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.1655322313308716, loss=2.630157947540283
I0204 01:44:15.766521 139923852027648 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.1062055826187134, loss=2.4767565727233887
I0204 01:45:02.946671 139923868813056 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.0418847799301147, loss=3.680086612701416
I0204 01:45:19.554597 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:45:29.854885 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:46:04.250423 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:46:05.888825 140085747812160 submission_runner.py:408] Time since start: 28431.17s, 	Step: 55137, 	{'train/accuracy': 0.6678124666213989, 'train/loss': 1.3960999250411987, 'validation/accuracy': 0.5922799706459045, 'validation/loss': 1.7475827932357788, 'validation/num_examples': 50000, 'test/accuracy': 0.4723000228404999, 'test/loss': 2.4174678325653076, 'test/num_examples': 10000, 'score': 25668.903258800507, 'total_duration': 28431.168878793716, 'accumulated_submission_time': 25668.903258800507, 'accumulated_eval_time': 2756.7333924770355, 'accumulated_logging_time': 2.421182155609131}
I0204 01:46:05.915944 139923852027648 logging_writer.py:48] [55137] accumulated_eval_time=2756.733392, accumulated_logging_time=2.421182, accumulated_submission_time=25668.903259, global_step=55137, preemption_count=0, score=25668.903259, test/accuracy=0.472300, test/loss=2.417468, test/num_examples=10000, total_duration=28431.168879, train/accuracy=0.667812, train/loss=1.396100, validation/accuracy=0.592280, validation/loss=1.747583, validation/num_examples=50000
I0204 01:46:32.164969 139923868813056 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.2485181093215942, loss=2.7007458209991455
I0204 01:47:18.507555 139923852027648 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.1795318126678467, loss=2.6583752632141113
I0204 01:48:05.535521 139923868813056 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2597867250442505, loss=2.5994396209716797
I0204 01:48:52.544526 139923852027648 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.9339213371276855, loss=5.223371982574463
I0204 01:49:39.528035 139923868813056 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.1300841569900513, loss=2.7920095920562744
I0204 01:50:26.646628 139923852027648 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.066461443901062, loss=4.139466762542725
I0204 01:51:13.526319 139923868813056 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0679082870483398, loss=4.199644565582275
I0204 01:52:00.234597 139923852027648 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2508238554000854, loss=2.600546360015869
I0204 01:52:47.435797 139923868813056 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.9987562894821167, loss=5.014520645141602
I0204 01:53:05.967896 140085747812160 spec.py:321] Evaluating on the training split.
I0204 01:53:16.561942 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 01:53:51.399072 140085747812160 spec.py:349] Evaluating on the test split.
I0204 01:53:53.038273 140085747812160 submission_runner.py:408] Time since start: 28898.32s, 	Step: 56041, 	{'train/accuracy': 0.6312109231948853, 'train/loss': 1.5431017875671387, 'validation/accuracy': 0.5813199877738953, 'validation/loss': 1.7773977518081665, 'validation/num_examples': 50000, 'test/accuracy': 0.4618000090122223, 'test/loss': 2.443195343017578, 'test/num_examples': 10000, 'score': 26088.894425868988, 'total_duration': 28898.318333864212, 'accumulated_submission_time': 26088.894425868988, 'accumulated_eval_time': 2803.8037741184235, 'accumulated_logging_time': 2.4583358764648438}
I0204 01:53:53.067517 139923852027648 logging_writer.py:48] [56041] accumulated_eval_time=2803.803774, accumulated_logging_time=2.458336, accumulated_submission_time=26088.894426, global_step=56041, preemption_count=0, score=26088.894426, test/accuracy=0.461800, test/loss=2.443195, test/num_examples=10000, total_duration=28898.318334, train/accuracy=0.631211, train/loss=1.543102, validation/accuracy=0.581320, validation/loss=1.777398, validation/num_examples=50000
I0204 01:54:17.685034 139923868813056 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.4833972454071045, loss=2.93800950050354
I0204 01:55:03.825294 139923852027648 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.1762064695358276, loss=2.705421209335327
I0204 01:55:51.110134 139923868813056 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.9449014663696289, loss=3.951747417449951
I0204 01:56:38.076531 139923852027648 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.1994627714157104, loss=2.6620864868164062
I0204 01:57:24.894523 139923868813056 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1420085430145264, loss=3.3060193061828613
I0204 01:58:11.899567 139923852027648 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.1106722354888916, loss=2.5146148204803467
I0204 01:58:58.840785 139923868813056 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.1847798824310303, loss=2.581321954727173
I0204 01:59:45.823413 139923852027648 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.3184311389923096, loss=2.6904921531677246
I0204 02:00:32.568343 139923868813056 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.3501880168914795, loss=2.5531280040740967
I0204 02:00:53.431604 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:01:03.898116 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:01:35.059984 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:01:36.704313 140085747812160 submission_runner.py:408] Time since start: 29361.98s, 	Step: 56946, 	{'train/accuracy': 0.6483007669448853, 'train/loss': 1.5265979766845703, 'validation/accuracy': 0.598039984703064, 'validation/loss': 1.7617758512496948, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.4116508960723877, 'test/num_examples': 10000, 'score': 26509.195707321167, 'total_duration': 29361.984380960464, 'accumulated_submission_time': 26509.195707321167, 'accumulated_eval_time': 2847.0764875411987, 'accumulated_logging_time': 2.49930739402771}
I0204 02:01:36.731533 139923852027648 logging_writer.py:48] [56946] accumulated_eval_time=2847.076488, accumulated_logging_time=2.499307, accumulated_submission_time=26509.195707, global_step=56946, preemption_count=0, score=26509.195707, test/accuracy=0.478300, test/loss=2.411651, test/num_examples=10000, total_duration=29361.984381, train/accuracy=0.648301, train/loss=1.526598, validation/accuracy=0.598040, validation/loss=1.761776, validation/num_examples=50000
I0204 02:01:59.330466 139923868813056 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.4203733205795288, loss=2.583954095840454
I0204 02:02:45.181566 139923852027648 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.221075415611267, loss=2.5767736434936523
I0204 02:03:32.263551 139923868813056 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.0298742055892944, loss=3.0681991577148438
I0204 02:04:19.041689 139923852027648 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.1213335990905762, loss=2.79638409614563
I0204 02:05:06.026423 139923868813056 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.119860053062439, loss=2.987828016281128
I0204 02:05:52.807994 139923852027648 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.3180418014526367, loss=2.490861415863037
I0204 02:06:40.169619 139923868813056 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.9994921684265137, loss=4.160463333129883
I0204 02:07:27.401561 139923852027648 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.1734732389450073, loss=2.9329094886779785
I0204 02:08:14.411284 139923868813056 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.2034928798675537, loss=2.7918097972869873
I0204 02:08:37.130510 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:08:47.577311 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:09:12.205043 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:09:13.847540 140085747812160 submission_runner.py:408] Time since start: 29819.13s, 	Step: 57850, 	{'train/accuracy': 0.6579492092132568, 'train/loss': 1.4141149520874023, 'validation/accuracy': 0.590499997138977, 'validation/loss': 1.743949294090271, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.402419328689575, 'test/num_examples': 10000, 'score': 26929.53288602829, 'total_duration': 29819.127604722977, 'accumulated_submission_time': 26929.53288602829, 'accumulated_eval_time': 2883.793509721756, 'accumulated_logging_time': 2.5372140407562256}
I0204 02:09:13.877418 139923852027648 logging_writer.py:48] [57850] accumulated_eval_time=2883.793510, accumulated_logging_time=2.537214, accumulated_submission_time=26929.532886, global_step=57850, preemption_count=0, score=26929.532886, test/accuracy=0.470800, test/loss=2.402419, test/num_examples=10000, total_duration=29819.127605, train/accuracy=0.657949, train/loss=1.414115, validation/accuracy=0.590500, validation/loss=1.743949, validation/num_examples=50000
I0204 02:09:34.834724 139923868813056 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2589105367660522, loss=2.491863489151001
I0204 02:10:20.285367 139923852027648 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.9995558261871338, loss=5.129337787628174
I0204 02:11:07.420414 139923868813056 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.983525276184082, loss=5.197719573974609
I0204 02:11:54.422970 139923852027648 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.1628446578979492, loss=2.4813361167907715
I0204 02:12:41.507052 139923868813056 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.288965106010437, loss=2.6402339935302734
I0204 02:13:28.272467 139923852027648 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.0242973566055298, loss=4.560875415802002
I0204 02:14:15.027399 139923868813056 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2456427812576294, loss=2.728839635848999
I0204 02:15:01.850083 139923852027648 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.9487177133560181, loss=4.799366474151611
I0204 02:15:48.823782 139923868813056 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2859556674957275, loss=2.5964157581329346
I0204 02:16:13.886960 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:16:24.288898 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:17:00.885888 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:17:02.537173 140085747812160 submission_runner.py:408] Time since start: 30287.82s, 	Step: 58755, 	{'train/accuracy': 0.6382421851158142, 'train/loss': 1.5162142515182495, 'validation/accuracy': 0.5955199599266052, 'validation/loss': 1.725347638130188, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.3848416805267334, 'test/num_examples': 10000, 'score': 27349.482364177704, 'total_duration': 30287.817235708237, 'accumulated_submission_time': 27349.482364177704, 'accumulated_eval_time': 2932.443725347519, 'accumulated_logging_time': 2.5766994953155518}
I0204 02:17:02.564605 139923852027648 logging_writer.py:48] [58755] accumulated_eval_time=2932.443725, accumulated_logging_time=2.576699, accumulated_submission_time=27349.482364, global_step=58755, preemption_count=0, score=27349.482364, test/accuracy=0.478600, test/loss=2.384842, test/num_examples=10000, total_duration=30287.817236, train/accuracy=0.638242, train/loss=1.516214, validation/accuracy=0.595520, validation/loss=1.725348, validation/num_examples=50000
I0204 02:17:21.449255 139923868813056 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.9187762141227722, loss=4.71691370010376
I0204 02:18:06.424050 139923852027648 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.9276758432388306, loss=4.9742817878723145
I0204 02:18:53.361454 139923868813056 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2614673376083374, loss=2.664247989654541
I0204 02:19:40.382521 139923852027648 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.2660400867462158, loss=2.484436511993408
I0204 02:20:27.329978 139923868813056 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.3615676164627075, loss=2.7130813598632812
I0204 02:21:14.097006 139923852027648 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.9590120315551758, loss=4.3677449226379395
I0204 02:22:00.959494 139923868813056 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.3727033138275146, loss=2.620192050933838
I0204 02:22:47.868188 139923852027648 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.0210210084915161, loss=3.30546498298645
I0204 02:23:34.610403 139923868813056 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.4317841529846191, loss=2.6949732303619385
I0204 02:24:02.830936 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:24:13.418848 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:24:49.620559 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:24:51.263828 140085747812160 submission_runner.py:408] Time since start: 30756.54s, 	Step: 59662, 	{'train/accuracy': 0.6444531083106995, 'train/loss': 1.4967687129974365, 'validation/accuracy': 0.5925599932670593, 'validation/loss': 1.7437061071395874, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.3955695629119873, 'test/num_examples': 10000, 'score': 27769.687192440033, 'total_duration': 30756.543888568878, 'accumulated_submission_time': 27769.687192440033, 'accumulated_eval_time': 2980.876615524292, 'accumulated_logging_time': 2.613621950149536}
I0204 02:24:51.290724 139923852027648 logging_writer.py:48] [59662] accumulated_eval_time=2980.876616, accumulated_logging_time=2.613622, accumulated_submission_time=27769.687192, global_step=59662, preemption_count=0, score=27769.687192, test/accuracy=0.477500, test/loss=2.395570, test/num_examples=10000, total_duration=30756.543889, train/accuracy=0.644453, train/loss=1.496769, validation/accuracy=0.592560, validation/loss=1.743706, validation/num_examples=50000
I0204 02:25:07.320664 139923868813056 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2336816787719727, loss=2.842782497406006
I0204 02:25:51.851924 139923852027648 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1065641641616821, loss=5.02984094619751
I0204 02:26:38.858414 139923868813056 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.2302151918411255, loss=2.6203932762145996
I0204 02:27:25.583849 139923852027648 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.9842166304588318, loss=4.295238494873047
I0204 02:28:12.888238 139923868813056 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.3222756385803223, loss=2.615384578704834
I0204 02:28:59.841781 139923852027648 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1530481576919556, loss=2.9028239250183105
I0204 02:29:46.832196 139923868813056 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8459643125534058, loss=2.482320785522461
I0204 02:30:33.844788 139923852027648 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1391801834106445, loss=2.7648558616638184
I0204 02:31:20.785657 139923868813056 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.145717978477478, loss=3.396742582321167
I0204 02:31:51.345106 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:32:01.809836 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:32:37.341652 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:32:38.986865 140085747812160 submission_runner.py:408] Time since start: 31224.27s, 	Step: 60567, 	{'train/accuracy': 0.6565039157867432, 'train/loss': 1.4547697305679321, 'validation/accuracy': 0.5924599766731262, 'validation/loss': 1.755020260810852, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.3964974880218506, 'test/num_examples': 10000, 'score': 28189.677449941635, 'total_duration': 31224.26690030098, 'accumulated_submission_time': 28189.677449941635, 'accumulated_eval_time': 3028.5183403491974, 'accumulated_logging_time': 2.6536691188812256}
I0204 02:32:39.016494 139923852027648 logging_writer.py:48] [60567] accumulated_eval_time=3028.518340, accumulated_logging_time=2.653669, accumulated_submission_time=28189.677450, global_step=60567, preemption_count=0, score=28189.677450, test/accuracy=0.475800, test/loss=2.396497, test/num_examples=10000, total_duration=31224.266900, train/accuracy=0.656504, train/loss=1.454770, validation/accuracy=0.592460, validation/loss=1.755020, validation/num_examples=50000
I0204 02:32:52.982875 139923868813056 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.3323181867599487, loss=2.6291115283966064
I0204 02:33:37.625718 139923852027648 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.0851330757141113, loss=3.846344470977783
I0204 02:34:24.585298 139923868813056 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.271683931350708, loss=2.56546950340271
I0204 02:35:11.663108 139923852027648 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.1097224950790405, loss=5.064237594604492
I0204 02:35:58.543105 139923868813056 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.1270077228546143, loss=3.309669017791748
I0204 02:36:45.571197 139923852027648 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2383536100387573, loss=2.43381929397583
I0204 02:37:32.557761 139923868813056 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.271957278251648, loss=2.613600492477417
I0204 02:38:19.831036 139923852027648 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.9928129315376282, loss=4.611010551452637
I0204 02:39:07.078710 139923868813056 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.3769502639770508, loss=2.6417112350463867
I0204 02:39:39.281886 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:39:49.859205 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:40:18.419136 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:40:20.067438 140085747812160 submission_runner.py:408] Time since start: 31685.35s, 	Step: 61470, 	{'train/accuracy': 0.650585949420929, 'train/loss': 1.4309154748916626, 'validation/accuracy': 0.6028599739074707, 'validation/loss': 1.6541436910629272, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.325901508331299, 'test/num_examples': 10000, 'score': 28609.880881786346, 'total_duration': 31685.347499847412, 'accumulated_submission_time': 28609.880881786346, 'accumulated_eval_time': 3069.303905725479, 'accumulated_logging_time': 2.693972587585449}
I0204 02:40:20.096140 139923852027648 logging_writer.py:48] [61470] accumulated_eval_time=3069.303906, accumulated_logging_time=2.693973, accumulated_submission_time=28609.880882, global_step=61470, preemption_count=0, score=28609.880882, test/accuracy=0.480800, test/loss=2.325902, test/num_examples=10000, total_duration=31685.347500, train/accuracy=0.650586, train/loss=1.430915, validation/accuracy=0.602860, validation/loss=1.654144, validation/num_examples=50000
I0204 02:40:32.835851 139923868813056 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.2901463508605957, loss=2.708174228668213
I0204 02:41:17.339940 139923852027648 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.01631760597229, loss=4.325140953063965
I0204 02:42:04.538299 139923868813056 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.1234406232833862, loss=2.63244366645813
I0204 02:42:51.612672 139923852027648 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.2351019382476807, loss=2.623074531555176
I0204 02:43:38.582020 139923868813056 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.1777493953704834, loss=2.9357786178588867
I0204 02:44:25.342046 139923852027648 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.0793834924697876, loss=4.413041114807129
I0204 02:45:12.267030 139923868813056 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3439743518829346, loss=2.5913963317871094
I0204 02:45:58.912399 139923852027648 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.087872862815857, loss=4.616364479064941
I0204 02:46:45.844058 139923868813056 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.1270262002944946, loss=2.9750936031341553
I0204 02:47:20.282874 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:47:30.696626 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:48:06.565974 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:48:08.211124 140085747812160 submission_runner.py:408] Time since start: 32153.49s, 	Step: 62375, 	{'train/accuracy': 0.6543945074081421, 'train/loss': 1.4196845293045044, 'validation/accuracy': 0.60725998878479, 'validation/loss': 1.649720549583435, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3273630142211914, 'test/num_examples': 10000, 'score': 29030.005185365677, 'total_duration': 32153.49118900299, 'accumulated_submission_time': 29030.005185365677, 'accumulated_eval_time': 3117.232168197632, 'accumulated_logging_time': 2.733799457550049}
I0204 02:48:08.242417 139923852027648 logging_writer.py:48] [62375] accumulated_eval_time=3117.232168, accumulated_logging_time=2.733799, accumulated_submission_time=29030.005185, global_step=62375, preemption_count=0, score=29030.005185, test/accuracy=0.488600, test/loss=2.327363, test/num_examples=10000, total_duration=32153.491189, train/accuracy=0.654395, train/loss=1.419685, validation/accuracy=0.607260, validation/loss=1.649721, validation/num_examples=50000
I0204 02:48:18.932908 139923868813056 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.1821794509887695, loss=2.824225902557373
I0204 02:49:02.842740 139923852027648 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0672906637191772, loss=3.470209836959839
I0204 02:49:50.009643 139923868813056 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.00016450881958, loss=4.285569190979004
I0204 02:50:37.090910 139923852027648 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.103326439857483, loss=2.7741963863372803
I0204 02:51:24.227966 139923868813056 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.1649894714355469, loss=5.183321952819824
I0204 02:52:11.063210 139923852027648 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.5902892351150513, loss=2.5233371257781982
I0204 02:52:58.219521 139923868813056 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.3043324947357178, loss=2.563148260116577
I0204 02:53:45.153367 139923852027648 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2289183139801025, loss=2.4175829887390137
I0204 02:54:31.977479 139923868813056 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.9516312479972839, loss=4.523256301879883
I0204 02:55:08.638897 140085747812160 spec.py:321] Evaluating on the training split.
I0204 02:55:18.903774 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 02:55:48.060193 140085747812160 spec.py:349] Evaluating on the test split.
I0204 02:55:49.709747 140085747812160 submission_runner.py:408] Time since start: 32614.99s, 	Step: 63280, 	{'train/accuracy': 0.6655077934265137, 'train/loss': 1.3871628046035767, 'validation/accuracy': 0.6027199625968933, 'validation/loss': 1.683455228805542, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3284871578216553, 'test/num_examples': 10000, 'score': 29450.34034228325, 'total_duration': 32614.989812135696, 'accumulated_submission_time': 29450.34034228325, 'accumulated_eval_time': 3158.303008079529, 'accumulated_logging_time': 2.775385618209839}
I0204 02:55:49.740441 139923852027648 logging_writer.py:48] [63280] accumulated_eval_time=3158.303008, accumulated_logging_time=2.775386, accumulated_submission_time=29450.340342, global_step=63280, preemption_count=0, score=29450.340342, test/accuracy=0.484000, test/loss=2.328487, test/num_examples=10000, total_duration=32614.989812, train/accuracy=0.665508, train/loss=1.387163, validation/accuracy=0.602720, validation/loss=1.683455, validation/num_examples=50000
I0204 02:55:58.377964 139923868813056 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.1725181341171265, loss=5.026672840118408
I0204 02:56:42.244389 139923852027648 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.9740644693374634, loss=5.023361682891846
I0204 02:57:29.135645 139923868813056 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2413880825042725, loss=2.4269583225250244
I0204 02:58:16.278997 139923852027648 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.1755939722061157, loss=4.532395362854004
I0204 02:59:03.151470 139923868813056 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.2520983219146729, loss=2.688847541809082
I0204 02:59:50.414770 139923852027648 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.262270450592041, loss=2.531438112258911
I0204 03:00:37.226966 139923868813056 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.0551866292953491, loss=3.444256544113159
I0204 03:01:24.185314 139923852027648 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3057425022125244, loss=2.6417157649993896
I0204 03:02:11.218285 139923868813056 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2474533319473267, loss=2.6394100189208984
I0204 03:02:49.886146 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:03:00.726792 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:03:30.930169 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:03:32.579195 140085747812160 submission_runner.py:408] Time since start: 33077.86s, 	Step: 64183, 	{'train/accuracy': 0.6518945097923279, 'train/loss': 1.4362150430679321, 'validation/accuracy': 0.605139970779419, 'validation/loss': 1.6577297449111938, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.3341472148895264, 'test/num_examples': 10000, 'score': 29870.425412654877, 'total_duration': 33077.85925221443, 'accumulated_submission_time': 29870.425412654877, 'accumulated_eval_time': 3200.9960539340973, 'accumulated_logging_time': 2.816265344619751}
I0204 03:03:32.605970 139923852027648 logging_writer.py:48] [64183] accumulated_eval_time=3200.996054, accumulated_logging_time=2.816265, accumulated_submission_time=29870.425413, global_step=64183, preemption_count=0, score=29870.425413, test/accuracy=0.480600, test/loss=2.334147, test/num_examples=10000, total_duration=33077.859252, train/accuracy=0.651895, train/loss=1.436215, validation/accuracy=0.605140, validation/loss=1.657730, validation/num_examples=50000
I0204 03:03:40.003667 139923868813056 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.1446709632873535, loss=3.8640336990356445
I0204 03:04:23.574940 139923852027648 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.1495829820632935, loss=2.3899028301239014
I0204 03:05:10.544591 139923868813056 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.360627293586731, loss=2.681751251220703
I0204 03:05:57.252547 139923852027648 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3270866870880127, loss=2.437692642211914
I0204 03:06:44.296937 139923868813056 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.531030297279358, loss=2.595599412918091
I0204 03:07:31.076359 139923852027648 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.1989418268203735, loss=4.880176544189453
I0204 03:08:17.940768 139923868813056 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.4360603094100952, loss=2.5726394653320312
I0204 03:09:04.723584 139923852027648 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.1834123134613037, loss=3.0747878551483154
I0204 03:09:51.666383 139923868813056 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2401551008224487, loss=2.6696810722351074
I0204 03:10:32.892981 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:10:43.342260 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:11:18.430554 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:11:20.073605 140085747812160 submission_runner.py:408] Time since start: 33545.35s, 	Step: 65089, 	{'train/accuracy': 0.6542773246765137, 'train/loss': 1.4176008701324463, 'validation/accuracy': 0.6029399633407593, 'validation/loss': 1.6662062406539917, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.3240888118743896, 'test/num_examples': 10000, 'score': 30290.650116682053, 'total_duration': 33545.353556632996, 'accumulated_submission_time': 30290.650116682053, 'accumulated_eval_time': 3248.1766040325165, 'accumulated_logging_time': 2.854301929473877}
I0204 03:11:20.100999 139923852027648 logging_writer.py:48] [65089] accumulated_eval_time=3248.176604, accumulated_logging_time=2.854302, accumulated_submission_time=30290.650117, global_step=65089, preemption_count=0, score=30290.650117, test/accuracy=0.488000, test/loss=2.324089, test/num_examples=10000, total_duration=33545.353557, train/accuracy=0.654277, train/loss=1.417601, validation/accuracy=0.602940, validation/loss=1.666206, validation/num_examples=50000
I0204 03:11:25.035990 139923868813056 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.2540167570114136, loss=2.4949002265930176
I0204 03:12:08.159962 139923852027648 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.3447984457015991, loss=2.495296001434326
I0204 03:12:55.183401 139923868813056 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.15541672706604, loss=2.3823938369750977
I0204 03:13:42.272519 139923852027648 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.2866114377975464, loss=2.500105857849121
I0204 03:14:29.144370 139923868813056 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3707857131958008, loss=2.682910919189453
I0204 03:15:15.794033 139923852027648 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1943919658660889, loss=3.6763553619384766
I0204 03:16:02.832135 139923868813056 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.2755556106567383, loss=3.0062789916992188
I0204 03:16:49.243268 139923852027648 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.2801320552825928, loss=2.398214817047119
I0204 03:17:35.986821 139923868813056 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.010661244392395, loss=5.098768711090088
I0204 03:18:20.292396 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:18:30.843965 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:19:03.515782 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:19:05.156637 140085747812160 submission_runner.py:408] Time since start: 34010.44s, 	Step: 65996, 	{'train/accuracy': 0.667675793170929, 'train/loss': 1.373803734779358, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.6528773307800293, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.314547300338745, 'test/num_examples': 10000, 'score': 30710.779942512512, 'total_duration': 34010.43670129776, 'accumulated_submission_time': 30710.779942512512, 'accumulated_eval_time': 3293.040856361389, 'accumulated_logging_time': 2.891425609588623}
I0204 03:19:05.186774 139923852027648 logging_writer.py:48] [65996] accumulated_eval_time=3293.040856, accumulated_logging_time=2.891426, accumulated_submission_time=30710.779943, global_step=65996, preemption_count=0, score=30710.779943, test/accuracy=0.489900, test/loss=2.314547, test/num_examples=10000, total_duration=34010.436701, train/accuracy=0.667676, train/loss=1.373804, validation/accuracy=0.610440, validation/loss=1.652877, validation/num_examples=50000
I0204 03:19:07.247685 139923868813056 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.0564194917678833, loss=3.0640249252319336
I0204 03:19:49.983923 139923852027648 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.0019967555999756, loss=4.825778961181641
I0204 03:20:36.886604 139923868813056 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.9994933605194092, loss=4.274646759033203
I0204 03:21:24.165117 139923852027648 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.1331086158752441, loss=3.4765336513519287
I0204 03:22:10.917976 139923868813056 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.0654898881912231, loss=4.436786651611328
I0204 03:22:58.087039 139923852027648 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2066940069198608, loss=4.9823174476623535
I0204 03:23:45.147530 139923868813056 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.261772871017456, loss=2.456129550933838
I0204 03:24:32.020706 139923852027648 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.2897239923477173, loss=2.524259567260742
I0204 03:25:18.826568 139923868813056 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.4721479415893555, loss=2.582695960998535
I0204 03:26:05.670276 139923852027648 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3096891641616821, loss=2.4620158672332764
I0204 03:26:05.687303 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:26:16.339986 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:26:44.101824 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:26:45.750103 140085747812160 submission_runner.py:408] Time since start: 34471.03s, 	Step: 66901, 	{'train/accuracy': 0.6533398032188416, 'train/loss': 1.4194730520248413, 'validation/accuracy': 0.6050599813461304, 'validation/loss': 1.660703182220459, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.354022741317749, 'test/num_examples': 10000, 'score': 31131.218547344208, 'total_duration': 34471.03016901016, 'accumulated_submission_time': 31131.218547344208, 'accumulated_eval_time': 3333.1036410331726, 'accumulated_logging_time': 2.93249773979187}
I0204 03:26:45.776965 139923868813056 logging_writer.py:48] [66901] accumulated_eval_time=3333.103641, accumulated_logging_time=2.932498, accumulated_submission_time=31131.218547, global_step=66901, preemption_count=0, score=31131.218547, test/accuracy=0.481600, test/loss=2.354023, test/num_examples=10000, total_duration=34471.030169, train/accuracy=0.653340, train/loss=1.419473, validation/accuracy=0.605060, validation/loss=1.660703, validation/num_examples=50000
I0204 03:27:28.537983 139923852027648 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.358466625213623, loss=2.5373411178588867
I0204 03:28:15.434064 139923868813056 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.1670984029769897, loss=2.9549498558044434
I0204 03:29:02.657746 139923852027648 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.1887611150741577, loss=2.7835168838500977
I0204 03:29:49.407418 139923868813056 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.151511549949646, loss=2.6587538719177246
I0204 03:30:36.093138 139923852027648 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.014561414718628, loss=4.469717979431152
I0204 03:31:23.261391 139923868813056 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.3211289644241333, loss=2.5434558391571045
I0204 03:32:10.137428 139923852027648 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.9976350665092468, loss=3.868765354156494
I0204 03:32:57.259312 139923868813056 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.2442052364349365, loss=2.689166784286499
I0204 03:33:44.209712 139923852027648 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.2063225507736206, loss=2.470966339111328
I0204 03:33:45.751352 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:33:56.288520 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:34:28.697230 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:34:30.344641 140085747812160 submission_runner.py:408] Time since start: 34935.62s, 	Step: 67805, 	{'train/accuracy': 0.6598047018051147, 'train/loss': 1.3968747854232788, 'validation/accuracy': 0.6152600049972534, 'validation/loss': 1.627685785293579, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.301361322402954, 'test/num_examples': 10000, 'score': 31551.129816770554, 'total_duration': 34935.6247048378, 'accumulated_submission_time': 31551.129816770554, 'accumulated_eval_time': 3377.696927547455, 'accumulated_logging_time': 2.9715535640716553}
I0204 03:34:30.372423 139923868813056 logging_writer.py:48] [67805] accumulated_eval_time=3377.696928, accumulated_logging_time=2.971554, accumulated_submission_time=31551.129817, global_step=67805, preemption_count=0, score=31551.129817, test/accuracy=0.492900, test/loss=2.301361, test/num_examples=10000, total_duration=34935.624705, train/accuracy=0.659805, train/loss=1.396875, validation/accuracy=0.615260, validation/loss=1.627686, validation/num_examples=50000
I0204 03:35:11.103354 139923852027648 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.1998379230499268, loss=3.7551279067993164
I0204 03:35:57.666007 139923868813056 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.2724878787994385, loss=2.766557455062866
I0204 03:36:44.628991 139923852027648 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2814799547195435, loss=2.450747013092041
I0204 03:37:31.526927 139923868813056 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3027024269104004, loss=2.5495035648345947
I0204 03:38:18.533205 139923852027648 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3559316396713257, loss=4.090486526489258
I0204 03:39:05.423187 139923868813056 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3076543807983398, loss=2.4757018089294434
I0204 03:39:52.304321 139923852027648 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.1124553680419922, loss=4.268525123596191
I0204 03:40:39.213637 139923868813056 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.0089906454086304, loss=5.042975425720215
I0204 03:41:25.878752 139923852027648 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.097357988357544, loss=3.218369245529175
I0204 03:41:30.720798 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:41:41.216151 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:42:15.191007 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:42:16.838229 140085747812160 submission_runner.py:408] Time since start: 35402.12s, 	Step: 68712, 	{'train/accuracy': 0.6655663847923279, 'train/loss': 1.3830251693725586, 'validation/accuracy': 0.6067999601364136, 'validation/loss': 1.658873438835144, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.339237689971924, 'test/num_examples': 10000, 'score': 31971.418674468994, 'total_duration': 35402.118267059326, 'accumulated_submission_time': 31971.418674468994, 'accumulated_eval_time': 3423.8143379688263, 'accumulated_logging_time': 3.0081887245178223}
I0204 03:42:16.871782 139923868813056 logging_writer.py:48] [68712] accumulated_eval_time=3423.814338, accumulated_logging_time=3.008189, accumulated_submission_time=31971.418674, global_step=68712, preemption_count=0, score=31971.418674, test/accuracy=0.483300, test/loss=2.339238, test/num_examples=10000, total_duration=35402.118267, train/accuracy=0.665566, train/loss=1.383025, validation/accuracy=0.606800, validation/loss=1.658873, validation/num_examples=50000
I0204 03:42:54.514716 139923852027648 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.404589295387268, loss=2.504448652267456
I0204 03:43:41.029541 139923868813056 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2924503087997437, loss=2.6584508419036865
I0204 03:44:27.995851 139923852027648 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1265690326690674, loss=3.456663131713867
I0204 03:45:14.626186 139923868813056 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.9952675700187683, loss=4.439774513244629
I0204 03:46:01.503659 139923852027648 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.0583666563034058, loss=3.9482083320617676
I0204 03:46:48.259668 139923868813056 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.279361367225647, loss=2.9489665031433105
I0204 03:47:34.962312 139923852027648 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.0959950685501099, loss=4.424647808074951
I0204 03:48:21.756986 139923868813056 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2473959922790527, loss=2.8903138637542725
I0204 03:49:08.514361 139923852027648 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.1089282035827637, loss=3.3764843940734863
I0204 03:49:17.123292 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:49:27.486679 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:49:57.799365 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:49:59.441427 140085747812160 submission_runner.py:408] Time since start: 35864.72s, 	Step: 69620, 	{'train/accuracy': 0.6632031202316284, 'train/loss': 1.383965253829956, 'validation/accuracy': 0.610539972782135, 'validation/loss': 1.6313594579696655, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3167552947998047, 'test/num_examples': 10000, 'score': 32391.608671188354, 'total_duration': 35864.72148346901, 'accumulated_submission_time': 32391.608671188354, 'accumulated_eval_time': 3466.132476091385, 'accumulated_logging_time': 3.0513832569122314}
I0204 03:49:59.469957 139923868813056 logging_writer.py:48] [69620] accumulated_eval_time=3466.132476, accumulated_logging_time=3.051383, accumulated_submission_time=32391.608671, global_step=69620, preemption_count=0, score=32391.608671, test/accuracy=0.485600, test/loss=2.316755, test/num_examples=10000, total_duration=35864.721483, train/accuracy=0.663203, train/loss=1.383965, validation/accuracy=0.610540, validation/loss=1.631359, validation/num_examples=50000
I0204 03:50:33.169360 139923852027648 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.319165825843811, loss=2.5971500873565674
I0204 03:51:19.917338 139923868813056 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.3380895853042603, loss=2.3346505165100098
I0204 03:52:06.708605 139923852027648 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3443835973739624, loss=2.3982315063476562
I0204 03:52:53.791170 139923868813056 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2615101337432861, loss=3.464432954788208
I0204 03:53:41.045412 139923852027648 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.275307059288025, loss=2.4907569885253906
I0204 03:54:28.225064 139923868813056 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.1480599641799927, loss=3.212019443511963
I0204 03:55:15.092572 139923852027648 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.1910089254379272, loss=2.4209091663360596
I0204 03:56:02.212517 139923868813056 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.1452561616897583, loss=4.074098587036133
I0204 03:56:49.166127 139923852027648 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.1491807699203491, loss=4.924539089202881
I0204 03:56:59.630165 140085747812160 spec.py:321] Evaluating on the training split.
I0204 03:57:10.421961 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 03:57:44.814926 140085747812160 spec.py:349] Evaluating on the test split.
I0204 03:57:46.451037 140085747812160 submission_runner.py:408] Time since start: 36331.73s, 	Step: 70524, 	{'train/accuracy': 0.6672461032867432, 'train/loss': 1.3634191751480103, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.6041995286941528, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.2632946968078613, 'test/num_examples': 10000, 'score': 32811.70836234093, 'total_duration': 36331.73110461235, 'accumulated_submission_time': 32811.70836234093, 'accumulated_eval_time': 3512.9533495903015, 'accumulated_logging_time': 3.0895795822143555}
I0204 03:57:46.480573 139923868813056 logging_writer.py:48] [70524] accumulated_eval_time=3512.953350, accumulated_logging_time=3.089580, accumulated_submission_time=32811.708362, global_step=70524, preemption_count=0, score=32811.708362, test/accuracy=0.495000, test/loss=2.263295, test/num_examples=10000, total_duration=36331.731105, train/accuracy=0.667246, train/loss=1.363419, validation/accuracy=0.617040, validation/loss=1.604200, validation/num_examples=50000
I0204 03:58:18.551611 139923852027648 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2093251943588257, loss=2.4474759101867676
I0204 03:59:05.418787 139923868813056 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.283453345298767, loss=2.517764091491699
I0204 03:59:52.510477 139923852027648 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3460291624069214, loss=2.389641761779785
I0204 04:00:39.297895 139923868813056 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.0399812459945679, loss=4.0722198486328125
I0204 04:01:26.191449 139923852027648 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2732288837432861, loss=2.7725863456726074
I0204 04:02:13.238257 139923868813056 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2408692836761475, loss=4.951352119445801
I0204 04:03:00.567813 139923852027648 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3143880367279053, loss=2.5093088150024414
I0204 04:03:47.942642 139923868813056 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.0605303049087524, loss=3.969291925430298
I0204 04:04:34.994918 139923852027648 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.247524619102478, loss=2.4202632904052734
I0204 04:04:46.503344 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:04:56.900244 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:05:28.627896 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:05:30.271584 140085747812160 submission_runner.py:408] Time since start: 36795.55s, 	Step: 71426, 	{'train/accuracy': 0.6733202934265137, 'train/loss': 1.3281680345535278, 'validation/accuracy': 0.6173200011253357, 'validation/loss': 1.5995622873306274, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2829320430755615, 'test/num_examples': 10000, 'score': 33231.66958928108, 'total_duration': 36795.55163145065, 'accumulated_submission_time': 33231.66958928108, 'accumulated_eval_time': 3556.7215859889984, 'accumulated_logging_time': 3.1303675174713135}
I0204 04:05:30.316711 139923868813056 logging_writer.py:48] [71426] accumulated_eval_time=3556.721586, accumulated_logging_time=3.130368, accumulated_submission_time=33231.669589, global_step=71426, preemption_count=0, score=33231.669589, test/accuracy=0.494300, test/loss=2.282932, test/num_examples=10000, total_duration=36795.551631, train/accuracy=0.673320, train/loss=1.328168, validation/accuracy=0.617320, validation/loss=1.599562, validation/num_examples=50000
I0204 04:06:01.169359 139923852027648 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.474074125289917, loss=2.5970048904418945
I0204 04:06:47.979570 139923868813056 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.0955188274383545, loss=3.71761155128479
I0204 04:07:35.337280 139923852027648 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.291606068611145, loss=2.3976855278015137
I0204 04:08:22.282979 139923868813056 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.147440791130066, loss=3.5259652137756348
I0204 04:09:09.095546 139923852027648 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.2034633159637451, loss=2.9003701210021973
I0204 04:09:56.021914 139923868813056 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.2762576341629028, loss=3.298025369644165
I0204 04:10:43.046800 139923852027648 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.1276050806045532, loss=3.0898709297180176
I0204 04:11:29.947648 139923868813056 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.5495575666427612, loss=2.6697680950164795
I0204 04:12:16.955777 139923852027648 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3975939750671387, loss=2.478140354156494
I0204 04:12:30.423861 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:12:41.130179 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:13:16.912833 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:13:18.558392 140085747812160 submission_runner.py:408] Time since start: 37263.84s, 	Step: 72330, 	{'train/accuracy': 0.6946874856948853, 'train/loss': 1.2652043104171753, 'validation/accuracy': 0.6173799633979797, 'validation/loss': 1.6047745943069458, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.269059896469116, 'test/num_examples': 10000, 'score': 33651.71494960785, 'total_duration': 37263.838456869125, 'accumulated_submission_time': 33651.71494960785, 'accumulated_eval_time': 3604.856119155884, 'accumulated_logging_time': 3.186277151107788}
I0204 04:13:18.589103 139923868813056 logging_writer.py:48] [72330] accumulated_eval_time=3604.856119, accumulated_logging_time=3.186277, accumulated_submission_time=33651.714950, global_step=72330, preemption_count=0, score=33651.714950, test/accuracy=0.491800, test/loss=2.269060, test/num_examples=10000, total_duration=37263.838457, train/accuracy=0.694687, train/loss=1.265204, validation/accuracy=0.617380, validation/loss=1.604775, validation/num_examples=50000
I0204 04:13:47.722864 139923852027648 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3714078664779663, loss=2.6436338424682617
I0204 04:14:34.488860 139923868813056 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2052052021026611, loss=4.75351095199585
I0204 04:15:21.931933 139923852027648 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.281964898109436, loss=2.393824815750122
I0204 04:16:08.606920 139923868813056 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.0285403728485107, loss=4.773346900939941
I0204 04:16:55.553280 139923852027648 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.4212703704833984, loss=2.538609027862549
I0204 04:17:42.512177 139923868813056 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.0871155261993408, loss=4.48508882522583
I0204 04:18:29.318881 139923852027648 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.123453140258789, loss=3.356043815612793
I0204 04:19:16.127362 139923868813056 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.2756158113479614, loss=2.3869075775146484
I0204 04:20:02.925682 139923852027648 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.356449842453003, loss=2.4443869590759277
I0204 04:20:19.015871 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:20:29.516016 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:21:05.498084 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:21:07.155031 140085747812160 submission_runner.py:408] Time since start: 37732.44s, 	Step: 73236, 	{'train/accuracy': 0.6619726419448853, 'train/loss': 1.3814362287521362, 'validation/accuracy': 0.613860011100769, 'validation/loss': 1.6136529445648193, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.2791812419891357, 'test/num_examples': 10000, 'score': 34072.08019518852, 'total_duration': 37732.43508505821, 'accumulated_submission_time': 34072.08019518852, 'accumulated_eval_time': 3652.9952659606934, 'accumulated_logging_time': 3.2280800342559814}
I0204 04:21:07.186611 139923868813056 logging_writer.py:48] [73236] accumulated_eval_time=3652.995266, accumulated_logging_time=3.228080, accumulated_submission_time=34072.080195, global_step=73236, preemption_count=0, score=34072.080195, test/accuracy=0.492700, test/loss=2.279181, test/num_examples=10000, total_duration=37732.435085, train/accuracy=0.661973, train/loss=1.381436, validation/accuracy=0.613860, validation/loss=1.613653, validation/num_examples=50000
I0204 04:21:33.871301 139923852027648 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.4600293636322021, loss=2.3525688648223877
I0204 04:22:20.172924 139923868813056 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.4919164180755615, loss=2.446316957473755
I0204 04:23:07.803836 139923852027648 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.2525575160980225, loss=2.8757898807525635
I0204 04:23:54.745292 139923868813056 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.4291236400604248, loss=2.4168996810913086
I0204 04:24:41.756669 139923852027648 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3943088054656982, loss=2.4988787174224854
I0204 04:25:28.888049 139923868813056 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.2846282720565796, loss=2.363962411880493
I0204 04:26:15.884749 139923852027648 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.1592813730239868, loss=4.458930015563965
I0204 04:27:02.579639 139923868813056 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.1939657926559448, loss=3.3584446907043457
I0204 04:27:49.651300 139923852027648 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.2345058917999268, loss=2.684713840484619
I0204 04:28:07.194679 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:28:17.995252 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:28:46.939451 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:28:48.582542 140085747812160 submission_runner.py:408] Time since start: 38193.86s, 	Step: 74139, 	{'train/accuracy': 0.6750390529632568, 'train/loss': 1.3150864839553833, 'validation/accuracy': 0.6202999949455261, 'validation/loss': 1.5818755626678467, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.2524607181549072, 'test/num_examples': 10000, 'score': 34492.027354717255, 'total_duration': 38193.862604141235, 'accumulated_submission_time': 34492.027354717255, 'accumulated_eval_time': 3694.3831102848053, 'accumulated_logging_time': 3.2697389125823975}
I0204 04:28:48.611938 139923868813056 logging_writer.py:48] [74139] accumulated_eval_time=3694.383110, accumulated_logging_time=3.269739, accumulated_submission_time=34492.027355, global_step=74139, preemption_count=0, score=34492.027355, test/accuracy=0.502100, test/loss=2.252461, test/num_examples=10000, total_duration=38193.862604, train/accuracy=0.675039, train/loss=1.315086, validation/accuracy=0.620300, validation/loss=1.581876, validation/num_examples=50000
I0204 04:29:14.081203 139923852027648 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.137862205505371, loss=5.0367937088012695
I0204 04:30:00.421612 139923868813056 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.2406678199768066, loss=2.3371407985687256
I0204 04:30:47.363464 139923852027648 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.5330785512924194, loss=2.6068978309631348
I0204 04:31:34.025570 139923868813056 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.4267330169677734, loss=2.6225640773773193
I0204 04:32:20.790897 139923852027648 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.34001624584198, loss=2.5949225425720215
I0204 04:33:07.938875 139923868813056 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.5791733264923096, loss=2.536065101623535
I0204 04:33:54.722425 139923852027648 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.558669090270996, loss=2.260010242462158
I0204 04:34:41.509627 139923868813056 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.1027121543884277, loss=3.404481887817383
I0204 04:35:28.560209 139923852027648 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.2665292024612427, loss=2.32330322265625
I0204 04:35:48.797735 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:35:59.041331 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:36:34.359663 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:36:36.009208 140085747812160 submission_runner.py:408] Time since start: 38661.29s, 	Step: 75045, 	{'train/accuracy': 0.6972851157188416, 'train/loss': 1.2580310106277466, 'validation/accuracy': 0.6181199550628662, 'validation/loss': 1.6094812154769897, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.2771427631378174, 'test/num_examples': 10000, 'score': 34912.15011835098, 'total_duration': 38661.28926753998, 'accumulated_submission_time': 34912.15011835098, 'accumulated_eval_time': 3741.594585418701, 'accumulated_logging_time': 3.3109562397003174}
I0204 04:36:36.039395 139923868813056 logging_writer.py:48] [75045] accumulated_eval_time=3741.594585, accumulated_logging_time=3.310956, accumulated_submission_time=34912.150118, global_step=75045, preemption_count=0, score=34912.150118, test/accuracy=0.498400, test/loss=2.277143, test/num_examples=10000, total_duration=38661.289268, train/accuracy=0.697285, train/loss=1.258031, validation/accuracy=0.618120, validation/loss=1.609481, validation/num_examples=50000
I0204 04:36:59.026134 139923852027648 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.4687305688858032, loss=2.485107421875
I0204 04:37:44.440762 139923868813056 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1905008554458618, loss=2.821324110031128
I0204 04:38:31.235604 139923852027648 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.0029637813568115, loss=4.5904860496521
I0204 04:39:17.955668 139923868813056 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.487781047821045, loss=2.778024911880493
I0204 04:40:04.723321 139923852027648 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.3392086029052734, loss=2.303536891937256
I0204 04:40:51.641733 139923868813056 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.308411717414856, loss=2.479132652282715
I0204 04:41:38.514797 139923852027648 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.098170280456543, loss=4.9601054191589355
I0204 04:42:25.419562 139923868813056 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.3043692111968994, loss=2.6174042224884033
I0204 04:43:12.288266 139923852027648 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.1340994834899902, loss=4.414889335632324
I0204 04:43:36.234314 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:43:46.858500 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:44:19.709312 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:44:21.346515 140085747812160 submission_runner.py:408] Time since start: 39126.63s, 	Step: 75953, 	{'train/accuracy': 0.6793164014816284, 'train/loss': 1.3091703653335571, 'validation/accuracy': 0.6315000057220459, 'validation/loss': 1.5481455326080322, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2062301635742188, 'test/num_examples': 10000, 'score': 35332.28189110756, 'total_duration': 39126.626574754715, 'accumulated_submission_time': 35332.28189110756, 'accumulated_eval_time': 3786.706786632538, 'accumulated_logging_time': 3.353020668029785}
I0204 04:44:21.379133 139923868813056 logging_writer.py:48] [75953] accumulated_eval_time=3786.706787, accumulated_logging_time=3.353021, accumulated_submission_time=35332.281891, global_step=75953, preemption_count=0, score=35332.281891, test/accuracy=0.506900, test/loss=2.206230, test/num_examples=10000, total_duration=39126.626575, train/accuracy=0.679316, train/loss=1.309170, validation/accuracy=0.631500, validation/loss=1.548146, validation/num_examples=50000
I0204 04:44:41.097421 139923852027648 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3815182447433472, loss=2.411543607711792
I0204 04:45:26.235352 139923868813056 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4107195138931274, loss=2.5005862712860107
I0204 04:46:13.211468 139923852027648 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2207809686660767, loss=2.3921823501586914
I0204 04:46:59.804979 139923868813056 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.2672085762023926, loss=2.5099148750305176
I0204 04:47:46.800124 139923852027648 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.3021843433380127, loss=2.2969629764556885
I0204 04:48:33.460861 139923868813056 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.4170104265213013, loss=2.374263286590576
I0204 04:49:20.348111 139923852027648 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4749512672424316, loss=2.529949188232422
I0204 04:50:07.119463 139923868813056 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.395772099494934, loss=2.315922260284424
I0204 04:50:54.079872 139923852027648 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.1563327312469482, loss=4.918168544769287
I0204 04:51:21.786231 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:51:32.409009 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:52:10.248398 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:52:11.896033 140085747812160 submission_runner.py:408] Time since start: 39597.18s, 	Step: 76861, 	{'train/accuracy': 0.6729882955551147, 'train/loss': 1.3250367641448975, 'validation/accuracy': 0.6203599572181702, 'validation/loss': 1.5800641775131226, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2438337802886963, 'test/num_examples': 10000, 'score': 35752.626155376434, 'total_duration': 39597.17609834671, 'accumulated_submission_time': 35752.626155376434, 'accumulated_eval_time': 3836.8165822029114, 'accumulated_logging_time': 3.397064447402954}
I0204 04:52:11.925865 139923868813056 logging_writer.py:48] [76861] accumulated_eval_time=3836.816582, accumulated_logging_time=3.397064, accumulated_submission_time=35752.626155, global_step=76861, preemption_count=0, score=35752.626155, test/accuracy=0.497400, test/loss=2.243834, test/num_examples=10000, total_duration=39597.176098, train/accuracy=0.672988, train/loss=1.325037, validation/accuracy=0.620360, validation/loss=1.580064, validation/num_examples=50000
I0204 04:52:28.355315 139923852027648 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.2843430042266846, loss=2.3676514625549316
I0204 04:53:13.190439 139923868813056 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.1970046758651733, loss=4.943690299987793
I0204 04:54:00.093046 139923852027648 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.0882328748703003, loss=4.339757442474365
I0204 04:54:47.064720 139923868813056 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.1380772590637207, loss=4.78831148147583
I0204 04:55:33.949654 139923852027648 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.0821031332015991, loss=4.114560604095459
I0204 04:56:20.846552 139923868813056 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.352730631828308, loss=2.318605422973633
I0204 04:57:07.602325 139923852027648 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.07254159450531, loss=4.406498432159424
I0204 04:57:54.793806 139923868813056 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.1724368333816528, loss=3.265350341796875
I0204 04:58:41.780893 139923852027648 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.2867382764816284, loss=2.397670030593872
I0204 04:59:11.938741 140085747812160 spec.py:321] Evaluating on the training split.
I0204 04:59:22.424911 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 04:59:57.959628 140085747812160 spec.py:349] Evaluating on the test split.
I0204 04:59:59.633109 140085747812160 submission_runner.py:408] Time since start: 40064.91s, 	Step: 77766, 	{'train/accuracy': 0.7009375095367432, 'train/loss': 1.209425687789917, 'validation/accuracy': 0.6351000070571899, 'validation/loss': 1.5253475904464722, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.1836676597595215, 'test/num_examples': 10000, 'score': 36172.57538366318, 'total_duration': 40064.91317510605, 'accumulated_submission_time': 36172.57538366318, 'accumulated_eval_time': 3884.5109570026398, 'accumulated_logging_time': 3.4398858547210693}
I0204 04:59:59.664980 139923868813056 logging_writer.py:48] [77766] accumulated_eval_time=3884.510957, accumulated_logging_time=3.439886, accumulated_submission_time=36172.575384, global_step=77766, preemption_count=0, score=36172.575384, test/accuracy=0.511200, test/loss=2.183668, test/num_examples=10000, total_duration=40064.913175, train/accuracy=0.700938, train/loss=1.209426, validation/accuracy=0.635100, validation/loss=1.525348, validation/num_examples=50000
I0204 05:00:14.028261 139923852027648 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.1021620035171509, loss=3.729193687438965
I0204 05:00:58.593410 139923868813056 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2586870193481445, loss=2.4357967376708984
I0204 05:01:45.818968 139923852027648 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.5579136610031128, loss=2.418530225753784
I0204 05:02:33.240560 139923868813056 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.4479212760925293, loss=2.4027562141418457
I0204 05:03:20.128367 139923852027648 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2464025020599365, loss=2.5786821842193604
I0204 05:04:07.140651 139923868813056 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.2113697528839111, loss=4.480339527130127
I0204 05:04:53.865699 139923852027648 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.4473053216934204, loss=2.3609819412231445
I0204 05:05:40.794902 139923868813056 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.0910588502883911, loss=4.726155757904053
I0204 05:06:27.725194 139923852027648 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.330397367477417, loss=2.409619092941284
I0204 05:06:59.962411 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:07:10.798430 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:07:47.057938 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:07:48.687109 140085747812160 submission_runner.py:408] Time since start: 40533.97s, 	Step: 78670, 	{'train/accuracy': 0.6809374690055847, 'train/loss': 1.2866195440292358, 'validation/accuracy': 0.6292799711227417, 'validation/loss': 1.5328986644744873, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.1805171966552734, 'test/num_examples': 10000, 'score': 36592.812076091766, 'total_duration': 40533.96717476845, 'accumulated_submission_time': 36592.812076091766, 'accumulated_eval_time': 3933.235659122467, 'accumulated_logging_time': 3.4815621376037598}
I0204 05:07:48.715395 139923868813056 logging_writer.py:48] [78670] accumulated_eval_time=3933.235659, accumulated_logging_time=3.481562, accumulated_submission_time=36592.812076, global_step=78670, preemption_count=0, score=36592.812076, test/accuracy=0.509900, test/loss=2.180517, test/num_examples=10000, total_duration=40533.967175, train/accuracy=0.680937, train/loss=1.286620, validation/accuracy=0.629280, validation/loss=1.532899, validation/num_examples=50000
I0204 05:08:01.442665 139923852027648 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.3339753150939941, loss=2.737722396850586
I0204 05:08:45.997490 139923868813056 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.4029347896575928, loss=2.4015297889709473
I0204 05:09:33.064429 139923852027648 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.3714858293533325, loss=2.389209270477295
I0204 05:10:19.969256 139923868813056 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.2591133117675781, loss=2.888084888458252
I0204 05:11:07.121393 139923852027648 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.328973650932312, loss=2.351229667663574
I0204 05:11:54.087097 139923868813056 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.2805980443954468, loss=2.7149341106414795
I0204 05:12:41.248432 139923852027648 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3873168230056763, loss=3.80869197845459
I0204 05:13:28.193855 139923868813056 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.3016672134399414, loss=2.7328438758850098
I0204 05:14:15.052207 139923852027648 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2187647819519043, loss=3.2604894638061523
I0204 05:14:48.958607 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:14:59.390292 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:15:35.731002 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:15:37.370577 140085747812160 submission_runner.py:408] Time since start: 41002.65s, 	Step: 79574, 	{'train/accuracy': 0.6755468845367432, 'train/loss': 1.3667091131210327, 'validation/accuracy': 0.6249200105667114, 'validation/loss': 1.604951024055481, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.2638602256774902, 'test/num_examples': 10000, 'score': 37012.992997169495, 'total_duration': 41002.65064263344, 'accumulated_submission_time': 37012.992997169495, 'accumulated_eval_time': 3981.6476192474365, 'accumulated_logging_time': 3.5212857723236084}
I0204 05:15:37.399833 139923868813056 logging_writer.py:48] [79574] accumulated_eval_time=3981.647619, accumulated_logging_time=3.521286, accumulated_submission_time=37012.992997, global_step=79574, preemption_count=0, score=37012.992997, test/accuracy=0.504700, test/loss=2.263860, test/num_examples=10000, total_duration=41002.650643, train/accuracy=0.675547, train/loss=1.366709, validation/accuracy=0.624920, validation/loss=1.604951, validation/num_examples=50000
I0204 05:15:48.492457 139923852027648 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.274793028831482, loss=2.7943687438964844
I0204 05:16:32.871024 139923868813056 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.5030699968338013, loss=2.311843156814575
I0204 05:17:19.560121 139923852027648 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.5101699829101562, loss=2.4430253505706787
I0204 05:18:06.243187 139923868813056 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.298609733581543, loss=2.3248400688171387
I0204 05:18:53.002760 139923852027648 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.2995574474334717, loss=3.710655689239502
I0204 05:19:39.908919 139923868813056 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3466267585754395, loss=2.607973575592041
I0204 05:20:26.857856 139923852027648 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.1052683591842651, loss=4.428582668304443
I0204 05:21:13.640364 139923868813056 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3351538181304932, loss=2.675132989883423
I0204 05:22:00.588483 139923852027648 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4333621263504028, loss=2.5691075325012207
I0204 05:22:37.414022 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:22:47.957978 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:23:15.757721 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:23:17.395323 140085747812160 submission_runner.py:408] Time since start: 41462.68s, 	Step: 80480, 	{'train/accuracy': 0.6903710961341858, 'train/loss': 1.2400176525115967, 'validation/accuracy': 0.6298399567604065, 'validation/loss': 1.541815161705017, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.2130167484283447, 'test/num_examples': 10000, 'score': 37432.94675517082, 'total_duration': 41462.675387859344, 'accumulated_submission_time': 37432.94675517082, 'accumulated_eval_time': 4021.6289196014404, 'accumulated_logging_time': 3.5604658126831055}
I0204 05:23:17.424834 139923868813056 logging_writer.py:48] [80480] accumulated_eval_time=4021.628920, accumulated_logging_time=3.560466, accumulated_submission_time=37432.946755, global_step=80480, preemption_count=0, score=37432.946755, test/accuracy=0.501500, test/loss=2.213017, test/num_examples=10000, total_duration=41462.675388, train/accuracy=0.690371, train/loss=1.240018, validation/accuracy=0.629840, validation/loss=1.541815, validation/num_examples=50000
I0204 05:23:26.053568 139923852027648 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.4004929065704346, loss=2.3967058658599854
I0204 05:24:09.747054 139923868813056 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.2899951934814453, loss=2.349238634109497
I0204 05:24:56.479125 139923852027648 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4379335641860962, loss=2.626530885696411
I0204 05:25:43.643933 139923868813056 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.406693696975708, loss=2.5420196056365967
I0204 05:26:30.479608 139923852027648 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.4687429666519165, loss=2.397902011871338
I0204 05:27:17.328636 139923868813056 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.2515028715133667, loss=2.3796093463897705
I0204 05:28:04.512794 139923852027648 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.2302002906799316, loss=3.0562899112701416
I0204 05:28:51.378778 139923868813056 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.369181513786316, loss=2.32353138923645
I0204 05:29:38.339751 139923852027648 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.3340976238250732, loss=2.3823328018188477
I0204 05:30:17.396322 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:30:28.046946 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:31:03.499592 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:31:05.142990 140085747812160 submission_runner.py:408] Time since start: 41930.42s, 	Step: 81384, 	{'train/accuracy': 0.6767382621765137, 'train/loss': 1.3347312211990356, 'validation/accuracy': 0.6283999681472778, 'validation/loss': 1.5647926330566406, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2255892753601074, 'test/num_examples': 10000, 'score': 37852.857625961304, 'total_duration': 41930.42304897308, 'accumulated_submission_time': 37852.857625961304, 'accumulated_eval_time': 4069.3755803108215, 'accumulated_logging_time': 3.6002750396728516}
I0204 05:31:05.172757 139923868813056 logging_writer.py:48] [81384] accumulated_eval_time=4069.375580, accumulated_logging_time=3.600275, accumulated_submission_time=37852.857626, global_step=81384, preemption_count=0, score=37852.857626, test/accuracy=0.502900, test/loss=2.225589, test/num_examples=10000, total_duration=41930.423049, train/accuracy=0.676738, train/loss=1.334731, validation/accuracy=0.628400, validation/loss=1.564793, validation/num_examples=50000
I0204 05:31:12.153391 139923852027648 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.3207719326019287, loss=2.4220404624938965
I0204 05:31:55.553590 139923868813056 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.3501958847045898, loss=2.3429155349731445
I0204 05:32:42.882002 139923852027648 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.371358871459961, loss=2.784569025039673
I0204 05:33:30.007826 139923868813056 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.0859938859939575, loss=4.898921966552734
I0204 05:34:16.958525 139923852027648 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.3368581533432007, loss=2.2608423233032227
I0204 05:35:03.834311 139923868813056 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.338343620300293, loss=2.3986566066741943
I0204 05:35:50.725397 139923852027648 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.2453303337097168, loss=3.2075843811035156
I0204 05:36:37.666122 139923868813056 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.7247248888015747, loss=2.3198957443237305
I0204 05:37:24.284294 139923852027648 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.1998095512390137, loss=4.098297119140625
I0204 05:38:05.206310 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:38:15.712709 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:38:51.374638 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:38:53.023829 140085747812160 submission_runner.py:408] Time since start: 42398.30s, 	Step: 82289, 	{'train/accuracy': 0.6830077767372131, 'train/loss': 1.3016310930252075, 'validation/accuracy': 0.6261599659919739, 'validation/loss': 1.565454125404358, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2108993530273438, 'test/num_examples': 10000, 'score': 38272.83093523979, 'total_duration': 42398.30388045311, 'accumulated_submission_time': 38272.83093523979, 'accumulated_eval_time': 4117.19310426712, 'accumulated_logging_time': 3.6395747661590576}
I0204 05:38:53.055930 139923868813056 logging_writer.py:48] [82289] accumulated_eval_time=4117.193104, accumulated_logging_time=3.639575, accumulated_submission_time=38272.830935, global_step=82289, preemption_count=0, score=38272.830935, test/accuracy=0.505400, test/loss=2.210899, test/num_examples=10000, total_duration=42398.303880, train/accuracy=0.683008, train/loss=1.301631, validation/accuracy=0.626160, validation/loss=1.565454, validation/num_examples=50000
I0204 05:38:57.985561 139923852027648 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.358561635017395, loss=2.2816901206970215
I0204 05:39:40.940654 139923868813056 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.3463400602340698, loss=2.3630642890930176
I0204 05:40:27.774378 139923852027648 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.2506942749023438, loss=4.926218509674072
I0204 05:41:14.819154 139923868813056 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.4231113195419312, loss=2.489968776702881
I0204 05:42:01.684081 139923852027648 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.4435288906097412, loss=2.254380702972412
I0204 05:42:48.711845 139923868813056 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.606890082359314, loss=2.2489991188049316
I0204 05:43:35.646486 139923852027648 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.3009727001190186, loss=3.4013404846191406
I0204 05:44:22.376131 139923868813056 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4423044919967651, loss=2.435469150543213
I0204 05:45:09.170609 139923852027648 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.2443163394927979, loss=4.428008556365967
I0204 05:45:53.199792 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:46:03.739604 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:46:42.463874 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:46:44.103926 140085747812160 submission_runner.py:408] Time since start: 42869.38s, 	Step: 83196, 	{'train/accuracy': 0.6936327815055847, 'train/loss': 1.2280399799346924, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5165338516235352, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.164104700088501, 'test/num_examples': 10000, 'score': 38692.91235637665, 'total_duration': 42869.38398528099, 'accumulated_submission_time': 38692.91235637665, 'accumulated_eval_time': 4168.097243785858, 'accumulated_logging_time': 3.6829824447631836}
I0204 05:46:44.134094 139923868813056 logging_writer.py:48] [83196] accumulated_eval_time=4168.097244, accumulated_logging_time=3.682982, accumulated_submission_time=38692.912356, global_step=83196, preemption_count=0, score=38692.912356, test/accuracy=0.511200, test/loss=2.164105, test/num_examples=10000, total_duration=42869.383985, train/accuracy=0.693633, train/loss=1.228040, validation/accuracy=0.635800, validation/loss=1.516534, validation/num_examples=50000
I0204 05:46:46.190488 139923852027648 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2049651145935059, loss=4.111353874206543
I0204 05:47:28.776549 139923868813056 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.2781869173049927, loss=4.380292892456055
I0204 05:48:15.583447 139923852027648 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.3359320163726807, loss=2.8662989139556885
I0204 05:49:02.523743 139923868813056 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.2351731061935425, loss=3.430828809738159
I0204 05:49:49.489202 139923852027648 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.154021143913269, loss=4.177370548248291
I0204 05:50:36.515214 139923868813056 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.479422688484192, loss=2.4168527126312256
I0204 05:51:23.487669 139923852027648 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.4020113945007324, loss=2.285518169403076
I0204 05:52:10.608364 139923868813056 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.0694255828857422, loss=4.00417947769165
I0204 05:52:57.872596 139923852027648 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.5586469173431396, loss=2.351804256439209
I0204 05:53:44.295791 140085747812160 spec.py:321] Evaluating on the training split.
I0204 05:53:54.920253 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 05:54:29.016037 140085747812160 spec.py:349] Evaluating on the test split.
I0204 05:54:30.658698 140085747812160 submission_runner.py:408] Time since start: 43335.94s, 	Step: 84100, 	{'train/accuracy': 0.6860546469688416, 'train/loss': 1.2814973592758179, 'validation/accuracy': 0.638260006904602, 'validation/loss': 1.5209600925445557, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1791257858276367, 'test/num_examples': 10000, 'score': 39113.014108896255, 'total_duration': 43335.938762664795, 'accumulated_submission_time': 39113.014108896255, 'accumulated_eval_time': 4214.460152864456, 'accumulated_logging_time': 3.7228505611419678}
I0204 05:54:30.691216 139923868813056 logging_writer.py:48] [84100] accumulated_eval_time=4214.460153, accumulated_logging_time=3.722851, accumulated_submission_time=39113.014109, global_step=84100, preemption_count=0, score=39113.014109, test/accuracy=0.513700, test/loss=2.179126, test/num_examples=10000, total_duration=43335.938763, train/accuracy=0.686055, train/loss=1.281497, validation/accuracy=0.638260, validation/loss=1.520960, validation/num_examples=50000
I0204 05:54:31.111109 139923852027648 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.1777526140213013, loss=3.0637576580047607
I0204 05:55:13.803076 139923868813056 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2326565980911255, loss=3.0435848236083984
I0204 05:56:00.399669 139923852027648 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.3933525085449219, loss=2.3629274368286133
I0204 05:56:47.257735 139923868813056 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.4964066743850708, loss=2.2862749099731445
I0204 05:57:34.185540 139923852027648 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.317714810371399, loss=2.853018045425415
I0204 05:58:21.084699 139923868813056 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.154464602470398, loss=4.249904632568359
I0204 05:59:07.828835 139923852027648 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.3080477714538574, loss=4.067863464355469
I0204 05:59:54.772176 139923868813056 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.4803155660629272, loss=2.5613725185394287
I0204 06:00:41.547512 139923852027648 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.4243855476379395, loss=2.3224427700042725
I0204 06:01:28.302175 139923868813056 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.3187834024429321, loss=4.053368091583252
I0204 06:01:30.880869 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:01:41.365336 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:02:17.630709 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:02:19.269320 140085747812160 submission_runner.py:408] Time since start: 43804.55s, 	Step: 85007, 	{'train/accuracy': 0.6898437142372131, 'train/loss': 1.2605102062225342, 'validation/accuracy': 0.6386399865150452, 'validation/loss': 1.5136559009552002, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.1877145767211914, 'test/num_examples': 10000, 'score': 39533.14155960083, 'total_duration': 43804.54938220978, 'accumulated_submission_time': 39533.14155960083, 'accumulated_eval_time': 4262.8486523628235, 'accumulated_logging_time': 3.7656800746917725}
I0204 06:02:19.301122 139923852027648 logging_writer.py:48] [85007] accumulated_eval_time=4262.848652, accumulated_logging_time=3.765680, accumulated_submission_time=39533.141560, global_step=85007, preemption_count=0, score=39533.141560, test/accuracy=0.511800, test/loss=2.187715, test/num_examples=10000, total_duration=43804.549382, train/accuracy=0.689844, train/loss=1.260510, validation/accuracy=0.638640, validation/loss=1.513656, validation/num_examples=50000
I0204 06:02:59.070206 139923868813056 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.241309404373169, loss=3.525371551513672
I0204 06:03:45.769210 139923852027648 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.283014178276062, loss=4.808389663696289
I0204 06:04:32.763984 139923868813056 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.1183396577835083, loss=2.8291897773742676
I0204 06:05:19.576031 139923852027648 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.3856613636016846, loss=2.411630392074585
I0204 06:06:06.471574 139923868813056 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.290952444076538, loss=2.7517096996307373
I0204 06:06:53.087779 139923852027648 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.508710265159607, loss=2.408914804458618
I0204 06:07:39.938074 139923868813056 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.4161311388015747, loss=2.5657033920288086
I0204 06:08:26.762310 139923852027648 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.3328769207000732, loss=3.12593412399292
I0204 06:09:13.773066 139923868813056 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.489189624786377, loss=2.659818410873413
I0204 06:09:19.454995 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:09:29.970630 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:10:05.891522 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:10:07.536124 140085747812160 submission_runner.py:408] Time since start: 44272.82s, 	Step: 85914, 	{'train/accuracy': 0.6995507478713989, 'train/loss': 1.186415195465088, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4698925018310547, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.127619743347168, 'test/num_examples': 10000, 'score': 39953.234171152115, 'total_duration': 44272.816187381744, 'accumulated_submission_time': 39953.234171152115, 'accumulated_eval_time': 4310.929792881012, 'accumulated_logging_time': 3.8074779510498047}
I0204 06:10:07.572711 139923852027648 logging_writer.py:48] [85914] accumulated_eval_time=4310.929793, accumulated_logging_time=3.807478, accumulated_submission_time=39953.234171, global_step=85914, preemption_count=0, score=39953.234171, test/accuracy=0.521100, test/loss=2.127620, test/num_examples=10000, total_duration=44272.816187, train/accuracy=0.699551, train/loss=1.186415, validation/accuracy=0.642300, validation/loss=1.469893, validation/num_examples=50000
I0204 06:10:44.064584 139923868813056 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.1567307710647583, loss=4.478012561798096
I0204 06:11:30.933440 139923852027648 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.6310088634490967, loss=2.371795177459717
I0204 06:12:18.215482 139923868813056 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.3061190843582153, loss=2.980053424835205
I0204 06:13:05.408079 139923852027648 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.366047739982605, loss=2.374443531036377
I0204 06:13:52.671148 139923868813056 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.1189227104187012, loss=3.328073740005493
I0204 06:14:39.598093 139923852027648 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.3108078241348267, loss=3.065419912338257
I0204 06:15:26.585007 139923868813056 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.3694664239883423, loss=2.3187460899353027
I0204 06:16:13.631779 139923852027648 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.1674193143844604, loss=3.3443217277526855
I0204 06:17:00.607580 139923868813056 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.3480392694473267, loss=2.4664578437805176
I0204 06:17:07.901841 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:17:18.790350 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:17:53.858706 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:17:55.501250 140085747812160 submission_runner.py:408] Time since start: 44740.78s, 	Step: 86817, 	{'train/accuracy': 0.6941601634025574, 'train/loss': 1.2347404956817627, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.4868887662887573, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.128563165664673, 'test/num_examples': 10000, 'score': 40373.50329899788, 'total_duration': 44740.781307935715, 'accumulated_submission_time': 40373.50329899788, 'accumulated_eval_time': 4358.529216766357, 'accumulated_logging_time': 3.8530352115631104}
I0204 06:17:55.533337 139923852027648 logging_writer.py:48] [86817] accumulated_eval_time=4358.529217, accumulated_logging_time=3.853035, accumulated_submission_time=40373.503299, global_step=86817, preemption_count=0, score=40373.503299, test/accuracy=0.519100, test/loss=2.128563, test/num_examples=10000, total_duration=44740.781308, train/accuracy=0.694160, train/loss=1.234740, validation/accuracy=0.640620, validation/loss=1.486889, validation/num_examples=50000
I0204 06:18:30.632899 139923868813056 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.2571966648101807, loss=4.977653980255127
I0204 06:19:17.232339 139923852027648 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.7207990884780884, loss=2.307485818862915
I0204 06:20:04.212746 139923868813056 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.494016408920288, loss=4.908795356750488
I0204 06:20:51.008758 139923852027648 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.5763190984725952, loss=2.456709623336792
I0204 06:21:38.001034 139923868813056 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.1691362857818604, loss=4.462518692016602
I0204 06:22:25.434878 139923852027648 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3510847091674805, loss=2.3705601692199707
I0204 06:23:12.743579 139923868813056 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.2831175327301025, loss=2.9257826805114746
I0204 06:23:59.955484 139923852027648 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.2990604639053345, loss=2.797544240951538
I0204 06:24:46.880576 139923868813056 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.5126861333847046, loss=2.258753538131714
I0204 06:24:55.514229 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:25:05.992029 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:25:39.636299 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:25:41.281192 140085747812160 submission_runner.py:408] Time since start: 45206.56s, 	Step: 87720, 	{'train/accuracy': 0.69593745470047, 'train/loss': 1.2813823223114014, 'validation/accuracy': 0.6403999924659729, 'validation/loss': 1.535065770149231, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.182572841644287, 'test/num_examples': 10000, 'score': 40793.42196941376, 'total_duration': 45206.561255693436, 'accumulated_submission_time': 40793.42196941376, 'accumulated_eval_time': 4404.2962164878845, 'accumulated_logging_time': 3.8958733081817627}
I0204 06:25:41.311179 139923852027648 logging_writer.py:48] [87720] accumulated_eval_time=4404.296216, accumulated_logging_time=3.895873, accumulated_submission_time=40793.421969, global_step=87720, preemption_count=0, score=40793.421969, test/accuracy=0.513900, test/loss=2.182573, test/num_examples=10000, total_duration=45206.561256, train/accuracy=0.695937, train/loss=1.281382, validation/accuracy=0.640400, validation/loss=1.535066, validation/num_examples=50000
I0204 06:26:15.238075 139923868813056 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.3626455068588257, loss=2.28951096534729
I0204 06:27:01.876611 139923852027648 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.4652791023254395, loss=2.313662528991699
I0204 06:27:48.887944 139923868813056 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.427815556526184, loss=2.3223538398742676
I0204 06:28:35.857126 139923852027648 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.409056544303894, loss=3.4601566791534424
I0204 06:29:22.697887 139923868813056 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.2438746690750122, loss=2.790997266769409
I0204 06:30:09.576068 139923852027648 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.2912596464157104, loss=3.082636833190918
I0204 06:30:56.336285 139923868813056 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.2195836305618286, loss=3.615760564804077
I0204 06:31:43.193705 139923852027648 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.296132206916809, loss=2.3488266468048096
I0204 06:32:30.363262 139923868813056 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.3969606161117554, loss=2.350944757461548
I0204 06:32:41.318536 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:32:51.715634 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:33:31.619952 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:33:33.257177 140085747812160 submission_runner.py:408] Time since start: 45678.54s, 	Step: 88625, 	{'train/accuracy': 0.7049609422683716, 'train/loss': 1.2156800031661987, 'validation/accuracy': 0.6411799788475037, 'validation/loss': 1.4978916645050049, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.1546852588653564, 'test/num_examples': 10000, 'score': 41213.36802792549, 'total_duration': 45678.537242889404, 'accumulated_submission_time': 41213.36802792549, 'accumulated_eval_time': 4456.234867095947, 'accumulated_logging_time': 3.9354004859924316}
I0204 06:33:33.287688 139923852027648 logging_writer.py:48] [88625] accumulated_eval_time=4456.234867, accumulated_logging_time=3.935400, accumulated_submission_time=41213.368028, global_step=88625, preemption_count=0, score=41213.368028, test/accuracy=0.516100, test/loss=2.154685, test/num_examples=10000, total_duration=45678.537243, train/accuracy=0.704961, train/loss=1.215680, validation/accuracy=0.641180, validation/loss=1.497892, validation/num_examples=50000
I0204 06:34:04.556411 139923868813056 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4443421363830566, loss=4.236131191253662
I0204 06:34:51.113196 139923852027648 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.3975985050201416, loss=2.2196764945983887
I0204 06:35:38.360758 139923868813056 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.5364980697631836, loss=2.3828189373016357
I0204 06:36:25.142134 139923852027648 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.2879185676574707, loss=3.2030282020568848
I0204 06:37:12.164165 139923868813056 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.4570313692092896, loss=3.222634792327881
I0204 06:37:58.773606 139923852027648 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.331146001815796, loss=2.2918906211853027
I0204 06:38:45.561702 139923868813056 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.333688497543335, loss=2.3881731033325195
I0204 06:39:32.430828 139923852027648 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.1615808010101318, loss=3.8319671154022217
I0204 06:40:19.303741 139923868813056 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.5566575527191162, loss=2.336982250213623
I0204 06:40:33.564185 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:40:44.140879 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:41:20.091905 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:41:21.734616 140085747812160 submission_runner.py:408] Time since start: 46147.01s, 	Step: 89532, 	{'train/accuracy': 0.7158789038658142, 'train/loss': 1.1632670164108276, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.5002963542938232, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1391379833221436, 'test/num_examples': 10000, 'score': 41633.58397102356, 'total_duration': 46147.01468038559, 'accumulated_submission_time': 41633.58397102356, 'accumulated_eval_time': 4504.405340433121, 'accumulated_logging_time': 3.9754011631011963}
I0204 06:41:21.771584 139923852027648 logging_writer.py:48] [89532] accumulated_eval_time=4504.405340, accumulated_logging_time=3.975401, accumulated_submission_time=41633.583971, global_step=89532, preemption_count=0, score=41633.583971, test/accuracy=0.523400, test/loss=2.139138, test/num_examples=10000, total_duration=46147.014680, train/accuracy=0.715879, train/loss=1.163267, validation/accuracy=0.641300, validation/loss=1.500296, validation/num_examples=50000
I0204 06:41:50.097164 139923868813056 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.2365000247955322, loss=4.9078168869018555
I0204 06:42:36.420227 139923852027648 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.2410457134246826, loss=3.305283784866333
I0204 06:43:23.732255 139923868813056 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.4596835374832153, loss=2.7563436031341553
I0204 06:44:10.736171 139923852027648 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.6138815879821777, loss=2.3659610748291016
I0204 06:44:57.540355 139923868813056 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.3750762939453125, loss=2.3165364265441895
I0204 06:45:44.620308 139923852027648 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.3033812046051025, loss=2.7986342906951904
I0204 06:46:31.479270 139923868813056 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.2856885194778442, loss=2.710836887359619
I0204 06:47:18.594230 139923852027648 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.6544904708862305, loss=2.4762988090515137
I0204 06:48:05.543431 139923868813056 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2437262535095215, loss=3.5741775035858154
I0204 06:48:22.050274 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:48:32.895320 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:49:07.517658 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:49:09.150151 140085747812160 submission_runner.py:408] Time since start: 46614.43s, 	Step: 90437, 	{'train/accuracy': 0.6942968368530273, 'train/loss': 1.242924690246582, 'validation/accuracy': 0.6450600028038025, 'validation/loss': 1.4841945171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.13555645942688, 'test/num_examples': 10000, 'score': 42053.79886484146, 'total_duration': 46614.43021249771, 'accumulated_submission_time': 42053.79886484146, 'accumulated_eval_time': 4551.505216121674, 'accumulated_logging_time': 4.02495002746582}
I0204 06:49:09.182476 139923852027648 logging_writer.py:48] [90437] accumulated_eval_time=4551.505216, accumulated_logging_time=4.024950, accumulated_submission_time=42053.798865, global_step=90437, preemption_count=0, score=42053.798865, test/accuracy=0.522000, test/loss=2.135556, test/num_examples=10000, total_duration=46614.430212, train/accuracy=0.694297, train/loss=1.242925, validation/accuracy=0.645060, validation/loss=1.484195, validation/num_examples=50000
I0204 06:49:35.441891 139923868813056 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.487367033958435, loss=2.255520820617676
I0204 06:50:21.525147 139923852027648 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.2742908000946045, loss=3.878755569458008
I0204 06:51:08.293346 139923868813056 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.1317806243896484, loss=3.9929540157318115
I0204 06:51:55.207086 139923852027648 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.3983219861984253, loss=2.1759390830993652
I0204 06:52:42.146162 139923868813056 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.3796255588531494, loss=2.2272274494171143
I0204 06:53:28.866410 139923852027648 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.369317889213562, loss=3.3367059230804443
I0204 06:54:15.772089 139923868813056 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.1916213035583496, loss=3.50608491897583
I0204 06:55:02.726031 139923852027648 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.1623318195343018, loss=4.2085137367248535
I0204 06:55:49.641143 139923868813056 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.2744109630584717, loss=2.5457799434661865
I0204 06:56:09.579343 140085747812160 spec.py:321] Evaluating on the training split.
I0204 06:56:19.814991 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 06:56:54.546012 140085747812160 spec.py:349] Evaluating on the test split.
I0204 06:56:56.179101 140085747812160 submission_runner.py:408] Time since start: 47081.46s, 	Step: 91344, 	{'train/accuracy': 0.7076367139816284, 'train/loss': 1.2079362869262695, 'validation/accuracy': 0.644320011138916, 'validation/loss': 1.4906361103057861, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1351263523101807, 'test/num_examples': 10000, 'score': 42474.13457107544, 'total_duration': 47081.45916390419, 'accumulated_submission_time': 42474.13457107544, 'accumulated_eval_time': 4598.1049790382385, 'accumulated_logging_time': 4.066986322402954}
I0204 06:56:56.211611 139923852027648 logging_writer.py:48] [91344] accumulated_eval_time=4598.104979, accumulated_logging_time=4.066986, accumulated_submission_time=42474.134571, global_step=91344, preemption_count=0, score=42474.134571, test/accuracy=0.529000, test/loss=2.135126, test/num_examples=10000, total_duration=47081.459164, train/accuracy=0.707637, train/loss=1.207936, validation/accuracy=0.644320, validation/loss=1.490636, validation/num_examples=50000
I0204 06:57:19.611176 139923868813056 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.3788411617279053, loss=4.412656784057617
I0204 06:58:05.315219 139923852027648 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.2535853385925293, loss=4.162161827087402
I0204 06:58:52.613412 139923868813056 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.4138963222503662, loss=2.4105429649353027
I0204 06:59:39.573471 139923852027648 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.3860918283462524, loss=2.2919673919677734
I0204 07:00:28.255414 139923868813056 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.4288127422332764, loss=2.226349353790283
I0204 07:01:15.234139 139923852027648 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.2353649139404297, loss=3.8535590171813965
I0204 07:02:02.011755 139923868813056 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.6147942543029785, loss=2.2588424682617188
I0204 07:02:48.968976 139923852027648 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.4321008920669556, loss=2.6455936431884766
I0204 07:03:35.792510 139923868813056 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.2241020202636719, loss=4.632148265838623
I0204 07:03:56.538705 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:04:07.009800 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:04:43.905185 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:04:45.541615 140085747812160 submission_runner.py:408] Time since start: 47550.82s, 	Step: 92246, 	{'train/accuracy': 0.7279687523841858, 'train/loss': 1.1314303874969482, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.4939802885055542, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1604013442993164, 'test/num_examples': 10000, 'score': 42894.400421381, 'total_duration': 47550.821682453156, 'accumulated_submission_time': 42894.400421381, 'accumulated_eval_time': 4647.107894182205, 'accumulated_logging_time': 4.110424280166626}
I0204 07:04:45.575148 139923852027648 logging_writer.py:48] [92246] accumulated_eval_time=4647.107894, accumulated_logging_time=4.110424, accumulated_submission_time=42894.400421, global_step=92246, preemption_count=0, score=42894.400421, test/accuracy=0.522300, test/loss=2.160401, test/num_examples=10000, total_duration=47550.821682, train/accuracy=0.727969, train/loss=1.131430, validation/accuracy=0.643120, validation/loss=1.493980, validation/num_examples=50000
I0204 07:05:08.164795 139923868813056 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.5604445934295654, loss=2.1523425579071045
I0204 07:05:53.838778 139923852027648 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.4503798484802246, loss=2.544139862060547
I0204 07:06:40.687625 139923868813056 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.4366577863693237, loss=2.103774070739746
I0204 07:07:27.645706 139923852027648 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.3610011339187622, loss=2.3022947311401367
I0204 07:08:14.649630 139923868813056 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.1802401542663574, loss=4.5746917724609375
I0204 07:09:01.489305 139923852027648 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.4164162874221802, loss=2.705374002456665
I0204 07:09:48.351772 139923868813056 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.4129438400268555, loss=2.331878185272217
I0204 07:10:35.372085 139923852027648 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.3670530319213867, loss=4.874252796173096
I0204 07:11:22.273149 139923868813056 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.535542368888855, loss=2.264427423477173
I0204 07:11:45.836628 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:11:56.705277 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:12:32.254819 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:12:33.905200 140085747812160 submission_runner.py:408] Time since start: 48019.19s, 	Step: 93152, 	{'train/accuracy': 0.7049609422683716, 'train/loss': 1.1868587732315063, 'validation/accuracy': 0.6511799693107605, 'validation/loss': 1.4446868896484375, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.0929665565490723, 'test/num_examples': 10000, 'score': 43314.599668979645, 'total_duration': 48019.18526363373, 'accumulated_submission_time': 43314.599668979645, 'accumulated_eval_time': 4695.176491975784, 'accumulated_logging_time': 4.155153512954712}
I0204 07:12:33.939794 139923852027648 logging_writer.py:48] [93152] accumulated_eval_time=4695.176492, accumulated_logging_time=4.155154, accumulated_submission_time=43314.599669, global_step=93152, preemption_count=0, score=43314.599669, test/accuracy=0.529500, test/loss=2.092967, test/num_examples=10000, total_duration=48019.185264, train/accuracy=0.704961, train/loss=1.186859, validation/accuracy=0.651180, validation/loss=1.444687, validation/num_examples=50000
I0204 07:12:54.071702 139923868813056 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4892652034759521, loss=2.6504313945770264
I0204 07:13:39.470025 139923852027648 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.2507350444793701, loss=4.311888217926025
I0204 07:14:26.448052 139923868813056 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.515105128288269, loss=2.176356554031372
I0204 07:15:13.345284 139923852027648 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.4485416412353516, loss=2.227557420730591
I0204 07:16:00.289865 139923868813056 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.375434160232544, loss=3.1528420448303223
I0204 07:16:47.127735 139923852027648 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.4745972156524658, loss=2.2530064582824707
I0204 07:17:33.933510 139923868813056 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.1705490350723267, loss=4.3721208572387695
I0204 07:18:21.251290 139923852027648 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.298755407333374, loss=3.624795436859131
I0204 07:19:07.986818 139923868813056 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.5226088762283325, loss=2.166900157928467
I0204 07:19:33.932613 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:19:44.658951 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:20:20.226557 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:20:21.869309 140085747812160 submission_runner.py:408] Time since start: 48487.15s, 	Step: 94057, 	{'train/accuracy': 0.7050195336341858, 'train/loss': 1.1846143007278442, 'validation/accuracy': 0.6482399702072144, 'validation/loss': 1.4538512229919434, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.119861364364624, 'test/num_examples': 10000, 'score': 43734.53122782707, 'total_duration': 48487.1493666172, 'accumulated_submission_time': 43734.53122782707, 'accumulated_eval_time': 4743.113221168518, 'accumulated_logging_time': 4.200689315795898}
I0204 07:20:21.903015 139923852027648 logging_writer.py:48] [94057] accumulated_eval_time=4743.113221, accumulated_logging_time=4.200689, accumulated_submission_time=43734.531228, global_step=94057, preemption_count=0, score=43734.531228, test/accuracy=0.524200, test/loss=2.119861, test/num_examples=10000, total_duration=48487.149367, train/accuracy=0.705020, train/loss=1.184614, validation/accuracy=0.648240, validation/loss=1.453851, validation/num_examples=50000
I0204 07:20:39.956775 139923868813056 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.5163177251815796, loss=2.2691843509674072
I0204 07:21:25.150530 139923852027648 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.4646880626678467, loss=2.3083040714263916
I0204 07:22:12.167624 139923868813056 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.5115185976028442, loss=2.255868434906006
I0204 07:22:59.252075 139923852027648 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.5252443552017212, loss=2.9181911945343018
I0204 07:23:46.056639 139923868813056 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.665186882019043, loss=2.377850294113159
I0204 07:24:32.966683 139923852027648 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.4378091096878052, loss=2.2638871669769287
I0204 07:25:19.902793 139923868813056 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.1883472204208374, loss=4.646890640258789
I0204 07:26:07.070708 139923852027648 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4003959894180298, loss=2.261667251586914
I0204 07:26:53.910328 139923868813056 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.4124795198440552, loss=2.326592206954956
I0204 07:27:22.152258 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:27:33.052656 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:28:05.521518 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:28:07.163585 140085747812160 submission_runner.py:408] Time since start: 48952.44s, 	Step: 94962, 	{'train/accuracy': 0.7193359136581421, 'train/loss': 1.1572200059890747, 'validation/accuracy': 0.6502999663352966, 'validation/loss': 1.4794443845748901, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1172285079956055, 'test/num_examples': 10000, 'score': 44154.71737384796, 'total_duration': 48952.4436519146, 'accumulated_submission_time': 44154.71737384796, 'accumulated_eval_time': 4788.12454199791, 'accumulated_logging_time': 4.246379375457764}
I0204 07:28:07.198108 139923852027648 logging_writer.py:48] [94962] accumulated_eval_time=4788.124542, accumulated_logging_time=4.246379, accumulated_submission_time=44154.717374, global_step=94962, preemption_count=0, score=44154.717374, test/accuracy=0.527400, test/loss=2.117229, test/num_examples=10000, total_duration=48952.443652, train/accuracy=0.719336, train/loss=1.157220, validation/accuracy=0.650300, validation/loss=1.479444, validation/num_examples=50000
I0204 07:28:23.217333 139923868813056 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.4182820320129395, loss=2.3820948600769043
I0204 07:29:08.164381 139923852027648 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.3811537027359009, loss=2.072721004486084
I0204 07:29:55.058017 139923868813056 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.655005693435669, loss=2.408736228942871
I0204 07:30:42.123692 139923852027648 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.2587021589279175, loss=4.868654251098633
I0204 07:31:28.822278 139923868813056 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.1607248783111572, loss=4.144032955169678
I0204 07:32:15.596613 139923852027648 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.160422682762146, loss=3.1963326930999756
I0204 07:33:02.691217 139923868813056 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.543765664100647, loss=2.195963144302368
I0204 07:33:49.540394 139923852027648 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3844376802444458, loss=2.1204757690429688
I0204 07:34:36.511035 139923868813056 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.4629205465316772, loss=2.6001603603363037
I0204 07:35:07.546597 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:35:18.118556 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:35:54.768827 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:35:56.419852 140085747812160 submission_runner.py:408] Time since start: 49421.70s, 	Step: 95868, 	{'train/accuracy': 0.7100390195846558, 'train/loss': 1.2094417810440063, 'validation/accuracy': 0.6509999632835388, 'validation/loss': 1.471266269683838, 'validation/num_examples': 50000, 'test/accuracy': 0.531000018119812, 'test/loss': 2.119685411453247, 'test/num_examples': 10000, 'score': 44575.002772808075, 'total_duration': 49421.69991064072, 'accumulated_submission_time': 44575.002772808075, 'accumulated_eval_time': 4836.9977996349335, 'accumulated_logging_time': 4.29305100440979}
I0204 07:35:56.454362 139923852027648 logging_writer.py:48] [95868] accumulated_eval_time=4836.997800, accumulated_logging_time=4.293051, accumulated_submission_time=44575.002773, global_step=95868, preemption_count=0, score=44575.002773, test/accuracy=0.531000, test/loss=2.119685, test/num_examples=10000, total_duration=49421.699911, train/accuracy=0.710039, train/loss=1.209442, validation/accuracy=0.651000, validation/loss=1.471266, validation/num_examples=50000
I0204 07:36:09.999456 139923868813056 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.3697795867919922, loss=4.713035583496094
I0204 07:36:54.189929 139923852027648 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.1961007118225098, loss=4.090169906616211
I0204 07:37:41.053110 139923868813056 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.5228363275527954, loss=2.144160509109497
I0204 07:38:27.867522 139923852027648 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.3721240758895874, loss=2.6106529235839844
I0204 07:39:15.044752 139923868813056 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.5043728351593018, loss=2.3944954872131348
I0204 07:40:02.176365 139923852027648 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3684669733047485, loss=4.6899566650390625
I0204 07:40:49.239683 139923868813056 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.4442392587661743, loss=2.177197217941284
I0204 07:41:36.098298 139923852027648 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.2528117895126343, loss=3.84340763092041
I0204 07:42:22.887147 139923868813056 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.5244771242141724, loss=2.274660110473633
I0204 07:42:56.965353 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:43:07.485622 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:43:44.306527 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:43:45.958488 140085747812160 submission_runner.py:408] Time since start: 49891.24s, 	Step: 96774, 	{'train/accuracy': 0.7082812190055847, 'train/loss': 1.2541940212249756, 'validation/accuracy': 0.6438999772071838, 'validation/loss': 1.5317820310592651, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.174257516860962, 'test/num_examples': 10000, 'score': 44995.451451301575, 'total_duration': 49891.23855257034, 'accumulated_submission_time': 44995.451451301575, 'accumulated_eval_time': 4885.990926504135, 'accumulated_logging_time': 4.338782787322998}
I0204 07:43:45.990580 139923852027648 logging_writer.py:48] [96774] accumulated_eval_time=4885.990927, accumulated_logging_time=4.338783, accumulated_submission_time=44995.451451, global_step=96774, preemption_count=0, score=44995.451451, test/accuracy=0.525600, test/loss=2.174258, test/num_examples=10000, total_duration=49891.238553, train/accuracy=0.708281, train/loss=1.254194, validation/accuracy=0.643900, validation/loss=1.531782, validation/num_examples=50000
I0204 07:43:57.078852 139923868813056 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.354027509689331, loss=3.7613067626953125
I0204 07:44:41.128455 139923852027648 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.603184700012207, loss=2.2531251907348633
I0204 07:45:28.021586 139923868813056 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.4328768253326416, loss=2.2518672943115234
I0204 07:46:14.760855 139923852027648 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.252548098564148, loss=3.3872125148773193
I0204 07:47:01.672016 139923868813056 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.2804992198944092, loss=3.9317171573638916
I0204 07:47:48.532163 139923852027648 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.6908634901046753, loss=2.2399916648864746
I0204 07:48:35.340979 139923868813056 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.512845516204834, loss=2.1463303565979004
I0204 07:49:22.068838 139923852027648 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.4420744180679321, loss=2.2583091259002686
I0204 07:50:09.166586 139923868813056 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.4328519105911255, loss=2.2799863815307617
I0204 07:50:46.173730 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:50:56.611360 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:51:32.265214 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:51:33.909041 140085747812160 submission_runner.py:408] Time since start: 50359.19s, 	Step: 97681, 	{'train/accuracy': 0.7260546684265137, 'train/loss': 1.110514760017395, 'validation/accuracy': 0.6534599661827087, 'validation/loss': 1.432780385017395, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.0747039318084717, 'test/num_examples': 10000, 'score': 45415.572922706604, 'total_duration': 50359.1891078949, 'accumulated_submission_time': 45415.572922706604, 'accumulated_eval_time': 4933.7262399196625, 'accumulated_logging_time': 4.382209062576294}
I0204 07:51:33.944007 139923852027648 logging_writer.py:48] [97681] accumulated_eval_time=4933.726240, accumulated_logging_time=4.382209, accumulated_submission_time=45415.572923, global_step=97681, preemption_count=0, score=45415.572923, test/accuracy=0.531300, test/loss=2.074704, test/num_examples=10000, total_duration=50359.189108, train/accuracy=0.726055, train/loss=1.110515, validation/accuracy=0.653460, validation/loss=1.432780, validation/num_examples=50000
I0204 07:51:42.171510 139923868813056 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.4114171266555786, loss=2.2183475494384766
I0204 07:52:26.241364 139923852027648 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.5695910453796387, loss=2.242610454559326
I0204 07:53:13.492550 139923868813056 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.4403777122497559, loss=2.268855094909668
I0204 07:54:00.380573 139923852027648 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.4231611490249634, loss=2.1375184059143066
I0204 07:54:47.286793 139923868813056 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.4560409784317017, loss=2.3237242698669434
I0204 07:55:34.235232 139923852027648 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.2534568309783936, loss=4.129848480224609
I0204 07:56:21.455524 139923868813056 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.3909087181091309, loss=2.6142029762268066
I0204 07:57:08.197661 139923852027648 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.4256343841552734, loss=2.171687364578247
I0204 07:57:55.185416 139923868813056 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6185506582260132, loss=2.092043876647949
I0204 07:58:34.045620 140085747812160 spec.py:321] Evaluating on the training split.
I0204 07:58:44.554547 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 07:59:16.478329 140085747812160 spec.py:349] Evaluating on the test split.
I0204 07:59:18.123713 140085747812160 submission_runner.py:408] Time since start: 50823.40s, 	Step: 98584, 	{'train/accuracy': 0.7095507383346558, 'train/loss': 1.2087146043777466, 'validation/accuracy': 0.6518399715423584, 'validation/loss': 1.4704029560089111, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.124727725982666, 'test/num_examples': 10000, 'score': 45835.61195087433, 'total_duration': 50823.40377354622, 'accumulated_submission_time': 45835.61195087433, 'accumulated_eval_time': 4977.804324626923, 'accumulated_logging_time': 4.428659439086914}
I0204 07:59:18.157726 139923852027648 logging_writer.py:48] [98584] accumulated_eval_time=4977.804325, accumulated_logging_time=4.428659, accumulated_submission_time=45835.611951, global_step=98584, preemption_count=0, score=45835.611951, test/accuracy=0.531300, test/loss=2.124728, test/num_examples=10000, total_duration=50823.403774, train/accuracy=0.709551, train/loss=1.208715, validation/accuracy=0.651840, validation/loss=1.470403, validation/num_examples=50000
I0204 07:59:25.180889 139923868813056 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.280852198600769, loss=4.697167873382568
I0204 08:00:08.776508 139923852027648 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.5874648094177246, loss=2.1125309467315674
I0204 08:00:55.613435 139923868813056 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.4840812683105469, loss=2.222193717956543
I0204 08:01:42.608084 139923852027648 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.6121426820755005, loss=2.2333734035491943
I0204 08:02:29.756963 139923868813056 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.5781346559524536, loss=2.1864447593688965
I0204 08:03:16.984470 139923852027648 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.2306740283966064, loss=3.8085641860961914
I0204 08:04:04.131663 139923868813056 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.2184903621673584, loss=3.449765920639038
I0204 08:04:51.109901 139923852027648 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.4639644622802734, loss=2.156688690185547
I0204 08:05:38.128304 139923868813056 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.4535815715789795, loss=3.1090922355651855
I0204 08:06:18.398997 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:06:28.639702 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:07:04.047591 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:07:05.687779 140085747812160 submission_runner.py:408] Time since start: 51290.97s, 	Step: 99487, 	{'train/accuracy': 0.7159179449081421, 'train/loss': 1.143684983253479, 'validation/accuracy': 0.660539984703064, 'validation/loss': 1.4055538177490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.0649092197418213, 'test/num_examples': 10000, 'score': 46255.788565158844, 'total_duration': 51290.96784090996, 'accumulated_submission_time': 46255.788565158844, 'accumulated_eval_time': 5025.0931096076965, 'accumulated_logging_time': 4.4764626026153564}
I0204 08:07:05.724090 139923852027648 logging_writer.py:48] [99487] accumulated_eval_time=5025.093110, accumulated_logging_time=4.476463, accumulated_submission_time=46255.788565, global_step=99487, preemption_count=0, score=46255.788565, test/accuracy=0.533300, test/loss=2.064909, test/num_examples=10000, total_duration=51290.967841, train/accuracy=0.715918, train/loss=1.143685, validation/accuracy=0.660540, validation/loss=1.405554, validation/num_examples=50000
I0204 08:07:11.467826 139923868813056 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.271440029144287, loss=4.802410125732422
I0204 08:07:54.816219 139923852027648 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.5078206062316895, loss=2.2905328273773193
I0204 08:08:41.504910 139923868813056 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.3550758361816406, loss=2.6889524459838867
I0204 08:09:28.501775 139923852027648 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.5510553121566772, loss=2.435023307800293
I0204 08:10:15.446130 139923868813056 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.5858738422393799, loss=2.2992453575134277
I0204 08:11:02.125244 139923852027648 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.68672513961792, loss=2.1836578845977783
I0204 08:11:49.236704 139923868813056 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.618911862373352, loss=2.3125622272491455
I0204 08:12:36.078021 139923852027648 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.2592248916625977, loss=3.428504705429077
I0204 08:13:22.839687 139923868813056 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.6392087936401367, loss=2.2900333404541016
I0204 08:14:06.098170 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:14:16.203404 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:14:53.002228 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:14:54.644709 140085747812160 submission_runner.py:408] Time since start: 51759.92s, 	Step: 100394, 	{'train/accuracy': 0.7255663871765137, 'train/loss': 1.114113211631775, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.4175740480422974, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.0686087608337402, 'test/num_examples': 10000, 'score': 46676.1024954319, 'total_duration': 51759.92477321625, 'accumulated_submission_time': 46676.1024954319, 'accumulated_eval_time': 5073.639664173126, 'accumulated_logging_time': 4.521659851074219}
I0204 08:14:54.679857 139923852027648 logging_writer.py:48] [100394] accumulated_eval_time=5073.639664, accumulated_logging_time=4.521660, accumulated_submission_time=46676.102495, global_step=100394, preemption_count=0, score=46676.102495, test/accuracy=0.533100, test/loss=2.068609, test/num_examples=10000, total_duration=51759.924773, train/accuracy=0.725566, train/loss=1.114113, validation/accuracy=0.662360, validation/loss=1.417574, validation/num_examples=50000
I0204 08:14:57.559308 139923868813056 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.5163856744766235, loss=2.2438347339630127
I0204 08:15:40.324557 139923852027648 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.2928961515426636, loss=3.982487678527832
I0204 08:16:27.056354 139923868813056 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.352112054824829, loss=3.2840895652770996
I0204 08:17:13.674731 139923852027648 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.364516258239746, loss=3.2523045539855957
I0204 08:18:00.372387 139923868813056 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.3772192001342773, loss=3.33870530128479
I0204 08:18:47.246145 139923852027648 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.3838049173355103, loss=3.700446605682373
I0204 08:19:34.066644 139923868813056 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.5959450006484985, loss=2.2326133251190186
I0204 08:20:20.982266 139923852027648 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.5163651704788208, loss=2.2905116081237793
I0204 08:21:07.716350 139923868813056 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.5662834644317627, loss=2.1159250736236572
I0204 08:21:54.385902 139923852027648 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.5570868253707886, loss=2.5978517532348633
I0204 08:21:55.038969 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:22:05.506173 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:22:40.363876 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:22:42.009256 140085747812160 submission_runner.py:408] Time since start: 52227.29s, 	Step: 101303, 	{'train/accuracy': 0.7194140553474426, 'train/loss': 1.1470216512680054, 'validation/accuracy': 0.6613399982452393, 'validation/loss': 1.3947124481201172, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.0668838024139404, 'test/num_examples': 10000, 'score': 47096.401584386826, 'total_duration': 52227.28931951523, 'accumulated_submission_time': 47096.401584386826, 'accumulated_eval_time': 5120.609957456589, 'accumulated_logging_time': 4.565702676773071}
I0204 08:22:42.043279 139923868813056 logging_writer.py:48] [101303] accumulated_eval_time=5120.609957, accumulated_logging_time=4.565703, accumulated_submission_time=47096.401584, global_step=101303, preemption_count=0, score=47096.401584, test/accuracy=0.537200, test/loss=2.066884, test/num_examples=10000, total_duration=52227.289320, train/accuracy=0.719414, train/loss=1.147022, validation/accuracy=0.661340, validation/loss=1.394712, validation/num_examples=50000
I0204 08:23:23.471230 139923852027648 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.5081318616867065, loss=2.0592572689056396
I0204 08:24:10.054796 139923868813056 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.6398115158081055, loss=2.529466152191162
I0204 08:24:56.969151 139923852027648 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.559086561203003, loss=2.5089991092681885
I0204 08:25:43.730262 139923868813056 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.354265809059143, loss=2.873566150665283
I0204 08:26:30.440206 139923852027648 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.3630133867263794, loss=4.7531514167785645
I0204 08:27:17.371466 139923868813056 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.4177459478378296, loss=4.4657182693481445
I0204 08:28:04.312099 139923852027648 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.4660108089447021, loss=2.444016933441162
I0204 08:28:51.132410 139923868813056 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.5177900791168213, loss=2.1934080123901367
I0204 08:29:38.060637 139923852027648 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4594882726669312, loss=2.3200645446777344
I0204 08:29:42.395052 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:29:52.837008 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:30:26.095779 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:30:27.734855 140085747812160 submission_runner.py:408] Time since start: 52693.01s, 	Step: 102211, 	{'train/accuracy': 0.7141796946525574, 'train/loss': 1.1836493015289307, 'validation/accuracy': 0.6551799774169922, 'validation/loss': 1.4438341856002808, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.0949273109436035, 'test/num_examples': 10000, 'score': 47516.692006111145, 'total_duration': 52693.0149166584, 'accumulated_submission_time': 47516.692006111145, 'accumulated_eval_time': 5165.949766874313, 'accumulated_logging_time': 4.6092352867126465}
I0204 08:30:27.767603 139923868813056 logging_writer.py:48] [102211] accumulated_eval_time=5165.949767, accumulated_logging_time=4.609235, accumulated_submission_time=47516.692006, global_step=102211, preemption_count=0, score=47516.692006, test/accuracy=0.534400, test/loss=2.094927, test/num_examples=10000, total_duration=52693.014917, train/accuracy=0.714180, train/loss=1.183649, validation/accuracy=0.655180, validation/loss=1.443834, validation/num_examples=50000
I0204 08:31:05.704549 139923852027648 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.5099185705184937, loss=2.2441534996032715
I0204 08:31:52.358347 139923868813056 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.9999172687530518, loss=2.1054012775421143
I0204 08:32:39.731448 139923852027648 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.356481671333313, loss=4.606976509094238
I0204 08:33:26.763062 139923868813056 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.4719600677490234, loss=2.141453266143799
I0204 08:34:13.870272 139923852027648 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.3353325128555298, loss=3.932838201522827
I0204 08:35:00.887802 139923868813056 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.4933449029922485, loss=4.679980278015137
I0204 08:35:47.856462 139923852027648 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.262463092803955, loss=3.1458399295806885
I0204 08:36:34.765044 139923868813056 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.394581913948059, loss=4.709988594055176
I0204 08:37:21.936248 139923852027648 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.615706205368042, loss=2.256441116333008
I0204 08:37:28.157751 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:37:38.878471 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:38:14.903829 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:38:16.550471 140085747812160 submission_runner.py:408] Time since start: 53161.83s, 	Step: 103115, 	{'train/accuracy': 0.7273827791213989, 'train/loss': 1.1171965599060059, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.4077105522155762, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.0607264041900635, 'test/num_examples': 10000, 'score': 47937.02245235443, 'total_duration': 53161.830530405045, 'accumulated_submission_time': 47937.02245235443, 'accumulated_eval_time': 5214.342481613159, 'accumulated_logging_time': 4.651084899902344}
I0204 08:38:16.582557 139923868813056 logging_writer.py:48] [103115] accumulated_eval_time=5214.342482, accumulated_logging_time=4.651085, accumulated_submission_time=47937.022452, global_step=103115, preemption_count=0, score=47937.022452, test/accuracy=0.538400, test/loss=2.060726, test/num_examples=10000, total_duration=53161.830530, train/accuracy=0.727383, train/loss=1.117197, validation/accuracy=0.662120, validation/loss=1.407711, validation/num_examples=50000
I0204 08:38:52.834064 139923852027648 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.2848304510116577, loss=3.1701409816741943
I0204 08:39:39.691386 139923868813056 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.4656929969787598, loss=1.9701019525527954
I0204 08:40:26.964778 139923852027648 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.4565588235855103, loss=2.050346851348877
I0204 08:41:13.736668 139923868813056 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.5460492372512817, loss=2.2502281665802
I0204 08:42:01.025274 139923852027648 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.4254884719848633, loss=3.481451988220215
I0204 08:42:47.978274 139923868813056 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.4407624006271362, loss=2.0257468223571777
I0204 08:43:34.911556 139923852027648 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.6322455406188965, loss=2.1742441654205322
I0204 08:44:22.010985 139923868813056 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.5120735168457031, loss=2.234570026397705
I0204 08:45:09.116093 139923852027648 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.2971161603927612, loss=4.268691062927246
I0204 08:45:16.789877 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:45:27.379399 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:46:00.155995 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:46:01.793299 140085747812160 submission_runner.py:408] Time since start: 53627.07s, 	Step: 104018, 	{'train/accuracy': 0.7233007550239563, 'train/loss': 1.127516508102417, 'validation/accuracy': 0.6676799654960632, 'validation/loss': 1.3816344738006592, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.032897472381592, 'test/num_examples': 10000, 'score': 48357.16518211365, 'total_duration': 53627.07335996628, 'accumulated_submission_time': 48357.16518211365, 'accumulated_eval_time': 5259.345903396606, 'accumulated_logging_time': 4.697351932525635}
I0204 08:46:01.827293 139923868813056 logging_writer.py:48] [104018] accumulated_eval_time=5259.345903, accumulated_logging_time=4.697352, accumulated_submission_time=48357.165182, global_step=104018, preemption_count=0, score=48357.165182, test/accuracy=0.541800, test/loss=2.032897, test/num_examples=10000, total_duration=53627.073360, train/accuracy=0.723301, train/loss=1.127517, validation/accuracy=0.667680, validation/loss=1.381634, validation/num_examples=50000
I0204 08:46:36.195032 139923852027648 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.6316043138504028, loss=2.2066357135772705
I0204 08:47:22.693660 139923868813056 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.4884073734283447, loss=2.4605987071990967
I0204 08:48:09.977094 139923852027648 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.7322056293487549, loss=2.1890053749084473
I0204 08:48:56.856193 139923868813056 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.8172733783721924, loss=4.2672600746154785
I0204 08:49:43.805185 139923852027648 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.576374888420105, loss=2.2126262187957764
I0204 08:50:30.754209 139923868813056 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.475809097290039, loss=2.4252562522888184
I0204 08:51:17.761729 139923852027648 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.545039176940918, loss=2.126080274581909
I0204 08:52:04.639616 139923868813056 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.313130497932434, loss=3.0243191719055176
I0204 08:52:51.912256 139923852027648 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.5469797849655151, loss=2.1025049686431885
I0204 08:53:02.023082 140085747812160 spec.py:321] Evaluating on the training split.
I0204 08:53:12.573518 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 08:53:47.300551 140085747812160 spec.py:349] Evaluating on the test split.
I0204 08:53:48.945068 140085747812160 submission_runner.py:408] Time since start: 54094.23s, 	Step: 104923, 	{'train/accuracy': 0.7226171493530273, 'train/loss': 1.2302206754684448, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.492753267288208, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.138030767440796, 'test/num_examples': 10000, 'score': 48777.29816532135, 'total_duration': 54094.22512769699, 'accumulated_submission_time': 48777.29816532135, 'accumulated_eval_time': 5306.26788520813, 'accumulated_logging_time': 4.742557764053345}
I0204 08:53:48.983227 139923868813056 logging_writer.py:48] [104923] accumulated_eval_time=5306.267885, accumulated_logging_time=4.742558, accumulated_submission_time=48777.298165, global_step=104923, preemption_count=0, score=48777.298165, test/accuracy=0.538100, test/loss=2.138031, test/num_examples=10000, total_duration=54094.225128, train/accuracy=0.722617, train/loss=1.230221, validation/accuracy=0.663360, validation/loss=1.492753, validation/num_examples=50000
I0204 08:54:21.203476 139923852027648 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.6066869497299194, loss=2.129081964492798
I0204 08:55:08.138236 139923868813056 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.534266710281372, loss=2.22059965133667
I0204 08:55:55.020212 139923852027648 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.2610331773757935, loss=4.341554641723633
I0204 08:56:42.038112 139923868813056 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.3398933410644531, loss=2.5768113136291504
I0204 08:57:28.866814 139923852027648 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.47701096534729, loss=2.717667579650879
I0204 08:58:15.828972 139923868813056 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.491729974746704, loss=2.0729172229766846
I0204 08:59:02.739887 139923852027648 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.6943978071212769, loss=2.0976552963256836
I0204 08:59:49.754696 139923868813056 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.3791453838348389, loss=3.423144817352295
I0204 09:00:36.661453 139923852027648 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.5469861030578613, loss=2.221681833267212
I0204 09:00:48.958067 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:00:59.580625 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:01:32.648791 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:01:34.287827 140085747812160 submission_runner.py:408] Time since start: 54559.57s, 	Step: 105828, 	{'train/accuracy': 0.7344140410423279, 'train/loss': 1.0741634368896484, 'validation/accuracy': 0.6691799759864807, 'validation/loss': 1.373185396194458, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 2.021711826324463, 'test/num_examples': 10000, 'score': 49197.20958852768, 'total_duration': 54559.56789302826, 'accumulated_submission_time': 49197.20958852768, 'accumulated_eval_time': 5351.597653627396, 'accumulated_logging_time': 4.792775630950928}
I0204 09:01:34.324015 139923868813056 logging_writer.py:48] [105828] accumulated_eval_time=5351.597654, accumulated_logging_time=4.792776, accumulated_submission_time=49197.209589, global_step=105828, preemption_count=0, score=49197.209589, test/accuracy=0.549200, test/loss=2.021712, test/num_examples=10000, total_duration=54559.567893, train/accuracy=0.734414, train/loss=1.074163, validation/accuracy=0.669180, validation/loss=1.373185, validation/num_examples=50000
I0204 09:02:04.326538 139923852027648 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.351854920387268, loss=3.9738893508911133
I0204 09:02:50.802222 139923868813056 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.4582504034042358, loss=4.124207019805908
I0204 09:03:37.959109 139923852027648 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.5904512405395508, loss=2.2191176414489746
I0204 09:04:24.631524 139923868813056 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.4766712188720703, loss=2.41235613822937
I0204 09:05:11.485139 139923852027648 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.4572291374206543, loss=4.7131829261779785
I0204 09:05:58.846750 139923868813056 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.6745051145553589, loss=2.2273850440979004
I0204 09:06:45.830029 139923852027648 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.697096824645996, loss=2.6820743083953857
I0204 09:07:32.806589 139923868813056 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.5803327560424805, loss=2.1984453201293945
I0204 09:08:19.773261 139923852027648 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.8129041194915771, loss=2.1233294010162354
I0204 09:08:34.464975 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:08:44.965574 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:09:21.906929 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:09:23.550626 140085747812160 submission_runner.py:408] Time since start: 55028.83s, 	Step: 106733, 	{'train/accuracy': 0.7335546612739563, 'train/loss': 1.0971087217330933, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.4038810729980469, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.057142734527588, 'test/num_examples': 10000, 'score': 49617.28916144371, 'total_duration': 55028.830676317215, 'accumulated_submission_time': 49617.28916144371, 'accumulated_eval_time': 5400.683298826218, 'accumulated_logging_time': 4.839393138885498}
I0204 09:09:23.586633 139923868813056 logging_writer.py:48] [106733] accumulated_eval_time=5400.683299, accumulated_logging_time=4.839393, accumulated_submission_time=49617.289161, global_step=106733, preemption_count=0, score=49617.289161, test/accuracy=0.544500, test/loss=2.057143, test/num_examples=10000, total_duration=55028.830676, train/accuracy=0.733555, train/loss=1.097109, validation/accuracy=0.665760, validation/loss=1.403881, validation/num_examples=50000
I0204 09:09:51.504876 139923852027648 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.5993239879608154, loss=2.3119940757751465
I0204 09:10:37.728316 139923868813056 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.5868281126022339, loss=3.2269134521484375
I0204 09:11:24.974845 139923852027648 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.4834961891174316, loss=2.5897326469421387
I0204 09:12:11.848170 139923868813056 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.6629546880722046, loss=2.213200569152832
I0204 09:12:59.080751 139923852027648 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.591628074645996, loss=2.1182241439819336
I0204 09:13:45.856029 139923868813056 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.6650948524475098, loss=2.2617056369781494
I0204 09:14:32.747796 139923852027648 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.4578126668930054, loss=2.5109119415283203
I0204 09:15:19.892702 139923868813056 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.5753109455108643, loss=2.19144606590271
I0204 09:16:07.087343 139923852027648 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.5304864645004272, loss=2.0577893257141113
I0204 09:16:24.051306 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:16:34.591263 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:17:05.949336 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:17:07.582270 140085747812160 submission_runner.py:408] Time since start: 55492.86s, 	Step: 107638, 	{'train/accuracy': 0.7317773103713989, 'train/loss': 1.0785207748413086, 'validation/accuracy': 0.6732400059700012, 'validation/loss': 1.3483539819717407, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 1.9994500875473022, 'test/num_examples': 10000, 'score': 50037.69368457794, 'total_duration': 55492.862330675125, 'accumulated_submission_time': 50037.69368457794, 'accumulated_eval_time': 5444.214246273041, 'accumulated_logging_time': 4.885617017745972}
I0204 09:17:07.615645 139923868813056 logging_writer.py:48] [107638] accumulated_eval_time=5444.214246, accumulated_logging_time=4.885617, accumulated_submission_time=50037.693685, global_step=107638, preemption_count=0, score=50037.693685, test/accuracy=0.549200, test/loss=1.999450, test/num_examples=10000, total_duration=55492.862331, train/accuracy=0.731777, train/loss=1.078521, validation/accuracy=0.673240, validation/loss=1.348354, validation/num_examples=50000
I0204 09:17:33.475795 139923852027648 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.6855720281600952, loss=2.2709951400756836
I0204 09:18:19.598486 139923868813056 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.644771695137024, loss=2.137068271636963
I0204 09:19:06.870831 139923852027648 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.3365230560302734, loss=3.4661338329315186
I0204 09:19:53.943901 139923868813056 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.7185875177383423, loss=2.1076154708862305
I0204 09:20:40.980227 139923852027648 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.5883193016052246, loss=2.7476396560668945
I0204 09:21:28.025675 139923868813056 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.3740025758743286, loss=4.670665264129639
I0204 09:22:14.955695 139923852027648 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.6617411375045776, loss=2.0819711685180664
I0204 09:23:02.076584 139923868813056 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.8809778690338135, loss=2.157006025314331
I0204 09:23:49.041040 139923852027648 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.3627874851226807, loss=4.457324028015137
I0204 09:24:08.030141 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:24:18.650260 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:24:54.226450 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:24:55.871407 140085747812160 submission_runner.py:408] Time since start: 55961.15s, 	Step: 108542, 	{'train/accuracy': 0.7343358993530273, 'train/loss': 1.076116919517517, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.35907781124115, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.020024299621582, 'test/num_examples': 10000, 'score': 50458.04724955559, 'total_duration': 55961.151431560516, 'accumulated_submission_time': 50458.04724955559, 'accumulated_eval_time': 5492.0554802417755, 'accumulated_logging_time': 4.9278857707977295}
I0204 09:24:55.909278 139923868813056 logging_writer.py:48] [108542] accumulated_eval_time=5492.055480, accumulated_logging_time=4.927886, accumulated_submission_time=50458.047250, global_step=108542, preemption_count=0, score=50458.047250, test/accuracy=0.552000, test/loss=2.020024, test/num_examples=10000, total_duration=55961.151432, train/accuracy=0.734336, train/loss=1.076117, validation/accuracy=0.670260, validation/loss=1.359078, validation/num_examples=50000
I0204 09:25:20.403360 139923852027648 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.487294316291809, loss=3.4618492126464844
I0204 09:26:06.414500 139923868813056 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.716110348701477, loss=2.0947484970092773
I0204 09:26:53.716601 139923852027648 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.5243879556655884, loss=2.0794246196746826
I0204 09:27:40.785559 139923868813056 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.6573737859725952, loss=2.169231414794922
I0204 09:28:27.792021 139923852027648 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.4933353662490845, loss=2.405029773712158
I0204 09:29:14.922878 139923868813056 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.5963616371154785, loss=2.3065757751464844
I0204 09:30:01.826839 139923852027648 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.5679086446762085, loss=2.5627105236053467
I0204 09:30:48.861949 139923868813056 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.4879052639007568, loss=3.320497512817383
I0204 09:31:35.800045 139923852027648 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.8016180992126465, loss=2.1001791954040527
I0204 09:31:55.878633 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:32:06.552125 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:32:41.319463 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:32:42.959806 140085747812160 submission_runner.py:408] Time since start: 56428.24s, 	Step: 109444, 	{'train/accuracy': 0.7549999952316284, 'train/loss': 1.0015008449554443, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.374889612197876, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0300540924072266, 'test/num_examples': 10000, 'score': 50877.9556055069, 'total_duration': 56428.23986721039, 'accumulated_submission_time': 50877.9556055069, 'accumulated_eval_time': 5539.1366448402405, 'accumulated_logging_time': 4.976301908493042}
I0204 09:32:42.996282 139923868813056 logging_writer.py:48] [109444] accumulated_eval_time=5539.136645, accumulated_logging_time=4.976302, accumulated_submission_time=50877.955606, global_step=109444, preemption_count=0, score=50877.955606, test/accuracy=0.546600, test/loss=2.030054, test/num_examples=10000, total_duration=56428.239867, train/accuracy=0.755000, train/loss=1.001501, validation/accuracy=0.670920, validation/loss=1.374890, validation/num_examples=50000
I0204 09:33:06.411225 139923852027648 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.540439486503601, loss=3.572152853012085
I0204 09:33:52.010612 139923868813056 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.7085734605789185, loss=2.132964849472046
I0204 09:34:38.902376 139923852027648 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.275041103363037, loss=2.94563627243042
I0204 09:35:25.827109 139923868813056 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.5360318422317505, loss=2.0121512413024902
I0204 09:36:12.669563 139923852027648 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.7739765644073486, loss=2.164423704147339
I0204 09:36:59.370476 139923868813056 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.5034339427947998, loss=2.9152050018310547
I0204 09:37:46.245022 139923852027648 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.6326251029968262, loss=4.1438212394714355
I0204 09:38:33.112063 139923868813056 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.6961398124694824, loss=2.0801327228546143
I0204 09:39:19.723532 139923852027648 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.458240270614624, loss=4.289194107055664
I0204 09:39:43.245782 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:39:53.962044 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:40:28.733251 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:40:30.373992 140085747812160 submission_runner.py:408] Time since start: 56895.65s, 	Step: 110352, 	{'train/accuracy': 0.7322655916213989, 'train/loss': 1.0748839378356934, 'validation/accuracy': 0.6771999597549438, 'validation/loss': 1.3334211111068726, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 1.9860961437225342, 'test/num_examples': 10000, 'score': 51298.14277744293, 'total_duration': 56895.654056310654, 'accumulated_submission_time': 51298.14277744293, 'accumulated_eval_time': 5586.264869213104, 'accumulated_logging_time': 5.022613763809204}
I0204 09:40:30.411237 139923868813056 logging_writer.py:48] [110352] accumulated_eval_time=5586.264869, accumulated_logging_time=5.022614, accumulated_submission_time=51298.142777, global_step=110352, preemption_count=0, score=51298.142777, test/accuracy=0.546300, test/loss=1.986096, test/num_examples=10000, total_duration=56895.654056, train/accuracy=0.732266, train/loss=1.074884, validation/accuracy=0.677200, validation/loss=1.333421, validation/num_examples=50000
I0204 09:40:50.519025 139923852027648 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.3774551153182983, loss=4.31142520904541
I0204 09:41:35.743953 139923868813056 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.5213661193847656, loss=3.906841278076172
I0204 09:42:22.841351 139923852027648 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.619951844215393, loss=1.965844750404358
I0204 09:43:09.933155 139923868813056 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.7804025411605835, loss=2.068782091140747
I0204 09:43:56.688086 139923852027648 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.6896648406982422, loss=1.9765936136245728
I0204 09:44:43.431540 139923868813056 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.6526721715927124, loss=2.433955669403076
I0204 09:45:30.297375 139923852027648 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.5638389587402344, loss=2.2745041847229004
I0204 09:46:17.117278 139923868813056 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.5074371099472046, loss=4.663112640380859
I0204 09:47:04.176671 139923852027648 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.5265005826950073, loss=4.584397792816162
I0204 09:47:30.569778 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:47:41.361579 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:48:17.287980 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:48:18.920418 140085747812160 submission_runner.py:408] Time since start: 57364.20s, 	Step: 111258, 	{'train/accuracy': 0.7407421469688416, 'train/loss': 1.0589429140090942, 'validation/accuracy': 0.6723399758338928, 'validation/loss': 1.357522964477539, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.025487184524536, 'test/num_examples': 10000, 'score': 51718.239069223404, 'total_duration': 57364.200476169586, 'accumulated_submission_time': 51718.239069223404, 'accumulated_eval_time': 5634.615519762039, 'accumulated_logging_time': 5.071029901504517}
I0204 09:48:18.955990 139923868813056 logging_writer.py:48] [111258] accumulated_eval_time=5634.615520, accumulated_logging_time=5.071030, accumulated_submission_time=51718.239069, global_step=111258, preemption_count=0, score=51718.239069, test/accuracy=0.545800, test/loss=2.025487, test/num_examples=10000, total_duration=57364.200476, train/accuracy=0.740742, train/loss=1.058943, validation/accuracy=0.672340, validation/loss=1.357523, validation/num_examples=50000
I0204 09:48:36.619567 139923852027648 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.454260230064392, loss=2.6645257472991943
I0204 09:49:21.650238 139923868813056 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.3891862630844116, loss=3.1109015941619873
I0204 09:50:08.271517 139923852027648 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.6987711191177368, loss=2.073399543762207
I0204 09:50:55.119760 139923868813056 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.6020324230194092, loss=3.4529225826263428
I0204 09:51:42.135043 139923852027648 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.6010363101959229, loss=2.349531888961792
I0204 09:52:28.910065 139923868813056 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.4458709955215454, loss=4.727163791656494
I0204 09:53:16.135846 139923852027648 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.5664268732070923, loss=3.3872597217559814
I0204 09:54:03.025526 139923868813056 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.687331199645996, loss=2.1337814331054688
I0204 09:54:49.813297 139923852027648 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.517637014389038, loss=3.547860860824585
I0204 09:55:18.976847 140085747812160 spec.py:321] Evaluating on the training split.
I0204 09:55:29.414293 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 09:56:05.399663 140085747812160 spec.py:349] Evaluating on the test split.
I0204 09:56:07.041480 140085747812160 submission_runner.py:408] Time since start: 57832.32s, 	Step: 112164, 	{'train/accuracy': 0.7571874856948853, 'train/loss': 0.9590351581573486, 'validation/accuracy': 0.6782799959182739, 'validation/loss': 1.3155368566513062, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 1.9666587114334106, 'test/num_examples': 10000, 'score': 52138.199348688126, 'total_duration': 57832.32154393196, 'accumulated_submission_time': 52138.199348688126, 'accumulated_eval_time': 5682.680174589157, 'accumulated_logging_time': 5.116419792175293}
I0204 09:56:07.078801 139923868813056 logging_writer.py:48] [112164] accumulated_eval_time=5682.680175, accumulated_logging_time=5.116420, accumulated_submission_time=52138.199349, global_step=112164, preemption_count=0, score=52138.199349, test/accuracy=0.550000, test/loss=1.966659, test/num_examples=10000, total_duration=57832.321544, train/accuracy=0.757187, train/loss=0.959035, validation/accuracy=0.678280, validation/loss=1.315537, validation/num_examples=50000
I0204 09:56:22.294837 139923852027648 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.5582064390182495, loss=4.509625434875488
I0204 09:57:06.848469 139923868813056 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.3826556205749512, loss=2.95285701751709
I0204 09:57:54.144358 139923852027648 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.73843252658844, loss=2.365145683288574
I0204 09:58:40.915627 139923868813056 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.6792200803756714, loss=2.1888344287872314
I0204 09:59:28.080716 139923852027648 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.548925757408142, loss=3.5313613414764404
I0204 10:00:15.058115 139923868813056 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.719408392906189, loss=2.088742256164551
I0204 10:01:02.315195 139923852027648 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.3250738382339478, loss=3.7610044479370117
I0204 10:01:49.443949 139923868813056 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.5571287870407104, loss=2.1129415035247803
I0204 10:02:36.758912 139923852027648 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.4676839113235474, loss=4.210748672485352
I0204 10:03:07.050475 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:03:17.957169 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:03:52.902258 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:03:54.546148 140085747812160 submission_runner.py:408] Time since start: 58299.83s, 	Step: 113066, 	{'train/accuracy': 0.7348827719688416, 'train/loss': 1.1102678775787354, 'validation/accuracy': 0.6744199991226196, 'validation/loss': 1.374962568283081, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.0241758823394775, 'test/num_examples': 10000, 'score': 52558.10915327072, 'total_duration': 58299.82621335983, 'accumulated_submission_time': 52558.10915327072, 'accumulated_eval_time': 5730.1758716106415, 'accumulated_logging_time': 5.1636152267456055}
I0204 10:03:54.582047 139923868813056 logging_writer.py:48] [113066] accumulated_eval_time=5730.175872, accumulated_logging_time=5.163615, accumulated_submission_time=52558.109153, global_step=113066, preemption_count=0, score=52558.109153, test/accuracy=0.548100, test/loss=2.024176, test/num_examples=10000, total_duration=58299.826213, train/accuracy=0.734883, train/loss=1.110268, validation/accuracy=0.674420, validation/loss=1.374963, validation/num_examples=50000
I0204 10:04:08.947086 139923852027648 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.9157061576843262, loss=2.193915843963623
I0204 10:04:53.371390 139923868813056 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.451971173286438, loss=3.4851691722869873
I0204 10:05:40.360552 139923852027648 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.7692437171936035, loss=2.085353374481201
I0204 10:06:27.450739 139923868813056 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.689677357673645, loss=2.0202014446258545
I0204 10:07:14.589529 139923852027648 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.7081305980682373, loss=2.374462366104126
I0204 10:08:01.434712 139923868813056 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.7083231210708618, loss=2.0300540924072266
I0204 10:08:48.435056 139923852027648 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.6739939451217651, loss=2.027184247970581
I0204 10:09:35.562519 139923868813056 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.8455835580825806, loss=2.0384132862091064
I0204 10:10:22.839874 139923852027648 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.4031163454055786, loss=4.126785755157471
I0204 10:10:54.937497 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:11:05.916131 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:11:41.198825 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:11:42.835675 140085747812160 submission_runner.py:408] Time since start: 58768.12s, 	Step: 113970, 	{'train/accuracy': 0.7422655820846558, 'train/loss': 1.044321060180664, 'validation/accuracy': 0.6773599982261658, 'validation/loss': 1.3351013660430908, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.0046536922454834, 'test/num_examples': 10000, 'score': 52978.40332150459, 'total_duration': 58768.11574149132, 'accumulated_submission_time': 52978.40332150459, 'accumulated_eval_time': 5778.074056625366, 'accumulated_logging_time': 5.209723234176636}
I0204 10:11:42.876996 139923868813056 logging_writer.py:48] [113970] accumulated_eval_time=5778.074057, accumulated_logging_time=5.209723, accumulated_submission_time=52978.403322, global_step=113970, preemption_count=0, score=52978.403322, test/accuracy=0.549500, test/loss=2.004654, test/num_examples=10000, total_duration=58768.115741, train/accuracy=0.742266, train/loss=1.044321, validation/accuracy=0.677360, validation/loss=1.335101, validation/num_examples=50000
I0204 10:11:55.604270 139923852027648 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.5578348636627197, loss=4.272726058959961
I0204 10:12:40.187528 139923868813056 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.604882836341858, loss=2.1525721549987793
I0204 10:13:27.221096 139923852027648 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.6230549812316895, loss=3.061328172683716
I0204 10:14:14.401902 139923868813056 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.4678477048873901, loss=3.8113198280334473
I0204 10:15:01.699828 139923852027648 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.7670871019363403, loss=2.059447765350342
I0204 10:15:48.594920 139923868813056 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.4316388368606567, loss=2.7777976989746094
I0204 10:16:35.761270 139923852027648 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.7926666736602783, loss=2.1088006496429443
I0204 10:17:22.742538 139923868813056 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.8612656593322754, loss=2.1997249126434326
I0204 10:18:09.791259 139923852027648 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.6824901103973389, loss=3.6859776973724365
I0204 10:18:42.996337 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:18:53.635164 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:19:24.939633 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:19:26.583256 140085747812160 submission_runner.py:408] Time since start: 59231.86s, 	Step: 114873, 	{'train/accuracy': 0.7559765577316284, 'train/loss': 0.9663622975349426, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3073861598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 1.9546692371368408, 'test/num_examples': 10000, 'score': 53398.46142745018, 'total_duration': 59231.86329960823, 'accumulated_submission_time': 53398.46142745018, 'accumulated_eval_time': 5821.660964488983, 'accumulated_logging_time': 5.261880159378052}
I0204 10:19:26.619900 139923868813056 logging_writer.py:48] [114873] accumulated_eval_time=5821.660964, accumulated_logging_time=5.261880, accumulated_submission_time=53398.461427, global_step=114873, preemption_count=0, score=53398.461427, test/accuracy=0.551500, test/loss=1.954669, test/num_examples=10000, total_duration=59231.863300, train/accuracy=0.755977, train/loss=0.966362, validation/accuracy=0.679620, validation/loss=1.307386, validation/num_examples=50000
I0204 10:19:38.120740 139923852027648 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.673076868057251, loss=2.016373634338379
I0204 10:20:22.424402 139923868813056 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.5195965766906738, loss=4.387591361999512
I0204 10:21:09.528444 139923852027648 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.6910351514816284, loss=2.0956931114196777
I0204 10:21:56.580183 139923868813056 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.6587235927581787, loss=2.156433343887329
I0204 10:22:43.654269 139923852027648 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.7116402387619019, loss=1.9810701608657837
I0204 10:23:30.571532 139923868813056 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.6542335748672485, loss=2.411947250366211
I0204 10:24:17.627332 139923852027648 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.4180965423583984, loss=2.9050421714782715
I0204 10:25:04.597927 139923868813056 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.5808957815170288, loss=3.960810661315918
I0204 10:25:51.306195 139923852027648 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.7490606307983398, loss=1.9964838027954102
I0204 10:26:26.730225 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:26:37.378781 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:27:13.594373 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:27:15.244668 140085747812160 submission_runner.py:408] Time since start: 59700.52s, 	Step: 115777, 	{'train/accuracy': 0.7392773032188416, 'train/loss': 1.056602954864502, 'validation/accuracy': 0.6826800107955933, 'validation/loss': 1.3141595125198364, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9466331005096436, 'test/num_examples': 10000, 'score': 53818.511486291885, 'total_duration': 59700.5247297287, 'accumulated_submission_time': 53818.511486291885, 'accumulated_eval_time': 5870.175407886505, 'accumulated_logging_time': 5.30842137336731}
I0204 10:27:15.281977 139923868813056 logging_writer.py:48] [115777] accumulated_eval_time=5870.175408, accumulated_logging_time=5.308421, accumulated_submission_time=53818.511486, global_step=115777, preemption_count=0, score=53818.511486, test/accuracy=0.563400, test/loss=1.946633, test/num_examples=10000, total_duration=59700.524730, train/accuracy=0.739277, train/loss=1.056603, validation/accuracy=0.682680, validation/loss=1.314160, validation/num_examples=50000
I0204 10:27:25.137170 139923852027648 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.5443977117538452, loss=2.624790906906128
I0204 10:28:09.028686 139923868813056 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.6796660423278809, loss=1.9194705486297607
I0204 10:28:55.784734 139923852027648 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.7409332990646362, loss=2.007111072540283
I0204 10:29:42.708720 139923868813056 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.6927988529205322, loss=2.0914416313171387
I0204 10:30:29.580579 139923852027648 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.6732176542282104, loss=2.180220365524292
I0204 10:31:16.415939 139923868813056 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.5705510377883911, loss=1.9153306484222412
I0204 10:32:03.590466 139923852027648 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.6290556192398071, loss=1.9956260919570923
I0204 10:32:50.661844 139923868813056 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.6705834865570068, loss=2.2738332748413086
I0204 10:33:37.611406 139923852027648 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.5800145864486694, loss=2.1720235347747803
I0204 10:34:15.377551 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:34:25.664170 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:35:02.672142 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:35:04.306617 140085747812160 submission_runner.py:408] Time since start: 60169.59s, 	Step: 116682, 	{'train/accuracy': 0.7460546493530273, 'train/loss': 1.0224932432174683, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3186691999435425, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 1.9711726903915405, 'test/num_examples': 10000, 'score': 54238.54416680336, 'total_duration': 60169.58668136597, 'accumulated_submission_time': 54238.54416680336, 'accumulated_eval_time': 5919.104496240616, 'accumulated_logging_time': 5.356947898864746}
I0204 10:35:04.343149 139923868813056 logging_writer.py:48] [116682] accumulated_eval_time=5919.104496, accumulated_logging_time=5.356948, accumulated_submission_time=54238.544167, global_step=116682, preemption_count=0, score=54238.544167, test/accuracy=0.553200, test/loss=1.971173, test/num_examples=10000, total_duration=60169.586681, train/accuracy=0.746055, train/loss=1.022493, validation/accuracy=0.679620, validation/loss=1.318669, validation/num_examples=50000
I0204 10:35:12.153897 139923852027648 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.5965816974639893, loss=2.504565477371216
I0204 10:35:55.503492 139923868813056 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.642417311668396, loss=2.114065170288086
I0204 10:36:42.594090 139923852027648 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.6577036380767822, loss=2.148674249649048
I0204 10:37:29.720594 139923868813056 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.4338980913162231, loss=4.013454437255859
I0204 10:38:16.653963 139923852027648 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.719023585319519, loss=2.0419485569000244
I0204 10:39:03.478161 139923868813056 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.8390212059020996, loss=2.311183214187622
I0204 10:39:50.343985 139923852027648 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.7589318752288818, loss=2.0482873916625977
I0204 10:40:37.490547 139923868813056 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.5034044981002808, loss=2.8812317848205566
I0204 10:41:24.255176 139923852027648 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.7346079349517822, loss=4.1216230392456055
I0204 10:42:04.393179 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:42:14.831254 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:42:49.836717 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:42:51.480367 140085747812160 submission_runner.py:408] Time since start: 60636.76s, 	Step: 117587, 	{'train/accuracy': 0.7542187571525574, 'train/loss': 1.0017627477645874, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.3278917074203491, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.97365140914917, 'test/num_examples': 10000, 'score': 54658.53148937225, 'total_duration': 60636.760419130325, 'accumulated_submission_time': 54658.53148937225, 'accumulated_eval_time': 5966.191679239273, 'accumulated_logging_time': 5.405437469482422}
I0204 10:42:51.518975 139923868813056 logging_writer.py:48] [117587] accumulated_eval_time=5966.191679, accumulated_logging_time=5.405437, accumulated_submission_time=54658.531489, global_step=117587, preemption_count=0, score=54658.531489, test/accuracy=0.563900, test/loss=1.973651, test/num_examples=10000, total_duration=60636.760419, train/accuracy=0.754219, train/loss=1.001763, validation/accuracy=0.681320, validation/loss=1.327892, validation/num_examples=50000
I0204 10:42:57.273834 139923852027648 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.6687289476394653, loss=2.146583080291748
I0204 10:43:40.488786 139923868813056 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.6101329326629639, loss=4.494602680206299
I0204 10:44:27.125368 139923852027648 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.7817319631576538, loss=1.9452760219573975
I0204 10:45:14.109171 139923868813056 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.7290589809417725, loss=2.4142303466796875
I0204 10:46:01.037169 139923852027648 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.8997483253479004, loss=1.987328052520752
I0204 10:46:47.963253 139923868813056 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.5433789491653442, loss=4.4749755859375
I0204 10:47:34.893081 139923852027648 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.536064863204956, loss=3.8580121994018555
I0204 10:48:21.736581 139923868813056 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.6090158224105835, loss=3.05260968208313
I0204 10:49:08.822675 139923852027648 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.08408522605896, loss=1.9561221599578857
I0204 10:49:51.553418 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:50:02.030221 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:50:37.981646 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:50:39.628144 140085747812160 submission_runner.py:408] Time since start: 61104.91s, 	Step: 118493, 	{'train/accuracy': 0.7477148175239563, 'train/loss': 1.0368415117263794, 'validation/accuracy': 0.6837999820709229, 'validation/loss': 1.3157402276992798, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9573954343795776, 'test/num_examples': 10000, 'score': 55078.50196790695, 'total_duration': 61104.90820026398, 'accumulated_submission_time': 55078.50196790695, 'accumulated_eval_time': 6014.26641201973, 'accumulated_logging_time': 5.455573797225952}
I0204 10:50:39.663703 139923868813056 logging_writer.py:48] [118493] accumulated_eval_time=6014.266412, accumulated_logging_time=5.455574, accumulated_submission_time=55078.501968, global_step=118493, preemption_count=0, score=55078.501968, test/accuracy=0.558400, test/loss=1.957395, test/num_examples=10000, total_duration=61104.908200, train/accuracy=0.747715, train/loss=1.036842, validation/accuracy=0.683800, validation/loss=1.315740, validation/num_examples=50000
I0204 10:50:42.953060 139923852027648 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.5501465797424316, loss=4.046130180358887
I0204 10:51:25.972059 139923868813056 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.4954782724380493, loss=2.843284845352173
I0204 10:52:12.831734 139923852027648 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.6176855564117432, loss=2.099320411682129
I0204 10:52:59.926103 139923868813056 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.6389118432998657, loss=3.253880023956299
I0204 10:53:47.195633 139923852027648 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.1899425983428955, loss=2.127779006958008
I0204 10:54:34.057461 139923868813056 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.6311670541763306, loss=1.975321650505066
I0204 10:55:20.856782 139923852027648 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.6003938913345337, loss=4.632906913757324
I0204 10:56:07.772137 139923868813056 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.7167181968688965, loss=1.9649760723114014
I0204 10:56:54.399288 139923852027648 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.6670018434524536, loss=2.582629919052124
I0204 10:57:39.936840 140085747812160 spec.py:321] Evaluating on the training split.
I0204 10:57:50.387049 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 10:58:20.360739 140085747812160 spec.py:349] Evaluating on the test split.
I0204 10:58:22.002594 140085747812160 submission_runner.py:408] Time since start: 61567.28s, 	Step: 119399, 	{'train/accuracy': 0.7528710961341858, 'train/loss': 0.9768688082695007, 'validation/accuracy': 0.6881799697875977, 'validation/loss': 1.2712483406066895, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.9199223518371582, 'test/num_examples': 10000, 'score': 55498.71514606476, 'total_duration': 61567.28264904022, 'accumulated_submission_time': 55498.71514606476, 'accumulated_eval_time': 6056.332176923752, 'accumulated_logging_time': 5.500463962554932}
I0204 10:58:22.038594 139923868813056 logging_writer.py:48] [119399] accumulated_eval_time=6056.332177, accumulated_logging_time=5.500464, accumulated_submission_time=55498.715146, global_step=119399, preemption_count=0, score=55498.715146, test/accuracy=0.564900, test/loss=1.919922, test/num_examples=10000, total_duration=61567.282649, train/accuracy=0.752871, train/loss=0.976869, validation/accuracy=0.688180, validation/loss=1.271248, validation/num_examples=50000
I0204 10:58:22.902866 139923852027648 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.7330201864242554, loss=2.432744026184082
I0204 10:59:05.571923 139923868813056 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.8922441005706787, loss=4.374652862548828
I0204 10:59:52.308703 139923852027648 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.574334740638733, loss=1.9929202795028687
I0204 11:00:39.216067 139923868813056 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.8065412044525146, loss=1.971128225326538
I0204 11:01:26.517789 139923852027648 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.6593692302703857, loss=1.7908716201782227
I0204 11:02:13.377861 139923868813056 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.6329128742218018, loss=1.925918698310852
I0204 11:03:00.750884 139923852027648 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.7785835266113281, loss=2.022136926651001
I0204 11:03:48.234629 139923868813056 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.5940014123916626, loss=2.2865962982177734
I0204 11:04:34.813656 139923852027648 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.662693977355957, loss=1.9683630466461182
I0204 11:05:22.229890 139923868813056 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.7468429803848267, loss=1.969655990600586
I0204 11:05:22.245957 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:05:33.019120 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:06:06.333425 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:06:07.978055 140085747812160 submission_runner.py:408] Time since start: 62033.26s, 	Step: 120301, 	{'train/accuracy': 0.7587109208106995, 'train/loss': 0.9645220637321472, 'validation/accuracy': 0.6875999569892883, 'validation/loss': 1.2869911193847656, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 1.9446827173233032, 'test/num_examples': 10000, 'score': 55918.85990691185, 'total_duration': 62033.258112192154, 'accumulated_submission_time': 55918.85990691185, 'accumulated_eval_time': 6102.064264535904, 'accumulated_logging_time': 5.548049688339233}
I0204 11:06:08.015183 139923852027648 logging_writer.py:48] [120301] accumulated_eval_time=6102.064265, accumulated_logging_time=5.548050, accumulated_submission_time=55918.859907, global_step=120301, preemption_count=0, score=55918.859907, test/accuracy=0.564500, test/loss=1.944683, test/num_examples=10000, total_duration=62033.258112, train/accuracy=0.758711, train/loss=0.964522, validation/accuracy=0.687600, validation/loss=1.286991, validation/num_examples=50000
I0204 11:06:50.359015 139923868813056 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.778051733970642, loss=2.283557891845703
I0204 11:07:37.068269 139923852027648 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.83222496509552, loss=2.102626085281372
I0204 11:08:24.255243 139923868813056 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.479880452156067, loss=2.783632516860962
I0204 11:09:11.211237 139923852027648 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.7765942811965942, loss=3.455512046813965
I0204 11:09:57.938739 139923868813056 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.692423939704895, loss=2.205918550491333
I0204 11:10:44.729905 139923852027648 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.4342997074127197, loss=2.626828670501709
I0204 11:11:31.702745 139923868813056 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.7890208959579468, loss=4.448207855224609
I0204 11:12:18.880285 139923852027648 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.6912537813186646, loss=3.2695579528808594
I0204 11:13:05.874415 139923868813056 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.5865331888198853, loss=4.365386009216309
I0204 11:13:07.982996 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:13:18.500115 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:13:53.558072 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:13:55.195835 140085747812160 submission_runner.py:408] Time since start: 62500.48s, 	Step: 121206, 	{'train/accuracy': 0.749804675579071, 'train/loss': 0.9959432482719421, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.2766941785812378, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9142930507659912, 'test/num_examples': 10000, 'score': 56338.76570916176, 'total_duration': 62500.47589445114, 'accumulated_submission_time': 56338.76570916176, 'accumulated_eval_time': 6149.277095556259, 'accumulated_logging_time': 5.596048831939697}
I0204 11:13:55.230505 139923852027648 logging_writer.py:48] [121206] accumulated_eval_time=6149.277096, accumulated_logging_time=5.596049, accumulated_submission_time=56338.765709, global_step=121206, preemption_count=0, score=56338.765709, test/accuracy=0.565200, test/loss=1.914293, test/num_examples=10000, total_duration=62500.475894, train/accuracy=0.749805, train/loss=0.995943, validation/accuracy=0.690240, validation/loss=1.276694, validation/num_examples=50000
I0204 11:14:35.690443 139923868813056 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.7821069955825806, loss=4.627163410186768
I0204 11:15:22.505258 139923852027648 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.667570948600769, loss=4.414975166320801
I0204 11:16:09.656654 139923868813056 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.641305923461914, loss=3.128220796585083
I0204 11:16:56.486689 139923852027648 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.5838611125946045, loss=4.504274845123291
I0204 11:17:43.477911 139923868813056 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.6584726572036743, loss=1.9811832904815674
I0204 11:18:30.590402 139923852027648 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.5602085590362549, loss=3.7801480293273926
I0204 11:19:17.487990 139923868813056 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.9847519397735596, loss=1.8528952598571777
I0204 11:20:04.394322 139923852027648 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.716822624206543, loss=2.3103251457214355
I0204 11:20:51.453440 139923868813056 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.700093150138855, loss=3.2874045372009277
I0204 11:20:55.384914 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:21:05.903397 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:21:42.497782 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:21:44.137247 140085747812160 submission_runner.py:408] Time since start: 62969.42s, 	Step: 122110, 	{'train/accuracy': 0.7586718797683716, 'train/loss': 0.9999610781669617, 'validation/accuracy': 0.6941399574279785, 'validation/loss': 1.2884547710418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9204736948013306, 'test/num_examples': 10000, 'score': 56758.85825443268, 'total_duration': 62969.41731142998, 'accumulated_submission_time': 56758.85825443268, 'accumulated_eval_time': 6198.029419660568, 'accumulated_logging_time': 5.6411542892456055}
I0204 11:21:44.172724 139923852027648 logging_writer.py:48] [122110] accumulated_eval_time=6198.029420, accumulated_logging_time=5.641154, accumulated_submission_time=56758.858254, global_step=122110, preemption_count=0, score=56758.858254, test/accuracy=0.568100, test/loss=1.920474, test/num_examples=10000, total_duration=62969.417311, train/accuracy=0.758672, train/loss=0.999961, validation/accuracy=0.694140, validation/loss=1.288455, validation/num_examples=50000
I0204 11:22:22.715522 139923868813056 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.6837409734725952, loss=1.8932116031646729
I0204 11:23:09.382979 139923852027648 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.5770132541656494, loss=4.534891605377197
I0204 11:23:56.219028 139923868813056 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.8459585905075073, loss=2.2067620754241943
I0204 11:24:42.948675 139923852027648 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.755998969078064, loss=3.0773801803588867
I0204 11:25:30.061002 139923868813056 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.641330361366272, loss=1.9920016527175903
I0204 11:26:16.870126 139923852027648 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.7291793823242188, loss=2.976740598678589
I0204 11:27:03.592700 139923868813056 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.7179958820343018, loss=3.7154953479766846
I0204 11:27:50.397444 139923852027648 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.6671637296676636, loss=2.0505783557891846
I0204 11:28:37.051226 139923868813056 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.8471297025680542, loss=1.9397213459014893
I0204 11:28:44.161991 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:28:54.614674 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:29:30.891127 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:29:32.531623 140085747812160 submission_runner.py:408] Time since start: 63437.81s, 	Step: 123017, 	{'train/accuracy': 0.7599804401397705, 'train/loss': 0.9783530235290527, 'validation/accuracy': 0.6932199597358704, 'validation/loss': 1.2936484813690186, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9345817565917969, 'test/num_examples': 10000, 'score': 57178.785105228424, 'total_duration': 63437.81168818474, 'accumulated_submission_time': 57178.785105228424, 'accumulated_eval_time': 6246.399075984955, 'accumulated_logging_time': 5.687482833862305}
I0204 11:29:32.569340 139923852027648 logging_writer.py:48] [123017] accumulated_eval_time=6246.399076, accumulated_logging_time=5.687483, accumulated_submission_time=57178.785105, global_step=123017, preemption_count=0, score=57178.785105, test/accuracy=0.565400, test/loss=1.934582, test/num_examples=10000, total_duration=63437.811688, train/accuracy=0.759980, train/loss=0.978353, validation/accuracy=0.693220, validation/loss=1.293648, validation/num_examples=50000
I0204 11:30:07.807921 139923868813056 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.8025178909301758, loss=4.07159423828125
I0204 11:30:54.136013 139923852027648 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.7389416694641113, loss=1.901914119720459
I0204 11:31:41.359951 139923868813056 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.5669280290603638, loss=2.587067127227783
I0204 11:32:28.153052 139923852027648 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.7585818767547607, loss=1.8297256231307983
I0204 11:33:14.846414 139923868813056 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.7538466453552246, loss=1.8519604206085205
I0204 11:34:01.642849 139923852027648 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.7286795377731323, loss=3.6840033531188965
I0204 11:34:48.645626 139923868813056 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.7126520872116089, loss=2.018256902694702
I0204 11:35:35.448769 139923852027648 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.685906171798706, loss=2.0103626251220703
I0204 11:36:22.589254 139923868813056 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.8310004472732544, loss=1.983379602432251
I0204 11:36:32.552135 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:36:43.077978 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:37:18.146056 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:37:19.783977 140085747812160 submission_runner.py:408] Time since start: 63905.06s, 	Step: 123923, 	{'train/accuracy': 0.7659960985183716, 'train/loss': 0.9347695708274841, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.2383933067321777, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.8837766647338867, 'test/num_examples': 10000, 'score': 57598.70680522919, 'total_duration': 63905.06403899193, 'accumulated_submission_time': 57598.70680522919, 'accumulated_eval_time': 6293.630912065506, 'accumulated_logging_time': 5.734906911849976}
I0204 11:37:19.818261 139923852027648 logging_writer.py:48] [123923] accumulated_eval_time=6293.630912, accumulated_logging_time=5.734907, accumulated_submission_time=57598.706805, global_step=123923, preemption_count=0, score=57598.706805, test/accuracy=0.571300, test/loss=1.883777, test/num_examples=10000, total_duration=63905.064039, train/accuracy=0.765996, train/loss=0.934770, validation/accuracy=0.695400, validation/loss=1.238393, validation/num_examples=50000
I0204 11:37:52.276046 139923868813056 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.9021100997924805, loss=3.1002519130706787
I0204 11:38:39.056524 139923852027648 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.661385178565979, loss=3.125736713409424
I0204 11:39:26.060278 139923868813056 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7442626953125, loss=2.212449550628662
I0204 11:40:12.800280 139923852027648 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.8182404041290283, loss=1.953322410583496
I0204 11:40:59.599136 139923868813056 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5913805961608887, loss=1.8804099559783936
I0204 11:41:46.545751 139923852027648 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.7949591875076294, loss=3.606384754180908
I0204 11:42:33.645309 139923868813056 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.7865583896636963, loss=2.4664552211761475
I0204 11:43:20.316940 139923852027648 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.57621169090271, loss=4.452853202819824
I0204 11:44:07.252183 139923868813056 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.875399112701416, loss=2.0419247150421143
I0204 11:44:20.114671 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:44:30.564497 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:45:08.731526 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:45:10.384527 140085747812160 submission_runner.py:408] Time since start: 64375.66s, 	Step: 124829, 	{'train/accuracy': 0.763378918170929, 'train/loss': 0.9280872344970703, 'validation/accuracy': 0.6961399912834167, 'validation/loss': 1.2279818058013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.8792723417282104, 'test/num_examples': 10000, 'score': 58018.94120192528, 'total_duration': 64375.66458725929, 'accumulated_submission_time': 58018.94120192528, 'accumulated_eval_time': 6343.900760173798, 'accumulated_logging_time': 5.780289649963379}
I0204 11:45:10.421068 139923852027648 logging_writer.py:48] [124829] accumulated_eval_time=6343.900760, accumulated_logging_time=5.780290, accumulated_submission_time=58018.941202, global_step=124829, preemption_count=0, score=58018.941202, test/accuracy=0.569900, test/loss=1.879272, test/num_examples=10000, total_duration=64375.664587, train/accuracy=0.763379, train/loss=0.928087, validation/accuracy=0.696140, validation/loss=1.227982, validation/num_examples=50000
I0204 11:45:40.001260 139923868813056 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.0290112495422363, loss=2.0101492404937744
I0204 11:46:26.503243 139923852027648 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.6377606391906738, loss=3.7455697059631348
I0204 11:47:13.976778 139923868813056 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.7130295038223267, loss=2.7928922176361084
I0204 11:48:00.907267 139923852027648 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.0048608779907227, loss=1.8644447326660156
I0204 11:48:47.842952 139923868813056 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.9357131719589233, loss=1.8867000341415405
I0204 11:49:34.789527 139923852027648 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.827837586402893, loss=3.6929879188537598
I0204 11:50:21.617527 139923868813056 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.8013427257537842, loss=1.886364221572876
I0204 11:51:08.734032 139923852027648 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.8726840019226074, loss=1.9832030534744263
I0204 11:51:55.705377 139923868813056 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.05737042427063, loss=1.9537514448165894
I0204 11:52:10.450118 140085747812160 spec.py:321] Evaluating on the training split.
I0204 11:52:21.349192 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 11:52:56.091253 140085747812160 spec.py:349] Evaluating on the test split.
I0204 11:52:57.733358 140085747812160 submission_runner.py:408] Time since start: 64843.01s, 	Step: 125733, 	{'train/accuracy': 0.7677538990974426, 'train/loss': 0.9241829514503479, 'validation/accuracy': 0.6993599534034729, 'validation/loss': 1.229907751083374, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.8638954162597656, 'test/num_examples': 10000, 'score': 58438.90938568115, 'total_duration': 64843.013377428055, 'accumulated_submission_time': 58438.90938568115, 'accumulated_eval_time': 6391.183966159821, 'accumulated_logging_time': 5.8267786502838135}
I0204 11:52:57.772321 139923852027648 logging_writer.py:48] [125733] accumulated_eval_time=6391.183966, accumulated_logging_time=5.826779, accumulated_submission_time=58438.909386, global_step=125733, preemption_count=0, score=58438.909386, test/accuracy=0.580100, test/loss=1.863895, test/num_examples=10000, total_duration=64843.013377, train/accuracy=0.767754, train/loss=0.924183, validation/accuracy=0.699360, validation/loss=1.229908, validation/num_examples=50000
I0204 11:53:25.689689 139923868813056 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.7613469362258911, loss=2.238003730773926
I0204 11:54:12.142962 139923852027648 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.8073660135269165, loss=1.9088788032531738
I0204 11:54:59.392769 139923868813056 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.0003254413604736, loss=1.7726852893829346
I0204 11:55:46.202570 139923852027648 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.6102464199066162, loss=2.8972792625427246
I0204 11:56:33.489776 139923868813056 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.5614783763885498, loss=3.994502305984497
I0204 11:57:20.505630 139923852027648 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.038604974746704, loss=1.9235094785690308
I0204 11:58:07.888060 139923868813056 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.7178019285202026, loss=4.069713115692139
I0204 11:58:54.975739 139923852027648 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.9336934089660645, loss=2.102358818054199
I0204 11:59:42.292086 139923868813056 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.8932121992111206, loss=1.976765751838684
I0204 11:59:58.011435 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:00:08.658290 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:00:45.887746 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:00:47.533264 140085747812160 submission_runner.py:408] Time since start: 65312.81s, 	Step: 126635, 	{'train/accuracy': 0.78236323595047, 'train/loss': 0.8538432121276855, 'validation/accuracy': 0.7017399668693542, 'validation/loss': 1.2105683088302612, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.8391834497451782, 'test/num_examples': 10000, 'score': 58859.0885219574, 'total_duration': 65312.81332349777, 'accumulated_submission_time': 58859.0885219574, 'accumulated_eval_time': 6440.7057983875275, 'accumulated_logging_time': 5.875086545944214}
I0204 12:00:47.570393 139923852027648 logging_writer.py:48] [126635] accumulated_eval_time=6440.705798, accumulated_logging_time=5.875087, accumulated_submission_time=58859.088522, global_step=126635, preemption_count=0, score=58859.088522, test/accuracy=0.575300, test/loss=1.839183, test/num_examples=10000, total_duration=65312.813323, train/accuracy=0.782363, train/loss=0.853843, validation/accuracy=0.701740, validation/loss=1.210568, validation/num_examples=50000
I0204 12:01:14.661758 139923868813056 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.7480558156967163, loss=4.5098466873168945
I0204 12:02:00.691542 139923852027648 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.9191194772720337, loss=2.10097074508667
I0204 12:02:47.812781 139923868813056 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.8545256853103638, loss=1.8283365964889526
I0204 12:03:34.461159 139923852027648 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.702338457107544, loss=4.36059045791626
I0204 12:04:21.363434 139923868813056 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.8828903436660767, loss=2.0507400035858154
I0204 12:05:08.160026 139923852027648 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.768038034439087, loss=2.0697669982910156
I0204 12:05:54.974449 139923868813056 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.9147953987121582, loss=1.836653709411621
I0204 12:06:41.978211 139923852027648 logging_writer.py:48] [127400] global_step=127400, grad_norm=1.8112999200820923, loss=1.8068424463272095
I0204 12:07:28.853819 139923868813056 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.7233860492706299, loss=4.446157455444336
I0204 12:07:47.807254 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:07:58.193314 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:08:32.491684 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:08:34.130948 140085747812160 submission_runner.py:408] Time since start: 65779.41s, 	Step: 127542, 	{'train/accuracy': 0.759082019329071, 'train/loss': 0.964400053024292, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.2564650774002075, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 1.9025813341140747, 'test/num_examples': 10000, 'score': 59279.26599240303, 'total_duration': 65779.41101312637, 'accumulated_submission_time': 59279.26599240303, 'accumulated_eval_time': 6487.029499053955, 'accumulated_logging_time': 5.920918226242065}
I0204 12:08:34.169880 139923852027648 logging_writer.py:48] [127542] accumulated_eval_time=6487.029499, accumulated_logging_time=5.920918, accumulated_submission_time=59279.265992, global_step=127542, preemption_count=0, score=59279.265992, test/accuracy=0.571800, test/loss=1.902581, test/num_examples=10000, total_duration=65779.411013, train/accuracy=0.759082, train/loss=0.964400, validation/accuracy=0.695540, validation/loss=1.256465, validation/num_examples=50000
I0204 12:08:58.399909 139923868813056 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.732335090637207, loss=1.8511861562728882
I0204 12:09:44.192426 139923852027648 logging_writer.py:48] [127700] global_step=127700, grad_norm=1.691643476486206, loss=3.691864490509033
I0204 12:10:30.790519 139923868813056 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.920709252357483, loss=4.001541614532471
I0204 12:11:17.816665 139923852027648 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.6496576070785522, loss=3.720475196838379
I0204 12:12:04.739638 139923868813056 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.119471549987793, loss=1.9558048248291016
I0204 12:12:52.029873 139923852027648 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.7231392860412598, loss=3.6473426818847656
I0204 12:13:38.871979 139923868813056 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.6522619724273682, loss=3.8884360790252686
I0204 12:14:26.001208 139923852027648 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.7354196310043335, loss=3.953094244003296
I0204 12:15:12.728158 139923868813056 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.2115278244018555, loss=2.0471622943878174
I0204 12:15:34.459683 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:15:44.947557 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:16:19.633391 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:16:21.274560 140085747812160 submission_runner.py:408] Time since start: 66246.55s, 	Step: 128448, 	{'train/accuracy': 0.774609386920929, 'train/loss': 0.8842771053314209, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2053685188293457, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.8524872064590454, 'test/num_examples': 10000, 'score': 59699.49506402016, 'total_duration': 66246.55462884903, 'accumulated_submission_time': 59699.49506402016, 'accumulated_eval_time': 6533.84437918663, 'accumulated_logging_time': 5.970127820968628}
I0204 12:16:21.311661 139923852027648 logging_writer.py:48] [128448] accumulated_eval_time=6533.844379, accumulated_logging_time=5.970128, accumulated_submission_time=59699.495064, global_step=128448, preemption_count=0, score=59699.495064, test/accuracy=0.575900, test/loss=1.852487, test/num_examples=10000, total_duration=66246.554629, train/accuracy=0.774609, train/loss=0.884277, validation/accuracy=0.701260, validation/loss=1.205369, validation/num_examples=50000
I0204 12:16:43.086788 139923868813056 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.953377604484558, loss=2.2039828300476074
I0204 12:17:28.637129 139923852027648 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.9368067979812622, loss=1.9818865060806274
I0204 12:18:15.845391 139923868813056 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.753116488456726, loss=1.8599324226379395
I0204 12:19:02.603747 139923852027648 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.1948485374450684, loss=1.9079487323760986
I0204 12:19:49.450205 139923868813056 logging_writer.py:48] [128900] global_step=128900, grad_norm=1.6642024517059326, loss=3.260971784591675
I0204 12:20:36.333499 139923852027648 logging_writer.py:48] [129000] global_step=129000, grad_norm=1.7574928998947144, loss=2.3387553691864014
I0204 12:21:23.233447 139923868813056 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.952911615371704, loss=1.8347265720367432
I0204 12:22:10.289749 139923852027648 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.7935243844985962, loss=3.0963382720947266
I0204 12:22:57.208633 139923868813056 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.0845673084259033, loss=1.8063938617706299
I0204 12:23:21.418771 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:23:31.862696 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:24:05.103277 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:24:06.752735 140085747812160 submission_runner.py:408] Time since start: 66712.03s, 	Step: 129353, 	{'train/accuracy': 0.7856835722923279, 'train/loss': 0.8250307440757751, 'validation/accuracy': 0.7035599946975708, 'validation/loss': 1.1953197717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.5803000330924988, 'test/loss': 1.8332209587097168, 'test/num_examples': 10000, 'score': 60119.54176044464, 'total_duration': 66712.03279447556, 'accumulated_submission_time': 60119.54176044464, 'accumulated_eval_time': 6579.178344249725, 'accumulated_logging_time': 6.016943454742432}
I0204 12:24:06.791392 139923852027648 logging_writer.py:48] [129353] accumulated_eval_time=6579.178344, accumulated_logging_time=6.016943, accumulated_submission_time=60119.541760, global_step=129353, preemption_count=0, score=60119.541760, test/accuracy=0.580300, test/loss=1.833221, test/num_examples=10000, total_duration=66712.032794, train/accuracy=0.785684, train/loss=0.825031, validation/accuracy=0.703560, validation/loss=1.195320, validation/num_examples=50000
I0204 12:24:26.662926 139923868813056 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.720835566520691, loss=4.015048027038574
I0204 12:25:11.570567 139923852027648 logging_writer.py:48] [129500] global_step=129500, grad_norm=1.7274067401885986, loss=3.682652473449707
I0204 12:25:58.688253 139923868813056 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.7734025716781616, loss=4.297849655151367
I0204 12:26:45.540325 139923852027648 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.8942662477493286, loss=1.8825106620788574
I0204 12:27:32.329509 139923868813056 logging_writer.py:48] [129800] global_step=129800, grad_norm=1.6999454498291016, loss=4.435969829559326
I0204 12:28:19.437101 139923852027648 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.2862026691436768, loss=1.9268970489501953
I0204 12:29:06.391217 139923868813056 logging_writer.py:48] [130000] global_step=130000, grad_norm=1.9784008264541626, loss=2.381103038787842
I0204 12:29:53.393660 139923852027648 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.922149658203125, loss=1.9824298620224
I0204 12:30:40.617623 139923868813056 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.0062787532806396, loss=1.8640220165252686
I0204 12:31:06.978039 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:31:17.540219 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:31:53.268169 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:31:54.907935 140085747812160 submission_runner.py:408] Time since start: 67180.19s, 	Step: 130258, 	{'train/accuracy': 0.7689648270606995, 'train/loss': 0.9314327239990234, 'validation/accuracy': 0.7040199637413025, 'validation/loss': 1.226270079612732, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.872269868850708, 'test/num_examples': 10000, 'score': 60539.66762089729, 'total_duration': 67180.1879966259, 'accumulated_submission_time': 60539.66762089729, 'accumulated_eval_time': 6627.108244419098, 'accumulated_logging_time': 6.065059423446655}
I0204 12:31:54.947886 139923852027648 logging_writer.py:48] [130258] accumulated_eval_time=6627.108244, accumulated_logging_time=6.065059, accumulated_submission_time=60539.667621, global_step=130258, preemption_count=0, score=60539.667621, test/accuracy=0.576300, test/loss=1.872270, test/num_examples=10000, total_duration=67180.187997, train/accuracy=0.768965, train/loss=0.931433, validation/accuracy=0.704020, validation/loss=1.226270, validation/num_examples=50000
I0204 12:32:12.607850 139923868813056 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.6443850994110107, loss=2.964484214782715
I0204 12:32:57.452762 139923852027648 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.8173109292984009, loss=2.5786991119384766
I0204 12:33:44.275238 139923868813056 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.097167491912842, loss=1.972644567489624
I0204 12:34:31.086616 139923852027648 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.7624704837799072, loss=1.7905783653259277
I0204 12:35:17.901805 139923868813056 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.1353445053100586, loss=2.252039909362793
I0204 12:36:04.776661 139923852027648 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.165299415588379, loss=4.451211929321289
I0204 12:36:51.856729 139923868813056 logging_writer.py:48] [130900] global_step=130900, grad_norm=1.751940131187439, loss=3.5266189575195312
I0204 12:37:38.750170 139923852027648 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.9267604351043701, loss=2.2871105670928955
I0204 12:38:25.446070 139923868813056 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.065990686416626, loss=2.0699803829193115
I0204 12:38:55.026290 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:39:05.501419 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:39:40.889898 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:39:42.533362 140085747812160 submission_runner.py:408] Time since start: 67647.81s, 	Step: 131165, 	{'train/accuracy': 0.7744531035423279, 'train/loss': 0.9301571249961853, 'validation/accuracy': 0.7041199803352356, 'validation/loss': 1.241104245185852, 'validation/num_examples': 50000, 'test/accuracy': 0.589400053024292, 'test/loss': 1.8679510354995728, 'test/num_examples': 10000, 'score': 60959.68307638168, 'total_duration': 67647.81341028214, 'accumulated_submission_time': 60959.68307638168, 'accumulated_eval_time': 6674.615309238434, 'accumulated_logging_time': 6.117140293121338}
I0204 12:39:42.573757 139923852027648 logging_writer.py:48] [131165] accumulated_eval_time=6674.615309, accumulated_logging_time=6.117140, accumulated_submission_time=60959.683076, global_step=131165, preemption_count=0, score=60959.683076, test/accuracy=0.589400, test/loss=1.867951, test/num_examples=10000, total_duration=67647.813410, train/accuracy=0.774453, train/loss=0.930157, validation/accuracy=0.704120, validation/loss=1.241104, validation/num_examples=50000
I0204 12:39:57.374319 139923868813056 logging_writer.py:48] [131200] global_step=131200, grad_norm=1.7590217590332031, loss=3.8210930824279785
I0204 12:40:41.689565 139923852027648 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.1726462841033936, loss=1.7858436107635498
I0204 12:41:29.066315 139923868813056 logging_writer.py:48] [131400] global_step=131400, grad_norm=1.7532933950424194, loss=2.5797955989837646
I0204 12:42:16.221796 139923852027648 logging_writer.py:48] [131500] global_step=131500, grad_norm=1.9527301788330078, loss=1.8486101627349854
I0204 12:43:03.376849 139923868813056 logging_writer.py:48] [131600] global_step=131600, grad_norm=1.8462605476379395, loss=1.895011305809021
I0204 12:43:50.394673 139923852027648 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.8001935482025146, loss=1.9270241260528564
I0204 12:44:37.290584 139923868813056 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.2245452404022217, loss=2.0919339656829834
I0204 12:45:24.306168 139923852027648 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.8225449323654175, loss=2.0719385147094727
I0204 12:46:11.015363 139923868813056 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8613953590393066, loss=1.9043046236038208
I0204 12:46:42.627421 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:46:53.112682 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:47:25.934417 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:47:27.574814 140085747812160 submission_runner.py:408] Time since start: 68112.85s, 	Step: 132069, 	{'train/accuracy': 0.7873827815055847, 'train/loss': 0.8701390624046326, 'validation/accuracy': 0.7058999538421631, 'validation/loss': 1.2221981287002563, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.8685365915298462, 'test/num_examples': 10000, 'score': 61379.67488145828, 'total_duration': 68112.85487318039, 'accumulated_submission_time': 61379.67488145828, 'accumulated_eval_time': 6719.562696695328, 'accumulated_logging_time': 6.168532371520996}
I0204 12:47:27.612163 139923852027648 logging_writer.py:48] [132069] accumulated_eval_time=6719.562697, accumulated_logging_time=6.168532, accumulated_submission_time=61379.674881, global_step=132069, preemption_count=0, score=61379.674881, test/accuracy=0.580000, test/loss=1.868537, test/num_examples=10000, total_duration=68112.854873, train/accuracy=0.787383, train/loss=0.870139, validation/accuracy=0.705900, validation/loss=1.222198, validation/num_examples=50000
I0204 12:47:40.759576 139923868813056 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.9027302265167236, loss=4.068385124206543
I0204 12:48:25.257584 139923852027648 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.014094591140747, loss=1.8556698560714722
I0204 12:49:12.148201 139923868813056 logging_writer.py:48] [132300] global_step=132300, grad_norm=1.7584102153778076, loss=3.980135917663574
I0204 12:49:59.063595 139923852027648 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.096677541732788, loss=1.734349012374878
I0204 12:50:45.747581 139923868813056 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.857827067375183, loss=4.063474655151367
I0204 12:51:32.793736 139923852027648 logging_writer.py:48] [132600] global_step=132600, grad_norm=1.9412891864776611, loss=1.801877498626709
I0204 12:52:20.155061 139923868813056 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.0139214992523193, loss=2.0175607204437256
I0204 12:53:07.271925 139923852027648 logging_writer.py:48] [132800] global_step=132800, grad_norm=1.886082649230957, loss=1.8860676288604736
I0204 12:53:54.193068 139923868813056 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.986413598060608, loss=1.9685858488082886
I0204 12:54:27.596170 140085747812160 spec.py:321] Evaluating on the training split.
I0204 12:54:38.261516 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 12:55:16.413039 140085747812160 spec.py:349] Evaluating on the test split.
I0204 12:55:18.050334 140085747812160 submission_runner.py:408] Time since start: 68583.33s, 	Step: 132973, 	{'train/accuracy': 0.7793359160423279, 'train/loss': 0.8791748881340027, 'validation/accuracy': 0.7087000012397766, 'validation/loss': 1.1858389377593994, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.819374918937683, 'test/num_examples': 10000, 'score': 61799.59830021858, 'total_duration': 68583.33038425446, 'accumulated_submission_time': 61799.59830021858, 'accumulated_eval_time': 6770.016849517822, 'accumulated_logging_time': 6.2154810428619385}
I0204 12:55:18.092896 139923852027648 logging_writer.py:48] [132973] accumulated_eval_time=6770.016850, accumulated_logging_time=6.215481, accumulated_submission_time=61799.598300, global_step=132973, preemption_count=0, score=61799.598300, test/accuracy=0.584500, test/loss=1.819375, test/num_examples=10000, total_duration=68583.330384, train/accuracy=0.779336, train/loss=0.879175, validation/accuracy=0.708700, validation/loss=1.185839, validation/num_examples=50000
I0204 12:55:29.596840 139923868813056 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.8543598651885986, loss=2.9447927474975586
I0204 12:56:13.646316 139923852027648 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.207322597503662, loss=2.0389134883880615
I0204 12:57:00.434363 139923868813056 logging_writer.py:48] [133200] global_step=133200, grad_norm=1.8022178411483765, loss=4.379990577697754
I0204 12:57:47.331306 139923852027648 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.9439384937286377, loss=1.8173911571502686
I0204 12:58:34.294257 139923868813056 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.808000922203064, loss=3.7095558643341064
I0204 12:59:21.206097 139923852027648 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.458296775817871, loss=1.8771796226501465
I0204 13:00:08.134118 139923868813056 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.0911247730255127, loss=1.821521520614624
I0204 13:00:55.105497 139923852027648 logging_writer.py:48] [133700] global_step=133700, grad_norm=1.7747926712036133, loss=3.7276480197906494
I0204 13:01:42.221075 139923868813056 logging_writer.py:48] [133800] global_step=133800, grad_norm=1.9978506565093994, loss=1.7653096914291382
I0204 13:02:18.327997 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:02:29.013808 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:03:03.558173 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:03:05.202196 140085747812160 submission_runner.py:408] Time since start: 69050.48s, 	Step: 133878, 	{'train/accuracy': 0.7819726467132568, 'train/loss': 0.849389910697937, 'validation/accuracy': 0.7128599882125854, 'validation/loss': 1.1591600179672241, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.7851358652114868, 'test/num_examples': 10000, 'score': 62219.77271294594, 'total_duration': 69050.48225259781, 'accumulated_submission_time': 62219.77271294594, 'accumulated_eval_time': 6816.891093969345, 'accumulated_logging_time': 6.26798677444458}
I0204 13:03:05.241437 139923852027648 logging_writer.py:48] [133878] accumulated_eval_time=6816.891094, accumulated_logging_time=6.267987, accumulated_submission_time=62219.772713, global_step=133878, preemption_count=0, score=62219.772713, test/accuracy=0.593700, test/loss=1.785136, test/num_examples=10000, total_duration=69050.482253, train/accuracy=0.781973, train/loss=0.849390, validation/accuracy=0.712860, validation/loss=1.159160, validation/num_examples=50000
I0204 13:03:14.693117 139923868813056 logging_writer.py:48] [133900] global_step=133900, grad_norm=1.9755253791809082, loss=1.8531866073608398
I0204 13:03:58.377009 139923852027648 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.9075016975402832, loss=1.937913417816162
I0204 13:04:45.329637 139923868813056 logging_writer.py:48] [134100] global_step=134100, grad_norm=1.9852023124694824, loss=1.8862237930297852
I0204 13:05:32.383705 139923852027648 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9987636804580688, loss=1.7979426383972168
I0204 13:06:19.690395 139923868813056 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.30143666267395, loss=1.8769240379333496
I0204 13:07:06.807988 139923852027648 logging_writer.py:48] [134400] global_step=134400, grad_norm=1.8936638832092285, loss=2.252574920654297
I0204 13:07:53.703997 139923868813056 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.9215954542160034, loss=1.8085592985153198
I0204 13:08:40.700305 139923852027648 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.0331060886383057, loss=4.340118408203125
I0204 13:09:27.702749 139923868813056 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.069291591644287, loss=2.063796281814575
I0204 13:10:05.426094 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:10:16.047179 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:10:53.523451 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:10:55.163547 140085747812160 submission_runner.py:408] Time since start: 69520.44s, 	Step: 134782, 	{'train/accuracy': 0.792773425579071, 'train/loss': 0.7919089198112488, 'validation/accuracy': 0.7138599753379822, 'validation/loss': 1.1439038515090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.7752066850662231, 'test/num_examples': 10000, 'score': 62639.89648962021, 'total_duration': 69520.44361186028, 'accumulated_submission_time': 62639.89648962021, 'accumulated_eval_time': 6866.628557682037, 'accumulated_logging_time': 6.316884279251099}
I0204 13:10:55.203175 139923852027648 logging_writer.py:48] [134782] accumulated_eval_time=6866.628558, accumulated_logging_time=6.316884, accumulated_submission_time=62639.896490, global_step=134782, preemption_count=0, score=62639.896490, test/accuracy=0.593400, test/loss=1.775207, test/num_examples=10000, total_duration=69520.443612, train/accuracy=0.792773, train/loss=0.791909, validation/accuracy=0.713860, validation/loss=1.143904, validation/num_examples=50000
I0204 13:11:03.435918 139923868813056 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.7888057231903076, loss=2.839916706085205
I0204 13:11:47.467669 139923852027648 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.14866042137146, loss=1.771436095237732
I0204 13:12:34.405730 139923868813056 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.1152703762054443, loss=2.0606918334960938
I0204 13:13:21.909063 139923852027648 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.8545374870300293, loss=4.010672092437744
I0204 13:14:09.194926 139923868813056 logging_writer.py:48] [135200] global_step=135200, grad_norm=1.9249041080474854, loss=2.970176935195923
I0204 13:14:56.124776 139923852027648 logging_writer.py:48] [135300] global_step=135300, grad_norm=1.9601620435714722, loss=4.255147933959961
I0204 13:15:43.160572 139923868813056 logging_writer.py:48] [135400] global_step=135400, grad_norm=1.9668200016021729, loss=2.4062952995300293
I0204 13:16:30.186414 139923852027648 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.2827954292297363, loss=1.8360790014266968
I0204 13:17:17.388931 139923868813056 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.8732484579086304, loss=1.6944729089736938
I0204 13:17:55.178178 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:18:05.925273 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:18:37.723712 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:18:39.370162 140085747812160 submission_runner.py:408] Time since start: 69984.65s, 	Step: 135682, 	{'train/accuracy': 0.7776952981948853, 'train/loss': 0.8922684788703918, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.2007648944854736, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.8380101919174194, 'test/num_examples': 10000, 'score': 63059.385607004166, 'total_duration': 69984.6502289772, 'accumulated_submission_time': 63059.385607004166, 'accumulated_eval_time': 6910.820559263229, 'accumulated_logging_time': 6.79111123085022}
I0204 13:18:39.410543 139923852027648 logging_writer.py:48] [135682] accumulated_eval_time=6910.820559, accumulated_logging_time=6.791111, accumulated_submission_time=63059.385607, global_step=135682, preemption_count=0, score=63059.385607, test/accuracy=0.586700, test/loss=1.838010, test/num_examples=10000, total_duration=69984.650229, train/accuracy=0.777695, train/loss=0.892268, validation/accuracy=0.709080, validation/loss=1.200765, validation/num_examples=50000
I0204 13:18:47.227360 139923868813056 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.173635959625244, loss=1.7948052883148193
I0204 13:19:30.788583 139923852027648 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.132683515548706, loss=3.3797879219055176
I0204 13:20:17.538300 139923868813056 logging_writer.py:48] [135900] global_step=135900, grad_norm=1.7877185344696045, loss=2.3881492614746094
I0204 13:21:04.609644 139923852027648 logging_writer.py:48] [136000] global_step=136000, grad_norm=1.8137025833129883, loss=3.3744683265686035
I0204 13:21:51.473142 139923868813056 logging_writer.py:48] [136100] global_step=136100, grad_norm=1.9841547012329102, loss=2.3902714252471924
I0204 13:22:38.670117 139923852027648 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.0393943786621094, loss=1.8646552562713623
I0204 13:23:25.724339 139923868813056 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.162437677383423, loss=1.8916456699371338
I0204 13:24:12.704093 139923852027648 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.2370169162750244, loss=1.9031236171722412
I0204 13:24:59.492790 139923868813056 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.2305891513824463, loss=1.8206994533538818
I0204 13:25:39.445241 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:25:50.111000 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:26:22.844587 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:26:24.483434 140085747812160 submission_runner.py:408] Time since start: 70449.76s, 	Step: 136587, 	{'train/accuracy': 0.7864648103713989, 'train/loss': 0.8539221286773682, 'validation/accuracy': 0.715499997138977, 'validation/loss': 1.170691967010498, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.7970510721206665, 'test/num_examples': 10000, 'score': 63479.35899710655, 'total_duration': 70449.76348471642, 'accumulated_submission_time': 63479.35899710655, 'accumulated_eval_time': 6955.8587481975555, 'accumulated_logging_time': 6.841918230056763}
I0204 13:26:24.525643 139923852027648 logging_writer.py:48] [136587] accumulated_eval_time=6955.858748, accumulated_logging_time=6.841918, accumulated_submission_time=63479.358997, global_step=136587, preemption_count=0, score=63479.358997, test/accuracy=0.591800, test/loss=1.797051, test/num_examples=10000, total_duration=70449.763485, train/accuracy=0.786465, train/loss=0.853922, validation/accuracy=0.715500, validation/loss=1.170692, validation/num_examples=50000
I0204 13:26:30.280046 139923868813056 logging_writer.py:48] [136600] global_step=136600, grad_norm=1.835213541984558, loss=3.7399141788482666
I0204 13:27:13.594424 139923852027648 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.0850605964660645, loss=1.7271462678909302
I0204 13:28:00.173806 139923868813056 logging_writer.py:48] [136800] global_step=136800, grad_norm=1.8357884883880615, loss=1.953851580619812
I0204 13:28:47.317431 139923852027648 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.0106663703918457, loss=2.1845343112945557
I0204 13:29:34.151100 139923868813056 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.0085015296936035, loss=1.7610586881637573
I0204 13:30:21.024909 139923852027648 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.9066293239593506, loss=3.2854092121124268
I0204 13:31:07.669878 139923868813056 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.068115234375, loss=1.7696870565414429
I0204 13:31:54.499686 139923852027648 logging_writer.py:48] [137300] global_step=137300, grad_norm=1.807161808013916, loss=2.9809560775756836
I0204 13:32:41.391183 139923868813056 logging_writer.py:48] [137400] global_step=137400, grad_norm=1.8705885410308838, loss=1.7252880334854126
I0204 13:33:24.839911 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:33:35.339817 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:34:12.384777 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:34:14.029513 140085747812160 submission_runner.py:408] Time since start: 70919.31s, 	Step: 137494, 	{'train/accuracy': 0.79066401720047, 'train/loss': 0.8283480405807495, 'validation/accuracy': 0.7112599611282349, 'validation/loss': 1.1805391311645508, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8127272129058838, 'test/num_examples': 10000, 'score': 63899.612374305725, 'total_duration': 70919.30957722664, 'accumulated_submission_time': 63899.612374305725, 'accumulated_eval_time': 7005.048368930817, 'accumulated_logging_time': 6.893892765045166}
I0204 13:34:14.070378 139923852027648 logging_writer.py:48] [137494] accumulated_eval_time=7005.048369, accumulated_logging_time=6.893893, accumulated_submission_time=63899.612374, global_step=137494, preemption_count=0, score=63899.612374, test/accuracy=0.587800, test/loss=1.812727, test/num_examples=10000, total_duration=70919.309577, train/accuracy=0.790664, train/loss=0.828348, validation/accuracy=0.711260, validation/loss=1.180539, validation/num_examples=50000
I0204 13:34:16.950349 139923868813056 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.0004398822784424, loss=1.8165059089660645
I0204 13:34:59.700390 139923852027648 logging_writer.py:48] [137600] global_step=137600, grad_norm=1.9474278688430786, loss=1.9739189147949219
I0204 13:35:46.524884 139923868813056 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.190761089324951, loss=3.8201377391815186
I0204 13:36:33.496247 139923852027648 logging_writer.py:48] [137800] global_step=137800, grad_norm=1.915563702583313, loss=1.839231252670288
I0204 13:37:20.295416 139923868813056 logging_writer.py:48] [137900] global_step=137900, grad_norm=1.9374299049377441, loss=3.2042593955993652
I0204 13:38:07.111864 139923852027648 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.0060529708862305, loss=2.253479480743408
I0204 13:38:54.008152 139923868813056 logging_writer.py:48] [138100] global_step=138100, grad_norm=1.9784382581710815, loss=2.884524345397949
I0204 13:39:40.956916 139923852027648 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.077033281326294, loss=3.893063545227051
I0204 13:40:27.672403 139923868813056 logging_writer.py:48] [138300] global_step=138300, grad_norm=1.9573348760604858, loss=1.7873351573944092
I0204 13:41:14.180602 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:41:24.381147 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:42:00.210869 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:42:01.856701 140085747812160 submission_runner.py:408] Time since start: 71387.14s, 	Step: 138400, 	{'train/accuracy': 0.7880859375, 'train/loss': 0.8579350709915161, 'validation/accuracy': 0.7173199653625488, 'validation/loss': 1.1725926399230957, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.8064581155776978, 'test/num_examples': 10000, 'score': 64319.660131692886, 'total_duration': 71387.13676571846, 'accumulated_submission_time': 64319.660131692886, 'accumulated_eval_time': 7052.7244708538055, 'accumulated_logging_time': 6.946126461029053}
I0204 13:42:01.896567 139923852027648 logging_writer.py:48] [138400] accumulated_eval_time=7052.724471, accumulated_logging_time=6.946126, accumulated_submission_time=64319.660132, global_step=138400, preemption_count=0, score=64319.660132, test/accuracy=0.591900, test/loss=1.806458, test/num_examples=10000, total_duration=71387.136766, train/accuracy=0.788086, train/loss=0.857935, validation/accuracy=0.717320, validation/loss=1.172593, validation/num_examples=50000
I0204 13:42:02.311740 139923868813056 logging_writer.py:48] [138400] global_step=138400, grad_norm=1.799307107925415, loss=3.9962847232818604
I0204 13:42:45.063472 139923852027648 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.0818402767181396, loss=4.383205413818359
I0204 13:43:31.831845 139923868813056 logging_writer.py:48] [138600] global_step=138600, grad_norm=1.944854497909546, loss=2.2873764038085938
I0204 13:44:19.052168 139923852027648 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.9671785831451416, loss=2.407928466796875
I0204 13:45:06.218274 139923868813056 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.977834939956665, loss=2.422656297683716
I0204 13:45:53.414407 139923852027648 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.081455707550049, loss=1.8352843523025513
I0204 13:46:40.300303 139923868813056 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.0451819896698, loss=3.6582374572753906
I0204 13:47:27.606841 139923852027648 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.0601534843444824, loss=1.7515815496444702
I0204 13:48:14.588453 139923868813056 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.1041653156280518, loss=1.8989850282669067
I0204 13:49:01.956538 139923852027648 logging_writer.py:48] [139300] global_step=139300, grad_norm=2.516014814376831, loss=2.0867490768432617
I0204 13:49:01.972240 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:49:12.697661 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:49:47.802513 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:49:49.441584 140085747812160 submission_runner.py:408] Time since start: 71854.72s, 	Step: 139301, 	{'train/accuracy': 0.7901171445846558, 'train/loss': 0.8141428828239441, 'validation/accuracy': 0.7179799675941467, 'validation/loss': 1.1384248733520508, 'validation/num_examples': 50000, 'test/accuracy': 0.5958999991416931, 'test/loss': 1.7701454162597656, 'test/num_examples': 10000, 'score': 64739.67565011978, 'total_duration': 71854.72164583206, 'accumulated_submission_time': 64739.67565011978, 'accumulated_eval_time': 7100.193810462952, 'accumulated_logging_time': 6.996117830276489}
I0204 13:49:49.487000 139923868813056 logging_writer.py:48] [139301] accumulated_eval_time=7100.193810, accumulated_logging_time=6.996118, accumulated_submission_time=64739.675650, global_step=139301, preemption_count=0, score=64739.675650, test/accuracy=0.595900, test/loss=1.770145, test/num_examples=10000, total_duration=71854.721646, train/accuracy=0.790117, train/loss=0.814143, validation/accuracy=0.717980, validation/loss=1.138425, validation/num_examples=50000
I0204 13:50:32.091663 139923852027648 logging_writer.py:48] [139400] global_step=139400, grad_norm=1.9240540266036987, loss=4.013544082641602
I0204 13:51:19.072756 139923868813056 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.444289445877075, loss=3.600393056869507
I0204 13:52:06.199918 139923852027648 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.263777017593384, loss=1.7393968105316162
I0204 13:52:53.274011 139923868813056 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.2988245487213135, loss=1.9417294263839722
I0204 13:53:40.217371 139923852027648 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.132805347442627, loss=1.7709404230117798
I0204 13:54:27.371510 139923868813056 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.1047167778015137, loss=1.5998921394348145
I0204 13:55:14.451086 139923852027648 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.040498971939087, loss=1.6788839101791382
I0204 13:56:01.272135 139923868813056 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.010824203491211, loss=2.0854880809783936
I0204 13:56:48.527732 139923852027648 logging_writer.py:48] [140200] global_step=140200, grad_norm=1.9287641048431396, loss=2.5483040809631348
I0204 13:56:49.617081 140085747812160 spec.py:321] Evaluating on the training split.
I0204 13:57:00.354127 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 13:57:37.466884 140085747812160 spec.py:349] Evaluating on the test split.
I0204 13:57:39.113593 140085747812160 submission_runner.py:408] Time since start: 72324.39s, 	Step: 140204, 	{'train/accuracy': 0.8003124594688416, 'train/loss': 0.7800408601760864, 'validation/accuracy': 0.717739999294281, 'validation/loss': 1.1377885341644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.7611976861953735, 'test/num_examples': 10000, 'score': 65159.74625015259, 'total_duration': 72324.3936612606, 'accumulated_submission_time': 65159.74625015259, 'accumulated_eval_time': 7149.690311908722, 'accumulated_logging_time': 7.050333499908447}
I0204 13:57:39.153417 139923868813056 logging_writer.py:48] [140204] accumulated_eval_time=7149.690312, accumulated_logging_time=7.050333, accumulated_submission_time=65159.746250, global_step=140204, preemption_count=0, score=65159.746250, test/accuracy=0.599600, test/loss=1.761198, test/num_examples=10000, total_duration=72324.393661, train/accuracy=0.800312, train/loss=0.780041, validation/accuracy=0.717740, validation/loss=1.137789, validation/num_examples=50000
I0204 13:58:20.193599 139923852027648 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.1763594150543213, loss=1.7938371896743774
I0204 13:59:07.157724 139923868813056 logging_writer.py:48] [140400] global_step=140400, grad_norm=1.8666023015975952, loss=2.2774550914764404
I0204 13:59:54.113274 139923852027648 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.0062522888183594, loss=3.3440070152282715
I0204 14:00:40.854096 139923868813056 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.337984323501587, loss=1.723784327507019
I0204 14:01:27.945211 139923852027648 logging_writer.py:48] [140700] global_step=140700, grad_norm=1.9030098915100098, loss=1.6677745580673218
I0204 14:02:15.056543 139923868813056 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.2709858417510986, loss=1.7691935300827026
I0204 14:03:02.177104 139923852027648 logging_writer.py:48] [140900] global_step=140900, grad_norm=1.9167044162750244, loss=1.702165126800537
I0204 14:03:49.023012 139923868813056 logging_writer.py:48] [141000] global_step=141000, grad_norm=2.2178430557250977, loss=2.196312427520752
I0204 14:04:35.996912 139923852027648 logging_writer.py:48] [141100] global_step=141100, grad_norm=2.096381664276123, loss=4.098793983459473
I0204 14:04:39.421313 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:04:49.956909 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:05:22.732181 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:05:24.371452 140085747812160 submission_runner.py:408] Time since start: 72789.65s, 	Step: 141109, 	{'train/accuracy': 0.7925195097923279, 'train/loss': 0.8001589179039001, 'validation/accuracy': 0.7177000045776367, 'validation/loss': 1.1277657747268677, 'validation/num_examples': 50000, 'test/accuracy': 0.5931000113487244, 'test/loss': 1.7584959268569946, 'test/num_examples': 10000, 'score': 65579.95306015015, 'total_duration': 72789.65150928497, 'accumulated_submission_time': 65579.95306015015, 'accumulated_eval_time': 7194.6404457092285, 'accumulated_logging_time': 7.10044264793396}
I0204 14:05:24.412458 139923868813056 logging_writer.py:48] [141109] accumulated_eval_time=7194.640446, accumulated_logging_time=7.100443, accumulated_submission_time=65579.953060, global_step=141109, preemption_count=0, score=65579.953060, test/accuracy=0.593100, test/loss=1.758496, test/num_examples=10000, total_duration=72789.651509, train/accuracy=0.792520, train/loss=0.800159, validation/accuracy=0.717700, validation/loss=1.127766, validation/num_examples=50000
I0204 14:06:03.256432 139923852027648 logging_writer.py:48] [141200] global_step=141200, grad_norm=1.8133244514465332, loss=3.1000866889953613
I0204 14:06:49.725926 139923868813056 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.0210585594177246, loss=3.843812942504883
I0204 14:07:37.200731 139923852027648 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.1175363063812256, loss=2.5355653762817383
I0204 14:08:24.043660 139923868813056 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.0001189708709717, loss=2.1262779235839844
I0204 14:09:10.886059 139923852027648 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.3166110515594482, loss=3.682342529296875
I0204 14:09:58.018889 139923868813056 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.1167263984680176, loss=3.760849952697754
I0204 14:10:44.750242 139923852027648 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.075335741043091, loss=1.6392961740493774
I0204 14:11:31.705131 139923868813056 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.3066699504852295, loss=4.305165767669678
I0204 14:12:18.691466 139923852027648 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.2773756980895996, loss=1.7724237442016602
I0204 14:12:24.480336 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:12:35.673655 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:13:12.641859 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:13:14.279969 140085747812160 submission_runner.py:408] Time since start: 73259.56s, 	Step: 142014, 	{'train/accuracy': 0.8015038967132568, 'train/loss': 0.7711244821548462, 'validation/accuracy': 0.722599983215332, 'validation/loss': 1.1164039373397827, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.7380754947662354, 'test/num_examples': 10000, 'score': 65999.96157240868, 'total_duration': 73259.56003165245, 'accumulated_submission_time': 65999.96157240868, 'accumulated_eval_time': 7244.440073251724, 'accumulated_logging_time': 7.150071620941162}
I0204 14:13:14.320732 139923868813056 logging_writer.py:48] [142014] accumulated_eval_time=7244.440073, accumulated_logging_time=7.150072, accumulated_submission_time=65999.961572, global_step=142014, preemption_count=0, score=65999.961572, test/accuracy=0.600500, test/loss=1.738075, test/num_examples=10000, total_duration=73259.560032, train/accuracy=0.801504, train/loss=0.771124, validation/accuracy=0.722600, validation/loss=1.116404, validation/num_examples=50000
I0204 14:13:50.878735 139923852027648 logging_writer.py:48] [142100] global_step=142100, grad_norm=1.9539694786071777, loss=2.8376288414001465
I0204 14:14:37.453655 139923868813056 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.07275128364563, loss=1.74772047996521
I0204 14:15:24.971489 139923852027648 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.001568078994751, loss=3.086057662963867
I0204 14:16:12.197585 139923868813056 logging_writer.py:48] [142400] global_step=142400, grad_norm=1.9314665794372559, loss=1.8925124406814575
I0204 14:16:59.052630 139923852027648 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.1218254566192627, loss=1.5268778800964355
I0204 14:17:46.034698 139923868813056 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.310873031616211, loss=1.757173776626587
I0204 14:18:33.448883 139923852027648 logging_writer.py:48] [142700] global_step=142700, grad_norm=1.9853194952011108, loss=2.2146875858306885
I0204 14:19:20.782088 139923868813056 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.116372585296631, loss=2.806339979171753
I0204 14:20:08.127434 139923852027648 logging_writer.py:48] [142900] global_step=142900, grad_norm=2.4252333641052246, loss=1.8022650480270386
I0204 14:20:14.500168 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:20:25.276421 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:21:01.432246 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:21:03.078637 140085747812160 submission_runner.py:408] Time since start: 73728.36s, 	Step: 142915, 	{'train/accuracy': 0.8013671636581421, 'train/loss': 0.7719994187355042, 'validation/accuracy': 0.721340000629425, 'validation/loss': 1.121065616607666, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.7464487552642822, 'test/num_examples': 10000, 'score': 66420.08053159714, 'total_duration': 73728.358700037, 'accumulated_submission_time': 66420.08053159714, 'accumulated_eval_time': 7293.018522024155, 'accumulated_logging_time': 7.19990348815918}
I0204 14:21:03.119523 139923868813056 logging_writer.py:48] [142915] accumulated_eval_time=7293.018522, accumulated_logging_time=7.199903, accumulated_submission_time=66420.080532, global_step=142915, preemption_count=0, score=66420.080532, test/accuracy=0.599900, test/loss=1.746449, test/num_examples=10000, total_duration=73728.358700, train/accuracy=0.801367, train/loss=0.771999, validation/accuracy=0.721340, validation/loss=1.121066, validation/num_examples=50000
I0204 14:21:39.084362 139923852027648 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.134497880935669, loss=3.9281558990478516
I0204 14:22:25.936365 139923868813056 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.1243062019348145, loss=1.6326526403427124
I0204 14:23:13.092419 139923852027648 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.198289155960083, loss=1.7625032663345337
I0204 14:24:00.088608 139923868813056 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.1813840866088867, loss=4.134807109832764
I0204 14:24:47.061711 139923852027648 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.2625694274902344, loss=1.7926990985870361
I0204 14:25:34.215712 139923868813056 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.175215721130371, loss=1.8548452854156494
I0204 14:26:21.257331 139923852027648 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.1225173473358154, loss=2.3575809001922607
I0204 14:27:08.262247 139923868813056 logging_writer.py:48] [143700] global_step=143700, grad_norm=1.9908874034881592, loss=2.8643062114715576
I0204 14:27:55.075035 139923852027648 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.707857608795166, loss=1.766823172569275
I0204 14:28:03.260297 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:28:14.065544 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:28:49.770019 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:28:51.404671 140085747812160 submission_runner.py:408] Time since start: 74196.68s, 	Step: 143819, 	{'train/accuracy': 0.8026562333106995, 'train/loss': 0.7779040932655334, 'validation/accuracy': 0.7221399545669556, 'validation/loss': 1.1373088359832764, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.7647473812103271, 'test/num_examples': 10000, 'score': 66840.15920996666, 'total_duration': 74196.6847281456, 'accumulated_submission_time': 66840.15920996666, 'accumulated_eval_time': 7341.1628839969635, 'accumulated_logging_time': 7.251514196395874}
I0204 14:28:51.445262 139923868813056 logging_writer.py:48] [143819] accumulated_eval_time=7341.162884, accumulated_logging_time=7.251514, accumulated_submission_time=66840.159210, global_step=143819, preemption_count=0, score=66840.159210, test/accuracy=0.599300, test/loss=1.764747, test/num_examples=10000, total_duration=74196.684728, train/accuracy=0.802656, train/loss=0.777904, validation/accuracy=0.722140, validation/loss=1.137309, validation/num_examples=50000
I0204 14:29:25.783943 139923852027648 logging_writer.py:48] [143900] global_step=143900, grad_norm=1.8852503299713135, loss=2.9991886615753174
I0204 14:30:12.080471 139923868813056 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.478135108947754, loss=1.6429592370986938
I0204 14:30:59.141736 139923852027648 logging_writer.py:48] [144100] global_step=144100, grad_norm=1.9453684091567993, loss=3.929771900177002
I0204 14:31:45.912755 139923868813056 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.3400862216949463, loss=1.780128002166748
I0204 14:32:33.280343 139923852027648 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.146013021469116, loss=1.625744104385376
I0204 14:33:20.364659 139923868813056 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.1876821517944336, loss=1.7342532873153687
I0204 14:34:07.242933 139923852027648 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.314025402069092, loss=1.74281644821167
I0204 14:34:54.223263 139923868813056 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.4542524814605713, loss=1.7342338562011719
I0204 14:35:40.985831 139923852027648 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.273207187652588, loss=1.688590168952942
I0204 14:35:51.477062 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:36:02.091868 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:36:33.836443 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:36:35.484373 140085747812160 submission_runner.py:408] Time since start: 74660.76s, 	Step: 144724, 	{'train/accuracy': 0.7992382645606995, 'train/loss': 0.7773339748382568, 'validation/accuracy': 0.7215799689292908, 'validation/loss': 1.1161545515060425, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.7557260990142822, 'test/num_examples': 10000, 'score': 67260.12916207314, 'total_duration': 74660.76443743706, 'accumulated_submission_time': 67260.12916207314, 'accumulated_eval_time': 7385.170194864273, 'accumulated_logging_time': 7.303040027618408}
I0204 14:36:35.523650 139923868813056 logging_writer.py:48] [144724] accumulated_eval_time=7385.170195, accumulated_logging_time=7.303040, accumulated_submission_time=67260.129162, global_step=144724, preemption_count=0, score=67260.129162, test/accuracy=0.598100, test/loss=1.755726, test/num_examples=10000, total_duration=74660.764437, train/accuracy=0.799238, train/loss=0.777334, validation/accuracy=0.721580, validation/loss=1.116155, validation/num_examples=50000
I0204 14:37:07.618565 139923852027648 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.2848422527313232, loss=2.2700085639953613
I0204 14:37:54.418966 139923868813056 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.4284098148345947, loss=1.7074503898620605
I0204 14:38:41.364647 139923852027648 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.055778980255127, loss=2.7483291625976562
I0204 14:39:28.246919 139923868813056 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.42909836769104, loss=1.8958269357681274
I0204 14:40:15.296754 139923852027648 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.312082052230835, loss=1.6871951818466187
I0204 14:41:02.168107 139923868813056 logging_writer.py:48] [145300] global_step=145300, grad_norm=2.0101003646850586, loss=1.6586973667144775
I0204 14:41:48.974048 139923852027648 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.1715691089630127, loss=1.7260661125183105
I0204 14:42:36.127268 139923868813056 logging_writer.py:48] [145500] global_step=145500, grad_norm=1.9792542457580566, loss=1.5981543064117432
I0204 14:43:23.162298 139923852027648 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.0794618129730225, loss=3.860625743865967
I0204 14:43:35.510243 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:43:46.349014 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:44:21.513297 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:44:23.157571 140085747812160 submission_runner.py:408] Time since start: 75128.44s, 	Step: 145628, 	{'train/accuracy': 0.80517578125, 'train/loss': 0.7599520683288574, 'validation/accuracy': 0.7254599928855896, 'validation/loss': 1.1081088781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.739145040512085, 'test/num_examples': 10000, 'score': 67680.05412721634, 'total_duration': 75128.43763279915, 'accumulated_submission_time': 67680.05412721634, 'accumulated_eval_time': 7432.81751871109, 'accumulated_logging_time': 7.352954149246216}
I0204 14:44:23.199861 139923868813056 logging_writer.py:48] [145628] accumulated_eval_time=7432.817519, accumulated_logging_time=7.352954, accumulated_submission_time=67680.054127, global_step=145628, preemption_count=0, score=67680.054127, test/accuracy=0.605100, test/loss=1.739145, test/num_examples=10000, total_duration=75128.437633, train/accuracy=0.805176, train/loss=0.759952, validation/accuracy=0.725460, validation/loss=1.108109, validation/num_examples=50000
I0204 14:44:53.179423 139923852027648 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.2456276416778564, loss=1.5612637996673584
I0204 14:45:39.783317 139923868813056 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.1708638668060303, loss=3.0653250217437744
I0204 14:46:26.763030 139923852027648 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.097752571105957, loss=3.503218650817871
I0204 14:47:13.437382 139923868813056 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.267463207244873, loss=1.863659143447876
I0204 14:48:00.290035 139923852027648 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.278824806213379, loss=1.6885288953781128
I0204 14:48:47.090250 139923868813056 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.3135201930999756, loss=1.620496392250061
I0204 14:49:33.895932 139923852027648 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.1387484073638916, loss=2.036123037338257
I0204 14:50:21.000474 139923868813056 logging_writer.py:48] [146400] global_step=146400, grad_norm=1.9863897562026978, loss=2.465761423110962
I0204 14:51:07.910270 139923852027648 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.277794122695923, loss=4.200492858886719
I0204 14:51:23.576227 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:51:34.097909 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:52:06.444599 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:52:08.088031 140085747812160 submission_runner.py:408] Time since start: 75593.37s, 	Step: 146535, 	{'train/accuracy': 0.8170703053474426, 'train/loss': 0.7173280119895935, 'validation/accuracy': 0.7281399965286255, 'validation/loss': 1.1051756143569946, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7289841175079346, 'test/num_examples': 10000, 'score': 68100.3693766594, 'total_duration': 75593.36808228493, 'accumulated_submission_time': 68100.3693766594, 'accumulated_eval_time': 7477.329308271408, 'accumulated_logging_time': 7.4053473472595215}
I0204 14:52:08.130915 139923868813056 logging_writer.py:48] [146535] accumulated_eval_time=7477.329308, accumulated_logging_time=7.405347, accumulated_submission_time=68100.369377, global_step=146535, preemption_count=0, score=68100.369377, test/accuracy=0.603300, test/loss=1.728984, test/num_examples=10000, total_duration=75593.368082, train/accuracy=0.817070, train/loss=0.717328, validation/accuracy=0.728140, validation/loss=1.105176, validation/num_examples=50000
I0204 14:52:35.228691 139923852027648 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.6657185554504395, loss=1.758439064025879
I0204 14:53:21.797322 139923868813056 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.565392017364502, loss=1.6969187259674072
I0204 14:54:08.987068 139923852027648 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.517544984817505, loss=4.198935031890869
I0204 14:54:55.866448 139923868813056 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.213761568069458, loss=2.1045780181884766
I0204 14:55:42.698885 139923852027648 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.229879379272461, loss=1.6569513082504272
I0204 14:56:29.622250 139923868813056 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.3407957553863525, loss=1.7619973421096802
I0204 14:57:16.433881 139923852027648 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.212890148162842, loss=3.260828971862793
I0204 14:58:03.422976 139923868813056 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.688603401184082, loss=1.6158121824264526
I0204 14:58:50.498866 139923852027648 logging_writer.py:48] [147400] global_step=147400, grad_norm=2.0830445289611816, loss=3.0651888847351074
I0204 14:59:08.342682 140085747812160 spec.py:321] Evaluating on the training split.
I0204 14:59:18.795272 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 14:59:53.026164 140085747812160 spec.py:349] Evaluating on the test split.
I0204 14:59:54.668281 140085747812160 submission_runner.py:408] Time since start: 76059.95s, 	Step: 147440, 	{'train/accuracy': 0.8044531345367432, 'train/loss': 0.7989120483398438, 'validation/accuracy': 0.7260000109672546, 'validation/loss': 1.1396101713180542, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.774980902671814, 'test/num_examples': 10000, 'score': 68520.51922082901, 'total_duration': 76059.94834542274, 'accumulated_submission_time': 68520.51922082901, 'accumulated_eval_time': 7523.654905796051, 'accumulated_logging_time': 7.458659648895264}
I0204 14:59:54.707524 139923868813056 logging_writer.py:48] [147440] accumulated_eval_time=7523.654906, accumulated_logging_time=7.458660, accumulated_submission_time=68520.519221, global_step=147440, preemption_count=0, score=68520.519221, test/accuracy=0.601900, test/loss=1.774981, test/num_examples=10000, total_duration=76059.948345, train/accuracy=0.804453, train/loss=0.798912, validation/accuracy=0.726000, validation/loss=1.139610, validation/num_examples=50000
I0204 15:00:19.748260 139923852027648 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.216217279434204, loss=2.607876777648926
I0204 15:01:05.322166 139923868813056 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.2562036514282227, loss=1.6628528833389282
I0204 15:01:52.617560 139923852027648 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.1793479919433594, loss=2.0638511180877686
I0204 15:02:39.471175 139923868813056 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.2798781394958496, loss=1.649177074432373
I0204 15:03:26.317565 139923852027648 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.524259567260742, loss=1.6377772092819214
I0204 15:04:13.081258 139923868813056 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.9992570877075195, loss=1.7017987966537476
I0204 15:04:59.790643 139923852027648 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.333249092102051, loss=1.9752355813980103
I0204 15:05:46.593025 139923868813056 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.2583045959472656, loss=1.7916103601455688
I0204 15:06:33.556318 139923852027648 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.369076728820801, loss=2.0051870346069336
I0204 15:06:54.820336 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:07:05.424273 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:07:37.375472 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:07:39.008582 140085747812160 submission_runner.py:408] Time since start: 76524.29s, 	Step: 148347, 	{'train/accuracy': 0.8073046803474426, 'train/loss': 0.7472729086875916, 'validation/accuracy': 0.7283799648284912, 'validation/loss': 1.0935417413711548, 'validation/num_examples': 50000, 'test/accuracy': 0.6075000166893005, 'test/loss': 1.7274893522262573, 'test/num_examples': 10000, 'score': 68940.56979131699, 'total_duration': 76524.28863739967, 'accumulated_submission_time': 68940.56979131699, 'accumulated_eval_time': 7567.843139410019, 'accumulated_logging_time': 7.50982141494751}
I0204 15:07:39.051178 139923868813056 logging_writer.py:48] [148347] accumulated_eval_time=7567.843139, accumulated_logging_time=7.509821, accumulated_submission_time=68940.569791, global_step=148347, preemption_count=0, score=68940.569791, test/accuracy=0.607500, test/loss=1.727489, test/num_examples=10000, total_duration=76524.288637, train/accuracy=0.807305, train/loss=0.747273, validation/accuracy=0.728380, validation/loss=1.093542, validation/num_examples=50000
I0204 15:08:01.233682 139923852027648 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.1995902061462402, loss=3.1363673210144043
I0204 15:08:46.568040 139923868813056 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.266193389892578, loss=3.568105697631836
I0204 15:09:33.787505 139923852027648 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.227429151535034, loss=1.6388726234436035
I0204 15:10:20.707862 139923868813056 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.405212163925171, loss=1.5416605472564697
I0204 15:11:07.786893 139923852027648 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.2030904293060303, loss=4.072867393493652
I0204 15:11:55.041466 139923868813056 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.078599214553833, loss=1.8576741218566895
I0204 15:12:42.431916 139923852027648 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.750922441482544, loss=1.5984472036361694
I0204 15:13:29.429518 139923868813056 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.498023748397827, loss=1.6604945659637451
I0204 15:14:16.404580 139923852027648 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.4690864086151123, loss=1.6006364822387695
I0204 15:14:39.198477 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:14:49.828721 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:15:21.165948 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:15:22.810136 140085747812160 submission_runner.py:408] Time since start: 76988.09s, 	Step: 149250, 	{'train/accuracy': 0.81214839220047, 'train/loss': 0.7340012192726135, 'validation/accuracy': 0.7254999876022339, 'validation/loss': 1.1147717237472534, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.7510703802108765, 'test/num_examples': 10000, 'score': 69360.65314507484, 'total_duration': 76988.09020090103, 'accumulated_submission_time': 69360.65314507484, 'accumulated_eval_time': 7611.454800367355, 'accumulated_logging_time': 7.564750909805298}
I0204 15:15:22.849727 139923868813056 logging_writer.py:48] [149250] accumulated_eval_time=7611.454800, accumulated_logging_time=7.564751, accumulated_submission_time=69360.653145, global_step=149250, preemption_count=0, score=69360.653145, test/accuracy=0.603200, test/loss=1.751070, test/num_examples=10000, total_duration=76988.090201, train/accuracy=0.812148, train/loss=0.734001, validation/accuracy=0.725500, validation/loss=1.114772, validation/num_examples=50000
I0204 15:15:43.796260 139923852027648 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.314749002456665, loss=2.277249813079834
I0204 15:16:28.913773 139923868813056 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.3720602989196777, loss=4.075362205505371
I0204 15:17:16.002510 139923852027648 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.311338186264038, loss=1.4804351329803467
I0204 15:18:02.800960 139923868813056 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.283851146697998, loss=2.5769124031066895
I0204 15:18:49.594690 139923852027648 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.1730473041534424, loss=2.9253687858581543
I0204 15:19:36.371350 139923868813056 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.0127651691436768, loss=1.7428921461105347
I0204 15:20:23.862345 139923852027648 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.4621474742889404, loss=1.6205371618270874
I0204 15:21:10.436022 139923868813056 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.241767168045044, loss=1.7181655168533325
I0204 15:21:57.200893 139923852027648 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.462934970855713, loss=2.8600103855133057
I0204 15:22:22.900278 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:22:33.807749 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:23:05.751633 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:23:07.388656 140085747812160 submission_runner.py:408] Time since start: 77452.67s, 	Step: 150156, 	{'train/accuracy': 0.8089648485183716, 'train/loss': 0.7477630972862244, 'validation/accuracy': 0.7283399701118469, 'validation/loss': 1.0934544801712036, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.714582920074463, 'test/num_examples': 10000, 'score': 69780.64332485199, 'total_duration': 77452.66871452332, 'accumulated_submission_time': 69780.64332485199, 'accumulated_eval_time': 7655.94317984581, 'accumulated_logging_time': 7.613767862319946}
I0204 15:23:07.432857 139923868813056 logging_writer.py:48] [150156] accumulated_eval_time=7655.943180, accumulated_logging_time=7.613768, accumulated_submission_time=69780.643325, global_step=150156, preemption_count=0, score=69780.643325, test/accuracy=0.611100, test/loss=1.714583, test/num_examples=10000, total_duration=77452.668715, train/accuracy=0.808965, train/loss=0.747763, validation/accuracy=0.728340, validation/loss=1.093454, validation/num_examples=50000
I0204 15:23:26.123710 139923852027648 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.3514504432678223, loss=1.6644264459609985
I0204 15:24:10.807588 139923868813056 logging_writer.py:48] [150300] global_step=150300, grad_norm=2.4512526988983154, loss=3.7000038623809814
I0204 15:24:57.730971 139923852027648 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.2135424613952637, loss=3.1474339962005615
I0204 15:25:44.702051 139923868813056 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.3896896839141846, loss=1.7202999591827393
I0204 15:26:31.641338 139923852027648 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.0731024742126465, loss=1.7465296983718872
I0204 15:27:18.512979 139923868813056 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.2075307369232178, loss=3.8411221504211426
I0204 15:28:05.419220 139923852027648 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.2208127975463867, loss=2.50708270072937
I0204 15:28:52.359877 139923868813056 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.239903211593628, loss=1.543284296989441
I0204 15:29:39.209279 139923852027648 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.43509840965271, loss=1.5411431789398193
I0204 15:30:07.463833 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:30:18.233520 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:30:51.292681 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:30:52.931971 140085747812160 submission_runner.py:408] Time since start: 77918.21s, 	Step: 151062, 	{'train/accuracy': 0.8106249570846558, 'train/loss': 0.7664503455162048, 'validation/accuracy': 0.7301999926567078, 'validation/loss': 1.119113564491272, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.7483646869659424, 'test/num_examples': 10000, 'score': 70200.61243200302, 'total_duration': 77918.21200990677, 'accumulated_submission_time': 70200.61243200302, 'accumulated_eval_time': 7701.411295890808, 'accumulated_logging_time': 7.668791770935059}
I0204 15:30:52.977808 139923868813056 logging_writer.py:48] [151062] accumulated_eval_time=7701.411296, accumulated_logging_time=7.668792, accumulated_submission_time=70200.612432, global_step=151062, preemption_count=0, score=70200.612432, test/accuracy=0.607700, test/loss=1.748365, test/num_examples=10000, total_duration=77918.212010, train/accuracy=0.810625, train/loss=0.766450, validation/accuracy=0.730200, validation/loss=1.119114, validation/num_examples=50000
I0204 15:31:08.984949 139923852027648 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.0199637413024902, loss=1.7466423511505127
I0204 15:31:53.538939 139923868813056 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.402801275253296, loss=1.5583370923995972
I0204 15:32:40.799746 139923852027648 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.2076704502105713, loss=3.8289499282836914
I0204 15:33:28.302636 139923868813056 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.241804838180542, loss=3.415555953979492
I0204 15:34:15.199560 139923852027648 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.464292049407959, loss=2.7305688858032227
I0204 15:35:01.932585 139923868813056 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.6375012397766113, loss=1.676331877708435
I0204 15:35:48.816250 139923852027648 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.931849718093872, loss=1.671284556388855
I0204 15:36:35.845209 139923868813056 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.3460164070129395, loss=2.2001683712005615
I0204 15:37:22.839418 139923852027648 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.3805696964263916, loss=1.5581039190292358
I0204 15:37:53.025291 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:38:03.612509 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:38:38.508679 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:38:40.151882 140085747812160 submission_runner.py:408] Time since start: 78385.43s, 	Step: 151966, 	{'train/accuracy': 0.8160937428474426, 'train/loss': 0.7283310294151306, 'validation/accuracy': 0.7328400015830994, 'validation/loss': 1.0975955724716187, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.7178739309310913, 'test/num_examples': 10000, 'score': 70620.5970556736, 'total_duration': 78385.43194818497, 'accumulated_submission_time': 70620.5970556736, 'accumulated_eval_time': 7748.537898540497, 'accumulated_logging_time': 7.726431369781494}
I0204 15:38:40.195799 139923868813056 logging_writer.py:48] [151966] accumulated_eval_time=7748.537899, accumulated_logging_time=7.726431, accumulated_submission_time=70620.597056, global_step=151966, preemption_count=0, score=70620.597056, test/accuracy=0.616400, test/loss=1.717874, test/num_examples=10000, total_duration=78385.431948, train/accuracy=0.816094, train/loss=0.728331, validation/accuracy=0.732840, validation/loss=1.097596, validation/num_examples=50000
I0204 15:38:54.566699 139923852027648 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.22882080078125, loss=1.6411141157150269
I0204 15:39:38.867104 139923868813056 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.460446834564209, loss=1.7317382097244263
I0204 15:40:25.924684 139923852027648 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.855259895324707, loss=1.6464101076126099
I0204 15:41:12.875940 139923868813056 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.2626662254333496, loss=3.1501617431640625
I0204 15:41:59.611394 139923852027648 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.355576276779175, loss=1.7965872287750244
I0204 15:42:46.598707 139923868813056 logging_writer.py:48] [152500] global_step=152500, grad_norm=2.546170473098755, loss=1.8497858047485352
I0204 15:43:33.516242 139923852027648 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.617598056793213, loss=2.0191986560821533
I0204 15:44:20.731338 139923868813056 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.3265604972839355, loss=1.520269751548767
I0204 15:45:07.627504 139923852027648 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.324989080429077, loss=1.6929033994674683
I0204 15:45:40.601763 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:45:51.161067 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:46:21.875846 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:46:23.515661 140085747812160 submission_runner.py:408] Time since start: 78848.80s, 	Step: 152872, 	{'train/accuracy': 0.8132226467132568, 'train/loss': 0.7306903004646301, 'validation/accuracy': 0.7350199818611145, 'validation/loss': 1.078985333442688, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.705723762512207, 'test/num_examples': 10000, 'score': 71040.94093084335, 'total_duration': 78848.79572200775, 'accumulated_submission_time': 71040.94093084335, 'accumulated_eval_time': 7791.45180106163, 'accumulated_logging_time': 7.7803356647491455}
I0204 15:46:23.560192 139923868813056 logging_writer.py:48] [152872] accumulated_eval_time=7791.451801, accumulated_logging_time=7.780336, accumulated_submission_time=71040.940931, global_step=152872, preemption_count=0, score=71040.940931, test/accuracy=0.609900, test/loss=1.705724, test/num_examples=10000, total_duration=78848.795722, train/accuracy=0.813223, train/loss=0.730690, validation/accuracy=0.735020, validation/loss=1.078985, validation/num_examples=50000
I0204 15:46:35.475908 139923852027648 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.4756319522857666, loss=3.6866750717163086
I0204 15:47:19.788456 139923868813056 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.2408652305603027, loss=2.961341142654419
I0204 15:48:07.043742 139923852027648 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.4255473613739014, loss=3.4870448112487793
I0204 15:48:53.929481 139923868813056 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.4715025424957275, loss=1.737502932548523
I0204 15:49:40.879954 139923852027648 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.6198699474334717, loss=2.224900245666504
I0204 15:50:27.842136 139923868813056 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.4728174209594727, loss=1.5294511318206787
I0204 15:51:14.821424 139923852027648 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.3386027812957764, loss=2.741339921951294
I0204 15:52:01.646409 139923868813056 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.319334030151367, loss=1.6270115375518799
I0204 15:52:48.959815 139923852027648 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.461895704269409, loss=1.6643770933151245
I0204 15:53:23.628233 140085747812160 spec.py:321] Evaluating on the training split.
I0204 15:53:34.056331 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 15:54:09.414874 140085747812160 spec.py:349] Evaluating on the test split.
I0204 15:54:11.059727 140085747812160 submission_runner.py:408] Time since start: 79316.34s, 	Step: 153775, 	{'train/accuracy': 0.81068354845047, 'train/loss': 0.7280431389808655, 'validation/accuracy': 0.7346599698066711, 'validation/loss': 1.074668526649475, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.7089613676071167, 'test/num_examples': 10000, 'score': 71460.94529819489, 'total_duration': 79316.3397808075, 'accumulated_submission_time': 71460.94529819489, 'accumulated_eval_time': 7838.883292198181, 'accumulated_logging_time': 7.837963819503784}
I0204 15:54:11.100770 139923868813056 logging_writer.py:48] [153775] accumulated_eval_time=7838.883292, accumulated_logging_time=7.837964, accumulated_submission_time=71460.945298, global_step=153775, preemption_count=0, score=71460.945298, test/accuracy=0.607600, test/loss=1.708961, test/num_examples=10000, total_duration=79316.339781, train/accuracy=0.810684, train/loss=0.728043, validation/accuracy=0.734660, validation/loss=1.074669, validation/num_examples=50000
I0204 15:54:21.779830 139923852027648 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.951817035675049, loss=2.604203701019287
I0204 15:55:06.112209 139923868813056 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.8206629753112793, loss=1.4994282722473145
I0204 15:55:52.777602 139923852027648 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.491765260696411, loss=1.561575174331665
I0204 15:56:39.637944 139923868813056 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.493286609649658, loss=4.009260177612305
I0204 15:57:26.429826 139923852027648 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.483126163482666, loss=1.7695424556732178
I0204 15:58:13.234554 139923868813056 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.3717448711395264, loss=1.966151237487793
I0204 15:59:00.126578 139923852027648 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.773163318634033, loss=1.5140262842178345
I0204 15:59:47.016553 139923868813056 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.3522119522094727, loss=2.330956220626831
I0204 16:00:33.781176 139923852027648 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.582669973373413, loss=1.5641871690750122
I0204 16:01:11.317060 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:01:21.820871 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:01:58.289990 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:01:59.923944 140085747812160 submission_runner.py:408] Time since start: 79785.20s, 	Step: 154682, 	{'train/accuracy': 0.8229491710662842, 'train/loss': 0.6811491250991821, 'validation/accuracy': 0.7339999675750732, 'validation/loss': 1.0581152439117432, 'validation/num_examples': 50000, 'test/accuracy': 0.6127000451087952, 'test/loss': 1.691803216934204, 'test/num_examples': 10000, 'score': 71881.10067653656, 'total_duration': 79785.20401096344, 'accumulated_submission_time': 71881.10067653656, 'accumulated_eval_time': 7887.490196943283, 'accumulated_logging_time': 7.888981103897095}
I0204 16:01:59.967312 139923868813056 logging_writer.py:48] [154682] accumulated_eval_time=7887.490197, accumulated_logging_time=7.888981, accumulated_submission_time=71881.100677, global_step=154682, preemption_count=0, score=71881.100677, test/accuracy=0.612700, test/loss=1.691803, test/num_examples=10000, total_duration=79785.204011, train/accuracy=0.822949, train/loss=0.681149, validation/accuracy=0.734000, validation/loss=1.058115, validation/num_examples=50000
I0204 16:02:07.771153 139923852027648 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.5415308475494385, loss=1.5967637300491333
I0204 16:02:51.463437 139923868813056 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.3001301288604736, loss=1.8472275733947754
I0204 16:03:38.610317 139923852027648 logging_writer.py:48] [154900] global_step=154900, grad_norm=2.636831283569336, loss=1.7452951669692993
I0204 16:04:25.639886 139923868813056 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.6931958198547363, loss=2.2654430866241455
I0204 16:05:12.785874 139923852027648 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.485895872116089, loss=1.769606590270996
I0204 16:05:59.841310 139923868813056 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.481534719467163, loss=1.6774533987045288
I0204 16:06:46.699995 139923852027648 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.438530206680298, loss=2.2164320945739746
I0204 16:07:33.632606 139923868813056 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.524606466293335, loss=1.6541759967803955
I0204 16:08:20.691623 139923852027648 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.429861068725586, loss=4.07816743850708
I0204 16:09:00.237123 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:09:10.900872 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:09:46.402019 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:09:48.047226 140085747812160 submission_runner.py:408] Time since start: 80253.33s, 	Step: 155586, 	{'train/accuracy': 0.8204687237739563, 'train/loss': 0.7243859767913818, 'validation/accuracy': 0.7372999787330627, 'validation/loss': 1.0768547058105469, 'validation/num_examples': 50000, 'test/accuracy': 0.6158000230789185, 'test/loss': 1.7009111642837524, 'test/num_examples': 10000, 'score': 72301.3064084053, 'total_duration': 80253.32729268074, 'accumulated_submission_time': 72301.3064084053, 'accumulated_eval_time': 7935.300303936005, 'accumulated_logging_time': 7.945384502410889}
I0204 16:09:48.090203 139923868813056 logging_writer.py:48] [155586] accumulated_eval_time=7935.300304, accumulated_logging_time=7.945385, accumulated_submission_time=72301.306408, global_step=155586, preemption_count=0, score=72301.306408, test/accuracy=0.615800, test/loss=1.700911, test/num_examples=10000, total_duration=80253.327293, train/accuracy=0.820469, train/loss=0.724386, validation/accuracy=0.737300, validation/loss=1.076855, validation/num_examples=50000
I0204 16:09:54.258497 139923852027648 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.5629374980926514, loss=1.6359368562698364
I0204 16:10:37.778813 139923868813056 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.346257448196411, loss=2.422240734100342
I0204 16:11:24.844126 139923852027648 logging_writer.py:48] [155800] global_step=155800, grad_norm=2.367727756500244, loss=1.592010736465454
I0204 16:12:11.953321 139923868813056 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.587564706802368, loss=2.131032705307007
I0204 16:12:59.271027 139923852027648 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.616161823272705, loss=1.737860918045044
I0204 16:13:46.274760 139923868813056 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.6275649070739746, loss=1.795947790145874
I0204 16:14:33.406423 139923852027648 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.5736050605773926, loss=1.632233738899231
I0204 16:15:21.015331 139923868813056 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.42651629447937, loss=3.3843390941619873
I0204 16:16:07.879107 139923852027648 logging_writer.py:48] [156400] global_step=156400, grad_norm=2.7916648387908936, loss=4.086505889892578
I0204 16:16:48.164532 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:16:58.930185 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:17:36.877061 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:17:38.513785 140085747812160 submission_runner.py:408] Time since start: 80723.79s, 	Step: 156487, 	{'train/accuracy': 0.8235155940055847, 'train/loss': 0.6817896962165833, 'validation/accuracy': 0.7389400005340576, 'validation/loss': 1.0453300476074219, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6857863664627075, 'test/num_examples': 10000, 'score': 72721.32059788704, 'total_duration': 80723.79385137558, 'accumulated_submission_time': 72721.32059788704, 'accumulated_eval_time': 7985.649563550949, 'accumulated_logging_time': 7.997928142547607}
I0204 16:17:38.555913 139923868813056 logging_writer.py:48] [156487] accumulated_eval_time=7985.649564, accumulated_logging_time=7.997928, accumulated_submission_time=72721.320598, global_step=156487, preemption_count=0, score=72721.320598, test/accuracy=0.615600, test/loss=1.685786, test/num_examples=10000, total_duration=80723.793851, train/accuracy=0.823516, train/loss=0.681790, validation/accuracy=0.738940, validation/loss=1.045330, validation/num_examples=50000
I0204 16:17:44.308027 139923852027648 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.387286424636841, loss=1.5607366561889648
I0204 16:18:27.727838 139923868813056 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.61163067817688, loss=1.7505844831466675
I0204 16:19:14.793731 139923852027648 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.6919775009155273, loss=1.606198787689209
I0204 16:20:01.763857 139923868813056 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.875579595565796, loss=1.5779865980148315
I0204 16:20:48.711567 139923852027648 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.3494861125946045, loss=1.6882003545761108
I0204 16:21:35.481759 139923868813056 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.6019299030303955, loss=1.42131769657135
I0204 16:22:22.398678 139923852027648 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.40631103515625, loss=1.5137120485305786
I0204 16:23:09.555232 139923868813056 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.2130441665649414, loss=3.029310703277588
I0204 16:23:56.307414 139923852027648 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.649146795272827, loss=1.5480964183807373
I0204 16:24:38.904740 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:24:49.252283 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:25:21.993634 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:25:23.629082 140085747812160 submission_runner.py:408] Time since start: 81188.91s, 	Step: 157392, 	{'train/accuracy': 0.8236913681030273, 'train/loss': 0.6922850012779236, 'validation/accuracy': 0.7382199764251709, 'validation/loss': 1.0663707256317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6150000095367432, 'test/loss': 1.6935813426971436, 'test/num_examples': 10000, 'score': 73141.60916376114, 'total_duration': 81188.90914106369, 'accumulated_submission_time': 73141.60916376114, 'accumulated_eval_time': 8030.373905658722, 'accumulated_logging_time': 8.049262046813965}
I0204 16:25:23.675543 139923868813056 logging_writer.py:48] [157392] accumulated_eval_time=8030.373906, accumulated_logging_time=8.049262, accumulated_submission_time=73141.609164, global_step=157392, preemption_count=0, score=73141.609164, test/accuracy=0.615000, test/loss=1.693581, test/num_examples=10000, total_duration=81188.909141, train/accuracy=0.823691, train/loss=0.692285, validation/accuracy=0.738220, validation/loss=1.066371, validation/num_examples=50000
I0204 16:25:27.386384 139923852027648 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.591330051422119, loss=1.5178617238998413
I0204 16:26:10.393581 139923868813056 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.9648823738098145, loss=1.4735560417175293
I0204 16:26:57.146209 139923852027648 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.584941864013672, loss=1.5368751287460327
I0204 16:27:44.183124 139923868813056 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.534377098083496, loss=1.5114127397537231
I0204 16:28:31.065140 139923852027648 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.2928225994110107, loss=1.8482614755630493
I0204 16:29:18.015146 139923868813056 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.6553585529327393, loss=1.5964970588684082
I0204 16:30:04.981109 139923852027648 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.655336380004883, loss=1.571340560913086
I0204 16:30:52.019934 139923868813056 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.8011770248413086, loss=1.55605149269104
I0204 16:31:38.984096 139923852027648 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.4296958446502686, loss=3.1540486812591553
I0204 16:32:23.964912 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:32:34.489106 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:33:10.512623 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:33:12.148255 140085747812160 submission_runner.py:408] Time since start: 81657.43s, 	Step: 158297, 	{'train/accuracy': 0.8206835985183716, 'train/loss': 0.6895581483840942, 'validation/accuracy': 0.7396399974822998, 'validation/loss': 1.0414702892303467, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.6642597913742065, 'test/num_examples': 10000, 'score': 73561.83675098419, 'total_duration': 81657.4282822609, 'accumulated_submission_time': 73561.83675098419, 'accumulated_eval_time': 8078.557241678238, 'accumulated_logging_time': 8.106578588485718}
I0204 16:33:12.198677 139923868813056 logging_writer.py:48] [158297] accumulated_eval_time=8078.557242, accumulated_logging_time=8.106579, accumulated_submission_time=73561.836751, global_step=158297, preemption_count=0, score=73561.836751, test/accuracy=0.620500, test/loss=1.664260, test/num_examples=10000, total_duration=81657.428282, train/accuracy=0.820684, train/loss=0.689558, validation/accuracy=0.739640, validation/loss=1.041470, validation/num_examples=50000
I0204 16:33:13.841392 139923852027648 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.487964630126953, loss=1.730435848236084
I0204 16:33:56.445239 139923868813056 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.6198456287384033, loss=1.6917108297348022
I0204 16:34:42.987266 139923852027648 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.451263666152954, loss=1.824350118637085
I0204 16:35:29.917188 139923868813056 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.502943992614746, loss=3.748594284057617
I0204 16:36:16.734429 139923852027648 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.335186004638672, loss=2.818203926086426
I0204 16:37:03.474389 139923868813056 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.3495702743530273, loss=3.578533411026001
I0204 16:37:50.587384 139923852027648 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.5490152835845947, loss=2.5048635005950928
I0204 16:38:37.393544 139923868813056 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.5020673274993896, loss=1.6320055723190308
I0204 16:39:24.250783 139923852027648 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.5510213375091553, loss=2.0595479011535645
I0204 16:40:11.184912 139923868813056 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.7404932975769043, loss=2.5910024642944336
I0204 16:40:12.291253 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:40:22.819448 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:40:55.950202 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:40:57.592770 140085747812160 submission_runner.py:408] Time since start: 82122.87s, 	Step: 159204, 	{'train/accuracy': 0.82533198595047, 'train/loss': 0.7197791934013367, 'validation/accuracy': 0.7388399839401245, 'validation/loss': 1.0863151550292969, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.7149657011032104, 'test/num_examples': 10000, 'score': 73981.86839866638, 'total_duration': 82122.87282919884, 'accumulated_submission_time': 73981.86839866638, 'accumulated_eval_time': 8123.8587374687195, 'accumulated_logging_time': 8.167153358459473}
I0204 16:40:57.635082 139923852027648 logging_writer.py:48] [159204] accumulated_eval_time=8123.858737, accumulated_logging_time=8.167153, accumulated_submission_time=73981.868399, global_step=159204, preemption_count=0, score=73981.868399, test/accuracy=0.618600, test/loss=1.714966, test/num_examples=10000, total_duration=82122.872829, train/accuracy=0.825332, train/loss=0.719779, validation/accuracy=0.738840, validation/loss=1.086315, validation/num_examples=50000
I0204 16:41:38.701343 139923868813056 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.4242892265319824, loss=1.6523430347442627
I0204 16:42:25.562515 139923852027648 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.5939693450927734, loss=1.672159194946289
I0204 16:43:12.533754 139923868813056 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.806499719619751, loss=1.4740869998931885
I0204 16:43:59.248117 139923852027648 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.5467112064361572, loss=3.2160606384277344
I0204 16:44:45.965722 139923868813056 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.329447031021118, loss=3.228985548019409
I0204 16:45:32.966951 139923852027648 logging_writer.py:48] [159800] global_step=159800, grad_norm=2.4533538818359375, loss=1.5151433944702148
I0204 16:46:19.782517 139923868813056 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.4878108501434326, loss=2.8046092987060547
I0204 16:47:06.906303 139923852027648 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.4433560371398926, loss=3.6749114990234375
I0204 16:47:53.760046 139923868813056 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.444871664047241, loss=3.096984624862671
I0204 16:47:57.622426 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:48:08.277706 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:48:40.497639 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:48:42.142321 140085747812160 submission_runner.py:408] Time since start: 82587.42s, 	Step: 160110, 	{'train/accuracy': 0.8278124928474426, 'train/loss': 0.6878633499145508, 'validation/accuracy': 0.7412199974060059, 'validation/loss': 1.0593788623809814, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.684755802154541, 'test/num_examples': 10000, 'score': 74401.79539680481, 'total_duration': 82587.42238020897, 'accumulated_submission_time': 74401.79539680481, 'accumulated_eval_time': 8168.3786380290985, 'accumulated_logging_time': 8.218732833862305}
I0204 16:48:42.184053 139923852027648 logging_writer.py:48] [160110] accumulated_eval_time=8168.378638, accumulated_logging_time=8.218733, accumulated_submission_time=74401.795397, global_step=160110, preemption_count=0, score=74401.795397, test/accuracy=0.618500, test/loss=1.684756, test/num_examples=10000, total_duration=82587.422380, train/accuracy=0.827812, train/loss=0.687863, validation/accuracy=0.741220, validation/loss=1.059379, validation/num_examples=50000
I0204 16:49:21.079798 139923868813056 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.8661038875579834, loss=3.3153915405273438
I0204 16:50:07.612531 139923852027648 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.486194372177124, loss=3.8870389461517334
I0204 16:50:54.601106 139923868813056 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.7260284423828125, loss=1.5966349840164185
I0204 16:51:41.468590 139923852027648 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.432875871658325, loss=1.3459218740463257
I0204 16:52:28.516290 139923868813056 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.560915231704712, loss=3.6953132152557373
I0204 16:53:15.490680 139923852027648 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.803367853164673, loss=1.6300880908966064
I0204 16:54:02.600384 139923868813056 logging_writer.py:48] [160800] global_step=160800, grad_norm=3.1448376178741455, loss=3.3387770652770996
I0204 16:54:49.282544 139923852027648 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.5803749561309814, loss=1.8355013132095337
I0204 16:55:36.188363 139923868813056 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.8331892490386963, loss=1.9931188821792603
I0204 16:55:42.409882 140085747812160 spec.py:321] Evaluating on the training split.
I0204 16:55:53.013500 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 16:56:29.942455 140085747812160 spec.py:349] Evaluating on the test split.
I0204 16:56:31.579740 140085747812160 submission_runner.py:408] Time since start: 83056.86s, 	Step: 161015, 	{'train/accuracy': 0.82972651720047, 'train/loss': 0.6675693392753601, 'validation/accuracy': 0.7438399791717529, 'validation/loss': 1.0392054319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.6687285900115967, 'test/num_examples': 10000, 'score': 74821.9603896141, 'total_duration': 83056.85979914665, 'accumulated_submission_time': 74821.9603896141, 'accumulated_eval_time': 8217.548476457596, 'accumulated_logging_time': 8.271214008331299}
I0204 16:56:31.622456 139923852027648 logging_writer.py:48] [161015] accumulated_eval_time=8217.548476, accumulated_logging_time=8.271214, accumulated_submission_time=74821.960390, global_step=161015, preemption_count=0, score=74821.960390, test/accuracy=0.622000, test/loss=1.668729, test/num_examples=10000, total_duration=83056.859799, train/accuracy=0.829727, train/loss=0.667569, validation/accuracy=0.743840, validation/loss=1.039205, validation/num_examples=50000
I0204 16:57:07.589964 139923868813056 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.3862686157226562, loss=1.4645938873291016
I0204 16:57:54.125090 139923852027648 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.649221181869507, loss=1.5305678844451904
I0204 16:58:41.070478 139923868813056 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.447204351425171, loss=1.587380290031433
I0204 16:59:28.068160 139923852027648 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.6806039810180664, loss=2.2315497398376465
I0204 17:00:14.881814 139923868813056 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.3649768829345703, loss=2.814690351486206
I0204 17:01:01.879244 139923852027648 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.705721378326416, loss=1.535461187362671
I0204 17:01:48.886404 139923868813056 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.020026922225952, loss=1.5081874132156372
I0204 17:02:35.953751 139923852027648 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.542419910430908, loss=1.400242805480957
I0204 17:03:22.955604 139923868813056 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.7272469997406006, loss=3.7463104724884033
I0204 17:03:32.027205 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:03:42.464104 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:04:17.386084 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:04:19.020453 140085747812160 submission_runner.py:408] Time since start: 83524.30s, 	Step: 161921, 	{'train/accuracy': 0.8286718726158142, 'train/loss': 0.672528088092804, 'validation/accuracy': 0.7439000010490417, 'validation/loss': 1.04596745967865, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6684740781784058, 'test/num_examples': 10000, 'score': 75242.30540776253, 'total_duration': 83524.30050992966, 'accumulated_submission_time': 75242.30540776253, 'accumulated_eval_time': 8264.541722774506, 'accumulated_logging_time': 8.323035478591919}
I0204 17:04:19.076736 139923852027648 logging_writer.py:48] [161921] accumulated_eval_time=8264.541723, accumulated_logging_time=8.323035, accumulated_submission_time=75242.305408, global_step=161921, preemption_count=0, score=75242.305408, test/accuracy=0.620600, test/loss=1.668474, test/num_examples=10000, total_duration=83524.300510, train/accuracy=0.828672, train/loss=0.672528, validation/accuracy=0.743900, validation/loss=1.045967, validation/num_examples=50000
I0204 17:04:52.240290 139923868813056 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.6415092945098877, loss=1.6487311124801636
I0204 17:05:39.213330 139923852027648 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.777207851409912, loss=3.923471689224243
I0204 17:06:26.197166 139923868813056 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.8710334300994873, loss=1.678196907043457
I0204 17:07:13.142559 139923852027648 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.930650234222412, loss=1.445612907409668
I0204 17:08:00.033271 139923868813056 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.6138837337493896, loss=1.448103904724121
I0204 17:08:46.898036 139923852027648 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.4990270137786865, loss=1.5483769178390503
I0204 17:09:33.668272 139923868813056 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.7141876220703125, loss=1.621527075767517
I0204 17:10:20.728892 139923852027648 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.7048072814941406, loss=1.4659628868103027
I0204 17:11:07.733852 139923868813056 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.6235671043395996, loss=3.67547869682312
I0204 17:11:19.075200 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:11:29.329773 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:12:02.716511 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:12:04.362236 140085747812160 submission_runner.py:408] Time since start: 83989.64s, 	Step: 162826, 	{'train/accuracy': 0.8308398127555847, 'train/loss': 0.6795992851257324, 'validation/accuracy': 0.743399977684021, 'validation/loss': 1.0521153211593628, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.676736831665039, 'test/num_examples': 10000, 'score': 75662.24251437187, 'total_duration': 83989.64229750633, 'accumulated_submission_time': 75662.24251437187, 'accumulated_eval_time': 8309.82877087593, 'accumulated_logging_time': 8.38948941230774}
I0204 17:12:04.405019 139923852027648 logging_writer.py:48] [162826] accumulated_eval_time=8309.828771, accumulated_logging_time=8.389489, accumulated_submission_time=75662.242514, global_step=162826, preemption_count=0, score=75662.242514, test/accuracy=0.623400, test/loss=1.676737, test/num_examples=10000, total_duration=83989.642298, train/accuracy=0.830840, train/loss=0.679599, validation/accuracy=0.743400, validation/loss=1.052115, validation/num_examples=50000
I0204 17:12:35.335909 139923868813056 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.5934088230133057, loss=3.168388843536377
I0204 17:13:21.927774 139923852027648 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.6659867763519287, loss=1.536267876625061
I0204 17:14:08.913464 139923868813056 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.7985963821411133, loss=2.524224042892456
I0204 17:14:55.658219 139923852027648 logging_writer.py:48] [163200] global_step=163200, grad_norm=2.6645004749298096, loss=1.4611084461212158
I0204 17:15:42.511118 139923868813056 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.713196039199829, loss=1.5984899997711182
I0204 17:16:29.246058 139923852027648 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.57633638381958, loss=1.6789249181747437
I0204 17:17:16.241396 139923868813056 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.0339207649230957, loss=3.0908572673797607
I0204 17:18:03.068033 139923852027648 logging_writer.py:48] [163600] global_step=163600, grad_norm=2.8945648670196533, loss=3.583655595779419
I0204 17:18:50.043134 139923868813056 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.574873685836792, loss=2.549994468688965
I0204 17:19:04.660427 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:19:14.957517 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:19:51.880507 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:19:53.517928 140085747812160 submission_runner.py:408] Time since start: 84458.80s, 	Step: 163733, 	{'train/accuracy': 0.8407226204872131, 'train/loss': 0.6239649653434753, 'validation/accuracy': 0.7476399540901184, 'validation/loss': 1.0164140462875366, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.6446024179458618, 'test/num_examples': 10000, 'score': 76082.43580412865, 'total_duration': 84458.79799222946, 'accumulated_submission_time': 76082.43580412865, 'accumulated_eval_time': 8358.68628025055, 'accumulated_logging_time': 8.442897319793701}
I0204 17:19:53.563008 139923852027648 logging_writer.py:48] [163733] accumulated_eval_time=8358.686280, accumulated_logging_time=8.442897, accumulated_submission_time=76082.435804, global_step=163733, preemption_count=0, score=76082.435804, test/accuracy=0.627600, test/loss=1.644602, test/num_examples=10000, total_duration=84458.797992, train/accuracy=0.840723, train/loss=0.623965, validation/accuracy=0.747640, validation/loss=1.016414, validation/num_examples=50000
I0204 17:20:21.470742 139923868813056 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.564358711242676, loss=1.5431797504425049
I0204 17:21:07.948644 139923852027648 logging_writer.py:48] [163900] global_step=163900, grad_norm=2.8627328872680664, loss=1.5472476482391357
I0204 17:21:54.899456 139923868813056 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.7156474590301514, loss=3.161571979522705
I0204 17:22:41.651217 139923852027648 logging_writer.py:48] [164100] global_step=164100, grad_norm=2.93593430519104, loss=4.0380144119262695
I0204 17:23:28.441097 139923868813056 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.8161723613739014, loss=3.9091310501098633
I0204 17:24:15.292931 139923852027648 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.6028411388397217, loss=3.5557663440704346
I0204 17:25:02.310716 139923868813056 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.9105148315429688, loss=1.6674281358718872
I0204 17:25:49.227682 139923852027648 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.792750358581543, loss=1.469393253326416
I0204 17:26:36.108120 139923868813056 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.7456858158111572, loss=1.47206449508667
I0204 17:26:53.583008 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:27:04.191286 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:27:39.837191 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:27:41.477115 140085747812160 submission_runner.py:408] Time since start: 84926.76s, 	Step: 164639, 	{'train/accuracy': 0.8356835842132568, 'train/loss': 0.6445589065551758, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.0233219861984253, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.6496158838272095, 'test/num_examples': 10000, 'score': 76502.39468336105, 'total_duration': 84926.75717353821, 'accumulated_submission_time': 76502.39468336105, 'accumulated_eval_time': 8406.580387592316, 'accumulated_logging_time': 8.498199224472046}
I0204 17:27:41.521493 139923852027648 logging_writer.py:48] [164639] accumulated_eval_time=8406.580388, accumulated_logging_time=8.498199, accumulated_submission_time=76502.394683, global_step=164639, preemption_count=0, score=76502.394683, test/accuracy=0.626700, test/loss=1.649616, test/num_examples=10000, total_duration=84926.757174, train/accuracy=0.835684, train/loss=0.644559, validation/accuracy=0.746860, validation/loss=1.023322, validation/num_examples=50000
I0204 17:28:06.973516 139923868813056 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.712296962738037, loss=3.2963435649871826
I0204 17:28:52.713820 139923852027648 logging_writer.py:48] [164800] global_step=164800, grad_norm=2.70200252532959, loss=2.157304048538208
I0204 17:29:39.650215 139923868813056 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.5650668144226074, loss=1.8063874244689941
I0204 17:30:26.526788 139923852027648 logging_writer.py:48] [165000] global_step=165000, grad_norm=2.5914742946624756, loss=1.5693761110305786
I0204 17:31:13.346637 139923868813056 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.7139809131622314, loss=3.0876522064208984
I0204 17:32:00.341508 139923852027648 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.7270729541778564, loss=1.6616851091384888
I0204 17:32:47.402020 139923868813056 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.2269084453582764, loss=1.8282915353775024
I0204 17:33:34.211920 139923852027648 logging_writer.py:48] [165400] global_step=165400, grad_norm=2.667860746383667, loss=1.5579872131347656
I0204 17:34:21.027760 139923868813056 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6266610622406006, loss=2.7488293647766113
I0204 17:34:41.760324 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:34:52.216081 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:35:24.838571 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:35:26.483063 140085747812160 submission_runner.py:408] Time since start: 85391.76s, 	Step: 165546, 	{'train/accuracy': 0.8334375023841858, 'train/loss': 0.6440165638923645, 'validation/accuracy': 0.7469399571418762, 'validation/loss': 1.0224330425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.6409838199615479, 'test/num_examples': 10000, 'score': 76922.57128500938, 'total_duration': 85391.76311731339, 'accumulated_submission_time': 76922.57128500938, 'accumulated_eval_time': 8451.303115844727, 'accumulated_logging_time': 8.553112983703613}
I0204 17:35:26.527547 139923852027648 logging_writer.py:48] [165546] accumulated_eval_time=8451.303116, accumulated_logging_time=8.553113, accumulated_submission_time=76922.571285, global_step=165546, preemption_count=0, score=76922.571285, test/accuracy=0.624100, test/loss=1.640984, test/num_examples=10000, total_duration=85391.763117, train/accuracy=0.833438, train/loss=0.644017, validation/accuracy=0.746940, validation/loss=1.022433, validation/num_examples=50000
I0204 17:35:49.109319 139923868813056 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.6849782466888428, loss=3.9360108375549316
I0204 17:36:34.782492 139923852027648 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.8948543071746826, loss=1.5247092247009277
I0204 17:37:22.021449 139923868813056 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.8603038787841797, loss=1.503745675086975
I0204 17:38:08.823132 139923852027648 logging_writer.py:48] [165900] global_step=165900, grad_norm=2.885875940322876, loss=2.0013957023620605
I0204 17:38:55.635651 139923868813056 logging_writer.py:48] [166000] global_step=166000, grad_norm=2.6745927333831787, loss=1.78609037399292
I0204 17:39:42.523883 139923852027648 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.0869808197021484, loss=2.1446194648742676
I0204 17:40:29.500227 139923868813056 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.614994764328003, loss=1.819627046585083
I0204 17:41:16.407139 139923852027648 logging_writer.py:48] [166300] global_step=166300, grad_norm=2.927298069000244, loss=1.7149630784988403
I0204 17:42:03.799994 139923868813056 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.997920274734497, loss=2.1705992221832275
I0204 17:42:26.932369 140085747812160 spec.py:321] Evaluating on the training split.
I0204 17:42:37.366321 140085747812160 spec.py:333] Evaluating on the validation split.
I0204 17:43:14.311621 140085747812160 spec.py:349] Evaluating on the test split.
I0204 17:43:15.950008 140085747812160 submission_runner.py:408] Time since start: 85861.23s, 	Step: 166451, 	{'train/accuracy': 0.8373242020606995, 'train/loss': 0.6317007541656494, 'validation/accuracy': 0.7473799586296082, 'validation/loss': 1.0245435237884521, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.6426352262496948, 'test/num_examples': 10000, 'score': 77342.91505241394, 'total_duration': 85861.23006987572, 'accumulated_submission_time': 77342.91505241394, 'accumulated_eval_time': 8500.320755720139, 'accumulated_logging_time': 8.607258081436157}
I0204 17:43:15.995514 139923852027648 logging_writer.py:48] [166451] accumulated_eval_time=8500.320756, accumulated_logging_time=8.607258, accumulated_submission_time=77342.915052, global_step=166451, preemption_count=0, score=77342.915052, test/accuracy=0.630900, test/loss=1.642635, test/num_examples=10000, total_duration=85861.230070, train/accuracy=0.837324, train/loss=0.631701, validation/accuracy=0.747380, validation/loss=1.024544, validation/num_examples=50000
I0204 17:43:36.526528 139923868813056 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.751465082168579, loss=1.5014407634735107
I0204 17:44:21.382811 139923852027648 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.722888231277466, loss=2.9765782356262207
I0204 17:45:08.246735 139923868813056 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.761916160583496, loss=1.9201632738113403
I0204 17:45:55.108418 139923852027648 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.9164209365844727, loss=1.5794451236724854
I0204 17:46:13.623998 139923868813056 logging_writer.py:48] [166841] global_step=166841, preemption_count=0, score=77520.456350
I0204 17:46:14.322808 140085747812160 checkpoints.py:490] Saving checkpoint at step: 166841
I0204 17:46:15.638568 140085747812160 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5/checkpoint_166841
I0204 17:46:15.665020 140085747812160 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/imagenet_vit_jax/trial_5/checkpoint_166841.
I0204 17:46:16.771515 140085747812160 submission_runner.py:583] Tuning trial 5/5
I0204 17:46:16.771792 140085747812160 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0204 17:46:16.785474 140085747812160 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008203124743886292, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 40.46163988113403, 'total_duration': 69.32669401168823, 'accumulated_submission_time': 40.46163988113403, 'accumulated_eval_time': 28.864896059036255, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (844, {'train/accuracy': 0.03410156071186066, 'train/loss': 5.897230625152588, 'validation/accuracy': 0.029819998890161514, 'validation/loss': 5.968258380889893, 'validation/num_examples': 50000, 'test/accuracy': 0.021900001913309097, 'test/loss': 6.086627006530762, 'test/num_examples': 10000, 'score': 460.644246339798, 'total_duration': 530.1097981929779, 'accumulated_submission_time': 460.644246339798, 'accumulated_eval_time': 69.4005184173584, 'accumulated_logging_time': 0.017852783203125, 'global_step': 844, 'preemption_count': 0}), (1745, {'train/accuracy': 0.07203125208616257, 'train/loss': 5.3030266761779785, 'validation/accuracy': 0.06712000072002411, 'validation/loss': 5.3553972244262695, 'validation/num_examples': 50000, 'test/accuracy': 0.05120000243186951, 'test/loss': 5.565213203430176, 'test/num_examples': 10000, 'score': 880.6717927455902, 'total_duration': 993.2836394309998, 'accumulated_submission_time': 880.6717927455902, 'accumulated_eval_time': 112.46707344055176, 'accumulated_logging_time': 0.04637718200683594, 'global_step': 1745, 'preemption_count': 0}), (2649, {'train/accuracy': 0.1213671863079071, 'train/loss': 4.764578342437744, 'validation/accuracy': 0.11085999757051468, 'validation/loss': 4.83726167678833, 'validation/num_examples': 50000, 'test/accuracy': 0.08620000630617142, 'test/loss': 5.133504390716553, 'test/num_examples': 10000, 'score': 1300.8934905529022, 'total_duration': 1458.8881137371063, 'accumulated_submission_time': 1300.8934905529022, 'accumulated_eval_time': 157.77245998382568, 'accumulated_logging_time': 0.07286334037780762, 'global_step': 2649, 'preemption_count': 0}), (3552, {'train/accuracy': 0.1814257800579071, 'train/loss': 4.194916248321533, 'validation/accuracy': 0.16040000319480896, 'validation/loss': 4.344925403594971, 'validation/num_examples': 50000, 'test/accuracy': 0.12460000813007355, 'test/loss': 4.737356662750244, 'test/num_examples': 10000, 'score': 1720.9098417758942, 'total_duration': 1923.9494013786316, 'accumulated_submission_time': 1720.9098417758942, 'accumulated_eval_time': 202.7385392189026, 'accumulated_logging_time': 0.10111761093139648, 'global_step': 3552, 'preemption_count': 0}), (4456, {'train/accuracy': 0.2257421761751175, 'train/loss': 3.8626036643981934, 'validation/accuracy': 0.20969998836517334, 'validation/loss': 3.9672608375549316, 'validation/num_examples': 50000, 'test/accuracy': 0.15650001168251038, 'test/loss': 4.419209003448486, 'test/num_examples': 10000, 'score': 2141.0544922351837, 'total_duration': 2386.887092113495, 'accumulated_submission_time': 2141.0544922351837, 'accumulated_eval_time': 245.45475339889526, 'accumulated_logging_time': 0.12727046012878418, 'global_step': 4456, 'preemption_count': 0}), (5359, {'train/accuracy': 0.2718164026737213, 'train/loss': 3.5856893062591553, 'validation/accuracy': 0.2515200078487396, 'validation/loss': 3.714376211166382, 'validation/num_examples': 50000, 'test/accuracy': 0.18800000846385956, 'test/loss': 4.178256511688232, 'test/num_examples': 10000, 'score': 2560.998118877411, 'total_duration': 2851.1777324676514, 'accumulated_submission_time': 2560.998118877411, 'accumulated_eval_time': 289.7249255180359, 'accumulated_logging_time': 0.15322399139404297, 'global_step': 5359, 'preemption_count': 0}), (6257, {'train/accuracy': 0.32466796040534973, 'train/loss': 3.229868173599243, 'validation/accuracy': 0.2886599898338318, 'validation/loss': 3.425157070159912, 'validation/num_examples': 50000, 'test/accuracy': 0.22220000624656677, 'test/loss': 3.962292432785034, 'test/num_examples': 10000, 'score': 2980.9549465179443, 'total_duration': 3313.470268011093, 'accumulated_submission_time': 2980.9549465179443, 'accumulated_eval_time': 331.98203206062317, 'accumulated_logging_time': 0.18113970756530762, 'global_step': 6257, 'preemption_count': 0}), (7164, {'train/accuracy': 0.3366992175579071, 'train/loss': 3.113884210586548, 'validation/accuracy': 0.3124600052833557, 'validation/loss': 3.242727041244507, 'validation/num_examples': 50000, 'test/accuracy': 0.23670001327991486, 'test/loss': 3.8153295516967773, 'test/num_examples': 10000, 'score': 3400.9355845451355, 'total_duration': 3776.551196575165, 'accumulated_submission_time': 3400.9355845451355, 'accumulated_eval_time': 375.00476264953613, 'accumulated_logging_time': 0.2081298828125, 'global_step': 7164, 'preemption_count': 0}), (8068, {'train/accuracy': 0.37083983421325684, 'train/loss': 2.9305522441864014, 'validation/accuracy': 0.338919997215271, 'validation/loss': 3.110529899597168, 'validation/num_examples': 50000, 'test/accuracy': 0.25710001587867737, 'test/loss': 3.680340051651001, 'test/num_examples': 10000, 'score': 3821.070417881012, 'total_duration': 4241.460793018341, 'accumulated_submission_time': 3821.070417881012, 'accumulated_eval_time': 419.69977498054504, 'accumulated_logging_time': 0.2372419834136963, 'global_step': 8068, 'preemption_count': 0}), (8974, {'train/accuracy': 0.3893359303474426, 'train/loss': 2.798448085784912, 'validation/accuracy': 0.35621997714042664, 'validation/loss': 2.991145610809326, 'validation/num_examples': 50000, 'test/accuracy': 0.27250000834465027, 'test/loss': 3.5983641147613525, 'test/num_examples': 10000, 'score': 4241.001017093658, 'total_duration': 4700.1675000190735, 'accumulated_submission_time': 4241.001017093658, 'accumulated_eval_time': 458.3977072238922, 'accumulated_logging_time': 0.26456356048583984, 'global_step': 8974, 'preemption_count': 0}), (9879, {'train/accuracy': 0.40507811307907104, 'train/loss': 2.719622850418091, 'validation/accuracy': 0.37549999356269836, 'validation/loss': 2.876948118209839, 'validation/num_examples': 50000, 'test/accuracy': 0.29280000925064087, 'test/loss': 3.4608092308044434, 'test/num_examples': 10000, 'score': 4661.186886072159, 'total_duration': 5162.8368492126465, 'accumulated_submission_time': 4661.186886072159, 'accumulated_eval_time': 500.7986707687378, 'accumulated_logging_time': 0.29593586921691895, 'global_step': 9879, 'preemption_count': 0}), (10783, {'train/accuracy': 0.4206640422344208, 'train/loss': 2.6827948093414307, 'validation/accuracy': 0.3864399790763855, 'validation/loss': 2.8488309383392334, 'validation/num_examples': 50000, 'test/accuracy': 0.3021000027656555, 'test/loss': 3.441816568374634, 'test/num_examples': 10000, 'score': 5081.527356147766, 'total_duration': 5628.701157808304, 'accumulated_submission_time': 5081.527356147766, 'accumulated_eval_time': 546.2405309677124, 'accumulated_logging_time': 0.3265047073364258, 'global_step': 10783, 'preemption_count': 0}), (11688, {'train/accuracy': 0.4415038824081421, 'train/loss': 2.5091471672058105, 'validation/accuracy': 0.405379980802536, 'validation/loss': 2.7154462337493896, 'validation/num_examples': 50000, 'test/accuracy': 0.31060001254081726, 'test/loss': 3.323500633239746, 'test/num_examples': 10000, 'score': 5501.6132843494415, 'total_duration': 6090.867266654968, 'accumulated_submission_time': 5501.6132843494415, 'accumulated_eval_time': 588.2387778759003, 'accumulated_logging_time': 0.35753369331359863, 'global_step': 11688, 'preemption_count': 0}), (12590, {'train/accuracy': 0.4447070360183716, 'train/loss': 2.521016836166382, 'validation/accuracy': 0.4148999750614166, 'validation/loss': 2.6761584281921387, 'validation/num_examples': 50000, 'test/accuracy': 0.3197000026702881, 'test/loss': 3.3049449920654297, 'test/num_examples': 10000, 'score': 5922.011283874512, 'total_duration': 6558.102586507797, 'accumulated_submission_time': 5922.011283874512, 'accumulated_eval_time': 634.9937858581543, 'accumulated_logging_time': 0.3894164562225342, 'global_step': 12590, 'preemption_count': 0}), (13496, {'train/accuracy': 0.4694921672344208, 'train/loss': 2.356694221496582, 'validation/accuracy': 0.435479998588562, 'validation/loss': 2.5276846885681152, 'validation/num_examples': 50000, 'test/accuracy': 0.33570000529289246, 'test/loss': 3.1664600372314453, 'test/num_examples': 10000, 'score': 6342.245297193527, 'total_duration': 7018.754692077637, 'accumulated_submission_time': 6342.245297193527, 'accumulated_eval_time': 675.3277657032013, 'accumulated_logging_time': 0.4220564365386963, 'global_step': 13496, 'preemption_count': 0}), (14400, {'train/accuracy': 0.47019529342651367, 'train/loss': 2.3852572441101074, 'validation/accuracy': 0.43121999502182007, 'validation/loss': 2.579622268676758, 'validation/num_examples': 50000, 'test/accuracy': 0.33650001883506775, 'test/loss': 3.197645664215088, 'test/num_examples': 10000, 'score': 6762.292719364166, 'total_duration': 7483.901390790939, 'accumulated_submission_time': 6762.292719364166, 'accumulated_eval_time': 720.3443946838379, 'accumulated_logging_time': 0.45426154136657715, 'global_step': 14400, 'preemption_count': 0}), (15307, {'train/accuracy': 0.4783593714237213, 'train/loss': 2.331885814666748, 'validation/accuracy': 0.4416399896144867, 'validation/loss': 2.534491539001465, 'validation/num_examples': 50000, 'test/accuracy': 0.34200000762939453, 'test/loss': 3.1562328338623047, 'test/num_examples': 10000, 'score': 7182.435878038406, 'total_duration': 7950.122545957565, 'accumulated_submission_time': 7182.435878038406, 'accumulated_eval_time': 766.3428432941437, 'accumulated_logging_time': 0.48224782943725586, 'global_step': 15307, 'preemption_count': 0}), (16215, {'train/accuracy': 0.4888867139816284, 'train/loss': 2.2850089073181152, 'validation/accuracy': 0.45423999428749084, 'validation/loss': 2.4670143127441406, 'validation/num_examples': 50000, 'test/accuracy': 0.3481000065803528, 'test/loss': 3.092832088470459, 'test/num_examples': 10000, 'score': 7602.47861289978, 'total_duration': 8417.176041603088, 'accumulated_submission_time': 7602.47861289978, 'accumulated_eval_time': 813.2723207473755, 'accumulated_logging_time': 0.5117042064666748, 'global_step': 16215, 'preemption_count': 0}), (17122, {'train/accuracy': 0.49986326694488525, 'train/loss': 2.2155532836914062, 'validation/accuracy': 0.46083998680114746, 'validation/loss': 2.4244303703308105, 'validation/num_examples': 50000, 'test/accuracy': 0.3571000099182129, 'test/loss': 3.0820775032043457, 'test/num_examples': 10000, 'score': 8022.711845397949, 'total_duration': 8880.401599168777, 'accumulated_submission_time': 8022.711845397949, 'accumulated_eval_time': 856.1803793907166, 'accumulated_logging_time': 0.5442097187042236, 'global_step': 17122, 'preemption_count': 0}), (18026, {'train/accuracy': 0.5294336080551147, 'train/loss': 2.071880340576172, 'validation/accuracy': 0.46521997451782227, 'validation/loss': 2.3781328201293945, 'validation/num_examples': 50000, 'test/accuracy': 0.35750001668930054, 'test/loss': 3.0405938625335693, 'test/num_examples': 10000, 'score': 8442.835622787476, 'total_duration': 9348.312663078308, 'accumulated_submission_time': 8442.835622787476, 'accumulated_eval_time': 903.8825159072876, 'accumulated_logging_time': 0.5782530307769775, 'global_step': 18026, 'preemption_count': 0}), (18931, {'train/accuracy': 0.5103710889816284, 'train/loss': 2.1674885749816895, 'validation/accuracy': 0.4759399890899658, 'validation/loss': 2.3467721939086914, 'validation/num_examples': 50000, 'test/accuracy': 0.36880001425743103, 'test/loss': 3.0019314289093018, 'test/num_examples': 10000, 'score': 8862.765635967255, 'total_duration': 9813.793103218079, 'accumulated_submission_time': 8862.765635967255, 'accumulated_eval_time': 949.3529365062714, 'accumulated_logging_time': 0.607285737991333, 'global_step': 18931, 'preemption_count': 0}), (19837, {'train/accuracy': 0.5275976657867432, 'train/loss': 2.005328416824341, 'validation/accuracy': 0.48749998211860657, 'validation/loss': 2.2312562465667725, 'validation/num_examples': 50000, 'test/accuracy': 0.37880000472068787, 'test/loss': 2.899568557739258, 'test/num_examples': 10000, 'score': 9283.02355337143, 'total_duration': 10281.213695764542, 'accumulated_submission_time': 9283.02355337143, 'accumulated_eval_time': 996.4339108467102, 'accumulated_logging_time': 0.6371915340423584, 'global_step': 19837, 'preemption_count': 0}), (20743, {'train/accuracy': 0.5421093702316284, 'train/loss': 2.062912940979004, 'validation/accuracy': 0.4810599982738495, 'validation/loss': 2.356579303741455, 'validation/num_examples': 50000, 'test/accuracy': 0.37370002269744873, 'test/loss': 2.998039484024048, 'test/num_examples': 10000, 'score': 9703.317378759384, 'total_duration': 10739.15369963646, 'accumulated_submission_time': 9703.317378759384, 'accumulated_eval_time': 1033.9986152648926, 'accumulated_logging_time': 0.6677725315093994, 'global_step': 20743, 'preemption_count': 0}), (21646, {'train/accuracy': 0.5312694907188416, 'train/loss': 2.0607404708862305, 'validation/accuracy': 0.4913399815559387, 'validation/loss': 2.2656807899475098, 'validation/num_examples': 50000, 'test/accuracy': 0.38190001249313354, 'test/loss': 2.9162533283233643, 'test/num_examples': 10000, 'score': 10123.384189367294, 'total_duration': 11206.499513149261, 'accumulated_submission_time': 10123.384189367294, 'accumulated_eval_time': 1081.1976935863495, 'accumulated_logging_time': 0.6967108249664307, 'global_step': 21646, 'preemption_count': 0}), (22552, {'train/accuracy': 0.5475195050239563, 'train/loss': 1.9644417762756348, 'validation/accuracy': 0.5002999901771545, 'validation/loss': 2.18509578704834, 'validation/num_examples': 50000, 'test/accuracy': 0.39170002937316895, 'test/loss': 2.8608169555664062, 'test/num_examples': 10000, 'score': 10543.569134950638, 'total_duration': 11671.593814611435, 'accumulated_submission_time': 10543.569134950638, 'accumulated_eval_time': 1126.02326130867, 'accumulated_logging_time': 0.7283682823181152, 'global_step': 22552, 'preemption_count': 0}), (23459, {'train/accuracy': 0.5622265338897705, 'train/loss': 1.8803318738937378, 'validation/accuracy': 0.5024799704551697, 'validation/loss': 2.188140630722046, 'validation/num_examples': 50000, 'test/accuracy': 0.3927000164985657, 'test/loss': 2.830965995788574, 'test/num_examples': 10000, 'score': 10963.765152215958, 'total_duration': 12140.410922527313, 'accumulated_submission_time': 10963.765152215958, 'accumulated_eval_time': 1174.560831785202, 'accumulated_logging_time': 0.7606453895568848, 'global_step': 23459, 'preemption_count': 0}), (24365, {'train/accuracy': 0.5406445264816284, 'train/loss': 2.031728982925415, 'validation/accuracy': 0.5039199590682983, 'validation/loss': 2.2272121906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.39340001344680786, 'test/loss': 2.869940996170044, 'test/num_examples': 10000, 'score': 11384.004220485687, 'total_duration': 12600.587675094604, 'accumulated_submission_time': 11384.004220485687, 'accumulated_eval_time': 1214.4176337718964, 'accumulated_logging_time': 0.7908592224121094, 'global_step': 24365, 'preemption_count': 0}), (25272, {'train/accuracy': 0.5468554496765137, 'train/loss': 1.9566597938537598, 'validation/accuracy': 0.5080199837684631, 'validation/loss': 2.159663438796997, 'validation/num_examples': 50000, 'test/accuracy': 0.3937000334262848, 'test/loss': 2.8316471576690674, 'test/num_examples': 10000, 'score': 11804.188628435135, 'total_duration': 13068.275243759155, 'accumulated_submission_time': 11804.188628435135, 'accumulated_eval_time': 1261.837417602539, 'accumulated_logging_time': 0.822918176651001, 'global_step': 25272, 'preemption_count': 0}), (26178, {'train/accuracy': 0.5712695121765137, 'train/loss': 1.8304411172866821, 'validation/accuracy': 0.519760012626648, 'validation/loss': 2.1027395725250244, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.7602477073669434, 'test/num_examples': 10000, 'score': 12224.113825082779, 'total_duration': 13530.477799415588, 'accumulated_submission_time': 12224.113825082779, 'accumulated_eval_time': 1304.0307462215424, 'accumulated_logging_time': 0.8551509380340576, 'global_step': 26178, 'preemption_count': 0}), (27083, {'train/accuracy': 0.5553905963897705, 'train/loss': 1.8902047872543335, 'validation/accuracy': 0.5142999887466431, 'validation/loss': 2.0941996574401855, 'validation/num_examples': 50000, 'test/accuracy': 0.4067000150680542, 'test/loss': 2.7455155849456787, 'test/num_examples': 10000, 'score': 12644.132603883743, 'total_duration': 13995.284102916718, 'accumulated_submission_time': 12644.132603883743, 'accumulated_eval_time': 1348.732283115387, 'accumulated_logging_time': 0.8903157711029053, 'global_step': 27083, 'preemption_count': 0}), (27989, {'train/accuracy': 0.5625976324081421, 'train/loss': 1.8604013919830322, 'validation/accuracy': 0.5207599997520447, 'validation/loss': 2.0770411491394043, 'validation/num_examples': 50000, 'test/accuracy': 0.4098000228404999, 'test/loss': 2.7530479431152344, 'test/num_examples': 10000, 'score': 13064.056718111038, 'total_duration': 14457.495115756989, 'accumulated_submission_time': 13064.056718111038, 'accumulated_eval_time': 1390.9314963817596, 'accumulated_logging_time': 0.926861047744751, 'global_step': 27989, 'preemption_count': 0}), (28896, {'train/accuracy': 0.5817968845367432, 'train/loss': 1.820831298828125, 'validation/accuracy': 0.5314399600028992, 'validation/loss': 2.074846029281616, 'validation/num_examples': 50000, 'test/accuracy': 0.4182000160217285, 'test/loss': 2.7303760051727295, 'test/num_examples': 10000, 'score': 13484.38296675682, 'total_duration': 14924.05919790268, 'accumulated_submission_time': 13484.38296675682, 'accumulated_eval_time': 1437.0800392627716, 'accumulated_logging_time': 0.9647171497344971, 'global_step': 28896, 'preemption_count': 0}), (29803, {'train/accuracy': 0.5716015696525574, 'train/loss': 1.8176162242889404, 'validation/accuracy': 0.5288999676704407, 'validation/loss': 2.0264298915863037, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.7066798210144043, 'test/num_examples': 10000, 'score': 13904.436289548874, 'total_duration': 15390.029060602188, 'accumulated_submission_time': 13904.436289548874, 'accumulated_eval_time': 1482.9127361774445, 'accumulated_logging_time': 0.9980545043945312, 'global_step': 29803, 'preemption_count': 0}), (30710, {'train/accuracy': 0.5737109184265137, 'train/loss': 1.8410923480987549, 'validation/accuracy': 0.5341199636459351, 'validation/loss': 2.0497286319732666, 'validation/num_examples': 50000, 'test/accuracy': 0.4155000150203705, 'test/loss': 2.7235465049743652, 'test/num_examples': 10000, 'score': 14324.511536359787, 'total_duration': 15857.354704141617, 'accumulated_submission_time': 14324.511536359787, 'accumulated_eval_time': 1530.0810143947601, 'accumulated_logging_time': 1.0294535160064697, 'global_step': 30710, 'preemption_count': 0}), (31613, {'train/accuracy': 0.5854882597923279, 'train/loss': 1.7064871788024902, 'validation/accuracy': 0.5389999747276306, 'validation/loss': 1.964975118637085, 'validation/num_examples': 50000, 'test/accuracy': 0.4191000163555145, 'test/loss': 2.6509931087493896, 'test/num_examples': 10000, 'score': 14744.565649032593, 'total_duration': 16323.816191673279, 'accumulated_submission_time': 14744.565649032593, 'accumulated_eval_time': 1576.4077117443085, 'accumulated_logging_time': 1.0594823360443115, 'global_step': 31613, 'preemption_count': 0}), (32519, {'train/accuracy': 0.5890039205551147, 'train/loss': 1.7607898712158203, 'validation/accuracy': 0.5417999625205994, 'validation/loss': 1.980234980583191, 'validation/num_examples': 50000, 'test/accuracy': 0.42820000648498535, 'test/loss': 2.6211376190185547, 'test/num_examples': 10000, 'score': 15164.755456447601, 'total_duration': 16786.085805416107, 'accumulated_submission_time': 15164.755456447601, 'accumulated_eval_time': 1618.4065613746643, 'accumulated_logging_time': 1.089357614517212, 'global_step': 32519, 'preemption_count': 0}), (33426, {'train/accuracy': 0.5855468511581421, 'train/loss': 1.7376009225845337, 'validation/accuracy': 0.5393800139427185, 'validation/loss': 1.9697725772857666, 'validation/num_examples': 50000, 'test/accuracy': 0.42080003023147583, 'test/loss': 2.6821558475494385, 'test/num_examples': 10000, 'score': 15585.092749595642, 'total_duration': 17252.33269262314, 'accumulated_submission_time': 15585.092749595642, 'accumulated_eval_time': 1664.2355268001556, 'accumulated_logging_time': 1.1192798614501953, 'global_step': 33426, 'preemption_count': 0}), (34332, {'train/accuracy': 0.5911523103713989, 'train/loss': 1.7255398035049438, 'validation/accuracy': 0.5455799698829651, 'validation/loss': 1.9740146398544312, 'validation/num_examples': 50000, 'test/accuracy': 0.42810001969337463, 'test/loss': 2.6390552520751953, 'test/num_examples': 10000, 'score': 16005.261153697968, 'total_duration': 17718.188776254654, 'accumulated_submission_time': 16005.261153697968, 'accumulated_eval_time': 1709.8396821022034, 'accumulated_logging_time': 1.1521375179290771, 'global_step': 34332, 'preemption_count': 0}), (35240, {'train/accuracy': 0.6228320002555847, 'train/loss': 1.5777928829193115, 'validation/accuracy': 0.555180013179779, 'validation/loss': 1.9033994674682617, 'validation/num_examples': 50000, 'test/accuracy': 0.44110003113746643, 'test/loss': 2.584881544113159, 'test/num_examples': 10000, 'score': 16425.55720925331, 'total_duration': 18184.633157491684, 'accumulated_submission_time': 16425.55720925331, 'accumulated_eval_time': 1755.8981931209564, 'accumulated_logging_time': 1.190577507019043, 'global_step': 35240, 'preemption_count': 0}), (36145, {'train/accuracy': 0.5803906321525574, 'train/loss': 1.8397955894470215, 'validation/accuracy': 0.5388000011444092, 'validation/loss': 2.0403988361358643, 'validation/num_examples': 50000, 'test/accuracy': 0.4221000075340271, 'test/loss': 2.6815497875213623, 'test/num_examples': 10000, 'score': 16845.653829813004, 'total_duration': 18650.0134100914, 'accumulated_submission_time': 16845.653829813004, 'accumulated_eval_time': 1801.0961050987244, 'accumulated_logging_time': 1.2253484725952148, 'global_step': 36145, 'preemption_count': 0}), (37050, {'train/accuracy': 0.6094335913658142, 'train/loss': 1.6258376836776733, 'validation/accuracy': 0.5605800151824951, 'validation/loss': 1.8679131269454956, 'validation/num_examples': 50000, 'test/accuracy': 0.44290003180503845, 'test/loss': 2.5216729640960693, 'test/num_examples': 10000, 'score': 17265.834097862244, 'total_duration': 19117.25606751442, 'accumulated_submission_time': 17265.834097862244, 'accumulated_eval_time': 1848.0710878372192, 'accumulated_logging_time': 1.2622802257537842, 'global_step': 37050, 'preemption_count': 0}), (37953, {'train/accuracy': 0.6267187595367432, 'train/loss': 1.5315265655517578, 'validation/accuracy': 0.5590599775314331, 'validation/loss': 1.8768879175186157, 'validation/num_examples': 50000, 'test/accuracy': 0.442300021648407, 'test/loss': 2.54549503326416, 'test/num_examples': 10000, 'score': 17685.957972049713, 'total_duration': 19579.371796131134, 'accumulated_submission_time': 17685.957972049713, 'accumulated_eval_time': 1889.9757521152496, 'accumulated_logging_time': 1.2984967231750488, 'global_step': 37953, 'preemption_count': 0}), (38858, {'train/accuracy': 0.6036718487739563, 'train/loss': 1.6944724321365356, 'validation/accuracy': 0.5605999827384949, 'validation/loss': 1.9091485738754272, 'validation/num_examples': 50000, 'test/accuracy': 0.44440001249313354, 'test/loss': 2.581925630569458, 'test/num_examples': 10000, 'score': 18105.92478275299, 'total_duration': 20047.60985803604, 'accumulated_submission_time': 18105.92478275299, 'accumulated_eval_time': 1938.1587483882904, 'accumulated_logging_time': 1.3353419303894043, 'global_step': 38858, 'preemption_count': 0}), (39766, {'train/accuracy': 0.6093164086341858, 'train/loss': 1.628745675086975, 'validation/accuracy': 0.5579000115394592, 'validation/loss': 1.8780062198638916, 'validation/num_examples': 50000, 'test/accuracy': 0.4431000351905823, 'test/loss': 2.5480268001556396, 'test/num_examples': 10000, 'score': 18526.282481193542, 'total_duration': 20515.20761990547, 'accumulated_submission_time': 18526.282481193542, 'accumulated_eval_time': 1985.312269449234, 'accumulated_logging_time': 1.3708126544952393, 'global_step': 39766, 'preemption_count': 0}), (40672, {'train/accuracy': 0.6217382550239563, 'train/loss': 1.5823453664779663, 'validation/accuracy': 0.5575399994850159, 'validation/loss': 1.8897854089736938, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.577730178833008, 'test/num_examples': 10000, 'score': 18946.474817752838, 'total_duration': 20981.583657741547, 'accumulated_submission_time': 18946.474817752838, 'accumulated_eval_time': 2031.4075977802277, 'accumulated_logging_time': 1.408151388168335, 'global_step': 40672, 'preemption_count': 0}), (41577, {'train/accuracy': 0.6031640768051147, 'train/loss': 1.6544686555862427, 'validation/accuracy': 0.5612999796867371, 'validation/loss': 1.8774502277374268, 'validation/num_examples': 50000, 'test/accuracy': 0.44270002841949463, 'test/loss': 2.5433125495910645, 'test/num_examples': 10000, 'score': 19366.68581557274, 'total_duration': 21450.455101966858, 'accumulated_submission_time': 19366.68581557274, 'accumulated_eval_time': 2079.980150461197, 'accumulated_logging_time': 1.4446847438812256, 'global_step': 41577, 'preemption_count': 0}), (42482, {'train/accuracy': 0.6094140410423279, 'train/loss': 1.6763795614242554, 'validation/accuracy': 0.5593799948692322, 'validation/loss': 1.911125898361206, 'validation/num_examples': 50000, 'test/accuracy': 0.4410000145435333, 'test/loss': 2.589221715927124, 'test/num_examples': 10000, 'score': 19786.95946097374, 'total_duration': 21913.58345246315, 'accumulated_submission_time': 19786.95946097374, 'accumulated_eval_time': 2122.7487738132477, 'accumulated_logging_time': 1.4790616035461426, 'global_step': 42482, 'preemption_count': 0}), (43388, {'train/accuracy': 0.6217382550239563, 'train/loss': 1.639617919921875, 'validation/accuracy': 0.5581200122833252, 'validation/loss': 1.9320470094680786, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.585341215133667, 'test/num_examples': 10000, 'score': 20207.082400798798, 'total_duration': 22379.42211842537, 'accumulated_submission_time': 20207.082400798798, 'accumulated_eval_time': 2168.378532409668, 'accumulated_logging_time': 1.5133049488067627, 'global_step': 43388, 'preemption_count': 0}), (44294, {'train/accuracy': 0.613964855670929, 'train/loss': 1.6308157444000244, 'validation/accuracy': 0.5718799829483032, 'validation/loss': 1.8435885906219482, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.4995086193084717, 'test/num_examples': 10000, 'score': 20627.26855278015, 'total_duration': 22847.91852927208, 'accumulated_submission_time': 20627.26855278015, 'accumulated_eval_time': 2216.601419687271, 'accumulated_logging_time': 1.5490844249725342, 'global_step': 44294, 'preemption_count': 0}), (45197, {'train/accuracy': 0.6221679449081421, 'train/loss': 1.5743632316589355, 'validation/accuracy': 0.5730199813842773, 'validation/loss': 1.8135331869125366, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.4988009929656982, 'test/num_examples': 10000, 'score': 21047.384136915207, 'total_duration': 23311.679278612137, 'accumulated_submission_time': 21047.384136915207, 'accumulated_eval_time': 2260.163228034973, 'accumulated_logging_time': 1.5819377899169922, 'global_step': 45197, 'preemption_count': 0}), (46101, {'train/accuracy': 0.6398242115974426, 'train/loss': 1.471780776977539, 'validation/accuracy': 0.5819199681282043, 'validation/loss': 1.7632266283035278, 'validation/num_examples': 50000, 'test/accuracy': 0.46000000834465027, 'test/loss': 2.4405603408813477, 'test/num_examples': 10000, 'score': 21468.033683538437, 'total_duration': 23779.937309980392, 'accumulated_submission_time': 21468.033683538437, 'accumulated_eval_time': 2307.6843173503876, 'accumulated_logging_time': 1.6177711486816406, 'global_step': 46101, 'preemption_count': 0}), (47003, {'train/accuracy': 0.6192382574081421, 'train/loss': 1.632784128189087, 'validation/accuracy': 0.5776799917221069, 'validation/loss': 1.838263750076294, 'validation/num_examples': 50000, 'test/accuracy': 0.45340001583099365, 'test/loss': 2.5132648944854736, 'test/num_examples': 10000, 'score': 21888.051579475403, 'total_duration': 24245.620250225067, 'accumulated_submission_time': 21888.051579475403, 'accumulated_eval_time': 2353.266669511795, 'accumulated_logging_time': 1.6493885517120361, 'global_step': 47003, 'preemption_count': 0}), (47905, {'train/accuracy': 0.6240624785423279, 'train/loss': 1.5510444641113281, 'validation/accuracy': 0.5730800032615662, 'validation/loss': 1.7903846502304077, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.462394952774048, 'test/num_examples': 10000, 'score': 22308.016907691956, 'total_duration': 24714.259820222855, 'accumulated_submission_time': 22308.016907691956, 'accumulated_eval_time': 2401.858140230179, 'accumulated_logging_time': 1.6812567710876465, 'global_step': 47905, 'preemption_count': 0}), (48809, {'train/accuracy': 0.6260156035423279, 'train/loss': 1.572181224822998, 'validation/accuracy': 0.5747199654579163, 'validation/loss': 1.8311009407043457, 'validation/num_examples': 50000, 'test/accuracy': 0.45680001378059387, 'test/loss': 2.4816720485687256, 'test/num_examples': 10000, 'score': 22728.048118829727, 'total_duration': 25173.80234694481, 'accumulated_submission_time': 22728.048118829727, 'accumulated_eval_time': 2441.2826936244965, 'accumulated_logging_time': 1.717482089996338, 'global_step': 48809, 'preemption_count': 0}), (49711, {'train/accuracy': 0.6138671636581421, 'train/loss': 1.6917517185211182, 'validation/accuracy': 0.5685399770736694, 'validation/loss': 1.90058434009552, 'validation/num_examples': 50000, 'test/accuracy': 0.44630002975463867, 'test/loss': 2.5759851932525635, 'test/num_examples': 10000, 'score': 23147.635646104813, 'total_duration': 25640.113765478134, 'accumulated_submission_time': 23147.635646104813, 'accumulated_eval_time': 2487.4757957458496, 'accumulated_logging_time': 2.1974527835845947, 'global_step': 49711, 'preemption_count': 0}), (50614, {'train/accuracy': 0.627246081829071, 'train/loss': 1.6077184677124023, 'validation/accuracy': 0.5752800107002258, 'validation/loss': 1.8396003246307373, 'validation/num_examples': 50000, 'test/accuracy': 0.4588000178337097, 'test/loss': 2.4899773597717285, 'test/num_examples': 10000, 'score': 23567.732134580612, 'total_duration': 26104.293008327484, 'accumulated_submission_time': 23567.732134580612, 'accumulated_eval_time': 2531.470423936844, 'accumulated_logging_time': 2.233795166015625, 'global_step': 50614, 'preemption_count': 0}), (51518, {'train/accuracy': 0.6355859041213989, 'train/loss': 1.5609021186828613, 'validation/accuracy': 0.5841400027275085, 'validation/loss': 1.812380313873291, 'validation/num_examples': 50000, 'test/accuracy': 0.4621000289916992, 'test/loss': 2.4717304706573486, 'test/num_examples': 10000, 'score': 23987.8773355484, 'total_duration': 26570.3987429142, 'accumulated_submission_time': 23987.8773355484, 'accumulated_eval_time': 2577.3440272808075, 'accumulated_logging_time': 2.2699053287506104, 'global_step': 51518, 'preemption_count': 0}), (52423, {'train/accuracy': 0.6373242139816284, 'train/loss': 1.5606440305709839, 'validation/accuracy': 0.5805400013923645, 'validation/loss': 1.8203362226486206, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.4787209033966064, 'test/num_examples': 10000, 'score': 24408.212697029114, 'total_duration': 27037.57492542267, 'accumulated_submission_time': 24408.212697029114, 'accumulated_eval_time': 2624.098313808441, 'accumulated_logging_time': 2.305191993713379, 'global_step': 52423, 'preemption_count': 0}), (53329, {'train/accuracy': 0.6320117115974426, 'train/loss': 1.5440112352371216, 'validation/accuracy': 0.586359977722168, 'validation/loss': 1.7737897634506226, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.432278871536255, 'test/num_examples': 10000, 'score': 24828.36497950554, 'total_duration': 27501.591789245605, 'accumulated_submission_time': 24828.36497950554, 'accumulated_eval_time': 2667.875230550766, 'accumulated_logging_time': 2.341886043548584, 'global_step': 53329, 'preemption_count': 0}), (54233, {'train/accuracy': 0.6431054472923279, 'train/loss': 1.4797742366790771, 'validation/accuracy': 0.5918599963188171, 'validation/loss': 1.7450494766235352, 'validation/num_examples': 50000, 'test/accuracy': 0.4805000126361847, 'test/loss': 2.3959012031555176, 'test/num_examples': 10000, 'score': 25248.749005794525, 'total_duration': 27964.58424758911, 'accumulated_submission_time': 25248.749005794525, 'accumulated_eval_time': 2710.3991668224335, 'accumulated_logging_time': 2.375976324081421, 'global_step': 54233, 'preemption_count': 0}), (55137, {'train/accuracy': 0.6678124666213989, 'train/loss': 1.3960999250411987, 'validation/accuracy': 0.5922799706459045, 'validation/loss': 1.7475827932357788, 'validation/num_examples': 50000, 'test/accuracy': 0.4723000228404999, 'test/loss': 2.4174678325653076, 'test/num_examples': 10000, 'score': 25668.903258800507, 'total_duration': 28431.168878793716, 'accumulated_submission_time': 25668.903258800507, 'accumulated_eval_time': 2756.7333924770355, 'accumulated_logging_time': 2.421182155609131, 'global_step': 55137, 'preemption_count': 0}), (56041, {'train/accuracy': 0.6312109231948853, 'train/loss': 1.5431017875671387, 'validation/accuracy': 0.5813199877738953, 'validation/loss': 1.7773977518081665, 'validation/num_examples': 50000, 'test/accuracy': 0.4618000090122223, 'test/loss': 2.443195343017578, 'test/num_examples': 10000, 'score': 26088.894425868988, 'total_duration': 28898.318333864212, 'accumulated_submission_time': 26088.894425868988, 'accumulated_eval_time': 2803.8037741184235, 'accumulated_logging_time': 2.4583358764648438, 'global_step': 56041, 'preemption_count': 0}), (56946, {'train/accuracy': 0.6483007669448853, 'train/loss': 1.5265979766845703, 'validation/accuracy': 0.598039984703064, 'validation/loss': 1.7617758512496948, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.4116508960723877, 'test/num_examples': 10000, 'score': 26509.195707321167, 'total_duration': 29361.984380960464, 'accumulated_submission_time': 26509.195707321167, 'accumulated_eval_time': 2847.0764875411987, 'accumulated_logging_time': 2.49930739402771, 'global_step': 56946, 'preemption_count': 0}), (57850, {'train/accuracy': 0.6579492092132568, 'train/loss': 1.4141149520874023, 'validation/accuracy': 0.590499997138977, 'validation/loss': 1.743949294090271, 'validation/num_examples': 50000, 'test/accuracy': 0.4708000123500824, 'test/loss': 2.402419328689575, 'test/num_examples': 10000, 'score': 26929.53288602829, 'total_duration': 29819.127604722977, 'accumulated_submission_time': 26929.53288602829, 'accumulated_eval_time': 2883.793509721756, 'accumulated_logging_time': 2.5372140407562256, 'global_step': 57850, 'preemption_count': 0}), (58755, {'train/accuracy': 0.6382421851158142, 'train/loss': 1.5162142515182495, 'validation/accuracy': 0.5955199599266052, 'validation/loss': 1.725347638130188, 'validation/num_examples': 50000, 'test/accuracy': 0.47860002517700195, 'test/loss': 2.3848416805267334, 'test/num_examples': 10000, 'score': 27349.482364177704, 'total_duration': 30287.817235708237, 'accumulated_submission_time': 27349.482364177704, 'accumulated_eval_time': 2932.443725347519, 'accumulated_logging_time': 2.5766994953155518, 'global_step': 58755, 'preemption_count': 0}), (59662, {'train/accuracy': 0.6444531083106995, 'train/loss': 1.4967687129974365, 'validation/accuracy': 0.5925599932670593, 'validation/loss': 1.7437061071395874, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.3955695629119873, 'test/num_examples': 10000, 'score': 27769.687192440033, 'total_duration': 30756.543888568878, 'accumulated_submission_time': 27769.687192440033, 'accumulated_eval_time': 2980.876615524292, 'accumulated_logging_time': 2.613621950149536, 'global_step': 59662, 'preemption_count': 0}), (60567, {'train/accuracy': 0.6565039157867432, 'train/loss': 1.4547697305679321, 'validation/accuracy': 0.5924599766731262, 'validation/loss': 1.755020260810852, 'validation/num_examples': 50000, 'test/accuracy': 0.4758000373840332, 'test/loss': 2.3964974880218506, 'test/num_examples': 10000, 'score': 28189.677449941635, 'total_duration': 31224.26690030098, 'accumulated_submission_time': 28189.677449941635, 'accumulated_eval_time': 3028.5183403491974, 'accumulated_logging_time': 2.6536691188812256, 'global_step': 60567, 'preemption_count': 0}), (61470, {'train/accuracy': 0.650585949420929, 'train/loss': 1.4309154748916626, 'validation/accuracy': 0.6028599739074707, 'validation/loss': 1.6541436910629272, 'validation/num_examples': 50000, 'test/accuracy': 0.4808000326156616, 'test/loss': 2.325901508331299, 'test/num_examples': 10000, 'score': 28609.880881786346, 'total_duration': 31685.347499847412, 'accumulated_submission_time': 28609.880881786346, 'accumulated_eval_time': 3069.303905725479, 'accumulated_logging_time': 2.693972587585449, 'global_step': 61470, 'preemption_count': 0}), (62375, {'train/accuracy': 0.6543945074081421, 'train/loss': 1.4196845293045044, 'validation/accuracy': 0.60725998878479, 'validation/loss': 1.649720549583435, 'validation/num_examples': 50000, 'test/accuracy': 0.4886000156402588, 'test/loss': 2.3273630142211914, 'test/num_examples': 10000, 'score': 29030.005185365677, 'total_duration': 32153.49118900299, 'accumulated_submission_time': 29030.005185365677, 'accumulated_eval_time': 3117.232168197632, 'accumulated_logging_time': 2.733799457550049, 'global_step': 62375, 'preemption_count': 0}), (63280, {'train/accuracy': 0.6655077934265137, 'train/loss': 1.3871628046035767, 'validation/accuracy': 0.6027199625968933, 'validation/loss': 1.683455228805542, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3284871578216553, 'test/num_examples': 10000, 'score': 29450.34034228325, 'total_duration': 32614.989812135696, 'accumulated_submission_time': 29450.34034228325, 'accumulated_eval_time': 3158.303008079529, 'accumulated_logging_time': 2.775385618209839, 'global_step': 63280, 'preemption_count': 0}), (64183, {'train/accuracy': 0.6518945097923279, 'train/loss': 1.4362150430679321, 'validation/accuracy': 0.605139970779419, 'validation/loss': 1.6577297449111938, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.3341472148895264, 'test/num_examples': 10000, 'score': 29870.425412654877, 'total_duration': 33077.85925221443, 'accumulated_submission_time': 29870.425412654877, 'accumulated_eval_time': 3200.9960539340973, 'accumulated_logging_time': 2.816265344619751, 'global_step': 64183, 'preemption_count': 0}), (65089, {'train/accuracy': 0.6542773246765137, 'train/loss': 1.4176008701324463, 'validation/accuracy': 0.6029399633407593, 'validation/loss': 1.6662062406539917, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.3240888118743896, 'test/num_examples': 10000, 'score': 30290.650116682053, 'total_duration': 33545.353556632996, 'accumulated_submission_time': 30290.650116682053, 'accumulated_eval_time': 3248.1766040325165, 'accumulated_logging_time': 2.854301929473877, 'global_step': 65089, 'preemption_count': 0}), (65996, {'train/accuracy': 0.667675793170929, 'train/loss': 1.373803734779358, 'validation/accuracy': 0.6104399561882019, 'validation/loss': 1.6528773307800293, 'validation/num_examples': 50000, 'test/accuracy': 0.48990002274513245, 'test/loss': 2.314547300338745, 'test/num_examples': 10000, 'score': 30710.779942512512, 'total_duration': 34010.43670129776, 'accumulated_submission_time': 30710.779942512512, 'accumulated_eval_time': 3293.040856361389, 'accumulated_logging_time': 2.891425609588623, 'global_step': 65996, 'preemption_count': 0}), (66901, {'train/accuracy': 0.6533398032188416, 'train/loss': 1.4194730520248413, 'validation/accuracy': 0.6050599813461304, 'validation/loss': 1.660703182220459, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.354022741317749, 'test/num_examples': 10000, 'score': 31131.218547344208, 'total_duration': 34471.03016901016, 'accumulated_submission_time': 31131.218547344208, 'accumulated_eval_time': 3333.1036410331726, 'accumulated_logging_time': 2.93249773979187, 'global_step': 66901, 'preemption_count': 0}), (67805, {'train/accuracy': 0.6598047018051147, 'train/loss': 1.3968747854232788, 'validation/accuracy': 0.6152600049972534, 'validation/loss': 1.627685785293579, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.301361322402954, 'test/num_examples': 10000, 'score': 31551.129816770554, 'total_duration': 34935.6247048378, 'accumulated_submission_time': 31551.129816770554, 'accumulated_eval_time': 3377.696927547455, 'accumulated_logging_time': 2.9715535640716553, 'global_step': 67805, 'preemption_count': 0}), (68712, {'train/accuracy': 0.6655663847923279, 'train/loss': 1.3830251693725586, 'validation/accuracy': 0.6067999601364136, 'validation/loss': 1.658873438835144, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.339237689971924, 'test/num_examples': 10000, 'score': 31971.418674468994, 'total_duration': 35402.118267059326, 'accumulated_submission_time': 31971.418674468994, 'accumulated_eval_time': 3423.8143379688263, 'accumulated_logging_time': 3.0081887245178223, 'global_step': 68712, 'preemption_count': 0}), (69620, {'train/accuracy': 0.6632031202316284, 'train/loss': 1.383965253829956, 'validation/accuracy': 0.610539972782135, 'validation/loss': 1.6313594579696655, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3167552947998047, 'test/num_examples': 10000, 'score': 32391.608671188354, 'total_duration': 35864.72148346901, 'accumulated_submission_time': 32391.608671188354, 'accumulated_eval_time': 3466.132476091385, 'accumulated_logging_time': 3.0513832569122314, 'global_step': 69620, 'preemption_count': 0}), (70524, {'train/accuracy': 0.6672461032867432, 'train/loss': 1.3634191751480103, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.6041995286941528, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.2632946968078613, 'test/num_examples': 10000, 'score': 32811.70836234093, 'total_duration': 36331.73110461235, 'accumulated_submission_time': 32811.70836234093, 'accumulated_eval_time': 3512.9533495903015, 'accumulated_logging_time': 3.0895795822143555, 'global_step': 70524, 'preemption_count': 0}), (71426, {'train/accuracy': 0.6733202934265137, 'train/loss': 1.3281680345535278, 'validation/accuracy': 0.6173200011253357, 'validation/loss': 1.5995622873306274, 'validation/num_examples': 50000, 'test/accuracy': 0.4943000376224518, 'test/loss': 2.2829320430755615, 'test/num_examples': 10000, 'score': 33231.66958928108, 'total_duration': 36795.55163145065, 'accumulated_submission_time': 33231.66958928108, 'accumulated_eval_time': 3556.7215859889984, 'accumulated_logging_time': 3.1303675174713135, 'global_step': 71426, 'preemption_count': 0}), (72330, {'train/accuracy': 0.6946874856948853, 'train/loss': 1.2652043104171753, 'validation/accuracy': 0.6173799633979797, 'validation/loss': 1.6047745943069458, 'validation/num_examples': 50000, 'test/accuracy': 0.4918000102043152, 'test/loss': 2.269059896469116, 'test/num_examples': 10000, 'score': 33651.71494960785, 'total_duration': 37263.838456869125, 'accumulated_submission_time': 33651.71494960785, 'accumulated_eval_time': 3604.856119155884, 'accumulated_logging_time': 3.186277151107788, 'global_step': 72330, 'preemption_count': 0}), (73236, {'train/accuracy': 0.6619726419448853, 'train/loss': 1.3814362287521362, 'validation/accuracy': 0.613860011100769, 'validation/loss': 1.6136529445648193, 'validation/num_examples': 50000, 'test/accuracy': 0.4927000105381012, 'test/loss': 2.2791812419891357, 'test/num_examples': 10000, 'score': 34072.08019518852, 'total_duration': 37732.43508505821, 'accumulated_submission_time': 34072.08019518852, 'accumulated_eval_time': 3652.9952659606934, 'accumulated_logging_time': 3.2280800342559814, 'global_step': 73236, 'preemption_count': 0}), (74139, {'train/accuracy': 0.6750390529632568, 'train/loss': 1.3150864839553833, 'validation/accuracy': 0.6202999949455261, 'validation/loss': 1.5818755626678467, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.2524607181549072, 'test/num_examples': 10000, 'score': 34492.027354717255, 'total_duration': 38193.862604141235, 'accumulated_submission_time': 34492.027354717255, 'accumulated_eval_time': 3694.3831102848053, 'accumulated_logging_time': 3.2697389125823975, 'global_step': 74139, 'preemption_count': 0}), (75045, {'train/accuracy': 0.6972851157188416, 'train/loss': 1.2580310106277466, 'validation/accuracy': 0.6181199550628662, 'validation/loss': 1.6094812154769897, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.2771427631378174, 'test/num_examples': 10000, 'score': 34912.15011835098, 'total_duration': 38661.28926753998, 'accumulated_submission_time': 34912.15011835098, 'accumulated_eval_time': 3741.594585418701, 'accumulated_logging_time': 3.3109562397003174, 'global_step': 75045, 'preemption_count': 0}), (75953, {'train/accuracy': 0.6793164014816284, 'train/loss': 1.3091703653335571, 'validation/accuracy': 0.6315000057220459, 'validation/loss': 1.5481455326080322, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.2062301635742188, 'test/num_examples': 10000, 'score': 35332.28189110756, 'total_duration': 39126.626574754715, 'accumulated_submission_time': 35332.28189110756, 'accumulated_eval_time': 3786.706786632538, 'accumulated_logging_time': 3.353020668029785, 'global_step': 75953, 'preemption_count': 0}), (76861, {'train/accuracy': 0.6729882955551147, 'train/loss': 1.3250367641448975, 'validation/accuracy': 0.6203599572181702, 'validation/loss': 1.5800641775131226, 'validation/num_examples': 50000, 'test/accuracy': 0.4974000155925751, 'test/loss': 2.2438337802886963, 'test/num_examples': 10000, 'score': 35752.626155376434, 'total_duration': 39597.17609834671, 'accumulated_submission_time': 35752.626155376434, 'accumulated_eval_time': 3836.8165822029114, 'accumulated_logging_time': 3.397064447402954, 'global_step': 76861, 'preemption_count': 0}), (77766, {'train/accuracy': 0.7009375095367432, 'train/loss': 1.209425687789917, 'validation/accuracy': 0.6351000070571899, 'validation/loss': 1.5253475904464722, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.1836676597595215, 'test/num_examples': 10000, 'score': 36172.57538366318, 'total_duration': 40064.91317510605, 'accumulated_submission_time': 36172.57538366318, 'accumulated_eval_time': 3884.5109570026398, 'accumulated_logging_time': 3.4398858547210693, 'global_step': 77766, 'preemption_count': 0}), (78670, {'train/accuracy': 0.6809374690055847, 'train/loss': 1.2866195440292358, 'validation/accuracy': 0.6292799711227417, 'validation/loss': 1.5328986644744873, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.1805171966552734, 'test/num_examples': 10000, 'score': 36592.812076091766, 'total_duration': 40533.96717476845, 'accumulated_submission_time': 36592.812076091766, 'accumulated_eval_time': 3933.235659122467, 'accumulated_logging_time': 3.4815621376037598, 'global_step': 78670, 'preemption_count': 0}), (79574, {'train/accuracy': 0.6755468845367432, 'train/loss': 1.3667091131210327, 'validation/accuracy': 0.6249200105667114, 'validation/loss': 1.604951024055481, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.2638602256774902, 'test/num_examples': 10000, 'score': 37012.992997169495, 'total_duration': 41002.65064263344, 'accumulated_submission_time': 37012.992997169495, 'accumulated_eval_time': 3981.6476192474365, 'accumulated_logging_time': 3.5212857723236084, 'global_step': 79574, 'preemption_count': 0}), (80480, {'train/accuracy': 0.6903710961341858, 'train/loss': 1.2400176525115967, 'validation/accuracy': 0.6298399567604065, 'validation/loss': 1.541815161705017, 'validation/num_examples': 50000, 'test/accuracy': 0.5015000104904175, 'test/loss': 2.2130167484283447, 'test/num_examples': 10000, 'score': 37432.94675517082, 'total_duration': 41462.675387859344, 'accumulated_submission_time': 37432.94675517082, 'accumulated_eval_time': 4021.6289196014404, 'accumulated_logging_time': 3.5604658126831055, 'global_step': 80480, 'preemption_count': 0}), (81384, {'train/accuracy': 0.6767382621765137, 'train/loss': 1.3347312211990356, 'validation/accuracy': 0.6283999681472778, 'validation/loss': 1.5647926330566406, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2255892753601074, 'test/num_examples': 10000, 'score': 37852.857625961304, 'total_duration': 41930.42304897308, 'accumulated_submission_time': 37852.857625961304, 'accumulated_eval_time': 4069.3755803108215, 'accumulated_logging_time': 3.6002750396728516, 'global_step': 81384, 'preemption_count': 0}), (82289, {'train/accuracy': 0.6830077767372131, 'train/loss': 1.3016310930252075, 'validation/accuracy': 0.6261599659919739, 'validation/loss': 1.565454125404358, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.2108993530273438, 'test/num_examples': 10000, 'score': 38272.83093523979, 'total_duration': 42398.30388045311, 'accumulated_submission_time': 38272.83093523979, 'accumulated_eval_time': 4117.19310426712, 'accumulated_logging_time': 3.6395747661590576, 'global_step': 82289, 'preemption_count': 0}), (83196, {'train/accuracy': 0.6936327815055847, 'train/loss': 1.2280399799346924, 'validation/accuracy': 0.6358000040054321, 'validation/loss': 1.5165338516235352, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.164104700088501, 'test/num_examples': 10000, 'score': 38692.91235637665, 'total_duration': 42869.38398528099, 'accumulated_submission_time': 38692.91235637665, 'accumulated_eval_time': 4168.097243785858, 'accumulated_logging_time': 3.6829824447631836, 'global_step': 83196, 'preemption_count': 0}), (84100, {'train/accuracy': 0.6860546469688416, 'train/loss': 1.2814973592758179, 'validation/accuracy': 0.638260006904602, 'validation/loss': 1.5209600925445557, 'validation/num_examples': 50000, 'test/accuracy': 0.513700008392334, 'test/loss': 2.1791257858276367, 'test/num_examples': 10000, 'score': 39113.014108896255, 'total_duration': 43335.938762664795, 'accumulated_submission_time': 39113.014108896255, 'accumulated_eval_time': 4214.460152864456, 'accumulated_logging_time': 3.7228505611419678, 'global_step': 84100, 'preemption_count': 0}), (85007, {'train/accuracy': 0.6898437142372131, 'train/loss': 1.2605102062225342, 'validation/accuracy': 0.6386399865150452, 'validation/loss': 1.5136559009552002, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.1877145767211914, 'test/num_examples': 10000, 'score': 39533.14155960083, 'total_duration': 43804.54938220978, 'accumulated_submission_time': 39533.14155960083, 'accumulated_eval_time': 4262.8486523628235, 'accumulated_logging_time': 3.7656800746917725, 'global_step': 85007, 'preemption_count': 0}), (85914, {'train/accuracy': 0.6995507478713989, 'train/loss': 1.186415195465088, 'validation/accuracy': 0.642300009727478, 'validation/loss': 1.4698925018310547, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.127619743347168, 'test/num_examples': 10000, 'score': 39953.234171152115, 'total_duration': 44272.816187381744, 'accumulated_submission_time': 39953.234171152115, 'accumulated_eval_time': 4310.929792881012, 'accumulated_logging_time': 3.8074779510498047, 'global_step': 85914, 'preemption_count': 0}), (86817, {'train/accuracy': 0.6941601634025574, 'train/loss': 1.2347404956817627, 'validation/accuracy': 0.6406199932098389, 'validation/loss': 1.4868887662887573, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.128563165664673, 'test/num_examples': 10000, 'score': 40373.50329899788, 'total_duration': 44740.781307935715, 'accumulated_submission_time': 40373.50329899788, 'accumulated_eval_time': 4358.529216766357, 'accumulated_logging_time': 3.8530352115631104, 'global_step': 86817, 'preemption_count': 0}), (87720, {'train/accuracy': 0.69593745470047, 'train/loss': 1.2813823223114014, 'validation/accuracy': 0.6403999924659729, 'validation/loss': 1.535065770149231, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.182572841644287, 'test/num_examples': 10000, 'score': 40793.42196941376, 'total_duration': 45206.561255693436, 'accumulated_submission_time': 40793.42196941376, 'accumulated_eval_time': 4404.2962164878845, 'accumulated_logging_time': 3.8958733081817627, 'global_step': 87720, 'preemption_count': 0}), (88625, {'train/accuracy': 0.7049609422683716, 'train/loss': 1.2156800031661987, 'validation/accuracy': 0.6411799788475037, 'validation/loss': 1.4978916645050049, 'validation/num_examples': 50000, 'test/accuracy': 0.5161000490188599, 'test/loss': 2.1546852588653564, 'test/num_examples': 10000, 'score': 41213.36802792549, 'total_duration': 45678.537242889404, 'accumulated_submission_time': 41213.36802792549, 'accumulated_eval_time': 4456.234867095947, 'accumulated_logging_time': 3.9354004859924316, 'global_step': 88625, 'preemption_count': 0}), (89532, {'train/accuracy': 0.7158789038658142, 'train/loss': 1.1632670164108276, 'validation/accuracy': 0.6412999629974365, 'validation/loss': 1.5002963542938232, 'validation/num_examples': 50000, 'test/accuracy': 0.5234000086784363, 'test/loss': 2.1391379833221436, 'test/num_examples': 10000, 'score': 41633.58397102356, 'total_duration': 46147.01468038559, 'accumulated_submission_time': 41633.58397102356, 'accumulated_eval_time': 4504.405340433121, 'accumulated_logging_time': 3.9754011631011963, 'global_step': 89532, 'preemption_count': 0}), (90437, {'train/accuracy': 0.6942968368530273, 'train/loss': 1.242924690246582, 'validation/accuracy': 0.6450600028038025, 'validation/loss': 1.4841945171356201, 'validation/num_examples': 50000, 'test/accuracy': 0.5220000147819519, 'test/loss': 2.13555645942688, 'test/num_examples': 10000, 'score': 42053.79886484146, 'total_duration': 46614.43021249771, 'accumulated_submission_time': 42053.79886484146, 'accumulated_eval_time': 4551.505216121674, 'accumulated_logging_time': 4.02495002746582, 'global_step': 90437, 'preemption_count': 0}), (91344, {'train/accuracy': 0.7076367139816284, 'train/loss': 1.2079362869262695, 'validation/accuracy': 0.644320011138916, 'validation/loss': 1.4906361103057861, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.1351263523101807, 'test/num_examples': 10000, 'score': 42474.13457107544, 'total_duration': 47081.45916390419, 'accumulated_submission_time': 42474.13457107544, 'accumulated_eval_time': 4598.1049790382385, 'accumulated_logging_time': 4.066986322402954, 'global_step': 91344, 'preemption_count': 0}), (92246, {'train/accuracy': 0.7279687523841858, 'train/loss': 1.1314303874969482, 'validation/accuracy': 0.6431199908256531, 'validation/loss': 1.4939802885055542, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1604013442993164, 'test/num_examples': 10000, 'score': 42894.400421381, 'total_duration': 47550.821682453156, 'accumulated_submission_time': 42894.400421381, 'accumulated_eval_time': 4647.107894182205, 'accumulated_logging_time': 4.110424280166626, 'global_step': 92246, 'preemption_count': 0}), (93152, {'train/accuracy': 0.7049609422683716, 'train/loss': 1.1868587732315063, 'validation/accuracy': 0.6511799693107605, 'validation/loss': 1.4446868896484375, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.0929665565490723, 'test/num_examples': 10000, 'score': 43314.599668979645, 'total_duration': 48019.18526363373, 'accumulated_submission_time': 43314.599668979645, 'accumulated_eval_time': 4695.176491975784, 'accumulated_logging_time': 4.155153512954712, 'global_step': 93152, 'preemption_count': 0}), (94057, {'train/accuracy': 0.7050195336341858, 'train/loss': 1.1846143007278442, 'validation/accuracy': 0.6482399702072144, 'validation/loss': 1.4538512229919434, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.119861364364624, 'test/num_examples': 10000, 'score': 43734.53122782707, 'total_duration': 48487.1493666172, 'accumulated_submission_time': 43734.53122782707, 'accumulated_eval_time': 4743.113221168518, 'accumulated_logging_time': 4.200689315795898, 'global_step': 94057, 'preemption_count': 0}), (94962, {'train/accuracy': 0.7193359136581421, 'train/loss': 1.1572200059890747, 'validation/accuracy': 0.6502999663352966, 'validation/loss': 1.4794443845748901, 'validation/num_examples': 50000, 'test/accuracy': 0.527400016784668, 'test/loss': 2.1172285079956055, 'test/num_examples': 10000, 'score': 44154.71737384796, 'total_duration': 48952.4436519146, 'accumulated_submission_time': 44154.71737384796, 'accumulated_eval_time': 4788.12454199791, 'accumulated_logging_time': 4.246379375457764, 'global_step': 94962, 'preemption_count': 0}), (95868, {'train/accuracy': 0.7100390195846558, 'train/loss': 1.2094417810440063, 'validation/accuracy': 0.6509999632835388, 'validation/loss': 1.471266269683838, 'validation/num_examples': 50000, 'test/accuracy': 0.531000018119812, 'test/loss': 2.119685411453247, 'test/num_examples': 10000, 'score': 44575.002772808075, 'total_duration': 49421.69991064072, 'accumulated_submission_time': 44575.002772808075, 'accumulated_eval_time': 4836.9977996349335, 'accumulated_logging_time': 4.29305100440979, 'global_step': 95868, 'preemption_count': 0}), (96774, {'train/accuracy': 0.7082812190055847, 'train/loss': 1.2541940212249756, 'validation/accuracy': 0.6438999772071838, 'validation/loss': 1.5317820310592651, 'validation/num_examples': 50000, 'test/accuracy': 0.525600016117096, 'test/loss': 2.174257516860962, 'test/num_examples': 10000, 'score': 44995.451451301575, 'total_duration': 49891.23855257034, 'accumulated_submission_time': 44995.451451301575, 'accumulated_eval_time': 4885.990926504135, 'accumulated_logging_time': 4.338782787322998, 'global_step': 96774, 'preemption_count': 0}), (97681, {'train/accuracy': 0.7260546684265137, 'train/loss': 1.110514760017395, 'validation/accuracy': 0.6534599661827087, 'validation/loss': 1.432780385017395, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.0747039318084717, 'test/num_examples': 10000, 'score': 45415.572922706604, 'total_duration': 50359.1891078949, 'accumulated_submission_time': 45415.572922706604, 'accumulated_eval_time': 4933.7262399196625, 'accumulated_logging_time': 4.382209062576294, 'global_step': 97681, 'preemption_count': 0}), (98584, {'train/accuracy': 0.7095507383346558, 'train/loss': 1.2087146043777466, 'validation/accuracy': 0.6518399715423584, 'validation/loss': 1.4704029560089111, 'validation/num_examples': 50000, 'test/accuracy': 0.5313000082969666, 'test/loss': 2.124727725982666, 'test/num_examples': 10000, 'score': 45835.61195087433, 'total_duration': 50823.40377354622, 'accumulated_submission_time': 45835.61195087433, 'accumulated_eval_time': 4977.804324626923, 'accumulated_logging_time': 4.428659439086914, 'global_step': 98584, 'preemption_count': 0}), (99487, {'train/accuracy': 0.7159179449081421, 'train/loss': 1.143684983253479, 'validation/accuracy': 0.660539984703064, 'validation/loss': 1.4055538177490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5333000421524048, 'test/loss': 2.0649092197418213, 'test/num_examples': 10000, 'score': 46255.788565158844, 'total_duration': 51290.96784090996, 'accumulated_submission_time': 46255.788565158844, 'accumulated_eval_time': 5025.0931096076965, 'accumulated_logging_time': 4.4764626026153564, 'global_step': 99487, 'preemption_count': 0}), (100394, {'train/accuracy': 0.7255663871765137, 'train/loss': 1.114113211631775, 'validation/accuracy': 0.6623600125312805, 'validation/loss': 1.4175740480422974, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.0686087608337402, 'test/num_examples': 10000, 'score': 46676.1024954319, 'total_duration': 51759.92477321625, 'accumulated_submission_time': 46676.1024954319, 'accumulated_eval_time': 5073.639664173126, 'accumulated_logging_time': 4.521659851074219, 'global_step': 100394, 'preemption_count': 0}), (101303, {'train/accuracy': 0.7194140553474426, 'train/loss': 1.1470216512680054, 'validation/accuracy': 0.6613399982452393, 'validation/loss': 1.3947124481201172, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.0668838024139404, 'test/num_examples': 10000, 'score': 47096.401584386826, 'total_duration': 52227.28931951523, 'accumulated_submission_time': 47096.401584386826, 'accumulated_eval_time': 5120.609957456589, 'accumulated_logging_time': 4.565702676773071, 'global_step': 101303, 'preemption_count': 0}), (102211, {'train/accuracy': 0.7141796946525574, 'train/loss': 1.1836493015289307, 'validation/accuracy': 0.6551799774169922, 'validation/loss': 1.4438341856002808, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.0949273109436035, 'test/num_examples': 10000, 'score': 47516.692006111145, 'total_duration': 52693.0149166584, 'accumulated_submission_time': 47516.692006111145, 'accumulated_eval_time': 5165.949766874313, 'accumulated_logging_time': 4.6092352867126465, 'global_step': 102211, 'preemption_count': 0}), (103115, {'train/accuracy': 0.7273827791213989, 'train/loss': 1.1171965599060059, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.4077105522155762, 'validation/num_examples': 50000, 'test/accuracy': 0.5384000539779663, 'test/loss': 2.0607264041900635, 'test/num_examples': 10000, 'score': 47937.02245235443, 'total_duration': 53161.830530405045, 'accumulated_submission_time': 47937.02245235443, 'accumulated_eval_time': 5214.342481613159, 'accumulated_logging_time': 4.651084899902344, 'global_step': 103115, 'preemption_count': 0}), (104018, {'train/accuracy': 0.7233007550239563, 'train/loss': 1.127516508102417, 'validation/accuracy': 0.6676799654960632, 'validation/loss': 1.3816344738006592, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.032897472381592, 'test/num_examples': 10000, 'score': 48357.16518211365, 'total_duration': 53627.07335996628, 'accumulated_submission_time': 48357.16518211365, 'accumulated_eval_time': 5259.345903396606, 'accumulated_logging_time': 4.697351932525635, 'global_step': 104018, 'preemption_count': 0}), (104923, {'train/accuracy': 0.7226171493530273, 'train/loss': 1.2302206754684448, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.492753267288208, 'validation/num_examples': 50000, 'test/accuracy': 0.538100004196167, 'test/loss': 2.138030767440796, 'test/num_examples': 10000, 'score': 48777.29816532135, 'total_duration': 54094.22512769699, 'accumulated_submission_time': 48777.29816532135, 'accumulated_eval_time': 5306.26788520813, 'accumulated_logging_time': 4.742557764053345, 'global_step': 104923, 'preemption_count': 0}), (105828, {'train/accuracy': 0.7344140410423279, 'train/loss': 1.0741634368896484, 'validation/accuracy': 0.6691799759864807, 'validation/loss': 1.373185396194458, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 2.021711826324463, 'test/num_examples': 10000, 'score': 49197.20958852768, 'total_duration': 54559.56789302826, 'accumulated_submission_time': 49197.20958852768, 'accumulated_eval_time': 5351.597653627396, 'accumulated_logging_time': 4.792775630950928, 'global_step': 105828, 'preemption_count': 0}), (106733, {'train/accuracy': 0.7335546612739563, 'train/loss': 1.0971087217330933, 'validation/accuracy': 0.6657599806785583, 'validation/loss': 1.4038810729980469, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.057142734527588, 'test/num_examples': 10000, 'score': 49617.28916144371, 'total_duration': 55028.830676317215, 'accumulated_submission_time': 49617.28916144371, 'accumulated_eval_time': 5400.683298826218, 'accumulated_logging_time': 4.839393138885498, 'global_step': 106733, 'preemption_count': 0}), (107638, {'train/accuracy': 0.7317773103713989, 'train/loss': 1.0785207748413086, 'validation/accuracy': 0.6732400059700012, 'validation/loss': 1.3483539819717407, 'validation/num_examples': 50000, 'test/accuracy': 0.5491999983787537, 'test/loss': 1.9994500875473022, 'test/num_examples': 10000, 'score': 50037.69368457794, 'total_duration': 55492.862330675125, 'accumulated_submission_time': 50037.69368457794, 'accumulated_eval_time': 5444.214246273041, 'accumulated_logging_time': 4.885617017745972, 'global_step': 107638, 'preemption_count': 0}), (108542, {'train/accuracy': 0.7343358993530273, 'train/loss': 1.076116919517517, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.35907781124115, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.020024299621582, 'test/num_examples': 10000, 'score': 50458.04724955559, 'total_duration': 55961.151431560516, 'accumulated_submission_time': 50458.04724955559, 'accumulated_eval_time': 5492.0554802417755, 'accumulated_logging_time': 4.9278857707977295, 'global_step': 108542, 'preemption_count': 0}), (109444, {'train/accuracy': 0.7549999952316284, 'train/loss': 1.0015008449554443, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.374889612197876, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.0300540924072266, 'test/num_examples': 10000, 'score': 50877.9556055069, 'total_duration': 56428.23986721039, 'accumulated_submission_time': 50877.9556055069, 'accumulated_eval_time': 5539.1366448402405, 'accumulated_logging_time': 4.976301908493042, 'global_step': 109444, 'preemption_count': 0}), (110352, {'train/accuracy': 0.7322655916213989, 'train/loss': 1.0748839378356934, 'validation/accuracy': 0.6771999597549438, 'validation/loss': 1.3334211111068726, 'validation/num_examples': 50000, 'test/accuracy': 0.5463000535964966, 'test/loss': 1.9860961437225342, 'test/num_examples': 10000, 'score': 51298.14277744293, 'total_duration': 56895.654056310654, 'accumulated_submission_time': 51298.14277744293, 'accumulated_eval_time': 5586.264869213104, 'accumulated_logging_time': 5.022613763809204, 'global_step': 110352, 'preemption_count': 0}), (111258, {'train/accuracy': 0.7407421469688416, 'train/loss': 1.0589429140090942, 'validation/accuracy': 0.6723399758338928, 'validation/loss': 1.357522964477539, 'validation/num_examples': 50000, 'test/accuracy': 0.5458000302314758, 'test/loss': 2.025487184524536, 'test/num_examples': 10000, 'score': 51718.239069223404, 'total_duration': 57364.200476169586, 'accumulated_submission_time': 51718.239069223404, 'accumulated_eval_time': 5634.615519762039, 'accumulated_logging_time': 5.071029901504517, 'global_step': 111258, 'preemption_count': 0}), (112164, {'train/accuracy': 0.7571874856948853, 'train/loss': 0.9590351581573486, 'validation/accuracy': 0.6782799959182739, 'validation/loss': 1.3155368566513062, 'validation/num_examples': 50000, 'test/accuracy': 0.550000011920929, 'test/loss': 1.9666587114334106, 'test/num_examples': 10000, 'score': 52138.199348688126, 'total_duration': 57832.32154393196, 'accumulated_submission_time': 52138.199348688126, 'accumulated_eval_time': 5682.680174589157, 'accumulated_logging_time': 5.116419792175293, 'global_step': 112164, 'preemption_count': 0}), (113066, {'train/accuracy': 0.7348827719688416, 'train/loss': 1.1102678775787354, 'validation/accuracy': 0.6744199991226196, 'validation/loss': 1.374962568283081, 'validation/num_examples': 50000, 'test/accuracy': 0.5481000542640686, 'test/loss': 2.0241758823394775, 'test/num_examples': 10000, 'score': 52558.10915327072, 'total_duration': 58299.82621335983, 'accumulated_submission_time': 52558.10915327072, 'accumulated_eval_time': 5730.1758716106415, 'accumulated_logging_time': 5.1636152267456055, 'global_step': 113066, 'preemption_count': 0}), (113970, {'train/accuracy': 0.7422655820846558, 'train/loss': 1.044321060180664, 'validation/accuracy': 0.6773599982261658, 'validation/loss': 1.3351013660430908, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.0046536922454834, 'test/num_examples': 10000, 'score': 52978.40332150459, 'total_duration': 58768.11574149132, 'accumulated_submission_time': 52978.40332150459, 'accumulated_eval_time': 5778.074056625366, 'accumulated_logging_time': 5.209723234176636, 'global_step': 113970, 'preemption_count': 0}), (114873, {'train/accuracy': 0.7559765577316284, 'train/loss': 0.9663622975349426, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3073861598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 1.9546692371368408, 'test/num_examples': 10000, 'score': 53398.46142745018, 'total_duration': 59231.86329960823, 'accumulated_submission_time': 53398.46142745018, 'accumulated_eval_time': 5821.660964488983, 'accumulated_logging_time': 5.261880159378052, 'global_step': 114873, 'preemption_count': 0}), (115777, {'train/accuracy': 0.7392773032188416, 'train/loss': 1.056602954864502, 'validation/accuracy': 0.6826800107955933, 'validation/loss': 1.3141595125198364, 'validation/num_examples': 50000, 'test/accuracy': 0.5634000301361084, 'test/loss': 1.9466331005096436, 'test/num_examples': 10000, 'score': 53818.511486291885, 'total_duration': 59700.5247297287, 'accumulated_submission_time': 53818.511486291885, 'accumulated_eval_time': 5870.175407886505, 'accumulated_logging_time': 5.30842137336731, 'global_step': 115777, 'preemption_count': 0}), (116682, {'train/accuracy': 0.7460546493530273, 'train/loss': 1.0224932432174683, 'validation/accuracy': 0.6796199679374695, 'validation/loss': 1.3186691999435425, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 1.9711726903915405, 'test/num_examples': 10000, 'score': 54238.54416680336, 'total_duration': 60169.58668136597, 'accumulated_submission_time': 54238.54416680336, 'accumulated_eval_time': 5919.104496240616, 'accumulated_logging_time': 5.356947898864746, 'global_step': 116682, 'preemption_count': 0}), (117587, {'train/accuracy': 0.7542187571525574, 'train/loss': 1.0017627477645874, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.3278917074203491, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.97365140914917, 'test/num_examples': 10000, 'score': 54658.53148937225, 'total_duration': 60636.760419130325, 'accumulated_submission_time': 54658.53148937225, 'accumulated_eval_time': 5966.191679239273, 'accumulated_logging_time': 5.405437469482422, 'global_step': 117587, 'preemption_count': 0}), (118493, {'train/accuracy': 0.7477148175239563, 'train/loss': 1.0368415117263794, 'validation/accuracy': 0.6837999820709229, 'validation/loss': 1.3157402276992798, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 1.9573954343795776, 'test/num_examples': 10000, 'score': 55078.50196790695, 'total_duration': 61104.90820026398, 'accumulated_submission_time': 55078.50196790695, 'accumulated_eval_time': 6014.26641201973, 'accumulated_logging_time': 5.455573797225952, 'global_step': 118493, 'preemption_count': 0}), (119399, {'train/accuracy': 0.7528710961341858, 'train/loss': 0.9768688082695007, 'validation/accuracy': 0.6881799697875977, 'validation/loss': 1.2712483406066895, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.9199223518371582, 'test/num_examples': 10000, 'score': 55498.71514606476, 'total_duration': 61567.28264904022, 'accumulated_submission_time': 55498.71514606476, 'accumulated_eval_time': 6056.332176923752, 'accumulated_logging_time': 5.500463962554932, 'global_step': 119399, 'preemption_count': 0}), (120301, {'train/accuracy': 0.7587109208106995, 'train/loss': 0.9645220637321472, 'validation/accuracy': 0.6875999569892883, 'validation/loss': 1.2869911193847656, 'validation/num_examples': 50000, 'test/accuracy': 0.5645000338554382, 'test/loss': 1.9446827173233032, 'test/num_examples': 10000, 'score': 55918.85990691185, 'total_duration': 62033.258112192154, 'accumulated_submission_time': 55918.85990691185, 'accumulated_eval_time': 6102.064264535904, 'accumulated_logging_time': 5.548049688339233, 'global_step': 120301, 'preemption_count': 0}), (121206, {'train/accuracy': 0.749804675579071, 'train/loss': 0.9959432482719421, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.2766941785812378, 'validation/num_examples': 50000, 'test/accuracy': 0.5652000308036804, 'test/loss': 1.9142930507659912, 'test/num_examples': 10000, 'score': 56338.76570916176, 'total_duration': 62500.47589445114, 'accumulated_submission_time': 56338.76570916176, 'accumulated_eval_time': 6149.277095556259, 'accumulated_logging_time': 5.596048831939697, 'global_step': 121206, 'preemption_count': 0}), (122110, {'train/accuracy': 0.7586718797683716, 'train/loss': 0.9999610781669617, 'validation/accuracy': 0.6941399574279785, 'validation/loss': 1.2884547710418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 1.9204736948013306, 'test/num_examples': 10000, 'score': 56758.85825443268, 'total_duration': 62969.41731142998, 'accumulated_submission_time': 56758.85825443268, 'accumulated_eval_time': 6198.029419660568, 'accumulated_logging_time': 5.6411542892456055, 'global_step': 122110, 'preemption_count': 0}), (123017, {'train/accuracy': 0.7599804401397705, 'train/loss': 0.9783530235290527, 'validation/accuracy': 0.6932199597358704, 'validation/loss': 1.2936484813690186, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9345817565917969, 'test/num_examples': 10000, 'score': 57178.785105228424, 'total_duration': 63437.81168818474, 'accumulated_submission_time': 57178.785105228424, 'accumulated_eval_time': 6246.399075984955, 'accumulated_logging_time': 5.687482833862305, 'global_step': 123017, 'preemption_count': 0}), (123923, {'train/accuracy': 0.7659960985183716, 'train/loss': 0.9347695708274841, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.2383933067321777, 'validation/num_examples': 50000, 'test/accuracy': 0.5713000297546387, 'test/loss': 1.8837766647338867, 'test/num_examples': 10000, 'score': 57598.70680522919, 'total_duration': 63905.06403899193, 'accumulated_submission_time': 57598.70680522919, 'accumulated_eval_time': 6293.630912065506, 'accumulated_logging_time': 5.734906911849976, 'global_step': 123923, 'preemption_count': 0}), (124829, {'train/accuracy': 0.763378918170929, 'train/loss': 0.9280872344970703, 'validation/accuracy': 0.6961399912834167, 'validation/loss': 1.2279818058013916, 'validation/num_examples': 50000, 'test/accuracy': 0.5699000358581543, 'test/loss': 1.8792723417282104, 'test/num_examples': 10000, 'score': 58018.94120192528, 'total_duration': 64375.66458725929, 'accumulated_submission_time': 58018.94120192528, 'accumulated_eval_time': 6343.900760173798, 'accumulated_logging_time': 5.780289649963379, 'global_step': 124829, 'preemption_count': 0}), (125733, {'train/accuracy': 0.7677538990974426, 'train/loss': 0.9241829514503479, 'validation/accuracy': 0.6993599534034729, 'validation/loss': 1.229907751083374, 'validation/num_examples': 50000, 'test/accuracy': 0.5800999999046326, 'test/loss': 1.8638954162597656, 'test/num_examples': 10000, 'score': 58438.90938568115, 'total_duration': 64843.013377428055, 'accumulated_submission_time': 58438.90938568115, 'accumulated_eval_time': 6391.183966159821, 'accumulated_logging_time': 5.8267786502838135, 'global_step': 125733, 'preemption_count': 0}), (126635, {'train/accuracy': 0.78236323595047, 'train/loss': 0.8538432121276855, 'validation/accuracy': 0.7017399668693542, 'validation/loss': 1.2105683088302612, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.8391834497451782, 'test/num_examples': 10000, 'score': 58859.0885219574, 'total_duration': 65312.81332349777, 'accumulated_submission_time': 58859.0885219574, 'accumulated_eval_time': 6440.7057983875275, 'accumulated_logging_time': 5.875086545944214, 'global_step': 126635, 'preemption_count': 0}), (127542, {'train/accuracy': 0.759082019329071, 'train/loss': 0.964400053024292, 'validation/accuracy': 0.6955400109291077, 'validation/loss': 1.2564650774002075, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 1.9025813341140747, 'test/num_examples': 10000, 'score': 59279.26599240303, 'total_duration': 65779.41101312637, 'accumulated_submission_time': 59279.26599240303, 'accumulated_eval_time': 6487.029499053955, 'accumulated_logging_time': 5.920918226242065, 'global_step': 127542, 'preemption_count': 0}), (128448, {'train/accuracy': 0.774609386920929, 'train/loss': 0.8842771053314209, 'validation/accuracy': 0.701259970664978, 'validation/loss': 1.2053685188293457, 'validation/num_examples': 50000, 'test/accuracy': 0.5759000182151794, 'test/loss': 1.8524872064590454, 'test/num_examples': 10000, 'score': 59699.49506402016, 'total_duration': 66246.55462884903, 'accumulated_submission_time': 59699.49506402016, 'accumulated_eval_time': 6533.84437918663, 'accumulated_logging_time': 5.970127820968628, 'global_step': 128448, 'preemption_count': 0}), (129353, {'train/accuracy': 0.7856835722923279, 'train/loss': 0.8250307440757751, 'validation/accuracy': 0.7035599946975708, 'validation/loss': 1.1953197717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.5803000330924988, 'test/loss': 1.8332209587097168, 'test/num_examples': 10000, 'score': 60119.54176044464, 'total_duration': 66712.03279447556, 'accumulated_submission_time': 60119.54176044464, 'accumulated_eval_time': 6579.178344249725, 'accumulated_logging_time': 6.016943454742432, 'global_step': 129353, 'preemption_count': 0}), (130258, {'train/accuracy': 0.7689648270606995, 'train/loss': 0.9314327239990234, 'validation/accuracy': 0.7040199637413025, 'validation/loss': 1.226270079612732, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.872269868850708, 'test/num_examples': 10000, 'score': 60539.66762089729, 'total_duration': 67180.1879966259, 'accumulated_submission_time': 60539.66762089729, 'accumulated_eval_time': 6627.108244419098, 'accumulated_logging_time': 6.065059423446655, 'global_step': 130258, 'preemption_count': 0}), (131165, {'train/accuracy': 0.7744531035423279, 'train/loss': 0.9301571249961853, 'validation/accuracy': 0.7041199803352356, 'validation/loss': 1.241104245185852, 'validation/num_examples': 50000, 'test/accuracy': 0.589400053024292, 'test/loss': 1.8679510354995728, 'test/num_examples': 10000, 'score': 60959.68307638168, 'total_duration': 67647.81341028214, 'accumulated_submission_time': 60959.68307638168, 'accumulated_eval_time': 6674.615309238434, 'accumulated_logging_time': 6.117140293121338, 'global_step': 131165, 'preemption_count': 0}), (132069, {'train/accuracy': 0.7873827815055847, 'train/loss': 0.8701390624046326, 'validation/accuracy': 0.7058999538421631, 'validation/loss': 1.2221981287002563, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.8685365915298462, 'test/num_examples': 10000, 'score': 61379.67488145828, 'total_duration': 68112.85487318039, 'accumulated_submission_time': 61379.67488145828, 'accumulated_eval_time': 6719.562696695328, 'accumulated_logging_time': 6.168532371520996, 'global_step': 132069, 'preemption_count': 0}), (132973, {'train/accuracy': 0.7793359160423279, 'train/loss': 0.8791748881340027, 'validation/accuracy': 0.7087000012397766, 'validation/loss': 1.1858389377593994, 'validation/num_examples': 50000, 'test/accuracy': 0.5845000147819519, 'test/loss': 1.819374918937683, 'test/num_examples': 10000, 'score': 61799.59830021858, 'total_duration': 68583.33038425446, 'accumulated_submission_time': 61799.59830021858, 'accumulated_eval_time': 6770.016849517822, 'accumulated_logging_time': 6.2154810428619385, 'global_step': 132973, 'preemption_count': 0}), (133878, {'train/accuracy': 0.7819726467132568, 'train/loss': 0.849389910697937, 'validation/accuracy': 0.7128599882125854, 'validation/loss': 1.1591600179672241, 'validation/num_examples': 50000, 'test/accuracy': 0.5937000513076782, 'test/loss': 1.7851358652114868, 'test/num_examples': 10000, 'score': 62219.77271294594, 'total_duration': 69050.48225259781, 'accumulated_submission_time': 62219.77271294594, 'accumulated_eval_time': 6816.891093969345, 'accumulated_logging_time': 6.26798677444458, 'global_step': 133878, 'preemption_count': 0}), (134782, {'train/accuracy': 0.792773425579071, 'train/loss': 0.7919089198112488, 'validation/accuracy': 0.7138599753379822, 'validation/loss': 1.1439038515090942, 'validation/num_examples': 50000, 'test/accuracy': 0.5934000015258789, 'test/loss': 1.7752066850662231, 'test/num_examples': 10000, 'score': 62639.89648962021, 'total_duration': 69520.44361186028, 'accumulated_submission_time': 62639.89648962021, 'accumulated_eval_time': 6866.628557682037, 'accumulated_logging_time': 6.316884279251099, 'global_step': 134782, 'preemption_count': 0}), (135682, {'train/accuracy': 0.7776952981948853, 'train/loss': 0.8922684788703918, 'validation/accuracy': 0.7090799808502197, 'validation/loss': 1.2007648944854736, 'validation/num_examples': 50000, 'test/accuracy': 0.5867000222206116, 'test/loss': 1.8380101919174194, 'test/num_examples': 10000, 'score': 63059.385607004166, 'total_duration': 69984.6502289772, 'accumulated_submission_time': 63059.385607004166, 'accumulated_eval_time': 6910.820559263229, 'accumulated_logging_time': 6.79111123085022, 'global_step': 135682, 'preemption_count': 0}), (136587, {'train/accuracy': 0.7864648103713989, 'train/loss': 0.8539221286773682, 'validation/accuracy': 0.715499997138977, 'validation/loss': 1.170691967010498, 'validation/num_examples': 50000, 'test/accuracy': 0.5918000340461731, 'test/loss': 1.7970510721206665, 'test/num_examples': 10000, 'score': 63479.35899710655, 'total_duration': 70449.76348471642, 'accumulated_submission_time': 63479.35899710655, 'accumulated_eval_time': 6955.8587481975555, 'accumulated_logging_time': 6.841918230056763, 'global_step': 136587, 'preemption_count': 0}), (137494, {'train/accuracy': 0.79066401720047, 'train/loss': 0.8283480405807495, 'validation/accuracy': 0.7112599611282349, 'validation/loss': 1.1805391311645508, 'validation/num_examples': 50000, 'test/accuracy': 0.5878000259399414, 'test/loss': 1.8127272129058838, 'test/num_examples': 10000, 'score': 63899.612374305725, 'total_duration': 70919.30957722664, 'accumulated_submission_time': 63899.612374305725, 'accumulated_eval_time': 7005.048368930817, 'accumulated_logging_time': 6.893892765045166, 'global_step': 137494, 'preemption_count': 0}), (138400, {'train/accuracy': 0.7880859375, 'train/loss': 0.8579350709915161, 'validation/accuracy': 0.7173199653625488, 'validation/loss': 1.1725926399230957, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.8064581155776978, 'test/num_examples': 10000, 'score': 64319.660131692886, 'total_duration': 71387.13676571846, 'accumulated_submission_time': 64319.660131692886, 'accumulated_eval_time': 7052.7244708538055, 'accumulated_logging_time': 6.946126461029053, 'global_step': 138400, 'preemption_count': 0}), (139301, {'train/accuracy': 0.7901171445846558, 'train/loss': 0.8141428828239441, 'validation/accuracy': 0.7179799675941467, 'validation/loss': 1.1384248733520508, 'validation/num_examples': 50000, 'test/accuracy': 0.5958999991416931, 'test/loss': 1.7701454162597656, 'test/num_examples': 10000, 'score': 64739.67565011978, 'total_duration': 71854.72164583206, 'accumulated_submission_time': 64739.67565011978, 'accumulated_eval_time': 7100.193810462952, 'accumulated_logging_time': 6.996117830276489, 'global_step': 139301, 'preemption_count': 0}), (140204, {'train/accuracy': 0.8003124594688416, 'train/loss': 0.7800408601760864, 'validation/accuracy': 0.717739999294281, 'validation/loss': 1.1377885341644287, 'validation/num_examples': 50000, 'test/accuracy': 0.5996000170707703, 'test/loss': 1.7611976861953735, 'test/num_examples': 10000, 'score': 65159.74625015259, 'total_duration': 72324.3936612606, 'accumulated_submission_time': 65159.74625015259, 'accumulated_eval_time': 7149.690311908722, 'accumulated_logging_time': 7.050333499908447, 'global_step': 140204, 'preemption_count': 0}), (141109, {'train/accuracy': 0.7925195097923279, 'train/loss': 0.8001589179039001, 'validation/accuracy': 0.7177000045776367, 'validation/loss': 1.1277657747268677, 'validation/num_examples': 50000, 'test/accuracy': 0.5931000113487244, 'test/loss': 1.7584959268569946, 'test/num_examples': 10000, 'score': 65579.95306015015, 'total_duration': 72789.65150928497, 'accumulated_submission_time': 65579.95306015015, 'accumulated_eval_time': 7194.6404457092285, 'accumulated_logging_time': 7.10044264793396, 'global_step': 141109, 'preemption_count': 0}), (142014, {'train/accuracy': 0.8015038967132568, 'train/loss': 0.7711244821548462, 'validation/accuracy': 0.722599983215332, 'validation/loss': 1.1164039373397827, 'validation/num_examples': 50000, 'test/accuracy': 0.6005000472068787, 'test/loss': 1.7380754947662354, 'test/num_examples': 10000, 'score': 65999.96157240868, 'total_duration': 73259.56003165245, 'accumulated_submission_time': 65999.96157240868, 'accumulated_eval_time': 7244.440073251724, 'accumulated_logging_time': 7.150071620941162, 'global_step': 142014, 'preemption_count': 0}), (142915, {'train/accuracy': 0.8013671636581421, 'train/loss': 0.7719994187355042, 'validation/accuracy': 0.721340000629425, 'validation/loss': 1.121065616607666, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.7464487552642822, 'test/num_examples': 10000, 'score': 66420.08053159714, 'total_duration': 73728.358700037, 'accumulated_submission_time': 66420.08053159714, 'accumulated_eval_time': 7293.018522024155, 'accumulated_logging_time': 7.19990348815918, 'global_step': 142915, 'preemption_count': 0}), (143819, {'train/accuracy': 0.8026562333106995, 'train/loss': 0.7779040932655334, 'validation/accuracy': 0.7221399545669556, 'validation/loss': 1.1373088359832764, 'validation/num_examples': 50000, 'test/accuracy': 0.5993000268936157, 'test/loss': 1.7647473812103271, 'test/num_examples': 10000, 'score': 66840.15920996666, 'total_duration': 74196.6847281456, 'accumulated_submission_time': 66840.15920996666, 'accumulated_eval_time': 7341.1628839969635, 'accumulated_logging_time': 7.251514196395874, 'global_step': 143819, 'preemption_count': 0}), (144724, {'train/accuracy': 0.7992382645606995, 'train/loss': 0.7773339748382568, 'validation/accuracy': 0.7215799689292908, 'validation/loss': 1.1161545515060425, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.7557260990142822, 'test/num_examples': 10000, 'score': 67260.12916207314, 'total_duration': 74660.76443743706, 'accumulated_submission_time': 67260.12916207314, 'accumulated_eval_time': 7385.170194864273, 'accumulated_logging_time': 7.303040027618408, 'global_step': 144724, 'preemption_count': 0}), (145628, {'train/accuracy': 0.80517578125, 'train/loss': 0.7599520683288574, 'validation/accuracy': 0.7254599928855896, 'validation/loss': 1.1081088781356812, 'validation/num_examples': 50000, 'test/accuracy': 0.6051000356674194, 'test/loss': 1.739145040512085, 'test/num_examples': 10000, 'score': 67680.05412721634, 'total_duration': 75128.43763279915, 'accumulated_submission_time': 67680.05412721634, 'accumulated_eval_time': 7432.81751871109, 'accumulated_logging_time': 7.352954149246216, 'global_step': 145628, 'preemption_count': 0}), (146535, {'train/accuracy': 0.8170703053474426, 'train/loss': 0.7173280119895935, 'validation/accuracy': 0.7281399965286255, 'validation/loss': 1.1051756143569946, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.7289841175079346, 'test/num_examples': 10000, 'score': 68100.3693766594, 'total_duration': 75593.36808228493, 'accumulated_submission_time': 68100.3693766594, 'accumulated_eval_time': 7477.329308271408, 'accumulated_logging_time': 7.4053473472595215, 'global_step': 146535, 'preemption_count': 0}), (147440, {'train/accuracy': 0.8044531345367432, 'train/loss': 0.7989120483398438, 'validation/accuracy': 0.7260000109672546, 'validation/loss': 1.1396101713180542, 'validation/num_examples': 50000, 'test/accuracy': 0.601900041103363, 'test/loss': 1.774980902671814, 'test/num_examples': 10000, 'score': 68520.51922082901, 'total_duration': 76059.94834542274, 'accumulated_submission_time': 68520.51922082901, 'accumulated_eval_time': 7523.654905796051, 'accumulated_logging_time': 7.458659648895264, 'global_step': 147440, 'preemption_count': 0}), (148347, {'train/accuracy': 0.8073046803474426, 'train/loss': 0.7472729086875916, 'validation/accuracy': 0.7283799648284912, 'validation/loss': 1.0935417413711548, 'validation/num_examples': 50000, 'test/accuracy': 0.6075000166893005, 'test/loss': 1.7274893522262573, 'test/num_examples': 10000, 'score': 68940.56979131699, 'total_duration': 76524.28863739967, 'accumulated_submission_time': 68940.56979131699, 'accumulated_eval_time': 7567.843139410019, 'accumulated_logging_time': 7.50982141494751, 'global_step': 148347, 'preemption_count': 0}), (149250, {'train/accuracy': 0.81214839220047, 'train/loss': 0.7340012192726135, 'validation/accuracy': 0.7254999876022339, 'validation/loss': 1.1147717237472534, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.7510703802108765, 'test/num_examples': 10000, 'score': 69360.65314507484, 'total_duration': 76988.09020090103, 'accumulated_submission_time': 69360.65314507484, 'accumulated_eval_time': 7611.454800367355, 'accumulated_logging_time': 7.564750909805298, 'global_step': 149250, 'preemption_count': 0}), (150156, {'train/accuracy': 0.8089648485183716, 'train/loss': 0.7477630972862244, 'validation/accuracy': 0.7283399701118469, 'validation/loss': 1.0934544801712036, 'validation/num_examples': 50000, 'test/accuracy': 0.6111000180244446, 'test/loss': 1.714582920074463, 'test/num_examples': 10000, 'score': 69780.64332485199, 'total_duration': 77452.66871452332, 'accumulated_submission_time': 69780.64332485199, 'accumulated_eval_time': 7655.94317984581, 'accumulated_logging_time': 7.613767862319946, 'global_step': 150156, 'preemption_count': 0}), (151062, {'train/accuracy': 0.8106249570846558, 'train/loss': 0.7664503455162048, 'validation/accuracy': 0.7301999926567078, 'validation/loss': 1.119113564491272, 'validation/num_examples': 50000, 'test/accuracy': 0.6077000498771667, 'test/loss': 1.7483646869659424, 'test/num_examples': 10000, 'score': 70200.61243200302, 'total_duration': 77918.21200990677, 'accumulated_submission_time': 70200.61243200302, 'accumulated_eval_time': 7701.411295890808, 'accumulated_logging_time': 7.668791770935059, 'global_step': 151062, 'preemption_count': 0}), (151966, {'train/accuracy': 0.8160937428474426, 'train/loss': 0.7283310294151306, 'validation/accuracy': 0.7328400015830994, 'validation/loss': 1.0975955724716187, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.7178739309310913, 'test/num_examples': 10000, 'score': 70620.5970556736, 'total_duration': 78385.43194818497, 'accumulated_submission_time': 70620.5970556736, 'accumulated_eval_time': 7748.537898540497, 'accumulated_logging_time': 7.726431369781494, 'global_step': 151966, 'preemption_count': 0}), (152872, {'train/accuracy': 0.8132226467132568, 'train/loss': 0.7306903004646301, 'validation/accuracy': 0.7350199818611145, 'validation/loss': 1.078985333442688, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.705723762512207, 'test/num_examples': 10000, 'score': 71040.94093084335, 'total_duration': 78848.79572200775, 'accumulated_submission_time': 71040.94093084335, 'accumulated_eval_time': 7791.45180106163, 'accumulated_logging_time': 7.7803356647491455, 'global_step': 152872, 'preemption_count': 0}), (153775, {'train/accuracy': 0.81068354845047, 'train/loss': 0.7280431389808655, 'validation/accuracy': 0.7346599698066711, 'validation/loss': 1.074668526649475, 'validation/num_examples': 50000, 'test/accuracy': 0.6076000332832336, 'test/loss': 1.7089613676071167, 'test/num_examples': 10000, 'score': 71460.94529819489, 'total_duration': 79316.3397808075, 'accumulated_submission_time': 71460.94529819489, 'accumulated_eval_time': 7838.883292198181, 'accumulated_logging_time': 7.837963819503784, 'global_step': 153775, 'preemption_count': 0}), (154682, {'train/accuracy': 0.8229491710662842, 'train/loss': 0.6811491250991821, 'validation/accuracy': 0.7339999675750732, 'validation/loss': 1.0581152439117432, 'validation/num_examples': 50000, 'test/accuracy': 0.6127000451087952, 'test/loss': 1.691803216934204, 'test/num_examples': 10000, 'score': 71881.10067653656, 'total_duration': 79785.20401096344, 'accumulated_submission_time': 71881.10067653656, 'accumulated_eval_time': 7887.490196943283, 'accumulated_logging_time': 7.888981103897095, 'global_step': 154682, 'preemption_count': 0}), (155586, {'train/accuracy': 0.8204687237739563, 'train/loss': 0.7243859767913818, 'validation/accuracy': 0.7372999787330627, 'validation/loss': 1.0768547058105469, 'validation/num_examples': 50000, 'test/accuracy': 0.6158000230789185, 'test/loss': 1.7009111642837524, 'test/num_examples': 10000, 'score': 72301.3064084053, 'total_duration': 80253.32729268074, 'accumulated_submission_time': 72301.3064084053, 'accumulated_eval_time': 7935.300303936005, 'accumulated_logging_time': 7.945384502410889, 'global_step': 155586, 'preemption_count': 0}), (156487, {'train/accuracy': 0.8235155940055847, 'train/loss': 0.6817896962165833, 'validation/accuracy': 0.7389400005340576, 'validation/loss': 1.0453300476074219, 'validation/num_examples': 50000, 'test/accuracy': 0.615600049495697, 'test/loss': 1.6857863664627075, 'test/num_examples': 10000, 'score': 72721.32059788704, 'total_duration': 80723.79385137558, 'accumulated_submission_time': 72721.32059788704, 'accumulated_eval_time': 7985.649563550949, 'accumulated_logging_time': 7.997928142547607, 'global_step': 156487, 'preemption_count': 0}), (157392, {'train/accuracy': 0.8236913681030273, 'train/loss': 0.6922850012779236, 'validation/accuracy': 0.7382199764251709, 'validation/loss': 1.0663707256317139, 'validation/num_examples': 50000, 'test/accuracy': 0.6150000095367432, 'test/loss': 1.6935813426971436, 'test/num_examples': 10000, 'score': 73141.60916376114, 'total_duration': 81188.90914106369, 'accumulated_submission_time': 73141.60916376114, 'accumulated_eval_time': 8030.373905658722, 'accumulated_logging_time': 8.049262046813965, 'global_step': 157392, 'preemption_count': 0}), (158297, {'train/accuracy': 0.8206835985183716, 'train/loss': 0.6895581483840942, 'validation/accuracy': 0.7396399974822998, 'validation/loss': 1.0414702892303467, 'validation/num_examples': 50000, 'test/accuracy': 0.6205000281333923, 'test/loss': 1.6642597913742065, 'test/num_examples': 10000, 'score': 73561.83675098419, 'total_duration': 81657.4282822609, 'accumulated_submission_time': 73561.83675098419, 'accumulated_eval_time': 8078.557241678238, 'accumulated_logging_time': 8.106578588485718, 'global_step': 158297, 'preemption_count': 0}), (159204, {'train/accuracy': 0.82533198595047, 'train/loss': 0.7197791934013367, 'validation/accuracy': 0.7388399839401245, 'validation/loss': 1.0863151550292969, 'validation/num_examples': 50000, 'test/accuracy': 0.6186000108718872, 'test/loss': 1.7149657011032104, 'test/num_examples': 10000, 'score': 73981.86839866638, 'total_duration': 82122.87282919884, 'accumulated_submission_time': 73981.86839866638, 'accumulated_eval_time': 8123.8587374687195, 'accumulated_logging_time': 8.167153358459473, 'global_step': 159204, 'preemption_count': 0}), (160110, {'train/accuracy': 0.8278124928474426, 'train/loss': 0.6878633499145508, 'validation/accuracy': 0.7412199974060059, 'validation/loss': 1.0593788623809814, 'validation/num_examples': 50000, 'test/accuracy': 0.6185000538825989, 'test/loss': 1.684755802154541, 'test/num_examples': 10000, 'score': 74401.79539680481, 'total_duration': 82587.42238020897, 'accumulated_submission_time': 74401.79539680481, 'accumulated_eval_time': 8168.3786380290985, 'accumulated_logging_time': 8.218732833862305, 'global_step': 160110, 'preemption_count': 0}), (161015, {'train/accuracy': 0.82972651720047, 'train/loss': 0.6675693392753601, 'validation/accuracy': 0.7438399791717529, 'validation/loss': 1.0392054319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.6687285900115967, 'test/num_examples': 10000, 'score': 74821.9603896141, 'total_duration': 83056.85979914665, 'accumulated_submission_time': 74821.9603896141, 'accumulated_eval_time': 8217.548476457596, 'accumulated_logging_time': 8.271214008331299, 'global_step': 161015, 'preemption_count': 0}), (161921, {'train/accuracy': 0.8286718726158142, 'train/loss': 0.672528088092804, 'validation/accuracy': 0.7439000010490417, 'validation/loss': 1.04596745967865, 'validation/num_examples': 50000, 'test/accuracy': 0.6206000447273254, 'test/loss': 1.6684740781784058, 'test/num_examples': 10000, 'score': 75242.30540776253, 'total_duration': 83524.30050992966, 'accumulated_submission_time': 75242.30540776253, 'accumulated_eval_time': 8264.541722774506, 'accumulated_logging_time': 8.323035478591919, 'global_step': 161921, 'preemption_count': 0}), (162826, {'train/accuracy': 0.8308398127555847, 'train/loss': 0.6795992851257324, 'validation/accuracy': 0.743399977684021, 'validation/loss': 1.0521153211593628, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.676736831665039, 'test/num_examples': 10000, 'score': 75662.24251437187, 'total_duration': 83989.64229750633, 'accumulated_submission_time': 75662.24251437187, 'accumulated_eval_time': 8309.82877087593, 'accumulated_logging_time': 8.38948941230774, 'global_step': 162826, 'preemption_count': 0}), (163733, {'train/accuracy': 0.8407226204872131, 'train/loss': 0.6239649653434753, 'validation/accuracy': 0.7476399540901184, 'validation/loss': 1.0164140462875366, 'validation/num_examples': 50000, 'test/accuracy': 0.6276000142097473, 'test/loss': 1.6446024179458618, 'test/num_examples': 10000, 'score': 76082.43580412865, 'total_duration': 84458.79799222946, 'accumulated_submission_time': 76082.43580412865, 'accumulated_eval_time': 8358.68628025055, 'accumulated_logging_time': 8.442897319793701, 'global_step': 163733, 'preemption_count': 0}), (164639, {'train/accuracy': 0.8356835842132568, 'train/loss': 0.6445589065551758, 'validation/accuracy': 0.7468599677085876, 'validation/loss': 1.0233219861984253, 'validation/num_examples': 50000, 'test/accuracy': 0.6267000436782837, 'test/loss': 1.6496158838272095, 'test/num_examples': 10000, 'score': 76502.39468336105, 'total_duration': 84926.75717353821, 'accumulated_submission_time': 76502.39468336105, 'accumulated_eval_time': 8406.580387592316, 'accumulated_logging_time': 8.498199224472046, 'global_step': 164639, 'preemption_count': 0}), (165546, {'train/accuracy': 0.8334375023841858, 'train/loss': 0.6440165638923645, 'validation/accuracy': 0.7469399571418762, 'validation/loss': 1.0224330425262451, 'validation/num_examples': 50000, 'test/accuracy': 0.6241000294685364, 'test/loss': 1.6409838199615479, 'test/num_examples': 10000, 'score': 76922.57128500938, 'total_duration': 85391.76311731339, 'accumulated_submission_time': 76922.57128500938, 'accumulated_eval_time': 8451.303115844727, 'accumulated_logging_time': 8.553112983703613, 'global_step': 165546, 'preemption_count': 0}), (166451, {'train/accuracy': 0.8373242020606995, 'train/loss': 0.6317007541656494, 'validation/accuracy': 0.7473799586296082, 'validation/loss': 1.0245435237884521, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.6426352262496948, 'test/num_examples': 10000, 'score': 77342.91505241394, 'total_duration': 85861.23006987572, 'accumulated_submission_time': 77342.91505241394, 'accumulated_eval_time': 8500.320755720139, 'accumulated_logging_time': 8.607258081436157, 'global_step': 166451, 'preemption_count': 0})], 'global_step': 166841}
I0204 17:46:16.786545 140085747812160 submission_runner.py:586] Timing: 77520.45635008812
I0204 17:46:16.786622 140085747812160 submission_runner.py:588] Total number of evals: 185
I0204 17:46:16.786666 140085747812160 submission_runner.py:589] ====================
I0204 17:46:16.790135 140085747812160 submission_runner.py:673] Final imagenet_vit score: 77520.09283590317
