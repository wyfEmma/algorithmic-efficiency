python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_4 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=3917441912 --max_global_steps=80000 2>&1 | tee -a /logs/ogbg_jax_02-05-2024-03-48-25.log
I0205 03:48:46.300340 139978932307776 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_4/ogbg_jax.
I0205 03:48:47.374751 139978932307776 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0205 03:48:47.375486 139978932307776 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0205 03:48:47.375622 139978932307776 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0205 03:48:47.376615 139978932307776 submission_runner.py:542] Using RNG seed 3917441912
I0205 03:48:52.774855 139978932307776 submission_runner.py:551] --- Tuning run 1/5 ---
I0205 03:48:52.775082 139978932307776 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1.
I0205 03:48:52.775254 139978932307776 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1/hparams.json.
I0205 03:48:52.956655 139978932307776 submission_runner.py:206] Initializing dataset.
I0205 03:48:53.066418 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:48:53.070867 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0205 03:48:53.319519 139978932307776 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0205 03:48:53.379877 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:48:53.453793 139978932307776 submission_runner.py:213] Initializing model.
I0205 03:48:58.007196 139978932307776 submission_runner.py:255] Initializing optimizer.
I0205 03:48:58.654567 139978932307776 submission_runner.py:262] Initializing metrics bundle.
I0205 03:48:58.654778 139978932307776 submission_runner.py:280] Initializing checkpoint and logger.
I0205 03:48:58.655751 139978932307776 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1 with prefix checkpoint_
I0205 03:48:58.655896 139978932307776 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1/meta_data_0.json.
I0205 03:48:58.656080 139978932307776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 03:48:58.656156 139978932307776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 03:48:59.078248 139978932307776 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 03:48:59.467761 139978932307776 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1/flags_0.json.
I0205 03:48:59.477784 139978932307776 submission_runner.py:314] Starting training loop.
I0205 03:49:23.733493 139817073497856 logging_writer.py:48] [0] global_step=0, grad_norm=3.1866564750671387, loss=0.7992708086967468
I0205 03:49:23.750306 139978932307776 spec.py:321] Evaluating on the training split.
I0205 03:49:23.756341 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:49:23.760440 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 03:49:23.830394 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:51:17.115287 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 03:51:17.118873 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:51:17.122894 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 03:51:17.189952 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:52:49.601635 139978932307776 spec.py:349] Evaluating on the test split.
I0205 03:52:49.605071 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:52:49.609230 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 03:52:49.675645 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 03:54:25.729911 139978932307776 submission_runner.py:408] Time since start: 326.25s, 	Step: 1, 	{'train/accuracy': 0.38876962661743164, 'train/loss': 0.7993155121803284, 'train/mean_average_precision': 0.021555210797481506, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.024826656551027976, 'validation/num_examples': 43793, 'test/accuracy': 0.3947486877441406, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026365955872687814, 'test/num_examples': 43793, 'score': 24.272481203079224, 'total_duration': 326.25207114219666, 'accumulated_submission_time': 24.272481203079224, 'accumulated_eval_time': 301.979544878006, 'accumulated_logging_time': 0}
I0205 03:54:25.747643 139805429618432 logging_writer.py:48] [1] accumulated_eval_time=301.979545, accumulated_logging_time=0, accumulated_submission_time=24.272481, global_step=1, preemption_count=0, score=24.272481, test/accuracy=0.394749, test/loss=0.795676, test/mean_average_precision=0.026366, test/num_examples=43793, total_duration=326.252071, train/accuracy=0.388770, train/loss=0.799316, train/mean_average_precision=0.021555, validation/accuracy=0.392645, validation/loss=0.797466, validation/mean_average_precision=0.024827, validation/num_examples=43793
I0205 03:54:57.159433 139806545798912 logging_writer.py:48] [100] global_step=100, grad_norm=0.7065517902374268, loss=0.5043694376945496
I0205 03:55:28.835339 139805429618432 logging_writer.py:48] [200] global_step=200, grad_norm=0.4114476442337036, loss=0.36354872584342957
I0205 03:56:00.069792 139806545798912 logging_writer.py:48] [300] global_step=300, grad_norm=0.3005561828613281, loss=0.2604981064796448
I0205 03:56:32.131556 139805429618432 logging_writer.py:48] [400] global_step=400, grad_norm=0.2015238255262375, loss=0.17510759830474854
I0205 03:57:03.359462 139806545798912 logging_writer.py:48] [500] global_step=500, grad_norm=0.12493377178907394, loss=0.12229780107736588
I0205 03:57:34.698821 139805429618432 logging_writer.py:48] [600] global_step=600, grad_norm=0.08027274161577225, loss=0.08971703797578812
I0205 03:58:06.352503 139806545798912 logging_writer.py:48] [700] global_step=700, grad_norm=0.07393092662096024, loss=0.08478512614965439
I0205 03:58:25.857544 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:00:14.294615 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:00:17.276381 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:00:20.228824 139978932307776 submission_runner.py:408] Time since start: 680.75s, 	Step: 763, 	{'train/accuracy': 0.9866331219673157, 'train/loss': 0.07056180387735367, 'train/mean_average_precision': 0.0359794845055697, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07843790203332901, 'validation/mean_average_precision': 0.03719516993500635, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0812012106180191, 'test/mean_average_precision': 0.03837000188530284, 'test/num_examples': 43793, 'score': 264.3512933254242, 'total_duration': 680.7509214878082, 'accumulated_submission_time': 264.3512933254242, 'accumulated_eval_time': 416.35071659088135, 'accumulated_logging_time': 0.02980351448059082}
I0205 04:00:20.247295 139806453528320 logging_writer.py:48] [763] accumulated_eval_time=416.350717, accumulated_logging_time=0.029804, accumulated_submission_time=264.351293, global_step=763, preemption_count=0, score=264.351293, test/accuracy=0.983142, test/loss=0.081201, test/mean_average_precision=0.038370, test/num_examples=43793, total_duration=680.750921, train/accuracy=0.986633, train/loss=0.070562, train/mean_average_precision=0.035979, validation/accuracy=0.984118, validation/loss=0.078438, validation/mean_average_precision=0.037195, validation/num_examples=43793
I0205 04:00:32.653272 139806646249216 logging_writer.py:48] [800] global_step=800, grad_norm=0.117510586977005, loss=0.06160292029380798
I0205 04:01:04.629497 139806453528320 logging_writer.py:48] [900] global_step=900, grad_norm=0.03538474813103676, loss=0.06213982775807381
I0205 04:01:36.088673 139806646249216 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.08266612142324448, loss=0.06107577309012413
I0205 04:02:07.924268 139806453528320 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.09521576762199402, loss=0.055198028683662415
I0205 04:02:40.049408 139806646249216 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.10820673406124115, loss=0.04897866025567055
I0205 04:03:11.839568 139806453528320 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.1392708122730255, loss=0.04677373915910721
I0205 04:03:43.667615 139806646249216 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.13944149017333984, loss=0.045182351022958755
I0205 04:04:15.657003 139806453528320 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.11165841668844223, loss=0.054504454135894775
I0205 04:04:20.353058 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:06:14.274258 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:06:17.299768 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:06:20.308743 139978932307776 submission_runner.py:408] Time since start: 1040.83s, 	Step: 1516, 	{'train/accuracy': 0.9874165058135986, 'train/loss': 0.048196982592344284, 'train/mean_average_precision': 0.08383394494491832, 'validation/accuracy': 0.9846513271331787, 'validation/loss': 0.057339392602443695, 'validation/mean_average_precision': 0.08842731813172773, 'validation/num_examples': 43793, 'test/accuracy': 0.9836373925209045, 'test/loss': 0.06053172051906586, 'test/mean_average_precision': 0.08886458962019851, 'test/num_examples': 43793, 'score': 504.42682576179504, 'total_duration': 1040.8308537006378, 'accumulated_submission_time': 504.42682576179504, 'accumulated_eval_time': 536.3063020706177, 'accumulated_logging_time': 0.05943036079406738}
I0205 04:06:20.324203 139806545798912 logging_writer.py:48] [1516] accumulated_eval_time=536.306302, accumulated_logging_time=0.059430, accumulated_submission_time=504.426826, global_step=1516, preemption_count=0, score=504.426826, test/accuracy=0.983637, test/loss=0.060532, test/mean_average_precision=0.088865, test/num_examples=43793, total_duration=1040.830854, train/accuracy=0.987417, train/loss=0.048197, train/mean_average_precision=0.083834, validation/accuracy=0.984651, validation/loss=0.057339, validation/mean_average_precision=0.088427, validation/num_examples=43793
I0205 04:06:47.291411 139806637856512 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.14471407234668732, loss=0.05231033265590668
I0205 04:07:18.934705 139806545798912 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.1906478852033615, loss=0.049712419509887695
I0205 04:07:50.475689 139806637856512 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.17152084410190582, loss=0.04815772920846939
I0205 04:08:22.409562 139806545798912 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0779879242181778, loss=0.04572933539748192
I0205 04:08:54.164975 139806637856512 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.17210911214351654, loss=0.05039246752858162
I0205 04:09:26.069794 139806545798912 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.10318855941295624, loss=0.04419908672571182
I0205 04:09:57.855572 139806637856512 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.18620502948760986, loss=0.04529964551329613
I0205 04:10:20.538011 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:12:17.736340 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:12:20.749544 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:12:23.779569 139978932307776 submission_runner.py:408] Time since start: 1404.30s, 	Step: 2272, 	{'train/accuracy': 0.9878497123718262, 'train/loss': 0.04417021572589874, 'train/mean_average_precision': 0.13175346800292792, 'validation/accuracy': 0.9849797487258911, 'validation/loss': 0.05390032008290291, 'validation/mean_average_precision': 0.13002132410664413, 'validation/num_examples': 43793, 'test/accuracy': 0.9840400815010071, 'test/loss': 0.056891318410634995, 'test/mean_average_precision': 0.1302070232937932, 'test/num_examples': 43793, 'score': 744.610445022583, 'total_duration': 1404.3017330169678, 'accumulated_submission_time': 744.610445022583, 'accumulated_eval_time': 659.5478177070618, 'accumulated_logging_time': 0.08603835105895996}
I0205 04:12:23.798467 139806554191616 logging_writer.py:48] [2272] accumulated_eval_time=659.547818, accumulated_logging_time=0.086038, accumulated_submission_time=744.610445, global_step=2272, preemption_count=0, score=744.610445, test/accuracy=0.984040, test/loss=0.056891, test/mean_average_precision=0.130207, test/num_examples=43793, total_duration=1404.301733, train/accuracy=0.987850, train/loss=0.044170, train/mean_average_precision=0.131753, validation/accuracy=0.984980, validation/loss=0.053900, validation/mean_average_precision=0.130021, validation/num_examples=43793
I0205 04:12:33.271315 139806646249216 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.15422120690345764, loss=0.04794273525476456
I0205 04:13:05.340320 139806554191616 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.17344005405902863, loss=0.04739619046449661
I0205 04:13:37.109845 139806646249216 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.22667956352233887, loss=0.0451061837375164
I0205 04:14:08.722430 139806554191616 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.07314138114452362, loss=0.047130949795246124
I0205 04:14:40.203689 139806646249216 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.11542914062738419, loss=0.04743319749832153
I0205 04:15:13.217515 139806554191616 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.09018253535032272, loss=0.04331975057721138
I0205 04:15:45.043577 139806646249216 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.07331360876560211, loss=0.04538874700665474
I0205 04:16:16.714419 139806554191616 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.10026911646127701, loss=0.04630818963050842
I0205 04:16:23.922999 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:18:22.270120 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:18:25.273914 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:18:28.200427 139978932307776 submission_runner.py:408] Time since start: 1768.72s, 	Step: 3024, 	{'train/accuracy': 0.9880422353744507, 'train/loss': 0.04227624461054802, 'train/mean_average_precision': 0.1592302566590645, 'validation/accuracy': 0.9852914810180664, 'validation/loss': 0.05124949291348457, 'validation/mean_average_precision': 0.15319013957304484, 'validation/num_examples': 43793, 'test/accuracy': 0.9843626618385315, 'test/loss': 0.0538056381046772, 'test/mean_average_precision': 0.1520338482806402, 'test/num_examples': 43793, 'score': 984.7043483257294, 'total_duration': 1768.7225918769836, 'accumulated_submission_time': 984.7043483257294, 'accumulated_eval_time': 783.8252048492432, 'accumulated_logging_time': 0.11647272109985352}
I0205 04:18:28.215787 139806537406208 logging_writer.py:48] [3024] accumulated_eval_time=783.825205, accumulated_logging_time=0.116473, accumulated_submission_time=984.704348, global_step=3024, preemption_count=0, score=984.704348, test/accuracy=0.984363, test/loss=0.053806, test/mean_average_precision=0.152034, test/num_examples=43793, total_duration=1768.722592, train/accuracy=0.988042, train/loss=0.042276, train/mean_average_precision=0.159230, validation/accuracy=0.985291, validation/loss=0.051249, validation/mean_average_precision=0.153190, validation/num_examples=43793
I0205 04:18:52.695438 139806637856512 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.06467708945274353, loss=0.041971366852521896
I0205 04:19:24.627590 139806537406208 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05693326145410538, loss=0.04366602376103401
I0205 04:19:56.643265 139806637856512 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.12190674990415573, loss=0.0395406074821949
I0205 04:20:28.635348 139806537406208 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.08756083250045776, loss=0.04175049066543579
I0205 04:21:00.488414 139806637856512 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05006648600101471, loss=0.04173208773136139
I0205 04:21:32.319067 139806537406208 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.06480187177658081, loss=0.04260404407978058
I0205 04:22:03.736350 139806637856512 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.08216144144535065, loss=0.04077441245317459
I0205 04:22:28.368095 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:24:24.431676 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:24:27.397204 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:24:30.347663 139978932307776 submission_runner.py:408] Time since start: 2130.87s, 	Step: 3778, 	{'train/accuracy': 0.9882649779319763, 'train/loss': 0.04051998257637024, 'train/mean_average_precision': 0.19136164647091158, 'validation/accuracy': 0.9853755235671997, 'validation/loss': 0.05026780813932419, 'validation/mean_average_precision': 0.1748420624477223, 'validation/num_examples': 43793, 'test/accuracy': 0.9844376444816589, 'test/loss': 0.05316726863384247, 'test/mean_average_precision': 0.17318849199594025, 'test/num_examples': 43793, 'score': 1224.8253166675568, 'total_duration': 2130.8698279857635, 'accumulated_submission_time': 1224.8253166675568, 'accumulated_eval_time': 905.8047299385071, 'accumulated_logging_time': 0.14408493041992188}
I0205 04:24:30.363248 139806554191616 logging_writer.py:48] [3778] accumulated_eval_time=905.804730, accumulated_logging_time=0.144085, accumulated_submission_time=1224.825317, global_step=3778, preemption_count=0, score=1224.825317, test/accuracy=0.984438, test/loss=0.053167, test/mean_average_precision=0.173188, test/num_examples=43793, total_duration=2130.869828, train/accuracy=0.988265, train/loss=0.040520, train/mean_average_precision=0.191362, validation/accuracy=0.985376, validation/loss=0.050268, validation/mean_average_precision=0.174842, validation/num_examples=43793
I0205 04:24:37.591816 139806646249216 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.10389339178800583, loss=0.04528000205755234
I0205 04:25:09.235952 139806554191616 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.047800660133361816, loss=0.040881529450416565
I0205 04:25:40.827332 139806646249216 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.06141716614365578, loss=0.04051753133535385
I0205 04:26:12.544126 139806554191616 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.09026829898357391, loss=0.04634418338537216
I0205 04:26:44.045307 139806646249216 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0784778818488121, loss=0.04531592130661011
I0205 04:27:15.428328 139806554191616 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.054945122450590134, loss=0.03881543129682541
I0205 04:27:47.039216 139806646249216 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.07798868417739868, loss=0.036641258746385574
I0205 04:28:18.858093 139806554191616 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03463214635848999, loss=0.041486214846372604
I0205 04:28:30.486781 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:30:27.625220 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:30:30.631288 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:30:33.517210 139978932307776 submission_runner.py:408] Time since start: 2494.04s, 	Step: 4538, 	{'train/accuracy': 0.9881364107131958, 'train/loss': 0.04055248573422432, 'train/mean_average_precision': 0.1995915133924315, 'validation/accuracy': 0.9855062365531921, 'validation/loss': 0.049841176718473434, 'validation/mean_average_precision': 0.18333610998019786, 'validation/num_examples': 43793, 'test/accuracy': 0.984628438949585, 'test/loss': 0.052729532122612, 'test/mean_average_precision': 0.17959885213536883, 'test/num_examples': 43793, 'score': 1464.9182755947113, 'total_duration': 2494.039370775223, 'accumulated_submission_time': 1464.9182755947113, 'accumulated_eval_time': 1028.835110425949, 'accumulated_logging_time': 0.17086505889892578}
I0205 04:30:33.535494 139806654641920 logging_writer.py:48] [4538] accumulated_eval_time=1028.835110, accumulated_logging_time=0.170865, accumulated_submission_time=1464.918276, global_step=4538, preemption_count=0, score=1464.918276, test/accuracy=0.984628, test/loss=0.052730, test/mean_average_precision=0.179599, test/num_examples=43793, total_duration=2494.039371, train/accuracy=0.988136, train/loss=0.040552, train/mean_average_precision=0.199592, validation/accuracy=0.985506, validation/loss=0.049841, validation/mean_average_precision=0.183336, validation/num_examples=43793
I0205 04:30:53.593070 139916436547328 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.10740913450717926, loss=0.042890049517154694
I0205 04:31:25.614426 139806654641920 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.07356064766645432, loss=0.04207149147987366
I0205 04:31:57.505997 139916436547328 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.03563027083873749, loss=0.04026912897825241
I0205 04:32:29.584127 139806654641920 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.03404191881418228, loss=0.044142112135887146
I0205 04:33:01.175411 139916436547328 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06880524009466171, loss=0.04425223544239998
I0205 04:33:33.143958 139806654641920 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.03507600724697113, loss=0.03885127604007721
I0205 04:34:05.077838 139916436547328 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0418720580637455, loss=0.040036238729953766
I0205 04:34:33.571460 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:36:33.645527 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:36:36.608405 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:36:39.511447 139978932307776 submission_runner.py:408] Time since start: 2860.03s, 	Step: 5291, 	{'train/accuracy': 0.9885979890823364, 'train/loss': 0.0390627458691597, 'train/mean_average_precision': 0.235640214546389, 'validation/accuracy': 0.9856986403465271, 'validation/loss': 0.04872404411435127, 'validation/mean_average_precision': 0.1980871299196871, 'validation/num_examples': 43793, 'test/accuracy': 0.9847902059555054, 'test/loss': 0.051404111087322235, 'test/mean_average_precision': 0.19305945672324892, 'test/num_examples': 43793, 'score': 1704.9223086833954, 'total_duration': 2860.033614873886, 'accumulated_submission_time': 1704.9223086833954, 'accumulated_eval_time': 1154.7750566005707, 'accumulated_logging_time': 0.2018733024597168}
I0205 04:36:39.528331 139817582135040 logging_writer.py:48] [5291] accumulated_eval_time=1154.775057, accumulated_logging_time=0.201873, accumulated_submission_time=1704.922309, global_step=5291, preemption_count=0, score=1704.922309, test/accuracy=0.984790, test/loss=0.051404, test/mean_average_precision=0.193059, test/num_examples=43793, total_duration=2860.033615, train/accuracy=0.988598, train/loss=0.039063, train/mean_average_precision=0.235640, validation/accuracy=0.985699, validation/loss=0.048724, validation/mean_average_precision=0.198087, validation/num_examples=43793
I0205 04:36:42.698506 139916444940032 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05566812679171562, loss=0.04330724477767944
I0205 04:37:14.620326 139817582135040 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05044330656528473, loss=0.037229880690574646
I0205 04:37:46.148266 139916444940032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03696670010685921, loss=0.04522302746772766
I0205 04:38:17.607362 139817582135040 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.04792652651667595, loss=0.03940282016992569
I0205 04:38:49.392339 139916444940032 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.042819101363420486, loss=0.04060159996151924
I0205 04:39:21.091129 139817582135040 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04099220037460327, loss=0.03908521682024002
I0205 04:39:53.141494 139916444940032 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.11071988195180893, loss=0.03893373906612396
I0205 04:40:25.112984 139817582135040 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.03548697754740715, loss=0.04175998643040657
I0205 04:40:39.796252 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:42:39.648333 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:42:43.141680 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:42:45.983533 139978932307776 submission_runner.py:408] Time since start: 3226.51s, 	Step: 6048, 	{'train/accuracy': 0.9889920353889465, 'train/loss': 0.03758462890982628, 'train/mean_average_precision': 0.24712975262076814, 'validation/accuracy': 0.9858882427215576, 'validation/loss': 0.0478384830057621, 'validation/mean_average_precision': 0.20302851230948468, 'validation/num_examples': 43793, 'test/accuracy': 0.9850159883499146, 'test/loss': 0.050465963780879974, 'test/mean_average_precision': 0.19885518823917242, 'test/num_examples': 43793, 'score': 1945.1591565608978, 'total_duration': 3226.5057003498077, 'accumulated_submission_time': 1945.1591565608978, 'accumulated_eval_time': 1280.962295293808, 'accumulated_logging_time': 0.2301163673400879}
I0205 04:42:45.999255 139806663034624 logging_writer.py:48] [6048] accumulated_eval_time=1280.962295, accumulated_logging_time=0.230116, accumulated_submission_time=1945.159157, global_step=6048, preemption_count=0, score=1945.159157, test/accuracy=0.985016, test/loss=0.050466, test/mean_average_precision=0.198855, test/num_examples=43793, total_duration=3226.505700, train/accuracy=0.988992, train/loss=0.037585, train/mean_average_precision=0.247130, validation/accuracy=0.985888, validation/loss=0.047838, validation/mean_average_precision=0.203029, validation/num_examples=43793
I0205 04:43:02.704306 139916436547328 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.037913598120212555, loss=0.04160701856017113
I0205 04:43:34.441522 139806663034624 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04132923111319542, loss=0.038250867277383804
I0205 04:44:07.541441 139916436547328 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03599930554628372, loss=0.039149392396211624
I0205 04:44:39.601627 139806663034624 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.03130856528878212, loss=0.045354440808296204
I0205 04:45:11.442608 139916436547328 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.032439734786748886, loss=0.04157731309533119
I0205 04:45:43.138275 139806663034624 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.0360243059694767, loss=0.038859885185956955
I0205 04:46:15.127469 139916436547328 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.02331911027431488, loss=0.03967245668172836
I0205 04:46:46.223492 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:48:44.550796 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:48:47.597295 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:48:50.476831 139978932307776 submission_runner.py:408] Time since start: 3591.00s, 	Step: 6799, 	{'train/accuracy': 0.9890881180763245, 'train/loss': 0.037440940737724304, 'train/mean_average_precision': 0.25415324042609444, 'validation/accuracy': 0.9859243631362915, 'validation/loss': 0.04757380113005638, 'validation/mean_average_precision': 0.20746549366052722, 'validation/num_examples': 43793, 'test/accuracy': 0.9850117564201355, 'test/loss': 0.05010205879807472, 'test/mean_average_precision': 0.2055948206630922, 'test/num_examples': 43793, 'score': 2185.3532433509827, 'total_duration': 3590.998979330063, 'accumulated_submission_time': 2185.3532433509827, 'accumulated_eval_time': 1405.2155866622925, 'accumulated_logging_time': 0.2566075325012207}
I0205 04:48:50.493191 139817573742336 logging_writer.py:48] [6799] accumulated_eval_time=1405.215587, accumulated_logging_time=0.256608, accumulated_submission_time=2185.353243, global_step=6799, preemption_count=0, score=2185.353243, test/accuracy=0.985012, test/loss=0.050102, test/mean_average_precision=0.205595, test/num_examples=43793, total_duration=3590.998979, train/accuracy=0.989088, train/loss=0.037441, train/mean_average_precision=0.254153, validation/accuracy=0.985924, validation/loss=0.047574, validation/mean_average_precision=0.207465, validation/num_examples=43793
I0205 04:48:51.143250 139916444940032 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.03033977933228016, loss=0.03820228576660156
I0205 04:49:23.187186 139817573742336 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.04384637251496315, loss=0.04187813773751259
I0205 04:49:55.142073 139916444940032 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.03363674879074097, loss=0.04154147207736969
I0205 04:50:26.869863 139817573742336 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.029737139120697975, loss=0.03794160112738609
I0205 04:50:58.317633 139916444940032 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.03942510485649109, loss=0.03802109882235527
I0205 04:51:29.929447 139817573742336 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.04574589803814888, loss=0.042527999728918076
I0205 04:52:01.335545 139916444940032 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.03090660646557808, loss=0.040992338210344315
I0205 04:52:33.608527 139817573742336 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.029143139719963074, loss=0.040420811623334885
I0205 04:52:50.776158 139978932307776 spec.py:321] Evaluating on the training split.
I0205 04:54:50.522501 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 04:54:53.546523 139978932307776 spec.py:349] Evaluating on the test split.
I0205 04:54:56.470656 139978932307776 submission_runner.py:408] Time since start: 3956.99s, 	Step: 7555, 	{'train/accuracy': 0.9891881942749023, 'train/loss': 0.03661906719207764, 'train/mean_average_precision': 0.2631767259260585, 'validation/accuracy': 0.9861549735069275, 'validation/loss': 0.04668034613132477, 'validation/mean_average_precision': 0.2189902151347329, 'validation/num_examples': 43793, 'test/accuracy': 0.9853213429450989, 'test/loss': 0.04916556179523468, 'test/mean_average_precision': 0.2203662111267828, 'test/num_examples': 43793, 'score': 2425.605710029602, 'total_duration': 3956.9928188323975, 'accumulated_submission_time': 2425.605710029602, 'accumulated_eval_time': 1530.9100363254547, 'accumulated_logging_time': 0.28405094146728516}
I0205 04:54:56.487433 139817582135040 logging_writer.py:48] [7555] accumulated_eval_time=1530.910036, accumulated_logging_time=0.284051, accumulated_submission_time=2425.605710, global_step=7555, preemption_count=0, score=2425.605710, test/accuracy=0.985321, test/loss=0.049166, test/mean_average_precision=0.220366, test/num_examples=43793, total_duration=3956.992819, train/accuracy=0.989188, train/loss=0.036619, train/mean_average_precision=0.263177, validation/accuracy=0.986155, validation/loss=0.046680, validation/mean_average_precision=0.218990, validation/num_examples=43793
I0205 04:55:11.220456 139916436547328 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03955572471022606, loss=0.04028752073645592
I0205 04:55:42.999110 139817582135040 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.025106800720095634, loss=0.039336804300546646
I0205 04:56:14.636111 139916436547328 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.027061600238084793, loss=0.03712251037359238
I0205 04:56:46.320707 139817582135040 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.029898839071393013, loss=0.03954968601465225
I0205 04:57:18.107510 139916436547328 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.031665876507759094, loss=0.040527764707803726
I0205 04:57:49.786813 139817582135040 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03426177427172661, loss=0.04430368170142174
I0205 04:58:21.108449 139916436547328 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03640737757086754, loss=0.03702419251203537
I0205 04:58:52.786918 139817582135040 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.021569734439253807, loss=0.03720404580235481
I0205 04:58:56.532771 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:00:56.525931 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:00:59.498323 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:01:02.442742 139978932307776 submission_runner.py:408] Time since start: 4322.96s, 	Step: 8313, 	{'train/accuracy': 0.989005982875824, 'train/loss': 0.03703552111983299, 'train/mean_average_precision': 0.25546423593524753, 'validation/accuracy': 0.9861886501312256, 'validation/loss': 0.046532753854990005, 'validation/mean_average_precision': 0.22607550617359756, 'validation/num_examples': 43793, 'test/accuracy': 0.9852766990661621, 'test/loss': 0.04921731725335121, 'test/mean_average_precision': 0.21973123956483565, 'test/num_examples': 43793, 'score': 2665.6187121868134, 'total_duration': 4322.964905500412, 'accumulated_submission_time': 2665.6187121868134, 'accumulated_eval_time': 1656.8199598789215, 'accumulated_logging_time': 0.31336045265197754}
I0205 05:01:02.458951 139806663034624 logging_writer.py:48] [8313] accumulated_eval_time=1656.819960, accumulated_logging_time=0.313360, accumulated_submission_time=2665.618712, global_step=8313, preemption_count=0, score=2665.618712, test/accuracy=0.985277, test/loss=0.049217, test/mean_average_precision=0.219731, test/num_examples=43793, total_duration=4322.964906, train/accuracy=0.989006, train/loss=0.037036, train/mean_average_precision=0.255464, validation/accuracy=0.986189, validation/loss=0.046533, validation/mean_average_precision=0.226076, validation/num_examples=43793
I0205 05:01:30.082433 139916444940032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.03398098796606064, loss=0.040022291243076324
I0205 05:02:01.754652 139806663034624 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03556650131940842, loss=0.03955501317977905
I0205 05:02:33.751570 139916444940032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.035133250057697296, loss=0.03950278460979462
I0205 05:03:05.559966 139806663034624 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.028790965676307678, loss=0.04068640619516373
I0205 05:03:37.111779 139916444940032 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.027099449187517166, loss=0.03975329175591469
I0205 05:04:08.811377 139806663034624 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.030476778745651245, loss=0.04000039026141167
I0205 05:04:40.725166 139916444940032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.028522441163659096, loss=0.043404143303632736
I0205 05:05:02.583786 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:07:04.298806 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:07:07.351336 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:07:10.267978 139978932307776 submission_runner.py:408] Time since start: 4690.79s, 	Step: 9070, 	{'train/accuracy': 0.9891762137413025, 'train/loss': 0.036489855498075485, 'train/mean_average_precision': 0.28288348213249725, 'validation/accuracy': 0.9861512780189514, 'validation/loss': 0.04660046100616455, 'validation/mean_average_precision': 0.22502004687923619, 'validation/num_examples': 43793, 'test/accuracy': 0.9853150248527527, 'test/loss': 0.049311891198158264, 'test/mean_average_precision': 0.2232171338319447, 'test/num_examples': 43793, 'score': 2905.712122440338, 'total_duration': 4690.790143251419, 'accumulated_submission_time': 2905.712122440338, 'accumulated_eval_time': 1784.504117488861, 'accumulated_logging_time': 0.34171271324157715}
I0205 05:07:10.284938 139805680953088 logging_writer.py:48] [9070] accumulated_eval_time=1784.504117, accumulated_logging_time=0.341713, accumulated_submission_time=2905.712122, global_step=9070, preemption_count=0, score=2905.712122, test/accuracy=0.985315, test/loss=0.049312, test/mean_average_precision=0.223217, test/num_examples=43793, total_duration=4690.790143, train/accuracy=0.989176, train/loss=0.036490, train/mean_average_precision=0.282883, validation/accuracy=0.986151, validation/loss=0.046600, validation/mean_average_precision=0.225020, validation/num_examples=43793
I0205 05:07:20.183058 139817573742336 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.026218773797154427, loss=0.0409872904419899
I0205 05:07:52.240593 139805680953088 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.02664390206336975, loss=0.03761563077569008
I0205 05:08:24.251451 139817573742336 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.030075697228312492, loss=0.042415644973516464
I0205 05:08:56.186099 139805680953088 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03783443570137024, loss=0.04181868955492973
I0205 05:09:28.270288 139817573742336 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.021896688267588615, loss=0.03608355671167374
I0205 05:10:00.185762 139805680953088 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.028602367267012596, loss=0.040191650390625
I0205 05:10:32.058444 139817573742336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.028998201712965965, loss=0.04178296774625778
I0205 05:11:04.377661 139805680953088 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.027667557820677757, loss=0.03614204376935959
I0205 05:11:10.325797 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:13:10.745710 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:13:13.781398 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:13:16.799395 139978932307776 submission_runner.py:408] Time since start: 5057.32s, 	Step: 9820, 	{'train/accuracy': 0.9896302819252014, 'train/loss': 0.03538098186254501, 'train/mean_average_precision': 0.2862586124008465, 'validation/accuracy': 0.9862738847732544, 'validation/loss': 0.04619408771395683, 'validation/mean_average_precision': 0.22457472816507262, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.04879970848560333, 'test/mean_average_precision': 0.22735471987155884, 'test/num_examples': 43793, 'score': 3145.721575975418, 'total_duration': 5057.321557760239, 'accumulated_submission_time': 3145.721575975418, 'accumulated_eval_time': 1910.9776666164398, 'accumulated_logging_time': 0.37102818489074707}
I0205 05:13:16.816433 139804984182528 logging_writer.py:48] [9820] accumulated_eval_time=1910.977667, accumulated_logging_time=0.371028, accumulated_submission_time=3145.721576, global_step=9820, preemption_count=0, score=3145.721576, test/accuracy=0.985397, test/loss=0.048800, test/mean_average_precision=0.227355, test/num_examples=43793, total_duration=5057.321558, train/accuracy=0.989630, train/loss=0.035381, train/mean_average_precision=0.286259, validation/accuracy=0.986274, validation/loss=0.046194, validation/mean_average_precision=0.224575, validation/num_examples=43793
I0205 05:13:43.042801 139806663034624 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.025644585490226746, loss=0.03861752152442932
I0205 05:14:15.323486 139804984182528 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02094656229019165, loss=0.037726640701293945
I0205 05:14:47.190908 139806663034624 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02045816369354725, loss=0.036501020193099976
I0205 05:15:19.398471 139804984182528 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02599380351603031, loss=0.03958755359053612
I0205 05:15:51.313490 139806663034624 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.022110577672719955, loss=0.035853222012519836
I0205 05:16:23.374699 139804984182528 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.022022530436515808, loss=0.03926960006356239
I0205 05:16:55.163590 139806663034624 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.024250205606222153, loss=0.03666194528341293
I0205 05:17:16.927972 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:19:16.635944 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:19:19.600040 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:19:22.522184 139978932307776 submission_runner.py:408] Time since start: 5423.04s, 	Step: 10569, 	{'train/accuracy': 0.9896215200424194, 'train/loss': 0.03488897532224655, 'train/mean_average_precision': 0.3107923658999827, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.045642539858818054, 'validation/mean_average_precision': 0.2417199449464199, 'validation/num_examples': 43793, 'test/accuracy': 0.9855096340179443, 'test/loss': 0.04835975170135498, 'test/mean_average_precision': 0.23351386698909332, 'test/num_examples': 43793, 'score': 3385.802992105484, 'total_duration': 5423.044348478317, 'accumulated_submission_time': 3385.802992105484, 'accumulated_eval_time': 2036.5718340873718, 'accumulated_logging_time': 0.3991975784301758}
I0205 05:19:22.538899 139804644198144 logging_writer.py:48] [10569] accumulated_eval_time=2036.571834, accumulated_logging_time=0.399198, accumulated_submission_time=3385.802992, global_step=10569, preemption_count=0, score=3385.802992, test/accuracy=0.985510, test/loss=0.048360, test/mean_average_precision=0.233514, test/num_examples=43793, total_duration=5423.044348, train/accuracy=0.989622, train/loss=0.034889, train/mean_average_precision=0.310792, validation/accuracy=0.986360, validation/loss=0.045643, validation/mean_average_precision=0.241720, validation/num_examples=43793
I0205 05:19:32.885658 139805009491712 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.025585608556866646, loss=0.036317065358161926
I0205 05:20:04.915338 139804644198144 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04564312472939491, loss=0.03579653427004814
I0205 05:20:36.788746 139805009491712 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.02818305417895317, loss=0.03828805312514305
I0205 05:21:08.625294 139804644198144 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0240308977663517, loss=0.03820737823843956
I0205 05:21:40.363261 139805009491712 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03946460410952568, loss=0.03883812204003334
I0205 05:22:11.990522 139804644198144 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03528491035103798, loss=0.037656303495168686
I0205 05:22:43.820225 139805009491712 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.032598987221717834, loss=0.03480030596256256
I0205 05:23:15.748624 139804644198144 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.027023430913686752, loss=0.03844989091157913
I0205 05:23:22.733406 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:25:26.139375 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:25:29.132288 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:25:32.162414 139978932307776 submission_runner.py:408] Time since start: 5792.68s, 	Step: 11323, 	{'train/accuracy': 0.989838182926178, 'train/loss': 0.03400369733572006, 'train/mean_average_precision': 0.32667529756985764, 'validation/accuracy': 0.9865649342536926, 'validation/loss': 0.045087773352861404, 'validation/mean_average_precision': 0.2468068704228591, 'validation/num_examples': 43793, 'test/accuracy': 0.9857492446899414, 'test/loss': 0.04749658703804016, 'test/mean_average_precision': 0.2476235995592323, 'test/num_examples': 43793, 'score': 3625.9665355682373, 'total_duration': 5792.6845734119415, 'accumulated_submission_time': 3625.9665355682373, 'accumulated_eval_time': 2166.000794649124, 'accumulated_logging_time': 0.427293062210083}
I0205 05:25:32.179969 139804984182528 logging_writer.py:48] [11323] accumulated_eval_time=2166.000795, accumulated_logging_time=0.427293, accumulated_submission_time=3625.966536, global_step=11323, preemption_count=0, score=3625.966536, test/accuracy=0.985749, test/loss=0.047497, test/mean_average_precision=0.247624, test/num_examples=43793, total_duration=5792.684573, train/accuracy=0.989838, train/loss=0.034004, train/mean_average_precision=0.326675, validation/accuracy=0.986565, validation/loss=0.045088, validation/mean_average_precision=0.246807, validation/num_examples=43793
I0205 05:25:56.933961 139806663034624 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.024788282811641693, loss=0.037898074835538864
I0205 05:26:28.841779 139804984182528 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.040271904319524765, loss=0.038879506289958954
I0205 05:27:00.351343 139806663034624 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03177446126937866, loss=0.03794127330183983
I0205 05:27:31.779083 139804984182528 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03746753931045532, loss=0.03864087536931038
I0205 05:28:03.669819 139806663034624 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.030740462243556976, loss=0.03448374941945076
I0205 05:28:35.418376 139804984182528 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0318479984998703, loss=0.03876219689846039
I0205 05:29:06.984201 139806663034624 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0337379164993763, loss=0.03730779513716698
I0205 05:29:32.198256 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:31:32.599627 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:31:35.579425 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:31:38.498490 139978932307776 submission_runner.py:408] Time since start: 6159.02s, 	Step: 12080, 	{'train/accuracy': 0.9900984764099121, 'train/loss': 0.03318807855248451, 'train/mean_average_precision': 0.34687909561369384, 'validation/accuracy': 0.9866153001785278, 'validation/loss': 0.04485723003745079, 'validation/mean_average_precision': 0.24687213253247628, 'validation/num_examples': 43793, 'test/accuracy': 0.9857353568077087, 'test/loss': 0.04748206213116646, 'test/mean_average_precision': 0.23909211752357484, 'test/num_examples': 43793, 'score': 3865.95241522789, 'total_duration': 6159.020653247833, 'accumulated_submission_time': 3865.95241522789, 'accumulated_eval_time': 2292.3009836673737, 'accumulated_logging_time': 0.4576456546783447}
I0205 05:31:38.515771 139805009491712 logging_writer.py:48] [12080] accumulated_eval_time=2292.300984, accumulated_logging_time=0.457646, accumulated_submission_time=3865.952415, global_step=12080, preemption_count=0, score=3865.952415, test/accuracy=0.985735, test/loss=0.047482, test/mean_average_precision=0.239092, test/num_examples=43793, total_duration=6159.020653, train/accuracy=0.990098, train/loss=0.033188, train/mean_average_precision=0.346879, validation/accuracy=0.986615, validation/loss=0.044857, validation/mean_average_precision=0.246872, validation/num_examples=43793
I0205 05:31:45.326173 139805680953088 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.04761622101068497, loss=0.036460138857364655
I0205 05:32:17.221704 139805009491712 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03255898132920265, loss=0.0369325689971447
I0205 05:32:49.185055 139805680953088 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.053045548498630524, loss=0.03783487156033516
I0205 05:33:20.943778 139805009491712 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.027934802696108818, loss=0.03572365641593933
I0205 05:33:53.174637 139805680953088 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.04530883580446243, loss=0.03733447194099426
I0205 05:34:25.081532 139805009491712 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.041830070316791534, loss=0.03524842485785484
I0205 05:34:56.873552 139805680953088 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03995151072740555, loss=0.03992443531751633
I0205 05:35:28.813317 139805009491712 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03189306706190109, loss=0.03616774082183838
I0205 05:35:38.607338 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:37:39.519548 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:37:42.546060 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:37:45.616330 139978932307776 submission_runner.py:408] Time since start: 6526.14s, 	Step: 12832, 	{'train/accuracy': 0.9901254177093506, 'train/loss': 0.032805345952510834, 'train/mean_average_precision': 0.355174481887847, 'validation/accuracy': 0.9865312576293945, 'validation/loss': 0.044936634600162506, 'validation/mean_average_precision': 0.24883223156314221, 'validation/num_examples': 43793, 'test/accuracy': 0.9856481552124023, 'test/loss': 0.04745563492178917, 'test/mean_average_precision': 0.24171528826891692, 'test/num_examples': 43793, 'score': 4106.013333797455, 'total_duration': 6526.13848400116, 'accumulated_submission_time': 4106.013333797455, 'accumulated_eval_time': 2419.3099246025085, 'accumulated_logging_time': 0.4859592914581299}
I0205 05:37:45.633781 139804644198144 logging_writer.py:48] [12832] accumulated_eval_time=2419.309925, accumulated_logging_time=0.485959, accumulated_submission_time=4106.013334, global_step=12832, preemption_count=0, score=4106.013334, test/accuracy=0.985648, test/loss=0.047456, test/mean_average_precision=0.241715, test/num_examples=43793, total_duration=6526.138484, train/accuracy=0.990125, train/loss=0.032805, train/mean_average_precision=0.355174, validation/accuracy=0.986531, validation/loss=0.044937, validation/mean_average_precision=0.248832, validation/num_examples=43793
I0205 05:38:07.570989 139806663034624 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03471885249018669, loss=0.03522477298974991
I0205 05:38:39.665119 139804644198144 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.03136087954044342, loss=0.03644007071852684
I0205 05:39:11.885007 139806663034624 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04681655764579773, loss=0.03329513594508171
I0205 05:39:43.734830 139804644198144 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.05124558508396149, loss=0.03473205864429474
I0205 05:40:15.653451 139806663034624 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.04226622357964516, loss=0.038246363401412964
I0205 05:40:48.037065 139804644198144 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02919406071305275, loss=0.035934336483478546
I0205 05:41:20.264154 139806663034624 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.04772645980119705, loss=0.038125667721033096
I0205 05:41:45.749290 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:43:46.624355 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:43:49.571038 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:43:52.493896 139978932307776 submission_runner.py:408] Time since start: 6893.02s, 	Step: 13581, 	{'train/accuracy': 0.9902234077453613, 'train/loss': 0.03212849050760269, 'train/mean_average_precision': 0.37092765121408844, 'validation/accuracy': 0.9865828156471252, 'validation/loss': 0.0451812818646431, 'validation/mean_average_precision': 0.24976972107872278, 'validation/num_examples': 43793, 'test/accuracy': 0.9857589602470398, 'test/loss': 0.04780641570687294, 'test/mean_average_precision': 0.24283336460482102, 'test/num_examples': 43793, 'score': 4346.096912384033, 'total_duration': 6893.016060590744, 'accumulated_submission_time': 4346.096912384033, 'accumulated_eval_time': 2546.0544838905334, 'accumulated_logging_time': 0.5157315731048584}
I0205 05:43:52.511795 139804984182528 logging_writer.py:48] [13581] accumulated_eval_time=2546.054484, accumulated_logging_time=0.515732, accumulated_submission_time=4346.096912, global_step=13581, preemption_count=0, score=4346.096912, test/accuracy=0.985759, test/loss=0.047806, test/mean_average_precision=0.242833, test/num_examples=43793, total_duration=6893.016061, train/accuracy=0.990223, train/loss=0.032128, train/mean_average_precision=0.370928, validation/accuracy=0.986583, validation/loss=0.045181, validation/mean_average_precision=0.249770, validation/num_examples=43793
I0205 05:43:59.512106 139805680953088 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.043175119906663895, loss=0.03937399759888649
I0205 05:44:31.589351 139804984182528 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.06622328609228134, loss=0.033300939947366714
I0205 05:45:03.349365 139805680953088 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03642842546105385, loss=0.03437741473317146
I0205 05:45:35.197929 139804984182528 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.0542970634996891, loss=0.04077133536338806
I0205 05:46:07.306856 139805680953088 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04925575107336044, loss=0.03527333214879036
I0205 05:46:39.072906 139804984182528 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04019865021109581, loss=0.03624642640352249
I0205 05:47:11.079155 139805680953088 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.039034269750118256, loss=0.04164908453822136
I0205 05:47:43.022454 139804984182528 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.06797067075967789, loss=0.036065105348825455
I0205 05:47:52.650234 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:49:58.511124 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:50:01.520383 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:50:04.489022 139978932307776 submission_runner.py:408] Time since start: 7265.01s, 	Step: 14331, 	{'train/accuracy': 0.9904870986938477, 'train/loss': 0.03172406554222107, 'train/mean_average_precision': 0.3738916030379458, 'validation/accuracy': 0.9866850972175598, 'validation/loss': 0.04454706236720085, 'validation/mean_average_precision': 0.2559132804697909, 'validation/num_examples': 43793, 'test/accuracy': 0.9858056902885437, 'test/loss': 0.04711402952671051, 'test/mean_average_precision': 0.24484617934449046, 'test/num_examples': 43793, 'score': 4586.202342510223, 'total_duration': 7265.011176109314, 'accumulated_submission_time': 4586.202342510223, 'accumulated_eval_time': 2677.8932163715363, 'accumulated_logging_time': 0.547095775604248}
I0205 05:50:04.511367 139804644198144 logging_writer.py:48] [14331] accumulated_eval_time=2677.893216, accumulated_logging_time=0.547096, accumulated_submission_time=4586.202343, global_step=14331, preemption_count=0, score=4586.202343, test/accuracy=0.985806, test/loss=0.047114, test/mean_average_precision=0.244846, test/num_examples=43793, total_duration=7265.011176, train/accuracy=0.990487, train/loss=0.031724, train/mean_average_precision=0.373892, validation/accuracy=0.986685, validation/loss=0.044547, validation/mean_average_precision=0.255913, validation/num_examples=43793
I0205 05:50:26.981606 139805009491712 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.045433830469846725, loss=0.04035123437643051
I0205 05:50:59.104950 139804644198144 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.045809462666511536, loss=0.035147588700056076
I0205 05:51:31.338471 139805009491712 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.08197629451751709, loss=0.03684784099459648
I0205 05:52:03.405766 139804644198144 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03898269310593605, loss=0.036831341683864594
I0205 05:52:35.797433 139805009491712 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04139057919383049, loss=0.0319414883852005
I0205 05:53:07.574992 139804644198144 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.07284030318260193, loss=0.038174066692590714
I0205 05:53:39.799923 139805009491712 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.064980648458004, loss=0.037767983973026276
I0205 05:54:04.515669 139978932307776 spec.py:321] Evaluating on the training split.
I0205 05:56:08.502779 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 05:56:11.593405 139978932307776 spec.py:349] Evaluating on the test split.
I0205 05:56:14.582801 139978932307776 submission_runner.py:408] Time since start: 7635.10s, 	Step: 15078, 	{'train/accuracy': 0.9903684258460999, 'train/loss': 0.031961578875780106, 'train/mean_average_precision': 0.3792209517081584, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.044694095849990845, 'validation/mean_average_precision': 0.25235253559489096, 'validation/num_examples': 43793, 'test/accuracy': 0.9858945608139038, 'test/loss': 0.04718709737062454, 'test/mean_average_precision': 0.24945004130953052, 'test/num_examples': 43793, 'score': 4826.174679040909, 'total_duration': 7635.104958295822, 'accumulated_submission_time': 4826.174679040909, 'accumulated_eval_time': 2807.9602975845337, 'accumulated_logging_time': 0.5814604759216309}
I0205 05:56:14.601388 139804984182528 logging_writer.py:48] [15078] accumulated_eval_time=2807.960298, accumulated_logging_time=0.581460, accumulated_submission_time=4826.174679, global_step=15078, preemption_count=0, score=4826.174679, test/accuracy=0.985895, test/loss=0.047187, test/mean_average_precision=0.249450, test/num_examples=43793, total_duration=7635.104958, train/accuracy=0.990368, train/loss=0.031962, train/mean_average_precision=0.379221, validation/accuracy=0.986720, validation/loss=0.044694, validation/mean_average_precision=0.252353, validation/num_examples=43793
I0205 05:56:22.176613 139806663034624 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.05279882997274399, loss=0.03333654627203941
I0205 05:56:54.665513 139804984182528 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0737384781241417, loss=0.03729742020368576
I0205 05:57:26.616913 139806663034624 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04853734001517296, loss=0.039935290813446045
I0205 05:57:58.744988 139804984182528 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.05562850832939148, loss=0.03470137342810631
I0205 05:58:30.869762 139806663034624 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.061104267835617065, loss=0.03673313185572624
I0205 05:59:02.938313 139804984182528 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.039010535925626755, loss=0.037181396037340164
I0205 05:59:35.448453 139806663034624 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.06308283656835556, loss=0.03845905140042305
I0205 06:00:07.710442 139804984182528 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04705532640218735, loss=0.03557989373803139
I0205 06:00:14.720932 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:02:19.136190 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:02:22.100468 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:02:25.266563 139978932307776 submission_runner.py:408] Time since start: 8005.79s, 	Step: 15823, 	{'train/accuracy': 0.9903414845466614, 'train/loss': 0.03201202303171158, 'train/mean_average_precision': 0.38660113116527683, 'validation/accuracy': 0.9866615533828735, 'validation/loss': 0.045002248138189316, 'validation/mean_average_precision': 0.25811686529120725, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.04766395315527916, 'test/mean_average_precision': 0.24804640159581487, 'test/num_examples': 43793, 'score': 5066.263954401016, 'total_duration': 8005.788725376129, 'accumulated_submission_time': 5066.263954401016, 'accumulated_eval_time': 2938.505881547928, 'accumulated_logging_time': 0.6110355854034424}
I0205 06:02:25.285778 139804644198144 logging_writer.py:48] [15823] accumulated_eval_time=2938.505882, accumulated_logging_time=0.611036, accumulated_submission_time=5066.263954, global_step=15823, preemption_count=0, score=5066.263954, test/accuracy=0.985842, test/loss=0.047664, test/mean_average_precision=0.248046, test/num_examples=43793, total_duration=8005.788725, train/accuracy=0.990341, train/loss=0.032012, train/mean_average_precision=0.386601, validation/accuracy=0.986662, validation/loss=0.045002, validation/mean_average_precision=0.258117, validation/num_examples=43793
I0205 06:02:50.478699 139805009491712 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.04695483669638634, loss=0.034140489995479584
I0205 06:03:22.605028 139804644198144 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.043243199586868286, loss=0.03149600699543953
I0205 06:03:54.802928 139805009491712 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.05658664181828499, loss=0.038429830223321915
I0205 06:04:26.822843 139804644198144 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.04807265102863312, loss=0.03386734798550606
I0205 06:04:59.072777 139805009491712 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.0688888356089592, loss=0.03322378545999527
I0205 06:05:30.938369 139804644198144 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04496864974498749, loss=0.03789689019322395
I0205 06:06:03.180039 139805009491712 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.04461240395903587, loss=0.031615905463695526
I0205 06:06:25.388639 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:08:28.026203 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:08:31.041120 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:08:34.016641 139978932307776 submission_runner.py:408] Time since start: 8374.54s, 	Step: 16570, 	{'train/accuracy': 0.9903900623321533, 'train/loss': 0.03188588097691536, 'train/mean_average_precision': 0.367818958616825, 'validation/accuracy': 0.9866932034492493, 'validation/loss': 0.04465251788496971, 'validation/mean_average_precision': 0.2544019229495246, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04692884162068367, 'test/mean_average_precision': 0.25699559465107674, 'test/num_examples': 43793, 'score': 5306.334696531296, 'total_duration': 8374.538804292679, 'accumulated_submission_time': 5306.334696531296, 'accumulated_eval_time': 3067.133838415146, 'accumulated_logging_time': 0.6414587497711182}
I0205 06:08:34.035191 139804984182528 logging_writer.py:48] [16570] accumulated_eval_time=3067.133838, accumulated_logging_time=0.641459, accumulated_submission_time=5306.334697, global_step=16570, preemption_count=0, score=5306.334697, test/accuracy=0.985962, test/loss=0.046929, test/mean_average_precision=0.256996, test/num_examples=43793, total_duration=8374.538804, train/accuracy=0.990390, train/loss=0.031886, train/mean_average_precision=0.367819, validation/accuracy=0.986693, validation/loss=0.044653, validation/mean_average_precision=0.254402, validation/num_examples=43793
I0205 06:08:43.916579 139805680953088 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.06421913951635361, loss=0.03459830582141876
I0205 06:09:15.920315 139804984182528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06784643977880478, loss=0.03734706714749336
I0205 06:09:47.828358 139805680953088 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.039677005261182785, loss=0.03440617397427559
I0205 06:10:19.819573 139804984182528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.07198808342218399, loss=0.03571426123380661
I0205 06:10:51.513442 139805680953088 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.07116255909204483, loss=0.0371396541595459
I0205 06:11:23.482798 139804984182528 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.06835642457008362, loss=0.03526143729686737
I0205 06:11:55.426377 139805680953088 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.051585011184215546, loss=0.03313923999667168
I0205 06:12:27.842127 139804984182528 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.05324883386492729, loss=0.03542466461658478
I0205 06:12:34.105947 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:14:36.931596 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:14:39.889004 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:14:42.829245 139978932307776 submission_runner.py:408] Time since start: 8743.35s, 	Step: 17320, 	{'train/accuracy': 0.9903742671012878, 'train/loss': 0.031720176339149475, 'train/mean_average_precision': 0.37926661622880675, 'validation/accuracy': 0.9867115020751953, 'validation/loss': 0.0447704903781414, 'validation/mean_average_precision': 0.2558755533093419, 'validation/num_examples': 43793, 'test/accuracy': 0.9859164953231812, 'test/loss': 0.04706665500998497, 'test/mean_average_precision': 0.25208275056372015, 'test/num_examples': 43793, 'score': 5546.373893737793, 'total_duration': 8743.351408958435, 'accumulated_submission_time': 5546.373893737793, 'accumulated_eval_time': 3195.8570890426636, 'accumulated_logging_time': 0.6710550785064697}
I0205 06:14:42.848689 139804644198144 logging_writer.py:48] [17320] accumulated_eval_time=3195.857089, accumulated_logging_time=0.671055, accumulated_submission_time=5546.373894, global_step=17320, preemption_count=0, score=5546.373894, test/accuracy=0.985916, test/loss=0.047067, test/mean_average_precision=0.252083, test/num_examples=43793, total_duration=8743.351409, train/accuracy=0.990374, train/loss=0.031720, train/mean_average_precision=0.379267, validation/accuracy=0.986712, validation/loss=0.044770, validation/mean_average_precision=0.255876, validation/num_examples=43793
I0205 06:15:08.765671 139806663034624 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.05724956467747688, loss=0.03351500630378723
I0205 06:15:41.008328 139804644198144 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06123819574713707, loss=0.03316386416554451
I0205 06:16:13.889965 139806663034624 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.06989334523677826, loss=0.034021008759737015
I0205 06:16:46.521138 139804644198144 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1341811567544937, loss=0.031158804893493652
I0205 06:17:19.155674 139806663034624 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.04509314149618149, loss=0.03248700127005577
I0205 06:17:51.455142 139804644198144 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.08970179408788681, loss=0.03447689488530159
I0205 06:18:23.937100 139806663034624 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.05411909148097038, loss=0.035851214081048965
I0205 06:18:42.859267 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:20:47.599099 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:20:50.628985 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:20:53.571517 139978932307776 submission_runner.py:408] Time since start: 9114.09s, 	Step: 18060, 	{'train/accuracy': 0.9906107783317566, 'train/loss': 0.031032945960760117, 'train/mean_average_precision': 0.38503022354033767, 'validation/accuracy': 0.9868434071540833, 'validation/loss': 0.04448278620839119, 'validation/mean_average_precision': 0.26346505720687874, 'validation/num_examples': 43793, 'test/accuracy': 0.9859927296638489, 'test/loss': 0.04704469069838524, 'test/mean_average_precision': 0.2545460441147208, 'test/num_examples': 43793, 'score': 5786.353320121765, 'total_duration': 9114.093678951263, 'accumulated_submission_time': 5786.353320121765, 'accumulated_eval_time': 3326.569318294525, 'accumulated_logging_time': 0.7022576332092285}
I0205 06:20:53.590554 139804984182528 logging_writer.py:48] [18060] accumulated_eval_time=3326.569318, accumulated_logging_time=0.702258, accumulated_submission_time=5786.353320, global_step=18060, preemption_count=0, score=5786.353320, test/accuracy=0.985993, test/loss=0.047045, test/mean_average_precision=0.254546, test/num_examples=43793, total_duration=9114.093679, train/accuracy=0.990611, train/loss=0.031033, train/mean_average_precision=0.385030, validation/accuracy=0.986843, validation/loss=0.044483, validation/mean_average_precision=0.263465, validation/num_examples=43793
I0205 06:21:07.048193 139805009491712 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1000116616487503, loss=0.03411303460597992
I0205 06:21:39.311859 139804984182528 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.0534549318253994, loss=0.03728614002466202
I0205 06:22:11.516378 139805009491712 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.06082027778029442, loss=0.036073990166187286
I0205 06:22:44.375432 139804984182528 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04911011457443237, loss=0.03352506086230278
I0205 06:23:16.737282 139805009491712 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.056070476770401, loss=0.03282519057393074
I0205 06:23:49.289930 139804984182528 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.06127690151333809, loss=0.03594035282731056
I0205 06:24:21.622378 139805009491712 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.059826381504535675, loss=0.03134213015437126
I0205 06:24:53.839356 139804984182528 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.07032410055398941, loss=0.03435380384325981
I0205 06:24:53.844465 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:27:00.401479 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:27:03.423790 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:27:06.345345 139978932307776 submission_runner.py:408] Time since start: 9486.87s, 	Step: 18801, 	{'train/accuracy': 0.9907031059265137, 'train/loss': 0.030958306044340134, 'train/mean_average_precision': 0.39886864563917124, 'validation/accuracy': 0.9866709113121033, 'validation/loss': 0.04486231133341789, 'validation/mean_average_precision': 0.25838409829558157, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.047371238470077515, 'test/mean_average_precision': 0.2486702052961499, 'test/num_examples': 43793, 'score': 6026.574734687805, 'total_duration': 9486.867498636246, 'accumulated_submission_time': 6026.574734687805, 'accumulated_eval_time': 3459.0701246261597, 'accumulated_logging_time': 0.7338111400604248}
I0205 06:27:06.364481 139804644198144 logging_writer.py:48] [18801] accumulated_eval_time=3459.070125, accumulated_logging_time=0.733811, accumulated_submission_time=6026.574735, global_step=18801, preemption_count=0, score=6026.574735, test/accuracy=0.985849, test/loss=0.047371, test/mean_average_precision=0.248670, test/num_examples=43793, total_duration=9486.867499, train/accuracy=0.990703, train/loss=0.030958, train/mean_average_precision=0.398869, validation/accuracy=0.986671, validation/loss=0.044862, validation/mean_average_precision=0.258384, validation/num_examples=43793
I0205 06:27:38.531002 139805680953088 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.05409127101302147, loss=0.0347207672894001
I0205 06:28:10.528173 139804644198144 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.06574201583862305, loss=0.03492545336484909
I0205 06:28:42.279697 139805680953088 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.0611959733068943, loss=0.03681418299674988
I0205 06:29:14.538658 139804644198144 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.10066567361354828, loss=0.038170225918293
I0205 06:29:46.404955 139805680953088 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.060354214161634445, loss=0.03635097295045853
I0205 06:30:18.656481 139804644198144 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.056563954800367355, loss=0.03603450581431389
I0205 06:30:50.758064 139805680953088 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.07345635443925858, loss=0.03269735351204872
I0205 06:31:06.549252 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:33:12.463917 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:33:15.434551 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:33:18.373564 139978932307776 submission_runner.py:408] Time since start: 9858.90s, 	Step: 19550, 	{'train/accuracy': 0.9908729195594788, 'train/loss': 0.03007735311985016, 'train/mean_average_precision': 0.42760700785825156, 'validation/accuracy': 0.9867585897445679, 'validation/loss': 0.04431118443608284, 'validation/mean_average_precision': 0.26550092224414445, 'validation/num_examples': 43793, 'test/accuracy': 0.9858562350273132, 'test/loss': 0.04700959846377373, 'test/mean_average_precision': 0.25293038105504523, 'test/num_examples': 43793, 'score': 6266.726683616638, 'total_duration': 9858.895729064941, 'accumulated_submission_time': 6266.726683616638, 'accumulated_eval_time': 3590.8943939208984, 'accumulated_logging_time': 0.7656044960021973}
I0205 06:33:18.392531 139804984182528 logging_writer.py:48] [19550] accumulated_eval_time=3590.894394, accumulated_logging_time=0.765604, accumulated_submission_time=6266.726684, global_step=19550, preemption_count=0, score=6266.726684, test/accuracy=0.985856, test/loss=0.047010, test/mean_average_precision=0.252930, test/num_examples=43793, total_duration=9858.895729, train/accuracy=0.990873, train/loss=0.030077, train/mean_average_precision=0.427607, validation/accuracy=0.986759, validation/loss=0.044311, validation/mean_average_precision=0.265501, validation/num_examples=43793
I0205 06:33:34.985412 139805009491712 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06439270079135895, loss=0.03402959927916527
I0205 06:34:07.563046 139804984182528 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.06591787189245224, loss=0.03209056705236435
I0205 06:34:39.794612 139805009491712 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.05470195412635803, loss=0.032636839896440506
I0205 06:35:11.732975 139804984182528 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.05298003926873207, loss=0.034115273505449295
I0205 06:35:44.287028 139805009491712 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.08028686046600342, loss=0.03241715207695961
I0205 06:36:16.581972 139804984182528 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.06822910159826279, loss=0.032353710383176804
I0205 06:36:48.555377 139804984182528 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.10036146640777588, loss=0.0298189464956522
I0205 06:37:18.562001 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:39:24.933973 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:39:27.942415 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:39:30.885666 139978932307776 submission_runner.py:408] Time since start: 10231.41s, 	Step: 20294, 	{'train/accuracy': 0.9909902811050415, 'train/loss': 0.029482202604413033, 'train/mean_average_precision': 0.4394125373132448, 'validation/accuracy': 0.9867805242538452, 'validation/loss': 0.044341910630464554, 'validation/mean_average_precision': 0.26305367815534236, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04684840515255928, 'test/mean_average_precision': 0.25876499272489484, 'test/num_examples': 43793, 'score': 6506.863411426544, 'total_duration': 10231.407826423645, 'accumulated_submission_time': 6506.863411426544, 'accumulated_eval_time': 3723.2180132865906, 'accumulated_logging_time': 0.7975211143493652}
I0205 06:39:30.905107 139805680953088 logging_writer.py:48] [20294] accumulated_eval_time=3723.218013, accumulated_logging_time=0.797521, accumulated_submission_time=6506.863411, global_step=20294, preemption_count=0, score=6506.863411, test/accuracy=0.985962, test/loss=0.046848, test/mean_average_precision=0.258765, test/num_examples=43793, total_duration=10231.407826, train/accuracy=0.990990, train/loss=0.029482, train/mean_average_precision=0.439413, validation/accuracy=0.986781, validation/loss=0.044342, validation/mean_average_precision=0.263054, validation/num_examples=43793
I0205 06:39:33.206621 139806663034624 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.05895921215415001, loss=0.033600520342588425
I0205 06:40:05.628256 139805680953088 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.07633832842111588, loss=0.03519691526889801
I0205 06:40:37.796960 139806663034624 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.06596668064594269, loss=0.03679577261209488
I0205 06:41:10.147975 139805680953088 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.07473061978816986, loss=0.02796153351664543
I0205 06:41:42.206067 139806663034624 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.05642819032073021, loss=0.03563433140516281
I0205 06:42:14.252571 139805680953088 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.08205942064523697, loss=0.03253500163555145
I0205 06:42:46.619188 139806663034624 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.09611622244119644, loss=0.03430699184536934
I0205 06:43:18.705831 139805680953088 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.09079676121473312, loss=0.03554335609078407
I0205 06:43:31.132409 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:45:32.836749 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:45:35.808911 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:45:38.737908 139978932307776 submission_runner.py:408] Time since start: 10599.26s, 	Step: 21040, 	{'train/accuracy': 0.9909581542015076, 'train/loss': 0.02934497594833374, 'train/mean_average_precision': 0.4319782718431922, 'validation/accuracy': 0.9868271946907043, 'validation/loss': 0.044256966561079025, 'validation/mean_average_precision': 0.2620292413708662, 'validation/num_examples': 43793, 'test/accuracy': 0.985920250415802, 'test/loss': 0.04709073156118393, 'test/mean_average_precision': 0.2505250804658075, 'test/num_examples': 43793, 'score': 6747.060025215149, 'total_duration': 10599.26006937027, 'accumulated_submission_time': 6747.060025215149, 'accumulated_eval_time': 3850.82346367836, 'accumulated_logging_time': 0.8276448249816895}
I0205 06:45:38.757027 139804644198144 logging_writer.py:48] [21040] accumulated_eval_time=3850.823464, accumulated_logging_time=0.827645, accumulated_submission_time=6747.060025, global_step=21040, preemption_count=0, score=6747.060025, test/accuracy=0.985920, test/loss=0.047091, test/mean_average_precision=0.250525, test/num_examples=43793, total_duration=10599.260069, train/accuracy=0.990958, train/loss=0.029345, train/mean_average_precision=0.431978, validation/accuracy=0.986827, validation/loss=0.044257, validation/mean_average_precision=0.262029, validation/num_examples=43793
I0205 06:45:58.549084 139804984182528 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.05619410052895546, loss=0.035497911274433136
I0205 06:46:30.525731 139804644198144 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.1068652793765068, loss=0.03447556495666504
I0205 06:47:02.469883 139804984182528 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.060547877103090286, loss=0.03346830606460571
I0205 06:47:34.772427 139804644198144 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.06458549201488495, loss=0.033911608159542084
I0205 06:48:06.913756 139804984182528 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.10861947387456894, loss=0.03760571405291557
I0205 06:48:38.871533 139804644198144 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.058333687484264374, loss=0.03378817439079285
I0205 06:49:10.759399 139804984182528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.05974351242184639, loss=0.029799804091453552
I0205 06:49:38.789507 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:51:41.309605 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:51:44.297823 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:51:47.244961 139978932307776 submission_runner.py:408] Time since start: 10967.77s, 	Step: 21788, 	{'train/accuracy': 0.9909757971763611, 'train/loss': 0.029786057770252228, 'train/mean_average_precision': 0.41127393537537804, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.044571686536073685, 'validation/mean_average_precision': 0.2534629003852076, 'validation/num_examples': 43793, 'test/accuracy': 0.9858971238136292, 'test/loss': 0.047172050923109055, 'test/mean_average_precision': 0.24506086347335104, 'test/num_examples': 43793, 'score': 6987.061242580414, 'total_duration': 10967.76712179184, 'accumulated_submission_time': 6987.061242580414, 'accumulated_eval_time': 3979.2788722515106, 'accumulated_logging_time': 0.8579602241516113}
I0205 06:51:47.264233 139805009491712 logging_writer.py:48] [21788] accumulated_eval_time=3979.278872, accumulated_logging_time=0.857960, accumulated_submission_time=6987.061243, global_step=21788, preemption_count=0, score=6987.061243, test/accuracy=0.985897, test/loss=0.047172, test/mean_average_precision=0.245061, test/num_examples=43793, total_duration=10967.767122, train/accuracy=0.990976, train/loss=0.029786, train/mean_average_precision=0.411274, validation/accuracy=0.986752, validation/loss=0.044572, validation/mean_average_precision=0.253463, validation/num_examples=43793
I0205 06:51:51.591847 139805680953088 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.08483314514160156, loss=0.03429757058620453
I0205 06:52:23.734184 139805009491712 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.0687461793422699, loss=0.03697432950139046
I0205 06:52:55.996052 139805680953088 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08450578898191452, loss=0.03601425513625145
I0205 06:53:28.420447 139805009491712 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.0851343646645546, loss=0.035910140722990036
I0205 06:54:00.356548 139805680953088 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.06193094700574875, loss=0.03472844138741493
I0205 06:54:32.487811 139805009491712 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.06915531307458878, loss=0.03282840922474861
I0205 06:55:04.592491 139805680953088 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.06217611953616142, loss=0.03481265902519226
I0205 06:55:36.764329 139805009491712 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.12807895243167877, loss=0.035230111330747604
I0205 06:55:47.252907 139978932307776 spec.py:321] Evaluating on the training split.
I0205 06:57:52.860697 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 06:57:55.873017 139978932307776 spec.py:349] Evaluating on the test split.
I0205 06:57:58.809475 139978932307776 submission_runner.py:408] Time since start: 11339.33s, 	Step: 22534, 	{'train/accuracy': 0.9908212423324585, 'train/loss': 0.03031608648598194, 'train/mean_average_precision': 0.4167117446792816, 'validation/accuracy': 0.9868998527526855, 'validation/loss': 0.044284187257289886, 'validation/mean_average_precision': 0.26396243707412637, 'validation/num_examples': 43793, 'test/accuracy': 0.9860268235206604, 'test/loss': 0.04701342061161995, 'test/mean_average_precision': 0.259165562861682, 'test/num_examples': 43793, 'score': 7227.019123077393, 'total_duration': 11339.331525325775, 'accumulated_submission_time': 7227.019123077393, 'accumulated_eval_time': 4110.835282087326, 'accumulated_logging_time': 0.8883786201477051}
I0205 06:57:58.829083 139804644198144 logging_writer.py:48] [22534] accumulated_eval_time=4110.835282, accumulated_logging_time=0.888379, accumulated_submission_time=7227.019123, global_step=22534, preemption_count=0, score=7227.019123, test/accuracy=0.986027, test/loss=0.047013, test/mean_average_precision=0.259166, test/num_examples=43793, total_duration=11339.331525, train/accuracy=0.990821, train/loss=0.030316, train/mean_average_precision=0.416712, validation/accuracy=0.986900, validation/loss=0.044284, validation/mean_average_precision=0.263962, validation/num_examples=43793
I0205 06:58:20.324329 139804984182528 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.08415912836790085, loss=0.03513228893280029
I0205 06:58:52.365339 139804644198144 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.06396111100912094, loss=0.03098304569721222
I0205 06:59:24.449882 139804984182528 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.08447562158107758, loss=0.036169443279504776
I0205 06:59:56.788927 139804644198144 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.07879677414894104, loss=0.03306209668517113
I0205 07:00:29.068584 139804984182528 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.07681526243686676, loss=0.03234580159187317
I0205 07:01:01.056849 139804644198144 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.07244598865509033, loss=0.032766424119472504
I0205 07:01:33.120683 139804984182528 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.07942355424165726, loss=0.03430355712771416
I0205 07:01:58.815116 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:04:02.437080 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:04:05.445137 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:04:08.462992 139978932307776 submission_runner.py:408] Time since start: 11708.99s, 	Step: 23281, 	{'train/accuracy': 0.9908854365348816, 'train/loss': 0.03038932941854, 'train/mean_average_precision': 0.4145162980485438, 'validation/accuracy': 0.986748456954956, 'validation/loss': 0.04439399763941765, 'validation/mean_average_precision': 0.26433337024141745, 'validation/num_examples': 43793, 'test/accuracy': 0.9858128428459167, 'test/loss': 0.046998314559459686, 'test/mean_average_precision': 0.2511379732354883, 'test/num_examples': 43793, 'score': 7466.973075628281, 'total_duration': 11708.985038518906, 'accumulated_submission_time': 7466.973075628281, 'accumulated_eval_time': 4240.48300409317, 'accumulated_logging_time': 0.9200398921966553}
I0205 07:04:08.482953 139805009491712 logging_writer.py:48] [23281] accumulated_eval_time=4240.483004, accumulated_logging_time=0.920040, accumulated_submission_time=7466.973076, global_step=23281, preemption_count=0, score=7466.973076, test/accuracy=0.985813, test/loss=0.046998, test/mean_average_precision=0.251138, test/num_examples=43793, total_duration=11708.985039, train/accuracy=0.990885, train/loss=0.030389, train/mean_average_precision=0.414516, validation/accuracy=0.986748, validation/loss=0.044394, validation/mean_average_precision=0.264333, validation/num_examples=43793
I0205 07:04:15.170120 139805680953088 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.06889577955007553, loss=0.03594637289643288
I0205 07:04:47.535392 139805009491712 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.06303855031728745, loss=0.030401665717363358
I0205 07:05:19.635832 139805680953088 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.07141071557998657, loss=0.03631528466939926
I0205 07:05:51.469946 139805009491712 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.11960209161043167, loss=0.031013915315270424
I0205 07:06:23.361458 139805680953088 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.10843870788812637, loss=0.034701328724622726
I0205 07:06:55.376772 139805009491712 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.07318896055221558, loss=0.03506001830101013
I0205 07:07:27.165544 139805680953088 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.0904809907078743, loss=0.0343792587518692
I0205 07:07:58.822674 139805009491712 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.07867780327796936, loss=0.03582771122455597
I0205 07:08:08.589962 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:10:12.995907 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:10:16.052637 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:10:19.040505 139978932307776 submission_runner.py:408] Time since start: 12079.56s, 	Step: 24031, 	{'train/accuracy': 0.9907976984977722, 'train/loss': 0.030122244730591774, 'train/mean_average_precision': 0.4307058110209221, 'validation/accuracy': 0.9866781830787659, 'validation/loss': 0.04450766369700432, 'validation/mean_average_precision': 0.2598207593992417, 'validation/num_examples': 43793, 'test/accuracy': 0.9858853220939636, 'test/loss': 0.04721352085471153, 'test/mean_average_precision': 0.2514192693563914, 'test/num_examples': 43793, 'score': 7706.785302639008, 'total_duration': 12079.562668561935, 'accumulated_submission_time': 7706.785302639008, 'accumulated_eval_time': 4370.933499574661, 'accumulated_logging_time': 1.214691162109375}
I0205 07:10:19.060202 139804984182528 logging_writer.py:48] [24031] accumulated_eval_time=4370.933500, accumulated_logging_time=1.214691, accumulated_submission_time=7706.785303, global_step=24031, preemption_count=0, score=7706.785303, test/accuracy=0.985885, test/loss=0.047214, test/mean_average_precision=0.251419, test/num_examples=43793, total_duration=12079.562669, train/accuracy=0.990798, train/loss=0.030122, train/mean_average_precision=0.430706, validation/accuracy=0.986678, validation/loss=0.044508, validation/mean_average_precision=0.259821, validation/num_examples=43793
I0205 07:10:41.320461 139806663034624 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.08464377373456955, loss=0.03411918878555298
I0205 07:11:13.253772 139804984182528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.08478215336799622, loss=0.03415794298052788
I0205 07:11:44.888082 139806663034624 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.08671452105045319, loss=0.02978431060910225
I0205 07:12:16.894985 139804984182528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.060444705188274384, loss=0.0291635412722826
I0205 07:12:48.772684 139806663034624 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0746261402964592, loss=0.03333292156457901
I0205 07:13:20.730943 139804984182528 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.0698738545179367, loss=0.03265388309955597
I0205 07:13:52.589113 139806663034624 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.06985338032245636, loss=0.03811536729335785
I0205 07:14:19.263817 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:16:21.247807 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:16:24.333730 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:16:27.297608 139978932307776 submission_runner.py:408] Time since start: 12447.82s, 	Step: 24784, 	{'train/accuracy': 0.9909395575523376, 'train/loss': 0.02971666306257248, 'train/mean_average_precision': 0.4196510958908334, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.044562917202711105, 'validation/mean_average_precision': 0.2656587042217679, 'validation/num_examples': 43793, 'test/accuracy': 0.9858975410461426, 'test/loss': 0.04716982692480087, 'test/mean_average_precision': 0.25909295942461513, 'test/num_examples': 43793, 'score': 7946.956842422485, 'total_duration': 12447.819665908813, 'accumulated_submission_time': 7946.956842422485, 'accumulated_eval_time': 4498.967143058777, 'accumulated_logging_time': 1.2464373111724854}
I0205 07:16:27.317037 139805009491712 logging_writer.py:48] [24784] accumulated_eval_time=4498.967143, accumulated_logging_time=1.246437, accumulated_submission_time=7946.956842, global_step=24784, preemption_count=0, score=7946.956842, test/accuracy=0.985898, test/loss=0.047170, test/mean_average_precision=0.259093, test/num_examples=43793, total_duration=12447.819666, train/accuracy=0.990940, train/loss=0.029717, train/mean_average_precision=0.419651, validation/accuracy=0.986783, validation/loss=0.044563, validation/mean_average_precision=0.265659, validation/num_examples=43793
I0205 07:16:32.706717 139805680953088 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.06647884845733643, loss=0.028219757601618767
I0205 07:17:04.234418 139805009491712 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.09443231672048569, loss=0.03446004167199135
I0205 07:17:35.943469 139805680953088 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.06564546376466751, loss=0.032864488661289215
I0205 07:18:07.551305 139805009491712 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.10241743177175522, loss=0.03063960373401642
I0205 07:18:39.179898 139805680953088 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.06747496128082275, loss=0.03347930684685707
I0205 07:19:10.847514 139805009491712 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.07287485897541046, loss=0.032834261655807495
I0205 07:19:42.768693 139805680953088 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07260483503341675, loss=0.03229039907455444
I0205 07:20:14.308903 139805009491712 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.06643721461296082, loss=0.031077813357114792
I0205 07:20:27.582053 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:22:27.442164 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:22:30.463667 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:22:33.426564 139978932307776 submission_runner.py:408] Time since start: 12813.95s, 	Step: 25543, 	{'train/accuracy': 0.991019070148468, 'train/loss': 0.02948913164436817, 'train/mean_average_precision': 0.4367420533835061, 'validation/accuracy': 0.9868320822715759, 'validation/loss': 0.04458141699433327, 'validation/mean_average_precision': 0.2679887542443359, 'validation/num_examples': 43793, 'test/accuracy': 0.9859825968742371, 'test/loss': 0.047204818576574326, 'test/mean_average_precision': 0.2633457291123027, 'test/num_examples': 43793, 'score': 8187.189737558365, 'total_duration': 12813.948610544205, 'accumulated_submission_time': 8187.189737558365, 'accumulated_eval_time': 4624.81149148941, 'accumulated_logging_time': 1.2782447338104248}
I0205 07:22:33.446488 139804984182528 logging_writer.py:48] [25543] accumulated_eval_time=4624.811491, accumulated_logging_time=1.278245, accumulated_submission_time=8187.189738, global_step=25543, preemption_count=0, score=8187.189738, test/accuracy=0.985983, test/loss=0.047205, test/mean_average_precision=0.263346, test/num_examples=43793, total_duration=12813.948611, train/accuracy=0.991019, train/loss=0.029489, train/mean_average_precision=0.436742, validation/accuracy=0.986832, validation/loss=0.044581, validation/mean_average_precision=0.267989, validation/num_examples=43793
I0205 07:22:51.793697 139806663034624 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.07340392470359802, loss=0.03360455483198166
I0205 07:23:23.316581 139804984182528 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.09442226588726044, loss=0.03655751049518585
I0205 07:23:54.773216 139806663034624 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.08066562563180923, loss=0.03342658281326294
I0205 07:24:26.642098 139804984182528 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.08534963428974152, loss=0.03783141449093819
I0205 07:24:58.186409 139806663034624 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.07593096792697906, loss=0.032697826623916626
I0205 07:25:29.782934 139804984182528 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.06881024688482285, loss=0.0350237712264061
I0205 07:26:01.536788 139806663034624 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.08718648552894592, loss=0.03504753112792969
I0205 07:26:33.049029 139804984182528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.08727914839982986, loss=0.03169124573469162
I0205 07:26:33.703323 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:28:33.723565 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:28:36.795001 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:28:39.751440 139978932307776 submission_runner.py:408] Time since start: 13180.27s, 	Step: 26303, 	{'train/accuracy': 0.9911377429962158, 'train/loss': 0.029033243656158447, 'train/mean_average_precision': 0.4396199351537836, 'validation/accuracy': 0.9868190884590149, 'validation/loss': 0.04453993961215019, 'validation/mean_average_precision': 0.26872231422311754, 'validation/num_examples': 43793, 'test/accuracy': 0.9859455227851868, 'test/loss': 0.04693721979856491, 'test/mean_average_precision': 0.2593809136492444, 'test/num_examples': 43793, 'score': 8427.414668560028, 'total_duration': 13180.273483514786, 'accumulated_submission_time': 8427.414668560028, 'accumulated_eval_time': 4750.8594517707825, 'accumulated_logging_time': 1.3099524974822998}
I0205 07:28:39.771286 139805009491712 logging_writer.py:48] [26303] accumulated_eval_time=4750.859452, accumulated_logging_time=1.309952, accumulated_submission_time=8427.414669, global_step=26303, preemption_count=0, score=8427.414669, test/accuracy=0.985946, test/loss=0.046937, test/mean_average_precision=0.259381, test/num_examples=43793, total_duration=13180.273484, train/accuracy=0.991138, train/loss=0.029033, train/mean_average_precision=0.439620, validation/accuracy=0.986819, validation/loss=0.044540, validation/mean_average_precision=0.268722, validation/num_examples=43793
I0205 07:29:10.845614 139805680953088 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.08343537151813507, loss=0.03393128141760826
I0205 07:29:42.856746 139805009491712 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.11214394867420197, loss=0.03289413824677467
I0205 07:30:14.743302 139805680953088 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.0812060683965683, loss=0.03335634246468544
I0205 07:30:46.286909 139805009491712 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10414627939462662, loss=0.03386444225907326
I0205 07:31:18.069623 139805680953088 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.07586365193128586, loss=0.033971913158893585
I0205 07:31:49.776639 139805009491712 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08717828243970871, loss=0.037790972739458084
I0205 07:32:21.390662 139805680953088 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0831269919872284, loss=0.03283155709505081
I0205 07:32:39.753612 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:34:43.159258 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:34:46.221627 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:34:49.275030 139978932307776 submission_runner.py:408] Time since start: 13549.80s, 	Step: 27058, 	{'train/accuracy': 0.9913957715034485, 'train/loss': 0.028460824862122536, 'train/mean_average_precision': 0.4587331702582208, 'validation/accuracy': 0.9867423176765442, 'validation/loss': 0.04439501091837883, 'validation/mean_average_precision': 0.2620780825173272, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.04702652990818024, 'test/mean_average_precision': 0.25268346439916317, 'test/num_examples': 43793, 'score': 8667.366248846054, 'total_duration': 13549.797183036804, 'accumulated_submission_time': 8667.366248846054, 'accumulated_eval_time': 4880.380812883377, 'accumulated_logging_time': 1.3404219150543213}
I0205 07:34:49.297789 139804644198144 logging_writer.py:48] [27058] accumulated_eval_time=4880.380813, accumulated_logging_time=1.340422, accumulated_submission_time=8667.366249, global_step=27058, preemption_count=0, score=8667.366249, test/accuracy=0.985877, test/loss=0.047027, test/mean_average_precision=0.252683, test/num_examples=43793, total_duration=13549.797183, train/accuracy=0.991396, train/loss=0.028461, train/mean_average_precision=0.458733, validation/accuracy=0.986742, validation/loss=0.044395, validation/mean_average_precision=0.262078, validation/num_examples=43793
I0205 07:35:03.195389 139806663034624 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06991469115018845, loss=0.03411313146352768
I0205 07:35:35.267665 139804644198144 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.06946057826280594, loss=0.03230719268321991
I0205 07:36:07.336016 139806663034624 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.06543615460395813, loss=0.031837981194257736
I0205 07:36:39.118060 139804644198144 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.08399521559476852, loss=0.03491100296378136
I0205 07:37:10.904268 139806663034624 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06788217276334763, loss=0.03481566160917282
I0205 07:37:42.581304 139804644198144 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.08448605984449387, loss=0.03241513669490814
I0205 07:38:14.317054 139806663034624 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.0678548812866211, loss=0.033262692391872406
I0205 07:38:45.870230 139804644198144 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.06967031955718994, loss=0.03284390643239021
I0205 07:38:49.322140 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:40:50.765851 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:40:53.851339 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:40:56.806063 139978932307776 submission_runner.py:408] Time since start: 13917.33s, 	Step: 27812, 	{'train/accuracy': 0.9915438890457153, 'train/loss': 0.02755151130259037, 'train/mean_average_precision': 0.478820071746103, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.04407647252082825, 'validation/mean_average_precision': 0.268497485961063, 'validation/num_examples': 43793, 'test/accuracy': 0.9860082864761353, 'test/loss': 0.046913038939237595, 'test/mean_average_precision': 0.256955366585077, 'test/num_examples': 43793, 'score': 8907.356931447983, 'total_duration': 13917.328118801117, 'accumulated_submission_time': 8907.356931447983, 'accumulated_eval_time': 5007.864626646042, 'accumulated_logging_time': 1.3766028881072998}
I0205 07:40:56.826446 139805009491712 logging_writer.py:48] [27812] accumulated_eval_time=5007.864627, accumulated_logging_time=1.376603, accumulated_submission_time=8907.356931, global_step=27812, preemption_count=0, score=8907.356931, test/accuracy=0.986008, test/loss=0.046913, test/mean_average_precision=0.256955, test/num_examples=43793, total_duration=13917.328119, train/accuracy=0.991544, train/loss=0.027552, train/mean_average_precision=0.478820, validation/accuracy=0.986845, validation/loss=0.044076, validation/mean_average_precision=0.268497, validation/num_examples=43793
I0205 07:41:25.115017 139805680953088 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0989619567990303, loss=0.034479908645153046
I0205 07:41:56.982056 139805009491712 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.08572288602590561, loss=0.034198712557554245
I0205 07:42:29.000308 139805680953088 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.07606206834316254, loss=0.03508496284484863
I0205 07:43:00.793520 139805009491712 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.06928088515996933, loss=0.03596383333206177
I0205 07:43:32.913599 139805680953088 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.09535422921180725, loss=0.03323930874466896
I0205 07:44:04.635276 139805009491712 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.09023384004831314, loss=0.032131947576999664
I0205 07:44:36.408737 139805680953088 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07108946144580841, loss=0.03267785906791687
I0205 07:44:57.039199 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:47:00.998489 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:47:04.063803 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:47:07.020736 139978932307776 submission_runner.py:408] Time since start: 14287.54s, 	Step: 28566, 	{'train/accuracy': 0.9915900230407715, 'train/loss': 0.02743000164628029, 'train/mean_average_precision': 0.4838465650251311, 'validation/accuracy': 0.9868852496147156, 'validation/loss': 0.04411470890045166, 'validation/mean_average_precision': 0.27742265750240147, 'validation/num_examples': 43793, 'test/accuracy': 0.9860162734985352, 'test/loss': 0.04679397866129875, 'test/mean_average_precision': 0.2587861733148328, 'test/num_examples': 43793, 'score': 9147.537345647812, 'total_duration': 14287.542890071869, 'accumulated_submission_time': 9147.537345647812, 'accumulated_eval_time': 5137.846126794815, 'accumulated_logging_time': 1.4091360569000244}
I0205 07:47:07.042291 139804644198144 logging_writer.py:48] [28566] accumulated_eval_time=5137.846127, accumulated_logging_time=1.409136, accumulated_submission_time=9147.537346, global_step=28566, preemption_count=0, score=9147.537346, test/accuracy=0.986016, test/loss=0.046794, test/mean_average_precision=0.258786, test/num_examples=43793, total_duration=14287.542890, train/accuracy=0.991590, train/loss=0.027430, train/mean_average_precision=0.483847, validation/accuracy=0.986885, validation/loss=0.044115, validation/mean_average_precision=0.277423, validation/num_examples=43793
I0205 07:47:18.407644 139806663034624 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.08961829543113708, loss=0.032985299825668335
I0205 07:47:50.495022 139804644198144 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.1016821339726448, loss=0.03347974643111229
I0205 07:48:22.468863 139806663034624 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.0789901465177536, loss=0.03260768949985504
I0205 07:48:54.517990 139804644198144 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.08307874947786331, loss=0.030513597652316093
I0205 07:49:26.325454 139806663034624 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0678439736366272, loss=0.02961329184472561
I0205 07:49:58.232126 139804644198144 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.10891643911600113, loss=0.03170362859964371
I0205 07:50:30.507215 139806663034624 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.08767353743314743, loss=0.03214822709560394
I0205 07:51:02.559172 139804644198144 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.08421777188777924, loss=0.034102603793144226
I0205 07:51:07.351369 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:53:13.112966 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:53:16.131475 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:53:19.077385 139978932307776 submission_runner.py:408] Time since start: 14659.60s, 	Step: 29316, 	{'train/accuracy': 0.9914113283157349, 'train/loss': 0.028087593615055084, 'train/mean_average_precision': 0.4503752334796423, 'validation/accuracy': 0.9868101477622986, 'validation/loss': 0.04454953595995903, 'validation/mean_average_precision': 0.26453523417580793, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.04737480357289314, 'test/mean_average_precision': 0.2550472083789822, 'test/num_examples': 43793, 'score': 9387.8157954216, 'total_duration': 14659.599410057068, 'accumulated_submission_time': 9387.8157954216, 'accumulated_eval_time': 5269.571957588196, 'accumulated_logging_time': 1.4414465427398682}
I0205 07:53:19.098010 139805009491712 logging_writer.py:48] [29316] accumulated_eval_time=5269.571958, accumulated_logging_time=1.441447, accumulated_submission_time=9387.815795, global_step=29316, preemption_count=0, score=9387.815795, test/accuracy=0.985925, test/loss=0.047375, test/mean_average_precision=0.255047, test/num_examples=43793, total_duration=14659.599410, train/accuracy=0.991411, train/loss=0.028088, train/mean_average_precision=0.450375, validation/accuracy=0.986810, validation/loss=0.044550, validation/mean_average_precision=0.264535, validation/num_examples=43793
I0205 07:53:45.999866 139805680953088 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06905331462621689, loss=0.03055933490395546
I0205 07:54:17.794283 139805009491712 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.08341700583696365, loss=0.03127767890691757
I0205 07:54:49.599314 139805680953088 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.09076210111379623, loss=0.03218630701303482
I0205 07:55:21.217028 139805009491712 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.08811471611261368, loss=0.03134023770689964
I0205 07:55:52.903665 139805680953088 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06826824694871902, loss=0.03146493434906006
I0205 07:56:24.519908 139805009491712 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.07928662747144699, loss=0.03333330526947975
I0205 07:56:56.107257 139805680953088 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.08338324725627899, loss=0.034472815692424774
I0205 07:57:19.133168 139978932307776 spec.py:321] Evaluating on the training split.
I0205 07:59:22.292004 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 07:59:25.324182 139978932307776 spec.py:349] Evaluating on the test split.
I0205 07:59:28.307922 139978932307776 submission_runner.py:408] Time since start: 15028.83s, 	Step: 30073, 	{'train/accuracy': 0.9912699460983276, 'train/loss': 0.028659606352448463, 'train/mean_average_precision': 0.44959692861203004, 'validation/accuracy': 0.9867662787437439, 'validation/loss': 0.044700346887111664, 'validation/mean_average_precision': 0.2619020218166186, 'validation/num_examples': 43793, 'test/accuracy': 0.9858600497245789, 'test/loss': 0.04760089889168739, 'test/mean_average_precision': 0.2547086969015181, 'test/num_examples': 43793, 'score': 9627.81943321228, 'total_duration': 15028.82996916771, 'accumulated_submission_time': 9627.81943321228, 'accumulated_eval_time': 5398.746550559998, 'accumulated_logging_time': 1.4734342098236084}
I0205 07:59:28.328914 139804644198144 logging_writer.py:48] [30073] accumulated_eval_time=5398.746551, accumulated_logging_time=1.473434, accumulated_submission_time=9627.819433, global_step=30073, preemption_count=0, score=9627.819433, test/accuracy=0.985860, test/loss=0.047601, test/mean_average_precision=0.254709, test/num_examples=43793, total_duration=15028.829969, train/accuracy=0.991270, train/loss=0.028660, train/mean_average_precision=0.449597, validation/accuracy=0.986766, validation/loss=0.044700, validation/mean_average_precision=0.261902, validation/num_examples=43793
I0205 07:59:37.154425 139806663034624 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.08841082453727722, loss=0.03358910605311394
I0205 08:00:08.893619 139804644198144 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.07193490862846375, loss=0.0317625068128109
I0205 08:00:40.631756 139806663034624 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.07591000944375992, loss=0.030412260442972183
I0205 08:01:12.726732 139804644198144 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.1071266233921051, loss=0.032625772058963776
I0205 08:01:44.413425 139806663034624 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.07972556352615356, loss=0.03462005406618118
I0205 08:02:16.308900 139804644198144 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.07761669158935547, loss=0.029312552884221077
I0205 08:02:47.970190 139806663034624 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.07133060693740845, loss=0.030042484402656555
I0205 08:03:19.388222 139804644198144 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1041136085987091, loss=0.03318071365356445
I0205 08:03:28.325727 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:05:31.439284 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:05:34.452181 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:05:37.397068 139978932307776 submission_runner.py:408] Time since start: 15397.92s, 	Step: 30829, 	{'train/accuracy': 0.9910553097724915, 'train/loss': 0.02904028445482254, 'train/mean_average_precision': 0.4345576765374209, 'validation/accuracy': 0.9868341088294983, 'validation/loss': 0.044944878667593, 'validation/mean_average_precision': 0.26515396532756713, 'validation/num_examples': 43793, 'test/accuracy': 0.985971212387085, 'test/loss': 0.047688573598861694, 'test/mean_average_precision': 0.25496920849557436, 'test/num_examples': 43793, 'score': 9867.785235404968, 'total_duration': 15397.919102430344, 'accumulated_submission_time': 9867.785235404968, 'accumulated_eval_time': 5527.817716121674, 'accumulated_logging_time': 1.5052387714385986}
I0205 08:05:37.418136 139804984182528 logging_writer.py:48] [30829] accumulated_eval_time=5527.817716, accumulated_logging_time=1.505239, accumulated_submission_time=9867.785235, global_step=30829, preemption_count=0, score=9867.785235, test/accuracy=0.985971, test/loss=0.047689, test/mean_average_precision=0.254969, test/num_examples=43793, total_duration=15397.919102, train/accuracy=0.991055, train/loss=0.029040, train/mean_average_precision=0.434558, validation/accuracy=0.986834, validation/loss=0.044945, validation/mean_average_precision=0.265154, validation/num_examples=43793
I0205 08:06:01.106933 139805680953088 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.11761872470378876, loss=0.033805955201387405
I0205 08:06:32.656710 139804984182528 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.09585526585578918, loss=0.03246576711535454
I0205 08:07:04.418302 139805680953088 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06541868299245834, loss=0.030813774093985558
I0205 08:07:35.952404 139804984182528 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.07975251972675323, loss=0.03347507119178772
I0205 08:08:07.530932 139805680953088 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.07298813760280609, loss=0.031595516949892044
I0205 08:08:38.806475 139804984182528 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.07907132059335709, loss=0.02991282194852829
I0205 08:09:10.366501 139805680953088 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.06719668209552765, loss=0.02811460942029953
I0205 08:09:37.550167 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:11:38.925678 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:11:41.959168 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:11:44.933017 139978932307776 submission_runner.py:408] Time since start: 15765.46s, 	Step: 31588, 	{'train/accuracy': 0.9913302063941956, 'train/loss': 0.028579991310834885, 'train/mean_average_precision': 0.45910879798687093, 'validation/accuracy': 0.9867050051689148, 'validation/loss': 0.04459002614021301, 'validation/mean_average_precision': 0.2681575482452986, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.04713777080178261, 'test/mean_average_precision': 0.25802998955314144, 'test/num_examples': 43793, 'score': 10107.885499715805, 'total_duration': 15765.455152750015, 'accumulated_submission_time': 10107.885499715805, 'accumulated_eval_time': 5655.2004935741425, 'accumulated_logging_time': 1.5384142398834229}
I0205 08:11:44.954250 139804644198144 logging_writer.py:48] [31588] accumulated_eval_time=5655.200494, accumulated_logging_time=1.538414, accumulated_submission_time=10107.885500, global_step=31588, preemption_count=0, score=10107.885500, test/accuracy=0.985917, test/loss=0.047138, test/mean_average_precision=0.258030, test/num_examples=43793, total_duration=15765.455153, train/accuracy=0.991330, train/loss=0.028580, train/mean_average_precision=0.459109, validation/accuracy=0.986705, validation/loss=0.044590, validation/mean_average_precision=0.268158, validation/num_examples=43793
I0205 08:11:49.103574 139805009491712 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.07443854957818985, loss=0.03424904868006706
I0205 08:12:20.824827 139804644198144 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.07930373400449753, loss=0.03188567981123924
I0205 08:12:52.845330 139805009491712 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.0908360704779625, loss=0.034168798476457596
I0205 08:13:24.591763 139804644198144 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.07993640750646591, loss=0.03329307213425636
I0205 08:13:56.037530 139805009491712 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.08775881677865982, loss=0.03343168646097183
I0205 08:14:27.464889 139804644198144 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.08147238940000534, loss=0.028928382322192192
I0205 08:14:59.141849 139805009491712 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.10597685724496841, loss=0.03459259495139122
I0205 08:15:30.550046 139804644198144 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.09032177180051804, loss=0.03118511103093624
I0205 08:15:44.982430 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:17:47.632397 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:17:50.608752 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:17:53.557918 139978932307776 submission_runner.py:408] Time since start: 16134.08s, 	Step: 32346, 	{'train/accuracy': 0.9914727210998535, 'train/loss': 0.0278371162712574, 'train/mean_average_precision': 0.47179791076157324, 'validation/accuracy': 0.9869274497032166, 'validation/loss': 0.043929535895586014, 'validation/mean_average_precision': 0.27411557041168505, 'validation/num_examples': 43793, 'test/accuracy': 0.9860752820968628, 'test/loss': 0.04677533358335495, 'test/mean_average_precision': 0.26618305739415, 'test/num_examples': 43793, 'score': 10347.881727457047, 'total_duration': 16134.079976081848, 'accumulated_submission_time': 10347.881727457047, 'accumulated_eval_time': 5783.775832414627, 'accumulated_logging_time': 1.572129487991333}
I0205 08:17:53.578985 139804984182528 logging_writer.py:48] [32346] accumulated_eval_time=5783.775832, accumulated_logging_time=1.572129, accumulated_submission_time=10347.881727, global_step=32346, preemption_count=0, score=10347.881727, test/accuracy=0.986075, test/loss=0.046775, test/mean_average_precision=0.266183, test/num_examples=43793, total_duration=16134.079976, train/accuracy=0.991473, train/loss=0.027837, train/mean_average_precision=0.471798, validation/accuracy=0.986927, validation/loss=0.043930, validation/mean_average_precision=0.274116, validation/num_examples=43793
I0205 08:18:10.955854 139806663034624 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.07445734739303589, loss=0.03170336037874222
I0205 08:18:42.733697 139804984182528 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.07455819845199585, loss=0.02973952889442444
I0205 08:19:14.340859 139806663034624 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.08016501367092133, loss=0.03291838616132736
I0205 08:19:46.033350 139804984182528 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.08455324172973633, loss=0.03288666903972626
I0205 08:20:17.927310 139806663034624 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.0797511413693428, loss=0.033899661153554916
I0205 08:20:49.528494 139804984182528 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.10822177678346634, loss=0.03565887361764908
I0205 08:21:21.533245 139806663034624 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.08412142843008041, loss=0.03175649046897888
I0205 08:21:53.313942 139804984182528 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.08662465214729309, loss=0.033611174672842026
I0205 08:21:53.633190 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:23:54.692252 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:23:57.666611 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:24:00.626331 139978932307776 submission_runner.py:408] Time since start: 16501.15s, 	Step: 33102, 	{'train/accuracy': 0.9913511276245117, 'train/loss': 0.028184767812490463, 'train/mean_average_precision': 0.45692966019385856, 'validation/accuracy': 0.9868227243423462, 'validation/loss': 0.04469173774123192, 'validation/mean_average_precision': 0.26549203945799665, 'validation/num_examples': 43793, 'test/accuracy': 0.985958993434906, 'test/loss': 0.047356247901916504, 'test/mean_average_precision': 0.2575073350230395, 'test/num_examples': 43793, 'score': 10587.903796672821, 'total_duration': 16501.14848446846, 'accumulated_submission_time': 10587.903796672821, 'accumulated_eval_time': 5910.768916606903, 'accumulated_logging_time': 1.6054506301879883}
I0205 08:24:00.647549 139804644198144 logging_writer.py:48] [33102] accumulated_eval_time=5910.768917, accumulated_logging_time=1.605451, accumulated_submission_time=10587.903797, global_step=33102, preemption_count=0, score=10587.903797, test/accuracy=0.985959, test/loss=0.047356, test/mean_average_precision=0.257507, test/num_examples=43793, total_duration=16501.148484, train/accuracy=0.991351, train/loss=0.028185, train/mean_average_precision=0.456930, validation/accuracy=0.986823, validation/loss=0.044692, validation/mean_average_precision=0.265492, validation/num_examples=43793
I0205 08:24:31.936132 139805009491712 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.08039142191410065, loss=0.030588701367378235
I0205 08:25:03.798378 139804644198144 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.07526127994060516, loss=0.032718949019908905
I0205 08:25:35.307245 139805009491712 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.11270269006490707, loss=0.03473075106739998
I0205 08:26:06.924761 139804644198144 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0777900293469429, loss=0.03351856768131256
I0205 08:26:38.930764 139805009491712 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.07237551361322403, loss=0.030128516256809235
I0205 08:27:10.558329 139804644198144 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.10761291533708572, loss=0.033804893493652344
I0205 08:27:42.050503 139805009491712 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.07227572053670883, loss=0.027987424284219742
I0205 08:28:00.823275 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:30:00.942781 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:30:04.007656 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:30:06.980895 139978932307776 submission_runner.py:408] Time since start: 16867.50s, 	Step: 33860, 	{'train/accuracy': 0.9915713667869568, 'train/loss': 0.02751315012574196, 'train/mean_average_precision': 0.4805088503117457, 'validation/accuracy': 0.9869164824485779, 'validation/loss': 0.04435295984148979, 'validation/mean_average_precision': 0.27338494682774434, 'validation/num_examples': 43793, 'test/accuracy': 0.9860659837722778, 'test/loss': 0.047166142612695694, 'test/mean_average_precision': 0.25900657790581455, 'test/num_examples': 43793, 'score': 10828.046881198883, 'total_duration': 16867.5029835701, 'accumulated_submission_time': 10828.046881198883, 'accumulated_eval_time': 6036.926429271698, 'accumulated_logging_time': 1.6387813091278076}
I0205 08:30:07.003678 139804984182528 logging_writer.py:48] [33860] accumulated_eval_time=6036.926429, accumulated_logging_time=1.638781, accumulated_submission_time=10828.046881, global_step=33860, preemption_count=0, score=10828.046881, test/accuracy=0.986066, test/loss=0.047166, test/mean_average_precision=0.259007, test/num_examples=43793, total_duration=16867.502984, train/accuracy=0.991571, train/loss=0.027513, train/mean_average_precision=0.480509, validation/accuracy=0.986916, validation/loss=0.044353, validation/mean_average_precision=0.273385, validation/num_examples=43793
I0205 08:30:20.187460 139805680953088 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.08511026948690414, loss=0.03164881840348244
I0205 08:30:52.149436 139804984182528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.09399857372045517, loss=0.032912999391555786
I0205 08:31:23.859571 139805680953088 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.08227077126502991, loss=0.03064672462642193
I0205 08:31:55.215053 139804984182528 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.12261055409908295, loss=0.029062403365969658
I0205 08:32:26.974217 139805680953088 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.10019470751285553, loss=0.03221387788653374
I0205 08:32:58.633027 139804984182528 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.10684150457382202, loss=0.035421472042798996
I0205 08:33:30.337020 139805680953088 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.09535568952560425, loss=0.032245565205812454
I0205 08:34:01.966347 139804984182528 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.08089996129274368, loss=0.03302923962473869
I0205 08:34:07.122481 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:36:06.268142 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:36:09.326587 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:36:12.301906 139978932307776 submission_runner.py:408] Time since start: 17232.82s, 	Step: 34617, 	{'train/accuracy': 0.9917721152305603, 'train/loss': 0.02674318104982376, 'train/mean_average_precision': 0.49386078200373046, 'validation/accuracy': 0.9869108200073242, 'validation/loss': 0.0445934534072876, 'validation/mean_average_precision': 0.2638629801469858, 'validation/num_examples': 43793, 'test/accuracy': 0.9859358668327332, 'test/loss': 0.047608476132154465, 'test/mean_average_precision': 0.2522764802677962, 'test/num_examples': 43793, 'score': 11068.133890151978, 'total_duration': 17232.824068784714, 'accumulated_submission_time': 11068.133890151978, 'accumulated_eval_time': 6162.1058077812195, 'accumulated_logging_time': 1.6727240085601807}
I0205 08:36:12.323742 139804644198144 logging_writer.py:48] [34617] accumulated_eval_time=6162.105808, accumulated_logging_time=1.672724, accumulated_submission_time=11068.133890, global_step=34617, preemption_count=0, score=11068.133890, test/accuracy=0.985936, test/loss=0.047608, test/mean_average_precision=0.252276, test/num_examples=43793, total_duration=17232.824069, train/accuracy=0.991772, train/loss=0.026743, train/mean_average_precision=0.493861, validation/accuracy=0.986911, validation/loss=0.044593, validation/mean_average_precision=0.263863, validation/num_examples=43793
I0205 08:36:38.302366 139806663034624 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.09463804215192795, loss=0.033071279525756836
I0205 08:37:09.926250 139804644198144 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.09519344568252563, loss=0.033546559512615204
I0205 08:37:41.878613 139806663034624 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.08349893242120743, loss=0.03228534013032913
I0205 08:38:13.886259 139804644198144 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0943077802658081, loss=0.03215030953288078
I0205 08:38:45.803399 139806663034624 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.08636994659900665, loss=0.033713988959789276
I0205 08:39:17.937619 139804644198144 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.0964922308921814, loss=0.030246030539274216
I0205 08:39:49.531472 139806663034624 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.07498389482498169, loss=0.03142425790429115
I0205 08:40:12.373948 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:42:12.076646 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:42:15.059818 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:42:18.005937 139978932307776 submission_runner.py:408] Time since start: 17598.53s, 	Step: 35373, 	{'train/accuracy': 0.9918096661567688, 'train/loss': 0.026530418545007706, 'train/mean_average_precision': 0.5029054677780674, 'validation/accuracy': 0.9868081212043762, 'validation/loss': 0.0446687713265419, 'validation/mean_average_precision': 0.2637214181806488, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04744245111942291, 'test/mean_average_precision': 0.2559087693871861, 'test/num_examples': 43793, 'score': 11308.152801513672, 'total_duration': 17598.52809739113, 'accumulated_submission_time': 11308.152801513672, 'accumulated_eval_time': 6287.737751483917, 'accumulated_logging_time': 1.7055885791778564}
I0205 08:42:18.030067 139805009491712 logging_writer.py:48] [35373] accumulated_eval_time=6287.737751, accumulated_logging_time=1.705589, accumulated_submission_time=11308.152802, global_step=35373, preemption_count=0, score=11308.152802, test/accuracy=0.985943, test/loss=0.047442, test/mean_average_precision=0.255909, test/num_examples=43793, total_duration=17598.528097, train/accuracy=0.991810, train/loss=0.026530, train/mean_average_precision=0.502905, validation/accuracy=0.986808, validation/loss=0.044669, validation/mean_average_precision=0.263721, validation/num_examples=43793
I0205 08:42:27.112842 139805680953088 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.08041704446077347, loss=0.03016624040901661
I0205 08:42:58.732592 139805009491712 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.10874491184949875, loss=0.03278829902410507
I0205 08:43:30.507938 139805680953088 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.07193248718976974, loss=0.029597220942378044
I0205 08:44:02.229113 139805009491712 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.07639655470848083, loss=0.031721893697977066
I0205 08:44:33.796146 139805680953088 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.09599994868040085, loss=0.03509281575679779
I0205 08:45:05.401246 139805009491712 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.11677366495132446, loss=0.032603632658720016
I0205 08:45:37.252523 139805680953088 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.10525485128164291, loss=0.03271770849823952
I0205 08:46:08.923679 139805009491712 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.0925721600651741, loss=0.030959373340010643
I0205 08:46:18.188720 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:48:17.993569 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:48:20.979928 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:48:23.971554 139978932307776 submission_runner.py:408] Time since start: 17964.49s, 	Step: 36130, 	{'train/accuracy': 0.9921783208847046, 'train/loss': 0.025464337319135666, 'train/mean_average_precision': 0.5405584099723635, 'validation/accuracy': 0.9869144558906555, 'validation/loss': 0.04474916309118271, 'validation/mean_average_precision': 0.2696709121978386, 'validation/num_examples': 43793, 'test/accuracy': 0.9859346151351929, 'test/loss': 0.04761963337659836, 'test/mean_average_precision': 0.254952374996752, 'test/num_examples': 43793, 'score': 11548.278539657593, 'total_duration': 17964.49371266365, 'accumulated_submission_time': 11548.278539657593, 'accumulated_eval_time': 6413.520535945892, 'accumulated_logging_time': 1.7420799732208252}
I0205 08:48:23.994562 139804644198144 logging_writer.py:48] [36130] accumulated_eval_time=6413.520536, accumulated_logging_time=1.742080, accumulated_submission_time=11548.278540, global_step=36130, preemption_count=0, score=11548.278540, test/accuracy=0.985935, test/loss=0.047620, test/mean_average_precision=0.254952, test/num_examples=43793, total_duration=17964.493713, train/accuracy=0.992178, train/loss=0.025464, train/mean_average_precision=0.540558, validation/accuracy=0.986914, validation/loss=0.044749, validation/mean_average_precision=0.269671, validation/num_examples=43793
I0205 08:48:46.876467 139806663034624 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.0844535082578659, loss=0.03222775459289551
I0205 08:49:18.739064 139804644198144 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.07872665673494339, loss=0.030828766524791718
I0205 08:49:50.513928 139806663034624 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.0892534852027893, loss=0.032023873180150986
I0205 08:50:22.578831 139804644198144 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.09096959233283997, loss=0.033576320856809616
I0205 08:50:54.582494 139806663034624 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0966995358467102, loss=0.03151698037981987
I0205 08:51:26.572946 139804644198144 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.10083206743001938, loss=0.03111443482339382
I0205 08:51:58.430370 139806663034624 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.09159985929727554, loss=0.0324406735599041
I0205 08:52:24.105674 139978932307776 spec.py:321] Evaluating on the training split.
I0205 08:54:25.766414 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 08:54:28.812334 139978932307776 spec.py:349] Evaluating on the test split.
I0205 08:54:31.793265 139978932307776 submission_runner.py:408] Time since start: 18332.32s, 	Step: 36881, 	{'train/accuracy': 0.9921891689300537, 'train/loss': 0.025626953691244125, 'train/mean_average_precision': 0.5054668073603061, 'validation/accuracy': 0.9867618083953857, 'validation/loss': 0.04484332352876663, 'validation/mean_average_precision': 0.2666542209544567, 'validation/num_examples': 43793, 'test/accuracy': 0.9858899712562561, 'test/loss': 0.04770039767026901, 'test/mean_average_precision': 0.25322437256657715, 'test/num_examples': 43793, 'score': 11788.357815027237, 'total_duration': 18332.315428972244, 'accumulated_submission_time': 11788.357815027237, 'accumulated_eval_time': 6541.208086490631, 'accumulated_logging_time': 1.7762196063995361}
I0205 08:54:31.815946 139805009491712 logging_writer.py:48] [36881] accumulated_eval_time=6541.208086, accumulated_logging_time=1.776220, accumulated_submission_time=11788.357815, global_step=36881, preemption_count=0, score=11788.357815, test/accuracy=0.985890, test/loss=0.047700, test/mean_average_precision=0.253224, test/num_examples=43793, total_duration=18332.315429, train/accuracy=0.992189, train/loss=0.025627, train/mean_average_precision=0.505467, validation/accuracy=0.986762, validation/loss=0.044843, validation/mean_average_precision=0.266654, validation/num_examples=43793
I0205 08:54:38.188711 139805680953088 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.09031686931848526, loss=0.03429799899458885
I0205 08:55:10.246454 139805009491712 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.09512336552143097, loss=0.030724365264177322
I0205 08:55:41.950079 139805680953088 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.09276752173900604, loss=0.030553516000509262
I0205 08:56:13.951016 139805009491712 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.08336511999368668, loss=0.028607305139303207
I0205 08:56:45.869029 139805680953088 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.09488111734390259, loss=0.03388909250497818
I0205 08:57:17.869313 139805009491712 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.0978899896144867, loss=0.03257640451192856
I0205 08:57:50.097194 139805680953088 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09594562649726868, loss=0.02984580397605896
I0205 08:58:22.121612 139805009491712 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.09227175265550613, loss=0.034058813005685806
I0205 08:58:31.810033 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:00:33.620777 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:00:36.681343 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:00:39.671319 139978932307776 submission_runner.py:408] Time since start: 18700.19s, 	Step: 37631, 	{'train/accuracy': 0.9919636249542236, 'train/loss': 0.02628188394010067, 'train/mean_average_precision': 0.5000979311796215, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044515348970890045, 'validation/mean_average_precision': 0.26927634371474507, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.04737162962555885, 'test/mean_average_precision': 0.2548070281969129, 'test/num_examples': 43793, 'score': 12028.320405721664, 'total_duration': 18700.193468809128, 'accumulated_submission_time': 12028.320405721664, 'accumulated_eval_time': 6669.069313287735, 'accumulated_logging_time': 1.8099312782287598}
I0205 09:00:39.695722 139804644198144 logging_writer.py:48] [37631] accumulated_eval_time=6669.069313, accumulated_logging_time=1.809931, accumulated_submission_time=12028.320406, global_step=37631, preemption_count=0, score=12028.320406, test/accuracy=0.985850, test/loss=0.047372, test/mean_average_precision=0.254807, test/num_examples=43793, total_duration=18700.193469, train/accuracy=0.991964, train/loss=0.026282, train/mean_average_precision=0.500098, validation/accuracy=0.986765, validation/loss=0.044515, validation/mean_average_precision=0.269276, validation/num_examples=43793
I0205 09:01:02.203725 139806663034624 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.08852344751358032, loss=0.031368568539619446
I0205 09:01:34.266368 139804644198144 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.08682548254728317, loss=0.03224600851535797
I0205 09:02:06.382973 139806663034624 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.08746930956840515, loss=0.0318915955722332
I0205 09:02:38.215186 139804644198144 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.08283442258834839, loss=0.030076999217271805
I0205 09:03:10.694150 139806663034624 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09643421322107315, loss=0.03289826214313507
I0205 09:03:42.999687 139804644198144 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.0849427655339241, loss=0.030841195955872536
I0205 09:04:15.183714 139806663034624 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.12101680040359497, loss=0.0323176234960556
I0205 09:04:39.812634 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:06:37.346073 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:06:40.347474 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:06:43.304529 139978932307776 submission_runner.py:408] Time since start: 19063.83s, 	Step: 38379, 	{'train/accuracy': 0.9917044639587402, 'train/loss': 0.026799645274877548, 'train/mean_average_precision': 0.49388362588191836, 'validation/accuracy': 0.986860454082489, 'validation/loss': 0.045009173452854156, 'validation/mean_average_precision': 0.2717148167414828, 'validation/num_examples': 43793, 'test/accuracy': 0.9860424399375916, 'test/loss': 0.0478706955909729, 'test/mean_average_precision': 0.25797355187363524, 'test/num_examples': 43793, 'score': 12268.405726671219, 'total_duration': 19063.826691389084, 'accumulated_submission_time': 12268.405726671219, 'accumulated_eval_time': 6792.561163425446, 'accumulated_logging_time': 1.8454234600067139}
I0205 09:06:43.327349 139805009491712 logging_writer.py:48] [38379] accumulated_eval_time=6792.561163, accumulated_logging_time=1.845423, accumulated_submission_time=12268.405727, global_step=38379, preemption_count=0, score=12268.405727, test/accuracy=0.986042, test/loss=0.047871, test/mean_average_precision=0.257974, test/num_examples=43793, total_duration=19063.826691, train/accuracy=0.991704, train/loss=0.026800, train/mean_average_precision=0.493884, validation/accuracy=0.986860, validation/loss=0.045009, validation/mean_average_precision=0.271715, validation/num_examples=43793
I0205 09:06:50.514062 139805680953088 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.08967451006174088, loss=0.031563445925712585
I0205 09:07:22.921432 139805009491712 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.11261773854494095, loss=0.033893734216690063
I0205 09:07:54.733607 139805680953088 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.09173951297998428, loss=0.03287827596068382
I0205 09:08:27.159187 139805009491712 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.08611217886209488, loss=0.028460169211030006
I0205 09:08:59.387566 139805680953088 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.09662739187479019, loss=0.030735984444618225
I0205 09:09:31.068827 139805009491712 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.10106092691421509, loss=0.029335888102650642
I0205 09:10:03.295599 139805680953088 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.10576830059289932, loss=0.03238601237535477
I0205 09:10:35.152558 139805009491712 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.08984080702066422, loss=0.029872335493564606
I0205 09:10:43.462295 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:12:43.637409 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:12:46.707324 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:12:49.704820 139978932307776 submission_runner.py:408] Time since start: 19430.23s, 	Step: 39127, 	{'train/accuracy': 0.9917911887168884, 'train/loss': 0.026672180742025375, 'train/mean_average_precision': 0.48954401239902784, 'validation/accuracy': 0.9869790077209473, 'validation/loss': 0.044631291180849075, 'validation/mean_average_precision': 0.2678226921038907, 'validation/num_examples': 43793, 'test/accuracy': 0.9860386252403259, 'test/loss': 0.04738719388842583, 'test/mean_average_precision': 0.2648304309454832, 'test/num_examples': 43793, 'score': 12508.509371519089, 'total_duration': 19430.22696685791, 'accumulated_submission_time': 12508.509371519089, 'accumulated_eval_time': 6918.80362701416, 'accumulated_logging_time': 1.879314661026001}
I0205 09:12:49.727932 139804644198144 logging_writer.py:48] [39127] accumulated_eval_time=6918.803627, accumulated_logging_time=1.879315, accumulated_submission_time=12508.509372, global_step=39127, preemption_count=0, score=12508.509372, test/accuracy=0.986039, test/loss=0.047387, test/mean_average_precision=0.264830, test/num_examples=43793, total_duration=19430.226967, train/accuracy=0.991791, train/loss=0.026672, train/mean_average_precision=0.489544, validation/accuracy=0.986979, validation/loss=0.044631, validation/mean_average_precision=0.267823, validation/num_examples=43793
I0205 09:13:13.549283 139804984182528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.1072615310549736, loss=0.030471740290522575
I0205 09:13:45.413112 139804644198144 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.0924009159207344, loss=0.030106177553534508
I0205 09:14:17.135434 139804984182528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.09390860795974731, loss=0.03263895958662033
I0205 09:14:48.785605 139804644198144 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.10437051951885223, loss=0.03255597874522209
I0205 09:15:20.577724 139804984182528 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.08212583512067795, loss=0.029981352388858795
I0205 09:15:51.885545 139804644198144 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07064848393201828, loss=0.0289172176271677
I0205 09:16:23.399346 139804984182528 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.09057234227657318, loss=0.03294560685753822
I0205 09:16:49.818252 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:18:44.554544 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:18:47.680952 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:18:50.675114 139978932307776 submission_runner.py:408] Time since start: 19791.20s, 	Step: 39885, 	{'train/accuracy': 0.9919468760490417, 'train/loss': 0.02608482353389263, 'train/mean_average_precision': 0.5204706545550591, 'validation/accuracy': 0.9870122671127319, 'validation/loss': 0.04427463188767433, 'validation/mean_average_precision': 0.2786177764963892, 'validation/num_examples': 43793, 'test/accuracy': 0.9861629009246826, 'test/loss': 0.04734284058213234, 'test/mean_average_precision': 0.26225317433379713, 'test/num_examples': 43793, 'score': 12748.566566467285, 'total_duration': 19791.197275161743, 'accumulated_submission_time': 12748.566566467285, 'accumulated_eval_time': 7039.660442113876, 'accumulated_logging_time': 1.91489839553833}
I0205 09:18:50.698843 139805680953088 logging_writer.py:48] [39885] accumulated_eval_time=7039.660442, accumulated_logging_time=1.914898, accumulated_submission_time=12748.566566, global_step=39885, preemption_count=0, score=12748.566566, test/accuracy=0.986163, test/loss=0.047343, test/mean_average_precision=0.262253, test/num_examples=43793, total_duration=19791.197275, train/accuracy=0.991947, train/loss=0.026085, train/mean_average_precision=0.520471, validation/accuracy=0.987012, validation/loss=0.044275, validation/mean_average_precision=0.278618, validation/num_examples=43793
I0205 09:18:55.833264 139806663034624 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.09485343843698502, loss=0.02995864301919937
I0205 09:19:27.702296 139805680953088 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.11570867896080017, loss=0.03566431254148483
I0205 09:19:59.464795 139806663034624 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.08460532873868942, loss=0.0295399259775877
I0205 09:20:31.320186 139805680953088 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.09999033808708191, loss=0.030670631676912308
I0205 09:21:03.239542 139806663034624 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.09511584043502808, loss=0.029736962169408798
I0205 09:21:34.885744 139805680953088 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.09902124851942062, loss=0.030394047498703003
I0205 09:22:06.759662 139806663034624 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.09636814147233963, loss=0.02958657406270504
I0205 09:22:38.588386 139805680953088 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.09303227066993713, loss=0.02872026525437832
I0205 09:22:50.700078 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:24:53.143292 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:24:56.166813 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:24:59.156917 139978932307776 submission_runner.py:408] Time since start: 20159.68s, 	Step: 40639, 	{'train/accuracy': 0.9919112324714661, 'train/loss': 0.026074526831507683, 'train/mean_average_precision': 0.5069046204867362, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.04517439007759094, 'validation/mean_average_precision': 0.2680662528745569, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.04800134524703026, 'test/mean_average_precision': 0.25675443142912335, 'test/num_examples': 43793, 'score': 12988.536489725113, 'total_duration': 20159.67905855179, 'accumulated_submission_time': 12988.536489725113, 'accumulated_eval_time': 7168.117209196091, 'accumulated_logging_time': 1.9494268894195557}
I0205 09:24:59.180506 139804984182528 logging_writer.py:48] [40639] accumulated_eval_time=7168.117209, accumulated_logging_time=1.949427, accumulated_submission_time=12988.536490, global_step=40639, preemption_count=0, score=12988.536490, test/accuracy=0.985961, test/loss=0.048001, test/mean_average_precision=0.256754, test/num_examples=43793, total_duration=20159.679059, train/accuracy=0.991911, train/loss=0.026075, train/mean_average_precision=0.506905, validation/accuracy=0.986781, validation/loss=0.045174, validation/mean_average_precision=0.268066, validation/num_examples=43793
I0205 09:25:18.948552 139805009491712 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.09783454239368439, loss=0.029476972296833992
I0205 09:25:50.692547 139804984182528 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.12614919245243073, loss=0.030581876635551453
I0205 09:26:22.335149 139805009491712 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08784876763820648, loss=0.03071012534201145
I0205 09:26:53.846944 139804984182528 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.09784449636936188, loss=0.031924065202474594
I0205 09:27:25.381084 139805009491712 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.09972532093524933, loss=0.03024240955710411
I0205 09:27:56.845084 139804984182528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.08756405860185623, loss=0.029258955270051956
I0205 09:28:28.478720 139805009491712 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.10254096984863281, loss=0.027926292270421982
I0205 09:28:59.321982 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:30:57.035997 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:31:00.050971 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:31:03.098606 139978932307776 submission_runner.py:408] Time since start: 20523.62s, 	Step: 41398, 	{'train/accuracy': 0.9920253753662109, 'train/loss': 0.025676319375634193, 'train/mean_average_precision': 0.508227948274832, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.04517970606684685, 'validation/mean_average_precision': 0.27326760721328275, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.047962479293346405, 'test/mean_average_precision': 0.26566777619756393, 'test/num_examples': 43793, 'score': 13228.646829366684, 'total_duration': 20523.62076807022, 'accumulated_submission_time': 13228.646829366684, 'accumulated_eval_time': 7291.893787145615, 'accumulated_logging_time': 1.9835777282714844}
I0205 09:31:03.121743 139804644198144 logging_writer.py:48] [41398] accumulated_eval_time=7291.893787, accumulated_logging_time=1.983578, accumulated_submission_time=13228.646829, global_step=41398, preemption_count=0, score=13228.646829, test/accuracy=0.985961, test/loss=0.047962, test/mean_average_precision=0.265668, test/num_examples=43793, total_duration=20523.620768, train/accuracy=0.992025, train/loss=0.025676, train/mean_average_precision=0.508228, validation/accuracy=0.986888, validation/loss=0.045180, validation/mean_average_precision=0.273268, validation/num_examples=43793
I0205 09:31:04.082754 139806663034624 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.09315229952335358, loss=0.02832825854420662
I0205 09:31:35.714901 139804644198144 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.10825098305940628, loss=0.03421255201101303
I0205 09:32:07.671825 139806663034624 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.10538145899772644, loss=0.03149934485554695
I0205 09:32:39.556424 139804644198144 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.08244450390338898, loss=0.02843203954398632
I0205 09:33:11.061969 139806663034624 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08741234242916107, loss=0.029350873082876205
I0205 09:33:42.630796 139804644198144 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0910053476691246, loss=0.02807803265750408
I0205 09:34:14.594468 139806663034624 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07933398336172104, loss=0.029013385996222496
I0205 09:34:46.207372 139804644198144 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.10502156615257263, loss=0.028547728434205055
I0205 09:35:03.237209 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:36:59.419300 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:37:02.488966 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:37:05.459156 139978932307776 submission_runner.py:408] Time since start: 20885.98s, 	Step: 42155, 	{'train/accuracy': 0.9922612905502319, 'train/loss': 0.024819236248731613, 'train/mean_average_precision': 0.5357011778114732, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.0453098826110363, 'validation/mean_average_precision': 0.26412438235599994, 'validation/num_examples': 43793, 'test/accuracy': 0.9859897494316101, 'test/loss': 0.04822394996881485, 'test/mean_average_precision': 0.25749466873757293, 'test/num_examples': 43793, 'score': 13468.731812000275, 'total_duration': 20885.98131942749, 'accumulated_submission_time': 13468.731812000275, 'accumulated_eval_time': 7414.11568903923, 'accumulated_logging_time': 2.017220973968506}
I0205 09:37:05.482643 139804984182528 logging_writer.py:48] [42155] accumulated_eval_time=7414.115689, accumulated_logging_time=2.017221, accumulated_submission_time=13468.731812, global_step=42155, preemption_count=0, score=13468.731812, test/accuracy=0.985990, test/loss=0.048224, test/mean_average_precision=0.257495, test/num_examples=43793, total_duration=20885.981319, train/accuracy=0.992261, train/loss=0.024819, train/mean_average_precision=0.535701, validation/accuracy=0.986861, validation/loss=0.045310, validation/mean_average_precision=0.264124, validation/num_examples=43793
I0205 09:37:20.368663 139805009491712 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1269519031047821, loss=0.0302285049110651
I0205 09:37:52.262794 139804984182528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.11883537471294403, loss=0.031776100397109985
I0205 09:38:23.893751 139805009491712 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09781917929649353, loss=0.030287791043519974
I0205 09:38:55.617065 139804984182528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.11196132004261017, loss=0.030200328677892685
I0205 09:39:27.487370 139805009491712 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.11049021035432816, loss=0.03073876164853573
I0205 09:39:59.202226 139804984182528 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.10763800144195557, loss=0.03016471303999424
I0205 09:40:31.303041 139805009491712 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.10298660397529602, loss=0.02938878908753395
I0205 09:41:03.371324 139804984182528 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.09890954196453094, loss=0.028041936457157135
I0205 09:41:05.547046 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:43:04.011273 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:43:07.046657 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:43:09.997018 139978932307776 submission_runner.py:408] Time since start: 21250.52s, 	Step: 42908, 	{'train/accuracy': 0.9926353096961975, 'train/loss': 0.02383463643491268, 'train/mean_average_precision': 0.5570144195376461, 'validation/accuracy': 0.986894965171814, 'validation/loss': 0.04515489563345909, 'validation/mean_average_precision': 0.26848816192137337, 'validation/num_examples': 43793, 'test/accuracy': 0.9860158562660217, 'test/loss': 0.04806399717926979, 'test/mean_average_precision': 0.26533230383810813, 'test/num_examples': 43793, 'score': 13708.764877796173, 'total_duration': 21250.519181251526, 'accumulated_submission_time': 13708.764877796173, 'accumulated_eval_time': 7538.565611362457, 'accumulated_logging_time': 2.051811456680298}
I0205 09:43:10.020634 139805680953088 logging_writer.py:48] [42908] accumulated_eval_time=7538.565611, accumulated_logging_time=2.051811, accumulated_submission_time=13708.764878, global_step=42908, preemption_count=0, score=13708.764878, test/accuracy=0.986016, test/loss=0.048064, test/mean_average_precision=0.265332, test/num_examples=43793, total_duration=21250.519181, train/accuracy=0.992635, train/loss=0.023835, train/mean_average_precision=0.557014, validation/accuracy=0.986895, validation/loss=0.045155, validation/mean_average_precision=0.268488, validation/num_examples=43793
I0205 09:43:39.622495 139806663034624 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.11789422482252121, loss=0.028441572561860085
I0205 09:44:11.356295 139805680953088 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.10186047852039337, loss=0.03167741000652313
I0205 09:44:43.077384 139806663034624 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.11433475464582443, loss=0.029901856556534767
I0205 09:45:14.820530 139805680953088 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.10793397575616837, loss=0.030420251190662384
I0205 09:45:46.455155 139806663034624 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.10693308711051941, loss=0.03207520768046379
I0205 09:46:18.298937 139805680953088 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.09412413835525513, loss=0.029719389975070953
I0205 09:46:49.819355 139806663034624 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.10663264989852905, loss=0.031191742047667503
I0205 09:47:10.074273 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:49:09.959346 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:49:12.989582 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:49:16.004767 139978932307776 submission_runner.py:408] Time since start: 21616.53s, 	Step: 43665, 	{'train/accuracy': 0.9925816655158997, 'train/loss': 0.023902637884020805, 'train/mean_average_precision': 0.569847598106749, 'validation/accuracy': 0.9868986010551453, 'validation/loss': 0.045283351093530655, 'validation/mean_average_precision': 0.2717307550178367, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04846496880054474, 'test/mean_average_precision': 0.2619368512501648, 'test/num_examples': 43793, 'score': 13948.78694844246, 'total_duration': 21616.52692937851, 'accumulated_submission_time': 13948.78694844246, 'accumulated_eval_time': 7664.496058940887, 'accumulated_logging_time': 2.0864064693450928}
I0205 09:49:16.028541 139804644198144 logging_writer.py:48] [43665] accumulated_eval_time=7664.496059, accumulated_logging_time=2.086406, accumulated_submission_time=13948.786948, global_step=43665, preemption_count=0, score=13948.786948, test/accuracy=0.985981, test/loss=0.048465, test/mean_average_precision=0.261937, test/num_examples=43793, total_duration=21616.526929, train/accuracy=0.992582, train/loss=0.023903, train/mean_average_precision=0.569848, validation/accuracy=0.986899, validation/loss=0.045283, validation/mean_average_precision=0.271731, validation/num_examples=43793
I0205 09:49:27.519863 139804984182528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.10259214788675308, loss=0.031144224107265472
I0205 09:49:59.261188 139804644198144 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.11965283751487732, loss=0.030288582667708397
I0205 09:50:31.176096 139804984182528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.0957261323928833, loss=0.028940359130501747
I0205 09:51:02.957222 139804644198144 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1038396954536438, loss=0.028820078819990158
I0205 09:51:34.832729 139804984182528 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.12321284413337708, loss=0.02892974205315113
I0205 09:52:06.788193 139804644198144 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.10507713258266449, loss=0.030402056872844696
I0205 09:52:38.926076 139804984182528 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.12222419679164886, loss=0.032345641404390335
I0205 09:53:10.996307 139804644198144 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1241036131978035, loss=0.026829469949007034
I0205 09:53:16.049514 139978932307776 spec.py:321] Evaluating on the training split.
I0205 09:55:18.579413 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 09:55:21.673095 139978932307776 spec.py:349] Evaluating on the test split.
I0205 09:55:24.711923 139978932307776 submission_runner.py:408] Time since start: 21985.23s, 	Step: 44417, 	{'train/accuracy': 0.9929485321044922, 'train/loss': 0.022788044065237045, 'train/mean_average_precision': 0.5749294484192354, 'validation/accuracy': 0.9868471026420593, 'validation/loss': 0.045269984751939774, 'validation/mean_average_precision': 0.27262849192091826, 'validation/num_examples': 43793, 'test/accuracy': 0.9859729409217834, 'test/loss': 0.0484565906226635, 'test/mean_average_precision': 0.25695242147863345, 'test/num_examples': 43793, 'score': 14188.775550365448, 'total_duration': 21985.23408293724, 'accumulated_submission_time': 14188.775550365448, 'accumulated_eval_time': 7793.158420085907, 'accumulated_logging_time': 2.121481418609619}
I0205 09:55:24.736901 139805680953088 logging_writer.py:48] [44417] accumulated_eval_time=7793.158420, accumulated_logging_time=2.121481, accumulated_submission_time=14188.775550, global_step=44417, preemption_count=0, score=14188.775550, test/accuracy=0.985973, test/loss=0.048457, test/mean_average_precision=0.256952, test/num_examples=43793, total_duration=21985.234083, train/accuracy=0.992949, train/loss=0.022788, train/mean_average_precision=0.574929, validation/accuracy=0.986847, validation/loss=0.045270, validation/mean_average_precision=0.272628, validation/num_examples=43793
I0205 09:55:51.713106 139806663034624 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.09335849434137344, loss=0.028818106278777122
I0205 09:56:23.745828 139805680953088 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1085338443517685, loss=0.03156373277306557
I0205 09:56:55.950999 139806663034624 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.10190971195697784, loss=0.027554597705602646
I0205 09:57:28.180154 139805680953088 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.11454326659440994, loss=0.029757846146821976
I0205 09:58:00.201957 139806663034624 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1432984471321106, loss=0.03077191859483719
I0205 09:58:32.491542 139805680953088 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.12356748431921005, loss=0.028536612167954445
I0205 09:59:04.687325 139806663034624 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.12866288423538208, loss=0.029647624120116234
I0205 09:59:24.955764 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:01:22.947186 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:01:25.975693 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:01:29.064832 139978932307776 submission_runner.py:408] Time since start: 22349.59s, 	Step: 45164, 	{'train/accuracy': 0.9925280213356018, 'train/loss': 0.024165211245417595, 'train/mean_average_precision': 0.5439482996360956, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.045505501329898834, 'validation/mean_average_precision': 0.27256189700011024, 'validation/num_examples': 43793, 'test/accuracy': 0.985913097858429, 'test/loss': 0.04859090596437454, 'test/mean_average_precision': 0.25627907897947927, 'test/num_examples': 43793, 'score': 14428.960973501205, 'total_duration': 22349.58697938919, 'accumulated_submission_time': 14428.960973501205, 'accumulated_eval_time': 7917.267426967621, 'accumulated_logging_time': 2.1592814922332764}
I0205 10:01:29.089205 139804984182528 logging_writer.py:48] [45164] accumulated_eval_time=7917.267427, accumulated_logging_time=2.159281, accumulated_submission_time=14428.960974, global_step=45164, preemption_count=0, score=14428.960974, test/accuracy=0.985913, test/loss=0.048591, test/mean_average_precision=0.256279, test/num_examples=43793, total_duration=22349.586979, train/accuracy=0.992528, train/loss=0.024165, train/mean_average_precision=0.543948, validation/accuracy=0.986795, validation/loss=0.045506, validation/mean_average_precision=0.272562, validation/num_examples=43793
I0205 10:01:41.137548 139805009491712 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.12317775189876556, loss=0.029521603137254715
I0205 10:02:13.410827 139804984182528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.11308669298887253, loss=0.027217824012041092
I0205 10:02:45.923934 139805009491712 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.10220862925052643, loss=0.02774377167224884
I0205 10:03:18.088752 139804984182528 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.10118784010410309, loss=0.027610722929239273
I0205 10:03:50.215730 139805009491712 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.10691255331039429, loss=0.029038310050964355
I0205 10:04:22.219901 139804984182528 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.12650248408317566, loss=0.029453808441758156
I0205 10:04:54.291996 139805009491712 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1165347620844841, loss=0.027632523328065872
I0205 10:05:26.515789 139804984182528 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.10512281209230423, loss=0.02877664752304554
I0205 10:05:29.359914 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:07:28.948925 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:07:31.979254 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:07:34.956739 139978932307776 submission_runner.py:408] Time since start: 22715.48s, 	Step: 45910, 	{'train/accuracy': 0.9924951195716858, 'train/loss': 0.024184413254261017, 'train/mean_average_precision': 0.5322754362561134, 'validation/accuracy': 0.9868247509002686, 'validation/loss': 0.045396801084280014, 'validation/mean_average_precision': 0.26942070758583075, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04840182512998581, 'test/mean_average_precision': 0.25797832627777517, 'test/num_examples': 43793, 'score': 14669.197919368744, 'total_duration': 22715.47890305519, 'accumulated_submission_time': 14669.197919368744, 'accumulated_eval_time': 8042.86420583725, 'accumulated_logging_time': 2.1964058876037598}
I0205 10:07:34.981384 139804644198144 logging_writer.py:48] [45910] accumulated_eval_time=8042.864206, accumulated_logging_time=2.196406, accumulated_submission_time=14669.197919, global_step=45910, preemption_count=0, score=14669.197919, test/accuracy=0.985895, test/loss=0.048402, test/mean_average_precision=0.257978, test/num_examples=43793, total_duration=22715.478903, train/accuracy=0.992495, train/loss=0.024184, train/mean_average_precision=0.532275, validation/accuracy=0.986825, validation/loss=0.045397, validation/mean_average_precision=0.269421, validation/num_examples=43793
I0205 10:08:04.291652 139805680953088 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.12623481452465057, loss=0.030972862616181374
I0205 10:08:36.079173 139804644198144 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.14347313344478607, loss=0.028046036139130592
I0205 10:09:08.293264 139805680953088 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.12406197935342789, loss=0.027746127918362617
I0205 10:09:40.547757 139804644198144 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.11261513829231262, loss=0.029012855142354965
I0205 10:10:13.015644 139805680953088 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.130660742521286, loss=0.027800902724266052
I0205 10:10:44.842691 139804644198144 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1108049526810646, loss=0.028868485242128372
I0205 10:11:16.996480 139805680953088 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1437707245349884, loss=0.029408717527985573
I0205 10:11:35.198174 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:13:34.388776 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:13:37.431337 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:13:40.446835 139978932307776 submission_runner.py:408] Time since start: 23080.97s, 	Step: 46658, 	{'train/accuracy': 0.9923391938209534, 'train/loss': 0.024557368829846382, 'train/mean_average_precision': 0.5505022282160861, 'validation/accuracy': 0.9869205355644226, 'validation/loss': 0.045789334923028946, 'validation/mean_average_precision': 0.26658504589910953, 'validation/num_examples': 43793, 'test/accuracy': 0.9860398769378662, 'test/loss': 0.04890056326985359, 'test/mean_average_precision': 0.2569565723669573, 'test/num_examples': 43793, 'score': 14909.381882667542, 'total_duration': 23080.968989133835, 'accumulated_submission_time': 14909.381882667542, 'accumulated_eval_time': 8168.112809181213, 'accumulated_logging_time': 2.2335081100463867}
I0205 10:13:40.471560 139804984182528 logging_writer.py:48] [46658] accumulated_eval_time=8168.112809, accumulated_logging_time=2.233508, accumulated_submission_time=14909.381883, global_step=46658, preemption_count=0, score=14909.381883, test/accuracy=0.986040, test/loss=0.048901, test/mean_average_precision=0.256957, test/num_examples=43793, total_duration=23080.968989, train/accuracy=0.992339, train/loss=0.024557, train/mean_average_precision=0.550502, validation/accuracy=0.986921, validation/loss=0.045789, validation/mean_average_precision=0.266585, validation/num_examples=43793
I0205 10:13:54.331572 139805009491712 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.12119438499212265, loss=0.032028961926698685
I0205 10:14:26.345733 139804984182528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.16352242231369019, loss=0.029585283249616623
I0205 10:14:58.233268 139805009491712 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.13167354464530945, loss=0.031716760247945786
I0205 10:15:30.356716 139804984182528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.10394197702407837, loss=0.026682758703827858
I0205 10:16:02.452792 139805009491712 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.12036214023828506, loss=0.028783492743968964
I0205 10:16:34.134189 139804984182528 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1250990629196167, loss=0.029972970485687256
I0205 10:17:05.959839 139805009491712 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.12815892696380615, loss=0.030969616025686264
I0205 10:17:37.763381 139804984182528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.11713304370641708, loss=0.027627144008874893
I0205 10:17:40.584304 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:19:40.561684 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:19:43.583608 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:19:46.643084 139978932307776 submission_runner.py:408] Time since start: 23447.17s, 	Step: 47410, 	{'train/accuracy': 0.9923987984657288, 'train/loss': 0.024354690685868263, 'train/mean_average_precision': 0.547945267187708, 'validation/accuracy': 0.9869290590286255, 'validation/loss': 0.04555318504571915, 'validation/mean_average_precision': 0.27309230894032954, 'validation/num_examples': 43793, 'test/accuracy': 0.9860478639602661, 'test/loss': 0.04859509691596031, 'test/mean_average_precision': 0.25842061742902916, 'test/num_examples': 43793, 'score': 15149.46195435524, 'total_duration': 23447.16524910927, 'accumulated_submission_time': 15149.46195435524, 'accumulated_eval_time': 8294.17154455185, 'accumulated_logging_time': 2.2706735134124756}
I0205 10:19:46.667768 139805680953088 logging_writer.py:48] [47410] accumulated_eval_time=8294.171545, accumulated_logging_time=2.270674, accumulated_submission_time=15149.461954, global_step=47410, preemption_count=0, score=15149.461954, test/accuracy=0.986048, test/loss=0.048595, test/mean_average_precision=0.258421, test/num_examples=43793, total_duration=23447.165249, train/accuracy=0.992399, train/loss=0.024355, train/mean_average_precision=0.547945, validation/accuracy=0.986929, validation/loss=0.045553, validation/mean_average_precision=0.273092, validation/num_examples=43793
I0205 10:20:16.022573 139806663034624 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.10864318907260895, loss=0.028531350195407867
I0205 10:20:47.941634 139805680953088 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.12509597837924957, loss=0.027682354673743248
I0205 10:21:19.961493 139806663034624 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.1268480271100998, loss=0.029499776661396027
I0205 10:21:51.443202 139805680953088 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.15290899574756622, loss=0.02909313701093197
I0205 10:22:23.490082 139806663034624 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.13367989659309387, loss=0.028117984533309937
I0205 10:22:55.422300 139805680953088 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.13860231637954712, loss=0.02631639689207077
I0205 10:23:27.016544 139806663034624 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.10917069017887115, loss=0.028197919949889183
I0205 10:23:46.928479 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:25:47.258636 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:25:50.309749 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:25:53.308255 139978932307776 submission_runner.py:408] Time since start: 23813.83s, 	Step: 48164, 	{'train/accuracy': 0.9926859736442566, 'train/loss': 0.023512663319706917, 'train/mean_average_precision': 0.5531412566917546, 'validation/accuracy': 0.9868718385696411, 'validation/loss': 0.045387301594018936, 'validation/mean_average_precision': 0.2729452931736282, 'validation/num_examples': 43793, 'test/accuracy': 0.9860175848007202, 'test/loss': 0.04826309159398079, 'test/mean_average_precision': 0.2613342734660124, 'test/num_examples': 43793, 'score': 15389.69013428688, 'total_duration': 23813.830407857895, 'accumulated_submission_time': 15389.69013428688, 'accumulated_eval_time': 8420.551263570786, 'accumulated_logging_time': 2.3075530529022217}
I0205 10:25:53.338927 139804644198144 logging_writer.py:48] [48164] accumulated_eval_time=8420.551264, accumulated_logging_time=2.307553, accumulated_submission_time=15389.690134, global_step=48164, preemption_count=0, score=15389.690134, test/accuracy=0.986018, test/loss=0.048263, test/mean_average_precision=0.261334, test/num_examples=43793, total_duration=23813.830408, train/accuracy=0.992686, train/loss=0.023513, train/mean_average_precision=0.553141, validation/accuracy=0.986872, validation/loss=0.045387, validation/mean_average_precision=0.272945, validation/num_examples=43793
I0205 10:26:05.222872 139805009491712 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.11457330733537674, loss=0.026731722056865692
I0205 10:26:36.595991 139804644198144 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.11483268439769745, loss=0.025079112499952316
I0205 10:27:07.961840 139805009491712 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.1264144778251648, loss=0.027979280799627304
I0205 10:27:39.722898 139804644198144 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.12307426333427429, loss=0.03139452263712883
I0205 10:28:11.807168 139805009491712 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.12660931050777435, loss=0.027703475207090378
I0205 10:28:43.157636 139804644198144 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.12933026254177094, loss=0.02822243794798851
I0205 10:29:14.853116 139805009491712 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.13588106632232666, loss=0.029250137507915497
I0205 10:29:46.347442 139804644198144 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.12437772750854492, loss=0.02679128386080265
I0205 10:29:53.309279 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:31:49.876762 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:31:52.903826 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:31:55.906233 139978932307776 submission_runner.py:408] Time since start: 24176.43s, 	Step: 48923, 	{'train/accuracy': 0.9927862286567688, 'train/loss': 0.02313043363392353, 'train/mean_average_precision': 0.5690608496377537, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.04562116786837578, 'validation/mean_average_precision': 0.26960942815402544, 'validation/num_examples': 43793, 'test/accuracy': 0.9859282970428467, 'test/loss': 0.04853885620832443, 'test/mean_average_precision': 0.26179905298209477, 'test/num_examples': 43793, 'score': 15629.627718687057, 'total_duration': 24176.42839407921, 'accumulated_submission_time': 15629.627718687057, 'accumulated_eval_time': 8543.148166894913, 'accumulated_logging_time': 2.350667953491211}
I0205 10:31:55.931799 139804984182528 logging_writer.py:48] [48923] accumulated_eval_time=8543.148167, accumulated_logging_time=2.350668, accumulated_submission_time=15629.627719, global_step=48923, preemption_count=0, score=15629.627719, test/accuracy=0.985928, test/loss=0.048539, test/mean_average_precision=0.261799, test/num_examples=43793, total_duration=24176.428394, train/accuracy=0.992786, train/loss=0.023130, train/mean_average_precision=0.569061, validation/accuracy=0.986802, validation/loss=0.045621, validation/mean_average_precision=0.269609, validation/num_examples=43793
I0205 10:32:21.095900 139805680953088 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.1184837743639946, loss=0.02815205603837967
I0205 10:32:52.739965 139804984182528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.13100451231002808, loss=0.026346558704972267
I0205 10:33:24.857388 139805680953088 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.1276339590549469, loss=0.03006587363779545
I0205 10:33:56.629008 139804984182528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.13059337437152863, loss=0.02770824171602726
I0205 10:34:28.296865 139805680953088 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.12052510678768158, loss=0.02775139920413494
I0205 10:35:00.064562 139804984182528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.13599573075771332, loss=0.026807673275470734
I0205 10:35:31.624643 139805680953088 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.1172088235616684, loss=0.027987681329250336
I0205 10:35:55.922303 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:37:55.308997 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:37:58.366605 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:38:01.430652 139978932307776 submission_runner.py:408] Time since start: 24541.95s, 	Step: 49677, 	{'train/accuracy': 0.9929873943328857, 'train/loss': 0.022309299558401108, 'train/mean_average_precision': 0.5790065918357813, 'validation/accuracy': 0.9868316650390625, 'validation/loss': 0.04610390216112137, 'validation/mean_average_precision': 0.27036258907015986, 'validation/num_examples': 43793, 'test/accuracy': 0.9859762787818909, 'test/loss': 0.04926516115665436, 'test/mean_average_precision': 0.2634007317030291, 'test/num_examples': 43793, 'score': 15869.585559368134, 'total_duration': 24541.952813386917, 'accumulated_submission_time': 15869.585559368134, 'accumulated_eval_time': 8668.656472444534, 'accumulated_logging_time': 2.388645648956299}
I0205 10:38:01.455698 139805009491712 logging_writer.py:48] [49677] accumulated_eval_time=8668.656472, accumulated_logging_time=2.388646, accumulated_submission_time=15869.585559, global_step=49677, preemption_count=0, score=15869.585559, test/accuracy=0.985976, test/loss=0.049265, test/mean_average_precision=0.263401, test/num_examples=43793, total_duration=24541.952813, train/accuracy=0.992987, train/loss=0.022309, train/mean_average_precision=0.579007, validation/accuracy=0.986832, validation/loss=0.046104, validation/mean_average_precision=0.270363, validation/num_examples=43793
I0205 10:38:09.055760 139806663034624 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1432274580001831, loss=0.030527545139193535
I0205 10:38:40.374463 139805009491712 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.12827205657958984, loss=0.028889555484056473
I0205 10:39:12.192902 139806663034624 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1287839561700821, loss=0.02727322094142437
I0205 10:39:43.757334 139805009491712 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.15228703618049622, loss=0.027398288249969482
I0205 10:40:16.092648 139806663034624 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.14454200863838196, loss=0.028470633551478386
I0205 10:40:47.478169 139805009491712 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.12688620388507843, loss=0.025501742959022522
I0205 10:41:19.341555 139806663034624 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.13173268735408783, loss=0.0258798748254776
I0205 10:41:50.949425 139805009491712 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.15081825852394104, loss=0.02616135962307453
I0205 10:42:01.708737 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:44:00.163849 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:44:03.273998 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:44:06.301130 139978932307776 submission_runner.py:408] Time since start: 24906.82s, 	Step: 50435, 	{'train/accuracy': 0.993471086025238, 'train/loss': 0.02111111953854561, 'train/mean_average_precision': 0.6146497697624851, 'validation/accuracy': 0.986735463142395, 'validation/loss': 0.04609406739473343, 'validation/mean_average_precision': 0.2744567062110209, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04910604655742645, 'test/mean_average_precision': 0.2610190774407657, 'test/num_examples': 43793, 'score': 16109.806084394455, 'total_duration': 24906.823295116425, 'accumulated_submission_time': 16109.806084394455, 'accumulated_eval_time': 8793.248821020126, 'accumulated_logging_time': 2.4257941246032715}
I0205 10:44:06.326282 139804644198144 logging_writer.py:48] [50435] accumulated_eval_time=8793.248821, accumulated_logging_time=2.425794, accumulated_submission_time=16109.806084, global_step=50435, preemption_count=0, score=16109.806084, test/accuracy=0.985860, test/loss=0.049106, test/mean_average_precision=0.261019, test/num_examples=43793, total_duration=24906.823295, train/accuracy=0.993471, train/loss=0.021111, train/mean_average_precision=0.614650, validation/accuracy=0.986735, validation/loss=0.046094, validation/mean_average_precision=0.274457, validation/num_examples=43793
I0205 10:44:27.557490 139804984182528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.12392140924930573, loss=0.02704194001853466
I0205 10:44:59.246538 139804644198144 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.13496330380439758, loss=0.02784273773431778
I0205 10:45:31.132268 139804984182528 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.13462528586387634, loss=0.02716083638370037
I0205 10:46:02.902236 139804644198144 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.13533547520637512, loss=0.02837679535150528
I0205 10:46:35.010992 139804984182528 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.14096704125404358, loss=0.02889683097600937
I0205 10:47:07.056625 139804644198144 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.17986252903938293, loss=0.027131466194987297
I0205 10:47:39.071527 139804984182528 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.15059804916381836, loss=0.028204988688230515
I0205 10:48:06.403856 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:50:05.551142 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:50:08.593586 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:50:11.589792 139978932307776 submission_runner.py:408] Time since start: 25272.11s, 	Step: 51187, 	{'train/accuracy': 0.9937585592269897, 'train/loss': 0.020372990518808365, 'train/mean_average_precision': 0.6229511751791663, 'validation/accuracy': 0.9867293238639832, 'validation/loss': 0.04670211300253868, 'validation/mean_average_precision': 0.2689869825711359, 'validation/num_examples': 43793, 'test/accuracy': 0.9857732653617859, 'test/loss': 0.04984183609485626, 'test/mean_average_precision': 0.2565580586568688, 'test/num_examples': 43793, 'score': 16349.852685928345, 'total_duration': 25272.11195421219, 'accumulated_submission_time': 16349.852685928345, 'accumulated_eval_time': 8918.434713840485, 'accumulated_logging_time': 2.461792469024658}
I0205 10:50:11.615666 139805009491712 logging_writer.py:48] [51187] accumulated_eval_time=8918.434714, accumulated_logging_time=2.461792, accumulated_submission_time=16349.852686, global_step=51187, preemption_count=0, score=16349.852686, test/accuracy=0.985773, test/loss=0.049842, test/mean_average_precision=0.256558, test/num_examples=43793, total_duration=25272.111954, train/accuracy=0.993759, train/loss=0.020373, train/mean_average_precision=0.622951, validation/accuracy=0.986729, validation/loss=0.046702, validation/mean_average_precision=0.268987, validation/num_examples=43793
I0205 10:50:16.090533 139805680953088 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.133017897605896, loss=0.02805989235639572
I0205 10:50:47.784794 139805009491712 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1181860864162445, loss=0.026387283578515053
I0205 10:51:19.805893 139805680953088 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.13229809701442719, loss=0.026348907500505447
I0205 10:51:51.540474 139805009491712 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.13810685276985168, loss=0.02713019959628582
I0205 10:52:23.402050 139805680953088 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.1426628828048706, loss=0.029427064582705498
I0205 10:52:54.911599 139805009491712 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.1324985921382904, loss=0.026802562177181244
I0205 10:53:27.022611 139805680953088 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.1414831131696701, loss=0.025686046108603477
I0205 10:53:59.157495 139805009491712 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.14683900773525238, loss=0.029188042506575584
I0205 10:54:11.618035 139978932307776 spec.py:321] Evaluating on the training split.
I0205 10:56:10.649928 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 10:56:13.678661 139978932307776 spec.py:349] Evaluating on the test split.
I0205 10:56:16.708189 139978932307776 submission_runner.py:408] Time since start: 25637.23s, 	Step: 51940, 	{'train/accuracy': 0.9935181140899658, 'train/loss': 0.02072816528379917, 'train/mean_average_precision': 0.6268248896791947, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.04659906029701233, 'validation/mean_average_precision': 0.2722384098665055, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.049767713993787766, 'test/mean_average_precision': 0.2659970159599488, 'test/num_examples': 43793, 'score': 16589.82354283333, 'total_duration': 25637.23035120964, 'accumulated_submission_time': 16589.82354283333, 'accumulated_eval_time': 9043.52482008934, 'accumulated_logging_time': 2.4987967014312744}
I0205 10:56:16.733677 139804984182528 logging_writer.py:48] [51940] accumulated_eval_time=9043.524820, accumulated_logging_time=2.498797, accumulated_submission_time=16589.823543, global_step=51940, preemption_count=0, score=16589.823543, test/accuracy=0.985946, test/loss=0.049768, test/mean_average_precision=0.265997, test/num_examples=43793, total_duration=25637.230351, train/accuracy=0.993518, train/loss=0.020728, train/mean_average_precision=0.626825, validation/accuracy=0.986815, validation/loss=0.046599, validation/mean_average_precision=0.272238, validation/num_examples=43793
I0205 10:56:36.276610 139806663034624 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.13115407526493073, loss=0.026754651218652725
I0205 10:57:08.112282 139804984182528 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.15439245104789734, loss=0.02815099060535431
I0205 10:57:40.282014 139806663034624 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.12748603522777557, loss=0.02568635158240795
I0205 10:58:12.077997 139804984182528 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.13862688839435577, loss=0.026479575783014297
I0205 10:58:44.113165 139806663034624 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.16701774299144745, loss=0.026949983090162277
I0205 10:59:16.070657 139804984182528 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.1349736452102661, loss=0.029472723603248596
I0205 10:59:48.324900 139806663034624 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.14271073043346405, loss=0.02785908430814743
I0205 11:00:16.773034 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:02:17.277940 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:02:20.367440 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:02:23.427048 139978932307776 submission_runner.py:408] Time since start: 26003.95s, 	Step: 52690, 	{'train/accuracy': 0.9936492443084717, 'train/loss': 0.020698348060250282, 'train/mean_average_precision': 0.6196390393205258, 'validation/accuracy': 0.9866599440574646, 'validation/loss': 0.046764276921749115, 'validation/mean_average_precision': 0.2696261098279769, 'validation/num_examples': 43793, 'test/accuracy': 0.9856839776039124, 'test/loss': 0.050197768956422806, 'test/mean_average_precision': 0.25498745121818817, 'test/num_examples': 43793, 'score': 16829.831008911133, 'total_duration': 26003.949209213257, 'accumulated_submission_time': 16829.831008911133, 'accumulated_eval_time': 9170.178788661957, 'accumulated_logging_time': 2.5351483821868896}
I0205 11:02:23.453817 139805009491712 logging_writer.py:48] [52690] accumulated_eval_time=9170.178789, accumulated_logging_time=2.535148, accumulated_submission_time=16829.831009, global_step=52690, preemption_count=0, score=16829.831009, test/accuracy=0.985684, test/loss=0.050198, test/mean_average_precision=0.254987, test/num_examples=43793, total_duration=26003.949209, train/accuracy=0.993649, train/loss=0.020698, train/mean_average_precision=0.619639, validation/accuracy=0.986660, validation/loss=0.046764, validation/mean_average_precision=0.269626, validation/num_examples=43793
I0205 11:02:27.082267 139805680953088 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.16075563430786133, loss=0.025686057284474373
I0205 11:02:58.724254 139805009491712 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1374097764492035, loss=0.02638723887503147
I0205 11:03:29.995153 139805680953088 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.16243301331996918, loss=0.029734129086136818
I0205 11:04:01.499520 139805009491712 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.13878905773162842, loss=0.0261498112231493
I0205 11:04:33.379354 139805680953088 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.15112222731113434, loss=0.027567297220230103
I0205 11:05:05.090489 139805009491712 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.14577218890190125, loss=0.025658151134848595
I0205 11:05:37.100870 139805680953088 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.15731950104236603, loss=0.03171740099787712
I0205 11:06:09.295615 139805009491712 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.15674349665641785, loss=0.025715749710798264
I0205 11:06:23.483359 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:08:21.939157 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:08:24.962838 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:08:28.036366 139978932307776 submission_runner.py:408] Time since start: 26368.56s, 	Step: 53446, 	{'train/accuracy': 0.9932865500450134, 'train/loss': 0.021415717899799347, 'train/mean_average_precision': 0.6066971485592483, 'validation/accuracy': 0.9867967367172241, 'validation/loss': 0.0472288504242897, 'validation/mean_average_precision': 0.2679517868571796, 'validation/num_examples': 43793, 'test/accuracy': 0.9859278798103333, 'test/loss': 0.05037239193916321, 'test/mean_average_precision': 0.25951235150770374, 'test/num_examples': 43793, 'score': 17069.828882217407, 'total_duration': 26368.558528900146, 'accumulated_submission_time': 17069.828882217407, 'accumulated_eval_time': 9294.731753826141, 'accumulated_logging_time': 2.572896957397461}
I0205 11:08:28.061895 139804644198144 logging_writer.py:48] [53446] accumulated_eval_time=9294.731754, accumulated_logging_time=2.572897, accumulated_submission_time=17069.828882, global_step=53446, preemption_count=0, score=17069.828882, test/accuracy=0.985928, test/loss=0.050372, test/mean_average_precision=0.259512, test/num_examples=43793, total_duration=26368.558529, train/accuracy=0.993287, train/loss=0.021416, train/mean_average_precision=0.606697, validation/accuracy=0.986797, validation/loss=0.047229, validation/mean_average_precision=0.267952, validation/num_examples=43793
I0205 11:08:45.568306 139804984182528 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.15917178988456726, loss=0.026511508971452713
I0205 11:09:17.196356 139804644198144 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.1642215996980667, loss=0.02702208422124386
I0205 11:09:48.904781 139804984182528 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.14852097630500793, loss=0.026185885071754456
I0205 11:10:20.680780 139804644198144 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.14854903519153595, loss=0.025889627635478973
I0205 11:10:52.442660 139804984182528 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.13866972923278809, loss=0.02564251609146595
I0205 11:11:24.412233 139804644198144 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.14715048670768738, loss=0.027316726744174957
I0205 11:11:56.040923 139804984182528 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.15716294944286346, loss=0.024396607652306557
I0205 11:12:28.022366 139804644198144 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.16785506904125214, loss=0.025879722088575363
I0205 11:12:28.347378 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:14:24.227741 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:14:27.493509 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:14:30.632795 139978932307776 submission_runner.py:408] Time since start: 26731.15s, 	Step: 54202, 	{'train/accuracy': 0.9932235479354858, 'train/loss': 0.021382365375757217, 'train/mean_average_precision': 0.6007852406767729, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.047565758228302, 'validation/mean_average_precision': 0.26813019900277196, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.050886500626802444, 'test/mean_average_precision': 0.25720875453985653, 'test/num_examples': 43793, 'score': 17310.08274435997, 'total_duration': 26731.154951810837, 'accumulated_submission_time': 17310.08274435997, 'accumulated_eval_time': 9417.017122030258, 'accumulated_logging_time': 2.609468460083008}
I0205 11:14:30.658277 139805009491712 logging_writer.py:48] [54202] accumulated_eval_time=9417.017122, accumulated_logging_time=2.609468, accumulated_submission_time=17310.082744, global_step=54202, preemption_count=0, score=17310.082744, test/accuracy=0.985984, test/loss=0.050887, test/mean_average_precision=0.257209, test/num_examples=43793, total_duration=26731.154952, train/accuracy=0.993224, train/loss=0.021382, train/mean_average_precision=0.600785, validation/accuracy=0.986802, validation/loss=0.047566, validation/mean_average_precision=0.268130, validation/num_examples=43793
I0205 11:15:02.396346 139806663034624 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.14784550666809082, loss=0.026487763971090317
I0205 11:15:33.966723 139805009491712 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.13538707792758942, loss=0.02537546493113041
I0205 11:16:05.385660 139806663034624 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.14247354865074158, loss=0.02573356404900551
I0205 11:16:37.054773 139805009491712 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.154397651553154, loss=0.024408945813775063
I0205 11:17:08.557261 139806663034624 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.17566458880901337, loss=0.027563713490962982
I0205 11:17:40.402888 139805009491712 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.145498126745224, loss=0.024370627477765083
I0205 11:18:12.394474 139806663034624 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.13920451700687408, loss=0.02576066181063652
I0205 11:18:30.922202 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:20:28.568358 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:20:31.576783 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:20:34.579229 139978932307776 submission_runner.py:408] Time since start: 27095.10s, 	Step: 54959, 	{'train/accuracy': 0.9933767318725586, 'train/loss': 0.021047839894890785, 'train/mean_average_precision': 0.6104609656727764, 'validation/accuracy': 0.9867898225784302, 'validation/loss': 0.047836292535066605, 'validation/mean_average_precision': 0.2712276426357609, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.05099121853709221, 'test/mean_average_precision': 0.2596567648926789, 'test/num_examples': 43793, 'score': 17550.31499028206, 'total_duration': 27095.101389169693, 'accumulated_submission_time': 17550.31499028206, 'accumulated_eval_time': 9540.674103498459, 'accumulated_logging_time': 2.645934820175171}
I0205 11:20:34.605490 139804644198144 logging_writer.py:48] [54959] accumulated_eval_time=9540.674103, accumulated_logging_time=2.645935, accumulated_submission_time=17550.314990, global_step=54959, preemption_count=0, score=17550.314990, test/accuracy=0.985782, test/loss=0.050991, test/mean_average_precision=0.259657, test/num_examples=43793, total_duration=27095.101389, train/accuracy=0.993377, train/loss=0.021048, train/mean_average_precision=0.610461, validation/accuracy=0.986790, validation/loss=0.047836, validation/mean_average_precision=0.271228, validation/num_examples=43793
I0205 11:20:48.310078 139805680953088 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.17584234476089478, loss=0.024914538487792015
I0205 11:21:20.161359 139804644198144 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.1578872948884964, loss=0.026412541046738625
I0205 11:21:52.026209 139805680953088 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1895783543586731, loss=0.024783724918961525
I0205 11:22:24.091045 139804644198144 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18214622139930725, loss=0.02658218704164028
I0205 11:22:56.262822 139805680953088 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1617484986782074, loss=0.026982596144080162
I0205 11:23:28.109493 139804644198144 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.14270834624767303, loss=0.024108758196234703
I0205 11:23:59.841370 139805680953088 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.1549384891986847, loss=0.02504788339138031
I0205 11:24:31.858678 139804644198144 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.15381747484207153, loss=0.025229323655366898
I0205 11:24:34.764598 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:26:30.209242 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:26:33.257468 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:26:36.246792 139978932307776 submission_runner.py:408] Time since start: 27456.77s, 	Step: 55710, 	{'train/accuracy': 0.9933373332023621, 'train/loss': 0.021069824695587158, 'train/mean_average_precision': 0.6063672334387846, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.048117153346538544, 'validation/mean_average_precision': 0.27011614070263557, 'validation/num_examples': 43793, 'test/accuracy': 0.9857547283172607, 'test/loss': 0.051404375582933426, 'test/mean_average_precision': 0.254248360382065, 'test/num_examples': 43793, 'score': 17790.44306921959, 'total_duration': 27456.768955230713, 'accumulated_submission_time': 17790.44306921959, 'accumulated_eval_time': 9662.156247615814, 'accumulated_logging_time': 2.6831068992614746}
I0205 11:26:36.272751 139805009491712 logging_writer.py:48] [55710] accumulated_eval_time=9662.156248, accumulated_logging_time=2.683107, accumulated_submission_time=17790.443069, global_step=55710, preemption_count=0, score=17790.443069, test/accuracy=0.985755, test/loss=0.051404, test/mean_average_precision=0.254248, test/num_examples=43793, total_duration=27456.768955, train/accuracy=0.993337, train/loss=0.021070, train/mean_average_precision=0.606367, validation/accuracy=0.986742, validation/loss=0.048117, validation/mean_average_precision=0.270116, validation/num_examples=43793
I0205 11:27:05.439206 139806663034624 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.16562756896018982, loss=0.024850014597177505
I0205 11:27:37.463856 139805009491712 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.15095636248588562, loss=0.024649448692798615
I0205 11:28:09.543239 139806663034624 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.14531658589839935, loss=0.022942278534173965
I0205 11:28:41.425862 139805009491712 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1732054054737091, loss=0.02608955278992653
I0205 11:29:13.218992 139806663034624 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.15897169709205627, loss=0.026503320783376694
I0205 11:29:44.932056 139805009491712 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.16475266218185425, loss=0.025078464299440384
I0205 11:30:16.848021 139806663034624 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.15710952877998352, loss=0.025126010179519653
I0205 11:30:36.503962 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:32:31.298930 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:32:34.354532 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:32:37.404930 139978932307776 submission_runner.py:408] Time since start: 27817.93s, 	Step: 56463, 	{'train/accuracy': 0.9936086535453796, 'train/loss': 0.02019779197871685, 'train/mean_average_precision': 0.6252101015049698, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.04820362105965614, 'validation/mean_average_precision': 0.2638393455795906, 'validation/num_examples': 43793, 'test/accuracy': 0.9857159852981567, 'test/loss': 0.05166997015476227, 'test/mean_average_precision': 0.25066363348113785, 'test/num_examples': 43793, 'score': 18030.641747236252, 'total_duration': 27817.927065372467, 'accumulated_submission_time': 18030.641747236252, 'accumulated_eval_time': 9783.057143211365, 'accumulated_logging_time': 2.7213125228881836}
I0205 11:32:37.431950 139804644198144 logging_writer.py:48] [56463] accumulated_eval_time=9783.057143, accumulated_logging_time=2.721313, accumulated_submission_time=18030.641747, global_step=56463, preemption_count=0, score=18030.641747, test/accuracy=0.985716, test/loss=0.051670, test/mean_average_precision=0.250664, test/num_examples=43793, total_duration=27817.927065, train/accuracy=0.993609, train/loss=0.020198, train/mean_average_precision=0.625210, validation/accuracy=0.986689, validation/loss=0.048204, validation/mean_average_precision=0.263839, validation/num_examples=43793
I0205 11:32:49.954208 139805680953088 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.16999219357967377, loss=0.025961697101593018
I0205 11:33:21.898327 139804644198144 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.142862007021904, loss=0.02357105351984501
I0205 11:33:53.500183 139805680953088 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.14980246126651764, loss=0.024275120347738266
I0205 11:34:25.536040 139804644198144 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1814827024936676, loss=0.02585107833147049
I0205 11:34:57.334213 139805680953088 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1543661206960678, loss=0.02326507866382599
I0205 11:35:29.450740 139804644198144 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1677839756011963, loss=0.02698475681245327
I0205 11:36:01.149236 139805680953088 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.18175414204597473, loss=0.02513495460152626
I0205 11:36:32.989538 139804644198144 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1733880341053009, loss=0.02498854510486126
I0205 11:36:37.703221 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:38:33.291604 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:38:36.294150 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:38:39.258503 139978932307776 submission_runner.py:408] Time since start: 28179.78s, 	Step: 57216, 	{'train/accuracy': 0.9939561486244202, 'train/loss': 0.01918221265077591, 'train/mean_average_precision': 0.6443242592758944, 'validation/accuracy': 0.9866737127304077, 'validation/loss': 0.04826799035072327, 'validation/mean_average_precision': 0.266641069144078, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.05154655501246452, 'test/mean_average_precision': 0.26062947479450477, 'test/num_examples': 43793, 'score': 18270.87906050682, 'total_duration': 28179.78066945076, 'accumulated_submission_time': 18270.87906050682, 'accumulated_eval_time': 9904.612380743027, 'accumulated_logging_time': 2.7617759704589844}
I0205 11:38:39.284242 139804984182528 logging_writer.py:48] [57216] accumulated_eval_time=9904.612381, accumulated_logging_time=2.761776, accumulated_submission_time=18270.879061, global_step=57216, preemption_count=0, score=18270.879061, test/accuracy=0.985850, test/loss=0.051547, test/mean_average_precision=0.260629, test/num_examples=43793, total_duration=28179.780669, train/accuracy=0.993956, train/loss=0.019182, train/mean_average_precision=0.644324, validation/accuracy=0.986674, validation/loss=0.048268, validation/mean_average_precision=0.266641, validation/num_examples=43793
I0205 11:39:06.090802 139805009491712 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.18042533099651337, loss=0.02520429529249668
I0205 11:39:37.833590 139804984182528 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1680941879749298, loss=0.02465515024960041
I0205 11:40:09.357925 139805009491712 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.21182747185230255, loss=0.02542976476252079
I0205 11:40:40.740730 139804984182528 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.1733938604593277, loss=0.0246114581823349
I0205 11:41:12.367675 139805009491712 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.22125476598739624, loss=0.026803113520145416
I0205 11:41:43.816906 139804984182528 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.1481224149465561, loss=0.022411447018384933
I0205 11:42:05.664759 139805009491712 logging_writer.py:48] [57870] global_step=57870, preemption_count=0, score=18477.214519
I0205 11:42:05.715472 139978932307776 checkpoints.py:490] Saving checkpoint at step: 57870
I0205 11:42:05.850323 139978932307776 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1/checkpoint_57870
I0205 11:42:05.851629 139978932307776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_1/checkpoint_57870.
I0205 11:42:06.022246 139978932307776 submission_runner.py:583] Tuning trial 1/5
I0205 11:42:06.022502 139978932307776 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 11:42:06.034921 139978932307776 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.38876962661743164, 'train/loss': 0.7993155121803284, 'train/mean_average_precision': 0.021555210797481506, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.024826656551027976, 'validation/num_examples': 43793, 'test/accuracy': 0.3947486877441406, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026365955872687814, 'test/num_examples': 43793, 'score': 24.272481203079224, 'total_duration': 326.25207114219666, 'accumulated_submission_time': 24.272481203079224, 'accumulated_eval_time': 301.979544878006, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (763, {'train/accuracy': 0.9866331219673157, 'train/loss': 0.07056180387735367, 'train/mean_average_precision': 0.0359794845055697, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07843790203332901, 'validation/mean_average_precision': 0.03719516993500635, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.0812012106180191, 'test/mean_average_precision': 0.03837000188530284, 'test/num_examples': 43793, 'score': 264.3512933254242, 'total_duration': 680.7509214878082, 'accumulated_submission_time': 264.3512933254242, 'accumulated_eval_time': 416.35071659088135, 'accumulated_logging_time': 0.02980351448059082, 'global_step': 763, 'preemption_count': 0}), (1516, {'train/accuracy': 0.9874165058135986, 'train/loss': 0.048196982592344284, 'train/mean_average_precision': 0.08383394494491832, 'validation/accuracy': 0.9846513271331787, 'validation/loss': 0.057339392602443695, 'validation/mean_average_precision': 0.08842731813172773, 'validation/num_examples': 43793, 'test/accuracy': 0.9836373925209045, 'test/loss': 0.06053172051906586, 'test/mean_average_precision': 0.08886458962019851, 'test/num_examples': 43793, 'score': 504.42682576179504, 'total_duration': 1040.8308537006378, 'accumulated_submission_time': 504.42682576179504, 'accumulated_eval_time': 536.3063020706177, 'accumulated_logging_time': 0.05943036079406738, 'global_step': 1516, 'preemption_count': 0}), (2272, {'train/accuracy': 0.9878497123718262, 'train/loss': 0.04417021572589874, 'train/mean_average_precision': 0.13175346800292792, 'validation/accuracy': 0.9849797487258911, 'validation/loss': 0.05390032008290291, 'validation/mean_average_precision': 0.13002132410664413, 'validation/num_examples': 43793, 'test/accuracy': 0.9840400815010071, 'test/loss': 0.056891318410634995, 'test/mean_average_precision': 0.1302070232937932, 'test/num_examples': 43793, 'score': 744.610445022583, 'total_duration': 1404.3017330169678, 'accumulated_submission_time': 744.610445022583, 'accumulated_eval_time': 659.5478177070618, 'accumulated_logging_time': 0.08603835105895996, 'global_step': 2272, 'preemption_count': 0}), (3024, {'train/accuracy': 0.9880422353744507, 'train/loss': 0.04227624461054802, 'train/mean_average_precision': 0.1592302566590645, 'validation/accuracy': 0.9852914810180664, 'validation/loss': 0.05124949291348457, 'validation/mean_average_precision': 0.15319013957304484, 'validation/num_examples': 43793, 'test/accuracy': 0.9843626618385315, 'test/loss': 0.0538056381046772, 'test/mean_average_precision': 0.1520338482806402, 'test/num_examples': 43793, 'score': 984.7043483257294, 'total_duration': 1768.7225918769836, 'accumulated_submission_time': 984.7043483257294, 'accumulated_eval_time': 783.8252048492432, 'accumulated_logging_time': 0.11647272109985352, 'global_step': 3024, 'preemption_count': 0}), (3778, {'train/accuracy': 0.9882649779319763, 'train/loss': 0.04051998257637024, 'train/mean_average_precision': 0.19136164647091158, 'validation/accuracy': 0.9853755235671997, 'validation/loss': 0.05026780813932419, 'validation/mean_average_precision': 0.1748420624477223, 'validation/num_examples': 43793, 'test/accuracy': 0.9844376444816589, 'test/loss': 0.05316726863384247, 'test/mean_average_precision': 0.17318849199594025, 'test/num_examples': 43793, 'score': 1224.8253166675568, 'total_duration': 2130.8698279857635, 'accumulated_submission_time': 1224.8253166675568, 'accumulated_eval_time': 905.8047299385071, 'accumulated_logging_time': 0.14408493041992188, 'global_step': 3778, 'preemption_count': 0}), (4538, {'train/accuracy': 0.9881364107131958, 'train/loss': 0.04055248573422432, 'train/mean_average_precision': 0.1995915133924315, 'validation/accuracy': 0.9855062365531921, 'validation/loss': 0.049841176718473434, 'validation/mean_average_precision': 0.18333610998019786, 'validation/num_examples': 43793, 'test/accuracy': 0.984628438949585, 'test/loss': 0.052729532122612, 'test/mean_average_precision': 0.17959885213536883, 'test/num_examples': 43793, 'score': 1464.9182755947113, 'total_duration': 2494.039370775223, 'accumulated_submission_time': 1464.9182755947113, 'accumulated_eval_time': 1028.835110425949, 'accumulated_logging_time': 0.17086505889892578, 'global_step': 4538, 'preemption_count': 0}), (5291, {'train/accuracy': 0.9885979890823364, 'train/loss': 0.0390627458691597, 'train/mean_average_precision': 0.235640214546389, 'validation/accuracy': 0.9856986403465271, 'validation/loss': 0.04872404411435127, 'validation/mean_average_precision': 0.1980871299196871, 'validation/num_examples': 43793, 'test/accuracy': 0.9847902059555054, 'test/loss': 0.051404111087322235, 'test/mean_average_precision': 0.19305945672324892, 'test/num_examples': 43793, 'score': 1704.9223086833954, 'total_duration': 2860.033614873886, 'accumulated_submission_time': 1704.9223086833954, 'accumulated_eval_time': 1154.7750566005707, 'accumulated_logging_time': 0.2018733024597168, 'global_step': 5291, 'preemption_count': 0}), (6048, {'train/accuracy': 0.9889920353889465, 'train/loss': 0.03758462890982628, 'train/mean_average_precision': 0.24712975262076814, 'validation/accuracy': 0.9858882427215576, 'validation/loss': 0.0478384830057621, 'validation/mean_average_precision': 0.20302851230948468, 'validation/num_examples': 43793, 'test/accuracy': 0.9850159883499146, 'test/loss': 0.050465963780879974, 'test/mean_average_precision': 0.19885518823917242, 'test/num_examples': 43793, 'score': 1945.1591565608978, 'total_duration': 3226.5057003498077, 'accumulated_submission_time': 1945.1591565608978, 'accumulated_eval_time': 1280.962295293808, 'accumulated_logging_time': 0.2301163673400879, 'global_step': 6048, 'preemption_count': 0}), (6799, {'train/accuracy': 0.9890881180763245, 'train/loss': 0.037440940737724304, 'train/mean_average_precision': 0.25415324042609444, 'validation/accuracy': 0.9859243631362915, 'validation/loss': 0.04757380113005638, 'validation/mean_average_precision': 0.20746549366052722, 'validation/num_examples': 43793, 'test/accuracy': 0.9850117564201355, 'test/loss': 0.05010205879807472, 'test/mean_average_precision': 0.2055948206630922, 'test/num_examples': 43793, 'score': 2185.3532433509827, 'total_duration': 3590.998979330063, 'accumulated_submission_time': 2185.3532433509827, 'accumulated_eval_time': 1405.2155866622925, 'accumulated_logging_time': 0.2566075325012207, 'global_step': 6799, 'preemption_count': 0}), (7555, {'train/accuracy': 0.9891881942749023, 'train/loss': 0.03661906719207764, 'train/mean_average_precision': 0.2631767259260585, 'validation/accuracy': 0.9861549735069275, 'validation/loss': 0.04668034613132477, 'validation/mean_average_precision': 0.2189902151347329, 'validation/num_examples': 43793, 'test/accuracy': 0.9853213429450989, 'test/loss': 0.04916556179523468, 'test/mean_average_precision': 0.2203662111267828, 'test/num_examples': 43793, 'score': 2425.605710029602, 'total_duration': 3956.9928188323975, 'accumulated_submission_time': 2425.605710029602, 'accumulated_eval_time': 1530.9100363254547, 'accumulated_logging_time': 0.28405094146728516, 'global_step': 7555, 'preemption_count': 0}), (8313, {'train/accuracy': 0.989005982875824, 'train/loss': 0.03703552111983299, 'train/mean_average_precision': 0.25546423593524753, 'validation/accuracy': 0.9861886501312256, 'validation/loss': 0.046532753854990005, 'validation/mean_average_precision': 0.22607550617359756, 'validation/num_examples': 43793, 'test/accuracy': 0.9852766990661621, 'test/loss': 0.04921731725335121, 'test/mean_average_precision': 0.21973123956483565, 'test/num_examples': 43793, 'score': 2665.6187121868134, 'total_duration': 4322.964905500412, 'accumulated_submission_time': 2665.6187121868134, 'accumulated_eval_time': 1656.8199598789215, 'accumulated_logging_time': 0.31336045265197754, 'global_step': 8313, 'preemption_count': 0}), (9070, {'train/accuracy': 0.9891762137413025, 'train/loss': 0.036489855498075485, 'train/mean_average_precision': 0.28288348213249725, 'validation/accuracy': 0.9861512780189514, 'validation/loss': 0.04660046100616455, 'validation/mean_average_precision': 0.22502004687923619, 'validation/num_examples': 43793, 'test/accuracy': 0.9853150248527527, 'test/loss': 0.049311891198158264, 'test/mean_average_precision': 0.2232171338319447, 'test/num_examples': 43793, 'score': 2905.712122440338, 'total_duration': 4690.790143251419, 'accumulated_submission_time': 2905.712122440338, 'accumulated_eval_time': 1784.504117488861, 'accumulated_logging_time': 0.34171271324157715, 'global_step': 9070, 'preemption_count': 0}), (9820, {'train/accuracy': 0.9896302819252014, 'train/loss': 0.03538098186254501, 'train/mean_average_precision': 0.2862586124008465, 'validation/accuracy': 0.9862738847732544, 'validation/loss': 0.04619408771395683, 'validation/mean_average_precision': 0.22457472816507262, 'validation/num_examples': 43793, 'test/accuracy': 0.9853967428207397, 'test/loss': 0.04879970848560333, 'test/mean_average_precision': 0.22735471987155884, 'test/num_examples': 43793, 'score': 3145.721575975418, 'total_duration': 5057.321557760239, 'accumulated_submission_time': 3145.721575975418, 'accumulated_eval_time': 1910.9776666164398, 'accumulated_logging_time': 0.37102818489074707, 'global_step': 9820, 'preemption_count': 0}), (10569, {'train/accuracy': 0.9896215200424194, 'train/loss': 0.03488897532224655, 'train/mean_average_precision': 0.3107923658999827, 'validation/accuracy': 0.9863603711128235, 'validation/loss': 0.045642539858818054, 'validation/mean_average_precision': 0.2417199449464199, 'validation/num_examples': 43793, 'test/accuracy': 0.9855096340179443, 'test/loss': 0.04835975170135498, 'test/mean_average_precision': 0.23351386698909332, 'test/num_examples': 43793, 'score': 3385.802992105484, 'total_duration': 5423.044348478317, 'accumulated_submission_time': 3385.802992105484, 'accumulated_eval_time': 2036.5718340873718, 'accumulated_logging_time': 0.3991975784301758, 'global_step': 10569, 'preemption_count': 0}), (11323, {'train/accuracy': 0.989838182926178, 'train/loss': 0.03400369733572006, 'train/mean_average_precision': 0.32667529756985764, 'validation/accuracy': 0.9865649342536926, 'validation/loss': 0.045087773352861404, 'validation/mean_average_precision': 0.2468068704228591, 'validation/num_examples': 43793, 'test/accuracy': 0.9857492446899414, 'test/loss': 0.04749658703804016, 'test/mean_average_precision': 0.2476235995592323, 'test/num_examples': 43793, 'score': 3625.9665355682373, 'total_duration': 5792.6845734119415, 'accumulated_submission_time': 3625.9665355682373, 'accumulated_eval_time': 2166.000794649124, 'accumulated_logging_time': 0.427293062210083, 'global_step': 11323, 'preemption_count': 0}), (12080, {'train/accuracy': 0.9900984764099121, 'train/loss': 0.03318807855248451, 'train/mean_average_precision': 0.34687909561369384, 'validation/accuracy': 0.9866153001785278, 'validation/loss': 0.04485723003745079, 'validation/mean_average_precision': 0.24687213253247628, 'validation/num_examples': 43793, 'test/accuracy': 0.9857353568077087, 'test/loss': 0.04748206213116646, 'test/mean_average_precision': 0.23909211752357484, 'test/num_examples': 43793, 'score': 3865.95241522789, 'total_duration': 6159.020653247833, 'accumulated_submission_time': 3865.95241522789, 'accumulated_eval_time': 2292.3009836673737, 'accumulated_logging_time': 0.4576456546783447, 'global_step': 12080, 'preemption_count': 0}), (12832, {'train/accuracy': 0.9901254177093506, 'train/loss': 0.032805345952510834, 'train/mean_average_precision': 0.355174481887847, 'validation/accuracy': 0.9865312576293945, 'validation/loss': 0.044936634600162506, 'validation/mean_average_precision': 0.24883223156314221, 'validation/num_examples': 43793, 'test/accuracy': 0.9856481552124023, 'test/loss': 0.04745563492178917, 'test/mean_average_precision': 0.24171528826891692, 'test/num_examples': 43793, 'score': 4106.013333797455, 'total_duration': 6526.13848400116, 'accumulated_submission_time': 4106.013333797455, 'accumulated_eval_time': 2419.3099246025085, 'accumulated_logging_time': 0.4859592914581299, 'global_step': 12832, 'preemption_count': 0}), (13581, {'train/accuracy': 0.9902234077453613, 'train/loss': 0.03212849050760269, 'train/mean_average_precision': 0.37092765121408844, 'validation/accuracy': 0.9865828156471252, 'validation/loss': 0.0451812818646431, 'validation/mean_average_precision': 0.24976972107872278, 'validation/num_examples': 43793, 'test/accuracy': 0.9857589602470398, 'test/loss': 0.04780641570687294, 'test/mean_average_precision': 0.24283336460482102, 'test/num_examples': 43793, 'score': 4346.096912384033, 'total_duration': 6893.016060590744, 'accumulated_submission_time': 4346.096912384033, 'accumulated_eval_time': 2546.0544838905334, 'accumulated_logging_time': 0.5157315731048584, 'global_step': 13581, 'preemption_count': 0}), (14331, {'train/accuracy': 0.9904870986938477, 'train/loss': 0.03172406554222107, 'train/mean_average_precision': 0.3738916030379458, 'validation/accuracy': 0.9866850972175598, 'validation/loss': 0.04454706236720085, 'validation/mean_average_precision': 0.2559132804697909, 'validation/num_examples': 43793, 'test/accuracy': 0.9858056902885437, 'test/loss': 0.04711402952671051, 'test/mean_average_precision': 0.24484617934449046, 'test/num_examples': 43793, 'score': 4586.202342510223, 'total_duration': 7265.011176109314, 'accumulated_submission_time': 4586.202342510223, 'accumulated_eval_time': 2677.8932163715363, 'accumulated_logging_time': 0.547095775604248, 'global_step': 14331, 'preemption_count': 0}), (15078, {'train/accuracy': 0.9903684258460999, 'train/loss': 0.031961578875780106, 'train/mean_average_precision': 0.3792209517081584, 'validation/accuracy': 0.9867200255393982, 'validation/loss': 0.044694095849990845, 'validation/mean_average_precision': 0.25235253559489096, 'validation/num_examples': 43793, 'test/accuracy': 0.9858945608139038, 'test/loss': 0.04718709737062454, 'test/mean_average_precision': 0.24945004130953052, 'test/num_examples': 43793, 'score': 4826.174679040909, 'total_duration': 7635.104958295822, 'accumulated_submission_time': 4826.174679040909, 'accumulated_eval_time': 2807.9602975845337, 'accumulated_logging_time': 0.5814604759216309, 'global_step': 15078, 'preemption_count': 0}), (15823, {'train/accuracy': 0.9903414845466614, 'train/loss': 0.03201202303171158, 'train/mean_average_precision': 0.38660113116527683, 'validation/accuracy': 0.9866615533828735, 'validation/loss': 0.045002248138189316, 'validation/mean_average_precision': 0.25811686529120725, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.04766395315527916, 'test/mean_average_precision': 0.24804640159581487, 'test/num_examples': 43793, 'score': 5066.263954401016, 'total_duration': 8005.788725376129, 'accumulated_submission_time': 5066.263954401016, 'accumulated_eval_time': 2938.505881547928, 'accumulated_logging_time': 0.6110355854034424, 'global_step': 15823, 'preemption_count': 0}), (16570, {'train/accuracy': 0.9903900623321533, 'train/loss': 0.03188588097691536, 'train/mean_average_precision': 0.367818958616825, 'validation/accuracy': 0.9866932034492493, 'validation/loss': 0.04465251788496971, 'validation/mean_average_precision': 0.2544019229495246, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04692884162068367, 'test/mean_average_precision': 0.25699559465107674, 'test/num_examples': 43793, 'score': 5306.334696531296, 'total_duration': 8374.538804292679, 'accumulated_submission_time': 5306.334696531296, 'accumulated_eval_time': 3067.133838415146, 'accumulated_logging_time': 0.6414587497711182, 'global_step': 16570, 'preemption_count': 0}), (17320, {'train/accuracy': 0.9903742671012878, 'train/loss': 0.031720176339149475, 'train/mean_average_precision': 0.37926661622880675, 'validation/accuracy': 0.9867115020751953, 'validation/loss': 0.0447704903781414, 'validation/mean_average_precision': 0.2558755533093419, 'validation/num_examples': 43793, 'test/accuracy': 0.9859164953231812, 'test/loss': 0.04706665500998497, 'test/mean_average_precision': 0.25208275056372015, 'test/num_examples': 43793, 'score': 5546.373893737793, 'total_duration': 8743.351408958435, 'accumulated_submission_time': 5546.373893737793, 'accumulated_eval_time': 3195.8570890426636, 'accumulated_logging_time': 0.6710550785064697, 'global_step': 17320, 'preemption_count': 0}), (18060, {'train/accuracy': 0.9906107783317566, 'train/loss': 0.031032945960760117, 'train/mean_average_precision': 0.38503022354033767, 'validation/accuracy': 0.9868434071540833, 'validation/loss': 0.04448278620839119, 'validation/mean_average_precision': 0.26346505720687874, 'validation/num_examples': 43793, 'test/accuracy': 0.9859927296638489, 'test/loss': 0.04704469069838524, 'test/mean_average_precision': 0.2545460441147208, 'test/num_examples': 43793, 'score': 5786.353320121765, 'total_duration': 9114.093678951263, 'accumulated_submission_time': 5786.353320121765, 'accumulated_eval_time': 3326.569318294525, 'accumulated_logging_time': 0.7022576332092285, 'global_step': 18060, 'preemption_count': 0}), (18801, {'train/accuracy': 0.9907031059265137, 'train/loss': 0.030958306044340134, 'train/mean_average_precision': 0.39886864563917124, 'validation/accuracy': 0.9866709113121033, 'validation/loss': 0.04486231133341789, 'validation/mean_average_precision': 0.25838409829558157, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.047371238470077515, 'test/mean_average_precision': 0.2486702052961499, 'test/num_examples': 43793, 'score': 6026.574734687805, 'total_duration': 9486.867498636246, 'accumulated_submission_time': 6026.574734687805, 'accumulated_eval_time': 3459.0701246261597, 'accumulated_logging_time': 0.7338111400604248, 'global_step': 18801, 'preemption_count': 0}), (19550, {'train/accuracy': 0.9908729195594788, 'train/loss': 0.03007735311985016, 'train/mean_average_precision': 0.42760700785825156, 'validation/accuracy': 0.9867585897445679, 'validation/loss': 0.04431118443608284, 'validation/mean_average_precision': 0.26550092224414445, 'validation/num_examples': 43793, 'test/accuracy': 0.9858562350273132, 'test/loss': 0.04700959846377373, 'test/mean_average_precision': 0.25293038105504523, 'test/num_examples': 43793, 'score': 6266.726683616638, 'total_duration': 9858.895729064941, 'accumulated_submission_time': 6266.726683616638, 'accumulated_eval_time': 3590.8943939208984, 'accumulated_logging_time': 0.7656044960021973, 'global_step': 19550, 'preemption_count': 0}), (20294, {'train/accuracy': 0.9909902811050415, 'train/loss': 0.029482202604413033, 'train/mean_average_precision': 0.4394125373132448, 'validation/accuracy': 0.9867805242538452, 'validation/loss': 0.044341910630464554, 'validation/mean_average_precision': 0.26305367815534236, 'validation/num_examples': 43793, 'test/accuracy': 0.9859615564346313, 'test/loss': 0.04684840515255928, 'test/mean_average_precision': 0.25876499272489484, 'test/num_examples': 43793, 'score': 6506.863411426544, 'total_duration': 10231.407826423645, 'accumulated_submission_time': 6506.863411426544, 'accumulated_eval_time': 3723.2180132865906, 'accumulated_logging_time': 0.7975211143493652, 'global_step': 20294, 'preemption_count': 0}), (21040, {'train/accuracy': 0.9909581542015076, 'train/loss': 0.02934497594833374, 'train/mean_average_precision': 0.4319782718431922, 'validation/accuracy': 0.9868271946907043, 'validation/loss': 0.044256966561079025, 'validation/mean_average_precision': 0.2620292413708662, 'validation/num_examples': 43793, 'test/accuracy': 0.985920250415802, 'test/loss': 0.04709073156118393, 'test/mean_average_precision': 0.2505250804658075, 'test/num_examples': 43793, 'score': 6747.060025215149, 'total_duration': 10599.26006937027, 'accumulated_submission_time': 6747.060025215149, 'accumulated_eval_time': 3850.82346367836, 'accumulated_logging_time': 0.8276448249816895, 'global_step': 21040, 'preemption_count': 0}), (21788, {'train/accuracy': 0.9909757971763611, 'train/loss': 0.029786057770252228, 'train/mean_average_precision': 0.41127393537537804, 'validation/accuracy': 0.9867516756057739, 'validation/loss': 0.044571686536073685, 'validation/mean_average_precision': 0.2534629003852076, 'validation/num_examples': 43793, 'test/accuracy': 0.9858971238136292, 'test/loss': 0.047172050923109055, 'test/mean_average_precision': 0.24506086347335104, 'test/num_examples': 43793, 'score': 6987.061242580414, 'total_duration': 10967.76712179184, 'accumulated_submission_time': 6987.061242580414, 'accumulated_eval_time': 3979.2788722515106, 'accumulated_logging_time': 0.8579602241516113, 'global_step': 21788, 'preemption_count': 0}), (22534, {'train/accuracy': 0.9908212423324585, 'train/loss': 0.03031608648598194, 'train/mean_average_precision': 0.4167117446792816, 'validation/accuracy': 0.9868998527526855, 'validation/loss': 0.044284187257289886, 'validation/mean_average_precision': 0.26396243707412637, 'validation/num_examples': 43793, 'test/accuracy': 0.9860268235206604, 'test/loss': 0.04701342061161995, 'test/mean_average_precision': 0.259165562861682, 'test/num_examples': 43793, 'score': 7227.019123077393, 'total_duration': 11339.331525325775, 'accumulated_submission_time': 7227.019123077393, 'accumulated_eval_time': 4110.835282087326, 'accumulated_logging_time': 0.8883786201477051, 'global_step': 22534, 'preemption_count': 0}), (23281, {'train/accuracy': 0.9908854365348816, 'train/loss': 0.03038932941854, 'train/mean_average_precision': 0.4145162980485438, 'validation/accuracy': 0.986748456954956, 'validation/loss': 0.04439399763941765, 'validation/mean_average_precision': 0.26433337024141745, 'validation/num_examples': 43793, 'test/accuracy': 0.9858128428459167, 'test/loss': 0.046998314559459686, 'test/mean_average_precision': 0.2511379732354883, 'test/num_examples': 43793, 'score': 7466.973075628281, 'total_duration': 11708.985038518906, 'accumulated_submission_time': 7466.973075628281, 'accumulated_eval_time': 4240.48300409317, 'accumulated_logging_time': 0.9200398921966553, 'global_step': 23281, 'preemption_count': 0}), (24031, {'train/accuracy': 0.9907976984977722, 'train/loss': 0.030122244730591774, 'train/mean_average_precision': 0.4307058110209221, 'validation/accuracy': 0.9866781830787659, 'validation/loss': 0.04450766369700432, 'validation/mean_average_precision': 0.2598207593992417, 'validation/num_examples': 43793, 'test/accuracy': 0.9858853220939636, 'test/loss': 0.04721352085471153, 'test/mean_average_precision': 0.2514192693563914, 'test/num_examples': 43793, 'score': 7706.785302639008, 'total_duration': 12079.562668561935, 'accumulated_submission_time': 7706.785302639008, 'accumulated_eval_time': 4370.933499574661, 'accumulated_logging_time': 1.214691162109375, 'global_step': 24031, 'preemption_count': 0}), (24784, {'train/accuracy': 0.9909395575523376, 'train/loss': 0.02971666306257248, 'train/mean_average_precision': 0.4196510958908334, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.044562917202711105, 'validation/mean_average_precision': 0.2656587042217679, 'validation/num_examples': 43793, 'test/accuracy': 0.9858975410461426, 'test/loss': 0.04716982692480087, 'test/mean_average_precision': 0.25909295942461513, 'test/num_examples': 43793, 'score': 7946.956842422485, 'total_duration': 12447.819665908813, 'accumulated_submission_time': 7946.956842422485, 'accumulated_eval_time': 4498.967143058777, 'accumulated_logging_time': 1.2464373111724854, 'global_step': 24784, 'preemption_count': 0}), (25543, {'train/accuracy': 0.991019070148468, 'train/loss': 0.02948913164436817, 'train/mean_average_precision': 0.4367420533835061, 'validation/accuracy': 0.9868320822715759, 'validation/loss': 0.04458141699433327, 'validation/mean_average_precision': 0.2679887542443359, 'validation/num_examples': 43793, 'test/accuracy': 0.9859825968742371, 'test/loss': 0.047204818576574326, 'test/mean_average_precision': 0.2633457291123027, 'test/num_examples': 43793, 'score': 8187.189737558365, 'total_duration': 12813.948610544205, 'accumulated_submission_time': 8187.189737558365, 'accumulated_eval_time': 4624.81149148941, 'accumulated_logging_time': 1.2782447338104248, 'global_step': 25543, 'preemption_count': 0}), (26303, {'train/accuracy': 0.9911377429962158, 'train/loss': 0.029033243656158447, 'train/mean_average_precision': 0.4396199351537836, 'validation/accuracy': 0.9868190884590149, 'validation/loss': 0.04453993961215019, 'validation/mean_average_precision': 0.26872231422311754, 'validation/num_examples': 43793, 'test/accuracy': 0.9859455227851868, 'test/loss': 0.04693721979856491, 'test/mean_average_precision': 0.2593809136492444, 'test/num_examples': 43793, 'score': 8427.414668560028, 'total_duration': 13180.273483514786, 'accumulated_submission_time': 8427.414668560028, 'accumulated_eval_time': 4750.8594517707825, 'accumulated_logging_time': 1.3099524974822998, 'global_step': 26303, 'preemption_count': 0}), (27058, {'train/accuracy': 0.9913957715034485, 'train/loss': 0.028460824862122536, 'train/mean_average_precision': 0.4587331702582208, 'validation/accuracy': 0.9867423176765442, 'validation/loss': 0.04439501091837883, 'validation/mean_average_precision': 0.2620780825173272, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.04702652990818024, 'test/mean_average_precision': 0.25268346439916317, 'test/num_examples': 43793, 'score': 8667.366248846054, 'total_duration': 13549.797183036804, 'accumulated_submission_time': 8667.366248846054, 'accumulated_eval_time': 4880.380812883377, 'accumulated_logging_time': 1.3404219150543213, 'global_step': 27058, 'preemption_count': 0}), (27812, {'train/accuracy': 0.9915438890457153, 'train/loss': 0.02755151130259037, 'train/mean_average_precision': 0.478820071746103, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.04407647252082825, 'validation/mean_average_precision': 0.268497485961063, 'validation/num_examples': 43793, 'test/accuracy': 0.9860082864761353, 'test/loss': 0.046913038939237595, 'test/mean_average_precision': 0.256955366585077, 'test/num_examples': 43793, 'score': 8907.356931447983, 'total_duration': 13917.328118801117, 'accumulated_submission_time': 8907.356931447983, 'accumulated_eval_time': 5007.864626646042, 'accumulated_logging_time': 1.3766028881072998, 'global_step': 27812, 'preemption_count': 0}), (28566, {'train/accuracy': 0.9915900230407715, 'train/loss': 0.02743000164628029, 'train/mean_average_precision': 0.4838465650251311, 'validation/accuracy': 0.9868852496147156, 'validation/loss': 0.04411470890045166, 'validation/mean_average_precision': 0.27742265750240147, 'validation/num_examples': 43793, 'test/accuracy': 0.9860162734985352, 'test/loss': 0.04679397866129875, 'test/mean_average_precision': 0.2587861733148328, 'test/num_examples': 43793, 'score': 9147.537345647812, 'total_duration': 14287.542890071869, 'accumulated_submission_time': 9147.537345647812, 'accumulated_eval_time': 5137.846126794815, 'accumulated_logging_time': 1.4091360569000244, 'global_step': 28566, 'preemption_count': 0}), (29316, {'train/accuracy': 0.9914113283157349, 'train/loss': 0.028087593615055084, 'train/mean_average_precision': 0.4503752334796423, 'validation/accuracy': 0.9868101477622986, 'validation/loss': 0.04454953595995903, 'validation/mean_average_precision': 0.26453523417580793, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.04737480357289314, 'test/mean_average_precision': 0.2550472083789822, 'test/num_examples': 43793, 'score': 9387.8157954216, 'total_duration': 14659.599410057068, 'accumulated_submission_time': 9387.8157954216, 'accumulated_eval_time': 5269.571957588196, 'accumulated_logging_time': 1.4414465427398682, 'global_step': 29316, 'preemption_count': 0}), (30073, {'train/accuracy': 0.9912699460983276, 'train/loss': 0.028659606352448463, 'train/mean_average_precision': 0.44959692861203004, 'validation/accuracy': 0.9867662787437439, 'validation/loss': 0.044700346887111664, 'validation/mean_average_precision': 0.2619020218166186, 'validation/num_examples': 43793, 'test/accuracy': 0.9858600497245789, 'test/loss': 0.04760089889168739, 'test/mean_average_precision': 0.2547086969015181, 'test/num_examples': 43793, 'score': 9627.81943321228, 'total_duration': 15028.82996916771, 'accumulated_submission_time': 9627.81943321228, 'accumulated_eval_time': 5398.746550559998, 'accumulated_logging_time': 1.4734342098236084, 'global_step': 30073, 'preemption_count': 0}), (30829, {'train/accuracy': 0.9910553097724915, 'train/loss': 0.02904028445482254, 'train/mean_average_precision': 0.4345576765374209, 'validation/accuracy': 0.9868341088294983, 'validation/loss': 0.044944878667593, 'validation/mean_average_precision': 0.26515396532756713, 'validation/num_examples': 43793, 'test/accuracy': 0.985971212387085, 'test/loss': 0.047688573598861694, 'test/mean_average_precision': 0.25496920849557436, 'test/num_examples': 43793, 'score': 9867.785235404968, 'total_duration': 15397.919102430344, 'accumulated_submission_time': 9867.785235404968, 'accumulated_eval_time': 5527.817716121674, 'accumulated_logging_time': 1.5052387714385986, 'global_step': 30829, 'preemption_count': 0}), (31588, {'train/accuracy': 0.9913302063941956, 'train/loss': 0.028579991310834885, 'train/mean_average_precision': 0.45910879798687093, 'validation/accuracy': 0.9867050051689148, 'validation/loss': 0.04459002614021301, 'validation/mean_average_precision': 0.2681575482452986, 'validation/num_examples': 43793, 'test/accuracy': 0.985917329788208, 'test/loss': 0.04713777080178261, 'test/mean_average_precision': 0.25802998955314144, 'test/num_examples': 43793, 'score': 10107.885499715805, 'total_duration': 15765.455152750015, 'accumulated_submission_time': 10107.885499715805, 'accumulated_eval_time': 5655.2004935741425, 'accumulated_logging_time': 1.5384142398834229, 'global_step': 31588, 'preemption_count': 0}), (32346, {'train/accuracy': 0.9914727210998535, 'train/loss': 0.0278371162712574, 'train/mean_average_precision': 0.47179791076157324, 'validation/accuracy': 0.9869274497032166, 'validation/loss': 0.043929535895586014, 'validation/mean_average_precision': 0.27411557041168505, 'validation/num_examples': 43793, 'test/accuracy': 0.9860752820968628, 'test/loss': 0.04677533358335495, 'test/mean_average_precision': 0.26618305739415, 'test/num_examples': 43793, 'score': 10347.881727457047, 'total_duration': 16134.079976081848, 'accumulated_submission_time': 10347.881727457047, 'accumulated_eval_time': 5783.775832414627, 'accumulated_logging_time': 1.572129487991333, 'global_step': 32346, 'preemption_count': 0}), (33102, {'train/accuracy': 0.9913511276245117, 'train/loss': 0.028184767812490463, 'train/mean_average_precision': 0.45692966019385856, 'validation/accuracy': 0.9868227243423462, 'validation/loss': 0.04469173774123192, 'validation/mean_average_precision': 0.26549203945799665, 'validation/num_examples': 43793, 'test/accuracy': 0.985958993434906, 'test/loss': 0.047356247901916504, 'test/mean_average_precision': 0.2575073350230395, 'test/num_examples': 43793, 'score': 10587.903796672821, 'total_duration': 16501.14848446846, 'accumulated_submission_time': 10587.903796672821, 'accumulated_eval_time': 5910.768916606903, 'accumulated_logging_time': 1.6054506301879883, 'global_step': 33102, 'preemption_count': 0}), (33860, {'train/accuracy': 0.9915713667869568, 'train/loss': 0.02751315012574196, 'train/mean_average_precision': 0.4805088503117457, 'validation/accuracy': 0.9869164824485779, 'validation/loss': 0.04435295984148979, 'validation/mean_average_precision': 0.27338494682774434, 'validation/num_examples': 43793, 'test/accuracy': 0.9860659837722778, 'test/loss': 0.047166142612695694, 'test/mean_average_precision': 0.25900657790581455, 'test/num_examples': 43793, 'score': 10828.046881198883, 'total_duration': 16867.5029835701, 'accumulated_submission_time': 10828.046881198883, 'accumulated_eval_time': 6036.926429271698, 'accumulated_logging_time': 1.6387813091278076, 'global_step': 33860, 'preemption_count': 0}), (34617, {'train/accuracy': 0.9917721152305603, 'train/loss': 0.02674318104982376, 'train/mean_average_precision': 0.49386078200373046, 'validation/accuracy': 0.9869108200073242, 'validation/loss': 0.0445934534072876, 'validation/mean_average_precision': 0.2638629801469858, 'validation/num_examples': 43793, 'test/accuracy': 0.9859358668327332, 'test/loss': 0.047608476132154465, 'test/mean_average_precision': 0.2522764802677962, 'test/num_examples': 43793, 'score': 11068.133890151978, 'total_duration': 17232.824068784714, 'accumulated_submission_time': 11068.133890151978, 'accumulated_eval_time': 6162.1058077812195, 'accumulated_logging_time': 1.6727240085601807, 'global_step': 34617, 'preemption_count': 0}), (35373, {'train/accuracy': 0.9918096661567688, 'train/loss': 0.026530418545007706, 'train/mean_average_precision': 0.5029054677780674, 'validation/accuracy': 0.9868081212043762, 'validation/loss': 0.0446687713265419, 'validation/mean_average_precision': 0.2637214181806488, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04744245111942291, 'test/mean_average_precision': 0.2559087693871861, 'test/num_examples': 43793, 'score': 11308.152801513672, 'total_duration': 17598.52809739113, 'accumulated_submission_time': 11308.152801513672, 'accumulated_eval_time': 6287.737751483917, 'accumulated_logging_time': 1.7055885791778564, 'global_step': 35373, 'preemption_count': 0}), (36130, {'train/accuracy': 0.9921783208847046, 'train/loss': 0.025464337319135666, 'train/mean_average_precision': 0.5405584099723635, 'validation/accuracy': 0.9869144558906555, 'validation/loss': 0.04474916309118271, 'validation/mean_average_precision': 0.2696709121978386, 'validation/num_examples': 43793, 'test/accuracy': 0.9859346151351929, 'test/loss': 0.04761963337659836, 'test/mean_average_precision': 0.254952374996752, 'test/num_examples': 43793, 'score': 11548.278539657593, 'total_duration': 17964.49371266365, 'accumulated_submission_time': 11548.278539657593, 'accumulated_eval_time': 6413.520535945892, 'accumulated_logging_time': 1.7420799732208252, 'global_step': 36130, 'preemption_count': 0}), (36881, {'train/accuracy': 0.9921891689300537, 'train/loss': 0.025626953691244125, 'train/mean_average_precision': 0.5054668073603061, 'validation/accuracy': 0.9867618083953857, 'validation/loss': 0.04484332352876663, 'validation/mean_average_precision': 0.2666542209544567, 'validation/num_examples': 43793, 'test/accuracy': 0.9858899712562561, 'test/loss': 0.04770039767026901, 'test/mean_average_precision': 0.25322437256657715, 'test/num_examples': 43793, 'score': 11788.357815027237, 'total_duration': 18332.315428972244, 'accumulated_submission_time': 11788.357815027237, 'accumulated_eval_time': 6541.208086490631, 'accumulated_logging_time': 1.7762196063995361, 'global_step': 36881, 'preemption_count': 0}), (37631, {'train/accuracy': 0.9919636249542236, 'train/loss': 0.02628188394010067, 'train/mean_average_precision': 0.5000979311796215, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.044515348970890045, 'validation/mean_average_precision': 0.26927634371474507, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.04737162962555885, 'test/mean_average_precision': 0.2548070281969129, 'test/num_examples': 43793, 'score': 12028.320405721664, 'total_duration': 18700.193468809128, 'accumulated_submission_time': 12028.320405721664, 'accumulated_eval_time': 6669.069313287735, 'accumulated_logging_time': 1.8099312782287598, 'global_step': 37631, 'preemption_count': 0}), (38379, {'train/accuracy': 0.9917044639587402, 'train/loss': 0.026799645274877548, 'train/mean_average_precision': 0.49388362588191836, 'validation/accuracy': 0.986860454082489, 'validation/loss': 0.045009173452854156, 'validation/mean_average_precision': 0.2717148167414828, 'validation/num_examples': 43793, 'test/accuracy': 0.9860424399375916, 'test/loss': 0.0478706955909729, 'test/mean_average_precision': 0.25797355187363524, 'test/num_examples': 43793, 'score': 12268.405726671219, 'total_duration': 19063.826691389084, 'accumulated_submission_time': 12268.405726671219, 'accumulated_eval_time': 6792.561163425446, 'accumulated_logging_time': 1.8454234600067139, 'global_step': 38379, 'preemption_count': 0}), (39127, {'train/accuracy': 0.9917911887168884, 'train/loss': 0.026672180742025375, 'train/mean_average_precision': 0.48954401239902784, 'validation/accuracy': 0.9869790077209473, 'validation/loss': 0.044631291180849075, 'validation/mean_average_precision': 0.2678226921038907, 'validation/num_examples': 43793, 'test/accuracy': 0.9860386252403259, 'test/loss': 0.04738719388842583, 'test/mean_average_precision': 0.2648304309454832, 'test/num_examples': 43793, 'score': 12508.509371519089, 'total_duration': 19430.22696685791, 'accumulated_submission_time': 12508.509371519089, 'accumulated_eval_time': 6918.80362701416, 'accumulated_logging_time': 1.879314661026001, 'global_step': 39127, 'preemption_count': 0}), (39885, {'train/accuracy': 0.9919468760490417, 'train/loss': 0.02608482353389263, 'train/mean_average_precision': 0.5204706545550591, 'validation/accuracy': 0.9870122671127319, 'validation/loss': 0.04427463188767433, 'validation/mean_average_precision': 0.2786177764963892, 'validation/num_examples': 43793, 'test/accuracy': 0.9861629009246826, 'test/loss': 0.04734284058213234, 'test/mean_average_precision': 0.26225317433379713, 'test/num_examples': 43793, 'score': 12748.566566467285, 'total_duration': 19791.197275161743, 'accumulated_submission_time': 12748.566566467285, 'accumulated_eval_time': 7039.660442113876, 'accumulated_logging_time': 1.91489839553833, 'global_step': 39885, 'preemption_count': 0}), (40639, {'train/accuracy': 0.9919112324714661, 'train/loss': 0.026074526831507683, 'train/mean_average_precision': 0.5069046204867362, 'validation/accuracy': 0.9867808818817139, 'validation/loss': 0.04517439007759094, 'validation/mean_average_precision': 0.2680662528745569, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.04800134524703026, 'test/mean_average_precision': 0.25675443142912335, 'test/num_examples': 43793, 'score': 12988.536489725113, 'total_duration': 20159.67905855179, 'accumulated_submission_time': 12988.536489725113, 'accumulated_eval_time': 7168.117209196091, 'accumulated_logging_time': 1.9494268894195557, 'global_step': 40639, 'preemption_count': 0}), (41398, {'train/accuracy': 0.9920253753662109, 'train/loss': 0.025676319375634193, 'train/mean_average_precision': 0.508227948274832, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.04517970606684685, 'validation/mean_average_precision': 0.27326760721328275, 'validation/num_examples': 43793, 'test/accuracy': 0.9859611392021179, 'test/loss': 0.047962479293346405, 'test/mean_average_precision': 0.26566777619756393, 'test/num_examples': 43793, 'score': 13228.646829366684, 'total_duration': 20523.62076807022, 'accumulated_submission_time': 13228.646829366684, 'accumulated_eval_time': 7291.893787145615, 'accumulated_logging_time': 1.9835777282714844, 'global_step': 41398, 'preemption_count': 0}), (42155, {'train/accuracy': 0.9922612905502319, 'train/loss': 0.024819236248731613, 'train/mean_average_precision': 0.5357011778114732, 'validation/accuracy': 0.9868612885475159, 'validation/loss': 0.0453098826110363, 'validation/mean_average_precision': 0.26412438235599994, 'validation/num_examples': 43793, 'test/accuracy': 0.9859897494316101, 'test/loss': 0.04822394996881485, 'test/mean_average_precision': 0.25749466873757293, 'test/num_examples': 43793, 'score': 13468.731812000275, 'total_duration': 20885.98131942749, 'accumulated_submission_time': 13468.731812000275, 'accumulated_eval_time': 7414.11568903923, 'accumulated_logging_time': 2.017220973968506, 'global_step': 42155, 'preemption_count': 0}), (42908, {'train/accuracy': 0.9926353096961975, 'train/loss': 0.02383463643491268, 'train/mean_average_precision': 0.5570144195376461, 'validation/accuracy': 0.986894965171814, 'validation/loss': 0.04515489563345909, 'validation/mean_average_precision': 0.26848816192137337, 'validation/num_examples': 43793, 'test/accuracy': 0.9860158562660217, 'test/loss': 0.04806399717926979, 'test/mean_average_precision': 0.26533230383810813, 'test/num_examples': 43793, 'score': 13708.764877796173, 'total_duration': 21250.519181251526, 'accumulated_submission_time': 13708.764877796173, 'accumulated_eval_time': 7538.565611362457, 'accumulated_logging_time': 2.051811456680298, 'global_step': 42908, 'preemption_count': 0}), (43665, {'train/accuracy': 0.9925816655158997, 'train/loss': 0.023902637884020805, 'train/mean_average_precision': 0.569847598106749, 'validation/accuracy': 0.9868986010551453, 'validation/loss': 0.045283351093530655, 'validation/mean_average_precision': 0.2717307550178367, 'validation/num_examples': 43793, 'test/accuracy': 0.9859805107116699, 'test/loss': 0.04846496880054474, 'test/mean_average_precision': 0.2619368512501648, 'test/num_examples': 43793, 'score': 13948.78694844246, 'total_duration': 21616.52692937851, 'accumulated_submission_time': 13948.78694844246, 'accumulated_eval_time': 7664.496058940887, 'accumulated_logging_time': 2.0864064693450928, 'global_step': 43665, 'preemption_count': 0}), (44417, {'train/accuracy': 0.9929485321044922, 'train/loss': 0.022788044065237045, 'train/mean_average_precision': 0.5749294484192354, 'validation/accuracy': 0.9868471026420593, 'validation/loss': 0.045269984751939774, 'validation/mean_average_precision': 0.27262849192091826, 'validation/num_examples': 43793, 'test/accuracy': 0.9859729409217834, 'test/loss': 0.0484565906226635, 'test/mean_average_precision': 0.25695242147863345, 'test/num_examples': 43793, 'score': 14188.775550365448, 'total_duration': 21985.23408293724, 'accumulated_submission_time': 14188.775550365448, 'accumulated_eval_time': 7793.158420085907, 'accumulated_logging_time': 2.121481418609619, 'global_step': 44417, 'preemption_count': 0}), (45164, {'train/accuracy': 0.9925280213356018, 'train/loss': 0.024165211245417595, 'train/mean_average_precision': 0.5439482996360956, 'validation/accuracy': 0.9867951273918152, 'validation/loss': 0.045505501329898834, 'validation/mean_average_precision': 0.27256189700011024, 'validation/num_examples': 43793, 'test/accuracy': 0.985913097858429, 'test/loss': 0.04859090596437454, 'test/mean_average_precision': 0.25627907897947927, 'test/num_examples': 43793, 'score': 14428.960973501205, 'total_duration': 22349.58697938919, 'accumulated_submission_time': 14428.960973501205, 'accumulated_eval_time': 7917.267426967621, 'accumulated_logging_time': 2.1592814922332764, 'global_step': 45164, 'preemption_count': 0}), (45910, {'train/accuracy': 0.9924951195716858, 'train/loss': 0.024184413254261017, 'train/mean_average_precision': 0.5322754362561134, 'validation/accuracy': 0.9868247509002686, 'validation/loss': 0.045396801084280014, 'validation/mean_average_precision': 0.26942070758583075, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04840182512998581, 'test/mean_average_precision': 0.25797832627777517, 'test/num_examples': 43793, 'score': 14669.197919368744, 'total_duration': 22715.47890305519, 'accumulated_submission_time': 14669.197919368744, 'accumulated_eval_time': 8042.86420583725, 'accumulated_logging_time': 2.1964058876037598, 'global_step': 45910, 'preemption_count': 0}), (46658, {'train/accuracy': 0.9923391938209534, 'train/loss': 0.024557368829846382, 'train/mean_average_precision': 0.5505022282160861, 'validation/accuracy': 0.9869205355644226, 'validation/loss': 0.045789334923028946, 'validation/mean_average_precision': 0.26658504589910953, 'validation/num_examples': 43793, 'test/accuracy': 0.9860398769378662, 'test/loss': 0.04890056326985359, 'test/mean_average_precision': 0.2569565723669573, 'test/num_examples': 43793, 'score': 14909.381882667542, 'total_duration': 23080.968989133835, 'accumulated_submission_time': 14909.381882667542, 'accumulated_eval_time': 8168.112809181213, 'accumulated_logging_time': 2.2335081100463867, 'global_step': 46658, 'preemption_count': 0}), (47410, {'train/accuracy': 0.9923987984657288, 'train/loss': 0.024354690685868263, 'train/mean_average_precision': 0.547945267187708, 'validation/accuracy': 0.9869290590286255, 'validation/loss': 0.04555318504571915, 'validation/mean_average_precision': 0.27309230894032954, 'validation/num_examples': 43793, 'test/accuracy': 0.9860478639602661, 'test/loss': 0.04859509691596031, 'test/mean_average_precision': 0.25842061742902916, 'test/num_examples': 43793, 'score': 15149.46195435524, 'total_duration': 23447.16524910927, 'accumulated_submission_time': 15149.46195435524, 'accumulated_eval_time': 8294.17154455185, 'accumulated_logging_time': 2.2706735134124756, 'global_step': 47410, 'preemption_count': 0}), (48164, {'train/accuracy': 0.9926859736442566, 'train/loss': 0.023512663319706917, 'train/mean_average_precision': 0.5531412566917546, 'validation/accuracy': 0.9868718385696411, 'validation/loss': 0.045387301594018936, 'validation/mean_average_precision': 0.2729452931736282, 'validation/num_examples': 43793, 'test/accuracy': 0.9860175848007202, 'test/loss': 0.04826309159398079, 'test/mean_average_precision': 0.2613342734660124, 'test/num_examples': 43793, 'score': 15389.69013428688, 'total_duration': 23813.830407857895, 'accumulated_submission_time': 15389.69013428688, 'accumulated_eval_time': 8420.551263570786, 'accumulated_logging_time': 2.3075530529022217, 'global_step': 48164, 'preemption_count': 0}), (48923, {'train/accuracy': 0.9927862286567688, 'train/loss': 0.02313043363392353, 'train/mean_average_precision': 0.5690608496377537, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.04562116786837578, 'validation/mean_average_precision': 0.26960942815402544, 'validation/num_examples': 43793, 'test/accuracy': 0.9859282970428467, 'test/loss': 0.04853885620832443, 'test/mean_average_precision': 0.26179905298209477, 'test/num_examples': 43793, 'score': 15629.627718687057, 'total_duration': 24176.42839407921, 'accumulated_submission_time': 15629.627718687057, 'accumulated_eval_time': 8543.148166894913, 'accumulated_logging_time': 2.350667953491211, 'global_step': 48923, 'preemption_count': 0}), (49677, {'train/accuracy': 0.9929873943328857, 'train/loss': 0.022309299558401108, 'train/mean_average_precision': 0.5790065918357813, 'validation/accuracy': 0.9868316650390625, 'validation/loss': 0.04610390216112137, 'validation/mean_average_precision': 0.27036258907015986, 'validation/num_examples': 43793, 'test/accuracy': 0.9859762787818909, 'test/loss': 0.04926516115665436, 'test/mean_average_precision': 0.2634007317030291, 'test/num_examples': 43793, 'score': 15869.585559368134, 'total_duration': 24541.952813386917, 'accumulated_submission_time': 15869.585559368134, 'accumulated_eval_time': 8668.656472444534, 'accumulated_logging_time': 2.388645648956299, 'global_step': 49677, 'preemption_count': 0}), (50435, {'train/accuracy': 0.993471086025238, 'train/loss': 0.02111111953854561, 'train/mean_average_precision': 0.6146497697624851, 'validation/accuracy': 0.986735463142395, 'validation/loss': 0.04609406739473343, 'validation/mean_average_precision': 0.2744567062110209, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04910604655742645, 'test/mean_average_precision': 0.2610190774407657, 'test/num_examples': 43793, 'score': 16109.806084394455, 'total_duration': 24906.823295116425, 'accumulated_submission_time': 16109.806084394455, 'accumulated_eval_time': 8793.248821020126, 'accumulated_logging_time': 2.4257941246032715, 'global_step': 50435, 'preemption_count': 0}), (51187, {'train/accuracy': 0.9937585592269897, 'train/loss': 0.020372990518808365, 'train/mean_average_precision': 0.6229511751791663, 'validation/accuracy': 0.9867293238639832, 'validation/loss': 0.04670211300253868, 'validation/mean_average_precision': 0.2689869825711359, 'validation/num_examples': 43793, 'test/accuracy': 0.9857732653617859, 'test/loss': 0.04984183609485626, 'test/mean_average_precision': 0.2565580586568688, 'test/num_examples': 43793, 'score': 16349.852685928345, 'total_duration': 25272.11195421219, 'accumulated_submission_time': 16349.852685928345, 'accumulated_eval_time': 8918.434713840485, 'accumulated_logging_time': 2.461792469024658, 'global_step': 51187, 'preemption_count': 0}), (51940, {'train/accuracy': 0.9935181140899658, 'train/loss': 0.02072816528379917, 'train/mean_average_precision': 0.6268248896791947, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.04659906029701233, 'validation/mean_average_precision': 0.2722384098665055, 'validation/num_examples': 43793, 'test/accuracy': 0.9859463572502136, 'test/loss': 0.049767713993787766, 'test/mean_average_precision': 0.2659970159599488, 'test/num_examples': 43793, 'score': 16589.82354283333, 'total_duration': 25637.23035120964, 'accumulated_submission_time': 16589.82354283333, 'accumulated_eval_time': 9043.52482008934, 'accumulated_logging_time': 2.4987967014312744, 'global_step': 51940, 'preemption_count': 0}), (52690, {'train/accuracy': 0.9936492443084717, 'train/loss': 0.020698348060250282, 'train/mean_average_precision': 0.6196390393205258, 'validation/accuracy': 0.9866599440574646, 'validation/loss': 0.046764276921749115, 'validation/mean_average_precision': 0.2696261098279769, 'validation/num_examples': 43793, 'test/accuracy': 0.9856839776039124, 'test/loss': 0.050197768956422806, 'test/mean_average_precision': 0.25498745121818817, 'test/num_examples': 43793, 'score': 16829.831008911133, 'total_duration': 26003.949209213257, 'accumulated_submission_time': 16829.831008911133, 'accumulated_eval_time': 9170.178788661957, 'accumulated_logging_time': 2.5351483821868896, 'global_step': 52690, 'preemption_count': 0}), (53446, {'train/accuracy': 0.9932865500450134, 'train/loss': 0.021415717899799347, 'train/mean_average_precision': 0.6066971485592483, 'validation/accuracy': 0.9867967367172241, 'validation/loss': 0.0472288504242897, 'validation/mean_average_precision': 0.2679517868571796, 'validation/num_examples': 43793, 'test/accuracy': 0.9859278798103333, 'test/loss': 0.05037239193916321, 'test/mean_average_precision': 0.25951235150770374, 'test/num_examples': 43793, 'score': 17069.828882217407, 'total_duration': 26368.558528900146, 'accumulated_submission_time': 17069.828882217407, 'accumulated_eval_time': 9294.731753826141, 'accumulated_logging_time': 2.572896957397461, 'global_step': 53446, 'preemption_count': 0}), (54202, {'train/accuracy': 0.9932235479354858, 'train/loss': 0.021382365375757217, 'train/mean_average_precision': 0.6007852406767729, 'validation/accuracy': 0.9868023991584778, 'validation/loss': 0.047565758228302, 'validation/mean_average_precision': 0.26813019900277196, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.050886500626802444, 'test/mean_average_precision': 0.25720875453985653, 'test/num_examples': 43793, 'score': 17310.08274435997, 'total_duration': 26731.154951810837, 'accumulated_submission_time': 17310.08274435997, 'accumulated_eval_time': 9417.017122030258, 'accumulated_logging_time': 2.609468460083008, 'global_step': 54202, 'preemption_count': 0}), (54959, {'train/accuracy': 0.9933767318725586, 'train/loss': 0.021047839894890785, 'train/mean_average_precision': 0.6104609656727764, 'validation/accuracy': 0.9867898225784302, 'validation/loss': 0.047836292535066605, 'validation/mean_average_precision': 0.2712276426357609, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.05099121853709221, 'test/mean_average_precision': 0.2596567648926789, 'test/num_examples': 43793, 'score': 17550.31499028206, 'total_duration': 27095.101389169693, 'accumulated_submission_time': 17550.31499028206, 'accumulated_eval_time': 9540.674103498459, 'accumulated_logging_time': 2.645934820175171, 'global_step': 54959, 'preemption_count': 0}), (55710, {'train/accuracy': 0.9933373332023621, 'train/loss': 0.021069824695587158, 'train/mean_average_precision': 0.6063672334387846, 'validation/accuracy': 0.9867419600486755, 'validation/loss': 0.048117153346538544, 'validation/mean_average_precision': 0.27011614070263557, 'validation/num_examples': 43793, 'test/accuracy': 0.9857547283172607, 'test/loss': 0.051404375582933426, 'test/mean_average_precision': 0.254248360382065, 'test/num_examples': 43793, 'score': 17790.44306921959, 'total_duration': 27456.768955230713, 'accumulated_submission_time': 17790.44306921959, 'accumulated_eval_time': 9662.156247615814, 'accumulated_logging_time': 2.6831068992614746, 'global_step': 55710, 'preemption_count': 0}), (56463, {'train/accuracy': 0.9936086535453796, 'train/loss': 0.02019779197871685, 'train/mean_average_precision': 0.6252101015049698, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.04820362105965614, 'validation/mean_average_precision': 0.2638393455795906, 'validation/num_examples': 43793, 'test/accuracy': 0.9857159852981567, 'test/loss': 0.05166997015476227, 'test/mean_average_precision': 0.25066363348113785, 'test/num_examples': 43793, 'score': 18030.641747236252, 'total_duration': 27817.927065372467, 'accumulated_submission_time': 18030.641747236252, 'accumulated_eval_time': 9783.057143211365, 'accumulated_logging_time': 2.7213125228881836, 'global_step': 56463, 'preemption_count': 0}), (57216, {'train/accuracy': 0.9939561486244202, 'train/loss': 0.01918221265077591, 'train/mean_average_precision': 0.6443242592758944, 'validation/accuracy': 0.9866737127304077, 'validation/loss': 0.04826799035072327, 'validation/mean_average_precision': 0.266641069144078, 'validation/num_examples': 43793, 'test/accuracy': 0.9858503341674805, 'test/loss': 0.05154655501246452, 'test/mean_average_precision': 0.26062947479450477, 'test/num_examples': 43793, 'score': 18270.87906050682, 'total_duration': 28179.78066945076, 'accumulated_submission_time': 18270.87906050682, 'accumulated_eval_time': 9904.612380743027, 'accumulated_logging_time': 2.7617759704589844, 'global_step': 57216, 'preemption_count': 0})], 'global_step': 57870}
I0205 11:42:06.035162 139978932307776 submission_runner.py:586] Timing: 18477.214519262314
I0205 11:42:06.035223 139978932307776 submission_runner.py:588] Total number of evals: 77
I0205 11:42:06.035268 139978932307776 submission_runner.py:589] ====================
I0205 11:42:06.035317 139978932307776 submission_runner.py:542] Using RNG seed 3917441912
I0205 11:42:06.106969 139978932307776 submission_runner.py:551] --- Tuning run 2/5 ---
I0205 11:42:06.107144 139978932307776 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2.
I0205 11:42:06.107378 139978932307776 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2/hparams.json.
I0205 11:42:06.250771 139978932307776 submission_runner.py:206] Initializing dataset.
I0205 11:42:06.344767 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 11:42:06.349056 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 11:42:06.782582 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 11:42:06.825383 139978932307776 submission_runner.py:213] Initializing model.
I0205 11:42:09.658321 139978932307776 submission_runner.py:255] Initializing optimizer.
I0205 11:42:10.295750 139978932307776 submission_runner.py:262] Initializing metrics bundle.
I0205 11:42:10.295948 139978932307776 submission_runner.py:280] Initializing checkpoint and logger.
I0205 11:42:10.296640 139978932307776 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2 with prefix checkpoint_
I0205 11:42:10.296771 139978932307776 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2/meta_data_0.json.
I0205 11:42:10.296997 139978932307776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 11:42:10.297060 139978932307776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 11:42:11.705472 139978932307776 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 11:42:13.086059 139978932307776 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2/flags_0.json.
I0205 11:42:13.095844 139978932307776 submission_runner.py:314] Starting training loop.
I0205 11:42:25.295285 139789400393472 logging_writer.py:48] [0] global_step=0, grad_norm=3.1922061443328857, loss=0.7994188070297241
I0205 11:42:25.307005 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:44:25.069494 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:44:28.218560 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:44:31.264696 139978932307776 submission_runner.py:408] Time since start: 138.17s, 	Step: 1, 	{'train/accuracy': 0.38869017362594604, 'train/loss': 0.7994009852409363, 'train/mean_average_precision': 0.02424800716606758, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02486102341495216, 'validation/num_examples': 43793, 'test/accuracy': 0.3947480618953705, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026378706204941857, 'test/num_examples': 43793, 'score': 12.211112976074219, 'total_duration': 138.16879105567932, 'accumulated_submission_time': 12.211112976074219, 'accumulated_eval_time': 125.95763325691223, 'accumulated_logging_time': 0}
I0205 11:44:31.273939 139789417207552 logging_writer.py:48] [1] accumulated_eval_time=125.957633, accumulated_logging_time=0, accumulated_submission_time=12.211113, global_step=1, preemption_count=0, score=12.211113, test/accuracy=0.394748, test/loss=0.795676, test/mean_average_precision=0.026379, test/num_examples=43793, total_duration=138.168791, train/accuracy=0.388690, train/loss=0.799401, train/mean_average_precision=0.024248, validation/accuracy=0.392645, validation/loss=0.797466, validation/mean_average_precision=0.024861, validation/num_examples=43793
I0205 11:45:04.335910 139804984182528 logging_writer.py:48] [100] global_step=100, grad_norm=0.544159471988678, loss=0.44529497623443604
I0205 11:45:36.221660 139789417207552 logging_writer.py:48] [200] global_step=200, grad_norm=0.3745654821395874, loss=0.32503458857536316
I0205 11:46:08.312725 139804984182528 logging_writer.py:48] [300] global_step=300, grad_norm=0.26810422539711, loss=0.22160494327545166
I0205 11:46:40.487158 139789417207552 logging_writer.py:48] [400] global_step=400, grad_norm=0.16979485750198364, loss=0.14550261199474335
I0205 11:47:12.289445 139804984182528 logging_writer.py:48] [500] global_step=500, grad_norm=0.10604898631572723, loss=0.1037682592868805
I0205 11:47:44.559552 139789417207552 logging_writer.py:48] [600] global_step=600, grad_norm=0.07218719273805618, loss=0.07864638417959213
I0205 11:48:17.074751 139804984182528 logging_writer.py:48] [700] global_step=700, grad_norm=0.06821112334728241, loss=0.07697360217571259
I0205 11:48:31.345925 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:50:26.199105 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:50:29.223019 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:50:32.223601 139978932307776 submission_runner.py:408] Time since start: 499.13s, 	Step: 746, 	{'train/accuracy': 0.9867796897888184, 'train/loss': 0.06802871823310852, 'train/mean_average_precision': 0.040247962780616715, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07673989236354828, 'validation/mean_average_precision': 0.03999616155129081, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07968270778656006, 'test/mean_average_precision': 0.040902716898336994, 'test/num_examples': 43793, 'score': 252.2515525817871, 'total_duration': 499.1276910305023, 'accumulated_submission_time': 252.2515525817871, 'accumulated_eval_time': 246.83525800704956, 'accumulated_logging_time': 0.02017378807067871}
I0205 11:50:32.239747 139789425600256 logging_writer.py:48] [746] accumulated_eval_time=246.835258, accumulated_logging_time=0.020174, accumulated_submission_time=252.251553, global_step=746, preemption_count=0, score=252.251553, test/accuracy=0.983142, test/loss=0.079683, test/mean_average_precision=0.040903, test/num_examples=43793, total_duration=499.127691, train/accuracy=0.986780, train/loss=0.068029, train/mean_average_precision=0.040248, validation/accuracy=0.984118, validation/loss=0.076740, validation/mean_average_precision=0.039996, validation/num_examples=43793
I0205 11:50:49.659258 139804644198144 logging_writer.py:48] [800] global_step=800, grad_norm=0.07519406825304031, loss=0.05761962756514549
I0205 11:51:21.767440 139789425600256 logging_writer.py:48] [900] global_step=900, grad_norm=0.03476531803607941, loss=0.05946551635861397
I0205 11:51:53.714202 139804644198144 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.06931044906377792, loss=0.05814927816390991
I0205 11:52:25.901530 139789425600256 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.04094342514872551, loss=0.052637822926044464
I0205 11:52:57.942725 139804644198144 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0557866096496582, loss=0.04724662005901337
I0205 11:53:30.038661 139789425600256 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.07405003905296326, loss=0.046464212238788605
I0205 11:54:01.982924 139804644198144 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.03808675333857536, loss=0.045371200889348984
I0205 11:54:32.343791 139978932307776 spec.py:321] Evaluating on the training split.
I0205 11:56:31.193392 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 11:56:34.255011 139978932307776 spec.py:349] Evaluating on the test split.
I0205 11:56:37.259962 139978932307776 submission_runner.py:408] Time since start: 864.16s, 	Step: 1496, 	{'train/accuracy': 0.9871765375137329, 'train/loss': 0.04924483224749565, 'train/mean_average_precision': 0.0841241205174674, 'validation/accuracy': 0.9844134449958801, 'validation/loss': 0.05902522802352905, 'validation/mean_average_precision': 0.0906389025904376, 'validation/num_examples': 43793, 'test/accuracy': 0.9833930730819702, 'test/loss': 0.06238542124629021, 'test/mean_average_precision': 0.08894742056757338, 'test/num_examples': 43793, 'score': 492.32368326187134, 'total_duration': 864.1640558242798, 'accumulated_submission_time': 492.32368326187134, 'accumulated_eval_time': 371.7513871192932, 'accumulated_logging_time': 0.04763197898864746}
I0205 11:56:37.275549 139788813231872 logging_writer.py:48] [1496] accumulated_eval_time=371.751387, accumulated_logging_time=0.047632, accumulated_submission_time=492.323683, global_step=1496, preemption_count=0, score=492.323683, test/accuracy=0.983393, test/loss=0.062385, test/mean_average_precision=0.088947, test/num_examples=43793, total_duration=864.164056, train/accuracy=0.987177, train/loss=0.049245, train/mean_average_precision=0.084124, validation/accuracy=0.984413, validation/loss=0.059025, validation/mean_average_precision=0.090639, validation/num_examples=43793
I0205 11:56:38.926452 139789433992960 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.05967770144343376, loss=0.05464226007461548
I0205 11:57:11.121247 139788813231872 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.11015059053897858, loss=0.05393122136592865
I0205 11:57:43.173662 139789433992960 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.09178949147462845, loss=0.05134527385234833
I0205 11:58:15.210708 139788813231872 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.028172248974442482, loss=0.04855531081557274
I0205 11:58:47.207296 139789433992960 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.038877759128808975, loss=0.04723259061574936
I0205 11:59:19.423629 139788813231872 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.045016128569841385, loss=0.05177890136837959
I0205 11:59:51.448567 139789433992960 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.07974577695131302, loss=0.045801304280757904
I0205 12:00:23.486963 139788813231872 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.06217603757977486, loss=0.04692772403359413
I0205 12:00:37.422941 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:02:32.000181 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:02:35.068689 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:02:38.099067 139978932307776 submission_runner.py:408] Time since start: 1225.00s, 	Step: 2245, 	{'train/accuracy': 0.9875968098640442, 'train/loss': 0.04472910985350609, 'train/mean_average_precision': 0.13863232248856314, 'validation/accuracy': 0.9848965406417847, 'validation/loss': 0.05360018089413643, 'validation/mean_average_precision': 0.13760270919551698, 'validation/num_examples': 43793, 'test/accuracy': 0.9838993549346924, 'test/loss': 0.05648729205131531, 'test/mean_average_precision': 0.13005957405048965, 'test/num_examples': 43793, 'score': 732.4389727115631, 'total_duration': 1225.0031561851501, 'accumulated_submission_time': 732.4389727115631, 'accumulated_eval_time': 492.42746329307556, 'accumulated_logging_time': 0.07472872734069824}
I0205 12:02:38.115248 139789425600256 logging_writer.py:48] [2245] accumulated_eval_time=492.427463, accumulated_logging_time=0.074729, accumulated_submission_time=732.438973, global_step=2245, preemption_count=0, score=732.438973, test/accuracy=0.983899, test/loss=0.056487, test/mean_average_precision=0.130060, test/num_examples=43793, total_duration=1225.003156, train/accuracy=0.987597, train/loss=0.044729, train/mean_average_precision=0.138632, validation/accuracy=0.984897, validation/loss=0.053600, validation/mean_average_precision=0.137603, validation/num_examples=43793
I0205 12:02:55.961472 139804984182528 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.07203465700149536, loss=0.0494496114552021
I0205 12:03:27.945319 139789425600256 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.04627930372953415, loss=0.04860178008675575
I0205 12:04:00.117651 139804984182528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.07235487550497055, loss=0.046270933002233505
I0205 12:04:32.048730 139789425600256 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.04495152831077576, loss=0.04979940131306648
I0205 12:05:04.044292 139804984182528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05543220415711403, loss=0.04932292923331261
I0205 12:05:36.099686 139789425600256 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.025476641952991486, loss=0.044786665588617325
I0205 12:06:08.175195 139804984182528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.021362116560339928, loss=0.04673139005899429
I0205 12:06:38.286803 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:08:34.193546 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:08:37.256865 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:08:40.285436 139978932307776 submission_runner.py:408] Time since start: 1587.19s, 	Step: 2995, 	{'train/accuracy': 0.9878544807434082, 'train/loss': 0.04250020161271095, 'train/mean_average_precision': 0.17002244232531422, 'validation/accuracy': 0.9851725697517395, 'validation/loss': 0.05186421796679497, 'validation/mean_average_precision': 0.15461868909853507, 'validation/num_examples': 43793, 'test/accuracy': 0.9841992855072021, 'test/loss': 0.05470949783921242, 'test/mean_average_precision': 0.15080300451331569, 'test/num_examples': 43793, 'score': 972.5787220001221, 'total_duration': 1587.1895275115967, 'accumulated_submission_time': 972.5787220001221, 'accumulated_eval_time': 614.4260520935059, 'accumulated_logging_time': 0.10235357284545898}
I0205 12:08:40.302138 139788813231872 logging_writer.py:48] [2995] accumulated_eval_time=614.426052, accumulated_logging_time=0.102354, accumulated_submission_time=972.578722, global_step=2995, preemption_count=0, score=972.578722, test/accuracy=0.984199, test/loss=0.054709, test/mean_average_precision=0.150803, test/num_examples=43793, total_duration=1587.189528, train/accuracy=0.987854, train/loss=0.042500, train/mean_average_precision=0.170022, validation/accuracy=0.985173, validation/loss=0.051864, validation/mean_average_precision=0.154619, validation/num_examples=43793
I0205 12:08:42.286515 139789433992960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.050537109375, loss=0.048366010189056396
I0205 12:09:14.791099 139788813231872 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.04106118157505989, loss=0.04480195790529251
I0205 12:09:47.149835 139789433992960 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0441555492579937, loss=0.04662221670150757
I0205 12:10:19.325633 139788813231872 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04411247372627258, loss=0.04166202247142792
I0205 12:10:51.543988 139789433992960 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.028024284169077873, loss=0.04373594745993614
I0205 12:11:23.649232 139788813231872 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.041842736303806305, loss=0.04670316353440285
I0205 12:11:55.895501 139789433992960 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.02091292105615139, loss=0.044462669640779495
I0205 12:12:28.121016 139788813231872 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.03639717772603035, loss=0.042567238211631775
I0205 12:12:40.539329 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:14:34.523243 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:14:37.592615 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:14:40.606118 139978932307776 submission_runner.py:408] Time since start: 1947.51s, 	Step: 3740, 	{'train/accuracy': 0.9881300330162048, 'train/loss': 0.041738566011190414, 'train/mean_average_precision': 0.19069986186483673, 'validation/accuracy': 0.9853069186210632, 'validation/loss': 0.05121837928891182, 'validation/mean_average_precision': 0.16984024146752147, 'validation/num_examples': 43793, 'test/accuracy': 0.984386682510376, 'test/loss': 0.053729746490716934, 'test/mean_average_precision': 0.16614425510623596, 'test/num_examples': 43793, 'score': 1212.7844922542572, 'total_duration': 1947.5102033615112, 'accumulated_submission_time': 1212.7844922542572, 'accumulated_eval_time': 734.4927840232849, 'accumulated_logging_time': 0.1301419734954834}
I0205 12:14:40.622709 139804644198144 logging_writer.py:48] [3740] accumulated_eval_time=734.492784, accumulated_logging_time=0.130142, accumulated_submission_time=1212.784492, global_step=3740, preemption_count=0, score=1212.784492, test/accuracy=0.984387, test/loss=0.053730, test/mean_average_precision=0.166144, test/num_examples=43793, total_duration=1947.510203, train/accuracy=0.988130, train/loss=0.041739, train/mean_average_precision=0.190700, validation/accuracy=0.985307, validation/loss=0.051218, validation/mean_average_precision=0.169840, validation/num_examples=43793
I0205 12:14:59.933527 139804984182528 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.021781671792268753, loss=0.046593330800533295
I0205 12:15:31.431017 139804644198144 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.024998193606734276, loss=0.0428701788187027
I0205 12:16:03.190441 139804984182528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.026413699612021446, loss=0.04320994019508362
I0205 12:16:34.618227 139804644198144 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.032613605260849, loss=0.04788079485297203
I0205 12:17:06.192372 139804984182528 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.02938307821750641, loss=0.04971323534846306
I0205 12:17:37.745536 139804644198144 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0212087444961071, loss=0.04145205020904541
I0205 12:18:09.095636 139804984182528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.02320912480354309, loss=0.03923042491078377
I0205 12:18:40.500023 139804644198144 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.015416954644024372, loss=0.04348786920309067
I0205 12:18:40.819135 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:20:36.490827 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:20:39.499561 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:20:42.473584 139978932307776 submission_runner.py:408] Time since start: 2309.38s, 	Step: 4502, 	{'train/accuracy': 0.9881775975227356, 'train/loss': 0.04063105210661888, 'train/mean_average_precision': 0.21152857113458517, 'validation/accuracy': 0.9853300452232361, 'validation/loss': 0.05041537433862686, 'validation/mean_average_precision': 0.1774918128223911, 'validation/num_examples': 43793, 'test/accuracy': 0.9844212532043457, 'test/loss': 0.053231094032526016, 'test/mean_average_precision': 0.17645561924256073, 'test/num_examples': 43793, 'score': 1452.9492535591125, 'total_duration': 2309.3776705265045, 'accumulated_submission_time': 1452.9492535591125, 'accumulated_eval_time': 856.147173166275, 'accumulated_logging_time': 0.15766167640686035}
I0205 12:20:42.489797 139788813231872 logging_writer.py:48] [4502] accumulated_eval_time=856.147173, accumulated_logging_time=0.157662, accumulated_submission_time=1452.949254, global_step=4502, preemption_count=0, score=1452.949254, test/accuracy=0.984421, test/loss=0.053231, test/mean_average_precision=0.176456, test/num_examples=43793, total_duration=2309.377671, train/accuracy=0.988178, train/loss=0.040631, train/mean_average_precision=0.211529, validation/accuracy=0.985330, validation/loss=0.050415, validation/mean_average_precision=0.177492, validation/num_examples=43793
I0205 12:21:14.713593 139789433992960 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.027546418830752373, loss=0.044296637177467346
I0205 12:21:46.609427 139788813231872 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.03139783442020416, loss=0.045071668922901154
I0205 12:22:19.137184 139789433992960 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.024592207744717598, loss=0.04199114814400673
I0205 12:22:51.256229 139788813231872 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.01373686920851469, loss=0.04597309231758118
I0205 12:23:23.021414 139789433992960 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.02197682298719883, loss=0.045558374375104904
I0205 12:23:54.937434 139788813231872 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.021471071988344193, loss=0.04147694259881973
I0205 12:24:27.081429 139789433992960 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01900603249669075, loss=0.041745901107788086
I0205 12:24:42.593965 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:26:40.661606 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:26:43.725024 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:26:46.713680 139978932307776 submission_runner.py:408] Time since start: 2673.62s, 	Step: 5250, 	{'train/accuracy': 0.9885719418525696, 'train/loss': 0.03916970640420914, 'train/mean_average_precision': 0.22069786053013857, 'validation/accuracy': 0.98569256067276, 'validation/loss': 0.04870147258043289, 'validation/mean_average_precision': 0.1930046659606876, 'validation/num_examples': 43793, 'test/accuracy': 0.9847843050956726, 'test/loss': 0.05126231163740158, 'test/mean_average_precision': 0.1949999632181195, 'test/num_examples': 43793, 'score': 1693.020597934723, 'total_duration': 2673.617773771286, 'accumulated_submission_time': 1693.020597934723, 'accumulated_eval_time': 980.266857624054, 'accumulated_logging_time': 0.18607282638549805}
I0205 12:26:46.729985 139789425600256 logging_writer.py:48] [5250] accumulated_eval_time=980.266858, accumulated_logging_time=0.186073, accumulated_submission_time=1693.020598, global_step=5250, preemption_count=0, score=1693.020598, test/accuracy=0.984784, test/loss=0.051262, test/mean_average_precision=0.195000, test/num_examples=43793, total_duration=2673.617774, train/accuracy=0.988572, train/loss=0.039170, train/mean_average_precision=0.220698, validation/accuracy=0.985693, validation/loss=0.048701, validation/mean_average_precision=0.193005, validation/num_examples=43793
I0205 12:27:03.228586 139804644198144 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.022263431921601295, loss=0.045071594417095184
I0205 12:27:35.132334 139789425600256 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.022636709734797478, loss=0.03913111984729767
I0205 12:28:07.108370 139804644198144 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.023084476590156555, loss=0.04676273837685585
I0205 12:28:38.823334 139789425600256 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.016988622024655342, loss=0.04099440574645996
I0205 12:29:10.440016 139804644198144 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.023811902850866318, loss=0.04267287626862526
I0205 12:29:42.208746 139789425600256 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.029170837253332138, loss=0.04146794602274895
I0205 12:30:14.061856 139804644198144 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.03139049932360649, loss=0.04026666283607483
I0205 12:30:45.698189 139789425600256 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01619003154337406, loss=0.04304424300789833
I0205 12:30:46.969433 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:32:41.755295 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:32:44.773756 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:32:47.787407 139978932307776 submission_runner.py:408] Time since start: 3034.69s, 	Step: 6005, 	{'train/accuracy': 0.98882657289505, 'train/loss': 0.038102976977825165, 'train/mean_average_precision': 0.24963631312924056, 'validation/accuracy': 0.9858545660972595, 'validation/loss': 0.048232465982437134, 'validation/mean_average_precision': 0.20242980529892032, 'validation/num_examples': 43793, 'test/accuracy': 0.984965443611145, 'test/loss': 0.050924960523843765, 'test/mean_average_precision': 0.2020465953537728, 'test/num_examples': 43793, 'score': 1933.2276899814606, 'total_duration': 3034.691485643387, 'accumulated_submission_time': 1933.2276899814606, 'accumulated_eval_time': 1101.0847754478455, 'accumulated_logging_time': 0.21455097198486328}
I0205 12:32:47.804751 139788813231872 logging_writer.py:48] [6005] accumulated_eval_time=1101.084775, accumulated_logging_time=0.214551, accumulated_submission_time=1933.227690, global_step=6005, preemption_count=0, score=1933.227690, test/accuracy=0.984965, test/loss=0.050925, test/mean_average_precision=0.202047, test/num_examples=43793, total_duration=3034.691486, train/accuracy=0.988827, train/loss=0.038103, train/mean_average_precision=0.249636, validation/accuracy=0.985855, validation/loss=0.048232, validation/mean_average_precision=0.202430, validation/num_examples=43793
I0205 12:33:18.310622 139789433992960 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01603545807301998, loss=0.04370354861021042
I0205 12:33:49.897044 139788813231872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.022015385329723358, loss=0.04020010679960251
I0205 12:34:21.420387 139789433992960 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.019398175179958344, loss=0.04101315140724182
I0205 12:34:53.226257 139788813231872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.015995627269148827, loss=0.04641710966825485
I0205 12:35:24.886497 139789433992960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.015086429193615913, loss=0.04339779168367386
I0205 12:35:56.411174 139788813231872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.015541316010057926, loss=0.040498826652765274
I0205 12:36:28.272133 139789433992960 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.014260352589190006, loss=0.042158227413892746
I0205 12:36:47.901543 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:38:43.783361 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:38:46.945032 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:38:49.937569 139978932307776 submission_runner.py:408] Time since start: 3396.84s, 	Step: 6763, 	{'train/accuracy': 0.9886496067047119, 'train/loss': 0.03849714994430542, 'train/mean_average_precision': 0.2566270749427176, 'validation/accuracy': 0.985815167427063, 'validation/loss': 0.048594117164611816, 'validation/mean_average_precision': 0.2162778550527938, 'validation/num_examples': 43793, 'test/accuracy': 0.9848862290382385, 'test/loss': 0.051545172929763794, 'test/mean_average_precision': 0.21594347952613854, 'test/num_examples': 43793, 'score': 2173.2930114269257, 'total_duration': 3396.8416588306427, 'accumulated_submission_time': 2173.2930114269257, 'accumulated_eval_time': 1223.1207497119904, 'accumulated_logging_time': 0.24296951293945312}
I0205 12:38:49.953845 139804644198144 logging_writer.py:48] [6763] accumulated_eval_time=1223.120750, accumulated_logging_time=0.242970, accumulated_submission_time=2173.293011, global_step=6763, preemption_count=0, score=2173.293011, test/accuracy=0.984886, test/loss=0.051545, test/mean_average_precision=0.215943, test/num_examples=43793, total_duration=3396.841659, train/accuracy=0.988650, train/loss=0.038497, train/mean_average_precision=0.256627, validation/accuracy=0.985815, validation/loss=0.048594, validation/mean_average_precision=0.216278, validation/num_examples=43793
I0205 12:39:02.211656 139916428277504 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.015108725987374783, loss=0.04034256190061569
I0205 12:39:33.945530 139804644198144 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.025606073439121246, loss=0.04381074756383896
I0205 12:40:05.844686 139916428277504 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.019333278760313988, loss=0.04311350733041763
I0205 12:40:37.506188 139804644198144 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.015280983410775661, loss=0.03920670971274376
I0205 12:41:09.511613 139916428277504 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.02391432784497738, loss=0.040130794048309326
I0205 12:41:41.432055 139804644198144 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0169178768992424, loss=0.043373409658670425
I0205 12:42:13.365465 139916428277504 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.02190045639872551, loss=0.04216448590159416
I0205 12:42:45.082958 139804644198144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.014861267991364002, loss=0.04179568961262703
I0205 12:42:50.217344 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:44:37.415542 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:44:40.491212 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:44:43.471073 139978932307776 submission_runner.py:408] Time since start: 3750.38s, 	Step: 7517, 	{'train/accuracy': 0.9889222979545593, 'train/loss': 0.03734235838055611, 'train/mean_average_precision': 0.2822505777691139, 'validation/accuracy': 0.9860039353370667, 'validation/loss': 0.04742446541786194, 'validation/mean_average_precision': 0.23078699973889746, 'validation/num_examples': 43793, 'test/accuracy': 0.9851423501968384, 'test/loss': 0.05007295683026314, 'test/mean_average_precision': 0.22818137385836698, 'test/num_examples': 43793, 'score': 2413.523932695389, 'total_duration': 3750.3751661777496, 'accumulated_submission_time': 2413.523932695389, 'accumulated_eval_time': 1336.374439239502, 'accumulated_logging_time': 0.2714407444000244}
I0205 12:44:43.487753 139789433992960 logging_writer.py:48] [7517] accumulated_eval_time=1336.374439, accumulated_logging_time=0.271441, accumulated_submission_time=2413.523933, global_step=7517, preemption_count=0, score=2413.523933, test/accuracy=0.985142, test/loss=0.050073, test/mean_average_precision=0.228181, test/num_examples=43793, total_duration=3750.375166, train/accuracy=0.988922, train/loss=0.037342, train/mean_average_precision=0.282251, validation/accuracy=0.986004, validation/loss=0.047424, validation/mean_average_precision=0.230787, validation/num_examples=43793
I0205 12:45:10.907062 139804984182528 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.016420695930719376, loss=0.04139995574951172
I0205 12:45:43.029325 139789433992960 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.02023809589445591, loss=0.04184410721063614
I0205 12:46:14.935769 139804984182528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.02556374855339527, loss=0.03948438912630081
I0205 12:46:46.918895 139789433992960 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.015258614905178547, loss=0.04046262428164482
I0205 12:47:19.301567 139804984182528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.022316794842481613, loss=0.04123967885971069
I0205 12:47:51.404486 139789433992960 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01815720647573471, loss=0.04454514756798744
I0205 12:48:23.601485 139804984182528 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.015709372237324715, loss=0.03813912346959114
I0205 12:48:43.597936 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:50:35.679565 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:50:38.737374 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:50:41.775490 139978932307776 submission_runner.py:408] Time since start: 4108.68s, 	Step: 8263, 	{'train/accuracy': 0.989389181137085, 'train/loss': 0.03597596287727356, 'train/mean_average_precision': 0.3080768857675089, 'validation/accuracy': 0.9861922860145569, 'validation/loss': 0.04698283225297928, 'validation/mean_average_precision': 0.22930599938566718, 'validation/num_examples': 43793, 'test/accuracy': 0.9852981567382812, 'test/loss': 0.04968796297907829, 'test/mean_average_precision': 0.23192088239179434, 'test/num_examples': 43793, 'score': 2653.601585626602, 'total_duration': 4108.679584980011, 'accumulated_submission_time': 2653.601585626602, 'accumulated_eval_time': 1454.5519473552704, 'accumulated_logging_time': 0.30036044120788574}
I0205 12:50:41.793351 139789425600256 logging_writer.py:48] [8263] accumulated_eval_time=1454.551947, accumulated_logging_time=0.300360, accumulated_submission_time=2653.601586, global_step=8263, preemption_count=0, score=2653.601586, test/accuracy=0.985298, test/loss=0.049688, test/mean_average_precision=0.231921, test/num_examples=43793, total_duration=4108.679585, train/accuracy=0.989389, train/loss=0.035976, train/mean_average_precision=0.308077, validation/accuracy=0.986192, validation/loss=0.046983, validation/mean_average_precision=0.229306, validation/num_examples=43793
I0205 12:50:53.928357 139817607313152 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.016043856739997864, loss=0.038674529641866684
I0205 12:51:26.006405 139789425600256 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.019234709441661835, loss=0.040968168526887894
I0205 12:51:57.761837 139817607313152 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.032775234431028366, loss=0.04197760298848152
I0205 12:52:30.045866 139789425600256 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02093849517405033, loss=0.04126889631152153
I0205 12:53:01.665555 139817607313152 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.01730281300842762, loss=0.04165980592370033
I0205 12:53:33.517013 139789425600256 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.016846660524606705, loss=0.04112200066447258
I0205 12:54:05.396265 139817607313152 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.016744563356041908, loss=0.040590692311525345
I0205 12:54:37.094713 139789425600256 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01619652286171913, loss=0.04362404718995094
I0205 12:54:41.890801 139978932307776 spec.py:321] Evaluating on the training split.
I0205 12:56:35.521899 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 12:56:38.554022 139978932307776 spec.py:349] Evaluating on the test split.
I0205 12:56:41.534221 139978932307776 submission_runner.py:408] Time since start: 4468.44s, 	Step: 9016, 	{'train/accuracy': 0.9894828796386719, 'train/loss': 0.03531305119395256, 'train/mean_average_precision': 0.3330498346157439, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.04666941240429878, 'validation/mean_average_precision': 0.23346777682977135, 'validation/num_examples': 43793, 'test/accuracy': 0.9853802919387817, 'test/loss': 0.049349281936883926, 'test/mean_average_precision': 0.23346119830653958, 'test/num_examples': 43793, 'score': 2893.6669194698334, 'total_duration': 4468.438313007355, 'accumulated_submission_time': 2893.6669194698334, 'accumulated_eval_time': 1574.195317029953, 'accumulated_logging_time': 0.32975268363952637}
I0205 12:56:41.551230 139789433992960 logging_writer.py:48] [9016] accumulated_eval_time=1574.195317, accumulated_logging_time=0.329753, accumulated_submission_time=2893.666919, global_step=9016, preemption_count=0, score=2893.666919, test/accuracy=0.985380, test/loss=0.049349, test/mean_average_precision=0.233461, test/num_examples=43793, total_duration=4468.438313, train/accuracy=0.989483, train/loss=0.035313, train/mean_average_precision=0.333050, validation/accuracy=0.986210, validation/loss=0.046669, validation/mean_average_precision=0.233468, validation/num_examples=43793
I0205 12:57:08.420711 139804984182528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.017069511115550995, loss=0.04248977452516556
I0205 12:57:40.015021 139789433992960 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.019303927198052406, loss=0.039524637162685394
I0205 12:58:11.730532 139804984182528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01816009171307087, loss=0.043212249875068665
I0205 12:58:43.265799 139789433992960 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.021268382668495178, loss=0.04210979491472244
I0205 12:59:15.206500 139804984182528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.019029095768928528, loss=0.0374695248901844
I0205 12:59:46.765330 139789433992960 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.020607462152838707, loss=0.04197068139910698
I0205 13:00:18.494987 139804984182528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.02042071335017681, loss=0.043063074350357056
I0205 13:00:41.684999 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:02:39.395852 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:02:42.496779 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:02:45.539722 139978932307776 submission_runner.py:408] Time since start: 4832.44s, 	Step: 9774, 	{'train/accuracy': 0.9897928833961487, 'train/loss': 0.0340859591960907, 'train/mean_average_precision': 0.34010763404387956, 'validation/accuracy': 0.9863518476486206, 'validation/loss': 0.04619797319173813, 'validation/mean_average_precision': 0.2403073041110711, 'validation/num_examples': 43793, 'test/accuracy': 0.9855226874351501, 'test/loss': 0.04886767640709877, 'test/mean_average_precision': 0.2379734807150239, 'test/num_examples': 43793, 'score': 3133.7673287391663, 'total_duration': 4832.443814992905, 'accumulated_submission_time': 3133.7673287391663, 'accumulated_eval_time': 1698.0499968528748, 'accumulated_logging_time': 0.35930538177490234}
I0205 13:02:45.557084 139789425600256 logging_writer.py:48] [9774] accumulated_eval_time=1698.049997, accumulated_logging_time=0.359305, accumulated_submission_time=3133.767329, global_step=9774, preemption_count=0, score=3133.767329, test/accuracy=0.985523, test/loss=0.048868, test/mean_average_precision=0.237973, test/num_examples=43793, total_duration=4832.443815, train/accuracy=0.989793, train/loss=0.034086, train/mean_average_precision=0.340108, validation/accuracy=0.986352, validation/loss=0.046198, validation/mean_average_precision=0.240307, validation/num_examples=43793
I0205 13:02:54.116355 139804644198144 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.019867461174726486, loss=0.03804237022995949
I0205 13:03:26.146433 139789425600256 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.017196854576468468, loss=0.03970182687044144
I0205 13:03:57.991675 139804644198144 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.014383595436811447, loss=0.03917425870895386
I0205 13:04:29.827550 139789425600256 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.01485470961779356, loss=0.03852235898375511
I0205 13:05:01.430427 139804644198144 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01846921816468239, loss=0.04142894968390465
I0205 13:05:33.433787 139789425600256 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.013717654161155224, loss=0.037498749792575836
I0205 13:06:05.380488 139804644198144 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01909424550831318, loss=0.04072195664048195
I0205 13:06:37.409203 139789425600256 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.01770750992000103, loss=0.038784246891736984
I0205 13:06:45.657738 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:08:44.944940 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:08:47.963496 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:08:50.961826 139978932307776 submission_runner.py:408] Time since start: 5197.87s, 	Step: 10527, 	{'train/accuracy': 0.9902619123458862, 'train/loss': 0.03265085816383362, 'train/mean_average_precision': 0.3747174656981753, 'validation/accuracy': 0.9864894151687622, 'validation/loss': 0.04541108012199402, 'validation/mean_average_precision': 0.24395419394193796, 'validation/num_examples': 43793, 'test/accuracy': 0.9856115579605103, 'test/loss': 0.04801317676901817, 'test/mean_average_precision': 0.24375515934203035, 'test/num_examples': 43793, 'score': 3373.8351554870605, 'total_duration': 5197.865916728973, 'accumulated_submission_time': 3373.8351554870605, 'accumulated_eval_time': 1823.354031085968, 'accumulated_logging_time': 0.38909125328063965}
I0205 13:08:50.979791 139804984182528 logging_writer.py:48] [10527] accumulated_eval_time=1823.354031, accumulated_logging_time=0.389091, accumulated_submission_time=3373.835155, global_step=10527, preemption_count=0, score=3373.835155, test/accuracy=0.985612, test/loss=0.048013, test/mean_average_precision=0.243755, test/num_examples=43793, total_duration=5197.865917, train/accuracy=0.990262, train/loss=0.032651, train/mean_average_precision=0.374717, validation/accuracy=0.986489, validation/loss=0.045411, validation/mean_average_precision=0.243954, validation/num_examples=43793
I0205 13:09:14.694382 139817607313152 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.021079102531075478, loss=0.03861311078071594
I0205 13:09:46.692072 139804984182528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.02322309836745262, loss=0.03766637668013573
I0205 13:10:18.792293 139817607313152 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.015113803558051586, loss=0.03862318396568298
I0205 13:10:50.693026 139804984182528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.014202920719981194, loss=0.03955518826842308
I0205 13:11:22.656551 139817607313152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.01724853925406933, loss=0.03895359858870506
I0205 13:11:54.456759 139804984182528 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01655345968902111, loss=0.039070580154657364
I0205 13:12:26.541872 139817607313152 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.015010716393589973, loss=0.03620711714029312
I0205 13:12:51.231547 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:14:46.585637 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:14:49.658975 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:14:52.613247 139978932307776 submission_runner.py:408] Time since start: 5559.52s, 	Step: 11279, 	{'train/accuracy': 0.9901916980743408, 'train/loss': 0.03254377841949463, 'train/mean_average_precision': 0.39628964259491495, 'validation/accuracy': 0.9865381717681885, 'validation/loss': 0.04554063454270363, 'validation/mean_average_precision': 0.24805487450052172, 'validation/num_examples': 43793, 'test/accuracy': 0.985668420791626, 'test/loss': 0.04814745858311653, 'test/mean_average_precision': 0.24561580856497117, 'test/num_examples': 43793, 'score': 3614.053644180298, 'total_duration': 5559.517339468002, 'accumulated_submission_time': 3614.053644180298, 'accumulated_eval_time': 1944.7356841564178, 'accumulated_logging_time': 0.41932129859924316}
I0205 13:14:52.630622 139789425600256 logging_writer.py:48] [11279] accumulated_eval_time=1944.735684, accumulated_logging_time=0.419321, accumulated_submission_time=3614.053644, global_step=11279, preemption_count=0, score=3614.053644, test/accuracy=0.985668, test/loss=0.048147, test/mean_average_precision=0.245616, test/num_examples=43793, total_duration=5559.517339, train/accuracy=0.990192, train/loss=0.032544, train/mean_average_precision=0.396290, validation/accuracy=0.986538, validation/loss=0.045541, validation/mean_average_precision=0.248055, validation/num_examples=43793
I0205 13:14:59.583767 139804644198144 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.016291270032525063, loss=0.03969345986843109
I0205 13:15:31.555467 139789425600256 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0164725910872221, loss=0.03874444589018822
I0205 13:16:03.398816 139804644198144 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.017022913321852684, loss=0.03954711928963661
I0205 13:16:35.355977 139789425600256 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.01595667563378811, loss=0.039349038153886795
I0205 13:17:07.004485 139804644198144 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.017549782991409302, loss=0.04026198014616966
I0205 13:17:38.701504 139789425600256 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.01618872582912445, loss=0.0359712652862072
I0205 13:18:10.617472 139804644198144 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.02228165976703167, loss=0.040026646107435226
I0205 13:18:42.930460 139789425600256 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.01432813424617052, loss=0.03851814940571785
I0205 13:18:52.867944 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:20:51.182958 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:20:54.226375 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:20:57.255685 139978932307776 submission_runner.py:408] Time since start: 5924.16s, 	Step: 12032, 	{'train/accuracy': 0.9903891086578369, 'train/loss': 0.0321890227496624, 'train/mean_average_precision': 0.3812690287674594, 'validation/accuracy': 0.986594557762146, 'validation/loss': 0.04520021006464958, 'validation/mean_average_precision': 0.2515214649864786, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04781940206885338, 'test/mean_average_precision': 0.24914318495248688, 'test/num_examples': 43793, 'score': 3854.257782936096, 'total_duration': 5924.159775972366, 'accumulated_submission_time': 3854.257782936096, 'accumulated_eval_time': 2069.1233773231506, 'accumulated_logging_time': 0.4490396976470947}
I0205 13:20:57.274656 139789433992960 logging_writer.py:48] [12032] accumulated_eval_time=2069.123377, accumulated_logging_time=0.449040, accumulated_submission_time=3854.257783, global_step=12032, preemption_count=0, score=3854.257783, test/accuracy=0.985786, test/loss=0.047819, test/mean_average_precision=0.249143, test/num_examples=43793, total_duration=5924.159776, train/accuracy=0.990389, train/loss=0.032189, train/mean_average_precision=0.381269, validation/accuracy=0.986595, validation/loss=0.045200, validation/mean_average_precision=0.251521, validation/num_examples=43793
I0205 13:21:19.755053 139817607313152 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.02162441797554493, loss=0.03764300420880318
I0205 13:21:51.711758 139789433992960 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.01615661010146141, loss=0.037490420043468475
I0205 13:22:24.360500 139817607313152 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.017718013375997543, loss=0.038926269859075546
I0205 13:22:56.972733 139789433992960 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.015571868978440762, loss=0.03674113377928734
I0205 13:23:29.255548 139817607313152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.01713266223669052, loss=0.038685813546180725
I0205 13:24:01.445822 139789433992960 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.020523257553577423, loss=0.03633302077651024
I0205 13:24:33.826519 139817607313152 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02118736505508423, loss=0.041590817272663116
I0205 13:24:57.289938 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:26:49.907636 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:26:53.757063 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:26:57.527161 139978932307776 submission_runner.py:408] Time since start: 6284.43s, 	Step: 12773, 	{'train/accuracy': 0.9904727935791016, 'train/loss': 0.03187404200434685, 'train/mean_average_precision': 0.38753860292977615, 'validation/accuracy': 0.9866684675216675, 'validation/loss': 0.04524100571870804, 'validation/mean_average_precision': 0.25419772745581926, 'validation/num_examples': 43793, 'test/accuracy': 0.9857690334320068, 'test/loss': 0.047946419566869736, 'test/mean_average_precision': 0.2452384303975356, 'test/num_examples': 43793, 'score': 4094.240201473236, 'total_duration': 6284.43124961853, 'accumulated_submission_time': 4094.240201473236, 'accumulated_eval_time': 2189.3605513572693, 'accumulated_logging_time': 0.48015785217285156}
I0205 13:26:57.545060 139789425600256 logging_writer.py:48] [12773] accumulated_eval_time=2189.360551, accumulated_logging_time=0.480158, accumulated_submission_time=4094.240201, global_step=12773, preemption_count=0, score=4094.240201, test/accuracy=0.985769, test/loss=0.047946, test/mean_average_precision=0.245238, test/num_examples=43793, total_duration=6284.431250, train/accuracy=0.990473, train/loss=0.031874, train/mean_average_precision=0.387539, validation/accuracy=0.986668, validation/loss=0.045241, validation/mean_average_precision=0.254198, validation/num_examples=43793
I0205 13:27:06.570619 139804644198144 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.017425378784537315, loss=0.03691290318965912
I0205 13:27:38.825985 139789425600256 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.018276456743478775, loss=0.03701240196824074
I0205 13:28:10.989334 139804644198144 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.01644996367394924, loss=0.03769586607813835
I0205 13:28:42.985107 139789425600256 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.022786663845181465, loss=0.03456633538007736
I0205 13:29:15.103734 139804644198144 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.020617390051484108, loss=0.03656230494379997
I0205 13:29:49.001052 139789425600256 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.019500702619552612, loss=0.03826737776398659
I0205 13:30:21.822281 139804644198144 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.018664751201868057, loss=0.03781936317682266
I0205 13:30:53.691053 139789425600256 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.024156952276825905, loss=0.03840741887688637
I0205 13:30:57.775081 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:32:57.916217 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:33:00.996131 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:33:04.111257 139978932307776 submission_runner.py:408] Time since start: 6651.02s, 	Step: 13514, 	{'train/accuracy': 0.9906947016716003, 'train/loss': 0.030974794179201126, 'train/mean_average_precision': 0.4179961264701088, 'validation/accuracy': 0.9867110848426819, 'validation/loss': 0.044889673590660095, 'validation/mean_average_precision': 0.2612030305011974, 'validation/num_examples': 43793, 'test/accuracy': 0.9857829809188843, 'test/loss': 0.04765735939145088, 'test/mean_average_precision': 0.24666270382631333, 'test/num_examples': 43793, 'score': 4334.438627481461, 'total_duration': 6651.01534485817, 'accumulated_submission_time': 4334.438627481461, 'accumulated_eval_time': 2315.6966729164124, 'accumulated_logging_time': 0.509023904800415}
I0205 13:33:04.129906 139789433992960 logging_writer.py:48] [13514] accumulated_eval_time=2315.696673, accumulated_logging_time=0.509024, accumulated_submission_time=4334.438627, global_step=13514, preemption_count=0, score=4334.438627, test/accuracy=0.985783, test/loss=0.047657, test/mean_average_precision=0.246663, test/num_examples=43793, total_duration=6651.015345, train/accuracy=0.990695, train/loss=0.030975, train/mean_average_precision=0.417996, validation/accuracy=0.986711, validation/loss=0.044890, validation/mean_average_precision=0.261203, validation/num_examples=43793
I0205 13:33:31.955254 139804984182528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.016378652304410934, loss=0.03919015824794769
I0205 13:34:04.112078 139789433992960 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.01656990870833397, loss=0.03524509817361832
I0205 13:34:36.322509 139804984182528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.019519101828336716, loss=0.035864535719156265
I0205 13:35:08.251156 139789433992960 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.01965167559683323, loss=0.04063309356570244
I0205 13:35:40.325650 139804984182528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.020348163321614265, loss=0.035531677305698395
I0205 13:36:12.331523 139789433992960 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.019324973225593567, loss=0.03703884407877922
I0205 13:36:44.152151 139804984182528 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.024472832679748535, loss=0.04218027740716934
I0205 13:37:04.360104 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:38:59.171812 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:39:02.309995 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:39:05.335709 139978932307776 submission_runner.py:408] Time since start: 7012.24s, 	Step: 14263, 	{'train/accuracy': 0.9907548427581787, 'train/loss': 0.03045141138136387, 'train/mean_average_precision': 0.43818958080159154, 'validation/accuracy': 0.9867098927497864, 'validation/loss': 0.045234255492687225, 'validation/mean_average_precision': 0.26432332057559815, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04785768315196037, 'test/mean_average_precision': 0.2506972280003634, 'test/num_examples': 43793, 'score': 4574.635347366333, 'total_duration': 7012.2398047447205, 'accumulated_submission_time': 4574.635347366333, 'accumulated_eval_time': 2436.6722359657288, 'accumulated_logging_time': 0.5402529239654541}
I0205 13:39:05.354144 139804644198144 logging_writer.py:48] [14263] accumulated_eval_time=2436.672236, accumulated_logging_time=0.540253, accumulated_submission_time=4574.635347, global_step=14263, preemption_count=0, score=4574.635347, test/accuracy=0.985802, test/loss=0.047858, test/mean_average_precision=0.250697, test/num_examples=43793, total_duration=7012.239805, train/accuracy=0.990755, train/loss=0.030451, train/mean_average_precision=0.438190, validation/accuracy=0.986710, validation/loss=0.045234, validation/mean_average_precision=0.264323, validation/num_examples=43793
I0205 13:39:17.580666 139817607313152 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.022452741861343384, loss=0.036371126770973206
I0205 13:39:49.221640 139804644198144 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.026577109470963478, loss=0.04122021794319153
I0205 13:40:20.951756 139817607313152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.02180931344628334, loss=0.03661195933818817
I0205 13:40:52.702058 139804644198144 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.018349232152104378, loss=0.03761677443981171
I0205 13:41:24.640723 139817607313152 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.016337774693965912, loss=0.037405237555503845
I0205 13:41:57.048012 139804644198144 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.018694216385483742, loss=0.03361193835735321
I0205 13:42:29.595865 139817607313152 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02506939508020878, loss=0.03840057551860809
I0205 13:43:01.268536 139804644198144 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.021960992366075516, loss=0.03709794953465462
I0205 13:43:05.462680 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:44:56.764571 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:44:59.862199 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:45:02.961861 139978932307776 submission_runner.py:408] Time since start: 7369.87s, 	Step: 15014, 	{'train/accuracy': 0.9909887313842773, 'train/loss': 0.02982170321047306, 'train/mean_average_precision': 0.4313697479562325, 'validation/accuracy': 0.9867955446243286, 'validation/loss': 0.044983476400375366, 'validation/mean_average_precision': 0.2626834619716003, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04784674197435379, 'test/mean_average_precision': 0.24305608194623227, 'test/num_examples': 43793, 'score': 4814.710758686066, 'total_duration': 7369.865949630737, 'accumulated_submission_time': 4814.710758686066, 'accumulated_eval_time': 2554.1713659763336, 'accumulated_logging_time': 0.5711688995361328}
I0205 13:45:02.980704 139789425600256 logging_writer.py:48] [15014] accumulated_eval_time=2554.171366, accumulated_logging_time=0.571169, accumulated_submission_time=4814.710759, global_step=15014, preemption_count=0, score=4814.710759, test/accuracy=0.985859, test/loss=0.047847, test/mean_average_precision=0.243056, test/num_examples=43793, total_duration=7369.865950, train/accuracy=0.990989, train/loss=0.029822, train/mean_average_precision=0.431370, validation/accuracy=0.986796, validation/loss=0.044983, validation/mean_average_precision=0.262683, validation/num_examples=43793
I0205 13:45:30.677866 139789433992960 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.02059735730290413, loss=0.03440587967634201
I0205 13:46:02.482472 139789425600256 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.020615512505173683, loss=0.036397479474544525
I0205 13:46:34.238691 139789433992960 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.020708132535219193, loss=0.03948717564344406
I0205 13:47:06.010121 139789425600256 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.018015597015619278, loss=0.03546159341931343
I0205 13:47:37.930591 139789433992960 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.02545979991555214, loss=0.03693009912967682
I0205 13:48:09.823444 139789425600256 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.02037866972386837, loss=0.037478070706129074
I0205 13:48:41.342049 139789433992960 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.023755993694067, loss=0.038346946239471436
I0205 13:49:03.080699 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:50:53.739814 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:50:56.791396 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:50:59.803872 139978932307776 submission_runner.py:408] Time since start: 7726.71s, 	Step: 15769, 	{'train/accuracy': 0.9910573363304138, 'train/loss': 0.02915920689702034, 'train/mean_average_precision': 0.4634965915873742, 'validation/accuracy': 0.9866985082626343, 'validation/loss': 0.045393701642751694, 'validation/mean_average_precision': 0.2624384497477145, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04817233234643936, 'test/mean_average_precision': 0.24892799014666894, 'test/num_examples': 43793, 'score': 5054.779027462006, 'total_duration': 7726.707957744598, 'accumulated_submission_time': 5054.779027462006, 'accumulated_eval_time': 2670.8944869041443, 'accumulated_logging_time': 0.6007318496704102}
I0205 13:50:59.821728 139788813231872 logging_writer.py:48] [15769] accumulated_eval_time=2670.894487, accumulated_logging_time=0.600732, accumulated_submission_time=5054.779027, global_step=15769, preemption_count=0, score=5054.779027, test/accuracy=0.985830, test/loss=0.048172, test/mean_average_precision=0.248928, test/num_examples=43793, total_duration=7726.707958, train/accuracy=0.991057, train/loss=0.029159, train/mean_average_precision=0.463497, validation/accuracy=0.986699, validation/loss=0.045394, validation/mean_average_precision=0.262438, validation/num_examples=43793
I0205 13:51:10.233227 139804644198144 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.020963335409760475, loss=0.035785507410764694
I0205 13:51:42.366974 139788813231872 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.02229137159883976, loss=0.03502066433429718
I0205 13:52:14.336972 139804644198144 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.018666256219148636, loss=0.0327497161924839
I0205 13:52:46.383328 139788813231872 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.026406822726130486, loss=0.03873373195528984
I0205 13:53:18.369506 139804644198144 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.02543848752975464, loss=0.035491861402988434
I0205 13:53:49.957848 139788813231872 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.023075882345438004, loss=0.03318050876259804
I0205 13:54:21.710474 139804644198144 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.025970065966248512, loss=0.03831975907087326
I0205 13:54:53.489691 139788813231872 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.021748533472418785, loss=0.03231566771864891
I0205 13:54:59.825702 139978932307776 spec.py:321] Evaluating on the training split.
I0205 13:56:52.979426 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 13:56:56.025882 139978932307776 spec.py:349] Evaluating on the test split.
I0205 13:56:59.054511 139978932307776 submission_runner.py:408] Time since start: 8085.96s, 	Step: 16521, 	{'train/accuracy': 0.9913237690925598, 'train/loss': 0.02804555930197239, 'train/mean_average_precision': 0.49196843643775773, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.04596276208758354, 'validation/mean_average_precision': 0.2659343145663652, 'validation/num_examples': 43793, 'test/accuracy': 0.9859126806259155, 'test/loss': 0.04872153699398041, 'test/mean_average_precision': 0.255597488873233, 'test/num_examples': 43793, 'score': 5294.750148057938, 'total_duration': 8085.958600759506, 'accumulated_submission_time': 5294.750148057938, 'accumulated_eval_time': 2790.1232488155365, 'accumulated_logging_time': 0.6307599544525146}
I0205 13:56:59.073210 139789425600256 logging_writer.py:48] [16521] accumulated_eval_time=2790.123249, accumulated_logging_time=0.630760, accumulated_submission_time=5294.750148, global_step=16521, preemption_count=0, score=5294.750148, test/accuracy=0.985913, test/loss=0.048722, test/mean_average_precision=0.255597, test/num_examples=43793, total_duration=8085.958601, train/accuracy=0.991324, train/loss=0.028046, train/mean_average_precision=0.491968, validation/accuracy=0.986754, validation/loss=0.045963, validation/mean_average_precision=0.265934, validation/num_examples=43793
I0205 13:57:25.005222 139804984182528 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.026553155854344368, loss=0.03500020131468773
I0205 13:57:57.167892 139789425600256 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.027117304503917694, loss=0.03699837625026703
I0205 13:58:29.352728 139804984182528 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.022094851359725, loss=0.035737745463848114
I0205 13:59:01.274190 139789425600256 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.023088732734322548, loss=0.03547672927379608
I0205 13:59:33.056626 139804984182528 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.022194501012563705, loss=0.03619827702641487
I0205 14:00:04.919083 139789425600256 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.023829881101846695, loss=0.03537052124738693
I0205 14:00:36.727227 139804984182528 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.022145146504044533, loss=0.03433243930339813
I0205 14:00:59.109160 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:02:54.629732 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:02:57.701658 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:03:00.714791 139978932307776 submission_runner.py:408] Time since start: 8447.62s, 	Step: 17272, 	{'train/accuracy': 0.9916371703147888, 'train/loss': 0.027226930484175682, 'train/mean_average_precision': 0.5029472729700839, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04549722000956535, 'validation/mean_average_precision': 0.26105591706887715, 'validation/num_examples': 43793, 'test/accuracy': 0.9858415126800537, 'test/loss': 0.04827072098851204, 'test/mean_average_precision': 0.250724034154077, 'test/num_examples': 43793, 'score': 5534.754784345627, 'total_duration': 8447.618882656097, 'accumulated_submission_time': 5534.754784345627, 'accumulated_eval_time': 2911.7288358211517, 'accumulated_logging_time': 0.6605877876281738}
I0205 14:03:00.734001 139788813231872 logging_writer.py:48] [17272] accumulated_eval_time=2911.728836, accumulated_logging_time=0.660588, accumulated_submission_time=5534.754784, global_step=17272, preemption_count=0, score=5534.754784, test/accuracy=0.985842, test/loss=0.048271, test/mean_average_precision=0.250724, test/num_examples=43793, total_duration=8447.618883, train/accuracy=0.991637, train/loss=0.027227, train/mean_average_precision=0.502947, validation/accuracy=0.986781, validation/loss=0.045497, validation/mean_average_precision=0.261056, validation/num_examples=43793
I0205 14:03:10.052149 139789433992960 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.02279258333146572, loss=0.03534958139061928
I0205 14:03:41.857002 139788813231872 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.027661550790071487, loss=0.034304387867450714
I0205 14:04:13.907947 139789433992960 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.026699164882302284, loss=0.034651193767786026
I0205 14:04:45.964599 139788813231872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02559102699160576, loss=0.03521040081977844
I0205 14:05:18.081268 139789433992960 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0225216057151556, loss=0.03190569579601288
I0205 14:05:49.766748 139788813231872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.0246124304831028, loss=0.03333122283220291
I0205 14:06:21.559458 139789433992960 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.02165866456925869, loss=0.035267628729343414
I0205 14:06:53.866680 139788813231872 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.02418845146894455, loss=0.03598378971219063
I0205 14:07:00.831739 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:08:52.743831 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:08:55.759876 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:08:58.748239 139978932307776 submission_runner.py:408] Time since start: 8805.65s, 	Step: 18023, 	{'train/accuracy': 0.9916689991950989, 'train/loss': 0.02668984793126583, 'train/mean_average_precision': 0.5299052978509329, 'validation/accuracy': 0.98673015832901, 'validation/loss': 0.04590317979454994, 'validation/mean_average_precision': 0.2613781244846068, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04870419576764107, 'test/mean_average_precision': 0.249591743371493, 'test/num_examples': 43793, 'score': 5774.820902109146, 'total_duration': 8805.652330636978, 'accumulated_submission_time': 5774.820902109146, 'accumulated_eval_time': 3029.6452848911285, 'accumulated_logging_time': 0.6906979084014893}
I0205 14:08:58.767954 139789425600256 logging_writer.py:48] [18023] accumulated_eval_time=3029.645285, accumulated_logging_time=0.690698, accumulated_submission_time=5774.820902, global_step=18023, preemption_count=0, score=5774.820902, test/accuracy=0.985922, test/loss=0.048704, test/mean_average_precision=0.249592, test/num_examples=43793, total_duration=8805.652331, train/accuracy=0.991669, train/loss=0.026690, train/mean_average_precision=0.529905, validation/accuracy=0.986730, validation/loss=0.045903, validation/mean_average_precision=0.261378, validation/num_examples=43793
I0205 14:09:23.513545 139804984182528 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.031742315739393234, loss=0.034720756113529205
I0205 14:09:54.772725 139789425600256 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.024233095347881317, loss=0.03668072074651718
I0205 14:10:26.136472 139804984182528 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.03057805448770523, loss=0.03569745644927025
I0205 14:10:57.767014 139789425600256 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.022842874750494957, loss=0.03351738676428795
I0205 14:11:29.437029 139804984182528 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.023383822292089462, loss=0.033288389444351196
I0205 14:12:01.213871 139789425600256 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.024191964417696, loss=0.03583899140357971
I0205 14:12:32.994440 139804984182528 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.02484952285885811, loss=0.032245367765426636
I0205 14:12:58.881207 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:14:51.913810 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:14:54.968008 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:14:58.003185 139978932307776 submission_runner.py:408] Time since start: 9164.91s, 	Step: 18783, 	{'train/accuracy': 0.9919673204421997, 'train/loss': 0.02644052729010582, 'train/mean_average_precision': 0.5123992699519384, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.04558531567454338, 'validation/mean_average_precision': 0.2624033645561057, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.048399362713098526, 'test/mean_average_precision': 0.25564902037549775, 'test/num_examples': 43793, 'score': 6014.902328491211, 'total_duration': 9164.90726852417, 'accumulated_submission_time': 6014.902328491211, 'accumulated_eval_time': 3148.767208337784, 'accumulated_logging_time': 0.7213609218597412}
I0205 14:14:58.022293 139788813231872 logging_writer.py:48] [18783] accumulated_eval_time=3148.767208, accumulated_logging_time=0.721361, accumulated_submission_time=6014.902328, global_step=18783, preemption_count=0, score=6014.902328, test/accuracy=0.985991, test/loss=0.048399, test/mean_average_precision=0.255649, test/num_examples=43793, total_duration=9164.907269, train/accuracy=0.991967, train/loss=0.026441, train/mean_average_precision=0.512399, validation/accuracy=0.986888, validation/loss=0.045585, validation/mean_average_precision=0.262403, validation/num_examples=43793
I0205 14:15:03.851464 139789433992960 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.02634437382221222, loss=0.034603457897901535
I0205 14:15:35.854822 139788813231872 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.02471024915575981, loss=0.034334972500801086
I0205 14:16:07.672477 139789433992960 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.027950741350650787, loss=0.03440506383776665
I0205 14:16:39.414417 139788813231872 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.02251068875193596, loss=0.0358089916408062
I0205 14:17:11.003583 139789433992960 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.025120384991168976, loss=0.036236368119716644
I0205 14:17:42.856938 139788813231872 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.02397395297884941, loss=0.035358186811208725
I0205 14:18:14.967619 139789433992960 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.022738657891750336, loss=0.03510914370417595
I0205 14:18:47.068692 139788813231872 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.025897622108459473, loss=0.033061448484659195
I0205 14:18:58.225094 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:20:52.066226 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:20:55.071391 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:20:58.073728 139978932307776 submission_runner.py:408] Time since start: 9524.98s, 	Step: 19536, 	{'train/accuracy': 0.9915534853935242, 'train/loss': 0.027401180937886238, 'train/mean_average_precision': 0.4995214715325733, 'validation/accuracy': 0.9868194460868835, 'validation/loss': 0.04584503918886185, 'validation/mean_average_precision': 0.26198630246261123, 'validation/num_examples': 43793, 'test/accuracy': 0.9859030246734619, 'test/loss': 0.048729538917541504, 'test/mean_average_precision': 0.24923894633275256, 'test/num_examples': 43793, 'score': 6255.073797464371, 'total_duration': 9524.977820158005, 'accumulated_submission_time': 6255.073797464371, 'accumulated_eval_time': 3268.615795850754, 'accumulated_logging_time': 0.7516143321990967}
I0205 14:20:58.092979 139789425600256 logging_writer.py:48] [19536] accumulated_eval_time=3268.615796, accumulated_logging_time=0.751614, accumulated_submission_time=6255.073797, global_step=19536, preemption_count=0, score=6255.073797, test/accuracy=0.985903, test/loss=0.048730, test/mean_average_precision=0.249239, test/num_examples=43793, total_duration=9524.977820, train/accuracy=0.991553, train/loss=0.027401, train/mean_average_precision=0.499521, validation/accuracy=0.986819, validation/loss=0.045845, validation/mean_average_precision=0.261986, validation/num_examples=43793
I0205 14:21:19.207924 139804644198144 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.024463769048452377, loss=0.033791281282901764
I0205 14:21:51.291440 139789425600256 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.024578027427196503, loss=0.03189247101545334
I0205 14:22:23.492924 139804644198144 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.024158673360943794, loss=0.03254084289073944
I0205 14:22:55.788113 139789425600256 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.024289147928357124, loss=0.03346819803118706
I0205 14:23:27.568451 139804644198144 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.022796152159571648, loss=0.03214479982852936
I0205 14:23:59.360990 139789425600256 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.031388234347105026, loss=0.03318250924348831
I0205 14:24:31.374262 139804644198144 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.028189584612846375, loss=0.030662769451737404
I0205 14:24:58.264346 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:26:48.340195 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:26:51.388923 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:26:54.401987 139978932307776 submission_runner.py:408] Time since start: 9881.31s, 	Step: 20286, 	{'train/accuracy': 0.9918997883796692, 'train/loss': 0.026583749800920486, 'train/mean_average_precision': 0.5098556117578865, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04583797976374626, 'validation/mean_average_precision': 0.2618027021554986, 'validation/num_examples': 43793, 'test/accuracy': 0.9859623908996582, 'test/loss': 0.04834187030792236, 'test/mean_average_precision': 0.2521318678421627, 'test/num_examples': 43793, 'score': 6495.213454723358, 'total_duration': 9881.306076049805, 'accumulated_submission_time': 6495.213454723358, 'accumulated_eval_time': 3384.753395795822, 'accumulated_logging_time': 0.7820303440093994}
I0205 14:26:54.422131 139788813231872 logging_writer.py:48] [20286] accumulated_eval_time=3384.753396, accumulated_logging_time=0.782030, accumulated_submission_time=6495.213455, global_step=20286, preemption_count=0, score=6495.213455, test/accuracy=0.985962, test/loss=0.048342, test/mean_average_precision=0.252132, test/num_examples=43793, total_duration=9881.306076, train/accuracy=0.991900, train/loss=0.026584, train/mean_average_precision=0.509856, validation/accuracy=0.986755, validation/loss=0.045838, validation/mean_average_precision=0.261803, validation/num_examples=43793
I0205 14:26:59.251791 139789433992960 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.03022460639476776, loss=0.03363698348402977
I0205 14:27:30.963679 139788813231872 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.02371494099497795, loss=0.03364063799381256
I0205 14:28:02.707745 139789433992960 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.032122042030096054, loss=0.035195693373680115
I0205 14:28:34.669158 139788813231872 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.02162093296647072, loss=0.028878550976514816
I0205 14:29:06.437043 139789433992960 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.024045715108513832, loss=0.034928735345602036
I0205 14:29:38.934101 139788813231872 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.0265284962952137, loss=0.032436247915029526
I0205 14:30:10.971481 139789433992960 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.02902534045279026, loss=0.03323293477296829
I0205 14:30:42.638897 139788813231872 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.038997601717710495, loss=0.03410622477531433
I0205 14:30:54.539746 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:32:51.078195 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:32:54.163070 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:32:57.203902 139978932307776 submission_runner.py:408] Time since start: 10244.11s, 	Step: 21038, 	{'train/accuracy': 0.9918867945671082, 'train/loss': 0.02638249844312668, 'train/mean_average_precision': 0.5210673442668449, 'validation/accuracy': 0.9867756366729736, 'validation/loss': 0.046111878007650375, 'validation/mean_average_precision': 0.2637069541466819, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04888920113444328, 'test/mean_average_precision': 0.2506835965684331, 'test/num_examples': 43793, 'score': 6735.299637794495, 'total_duration': 10244.107994318008, 'accumulated_submission_time': 6735.299637794495, 'accumulated_eval_time': 3507.4175040721893, 'accumulated_logging_time': 0.8135173320770264}
I0205 14:32:57.223188 139804644198144 logging_writer.py:48] [21038] accumulated_eval_time=3507.417504, accumulated_logging_time=0.813517, accumulated_submission_time=6735.299638, global_step=21038, preemption_count=0, score=6735.299638, test/accuracy=0.985906, test/loss=0.048889, test/mean_average_precision=0.250684, test/num_examples=43793, total_duration=10244.107994, train/accuracy=0.991887, train/loss=0.026382, train/mean_average_precision=0.521067, validation/accuracy=0.986776, validation/loss=0.046112, validation/mean_average_precision=0.263707, validation/num_examples=43793
I0205 14:33:17.715474 139804984182528 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.026643672958016396, loss=0.034845661371946335
I0205 14:33:50.168434 139804644198144 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.027596089988946915, loss=0.03287038952112198
I0205 14:34:22.356217 139804984182528 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0256176870316267, loss=0.03198981285095215
I0205 14:34:54.375914 139804644198144 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.02473207376897335, loss=0.033164482563734055
I0205 14:35:26.872620 139804984182528 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.03687034174799919, loss=0.036107320338487625
I0205 14:35:59.459952 139804644198144 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.027879634872078896, loss=0.03398482874035835
I0205 14:36:31.796762 139804984182528 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.029592733830213547, loss=0.030393457040190697
I0205 14:36:57.328005 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:38:51.230209 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:38:54.313627 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:38:57.404279 139978932307776 submission_runner.py:408] Time since start: 10604.31s, 	Step: 21776, 	{'train/accuracy': 0.9919347763061523, 'train/loss': 0.02610335312783718, 'train/mean_average_precision': 0.5254924327822671, 'validation/accuracy': 0.9867740273475647, 'validation/loss': 0.04628313332796097, 'validation/mean_average_precision': 0.26190666460667533, 'validation/num_examples': 43793, 'test/accuracy': 0.985910177230835, 'test/loss': 0.04922151193022728, 'test/mean_average_precision': 0.25348572629288885, 'test/num_examples': 43793, 'score': 6975.372453927994, 'total_duration': 10604.308366298676, 'accumulated_submission_time': 6975.372453927994, 'accumulated_eval_time': 3627.493724346161, 'accumulated_logging_time': 0.8439273834228516}
I0205 14:38:57.424555 139789425600256 logging_writer.py:48] [21776] accumulated_eval_time=3627.493724, accumulated_logging_time=0.843927, accumulated_submission_time=6975.372454, global_step=21776, preemption_count=0, score=6975.372454, test/accuracy=0.985910, test/loss=0.049222, test/mean_average_precision=0.253486, test/num_examples=43793, total_duration=10604.308366, train/accuracy=0.991935, train/loss=0.026103, train/mean_average_precision=0.525492, validation/accuracy=0.986774, validation/loss=0.046283, validation/mean_average_precision=0.261907, validation/num_examples=43793
I0205 14:39:05.614972 139789433992960 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.03080652840435505, loss=0.03371083363890648
I0205 14:39:37.731123 139789425600256 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.029158122837543488, loss=0.035533443093299866
I0205 14:40:09.569746 139789433992960 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.034427374601364136, loss=0.03459316864609718
I0205 14:40:42.155772 139789425600256 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.05021727457642555, loss=0.035532303154468536
I0205 14:41:14.266050 139789433992960 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03208506852388382, loss=0.03386620432138443
I0205 14:41:46.836157 139789425600256 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.02668941207230091, loss=0.032183319330215454
I0205 14:42:19.146313 139789433992960 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.03533625230193138, loss=0.034115128219127655
I0205 14:42:51.033030 139789425600256 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.028100550174713135, loss=0.03290913626551628
I0205 14:42:57.492085 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:44:48.139871 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:44:51.215673 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:44:54.216199 139978932307776 submission_runner.py:408] Time since start: 10961.12s, 	Step: 22521, 	{'train/accuracy': 0.9922031760215759, 'train/loss': 0.025213543325662613, 'train/mean_average_precision': 0.5488126686446302, 'validation/accuracy': 0.9866769909858704, 'validation/loss': 0.04680300131440163, 'validation/mean_average_precision': 0.256033160416531, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04948793724179268, 'test/mean_average_precision': 0.24698618209813553, 'test/num_examples': 43793, 'score': 7215.407013654709, 'total_duration': 10961.120171308517, 'accumulated_submission_time': 7215.407013654709, 'accumulated_eval_time': 3744.2176728248596, 'accumulated_logging_time': 0.8761856555938721}
I0205 14:44:54.237195 139788813231872 logging_writer.py:48] [22521] accumulated_eval_time=3744.217673, accumulated_logging_time=0.876186, accumulated_submission_time=7215.407014, global_step=22521, preemption_count=0, score=7215.407014, test/accuracy=0.985882, test/loss=0.049488, test/mean_average_precision=0.246986, test/num_examples=43793, total_duration=10961.120171, train/accuracy=0.992203, train/loss=0.025214, train/mean_average_precision=0.548813, validation/accuracy=0.986677, validation/loss=0.046803, validation/mean_average_precision=0.256033, validation/num_examples=43793
I0205 14:45:20.017688 139804644198144 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.02950400672852993, loss=0.03470408916473389
I0205 14:45:52.024309 139788813231872 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.028848659247159958, loss=0.031138289719820023
I0205 14:46:24.006095 139804644198144 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.03480878099799156, loss=0.0343792662024498
I0205 14:46:56.139502 139788813231872 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.029713552445173264, loss=0.031861912459135056
I0205 14:47:28.217274 139804644198144 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.03239389508962631, loss=0.03054012730717659
I0205 14:47:59.808444 139788813231872 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.03332696482539177, loss=0.032091010361909866
I0205 14:48:31.811544 139804644198144 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.033997196704149246, loss=0.03311995044350624
I0205 14:48:54.234468 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:50:40.567769 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:50:43.565387 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:50:46.571766 139978932307776 submission_runner.py:408] Time since start: 11313.48s, 	Step: 23271, 	{'train/accuracy': 0.9925545454025269, 'train/loss': 0.024180814623832703, 'train/mean_average_precision': 0.5611472685572075, 'validation/accuracy': 0.9866765737533569, 'validation/loss': 0.04666389152407646, 'validation/mean_average_precision': 0.25925900323497914, 'validation/num_examples': 43793, 'test/accuracy': 0.9858642220497131, 'test/loss': 0.04953271150588989, 'test/mean_average_precision': 0.2471890951017505, 'test/num_examples': 43793, 'score': 7455.3728721141815, 'total_duration': 11313.475859165192, 'accumulated_submission_time': 7455.3728721141815, 'accumulated_eval_time': 3856.554924964905, 'accumulated_logging_time': 0.9077761173248291}
I0205 14:50:46.591551 139789425600256 logging_writer.py:48] [23271] accumulated_eval_time=3856.554925, accumulated_logging_time=0.907776, accumulated_submission_time=7455.372872, global_step=23271, preemption_count=0, score=7455.372872, test/accuracy=0.985864, test/loss=0.049533, test/mean_average_precision=0.247189, test/num_examples=43793, total_duration=11313.475859, train/accuracy=0.992555, train/loss=0.024181, train/mean_average_precision=0.561147, validation/accuracy=0.986677, validation/loss=0.046664, validation/mean_average_precision=0.259259, validation/num_examples=43793
I0205 14:50:56.311235 139789433992960 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0327058807015419, loss=0.03318933770060539
I0205 14:51:28.416330 139789425600256 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.02864021062850952, loss=0.030014246702194214
I0205 14:51:59.837307 139789433992960 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.03230356425046921, loss=0.03473086282610893
I0205 14:52:31.650788 139789425600256 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.030874276533722878, loss=0.03083202615380287
I0205 14:53:03.050750 139789433992960 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.0354018472135067, loss=0.033923208713531494
I0205 14:53:34.619915 139789425600256 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.03394868224859238, loss=0.033192332834005356
I0205 14:54:06.413714 139789433992960 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.040335360914468765, loss=0.03298542648553848
I0205 14:54:38.042576 139789425600256 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.03165743872523308, loss=0.03326486051082611
I0205 14:54:46.861642 139978932307776 spec.py:321] Evaluating on the training split.
I0205 14:56:38.405286 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 14:56:43.025981 139978932307776 spec.py:349] Evaluating on the test split.
I0205 14:56:46.009743 139978932307776 submission_runner.py:408] Time since start: 11672.91s, 	Step: 24029, 	{'train/accuracy': 0.9927038550376892, 'train/loss': 0.023641152307391167, 'train/mean_average_precision': 0.5831575393716177, 'validation/accuracy': 0.9867139458656311, 'validation/loss': 0.046945538371801376, 'validation/mean_average_precision': 0.25290764972411883, 'validation/num_examples': 43793, 'test/accuracy': 0.9858760237693787, 'test/loss': 0.049796633422374725, 'test/mean_average_precision': 0.24441828961731113, 'test/num_examples': 43793, 'score': 7695.610192298889, 'total_duration': 11672.913708925247, 'accumulated_submission_time': 7695.610192298889, 'accumulated_eval_time': 3975.7028489112854, 'accumulated_logging_time': 0.9397668838500977}
I0205 14:56:46.030156 139804644198144 logging_writer.py:48] [24029] accumulated_eval_time=3975.702849, accumulated_logging_time=0.939767, accumulated_submission_time=7695.610192, global_step=24029, preemption_count=0, score=7695.610192, test/accuracy=0.985876, test/loss=0.049797, test/mean_average_precision=0.244418, test/num_examples=43793, total_duration=11672.913709, train/accuracy=0.992704, train/loss=0.023641, train/mean_average_precision=0.583158, validation/accuracy=0.986714, validation/loss=0.046946, validation/mean_average_precision=0.252908, validation/num_examples=43793
I0205 14:57:09.017195 139804984182528 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.03302115201950073, loss=0.032600853592157364
I0205 14:57:40.727610 139804644198144 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.040314964950084686, loss=0.034110844135284424
I0205 14:58:12.653887 139804984182528 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03334150090813637, loss=0.028951622545719147
I0205 14:58:44.432002 139804644198144 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.029916316270828247, loss=0.029926108196377754
I0205 14:59:16.112087 139804984182528 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.031251128762960434, loss=0.031811729073524475
I0205 14:59:48.062129 139804644198144 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03454941511154175, loss=0.032057080417871475
I0205 15:00:19.821717 139804984182528 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03626284748315811, loss=0.03568609803915024
I0205 15:00:46.229023 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:02:40.255612 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:02:43.287924 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:02:46.257112 139978932307776 submission_runner.py:408] Time since start: 12033.16s, 	Step: 24784, 	{'train/accuracy': 0.9929652214050293, 'train/loss': 0.022663027048110962, 'train/mean_average_precision': 0.6082374097411949, 'validation/accuracy': 0.9866116046905518, 'validation/loss': 0.04725179076194763, 'validation/mean_average_precision': 0.2516948713212647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.050070036202669144, 'test/mean_average_precision': 0.24522219280925725, 'test/num_examples': 43793, 'score': 7935.776249885559, 'total_duration': 12033.161201000214, 'accumulated_submission_time': 7935.776249885559, 'accumulated_eval_time': 4095.730885744095, 'accumulated_logging_time': 0.9724068641662598}
I0205 15:02:46.277053 139788813231872 logging_writer.py:48] [24784] accumulated_eval_time=4095.730886, accumulated_logging_time=0.972407, accumulated_submission_time=7935.776250, global_step=24784, preemption_count=0, score=7935.776250, test/accuracy=0.985854, test/loss=0.050070, test/mean_average_precision=0.245222, test/num_examples=43793, total_duration=12033.161201, train/accuracy=0.992965, train/loss=0.022663, train/mean_average_precision=0.608237, validation/accuracy=0.986612, validation/loss=0.047252, validation/mean_average_precision=0.251695, validation/num_examples=43793
I0205 15:02:51.800347 139789425600256 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.03105456568300724, loss=0.028448257595300674
I0205 15:03:24.105569 139788813231872 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.032333992421627045, loss=0.032715484499931335
I0205 15:03:56.083719 139789425600256 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.032811108976602554, loss=0.03169270232319832
I0205 15:04:28.163007 139788813231872 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03026764467358589, loss=0.030250901356339455
I0205 15:05:00.421132 139789425600256 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.030765363946557045, loss=0.03212757781147957
I0205 15:05:32.359943 139788813231872 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03508102521300316, loss=0.0315975546836853
I0205 15:06:04.269730 139789425600256 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.03335374966263771, loss=0.031014544889330864
I0205 15:06:36.331399 139788813231872 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.035145942121744156, loss=0.03042883612215519
I0205 15:06:46.308225 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:08:37.646822 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:08:40.811820 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:08:43.800860 139978932307776 submission_runner.py:408] Time since start: 12390.70s, 	Step: 25532, 	{'train/accuracy': 0.9932979941368103, 'train/loss': 0.021950002759695053, 'train/mean_average_precision': 0.6178963070088231, 'validation/accuracy': 0.9866976737976074, 'validation/loss': 0.047483768314123154, 'validation/mean_average_precision': 0.26251559570858335, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.050649408251047134, 'test/mean_average_precision': 0.2503859213316042, 'test/num_examples': 43793, 'score': 8175.77491402626, 'total_duration': 12390.704937458038, 'accumulated_submission_time': 8175.77491402626, 'accumulated_eval_time': 4213.223459482193, 'accumulated_logging_time': 1.0048103332519531}
I0205 15:08:43.820888 139789433992960 logging_writer.py:48] [25532] accumulated_eval_time=4213.223459, accumulated_logging_time=1.004810, accumulated_submission_time=8175.774914, global_step=25532, preemption_count=0, score=8175.774914, test/accuracy=0.985786, test/loss=0.050649, test/mean_average_precision=0.250386, test/num_examples=43793, total_duration=12390.704937, train/accuracy=0.993298, train/loss=0.021950, train/mean_average_precision=0.617896, validation/accuracy=0.986698, validation/loss=0.047484, validation/mean_average_precision=0.262516, validation/num_examples=43793
I0205 15:09:05.964023 139804644198144 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.035349152982234955, loss=0.03177192062139511
I0205 15:09:38.140726 139789433992960 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.03511988744139671, loss=0.03342251107096672
I0205 15:10:09.880708 139804644198144 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.03852156177163124, loss=0.03194332495331764
I0205 15:10:41.491042 139789433992960 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.039394404739141464, loss=0.03438583388924599
I0205 15:11:13.175281 139804644198144 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.044323842972517014, loss=0.03215911611914635
I0205 15:11:45.008197 139789433992960 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.036743342876434326, loss=0.03320144861936569
I0205 15:12:16.373114 139804644198144 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04706277698278427, loss=0.032426051795482635
I0205 15:12:43.863396 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:14:35.491927 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:14:38.526546 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:14:41.493772 139978932307776 submission_runner.py:408] Time since start: 12748.40s, 	Step: 26287, 	{'train/accuracy': 0.9928412437438965, 'train/loss': 0.02331540733575821, 'train/mean_average_precision': 0.5911147703334756, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.04788680747151375, 'validation/mean_average_precision': 0.24724923956040498, 'validation/num_examples': 43793, 'test/accuracy': 0.9856705069541931, 'test/loss': 0.050956495106220245, 'test/mean_average_precision': 0.2435300717592869, 'test/num_examples': 43793, 'score': 8415.784977912903, 'total_duration': 12748.397860050201, 'accumulated_submission_time': 8415.784977912903, 'accumulated_eval_time': 4330.8537838459015, 'accumulated_logging_time': 1.036971092224121}
I0205 15:14:41.514254 139788813231872 logging_writer.py:48] [26287] accumulated_eval_time=4330.853784, accumulated_logging_time=1.036971, accumulated_submission_time=8415.784978, global_step=26287, preemption_count=0, score=8415.784978, test/accuracy=0.985671, test/loss=0.050956, test/mean_average_precision=0.243530, test/num_examples=43793, total_duration=12748.397860, train/accuracy=0.992841, train/loss=0.023315, train/mean_average_precision=0.591115, validation/accuracy=0.986623, validation/loss=0.047887, validation/mean_average_precision=0.247249, validation/num_examples=43793
I0205 15:14:46.011828 139804984182528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04443999007344246, loss=0.030415747314691544
I0205 15:15:17.960536 139788813231872 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.04013563320040703, loss=0.03154153749346733
I0205 15:15:49.652412 139804984182528 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.040586065500974655, loss=0.03034619428217411
I0205 15:16:21.527716 139788813231872 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.04031563550233841, loss=0.031796760857105255
I0205 15:16:53.066510 139804984182528 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03818583860993385, loss=0.03224419802427292
I0205 15:17:24.608862 139788813231872 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.039059385657310486, loss=0.03244079276919365
I0205 15:17:56.015641 139804984182528 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.056139394640922546, loss=0.0355902835726738
I0205 15:18:27.737000 139788813231872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04431040585041046, loss=0.03221891447901726
I0205 15:18:41.648998 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:20:34.759960 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:20:37.768478 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:20:40.758145 139978932307776 submission_runner.py:408] Time since start: 13107.66s, 	Step: 27045, 	{'train/accuracy': 0.9928683638572693, 'train/loss': 0.02305789850652218, 'train/mean_average_precision': 0.5763528542791916, 'validation/accuracy': 0.9865012168884277, 'validation/loss': 0.04821200668811798, 'validation/mean_average_precision': 0.25106742497771933, 'validation/num_examples': 43793, 'test/accuracy': 0.9858288764953613, 'test/loss': 0.05099726840853691, 'test/mean_average_precision': 0.24492141409916557, 'test/num_examples': 43793, 'score': 8655.887106895447, 'total_duration': 13107.662237644196, 'accumulated_submission_time': 8655.887106895447, 'accumulated_eval_time': 4449.962881565094, 'accumulated_logging_time': 1.0698926448822021}
I0205 15:20:40.778427 139789433992960 logging_writer.py:48] [27045] accumulated_eval_time=4449.962882, accumulated_logging_time=1.069893, accumulated_submission_time=8655.887107, global_step=27045, preemption_count=0, score=8655.887107, test/accuracy=0.985829, test/loss=0.050997, test/mean_average_precision=0.244921, test/num_examples=43793, total_duration=13107.662238, train/accuracy=0.992868, train/loss=0.023058, train/mean_average_precision=0.576353, validation/accuracy=0.986501, validation/loss=0.048212, validation/mean_average_precision=0.251067, validation/num_examples=43793
I0205 15:20:58.724273 139804644198144 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.04022205248475075, loss=0.032402943819761276
I0205 15:21:30.210787 139789433992960 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04036629572510719, loss=0.03120564855635166
I0205 15:22:01.877543 139804644198144 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.04449009150266647, loss=0.030567718669772148
I0205 15:22:33.642315 139789433992960 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.04252338409423828, loss=0.033152103424072266
I0205 15:23:05.062583 139804644198144 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.04383367300033569, loss=0.03368804231286049
I0205 15:23:36.789741 139789433992960 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.041026290506124496, loss=0.03103049099445343
I0205 15:24:08.641583 139804644198144 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.041809987276792526, loss=0.031535256654024124
I0205 15:24:40.240201 139789433992960 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.041200824081897736, loss=0.030895616859197617
I0205 15:24:40.858444 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:26:36.823209 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:26:39.825454 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:26:42.852036 139978932307776 submission_runner.py:408] Time since start: 13469.76s, 	Step: 27803, 	{'train/accuracy': 0.9929357767105103, 'train/loss': 0.02285642921924591, 'train/mean_average_precision': 0.5939315356893492, 'validation/accuracy': 0.9865418076515198, 'validation/loss': 0.047837596386671066, 'validation/mean_average_precision': 0.2569972428572599, 'validation/num_examples': 43793, 'test/accuracy': 0.9856839776039124, 'test/loss': 0.050829917192459106, 'test/mean_average_precision': 0.24666290141204703, 'test/num_examples': 43793, 'score': 8895.934210538864, 'total_duration': 13469.756130218506, 'accumulated_submission_time': 8895.934210538864, 'accumulated_eval_time': 4571.956423997879, 'accumulated_logging_time': 1.1025707721710205}
I0205 15:26:42.872474 139789425600256 logging_writer.py:48] [27803] accumulated_eval_time=4571.956424, accumulated_logging_time=1.102571, accumulated_submission_time=8895.934211, global_step=27803, preemption_count=0, score=8895.934211, test/accuracy=0.985684, test/loss=0.050830, test/mean_average_precision=0.246663, test/num_examples=43793, total_duration=13469.756130, train/accuracy=0.992936, train/loss=0.022856, train/mean_average_precision=0.593932, validation/accuracy=0.986542, validation/loss=0.047838, validation/mean_average_precision=0.256997, validation/num_examples=43793
I0205 15:27:14.117821 139804984182528 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0418710894882679, loss=0.03227216377854347
I0205 15:27:45.815273 139789425600256 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.044497426599264145, loss=0.0322272963821888
I0205 15:28:17.629486 139804984182528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.04038538038730621, loss=0.03219437599182129
I0205 15:28:49.382586 139789425600256 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04184846207499504, loss=0.033043500036001205
I0205 15:29:21.450358 139804984182528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04797109216451645, loss=0.030986636877059937
I0205 15:29:52.952638 139789425600256 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.052351322025060654, loss=0.030697640031576157
I0205 15:30:25.016336 139804984182528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.04498874396085739, loss=0.03097834624350071
I0205 15:30:42.910652 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:32:29.516660 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:32:32.634882 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:32:35.619811 139978932307776 submission_runner.py:408] Time since start: 13822.52s, 	Step: 28557, 	{'train/accuracy': 0.9927763342857361, 'train/loss': 0.023140892386436462, 'train/mean_average_precision': 0.5826014335653862, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04844299331307411, 'validation/mean_average_precision': 0.25352724356934786, 'validation/num_examples': 43793, 'test/accuracy': 0.9858267903327942, 'test/loss': 0.05125714838504791, 'test/mean_average_precision': 0.24444329182030758, 'test/num_examples': 43793, 'score': 9135.94087100029, 'total_duration': 13822.52390408516, 'accumulated_submission_time': 9135.94087100029, 'accumulated_eval_time': 4684.665537118912, 'accumulated_logging_time': 1.1340579986572266}
I0205 15:32:35.640408 139788813231872 logging_writer.py:48] [28557] accumulated_eval_time=4684.665537, accumulated_logging_time=1.134058, accumulated_submission_time=9135.940871, global_step=28557, preemption_count=0, score=9135.940871, test/accuracy=0.985827, test/loss=0.051257, test/mean_average_precision=0.244443, test/num_examples=43793, total_duration=13822.523904, train/accuracy=0.992776, train/loss=0.023141, train/mean_average_precision=0.582601, validation/accuracy=0.986616, validation/loss=0.048443, validation/mean_average_precision=0.253527, validation/num_examples=43793
I0205 15:32:49.653952 139804644198144 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04727340117096901, loss=0.030420731753110886
I0205 15:33:21.370260 139788813231872 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.0386158786714077, loss=0.031001079827547073
I0205 15:33:53.114256 139804644198144 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04739084467291832, loss=0.030371947214007378
I0205 15:34:24.829915 139788813231872 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.0422305092215538, loss=0.02972026914358139
I0205 15:34:56.657560 139804644198144 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.03858356177806854, loss=0.028437448665499687
I0205 15:35:28.551719 139788813231872 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.03982725739479065, loss=0.029334381222724915
I0205 15:36:01.217334 139804644198144 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04145348072052002, loss=0.02956550568342209
I0205 15:36:32.854990 139788813231872 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.04268668591976166, loss=0.03139353543519974
I0205 15:36:35.677378 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:38:29.871750 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:38:32.894214 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:38:35.891845 139978932307776 submission_runner.py:408] Time since start: 14182.80s, 	Step: 29310, 	{'train/accuracy': 0.992730975151062, 'train/loss': 0.023071737959980965, 'train/mean_average_precision': 0.5945630507535318, 'validation/accuracy': 0.9866514205932617, 'validation/loss': 0.048861317336559296, 'validation/mean_average_precision': 0.25436434873632463, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.05192745476961136, 'test/mean_average_precision': 0.24519639591880044, 'test/num_examples': 43793, 'score': 9375.945397377014, 'total_duration': 14182.795937299728, 'accumulated_submission_time': 9375.945397377014, 'accumulated_eval_time': 4804.87996172905, 'accumulated_logging_time': 1.1669306755065918}
I0205 15:38:35.912659 139789425600256 logging_writer.py:48] [29310] accumulated_eval_time=4804.879962, accumulated_logging_time=1.166931, accumulated_submission_time=9375.945397, global_step=29310, preemption_count=0, score=9375.945397, test/accuracy=0.985849, test/loss=0.051927, test/mean_average_precision=0.245196, test/num_examples=43793, total_duration=14182.795937, train/accuracy=0.992731, train/loss=0.023072, train/mean_average_precision=0.594563, validation/accuracy=0.986651, validation/loss=0.048861, validation/mean_average_precision=0.254364, validation/num_examples=43793
I0205 15:39:05.494094 139804984182528 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04689796268939972, loss=0.028737714514136314
I0205 15:39:37.554177 139789425600256 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05480452626943588, loss=0.02946191094815731
I0205 15:40:09.490980 139804984182528 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.04714113101363182, loss=0.03011341765522957
I0205 15:40:41.415804 139789425600256 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.04813672602176666, loss=0.029959293082356453
I0205 15:41:13.481028 139804984182528 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.04318194091320038, loss=0.02899167127907276
I0205 15:41:45.270826 139789425600256 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.050055671483278275, loss=0.03218857944011688
I0205 15:42:17.003391 139804984182528 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.05065964162349701, loss=0.03152310475707054
I0205 15:42:35.974092 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:44:24.828747 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:44:27.881057 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:44:30.876610 139978932307776 submission_runner.py:408] Time since start: 14537.78s, 	Step: 30060, 	{'train/accuracy': 0.9930763244628906, 'train/loss': 0.022160854190587997, 'train/mean_average_precision': 0.6124207352776663, 'validation/accuracy': 0.9865061044692993, 'validation/loss': 0.04897496849298477, 'validation/mean_average_precision': 0.24776020527532236, 'validation/num_examples': 43793, 'test/accuracy': 0.9857067465782166, 'test/loss': 0.05196841433644295, 'test/mean_average_precision': 0.2419138446436762, 'test/num_examples': 43793, 'score': 9615.973546028137, 'total_duration': 14537.780702829361, 'accumulated_submission_time': 9615.973546028137, 'accumulated_eval_time': 4919.782430887222, 'accumulated_logging_time': 1.1999526023864746}
I0205 15:44:30.897664 139788813231872 logging_writer.py:48] [30060] accumulated_eval_time=4919.782431, accumulated_logging_time=1.199953, accumulated_submission_time=9615.973546, global_step=30060, preemption_count=0, score=9615.973546, test/accuracy=0.985707, test/loss=0.051968, test/mean_average_precision=0.241914, test/num_examples=43793, total_duration=14537.780703, train/accuracy=0.993076, train/loss=0.022161, train/mean_average_precision=0.612421, validation/accuracy=0.986506, validation/loss=0.048975, validation/mean_average_precision=0.247760, validation/num_examples=43793
I0205 15:44:43.951169 139804644198144 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.04182981327176094, loss=0.031271643936634064
I0205 15:45:15.950078 139788813231872 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.04166582226753235, loss=0.029804173856973648
I0205 15:45:47.902184 139804644198144 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.057181019335985184, loss=0.028774568811058998
I0205 15:46:19.970685 139788813231872 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.04589320346713066, loss=0.0298966895788908
I0205 15:46:51.654095 139804644198144 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.044763341546058655, loss=0.031167928129434586
I0205 15:47:23.071163 139788813231872 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.044442176818847656, loss=0.028313048183918
I0205 15:47:55.044610 139804644198144 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.042446836829185486, loss=0.028214149177074432
I0205 15:48:26.840188 139788813231872 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05534679815173149, loss=0.03147412836551666
I0205 15:48:31.080798 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:50:22.607617 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:50:25.702325 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:50:28.703580 139978932307776 submission_runner.py:408] Time since start: 14895.61s, 	Step: 30814, 	{'train/accuracy': 0.9933850169181824, 'train/loss': 0.021051522344350815, 'train/mean_average_precision': 0.6370004880408724, 'validation/accuracy': 0.986537754535675, 'validation/loss': 0.04920114204287529, 'validation/mean_average_precision': 0.24948712036916235, 'validation/num_examples': 43793, 'test/accuracy': 0.9857239723205566, 'test/loss': 0.052162785083055496, 'test/mean_average_precision': 0.24383115834249408, 'test/num_examples': 43793, 'score': 9856.123964548111, 'total_duration': 14895.607670545578, 'accumulated_submission_time': 9856.123964548111, 'accumulated_eval_time': 5037.405160903931, 'accumulated_logging_time': 1.233625888824463}
I0205 15:50:28.725308 139789433992960 logging_writer.py:48] [30814] accumulated_eval_time=5037.405161, accumulated_logging_time=1.233626, accumulated_submission_time=9856.123965, global_step=30814, preemption_count=0, score=9856.123965, test/accuracy=0.985724, test/loss=0.052163, test/mean_average_precision=0.243831, test/num_examples=43793, total_duration=14895.607671, train/accuracy=0.993385, train/loss=0.021052, train/mean_average_precision=0.637000, validation/accuracy=0.986538, validation/loss=0.049201, validation/mean_average_precision=0.249487, validation/num_examples=43793
I0205 15:50:56.362052 139804984182528 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05788080766797066, loss=0.030578207224607468
I0205 15:51:28.304105 139789433992960 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.046312134712934494, loss=0.029917703941464424
I0205 15:52:00.103874 139804984182528 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.04708462581038475, loss=0.02923257276415825
I0205 15:52:32.273789 139789433992960 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.05095995217561722, loss=0.030637409538030624
I0205 15:53:04.415172 139804984182528 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.04643386974930763, loss=0.02992703951895237
I0205 15:53:36.650165 139789433992960 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.05385332182049751, loss=0.027786916121840477
I0205 15:54:08.613466 139804984182528 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.045239631086587906, loss=0.027037328109145164
I0205 15:54:28.848409 139978932307776 spec.py:321] Evaluating on the training split.
I0205 15:56:19.349121 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 15:56:22.433264 139978932307776 spec.py:349] Evaluating on the test split.
I0205 15:56:25.505146 139978932307776 submission_runner.py:408] Time since start: 15252.41s, 	Step: 31564, 	{'train/accuracy': 0.9937571883201599, 'train/loss': 0.020146047696471214, 'train/mean_average_precision': 0.6480886876305693, 'validation/accuracy': 0.9864618182182312, 'validation/loss': 0.04921168461441994, 'validation/mean_average_precision': 0.24653370613843473, 'validation/num_examples': 43793, 'test/accuracy': 0.9856313467025757, 'test/loss': 0.05249027535319328, 'test/mean_average_precision': 0.2379547686815906, 'test/num_examples': 43793, 'score': 10096.213876724243, 'total_duration': 15252.409235239029, 'accumulated_submission_time': 10096.213876724243, 'accumulated_eval_time': 5154.061851263046, 'accumulated_logging_time': 1.2679109573364258}
I0205 15:56:25.527555 139788813231872 logging_writer.py:48] [31564] accumulated_eval_time=5154.061851, accumulated_logging_time=1.267911, accumulated_submission_time=10096.213877, global_step=31564, preemption_count=0, score=10096.213877, test/accuracy=0.985631, test/loss=0.052490, test/mean_average_precision=0.237955, test/num_examples=43793, total_duration=15252.409235, train/accuracy=0.993757, train/loss=0.020146, train/mean_average_precision=0.648089, validation/accuracy=0.986462, validation/loss=0.049212, validation/mean_average_precision=0.246534, validation/num_examples=43793
I0205 15:56:37.562099 139804644198144 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.04889264702796936, loss=0.030627155676484108
I0205 15:57:09.675870 139788813231872 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04503823071718216, loss=0.029757529497146606
I0205 15:57:41.724371 139804644198144 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.051342543214559555, loss=0.030244814231991768
I0205 15:58:14.287654 139788813231872 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0554010309278965, loss=0.030603444203734398
I0205 15:58:46.186356 139804644198144 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05345316603779793, loss=0.030160948634147644
I0205 15:59:18.194500 139788813231872 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.04705420881509781, loss=0.027524961158633232
I0205 15:59:49.801665 139804644198144 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.06978683918714523, loss=0.031367719173431396
I0205 16:00:21.712177 139788813231872 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.05000639334321022, loss=0.028999095782637596
I0205 16:00:25.510147 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:02:17.732646 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:02:20.767975 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:02:23.908771 139978932307776 submission_runner.py:408] Time since start: 15610.81s, 	Step: 32313, 	{'train/accuracy': 0.9943658113479614, 'train/loss': 0.01861678436398506, 'train/mean_average_precision': 0.6874966935541733, 'validation/accuracy': 0.9864301681518555, 'validation/loss': 0.049264438450336456, 'validation/mean_average_precision': 0.24959787384144577, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.052260324358940125, 'test/mean_average_precision': 0.2391222709634299, 'test/num_examples': 43793, 'score': 10336.165597438812, 'total_duration': 15610.812860250473, 'accumulated_submission_time': 10336.165597438812, 'accumulated_eval_time': 5272.460424661636, 'accumulated_logging_time': 1.301285743713379}
I0205 16:02:23.930541 139789425600256 logging_writer.py:48] [32313] accumulated_eval_time=5272.460425, accumulated_logging_time=1.301286, accumulated_submission_time=10336.165597, global_step=32313, preemption_count=0, score=10336.165597, test/accuracy=0.985685, test/loss=0.052260, test/mean_average_precision=0.239122, test/num_examples=43793, total_duration=15610.812860, train/accuracy=0.994366, train/loss=0.018617, train/mean_average_precision=0.687497, validation/accuracy=0.986430, validation/loss=0.049264, validation/mean_average_precision=0.249598, validation/num_examples=43793
I0205 16:02:51.929100 139789433992960 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.051810894161462784, loss=0.029493214562535286
I0205 16:03:23.602473 139789425600256 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.04854438826441765, loss=0.0281040258705616
I0205 16:03:55.228140 139789433992960 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.057380352169275284, loss=0.02941184863448143
I0205 16:04:26.849779 139789425600256 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05719749256968498, loss=0.03136307746171951
I0205 16:04:58.359266 139789433992960 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.06288755685091019, loss=0.0314624086022377
I0205 16:05:30.166569 139789425600256 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05641598626971245, loss=0.03174607828259468
I0205 16:06:01.738963 139789433992960 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.06032317504286766, loss=0.02861836366355419
I0205 16:06:23.986406 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:08:15.088411 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:08:18.087120 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:08:21.071070 139978932307776 submission_runner.py:408] Time since start: 15967.98s, 	Step: 33071, 	{'train/accuracy': 0.9944509863853455, 'train/loss': 0.018283959478139877, 'train/mean_average_precision': 0.6934071978381392, 'validation/accuracy': 0.9864094853401184, 'validation/loss': 0.0501183457672596, 'validation/mean_average_precision': 0.24276209453510667, 'validation/num_examples': 43793, 'test/accuracy': 0.9856359362602234, 'test/loss': 0.053267572075128555, 'test/mean_average_precision': 0.23607333376372316, 'test/num_examples': 43793, 'score': 10576.188651800156, 'total_duration': 15967.97516155243, 'accumulated_submission_time': 10576.188651800156, 'accumulated_eval_time': 5389.545041322708, 'accumulated_logging_time': 1.3359241485595703}
I0205 16:08:21.092763 139788813231872 logging_writer.py:48] [33071] accumulated_eval_time=5389.545041, accumulated_logging_time=1.335924, accumulated_submission_time=10576.188652, global_step=33071, preemption_count=0, score=10576.188652, test/accuracy=0.985636, test/loss=0.053268, test/mean_average_precision=0.236073, test/num_examples=43793, total_duration=15967.975162, train/accuracy=0.994451, train/loss=0.018284, train/mean_average_precision=0.693407, validation/accuracy=0.986409, validation/loss=0.050118, validation/mean_average_precision=0.242762, validation/num_examples=43793
I0205 16:08:30.762164 139804984182528 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05705447494983673, loss=0.030453572049736977
I0205 16:09:02.975471 139788813231872 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.05208462104201317, loss=0.028072897344827652
I0205 16:09:34.664098 139804984182528 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.052862707525491714, loss=0.03096078522503376
I0205 16:10:06.685033 139788813231872 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06573167443275452, loss=0.03182046860456467
I0205 16:10:38.657411 139804984182528 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.06282933056354523, loss=0.030277429148554802
I0205 16:11:10.556569 139788813231872 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05038919299840927, loss=0.028225963935256004
I0205 16:11:42.193724 139804984182528 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05417536944150925, loss=0.030790016055107117
I0205 16:12:14.263434 139788813231872 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.04982416704297066, loss=0.02685665525496006
I0205 16:12:21.247319 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:14:12.097290 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:14:15.096325 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:14:18.063044 139978932307776 submission_runner.py:408] Time since start: 16324.97s, 	Step: 33823, 	{'train/accuracy': 0.9944994449615479, 'train/loss': 0.01842135563492775, 'train/mean_average_precision': 0.6916567183095863, 'validation/accuracy': 0.9863566756248474, 'validation/loss': 0.04982062801718712, 'validation/mean_average_precision': 0.24413040287182172, 'validation/num_examples': 43793, 'test/accuracy': 0.9855618476867676, 'test/loss': 0.05272269994020462, 'test/mean_average_precision': 0.2433782019584294, 'test/num_examples': 43793, 'score': 10816.311703920364, 'total_duration': 16324.967136383057, 'accumulated_submission_time': 10816.311703920364, 'accumulated_eval_time': 5506.360715389252, 'accumulated_logging_time': 1.3688089847564697}
I0205 16:14:18.085656 139789433992960 logging_writer.py:48] [33823] accumulated_eval_time=5506.360715, accumulated_logging_time=1.368809, accumulated_submission_time=10816.311704, global_step=33823, preemption_count=0, score=10816.311704, test/accuracy=0.985562, test/loss=0.052723, test/mean_average_precision=0.243378, test/num_examples=43793, total_duration=16324.967136, train/accuracy=0.994499, train/loss=0.018421, train/mean_average_precision=0.691657, validation/accuracy=0.986357, validation/loss=0.049821, validation/mean_average_precision=0.244130, validation/num_examples=43793
I0205 16:14:42.889561 139804644198144 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.056531038135290146, loss=0.029213178902864456
I0205 16:15:14.533277 139789433992960 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06244385614991188, loss=0.02989332564175129
I0205 16:15:46.019589 139804644198144 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05011524632573128, loss=0.02833910472691059
I0205 16:16:17.799883 139789433992960 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.04890523478388786, loss=0.02718280628323555
I0205 16:16:49.573839 139804644198144 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.052346810698509216, loss=0.028814353048801422
I0205 16:17:21.476950 139789433992960 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.05869743973016739, loss=0.030816087499260902
I0205 16:17:53.416061 139804644198144 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06195291131734848, loss=0.029726045206189156
I0205 16:18:18.155695 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:20:09.275002 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:20:12.332630 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:20:15.291932 139978932307776 submission_runner.py:408] Time since start: 16682.20s, 	Step: 34579, 	{'train/accuracy': 0.9933765530586243, 'train/loss': 0.021374696865677834, 'train/mean_average_precision': 0.616448395606391, 'validation/accuracy': 0.9862044453620911, 'validation/loss': 0.050089042633771896, 'validation/mean_average_precision': 0.23755285265003115, 'validation/num_examples': 43793, 'test/accuracy': 0.9853659868240356, 'test/loss': 0.05300765857100487, 'test/mean_average_precision': 0.23444454646682908, 'test/num_examples': 43793, 'score': 11056.350311517715, 'total_duration': 16682.196016073227, 'accumulated_submission_time': 11056.350311517715, 'accumulated_eval_time': 5623.496901512146, 'accumulated_logging_time': 1.4026639461517334}
I0205 16:20:15.314136 139789425600256 logging_writer.py:48] [34579] accumulated_eval_time=5623.496902, accumulated_logging_time=1.402664, accumulated_submission_time=11056.350312, global_step=34579, preemption_count=0, score=11056.350312, test/accuracy=0.985366, test/loss=0.053008, test/mean_average_precision=0.234445, test/num_examples=43793, total_duration=16682.196016, train/accuracy=0.993377, train/loss=0.021375, train/mean_average_precision=0.616448, validation/accuracy=0.986204, validation/loss=0.050089, validation/mean_average_precision=0.237553, validation/num_examples=43793
I0205 16:20:22.379184 139804984182528 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06236959621310234, loss=0.02984154410660267
I0205 16:20:54.005233 139789425600256 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06336689740419388, loss=0.030267611145973206
I0205 16:21:26.041739 139804984182528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05875144898891449, loss=0.02994406409561634
I0205 16:21:57.922441 139789425600256 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.05799521133303642, loss=0.0290104690939188
I0205 16:22:30.116698 139804984182528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06336073577404022, loss=0.029302077367901802
I0205 16:23:01.897433 139789425600256 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06263044476509094, loss=0.03002735786139965
I0205 16:23:34.077769 139804984182528 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05527917295694351, loss=0.027814561501145363
I0205 16:24:06.204503 139789425600256 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06321347504854202, loss=0.02956017479300499
I0205 16:24:15.306738 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:26:03.607718 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:26:06.636261 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:26:09.646111 139978932307776 submission_runner.py:408] Time since start: 17036.55s, 	Step: 35330, 	{'train/accuracy': 0.9937258958816528, 'train/loss': 0.020007194951176643, 'train/mean_average_precision': 0.6484402300425647, 'validation/accuracy': 0.9864411354064941, 'validation/loss': 0.05105067417025566, 'validation/mean_average_precision': 0.24657327514169033, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.053873226046562195, 'test/mean_average_precision': 0.2414957014130568, 'test/num_examples': 43793, 'score': 11296.30923128128, 'total_duration': 17036.55020093918, 'accumulated_submission_time': 11296.30923128128, 'accumulated_eval_time': 5737.836226701736, 'accumulated_logging_time': 1.4372141361236572}
I0205 16:26:09.668892 139789433992960 logging_writer.py:48] [35330] accumulated_eval_time=5737.836227, accumulated_logging_time=1.437214, accumulated_submission_time=11296.309231, global_step=35330, preemption_count=0, score=11296.309231, test/accuracy=0.985740, test/loss=0.053873, test/mean_average_precision=0.241496, test/num_examples=43793, total_duration=17036.550201, train/accuracy=0.993726, train/loss=0.020007, train/mean_average_precision=0.648440, validation/accuracy=0.986441, validation/loss=0.051051, validation/mean_average_precision=0.246573, validation/num_examples=43793
I0205 16:26:32.227209 139804644198144 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.06132977455854416, loss=0.028778500854969025
I0205 16:27:04.119545 139789433992960 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.058836083859205246, loss=0.030253544449806213
I0205 16:27:35.942376 139804644198144 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.057672880589962006, loss=0.028032248839735985
I0205 16:28:07.855024 139789433992960 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.060090504586696625, loss=0.028083831071853638
I0205 16:28:40.399078 139804644198144 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.0694059282541275, loss=0.03154675289988518
I0205 16:29:12.543206 139789433992960 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05994164198637009, loss=0.029077982529997826
I0205 16:29:45.375986 139804644198144 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.08372616767883301, loss=0.02933359146118164
I0205 16:30:09.827265 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:32:02.246706 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:32:05.323173 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:32:08.352416 139978932307776 submission_runner.py:408] Time since start: 17395.26s, 	Step: 36077, 	{'train/accuracy': 0.9936544895172119, 'train/loss': 0.02011866867542267, 'train/mean_average_precision': 0.6498716669258316, 'validation/accuracy': 0.9863563179969788, 'validation/loss': 0.050975799560546875, 'validation/mean_average_precision': 0.24499919854954746, 'validation/num_examples': 43793, 'test/accuracy': 0.9854881167411804, 'test/loss': 0.054286763072013855, 'test/mean_average_precision': 0.233509910277653, 'test/num_examples': 43793, 'score': 11536.434869289398, 'total_duration': 17395.25648856163, 'accumulated_submission_time': 11536.434869289398, 'accumulated_eval_time': 5856.3613159656525, 'accumulated_logging_time': 1.4723279476165771}
I0205 16:32:08.375199 139789425600256 logging_writer.py:48] [36077] accumulated_eval_time=5856.361316, accumulated_logging_time=1.472328, accumulated_submission_time=11536.434869, global_step=36077, preemption_count=0, score=11536.434869, test/accuracy=0.985488, test/loss=0.054287, test/mean_average_precision=0.233510, test/num_examples=43793, total_duration=17395.256489, train/accuracy=0.993654, train/loss=0.020119, train/mean_average_precision=0.649872, validation/accuracy=0.986356, validation/loss=0.050976, validation/mean_average_precision=0.244999, validation/num_examples=43793
I0205 16:32:16.541402 139804984182528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.07200633734464645, loss=0.029027536511421204
I0205 16:32:48.961258 139789425600256 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05871804058551788, loss=0.02841019630432129
I0205 16:33:20.970372 139804984182528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.0634063184261322, loss=0.028745759278535843
I0205 16:33:52.932966 139789425600256 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.059700097888708115, loss=0.028990577906370163
I0205 16:34:25.219195 139804984182528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.062337398529052734, loss=0.030596686527132988
I0205 16:34:57.207634 139789425600256 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05994690954685211, loss=0.028116045519709587
I0205 16:35:29.811269 139804984182528 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.05749212205410004, loss=0.028057929128408432
I0205 16:36:02.223774 139789425600256 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06757591664791107, loss=0.029450709000229836
I0205 16:36:08.352542 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:38:00.635140 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:38:03.757990 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:38:06.891266 139978932307776 submission_runner.py:408] Time since start: 17753.80s, 	Step: 36820, 	{'train/accuracy': 0.9935859441757202, 'train/loss': 0.02019241265952587, 'train/mean_average_precision': 0.6461987033015026, 'validation/accuracy': 0.9864293336868286, 'validation/loss': 0.051526058465242386, 'validation/mean_average_precision': 0.24125879341817963, 'validation/num_examples': 43793, 'test/accuracy': 0.9855584502220154, 'test/loss': 0.05475538596510887, 'test/mean_average_precision': 0.234687398547803, 'test/num_examples': 43793, 'score': 11776.005351305008, 'total_duration': 17753.795357704163, 'accumulated_submission_time': 11776.005351305008, 'accumulated_eval_time': 5974.8999898433685, 'accumulated_logging_time': 1.880678653717041}
I0205 16:38:06.913971 139788813231872 logging_writer.py:48] [36820] accumulated_eval_time=5974.899990, accumulated_logging_time=1.880679, accumulated_submission_time=11776.005351, global_step=36820, preemption_count=0, score=11776.005351, test/accuracy=0.985558, test/loss=0.054755, test/mean_average_precision=0.234687, test/num_examples=43793, total_duration=17753.795358, train/accuracy=0.993586, train/loss=0.020192, train/mean_average_precision=0.646199, validation/accuracy=0.986429, validation/loss=0.051526, validation/mean_average_precision=0.241259, validation/num_examples=43793
I0205 16:38:32.994788 139804644198144 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.060884907841682434, loss=0.03041711077094078
I0205 16:39:04.982511 139788813231872 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06579602509737015, loss=0.02818909101188183
I0205 16:39:37.066612 139804644198144 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.05809136480093002, loss=0.027739016339182854
I0205 16:40:08.766388 139788813231872 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.061918705701828, loss=0.026238521561026573
I0205 16:40:41.302960 139804644198144 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06150323525071144, loss=0.02968769706785679
I0205 16:41:13.286600 139788813231872 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.07939228415489197, loss=0.02955879271030426
I0205 16:41:45.387186 139804644198144 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06339430809020996, loss=0.027388932183384895
I0205 16:42:07.141553 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:43:57.321548 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:44:00.331944 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:44:03.443418 139978932307776 submission_runner.py:408] Time since start: 18110.35s, 	Step: 37569, 	{'train/accuracy': 0.9937519431114197, 'train/loss': 0.019834209233522415, 'train/mean_average_precision': 0.6552595150078337, 'validation/accuracy': 0.9864228963851929, 'validation/loss': 0.05131762847304344, 'validation/mean_average_precision': 0.24673181391672155, 'validation/num_examples': 43793, 'test/accuracy': 0.9854522943496704, 'test/loss': 0.05443757772445679, 'test/mean_average_precision': 0.23268169125564953, 'test/num_examples': 43793, 'score': 12016.200606584549, 'total_duration': 18110.347497224808, 'accumulated_submission_time': 12016.200606584549, 'accumulated_eval_time': 6091.20179772377, 'accumulated_logging_time': 1.9143686294555664}
I0205 16:44:03.466156 139789433992960 logging_writer.py:48] [37569] accumulated_eval_time=6091.201798, accumulated_logging_time=1.914369, accumulated_submission_time=12016.200607, global_step=37569, preemption_count=0, score=12016.200607, test/accuracy=0.985452, test/loss=0.054438, test/mean_average_precision=0.232682, test/num_examples=43793, total_duration=18110.347497, train/accuracy=0.993752, train/loss=0.019834, train/mean_average_precision=0.655260, validation/accuracy=0.986423, validation/loss=0.051318, validation/mean_average_precision=0.246732, validation/num_examples=43793
I0205 16:44:13.762187 139804984182528 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06577043235301971, loss=0.02996961772441864
I0205 16:44:45.690426 139789433992960 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06713558733463287, loss=0.027577921748161316
I0205 16:45:17.216171 139804984182528 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.0652422308921814, loss=0.028453411534428596
I0205 16:45:48.955382 139789433992960 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06272374838590622, loss=0.029524339362978935
I0205 16:46:20.860503 139804984182528 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06076165661215782, loss=0.026903675869107246
I0205 16:46:52.396029 139789433992960 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06556625664234161, loss=0.02930489368736744
I0205 16:47:24.242702 139804984182528 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06454866379499435, loss=0.027191974222660065
I0205 16:47:56.044022 139789433992960 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07114408165216446, loss=0.028938155621290207
I0205 16:48:03.705936 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:49:54.015094 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:49:57.034861 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:50:00.002799 139978932307776 submission_runner.py:408] Time since start: 18466.91s, 	Step: 38325, 	{'train/accuracy': 0.9941584467887878, 'train/loss': 0.018329985439777374, 'train/mean_average_precision': 0.6959283207416935, 'validation/accuracy': 0.9863651990890503, 'validation/loss': 0.05230919271707535, 'validation/mean_average_precision': 0.24253230329771047, 'validation/num_examples': 43793, 'test/accuracy': 0.9854717254638672, 'test/loss': 0.05553451552987099, 'test/mean_average_precision': 0.22832626069700715, 'test/num_examples': 43793, 'score': 12256.40864610672, 'total_duration': 18466.906888246536, 'accumulated_submission_time': 12256.40864610672, 'accumulated_eval_time': 6207.4986119270325, 'accumulated_logging_time': 1.9481298923492432}
I0205 16:50:00.026263 139789425600256 logging_writer.py:48] [38325] accumulated_eval_time=6207.498612, accumulated_logging_time=1.948130, accumulated_submission_time=12256.408646, global_step=38325, preemption_count=0, score=12256.408646, test/accuracy=0.985472, test/loss=0.055535, test/mean_average_precision=0.228326, test/num_examples=43793, total_duration=18466.906888, train/accuracy=0.994158, train/loss=0.018330, train/mean_average_precision=0.695928, validation/accuracy=0.986365, validation/loss=0.052309, validation/mean_average_precision=0.242532, validation/num_examples=43793
I0205 16:50:24.218057 139804644198144 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.07199738174676895, loss=0.02804736979305744
I0205 16:50:55.753305 139789425600256 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07080934941768646, loss=0.029083777219057083
I0205 16:51:27.510288 139804644198144 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.07456761598587036, loss=0.029015012085437775
I0205 16:51:59.206658 139789425600256 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.0715104416012764, loss=0.026688074693083763
I0205 16:52:30.941714 139804644198144 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06663739681243896, loss=0.026902813464403152
I0205 16:53:02.517940 139789425600256 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.0745837613940239, loss=0.027033010497689247
I0205 16:53:33.812278 139804644198144 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07049573957920074, loss=0.030101241543889046
I0205 16:54:00.168242 139978932307776 spec.py:321] Evaluating on the training split.
I0205 16:55:50.625207 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 16:55:53.639757 139978932307776 spec.py:349] Evaluating on the test split.
I0205 16:55:56.718981 139978932307776 submission_runner.py:408] Time since start: 18823.62s, 	Step: 39084, 	{'train/accuracy': 0.9948763251304626, 'train/loss': 0.01684432290494442, 'train/mean_average_precision': 0.7108119769907197, 'validation/accuracy': 0.9862645268440247, 'validation/loss': 0.051835693418979645, 'validation/mean_average_precision': 0.24291645434349401, 'validation/num_examples': 43793, 'test/accuracy': 0.9854000806808472, 'test/loss': 0.05496647581458092, 'test/mean_average_precision': 0.23119369630632372, 'test/num_examples': 43793, 'score': 12496.516707897186, 'total_duration': 18823.623073339462, 'accumulated_submission_time': 12496.516707897186, 'accumulated_eval_time': 6324.049305677414, 'accumulated_logging_time': 1.984095573425293}
I0205 16:55:56.741501 139788813231872 logging_writer.py:48] [39084] accumulated_eval_time=6324.049306, accumulated_logging_time=1.984096, accumulated_submission_time=12496.516708, global_step=39084, preemption_count=0, score=12496.516708, test/accuracy=0.985400, test/loss=0.054966, test/mean_average_precision=0.231194, test/num_examples=43793, total_duration=18823.623073, train/accuracy=0.994876, train/loss=0.016844, train/mean_average_precision=0.710812, validation/accuracy=0.986265, validation/loss=0.051836, validation/mean_average_precision=0.242916, validation/num_examples=43793
I0205 16:56:02.172017 139804984182528 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06359323859214783, loss=0.027332501485943794
I0205 16:56:34.061464 139788813231872 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.06790228933095932, loss=0.027438560500741005
I0205 16:57:05.916890 139804984182528 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.06947579979896545, loss=0.02703414484858513
I0205 16:57:37.619643 139788813231872 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.07020866125822067, loss=0.02890726551413536
I0205 16:58:09.237507 139804984182528 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.06164911761879921, loss=0.02818993851542473
I0205 16:58:41.389129 139788813231872 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07261454313993454, loss=0.027547737583518028
I0205 16:59:13.356736 139804984182528 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.06829754263162613, loss=0.02664957195520401
I0205 16:59:45.220709 139788813231872 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.08969186991453171, loss=0.03017529286444187
I0205 16:59:56.975392 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:01:46.791468 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:01:49.819240 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:01:52.821520 139978932307776 submission_runner.py:408] Time since start: 19179.73s, 	Step: 39838, 	{'train/accuracy': 0.9953045845031738, 'train/loss': 0.015735676512122154, 'train/mean_average_precision': 0.743408396758674, 'validation/accuracy': 0.986170768737793, 'validation/loss': 0.05237202346324921, 'validation/mean_average_precision': 0.24157881853185634, 'validation/num_examples': 43793, 'test/accuracy': 0.9853402972221375, 'test/loss': 0.05575835704803467, 'test/mean_average_precision': 0.23236867697136018, 'test/num_examples': 43793, 'score': 12736.719133377075, 'total_duration': 19179.72560429573, 'accumulated_submission_time': 12736.719133377075, 'accumulated_eval_time': 6439.895386219025, 'accumulated_logging_time': 2.017606496810913}
I0205 17:01:52.844622 139789425600256 logging_writer.py:48] [39838] accumulated_eval_time=6439.895386, accumulated_logging_time=2.017606, accumulated_submission_time=12736.719133, global_step=39838, preemption_count=0, score=12736.719133, test/accuracy=0.985340, test/loss=0.055758, test/mean_average_precision=0.232369, test/num_examples=43793, total_duration=19179.725604, train/accuracy=0.995305, train/loss=0.015736, train/mean_average_precision=0.743408, validation/accuracy=0.986171, validation/loss=0.052372, validation/mean_average_precision=0.241579, validation/num_examples=43793
I0205 17:02:13.058404 139789433992960 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07176538556814194, loss=0.028102532029151917
I0205 17:02:45.378795 139789425600256 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0858287438750267, loss=0.02984619326889515
I0205 17:03:17.411871 139789433992960 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06366053968667984, loss=0.02684803120791912
I0205 17:03:49.304033 139789425600256 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06744374334812164, loss=0.02661091834306717
I0205 17:04:21.047717 139789433992960 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06692042946815491, loss=0.0264329444617033
I0205 17:04:52.930504 139789425600256 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07771947979927063, loss=0.027622472494840622
I0205 17:05:25.145581 139789433992960 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07297972589731216, loss=0.0266239233314991
I0205 17:05:52.821815 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:07:39.342024 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:07:42.448352 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:07:45.475351 139978932307776 submission_runner.py:408] Time since start: 19532.38s, 	Step: 40588, 	{'train/accuracy': 0.9955078363418579, 'train/loss': 0.015150746330618858, 'train/mean_average_precision': 0.7610764629387446, 'validation/accuracy': 0.9862247705459595, 'validation/loss': 0.05289914831519127, 'validation/mean_average_precision': 0.23656075026347692, 'validation/num_examples': 43793, 'test/accuracy': 0.9853579998016357, 'test/loss': 0.05615391209721565, 'test/mean_average_precision': 0.2277253445759712, 'test/num_examples': 43793, 'score': 12976.663056850433, 'total_duration': 19532.37944626808, 'accumulated_submission_time': 12976.663056850433, 'accumulated_eval_time': 6552.548879623413, 'accumulated_logging_time': 2.0531373023986816}
I0205 17:07:45.499243 139804644198144 logging_writer.py:48] [40588] accumulated_eval_time=6552.548880, accumulated_logging_time=2.053137, accumulated_submission_time=12976.663057, global_step=40588, preemption_count=0, score=12976.663057, test/accuracy=0.985358, test/loss=0.056154, test/mean_average_precision=0.227725, test/num_examples=43793, total_duration=19532.379446, train/accuracy=0.995508, train/loss=0.015151, train/mean_average_precision=0.761076, validation/accuracy=0.986225, validation/loss=0.052899, validation/mean_average_precision=0.236561, validation/num_examples=43793
I0205 17:07:49.578369 139804984182528 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.06491581350564957, loss=0.025902995839715004
I0205 17:08:21.572647 139804644198144 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.08326058089733124, loss=0.027347559109330177
I0205 17:08:53.609555 139804984182528 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07206325232982635, loss=0.026182115077972412
I0205 17:09:25.367600 139804644198144 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.0742451548576355, loss=0.027509914711117744
I0205 17:09:57.597286 139804984182528 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07868866622447968, loss=0.02810545265674591
I0205 17:10:30.078317 139804644198144 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07271910458803177, loss=0.027057068422436714
I0205 17:11:02.007181 139804984182528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07899589091539383, loss=0.027092870324850082
I0205 17:11:34.167784 139804644198144 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.06827129423618317, loss=0.025754597038030624
I0205 17:11:45.717077 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:13:31.563114 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:13:34.735914 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:13:37.780175 139978932307776 submission_runner.py:408] Time since start: 19884.68s, 	Step: 41337, 	{'train/accuracy': 0.9952184557914734, 'train/loss': 0.01582160033285618, 'train/mean_average_precision': 0.7453891223328923, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.05295562744140625, 'validation/mean_average_precision': 0.24187175966019298, 'validation/num_examples': 43793, 'test/accuracy': 0.985393762588501, 'test/loss': 0.056329395622015, 'test/mean_average_precision': 0.22938868223573725, 'test/num_examples': 43793, 'score': 13216.849076271057, 'total_duration': 19884.684267520905, 'accumulated_submission_time': 13216.849076271057, 'accumulated_eval_time': 6664.61195230484, 'accumulated_logging_time': 2.0880980491638184}
I0205 17:13:37.803267 139789425600256 logging_writer.py:48] [41337] accumulated_eval_time=6664.611952, accumulated_logging_time=2.088098, accumulated_submission_time=13216.849076, global_step=41337, preemption_count=0, score=13216.849076, test/accuracy=0.985394, test/loss=0.056329, test/mean_average_precision=0.229389, test/num_examples=43793, total_duration=19884.684268, train/accuracy=0.995218, train/loss=0.015822, train/mean_average_precision=0.745389, validation/accuracy=0.986210, validation/loss=0.052956, validation/mean_average_precision=0.241872, validation/num_examples=43793
I0205 17:13:58.376951 139789433992960 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.07094576209783554, loss=0.025611637160182
I0205 17:14:30.092032 139789425600256 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.08336233347654343, loss=0.029815323650836945
I0205 17:15:01.976331 139789433992960 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.08196914941072464, loss=0.02767839841544628
I0205 17:15:33.894151 139789425600256 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07497470825910568, loss=0.02608550526201725
I0205 17:16:05.879327 139789433992960 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.0704079121351242, loss=0.02634081244468689
I0205 17:16:37.614625 139789425600256 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07756412029266357, loss=0.026051905006170273
I0205 17:17:09.682862 139789433992960 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07507181912660599, loss=0.02670002356171608
I0205 17:17:38.094825 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:19:25.637205 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:19:28.712288 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:19:31.704575 139978932307776 submission_runner.py:408] Time since start: 20238.61s, 	Step: 42091, 	{'train/accuracy': 0.9948252439498901, 'train/loss': 0.016748318448662758, 'train/mean_average_precision': 0.7169117412420599, 'validation/accuracy': 0.9861918687820435, 'validation/loss': 0.05373343452811241, 'validation/mean_average_precision': 0.23533939751035776, 'validation/num_examples': 43793, 'test/accuracy': 0.985342800617218, 'test/loss': 0.056724365800619125, 'test/mean_average_precision': 0.231405241881548, 'test/num_examples': 43793, 'score': 13457.108339548111, 'total_duration': 20238.608663082123, 'accumulated_submission_time': 13457.108339548111, 'accumulated_eval_time': 6778.221654415131, 'accumulated_logging_time': 2.122004747390747}
I0205 17:19:31.729613 139788813231872 logging_writer.py:48] [42091] accumulated_eval_time=6778.221654, accumulated_logging_time=2.122005, accumulated_submission_time=13457.108340, global_step=42091, preemption_count=0, score=13457.108340, test/accuracy=0.985343, test/loss=0.056724, test/mean_average_precision=0.231405, test/num_examples=43793, total_duration=20238.608663, train/accuracy=0.994825, train/loss=0.016748, train/mean_average_precision=0.716912, validation/accuracy=0.986192, validation/loss=0.053733, validation/mean_average_precision=0.235339, validation/num_examples=43793
I0205 17:19:35.057878 139804984182528 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.06709083914756775, loss=0.025134563446044922
I0205 17:20:07.483371 139788813231872 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07810965180397034, loss=0.026598595082759857
I0205 17:20:39.303339 139804984182528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07683037221431732, loss=0.02849840559065342
I0205 17:21:11.374025 139788813231872 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.07918553799390793, loss=0.02739148773252964
I0205 17:21:43.137866 139804984182528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.06491657346487045, loss=0.0256511140614748
I0205 17:22:14.660367 139788813231872 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.07677115499973297, loss=0.026719318702816963
I0205 17:22:46.336587 139804984182528 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.07508673518896103, loss=0.026575051248073578
I0205 17:23:17.989499 139788813231872 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.08600445091724396, loss=0.027261964976787567
I0205 17:23:31.948712 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:25:18.762004 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:25:21.784834 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:25:24.767299 139978932307776 submission_runner.py:408] Time since start: 20591.67s, 	Step: 42845, 	{'train/accuracy': 0.9936889410018921, 'train/loss': 0.019539348781108856, 'train/mean_average_precision': 0.6601647292398976, 'validation/accuracy': 0.9862625002861023, 'validation/loss': 0.05407680943608284, 'validation/mean_average_precision': 0.2355572185518675, 'validation/num_examples': 43793, 'test/accuracy': 0.9853861927986145, 'test/loss': 0.057481955736875534, 'test/mean_average_precision': 0.22742230463765503, 'test/num_examples': 43793, 'score': 13697.295625209808, 'total_duration': 20591.67139029503, 'accumulated_submission_time': 13697.295625209808, 'accumulated_eval_time': 6891.040193080902, 'accumulated_logging_time': 2.158005714416504}
I0205 17:25:24.790883 139789425600256 logging_writer.py:48] [42845] accumulated_eval_time=6891.040193, accumulated_logging_time=2.158006, accumulated_submission_time=13697.295625, global_step=42845, preemption_count=0, score=13697.295625, test/accuracy=0.985386, test/loss=0.057482, test/mean_average_precision=0.227422, test/num_examples=43793, total_duration=20591.671390, train/accuracy=0.993689, train/loss=0.019539, train/mean_average_precision=0.660165, validation/accuracy=0.986263, validation/loss=0.054077, validation/mean_average_precision=0.235557, validation/num_examples=43793
I0205 17:25:42.579280 139789433992960 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.07446015626192093, loss=0.02494267001748085
I0205 17:26:14.368705 139789425600256 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0741787999868393, loss=0.025374388322234154
I0205 17:26:46.067632 139789433992960 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07725272327661514, loss=0.027055731043219566
I0205 17:27:17.954460 139789425600256 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07488639652729034, loss=0.02632945030927658
I0205 17:27:49.810874 139789433992960 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07579249888658524, loss=0.0268245879560709
I0205 17:28:21.464651 139789425600256 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08588723093271255, loss=0.028369465842843056
I0205 17:28:53.030143 139789433992960 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.09567371010780334, loss=0.02658589743077755
I0205 17:29:24.768828 139789425600256 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08019045740365982, loss=0.027218403294682503
I0205 17:29:24.774110 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:31:13.609555 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:31:16.684065 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:31:19.666254 139978932307776 submission_runner.py:408] Time since start: 20946.57s, 	Step: 43601, 	{'train/accuracy': 0.9939095377922058, 'train/loss': 0.01894284412264824, 'train/mean_average_precision': 0.6758613495698387, 'validation/accuracy': 0.9861545562744141, 'validation/loss': 0.05472370237112045, 'validation/mean_average_precision': 0.23235470043494322, 'validation/num_examples': 43793, 'test/accuracy': 0.9854089617729187, 'test/loss': 0.05789602920413017, 'test/mean_average_precision': 0.22984336740164787, 'test/num_examples': 43793, 'score': 13937.247032403946, 'total_duration': 20946.570340633392, 'accumulated_submission_time': 13937.247032403946, 'accumulated_eval_time': 7005.932266712189, 'accumulated_logging_time': 2.1925864219665527}
I0205 17:31:19.691011 139788813231872 logging_writer.py:48] [43601] accumulated_eval_time=7005.932267, accumulated_logging_time=2.192586, accumulated_submission_time=13937.247032, global_step=43601, preemption_count=0, score=13937.247032, test/accuracy=0.985409, test/loss=0.057896, test/mean_average_precision=0.229843, test/num_examples=43793, total_duration=20946.570341, train/accuracy=0.993910, train/loss=0.018943, train/mean_average_precision=0.675861, validation/accuracy=0.986155, validation/loss=0.054724, validation/mean_average_precision=0.232355, validation/num_examples=43793
I0205 17:31:51.821490 139804644198144 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08558152616024017, loss=0.027507495135068893
I0205 17:32:24.158164 139788813231872 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07075411081314087, loss=0.024913236498832703
I0205 17:32:56.089869 139804644198144 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.07975877821445465, loss=0.026353633031249046
I0205 17:33:28.044236 139788813231872 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08401297777891159, loss=0.02560577541589737
I0205 17:33:59.706686 139804644198144 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.0826546773314476, loss=0.02601582184433937
I0205 17:34:31.617763 139788813231872 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08163446187973022, loss=0.02587003819644451
I0205 17:35:03.500866 139804644198144 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.1026831716299057, loss=0.02815423533320427
I0205 17:35:19.744009 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:37:04.112395 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:37:07.184439 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:37:10.195155 139978932307776 submission_runner.py:408] Time since start: 21297.10s, 	Step: 44353, 	{'train/accuracy': 0.994534432888031, 'train/loss': 0.017085274681448936, 'train/mean_average_precision': 0.7098776166423071, 'validation/accuracy': 0.9860822558403015, 'validation/loss': 0.054855622351169586, 'validation/mean_average_precision': 0.2360641167700365, 'validation/num_examples': 43793, 'test/accuracy': 0.9852712154388428, 'test/loss': 0.058071594685316086, 'test/mean_average_precision': 0.22560777626673703, 'test/num_examples': 43793, 'score': 14177.268211841583, 'total_duration': 21297.09924530983, 'accumulated_submission_time': 14177.268211841583, 'accumulated_eval_time': 7116.383370637894, 'accumulated_logging_time': 2.228288173675537}
I0205 17:37:10.219722 139789425600256 logging_writer.py:48] [44353] accumulated_eval_time=7116.383371, accumulated_logging_time=2.228288, accumulated_submission_time=14177.268212, global_step=44353, preemption_count=0, score=14177.268212, test/accuracy=0.985271, test/loss=0.058072, test/mean_average_precision=0.225608, test/num_examples=43793, total_duration=21297.099245, train/accuracy=0.994534, train/loss=0.017085, train/mean_average_precision=0.709878, validation/accuracy=0.986082, validation/loss=0.054856, validation/mean_average_precision=0.236064, validation/num_examples=43793
I0205 17:37:25.743825 139804984182528 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.07483181357383728, loss=0.02380785346031189
I0205 17:37:57.739211 139789425600256 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07563183456659317, loss=0.02537609077990055
I0205 17:38:29.980902 139804984182528 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.09055326879024506, loss=0.027041910216212273
I0205 17:39:02.233848 139789425600256 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08372080326080322, loss=0.02485840395092964
I0205 17:39:34.362757 139804984182528 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.10300641506910324, loss=0.02650679275393486
I0205 17:40:06.901792 139789425600256 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.08372538536787033, loss=0.02705470472574234
I0205 17:40:39.091146 139804984182528 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07769646495580673, loss=0.025381386280059814
I0205 17:41:10.215025 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:42:58.228287 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:43:01.279804 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:43:04.317336 139978932307776 submission_runner.py:408] Time since start: 21651.22s, 	Step: 45098, 	{'train/accuracy': 0.9942643046379089, 'train/loss': 0.017705237492918968, 'train/mean_average_precision': 0.7059773931270805, 'validation/accuracy': 0.9860595464706421, 'validation/loss': 0.055160850286483765, 'validation/mean_average_precision': 0.23142329750694968, 'validation/num_examples': 43793, 'test/accuracy': 0.9852467775344849, 'test/loss': 0.05843737721443176, 'test/mean_average_precision': 0.2243313486320527, 'test/num_examples': 43793, 'score': 14417.231942176819, 'total_duration': 21651.221431732178, 'accumulated_submission_time': 14417.231942176819, 'accumulated_eval_time': 7230.485638141632, 'accumulated_logging_time': 2.2639756202697754}
I0205 17:43:04.341293 139788813231872 logging_writer.py:48] [45098] accumulated_eval_time=7230.485638, accumulated_logging_time=2.263976, accumulated_submission_time=14417.231942, global_step=45098, preemption_count=0, score=14417.231942, test/accuracy=0.985247, test/loss=0.058437, test/mean_average_precision=0.224331, test/num_examples=43793, total_duration=21651.221432, train/accuracy=0.994264, train/loss=0.017705, train/mean_average_precision=0.705977, validation/accuracy=0.986060, validation/loss=0.055161, validation/mean_average_precision=0.231423, validation/num_examples=43793
I0205 17:43:05.306960 139789433992960 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07904459536075592, loss=0.025996623560786247
I0205 17:43:37.448521 139788813231872 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.07550697773694992, loss=0.025934481993317604
I0205 17:44:09.622063 139789433992960 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07532941550016403, loss=0.024426216259598732
I0205 17:44:41.675227 139788813231872 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08350424468517303, loss=0.02473464049398899
I0205 17:45:13.948830 139789433992960 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0863843560218811, loss=0.025876684114336967
I0205 17:45:45.998053 139788813231872 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09700825065374374, loss=0.025566192343831062
I0205 17:46:18.079765 139789433992960 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08966132253408432, loss=0.025788314640522003
I0205 17:46:50.772294 139788813231872 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.08433765918016434, loss=0.024759981781244278
I0205 17:47:04.321254 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:48:51.310295 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:48:54.341563 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:48:57.320127 139978932307776 submission_runner.py:408] Time since start: 22004.22s, 	Step: 45843, 	{'train/accuracy': 0.9943100214004517, 'train/loss': 0.01757192611694336, 'train/mean_average_precision': 0.711232518020157, 'validation/accuracy': 0.9860436916351318, 'validation/loss': 0.05571623891592026, 'validation/mean_average_precision': 0.23105697250320886, 'validation/num_examples': 43793, 'test/accuracy': 0.9852017164230347, 'test/loss': 0.0595104843378067, 'test/mean_average_precision': 0.2203197279736102, 'test/num_examples': 43793, 'score': 14657.17865562439, 'total_duration': 22004.224217414856, 'accumulated_submission_time': 14657.17865562439, 'accumulated_eval_time': 7343.484463214874, 'accumulated_logging_time': 2.3002424240112305}
I0205 17:48:57.344751 139789425600256 logging_writer.py:48] [45843] accumulated_eval_time=7343.484463, accumulated_logging_time=2.300242, accumulated_submission_time=14657.178656, global_step=45843, preemption_count=0, score=14657.178656, test/accuracy=0.985202, test/loss=0.059510, test/mean_average_precision=0.220320, test/num_examples=43793, total_duration=22004.224217, train/accuracy=0.994310, train/loss=0.017572, train/mean_average_precision=0.711233, validation/accuracy=0.986044, validation/loss=0.055716, validation/mean_average_precision=0.231057, validation/num_examples=43793
I0205 17:49:16.054155 139804984182528 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.09271649271249771, loss=0.025311017408967018
I0205 17:49:48.010021 139789425600256 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.09367594122886658, loss=0.026465658098459244
I0205 17:50:19.937742 139804984182528 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.07527294009923935, loss=0.024363048374652863
I0205 17:50:51.883403 139789425600256 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08808152377605438, loss=0.02450053207576275
I0205 17:51:23.817331 139804984182528 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08942863345146179, loss=0.02580493874847889
I0205 17:51:56.022470 139789425600256 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.09449930489063263, loss=0.02446098066866398
I0205 17:52:28.187964 139804984182528 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.07845964282751083, loss=0.02635868825018406
I0205 17:52:57.413038 139978932307776 spec.py:321] Evaluating on the training split.
I0205 17:54:46.010782 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 17:54:49.074313 139978932307776 spec.py:349] Evaluating on the test split.
I0205 17:54:52.061642 139978932307776 submission_runner.py:408] Time since start: 22358.97s, 	Step: 46594, 	{'train/accuracy': 0.9962734580039978, 'train/loss': 0.012980311177670956, 'train/mean_average_precision': 0.8045735996318846, 'validation/accuracy': 0.9860225915908813, 'validation/loss': 0.05560600012540817, 'validation/mean_average_precision': 0.2332473908672151, 'validation/num_examples': 43793, 'test/accuracy': 0.9852076172828674, 'test/loss': 0.058608949184417725, 'test/mean_average_precision': 0.2262182866703456, 'test/num_examples': 43793, 'score': 14897.215474367142, 'total_duration': 22358.96573448181, 'accumulated_submission_time': 14897.215474367142, 'accumulated_eval_time': 7458.133021593094, 'accumulated_logging_time': 2.3357396125793457}
I0205 17:54:52.085479 139789433992960 logging_writer.py:48] [46594] accumulated_eval_time=7458.133022, accumulated_logging_time=2.335740, accumulated_submission_time=14897.215474, global_step=46594, preemption_count=0, score=14897.215474, test/accuracy=0.985208, test/loss=0.058609, test/mean_average_precision=0.226218, test/num_examples=43793, total_duration=22358.965734, train/accuracy=0.996273, train/loss=0.012980, train/mean_average_precision=0.804574, validation/accuracy=0.986023, validation/loss=0.055606, validation/mean_average_precision=0.233247, validation/num_examples=43793
I0205 17:54:54.280102 139804644198144 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.09377460926771164, loss=0.025890853255987167
I0205 17:55:26.246034 139789433992960 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.09980997443199158, loss=0.026971591636538506
I0205 17:55:57.906711 139804644198144 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.0869341567158699, loss=0.025428589433431625
I0205 17:56:29.618412 139789433992960 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08547117561101913, loss=0.02643352746963501
I0205 17:57:01.205577 139804644198144 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.07579167932271957, loss=0.023775914683938026
I0205 17:57:32.762502 139789433992960 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08567355573177338, loss=0.025634022429585457
I0205 17:58:04.128007 139804644198144 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.09150782227516174, loss=0.025371316820383072
I0205 17:58:36.195967 139789433992960 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.096219502389431, loss=0.026989208534359932
I0205 17:58:52.192318 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:00:36.111185 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:00:39.123810 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:00:42.099384 139978932307776 submission_runner.py:408] Time since start: 22709.00s, 	Step: 47350, 	{'train/accuracy': 0.9964053630828857, 'train/loss': 0.012584680691361427, 'train/mean_average_precision': 0.8052383302908812, 'validation/accuracy': 0.9860960841178894, 'validation/loss': 0.05634652078151703, 'validation/mean_average_precision': 0.2354068295352333, 'validation/num_examples': 43793, 'test/accuracy': 0.9852636456489563, 'test/loss': 0.0596487857401371, 'test/mean_average_precision': 0.22829275553690145, 'test/num_examples': 43793, 'score': 15137.289443016052, 'total_duration': 22709.003462314606, 'accumulated_submission_time': 15137.289443016052, 'accumulated_eval_time': 7568.040026426315, 'accumulated_logging_time': 2.3717122077941895}
I0205 18:00:42.123875 139789425600256 logging_writer.py:48] [47350] accumulated_eval_time=7568.040026, accumulated_logging_time=2.371712, accumulated_submission_time=15137.289443, global_step=47350, preemption_count=0, score=15137.289443, test/accuracy=0.985264, test/loss=0.059649, test/mean_average_precision=0.228293, test/num_examples=43793, total_duration=22709.003462, train/accuracy=0.996405, train/loss=0.012585, train/mean_average_precision=0.805238, validation/accuracy=0.986096, validation/loss=0.056347, validation/mean_average_precision=0.235407, validation/num_examples=43793
I0205 18:00:58.276236 139804984182528 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07993744313716888, loss=0.023923082277178764
I0205 18:01:31.826873 139789425600256 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09193450212478638, loss=0.02492833323776722
I0205 18:02:05.510792 139804984182528 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09512704610824585, loss=0.024832870811223984
I0205 18:02:37.560522 139789425600256 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.0798850730061531, loss=0.02469540573656559
I0205 18:03:09.217872 139804984182528 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.08504211157560349, loss=0.025036761537194252
I0205 18:03:40.840007 139789425600256 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09647974371910095, loss=0.025008931756019592
I0205 18:04:12.573190 139804984182528 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.09099619090557098, loss=0.023590832948684692
I0205 18:04:42.135076 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:06:28.017323 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:06:31.010878 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:06:33.965861 139978932307776 submission_runner.py:408] Time since start: 23060.87s, 	Step: 48094, 	{'train/accuracy': 0.9965007305145264, 'train/loss': 0.012393580749630928, 'train/mean_average_precision': 0.8195799484038679, 'validation/accuracy': 0.9859645366668701, 'validation/loss': 0.05673098936676979, 'validation/mean_average_precision': 0.22698595600141783, 'validation/num_examples': 43793, 'test/accuracy': 0.9851579070091248, 'test/loss': 0.06013302132487297, 'test/mean_average_precision': 0.22456300586969077, 'test/num_examples': 43793, 'score': 15377.269327640533, 'total_duration': 23060.869948148727, 'accumulated_submission_time': 15377.269327640533, 'accumulated_eval_time': 7679.870763301849, 'accumulated_logging_time': 2.407210350036621}
I0205 18:06:33.990264 139788813231872 logging_writer.py:48] [48094] accumulated_eval_time=7679.870763, accumulated_logging_time=2.407210, accumulated_submission_time=15377.269328, global_step=48094, preemption_count=0, score=15377.269328, test/accuracy=0.985158, test/loss=0.060133, test/mean_average_precision=0.224563, test/num_examples=43793, total_duration=23060.869948, train/accuracy=0.996501, train/loss=0.012394, train/mean_average_precision=0.819580, validation/accuracy=0.985965, validation/loss=0.056731, validation/mean_average_precision=0.226986, validation/num_examples=43793
I0205 18:06:36.378309 139789433992960 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07753385603427887, loss=0.0238740686327219
I0205 18:07:08.398785 139788813231872 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08931073546409607, loss=0.023902682587504387
I0205 18:07:40.393656 139789433992960 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.07416211813688278, loss=0.02273475006222725
I0205 18:08:12.257499 139788813231872 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.0804360955953598, loss=0.025114191696047783
I0205 18:08:44.156078 139789433992960 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.10091297328472137, loss=0.02652698941528797
I0205 18:09:16.098727 139788813231872 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.07853329181671143, loss=0.024135733023285866
I0205 18:09:48.099665 139789433992960 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.09481102228164673, loss=0.024442952126264572
I0205 18:10:19.886404 139788813231872 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.10014163702726364, loss=0.02557184547185898
I0205 18:10:34.257964 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:12:20.080128 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:12:23.263205 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:12:26.378599 139978932307776 submission_runner.py:408] Time since start: 23413.28s, 	Step: 48846, 	{'train/accuracy': 0.9960833191871643, 'train/loss': 0.013001606799662113, 'train/mean_average_precision': 0.8006489196358728, 'validation/accuracy': 0.9861443638801575, 'validation/loss': 0.05728606507182121, 'validation/mean_average_precision': 0.2332671815231344, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.06086369603872299, 'test/mean_average_precision': 0.22346355660484257, 'test/num_examples': 43793, 'score': 15617.50436925888, 'total_duration': 23413.28269124031, 'accumulated_submission_time': 15617.50436925888, 'accumulated_eval_time': 7791.991349935532, 'accumulated_logging_time': 2.443833827972412}
I0205 18:12:26.405446 139789425600256 logging_writer.py:48] [48846] accumulated_eval_time=7791.991350, accumulated_logging_time=2.443834, accumulated_submission_time=15617.504369, global_step=48846, preemption_count=0, score=15617.504369, test/accuracy=0.985250, test/loss=0.060864, test/mean_average_precision=0.223464, test/num_examples=43793, total_duration=23413.282691, train/accuracy=0.996083, train/loss=0.013002, train/mean_average_precision=0.800649, validation/accuracy=0.986144, validation/loss=0.057286, validation/mean_average_precision=0.233267, validation/num_examples=43793
I0205 18:12:44.233253 139804984182528 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09082023799419403, loss=0.023524893447756767
I0205 18:13:16.452584 139789425600256 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.10440069437026978, loss=0.024919888004660606
I0205 18:13:48.768248 139804984182528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.08144350349903107, loss=0.022993599995970726
I0205 18:14:20.625037 139789425600256 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08809445798397064, loss=0.025476064532995224
I0205 18:14:52.474800 139804984182528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.09607958048582077, loss=0.023837069049477577
I0205 18:15:24.406628 139789425600256 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.09669632464647293, loss=0.023955998942255974
I0205 18:15:56.435628 139804984182528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09427276253700256, loss=0.023759838193655014
I0205 18:16:26.559727 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:18:11.238687 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:18:14.317194 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:18:17.335884 139978932307776 submission_runner.py:408] Time since start: 23764.24s, 	Step: 49595, 	{'train/accuracy': 0.9956724047660828, 'train/loss': 0.013841000385582447, 'train/mean_average_precision': 0.797884302093334, 'validation/accuracy': 0.9861240983009338, 'validation/loss': 0.05729652941226959, 'validation/mean_average_precision': 0.23156148534243237, 'validation/num_examples': 43793, 'test/accuracy': 0.9852871894836426, 'test/loss': 0.06051979586482048, 'test/mean_average_precision': 0.22366266750840783, 'test/num_examples': 43793, 'score': 15857.626742601395, 'total_duration': 23764.239976644516, 'accumulated_submission_time': 15857.626742601395, 'accumulated_eval_time': 7902.767471790314, 'accumulated_logging_time': 2.482178211212158}
I0205 18:18:17.360942 139788813231872 logging_writer.py:48] [49595] accumulated_eval_time=7902.767472, accumulated_logging_time=2.482178, accumulated_submission_time=15857.626743, global_step=49595, preemption_count=0, score=15857.626743, test/accuracy=0.985287, test/loss=0.060520, test/mean_average_precision=0.223663, test/num_examples=43793, total_duration=23764.239977, train/accuracy=0.995672, train/loss=0.013841, train/mean_average_precision=0.797884, validation/accuracy=0.986124, validation/loss=0.057297, validation/mean_average_precision=0.231561, validation/num_examples=43793
I0205 18:18:19.347859 139789433992960 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.09118013828992844, loss=0.024196134880185127
I0205 18:18:51.710322 139788813231872 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.10755080729722977, loss=0.026295648887753487
I0205 18:19:23.967753 139789433992960 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08542166650295258, loss=0.02495908923447132
I0205 18:19:55.574223 139788813231872 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.10258734226226807, loss=0.02424810640513897
I0205 18:20:27.479791 139789433992960 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.10307306796312332, loss=0.023678528144955635
I0205 18:20:59.273351 139788813231872 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09015166759490967, loss=0.02398260496556759
I0205 18:21:30.973463 139789433992960 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.08551274985074997, loss=0.022324461489915848
I0205 18:22:02.853901 139788813231872 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09670053422451019, loss=0.02411685325205326
I0205 18:22:17.578388 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:23:57.621332 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:24:00.649020 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:24:03.654827 139978932307776 submission_runner.py:408] Time since start: 24110.56s, 	Step: 50347, 	{'train/accuracy': 0.9952304363250732, 'train/loss': 0.014908693730831146, 'train/mean_average_precision': 0.7654407735773269, 'validation/accuracy': 0.9860441088676453, 'validation/loss': 0.058121465146541595, 'validation/mean_average_precision': 0.231803685346562, 'validation/num_examples': 43793, 'test/accuracy': 0.9851604104042053, 'test/loss': 0.06149018928408623, 'test/mean_average_precision': 0.2223522837272326, 'test/num_examples': 43793, 'score': 16097.812799215317, 'total_duration': 24110.558915138245, 'accumulated_submission_time': 16097.812799215317, 'accumulated_eval_time': 8008.843861818314, 'accumulated_logging_time': 2.518423318862915}
I0205 18:24:03.679957 139804644198144 logging_writer.py:48] [50347] accumulated_eval_time=8008.843862, accumulated_logging_time=2.518423, accumulated_submission_time=16097.812799, global_step=50347, preemption_count=0, score=16097.812799, test/accuracy=0.985160, test/loss=0.061490, test/mean_average_precision=0.222352, test/num_examples=43793, total_duration=24110.558915, train/accuracy=0.995230, train/loss=0.014909, train/mean_average_precision=0.765441, validation/accuracy=0.986044, validation/loss=0.058121, validation/mean_average_precision=0.231804, validation/num_examples=43793
I0205 18:24:20.894961 139804984182528 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08440688252449036, loss=0.02295815944671631
I0205 18:24:52.939695 139804644198144 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08881361037492752, loss=0.023979373276233673
I0205 18:25:24.473570 139804984182528 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09105808287858963, loss=0.02420789562165737
I0205 18:25:56.554752 139804644198144 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.09265617281198502, loss=0.02457084320485592
I0205 18:26:28.389483 139804984182528 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.08992469310760498, loss=0.024765662848949432
I0205 18:27:00.137208 139804644198144 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.10119564831256866, loss=0.025339098647236824
I0205 18:27:32.180146 139804984182528 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.10129445046186447, loss=0.024123942479491234
I0205 18:28:03.673558 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:29:47.983495 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:29:51.048779 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:29:54.225374 139978932307776 submission_runner.py:408] Time since start: 24461.13s, 	Step: 51099, 	{'train/accuracy': 0.9944838285446167, 'train/loss': 0.01668316312134266, 'train/mean_average_precision': 0.735212748790653, 'validation/accuracy': 0.9861963391304016, 'validation/loss': 0.058478597551584244, 'validation/mean_average_precision': 0.23081843631601204, 'validation/num_examples': 43793, 'test/accuracy': 0.9852526783943176, 'test/loss': 0.06204073876142502, 'test/mean_average_precision': 0.22100602733148664, 'test/num_examples': 43793, 'score': 16337.775570392609, 'total_duration': 24461.129461288452, 'accumulated_submission_time': 16337.775570392609, 'accumulated_eval_time': 8119.3956298828125, 'accumulated_logging_time': 2.554534435272217}
I0205 18:29:54.250274 139788813231872 logging_writer.py:48] [51099] accumulated_eval_time=8119.395630, accumulated_logging_time=2.554534, accumulated_submission_time=16337.775570, global_step=51099, preemption_count=0, score=16337.775570, test/accuracy=0.985253, test/loss=0.062041, test/mean_average_precision=0.221006, test/num_examples=43793, total_duration=24461.129461, train/accuracy=0.994484, train/loss=0.016683, train/mean_average_precision=0.735213, validation/accuracy=0.986196, validation/loss=0.058479, validation/mean_average_precision=0.230818, validation/num_examples=43793
I0205 18:29:54.953195 139789425600256 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.09930264204740524, loss=0.023742463439702988
I0205 18:30:26.541559 139788813231872 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09680888801813126, loss=0.023895470425486565
I0205 18:30:58.178867 139789425600256 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.07435206323862076, loss=0.023126428946852684
I0205 18:31:30.028733 139788813231872 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09976913034915924, loss=0.02346017025411129
I0205 18:32:01.477133 139789425600256 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09134116768836975, loss=0.02329481579363346
I0205 18:32:33.227345 139788813231872 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.10473069548606873, loss=0.024575015529990196
I0205 18:33:04.794073 139789425600256 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.07915841788053513, loss=0.023239044472575188
I0205 18:33:36.897838 139788813231872 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.09603317826986313, loss=0.02421003021299839
I0205 18:33:54.235501 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:35:36.585152 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:35:39.596759 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:35:42.592781 139978932307776 submission_runner.py:408] Time since start: 24809.50s, 	Step: 51855, 	{'train/accuracy': 0.9948172569274902, 'train/loss': 0.0157752875238657, 'train/mean_average_precision': 0.7602446975067121, 'validation/accuracy': 0.9861037731170654, 'validation/loss': 0.0588659830391407, 'validation/mean_average_precision': 0.23148958688131108, 'validation/num_examples': 43793, 'test/accuracy': 0.9852792024612427, 'test/loss': 0.062334612011909485, 'test/mean_average_precision': 0.22103803290156743, 'test/num_examples': 43793, 'score': 16577.728811979294, 'total_duration': 24809.496871471405, 'accumulated_submission_time': 16577.728811979294, 'accumulated_eval_time': 8227.75286102295, 'accumulated_logging_time': 2.5908830165863037}
I0205 18:35:42.618878 139804644198144 logging_writer.py:48] [51855] accumulated_eval_time=8227.752861, accumulated_logging_time=2.590883, accumulated_submission_time=16577.728812, global_step=51855, preemption_count=0, score=16577.728812, test/accuracy=0.985279, test/loss=0.062335, test/mean_average_precision=0.221038, test/num_examples=43793, total_duration=24809.496871, train/accuracy=0.994817, train/loss=0.015775, train/mean_average_precision=0.760245, validation/accuracy=0.986104, validation/loss=0.058866, validation/mean_average_precision=0.231490, validation/num_examples=43793
I0205 18:35:57.572546 139804984182528 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10090698301792145, loss=0.02544246055185795
I0205 18:36:29.520472 139804644198144 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.08359913527965546, loss=0.024016762152314186
I0205 18:37:01.240936 139804984182528 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08747520297765732, loss=0.024643201380968094
I0205 18:37:32.615570 139804644198144 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.10101675242185593, loss=0.023384438827633858
I0205 18:38:04.373308 139804984182528 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.08731481432914734, loss=0.023547658696770668
I0205 18:38:36.114131 139804644198144 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.10975321382284164, loss=0.023790275678038597
I0205 18:39:07.802898 139804984182528 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.09911168366670609, loss=0.025183632969856262
I0205 18:39:39.408638 139804644198144 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.1448223888874054, loss=0.026994850486516953
I0205 18:39:42.859992 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:41:26.338890 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:41:29.371890 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:41:32.355086 139978932307776 submission_runner.py:408] Time since start: 25159.26s, 	Step: 52612, 	{'train/accuracy': 0.9952720999717712, 'train/loss': 0.01456244383007288, 'train/mean_average_precision': 0.7853455614625474, 'validation/accuracy': 0.9860274791717529, 'validation/loss': 0.05888041481375694, 'validation/mean_average_precision': 0.23159215014093068, 'validation/num_examples': 43793, 'test/accuracy': 0.9851730465888977, 'test/loss': 0.06262774020433426, 'test/mean_average_precision': 0.21721341671585395, 'test/num_examples': 43793, 'score': 16817.936812877655, 'total_duration': 25159.259149074554, 'accumulated_submission_time': 16817.936812877655, 'accumulated_eval_time': 8337.247876405716, 'accumulated_logging_time': 2.629739284515381}
I0205 18:41:32.380031 139789425600256 logging_writer.py:48] [52612] accumulated_eval_time=8337.247876, accumulated_logging_time=2.629739, accumulated_submission_time=16817.936813, global_step=52612, preemption_count=0, score=16817.936813, test/accuracy=0.985173, test/loss=0.062628, test/mean_average_precision=0.217213, test/num_examples=43793, total_duration=25159.259149, train/accuracy=0.995272, train/loss=0.014562, train/mean_average_precision=0.785346, validation/accuracy=0.986027, validation/loss=0.058880, validation/mean_average_precision=0.231592, validation/num_examples=43793
I0205 18:42:00.682950 139789433992960 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.0809498056769371, loss=0.022382769733667374
I0205 18:42:32.715995 139789425600256 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09022974967956543, loss=0.023623688146471977
I0205 18:43:04.710888 139789433992960 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09623348712921143, loss=0.024831708520650864
I0205 18:43:36.658056 139789425600256 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.0860983207821846, loss=0.023060744628310204
I0205 18:44:08.515215 139789433992960 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.08790735900402069, loss=0.02441314421594143
I0205 18:44:40.372832 139789425600256 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.10140519589185715, loss=0.023316796869039536
I0205 18:45:12.361045 139789433992960 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.14266034960746765, loss=0.02730981633067131
I0205 18:45:32.432886 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:47:18.737668 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:47:21.906618 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:47:24.961730 139978932307776 submission_runner.py:408] Time since start: 25511.87s, 	Step: 53365, 	{'train/accuracy': 0.9949951767921448, 'train/loss': 0.015158366411924362, 'train/mean_average_precision': 0.7691710098606686, 'validation/accuracy': 0.9860047698020935, 'validation/loss': 0.05945604667067528, 'validation/mean_average_precision': 0.229525978638166, 'validation/num_examples': 43793, 'test/accuracy': 0.9851570725440979, 'test/loss': 0.06314238905906677, 'test/mean_average_precision': 0.2154677620242295, 'test/num_examples': 43793, 'score': 17057.958690166473, 'total_duration': 25511.86580467224, 'accumulated_submission_time': 17057.958690166473, 'accumulated_eval_time': 8449.776660203934, 'accumulated_logging_time': 2.665452480316162}
I0205 18:47:24.987509 139788813231872 logging_writer.py:48] [53365] accumulated_eval_time=8449.776660, accumulated_logging_time=2.665452, accumulated_submission_time=17057.958690, global_step=53365, preemption_count=0, score=17057.958690, test/accuracy=0.985157, test/loss=0.063142, test/mean_average_precision=0.215468, test/num_examples=43793, total_duration=25511.865805, train/accuracy=0.994995, train/loss=0.015158, train/mean_average_precision=0.769171, validation/accuracy=0.986005, validation/loss=0.059456, validation/mean_average_precision=0.229526, validation/num_examples=43793
I0205 18:47:36.415648 139804644198144 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09959425032138824, loss=0.02330208010971546
I0205 18:48:08.244653 139788813231872 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.08705601096153259, loss=0.023073749616742134
I0205 18:48:40.453268 139804644198144 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.08876185119152069, loss=0.02306600660085678
I0205 18:49:12.399446 139788813231872 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.09317398816347122, loss=0.02312035672366619
I0205 18:49:43.870972 139804644198144 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.09205514192581177, loss=0.023237815126776695
I0205 18:50:15.303264 139788813231872 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.09943241626024246, loss=0.0233718603849411
I0205 18:50:47.380424 139804644198144 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.09648860991001129, loss=0.023862970992922783
I0205 18:51:19.290404 139788813231872 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08441890776157379, loss=0.022262467071413994
I0205 18:51:25.288143 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:53:08.616584 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:53:11.670036 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:53:14.661202 139978932307776 submission_runner.py:408] Time since start: 25861.57s, 	Step: 54120, 	{'train/accuracy': 0.9957131147384644, 'train/loss': 0.013677745126187801, 'train/mean_average_precision': 0.805361075076067, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.060508742928504944, 'validation/mean_average_precision': 0.23027808961236285, 'validation/num_examples': 43793, 'test/accuracy': 0.9852569103240967, 'test/loss': 0.06417310237884521, 'test/mean_average_precision': 0.21644151179846766, 'test/num_examples': 43793, 'score': 17298.226668834686, 'total_duration': 25861.56529688835, 'accumulated_submission_time': 17298.226668834686, 'accumulated_eval_time': 8559.149673700333, 'accumulated_logging_time': 2.7035751342773438}
I0205 18:53:14.687143 139789425600256 logging_writer.py:48] [54120] accumulated_eval_time=8559.149674, accumulated_logging_time=2.703575, accumulated_submission_time=17298.226669, global_step=54120, preemption_count=0, score=17298.226669, test/accuracy=0.985257, test/loss=0.064173, test/mean_average_precision=0.216442, test/num_examples=43793, total_duration=25861.565297, train/accuracy=0.995713, train/loss=0.013678, train/mean_average_precision=0.805361, validation/accuracy=0.986132, validation/loss=0.060509, validation/mean_average_precision=0.230278, validation/num_examples=43793
I0205 18:53:40.383522 139789433992960 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.0946938768029213, loss=0.022972026839852333
I0205 18:54:12.889112 139789425600256 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09639499336481094, loss=0.02306004799902439
I0205 18:54:44.567835 139789433992960 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.09084542840719223, loss=0.022222205996513367
I0205 18:55:16.661035 139789425600256 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.08917459845542908, loss=0.023143354803323746
I0205 18:55:48.552584 139789433992960 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.08474285155534744, loss=0.02253703400492668
I0205 18:56:20.712259 139789425600256 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.08957765251398087, loss=0.024188481271266937
I0205 18:56:52.825099 139789433992960 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.07344625145196915, loss=0.021825971081852913
I0205 18:57:14.820617 139978932307776 spec.py:321] Evaluating on the training split.
I0205 18:58:59.570165 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 18:59:02.693068 139978932307776 spec.py:349] Evaluating on the test split.
I0205 18:59:05.706648 139978932307776 submission_runner.py:408] Time since start: 26212.61s, 	Step: 54869, 	{'train/accuracy': 0.9979147911071777, 'train/loss': 0.009291942231357098, 'train/mean_average_precision': 0.8778368365167077, 'validation/accuracy': 0.9860007166862488, 'validation/loss': 0.0598200224339962, 'validation/mean_average_precision': 0.22958359651974927, 'validation/num_examples': 43793, 'test/accuracy': 0.9850584864616394, 'test/loss': 0.06363627314567566, 'test/mean_average_precision': 0.21293531794694817, 'test/num_examples': 43793, 'score': 17538.3272023201, 'total_duration': 26212.610743045807, 'accumulated_submission_time': 17538.3272023201, 'accumulated_eval_time': 8670.035662651062, 'accumulated_logging_time': 2.741891622543335}
I0205 18:59:05.732214 139788813231872 logging_writer.py:48] [54869] accumulated_eval_time=8670.035663, accumulated_logging_time=2.741892, accumulated_submission_time=17538.327202, global_step=54869, preemption_count=0, score=17538.327202, test/accuracy=0.985058, test/loss=0.063636, test/mean_average_precision=0.212935, test/num_examples=43793, total_duration=26212.610743, train/accuracy=0.997915, train/loss=0.009292, train/mean_average_precision=0.877837, validation/accuracy=0.986001, validation/loss=0.059820, validation/mean_average_precision=0.229584, validation/num_examples=43793
I0205 18:59:16.116995 139804984182528 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.11080549657344818, loss=0.0231209397315979
I0205 18:59:48.294773 139788813231872 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.08491816371679306, loss=0.0223601795732975
I0205 19:00:20.122704 139804984182528 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.08800136297941208, loss=0.023075871169567108
I0205 19:00:52.015007 139788813231872 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.09589561820030212, loss=0.022330928593873978
I0205 19:01:23.637226 139804984182528 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.08371707051992416, loss=0.022376932203769684
I0205 19:01:55.641887 139788813231872 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10156615823507309, loss=0.023602228611707687
I0205 19:02:27.648346 139804984182528 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.08847153186798096, loss=0.022171886637806892
I0205 19:02:59.758529 139788813231872 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.08489246666431427, loss=0.022639168426394463
I0205 19:03:05.943426 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:04:52.850600 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:04:55.854508 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:04:58.804928 139978932307776 submission_runner.py:408] Time since start: 26565.71s, 	Step: 55620, 	{'train/accuracy': 0.9977280497550964, 'train/loss': 0.009441524744033813, 'train/mean_average_precision': 0.886361785591973, 'validation/accuracy': 0.9860274791717529, 'validation/loss': 0.05996182933449745, 'validation/mean_average_precision': 0.22747897461906752, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.06353277713060379, 'test/mean_average_precision': 0.21295138216163578, 'test/num_examples': 43793, 'score': 17778.506385326385, 'total_duration': 26565.708988189697, 'accumulated_submission_time': 17778.506385326385, 'accumulated_eval_time': 8782.897083044052, 'accumulated_logging_time': 2.7787158489227295}
I0205 19:04:58.831719 139789433992960 logging_writer.py:48] [55620] accumulated_eval_time=8782.897083, accumulated_logging_time=2.778716, accumulated_submission_time=17778.506385, global_step=55620, preemption_count=0, score=17778.506385, test/accuracy=0.985092, test/loss=0.063533, test/mean_average_precision=0.212951, test/num_examples=43793, total_duration=26565.708988, train/accuracy=0.997728, train/loss=0.009442, train/mean_average_precision=0.886362, validation/accuracy=0.986027, validation/loss=0.059962, validation/mean_average_precision=0.227479, validation/num_examples=43793
I0205 19:05:24.732608 139804644198144 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.08138003200292587, loss=0.022143270820379257
I0205 19:05:56.653468 139789433992960 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.08411133289337158, loss=0.022568320855498314
I0205 19:06:28.867242 139804644198144 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.08325501531362534, loss=0.022508738562464714
I0205 19:07:00.902521 139789433992960 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.07626358419656754, loss=0.021688370034098625
I0205 19:07:32.740629 139804644198144 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.08825501054525375, loss=0.022831380367279053
I0205 19:08:04.511129 139789433992960 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.07980228960514069, loss=0.022851621732115746
I0205 19:08:36.423940 139804644198144 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.09355177730321884, loss=0.022213010117411613
I0205 19:08:58.984336 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:10:48.600853 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:10:51.733189 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:10:54.718258 139978932307776 submission_runner.py:408] Time since start: 26921.62s, 	Step: 56372, 	{'train/accuracy': 0.9974233508110046, 'train/loss': 0.009903688915073872, 'train/mean_average_precision': 0.8731948836777758, 'validation/accuracy': 0.9860575199127197, 'validation/loss': 0.060540780425071716, 'validation/mean_average_precision': 0.22406773213074888, 'validation/num_examples': 43793, 'test/accuracy': 0.9851629734039307, 'test/loss': 0.06434593349695206, 'test/mean_average_precision': 0.21004910877619207, 'test/num_examples': 43793, 'score': 18018.62752890587, 'total_duration': 26921.62234067917, 'accumulated_submission_time': 18018.62752890587, 'accumulated_eval_time': 8898.630964279175, 'accumulated_logging_time': 2.816796064376831}
I0205 19:10:54.745125 139789425600256 logging_writer.py:48] [56372] accumulated_eval_time=8898.630964, accumulated_logging_time=2.816796, accumulated_submission_time=18018.627529, global_step=56372, preemption_count=0, score=18018.627529, test/accuracy=0.985163, test/loss=0.064346, test/mean_average_precision=0.210049, test/num_examples=43793, total_duration=26921.622341, train/accuracy=0.997423, train/loss=0.009904, train/mean_average_precision=0.873195, validation/accuracy=0.986058, validation/loss=0.060541, validation/mean_average_precision=0.224068, validation/num_examples=43793
I0205 19:11:04.035334 139804984182528 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.08595392107963562, loss=0.022085264325141907
I0205 19:11:35.819750 139789425600256 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09087035059928894, loss=0.022926537320017815
I0205 19:12:07.634149 139804984182528 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1059943214058876, loss=0.02155143767595291
I0205 19:12:39.591066 139789425600256 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.09124279022216797, loss=0.022029243409633636
I0205 19:13:11.467429 139804984182528 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.0859408900141716, loss=0.022553477436304092
I0205 19:13:42.879148 139789425600256 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.0829915851354599, loss=0.021773045882582664
I0205 19:14:14.778228 139804984182528 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.08286864310503006, loss=0.0232087355107069
I0205 19:14:46.340086 139789425600256 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.09310004115104675, loss=0.022300872951745987
I0205 19:14:54.809671 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:16:39.582693 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:16:42.736878 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:16:45.766543 139978932307776 submission_runner.py:408] Time since start: 27272.67s, 	Step: 57128, 	{'train/accuracy': 0.9969244599342346, 'train/loss': 0.010655700229108334, 'train/mean_average_precision': 0.8580153795451726, 'validation/accuracy': 0.986025869846344, 'validation/loss': 0.06091301143169403, 'validation/mean_average_precision': 0.22415679755386675, 'validation/num_examples': 43793, 'test/accuracy': 0.9850812554359436, 'test/loss': 0.06488525122404099, 'test/mean_average_precision': 0.21352743763209386, 'test/num_examples': 43793, 'score': 18258.657961845398, 'total_duration': 27272.670634508133, 'accumulated_submission_time': 18258.657961845398, 'accumulated_eval_time': 9009.587788581848, 'accumulated_logging_time': 2.856903314590454}
I0205 19:16:45.793186 139788813231872 logging_writer.py:48] [57128] accumulated_eval_time=9009.587789, accumulated_logging_time=2.856903, accumulated_submission_time=18258.657962, global_step=57128, preemption_count=0, score=18258.657962, test/accuracy=0.985081, test/loss=0.064885, test/mean_average_precision=0.213527, test/num_examples=43793, total_duration=27272.670635, train/accuracy=0.996924, train/loss=0.010656, train/mean_average_precision=0.858015, validation/accuracy=0.986026, validation/loss=0.060913, validation/mean_average_precision=0.224157, validation/num_examples=43793
I0205 19:17:09.276320 139804644198144 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.07691746205091476, loss=0.021665610373020172
I0205 19:17:41.371880 139788813231872 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.09329620003700256, loss=0.02175426483154297
I0205 19:18:13.696937 139804644198144 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.0857868567109108, loss=0.022610517218708992
I0205 19:18:45.856282 139788813231872 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.08124464005231857, loss=0.021694786846637726
I0205 19:19:17.941433 139804644198144 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.09431933611631393, loss=0.02274150587618351
I0205 19:19:50.734322 139788813231872 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.10259047895669937, loss=0.022981297224760056
I0205 19:20:22.525233 139804644198144 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.07072325050830841, loss=0.021754728630185127
I0205 19:20:24.444780 139788813231872 logging_writer.py:48] [57807] global_step=57807, preemption_count=0, score=18477.262572
I0205 19:20:24.501305 139978932307776 checkpoints.py:490] Saving checkpoint at step: 57807
I0205 19:20:24.634439 139978932307776 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2/checkpoint_57807
I0205 19:20:24.635317 139978932307776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_2/checkpoint_57807.
I0205 19:20:24.815473 139978932307776 submission_runner.py:583] Tuning trial 2/5
I0205 19:20:24.815792 139978932307776 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0205 19:20:24.828673 139978932307776 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.38869017362594604, 'train/loss': 0.7994009852409363, 'train/mean_average_precision': 0.02424800716606758, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02486102341495216, 'validation/num_examples': 43793, 'test/accuracy': 0.3947480618953705, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026378706204941857, 'test/num_examples': 43793, 'score': 12.211112976074219, 'total_duration': 138.16879105567932, 'accumulated_submission_time': 12.211112976074219, 'accumulated_eval_time': 125.95763325691223, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (746, {'train/accuracy': 0.9867796897888184, 'train/loss': 0.06802871823310852, 'train/mean_average_precision': 0.040247962780616715, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07673989236354828, 'validation/mean_average_precision': 0.03999616155129081, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07968270778656006, 'test/mean_average_precision': 0.040902716898336994, 'test/num_examples': 43793, 'score': 252.2515525817871, 'total_duration': 499.1276910305023, 'accumulated_submission_time': 252.2515525817871, 'accumulated_eval_time': 246.83525800704956, 'accumulated_logging_time': 0.02017378807067871, 'global_step': 746, 'preemption_count': 0}), (1496, {'train/accuracy': 0.9871765375137329, 'train/loss': 0.04924483224749565, 'train/mean_average_precision': 0.0841241205174674, 'validation/accuracy': 0.9844134449958801, 'validation/loss': 0.05902522802352905, 'validation/mean_average_precision': 0.0906389025904376, 'validation/num_examples': 43793, 'test/accuracy': 0.9833930730819702, 'test/loss': 0.06238542124629021, 'test/mean_average_precision': 0.08894742056757338, 'test/num_examples': 43793, 'score': 492.32368326187134, 'total_duration': 864.1640558242798, 'accumulated_submission_time': 492.32368326187134, 'accumulated_eval_time': 371.7513871192932, 'accumulated_logging_time': 0.04763197898864746, 'global_step': 1496, 'preemption_count': 0}), (2245, {'train/accuracy': 0.9875968098640442, 'train/loss': 0.04472910985350609, 'train/mean_average_precision': 0.13863232248856314, 'validation/accuracy': 0.9848965406417847, 'validation/loss': 0.05360018089413643, 'validation/mean_average_precision': 0.13760270919551698, 'validation/num_examples': 43793, 'test/accuracy': 0.9838993549346924, 'test/loss': 0.05648729205131531, 'test/mean_average_precision': 0.13005957405048965, 'test/num_examples': 43793, 'score': 732.4389727115631, 'total_duration': 1225.0031561851501, 'accumulated_submission_time': 732.4389727115631, 'accumulated_eval_time': 492.42746329307556, 'accumulated_logging_time': 0.07472872734069824, 'global_step': 2245, 'preemption_count': 0}), (2995, {'train/accuracy': 0.9878544807434082, 'train/loss': 0.04250020161271095, 'train/mean_average_precision': 0.17002244232531422, 'validation/accuracy': 0.9851725697517395, 'validation/loss': 0.05186421796679497, 'validation/mean_average_precision': 0.15461868909853507, 'validation/num_examples': 43793, 'test/accuracy': 0.9841992855072021, 'test/loss': 0.05470949783921242, 'test/mean_average_precision': 0.15080300451331569, 'test/num_examples': 43793, 'score': 972.5787220001221, 'total_duration': 1587.1895275115967, 'accumulated_submission_time': 972.5787220001221, 'accumulated_eval_time': 614.4260520935059, 'accumulated_logging_time': 0.10235357284545898, 'global_step': 2995, 'preemption_count': 0}), (3740, {'train/accuracy': 0.9881300330162048, 'train/loss': 0.041738566011190414, 'train/mean_average_precision': 0.19069986186483673, 'validation/accuracy': 0.9853069186210632, 'validation/loss': 0.05121837928891182, 'validation/mean_average_precision': 0.16984024146752147, 'validation/num_examples': 43793, 'test/accuracy': 0.984386682510376, 'test/loss': 0.053729746490716934, 'test/mean_average_precision': 0.16614425510623596, 'test/num_examples': 43793, 'score': 1212.7844922542572, 'total_duration': 1947.5102033615112, 'accumulated_submission_time': 1212.7844922542572, 'accumulated_eval_time': 734.4927840232849, 'accumulated_logging_time': 0.1301419734954834, 'global_step': 3740, 'preemption_count': 0}), (4502, {'train/accuracy': 0.9881775975227356, 'train/loss': 0.04063105210661888, 'train/mean_average_precision': 0.21152857113458517, 'validation/accuracy': 0.9853300452232361, 'validation/loss': 0.05041537433862686, 'validation/mean_average_precision': 0.1774918128223911, 'validation/num_examples': 43793, 'test/accuracy': 0.9844212532043457, 'test/loss': 0.053231094032526016, 'test/mean_average_precision': 0.17645561924256073, 'test/num_examples': 43793, 'score': 1452.9492535591125, 'total_duration': 2309.3776705265045, 'accumulated_submission_time': 1452.9492535591125, 'accumulated_eval_time': 856.147173166275, 'accumulated_logging_time': 0.15766167640686035, 'global_step': 4502, 'preemption_count': 0}), (5250, {'train/accuracy': 0.9885719418525696, 'train/loss': 0.03916970640420914, 'train/mean_average_precision': 0.22069786053013857, 'validation/accuracy': 0.98569256067276, 'validation/loss': 0.04870147258043289, 'validation/mean_average_precision': 0.1930046659606876, 'validation/num_examples': 43793, 'test/accuracy': 0.9847843050956726, 'test/loss': 0.05126231163740158, 'test/mean_average_precision': 0.1949999632181195, 'test/num_examples': 43793, 'score': 1693.020597934723, 'total_duration': 2673.617773771286, 'accumulated_submission_time': 1693.020597934723, 'accumulated_eval_time': 980.266857624054, 'accumulated_logging_time': 0.18607282638549805, 'global_step': 5250, 'preemption_count': 0}), (6005, {'train/accuracy': 0.98882657289505, 'train/loss': 0.038102976977825165, 'train/mean_average_precision': 0.24963631312924056, 'validation/accuracy': 0.9858545660972595, 'validation/loss': 0.048232465982437134, 'validation/mean_average_precision': 0.20242980529892032, 'validation/num_examples': 43793, 'test/accuracy': 0.984965443611145, 'test/loss': 0.050924960523843765, 'test/mean_average_precision': 0.2020465953537728, 'test/num_examples': 43793, 'score': 1933.2276899814606, 'total_duration': 3034.691485643387, 'accumulated_submission_time': 1933.2276899814606, 'accumulated_eval_time': 1101.0847754478455, 'accumulated_logging_time': 0.21455097198486328, 'global_step': 6005, 'preemption_count': 0}), (6763, {'train/accuracy': 0.9886496067047119, 'train/loss': 0.03849714994430542, 'train/mean_average_precision': 0.2566270749427176, 'validation/accuracy': 0.985815167427063, 'validation/loss': 0.048594117164611816, 'validation/mean_average_precision': 0.2162778550527938, 'validation/num_examples': 43793, 'test/accuracy': 0.9848862290382385, 'test/loss': 0.051545172929763794, 'test/mean_average_precision': 0.21594347952613854, 'test/num_examples': 43793, 'score': 2173.2930114269257, 'total_duration': 3396.8416588306427, 'accumulated_submission_time': 2173.2930114269257, 'accumulated_eval_time': 1223.1207497119904, 'accumulated_logging_time': 0.24296951293945312, 'global_step': 6763, 'preemption_count': 0}), (7517, {'train/accuracy': 0.9889222979545593, 'train/loss': 0.03734235838055611, 'train/mean_average_precision': 0.2822505777691139, 'validation/accuracy': 0.9860039353370667, 'validation/loss': 0.04742446541786194, 'validation/mean_average_precision': 0.23078699973889746, 'validation/num_examples': 43793, 'test/accuracy': 0.9851423501968384, 'test/loss': 0.05007295683026314, 'test/mean_average_precision': 0.22818137385836698, 'test/num_examples': 43793, 'score': 2413.523932695389, 'total_duration': 3750.3751661777496, 'accumulated_submission_time': 2413.523932695389, 'accumulated_eval_time': 1336.374439239502, 'accumulated_logging_time': 0.2714407444000244, 'global_step': 7517, 'preemption_count': 0}), (8263, {'train/accuracy': 0.989389181137085, 'train/loss': 0.03597596287727356, 'train/mean_average_precision': 0.3080768857675089, 'validation/accuracy': 0.9861922860145569, 'validation/loss': 0.04698283225297928, 'validation/mean_average_precision': 0.22930599938566718, 'validation/num_examples': 43793, 'test/accuracy': 0.9852981567382812, 'test/loss': 0.04968796297907829, 'test/mean_average_precision': 0.23192088239179434, 'test/num_examples': 43793, 'score': 2653.601585626602, 'total_duration': 4108.679584980011, 'accumulated_submission_time': 2653.601585626602, 'accumulated_eval_time': 1454.5519473552704, 'accumulated_logging_time': 0.30036044120788574, 'global_step': 8263, 'preemption_count': 0}), (9016, {'train/accuracy': 0.9894828796386719, 'train/loss': 0.03531305119395256, 'train/mean_average_precision': 0.3330498346157439, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.04666941240429878, 'validation/mean_average_precision': 0.23346777682977135, 'validation/num_examples': 43793, 'test/accuracy': 0.9853802919387817, 'test/loss': 0.049349281936883926, 'test/mean_average_precision': 0.23346119830653958, 'test/num_examples': 43793, 'score': 2893.6669194698334, 'total_duration': 4468.438313007355, 'accumulated_submission_time': 2893.6669194698334, 'accumulated_eval_time': 1574.195317029953, 'accumulated_logging_time': 0.32975268363952637, 'global_step': 9016, 'preemption_count': 0}), (9774, {'train/accuracy': 0.9897928833961487, 'train/loss': 0.0340859591960907, 'train/mean_average_precision': 0.34010763404387956, 'validation/accuracy': 0.9863518476486206, 'validation/loss': 0.04619797319173813, 'validation/mean_average_precision': 0.2403073041110711, 'validation/num_examples': 43793, 'test/accuracy': 0.9855226874351501, 'test/loss': 0.04886767640709877, 'test/mean_average_precision': 0.2379734807150239, 'test/num_examples': 43793, 'score': 3133.7673287391663, 'total_duration': 4832.443814992905, 'accumulated_submission_time': 3133.7673287391663, 'accumulated_eval_time': 1698.0499968528748, 'accumulated_logging_time': 0.35930538177490234, 'global_step': 9774, 'preemption_count': 0}), (10527, {'train/accuracy': 0.9902619123458862, 'train/loss': 0.03265085816383362, 'train/mean_average_precision': 0.3747174656981753, 'validation/accuracy': 0.9864894151687622, 'validation/loss': 0.04541108012199402, 'validation/mean_average_precision': 0.24395419394193796, 'validation/num_examples': 43793, 'test/accuracy': 0.9856115579605103, 'test/loss': 0.04801317676901817, 'test/mean_average_precision': 0.24375515934203035, 'test/num_examples': 43793, 'score': 3373.8351554870605, 'total_duration': 5197.865916728973, 'accumulated_submission_time': 3373.8351554870605, 'accumulated_eval_time': 1823.354031085968, 'accumulated_logging_time': 0.38909125328063965, 'global_step': 10527, 'preemption_count': 0}), (11279, {'train/accuracy': 0.9901916980743408, 'train/loss': 0.03254377841949463, 'train/mean_average_precision': 0.39628964259491495, 'validation/accuracy': 0.9865381717681885, 'validation/loss': 0.04554063454270363, 'validation/mean_average_precision': 0.24805487450052172, 'validation/num_examples': 43793, 'test/accuracy': 0.985668420791626, 'test/loss': 0.04814745858311653, 'test/mean_average_precision': 0.24561580856497117, 'test/num_examples': 43793, 'score': 3614.053644180298, 'total_duration': 5559.517339468002, 'accumulated_submission_time': 3614.053644180298, 'accumulated_eval_time': 1944.7356841564178, 'accumulated_logging_time': 0.41932129859924316, 'global_step': 11279, 'preemption_count': 0}), (12032, {'train/accuracy': 0.9903891086578369, 'train/loss': 0.0321890227496624, 'train/mean_average_precision': 0.3812690287674594, 'validation/accuracy': 0.986594557762146, 'validation/loss': 0.04520021006464958, 'validation/mean_average_precision': 0.2515214649864786, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04781940206885338, 'test/mean_average_precision': 0.24914318495248688, 'test/num_examples': 43793, 'score': 3854.257782936096, 'total_duration': 5924.159775972366, 'accumulated_submission_time': 3854.257782936096, 'accumulated_eval_time': 2069.1233773231506, 'accumulated_logging_time': 0.4490396976470947, 'global_step': 12032, 'preemption_count': 0}), (12773, {'train/accuracy': 0.9904727935791016, 'train/loss': 0.03187404200434685, 'train/mean_average_precision': 0.38753860292977615, 'validation/accuracy': 0.9866684675216675, 'validation/loss': 0.04524100571870804, 'validation/mean_average_precision': 0.25419772745581926, 'validation/num_examples': 43793, 'test/accuracy': 0.9857690334320068, 'test/loss': 0.047946419566869736, 'test/mean_average_precision': 0.2452384303975356, 'test/num_examples': 43793, 'score': 4094.240201473236, 'total_duration': 6284.43124961853, 'accumulated_submission_time': 4094.240201473236, 'accumulated_eval_time': 2189.3605513572693, 'accumulated_logging_time': 0.48015785217285156, 'global_step': 12773, 'preemption_count': 0}), (13514, {'train/accuracy': 0.9906947016716003, 'train/loss': 0.030974794179201126, 'train/mean_average_precision': 0.4179961264701088, 'validation/accuracy': 0.9867110848426819, 'validation/loss': 0.044889673590660095, 'validation/mean_average_precision': 0.2612030305011974, 'validation/num_examples': 43793, 'test/accuracy': 0.9857829809188843, 'test/loss': 0.04765735939145088, 'test/mean_average_precision': 0.24666270382631333, 'test/num_examples': 43793, 'score': 4334.438627481461, 'total_duration': 6651.01534485817, 'accumulated_submission_time': 4334.438627481461, 'accumulated_eval_time': 2315.6966729164124, 'accumulated_logging_time': 0.509023904800415, 'global_step': 13514, 'preemption_count': 0}), (14263, {'train/accuracy': 0.9907548427581787, 'train/loss': 0.03045141138136387, 'train/mean_average_precision': 0.43818958080159154, 'validation/accuracy': 0.9867098927497864, 'validation/loss': 0.045234255492687225, 'validation/mean_average_precision': 0.26432332057559815, 'validation/num_examples': 43793, 'test/accuracy': 0.9858015179634094, 'test/loss': 0.04785768315196037, 'test/mean_average_precision': 0.2506972280003634, 'test/num_examples': 43793, 'score': 4574.635347366333, 'total_duration': 7012.2398047447205, 'accumulated_submission_time': 4574.635347366333, 'accumulated_eval_time': 2436.6722359657288, 'accumulated_logging_time': 0.5402529239654541, 'global_step': 14263, 'preemption_count': 0}), (15014, {'train/accuracy': 0.9909887313842773, 'train/loss': 0.02982170321047306, 'train/mean_average_precision': 0.4313697479562325, 'validation/accuracy': 0.9867955446243286, 'validation/loss': 0.044983476400375366, 'validation/mean_average_precision': 0.2626834619716003, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04784674197435379, 'test/mean_average_precision': 0.24305608194623227, 'test/num_examples': 43793, 'score': 4814.710758686066, 'total_duration': 7369.865949630737, 'accumulated_submission_time': 4814.710758686066, 'accumulated_eval_time': 2554.1713659763336, 'accumulated_logging_time': 0.5711688995361328, 'global_step': 15014, 'preemption_count': 0}), (15769, {'train/accuracy': 0.9910573363304138, 'train/loss': 0.02915920689702034, 'train/mean_average_precision': 0.4634965915873742, 'validation/accuracy': 0.9866985082626343, 'validation/loss': 0.045393701642751694, 'validation/mean_average_precision': 0.2624384497477145, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.04817233234643936, 'test/mean_average_precision': 0.24892799014666894, 'test/num_examples': 43793, 'score': 5054.779027462006, 'total_duration': 7726.707957744598, 'accumulated_submission_time': 5054.779027462006, 'accumulated_eval_time': 2670.8944869041443, 'accumulated_logging_time': 0.6007318496704102, 'global_step': 15769, 'preemption_count': 0}), (16521, {'train/accuracy': 0.9913237690925598, 'train/loss': 0.02804555930197239, 'train/mean_average_precision': 0.49196843643775773, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.04596276208758354, 'validation/mean_average_precision': 0.2659343145663652, 'validation/num_examples': 43793, 'test/accuracy': 0.9859126806259155, 'test/loss': 0.04872153699398041, 'test/mean_average_precision': 0.255597488873233, 'test/num_examples': 43793, 'score': 5294.750148057938, 'total_duration': 8085.958600759506, 'accumulated_submission_time': 5294.750148057938, 'accumulated_eval_time': 2790.1232488155365, 'accumulated_logging_time': 0.6307599544525146, 'global_step': 16521, 'preemption_count': 0}), (17272, {'train/accuracy': 0.9916371703147888, 'train/loss': 0.027226930484175682, 'train/mean_average_precision': 0.5029472729700839, 'validation/accuracy': 0.9867812991142273, 'validation/loss': 0.04549722000956535, 'validation/mean_average_precision': 0.26105591706887715, 'validation/num_examples': 43793, 'test/accuracy': 0.9858415126800537, 'test/loss': 0.04827072098851204, 'test/mean_average_precision': 0.250724034154077, 'test/num_examples': 43793, 'score': 5534.754784345627, 'total_duration': 8447.618882656097, 'accumulated_submission_time': 5534.754784345627, 'accumulated_eval_time': 2911.7288358211517, 'accumulated_logging_time': 0.6605877876281738, 'global_step': 17272, 'preemption_count': 0}), (18023, {'train/accuracy': 0.9916689991950989, 'train/loss': 0.02668984793126583, 'train/mean_average_precision': 0.5299052978509329, 'validation/accuracy': 0.98673015832901, 'validation/loss': 0.04590317979454994, 'validation/mean_average_precision': 0.2613781244846068, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04870419576764107, 'test/mean_average_precision': 0.249591743371493, 'test/num_examples': 43793, 'score': 5774.820902109146, 'total_duration': 8805.652330636978, 'accumulated_submission_time': 5774.820902109146, 'accumulated_eval_time': 3029.6452848911285, 'accumulated_logging_time': 0.6906979084014893, 'global_step': 18023, 'preemption_count': 0}), (18783, {'train/accuracy': 0.9919673204421997, 'train/loss': 0.02644052729010582, 'train/mean_average_precision': 0.5123992699519384, 'validation/accuracy': 0.98688805103302, 'validation/loss': 0.04558531567454338, 'validation/mean_average_precision': 0.2624033645561057, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.048399362713098526, 'test/mean_average_precision': 0.25564902037549775, 'test/num_examples': 43793, 'score': 6014.902328491211, 'total_duration': 9164.90726852417, 'accumulated_submission_time': 6014.902328491211, 'accumulated_eval_time': 3148.767208337784, 'accumulated_logging_time': 0.7213609218597412, 'global_step': 18783, 'preemption_count': 0}), (19536, {'train/accuracy': 0.9915534853935242, 'train/loss': 0.027401180937886238, 'train/mean_average_precision': 0.4995214715325733, 'validation/accuracy': 0.9868194460868835, 'validation/loss': 0.04584503918886185, 'validation/mean_average_precision': 0.26198630246261123, 'validation/num_examples': 43793, 'test/accuracy': 0.9859030246734619, 'test/loss': 0.048729538917541504, 'test/mean_average_precision': 0.24923894633275256, 'test/num_examples': 43793, 'score': 6255.073797464371, 'total_duration': 9524.977820158005, 'accumulated_submission_time': 6255.073797464371, 'accumulated_eval_time': 3268.615795850754, 'accumulated_logging_time': 0.7516143321990967, 'global_step': 19536, 'preemption_count': 0}), (20286, {'train/accuracy': 0.9918997883796692, 'train/loss': 0.026583749800920486, 'train/mean_average_precision': 0.5098556117578865, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04583797976374626, 'validation/mean_average_precision': 0.2618027021554986, 'validation/num_examples': 43793, 'test/accuracy': 0.9859623908996582, 'test/loss': 0.04834187030792236, 'test/mean_average_precision': 0.2521318678421627, 'test/num_examples': 43793, 'score': 6495.213454723358, 'total_duration': 9881.306076049805, 'accumulated_submission_time': 6495.213454723358, 'accumulated_eval_time': 3384.753395795822, 'accumulated_logging_time': 0.7820303440093994, 'global_step': 20286, 'preemption_count': 0}), (21038, {'train/accuracy': 0.9918867945671082, 'train/loss': 0.02638249844312668, 'train/mean_average_precision': 0.5210673442668449, 'validation/accuracy': 0.9867756366729736, 'validation/loss': 0.046111878007650375, 'validation/mean_average_precision': 0.2637069541466819, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04888920113444328, 'test/mean_average_precision': 0.2506835965684331, 'test/num_examples': 43793, 'score': 6735.299637794495, 'total_duration': 10244.107994318008, 'accumulated_submission_time': 6735.299637794495, 'accumulated_eval_time': 3507.4175040721893, 'accumulated_logging_time': 0.8135173320770264, 'global_step': 21038, 'preemption_count': 0}), (21776, {'train/accuracy': 0.9919347763061523, 'train/loss': 0.02610335312783718, 'train/mean_average_precision': 0.5254924327822671, 'validation/accuracy': 0.9867740273475647, 'validation/loss': 0.04628313332796097, 'validation/mean_average_precision': 0.26190666460667533, 'validation/num_examples': 43793, 'test/accuracy': 0.985910177230835, 'test/loss': 0.04922151193022728, 'test/mean_average_precision': 0.25348572629288885, 'test/num_examples': 43793, 'score': 6975.372453927994, 'total_duration': 10604.308366298676, 'accumulated_submission_time': 6975.372453927994, 'accumulated_eval_time': 3627.493724346161, 'accumulated_logging_time': 0.8439273834228516, 'global_step': 21776, 'preemption_count': 0}), (22521, {'train/accuracy': 0.9922031760215759, 'train/loss': 0.025213543325662613, 'train/mean_average_precision': 0.5488126686446302, 'validation/accuracy': 0.9866769909858704, 'validation/loss': 0.04680300131440163, 'validation/mean_average_precision': 0.256033160416531, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.04948793724179268, 'test/mean_average_precision': 0.24698618209813553, 'test/num_examples': 43793, 'score': 7215.407013654709, 'total_duration': 10961.120171308517, 'accumulated_submission_time': 7215.407013654709, 'accumulated_eval_time': 3744.2176728248596, 'accumulated_logging_time': 0.8761856555938721, 'global_step': 22521, 'preemption_count': 0}), (23271, {'train/accuracy': 0.9925545454025269, 'train/loss': 0.024180814623832703, 'train/mean_average_precision': 0.5611472685572075, 'validation/accuracy': 0.9866765737533569, 'validation/loss': 0.04666389152407646, 'validation/mean_average_precision': 0.25925900323497914, 'validation/num_examples': 43793, 'test/accuracy': 0.9858642220497131, 'test/loss': 0.04953271150588989, 'test/mean_average_precision': 0.2471890951017505, 'test/num_examples': 43793, 'score': 7455.3728721141815, 'total_duration': 11313.475859165192, 'accumulated_submission_time': 7455.3728721141815, 'accumulated_eval_time': 3856.554924964905, 'accumulated_logging_time': 0.9077761173248291, 'global_step': 23271, 'preemption_count': 0}), (24029, {'train/accuracy': 0.9927038550376892, 'train/loss': 0.023641152307391167, 'train/mean_average_precision': 0.5831575393716177, 'validation/accuracy': 0.9867139458656311, 'validation/loss': 0.046945538371801376, 'validation/mean_average_precision': 0.25290764972411883, 'validation/num_examples': 43793, 'test/accuracy': 0.9858760237693787, 'test/loss': 0.049796633422374725, 'test/mean_average_precision': 0.24441828961731113, 'test/num_examples': 43793, 'score': 7695.610192298889, 'total_duration': 11672.913708925247, 'accumulated_submission_time': 7695.610192298889, 'accumulated_eval_time': 3975.7028489112854, 'accumulated_logging_time': 0.9397668838500977, 'global_step': 24029, 'preemption_count': 0}), (24784, {'train/accuracy': 0.9929652214050293, 'train/loss': 0.022663027048110962, 'train/mean_average_precision': 0.6082374097411949, 'validation/accuracy': 0.9866116046905518, 'validation/loss': 0.04725179076194763, 'validation/mean_average_precision': 0.2516948713212647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.050070036202669144, 'test/mean_average_precision': 0.24522219280925725, 'test/num_examples': 43793, 'score': 7935.776249885559, 'total_duration': 12033.161201000214, 'accumulated_submission_time': 7935.776249885559, 'accumulated_eval_time': 4095.730885744095, 'accumulated_logging_time': 0.9724068641662598, 'global_step': 24784, 'preemption_count': 0}), (25532, {'train/accuracy': 0.9932979941368103, 'train/loss': 0.021950002759695053, 'train/mean_average_precision': 0.6178963070088231, 'validation/accuracy': 0.9866976737976074, 'validation/loss': 0.047483768314123154, 'validation/mean_average_precision': 0.26251559570858335, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.050649408251047134, 'test/mean_average_precision': 0.2503859213316042, 'test/num_examples': 43793, 'score': 8175.77491402626, 'total_duration': 12390.704937458038, 'accumulated_submission_time': 8175.77491402626, 'accumulated_eval_time': 4213.223459482193, 'accumulated_logging_time': 1.0048103332519531, 'global_step': 25532, 'preemption_count': 0}), (26287, {'train/accuracy': 0.9928412437438965, 'train/loss': 0.02331540733575821, 'train/mean_average_precision': 0.5911147703334756, 'validation/accuracy': 0.9866229891777039, 'validation/loss': 0.04788680747151375, 'validation/mean_average_precision': 0.24724923956040498, 'validation/num_examples': 43793, 'test/accuracy': 0.9856705069541931, 'test/loss': 0.050956495106220245, 'test/mean_average_precision': 0.2435300717592869, 'test/num_examples': 43793, 'score': 8415.784977912903, 'total_duration': 12748.397860050201, 'accumulated_submission_time': 8415.784977912903, 'accumulated_eval_time': 4330.8537838459015, 'accumulated_logging_time': 1.036971092224121, 'global_step': 26287, 'preemption_count': 0}), (27045, {'train/accuracy': 0.9928683638572693, 'train/loss': 0.02305789850652218, 'train/mean_average_precision': 0.5763528542791916, 'validation/accuracy': 0.9865012168884277, 'validation/loss': 0.04821200668811798, 'validation/mean_average_precision': 0.25106742497771933, 'validation/num_examples': 43793, 'test/accuracy': 0.9858288764953613, 'test/loss': 0.05099726840853691, 'test/mean_average_precision': 0.24492141409916557, 'test/num_examples': 43793, 'score': 8655.887106895447, 'total_duration': 13107.662237644196, 'accumulated_submission_time': 8655.887106895447, 'accumulated_eval_time': 4449.962881565094, 'accumulated_logging_time': 1.0698926448822021, 'global_step': 27045, 'preemption_count': 0}), (27803, {'train/accuracy': 0.9929357767105103, 'train/loss': 0.02285642921924591, 'train/mean_average_precision': 0.5939315356893492, 'validation/accuracy': 0.9865418076515198, 'validation/loss': 0.047837596386671066, 'validation/mean_average_precision': 0.2569972428572599, 'validation/num_examples': 43793, 'test/accuracy': 0.9856839776039124, 'test/loss': 0.050829917192459106, 'test/mean_average_precision': 0.24666290141204703, 'test/num_examples': 43793, 'score': 8895.934210538864, 'total_duration': 13469.756130218506, 'accumulated_submission_time': 8895.934210538864, 'accumulated_eval_time': 4571.956423997879, 'accumulated_logging_time': 1.1025707721710205, 'global_step': 27803, 'preemption_count': 0}), (28557, {'train/accuracy': 0.9927763342857361, 'train/loss': 0.023140892386436462, 'train/mean_average_precision': 0.5826014335653862, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04844299331307411, 'validation/mean_average_precision': 0.25352724356934786, 'validation/num_examples': 43793, 'test/accuracy': 0.9858267903327942, 'test/loss': 0.05125714838504791, 'test/mean_average_precision': 0.24444329182030758, 'test/num_examples': 43793, 'score': 9135.94087100029, 'total_duration': 13822.52390408516, 'accumulated_submission_time': 9135.94087100029, 'accumulated_eval_time': 4684.665537118912, 'accumulated_logging_time': 1.1340579986572266, 'global_step': 28557, 'preemption_count': 0}), (29310, {'train/accuracy': 0.992730975151062, 'train/loss': 0.023071737959980965, 'train/mean_average_precision': 0.5945630507535318, 'validation/accuracy': 0.9866514205932617, 'validation/loss': 0.048861317336559296, 'validation/mean_average_precision': 0.25436434873632463, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.05192745476961136, 'test/mean_average_precision': 0.24519639591880044, 'test/num_examples': 43793, 'score': 9375.945397377014, 'total_duration': 14182.795937299728, 'accumulated_submission_time': 9375.945397377014, 'accumulated_eval_time': 4804.87996172905, 'accumulated_logging_time': 1.1669306755065918, 'global_step': 29310, 'preemption_count': 0}), (30060, {'train/accuracy': 0.9930763244628906, 'train/loss': 0.022160854190587997, 'train/mean_average_precision': 0.6124207352776663, 'validation/accuracy': 0.9865061044692993, 'validation/loss': 0.04897496849298477, 'validation/mean_average_precision': 0.24776020527532236, 'validation/num_examples': 43793, 'test/accuracy': 0.9857067465782166, 'test/loss': 0.05196841433644295, 'test/mean_average_precision': 0.2419138446436762, 'test/num_examples': 43793, 'score': 9615.973546028137, 'total_duration': 14537.780702829361, 'accumulated_submission_time': 9615.973546028137, 'accumulated_eval_time': 4919.782430887222, 'accumulated_logging_time': 1.1999526023864746, 'global_step': 30060, 'preemption_count': 0}), (30814, {'train/accuracy': 0.9933850169181824, 'train/loss': 0.021051522344350815, 'train/mean_average_precision': 0.6370004880408724, 'validation/accuracy': 0.986537754535675, 'validation/loss': 0.04920114204287529, 'validation/mean_average_precision': 0.24948712036916235, 'validation/num_examples': 43793, 'test/accuracy': 0.9857239723205566, 'test/loss': 0.052162785083055496, 'test/mean_average_precision': 0.24383115834249408, 'test/num_examples': 43793, 'score': 9856.123964548111, 'total_duration': 14895.607670545578, 'accumulated_submission_time': 9856.123964548111, 'accumulated_eval_time': 5037.405160903931, 'accumulated_logging_time': 1.233625888824463, 'global_step': 30814, 'preemption_count': 0}), (31564, {'train/accuracy': 0.9937571883201599, 'train/loss': 0.020146047696471214, 'train/mean_average_precision': 0.6480886876305693, 'validation/accuracy': 0.9864618182182312, 'validation/loss': 0.04921168461441994, 'validation/mean_average_precision': 0.24653370613843473, 'validation/num_examples': 43793, 'test/accuracy': 0.9856313467025757, 'test/loss': 0.05249027535319328, 'test/mean_average_precision': 0.2379547686815906, 'test/num_examples': 43793, 'score': 10096.213876724243, 'total_duration': 15252.409235239029, 'accumulated_submission_time': 10096.213876724243, 'accumulated_eval_time': 5154.061851263046, 'accumulated_logging_time': 1.2679109573364258, 'global_step': 31564, 'preemption_count': 0}), (32313, {'train/accuracy': 0.9943658113479614, 'train/loss': 0.01861678436398506, 'train/mean_average_precision': 0.6874966935541733, 'validation/accuracy': 0.9864301681518555, 'validation/loss': 0.049264438450336456, 'validation/mean_average_precision': 0.24959787384144577, 'validation/num_examples': 43793, 'test/accuracy': 0.9856852293014526, 'test/loss': 0.052260324358940125, 'test/mean_average_precision': 0.2391222709634299, 'test/num_examples': 43793, 'score': 10336.165597438812, 'total_duration': 15610.812860250473, 'accumulated_submission_time': 10336.165597438812, 'accumulated_eval_time': 5272.460424661636, 'accumulated_logging_time': 1.301285743713379, 'global_step': 32313, 'preemption_count': 0}), (33071, {'train/accuracy': 0.9944509863853455, 'train/loss': 0.018283959478139877, 'train/mean_average_precision': 0.6934071978381392, 'validation/accuracy': 0.9864094853401184, 'validation/loss': 0.0501183457672596, 'validation/mean_average_precision': 0.24276209453510667, 'validation/num_examples': 43793, 'test/accuracy': 0.9856359362602234, 'test/loss': 0.053267572075128555, 'test/mean_average_precision': 0.23607333376372316, 'test/num_examples': 43793, 'score': 10576.188651800156, 'total_duration': 15967.97516155243, 'accumulated_submission_time': 10576.188651800156, 'accumulated_eval_time': 5389.545041322708, 'accumulated_logging_time': 1.3359241485595703, 'global_step': 33071, 'preemption_count': 0}), (33823, {'train/accuracy': 0.9944994449615479, 'train/loss': 0.01842135563492775, 'train/mean_average_precision': 0.6916567183095863, 'validation/accuracy': 0.9863566756248474, 'validation/loss': 0.04982062801718712, 'validation/mean_average_precision': 0.24413040287182172, 'validation/num_examples': 43793, 'test/accuracy': 0.9855618476867676, 'test/loss': 0.05272269994020462, 'test/mean_average_precision': 0.2433782019584294, 'test/num_examples': 43793, 'score': 10816.311703920364, 'total_duration': 16324.967136383057, 'accumulated_submission_time': 10816.311703920364, 'accumulated_eval_time': 5506.360715389252, 'accumulated_logging_time': 1.3688089847564697, 'global_step': 33823, 'preemption_count': 0}), (34579, {'train/accuracy': 0.9933765530586243, 'train/loss': 0.021374696865677834, 'train/mean_average_precision': 0.616448395606391, 'validation/accuracy': 0.9862044453620911, 'validation/loss': 0.050089042633771896, 'validation/mean_average_precision': 0.23755285265003115, 'validation/num_examples': 43793, 'test/accuracy': 0.9853659868240356, 'test/loss': 0.05300765857100487, 'test/mean_average_precision': 0.23444454646682908, 'test/num_examples': 43793, 'score': 11056.350311517715, 'total_duration': 16682.196016073227, 'accumulated_submission_time': 11056.350311517715, 'accumulated_eval_time': 5623.496901512146, 'accumulated_logging_time': 1.4026639461517334, 'global_step': 34579, 'preemption_count': 0}), (35330, {'train/accuracy': 0.9937258958816528, 'train/loss': 0.020007194951176643, 'train/mean_average_precision': 0.6484402300425647, 'validation/accuracy': 0.9864411354064941, 'validation/loss': 0.05105067417025566, 'validation/mean_average_precision': 0.24657327514169033, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.053873226046562195, 'test/mean_average_precision': 0.2414957014130568, 'test/num_examples': 43793, 'score': 11296.30923128128, 'total_duration': 17036.55020093918, 'accumulated_submission_time': 11296.30923128128, 'accumulated_eval_time': 5737.836226701736, 'accumulated_logging_time': 1.4372141361236572, 'global_step': 35330, 'preemption_count': 0}), (36077, {'train/accuracy': 0.9936544895172119, 'train/loss': 0.02011866867542267, 'train/mean_average_precision': 0.6498716669258316, 'validation/accuracy': 0.9863563179969788, 'validation/loss': 0.050975799560546875, 'validation/mean_average_precision': 0.24499919854954746, 'validation/num_examples': 43793, 'test/accuracy': 0.9854881167411804, 'test/loss': 0.054286763072013855, 'test/mean_average_precision': 0.233509910277653, 'test/num_examples': 43793, 'score': 11536.434869289398, 'total_duration': 17395.25648856163, 'accumulated_submission_time': 11536.434869289398, 'accumulated_eval_time': 5856.3613159656525, 'accumulated_logging_time': 1.4723279476165771, 'global_step': 36077, 'preemption_count': 0}), (36820, {'train/accuracy': 0.9935859441757202, 'train/loss': 0.02019241265952587, 'train/mean_average_precision': 0.6461987033015026, 'validation/accuracy': 0.9864293336868286, 'validation/loss': 0.051526058465242386, 'validation/mean_average_precision': 0.24125879341817963, 'validation/num_examples': 43793, 'test/accuracy': 0.9855584502220154, 'test/loss': 0.05475538596510887, 'test/mean_average_precision': 0.234687398547803, 'test/num_examples': 43793, 'score': 11776.005351305008, 'total_duration': 17753.795357704163, 'accumulated_submission_time': 11776.005351305008, 'accumulated_eval_time': 5974.8999898433685, 'accumulated_logging_time': 1.880678653717041, 'global_step': 36820, 'preemption_count': 0}), (37569, {'train/accuracy': 0.9937519431114197, 'train/loss': 0.019834209233522415, 'train/mean_average_precision': 0.6552595150078337, 'validation/accuracy': 0.9864228963851929, 'validation/loss': 0.05131762847304344, 'validation/mean_average_precision': 0.24673181391672155, 'validation/num_examples': 43793, 'test/accuracy': 0.9854522943496704, 'test/loss': 0.05443757772445679, 'test/mean_average_precision': 0.23268169125564953, 'test/num_examples': 43793, 'score': 12016.200606584549, 'total_duration': 18110.347497224808, 'accumulated_submission_time': 12016.200606584549, 'accumulated_eval_time': 6091.20179772377, 'accumulated_logging_time': 1.9143686294555664, 'global_step': 37569, 'preemption_count': 0}), (38325, {'train/accuracy': 0.9941584467887878, 'train/loss': 0.018329985439777374, 'train/mean_average_precision': 0.6959283207416935, 'validation/accuracy': 0.9863651990890503, 'validation/loss': 0.05230919271707535, 'validation/mean_average_precision': 0.24253230329771047, 'validation/num_examples': 43793, 'test/accuracy': 0.9854717254638672, 'test/loss': 0.05553451552987099, 'test/mean_average_precision': 0.22832626069700715, 'test/num_examples': 43793, 'score': 12256.40864610672, 'total_duration': 18466.906888246536, 'accumulated_submission_time': 12256.40864610672, 'accumulated_eval_time': 6207.4986119270325, 'accumulated_logging_time': 1.9481298923492432, 'global_step': 38325, 'preemption_count': 0}), (39084, {'train/accuracy': 0.9948763251304626, 'train/loss': 0.01684432290494442, 'train/mean_average_precision': 0.7108119769907197, 'validation/accuracy': 0.9862645268440247, 'validation/loss': 0.051835693418979645, 'validation/mean_average_precision': 0.24291645434349401, 'validation/num_examples': 43793, 'test/accuracy': 0.9854000806808472, 'test/loss': 0.05496647581458092, 'test/mean_average_precision': 0.23119369630632372, 'test/num_examples': 43793, 'score': 12496.516707897186, 'total_duration': 18823.623073339462, 'accumulated_submission_time': 12496.516707897186, 'accumulated_eval_time': 6324.049305677414, 'accumulated_logging_time': 1.984095573425293, 'global_step': 39084, 'preemption_count': 0}), (39838, {'train/accuracy': 0.9953045845031738, 'train/loss': 0.015735676512122154, 'train/mean_average_precision': 0.743408396758674, 'validation/accuracy': 0.986170768737793, 'validation/loss': 0.05237202346324921, 'validation/mean_average_precision': 0.24157881853185634, 'validation/num_examples': 43793, 'test/accuracy': 0.9853402972221375, 'test/loss': 0.05575835704803467, 'test/mean_average_precision': 0.23236867697136018, 'test/num_examples': 43793, 'score': 12736.719133377075, 'total_duration': 19179.72560429573, 'accumulated_submission_time': 12736.719133377075, 'accumulated_eval_time': 6439.895386219025, 'accumulated_logging_time': 2.017606496810913, 'global_step': 39838, 'preemption_count': 0}), (40588, {'train/accuracy': 0.9955078363418579, 'train/loss': 0.015150746330618858, 'train/mean_average_precision': 0.7610764629387446, 'validation/accuracy': 0.9862247705459595, 'validation/loss': 0.05289914831519127, 'validation/mean_average_precision': 0.23656075026347692, 'validation/num_examples': 43793, 'test/accuracy': 0.9853579998016357, 'test/loss': 0.05615391209721565, 'test/mean_average_precision': 0.2277253445759712, 'test/num_examples': 43793, 'score': 12976.663056850433, 'total_duration': 19532.37944626808, 'accumulated_submission_time': 12976.663056850433, 'accumulated_eval_time': 6552.548879623413, 'accumulated_logging_time': 2.0531373023986816, 'global_step': 40588, 'preemption_count': 0}), (41337, {'train/accuracy': 0.9952184557914734, 'train/loss': 0.01582160033285618, 'train/mean_average_precision': 0.7453891223328923, 'validation/accuracy': 0.9862101674079895, 'validation/loss': 0.05295562744140625, 'validation/mean_average_precision': 0.24187175966019298, 'validation/num_examples': 43793, 'test/accuracy': 0.985393762588501, 'test/loss': 0.056329395622015, 'test/mean_average_precision': 0.22938868223573725, 'test/num_examples': 43793, 'score': 13216.849076271057, 'total_duration': 19884.684267520905, 'accumulated_submission_time': 13216.849076271057, 'accumulated_eval_time': 6664.61195230484, 'accumulated_logging_time': 2.0880980491638184, 'global_step': 41337, 'preemption_count': 0}), (42091, {'train/accuracy': 0.9948252439498901, 'train/loss': 0.016748318448662758, 'train/mean_average_precision': 0.7169117412420599, 'validation/accuracy': 0.9861918687820435, 'validation/loss': 0.05373343452811241, 'validation/mean_average_precision': 0.23533939751035776, 'validation/num_examples': 43793, 'test/accuracy': 0.985342800617218, 'test/loss': 0.056724365800619125, 'test/mean_average_precision': 0.231405241881548, 'test/num_examples': 43793, 'score': 13457.108339548111, 'total_duration': 20238.608663082123, 'accumulated_submission_time': 13457.108339548111, 'accumulated_eval_time': 6778.221654415131, 'accumulated_logging_time': 2.122004747390747, 'global_step': 42091, 'preemption_count': 0}), (42845, {'train/accuracy': 0.9936889410018921, 'train/loss': 0.019539348781108856, 'train/mean_average_precision': 0.6601647292398976, 'validation/accuracy': 0.9862625002861023, 'validation/loss': 0.05407680943608284, 'validation/mean_average_precision': 0.2355572185518675, 'validation/num_examples': 43793, 'test/accuracy': 0.9853861927986145, 'test/loss': 0.057481955736875534, 'test/mean_average_precision': 0.22742230463765503, 'test/num_examples': 43793, 'score': 13697.295625209808, 'total_duration': 20591.67139029503, 'accumulated_submission_time': 13697.295625209808, 'accumulated_eval_time': 6891.040193080902, 'accumulated_logging_time': 2.158005714416504, 'global_step': 42845, 'preemption_count': 0}), (43601, {'train/accuracy': 0.9939095377922058, 'train/loss': 0.01894284412264824, 'train/mean_average_precision': 0.6758613495698387, 'validation/accuracy': 0.9861545562744141, 'validation/loss': 0.05472370237112045, 'validation/mean_average_precision': 0.23235470043494322, 'validation/num_examples': 43793, 'test/accuracy': 0.9854089617729187, 'test/loss': 0.05789602920413017, 'test/mean_average_precision': 0.22984336740164787, 'test/num_examples': 43793, 'score': 13937.247032403946, 'total_duration': 20946.570340633392, 'accumulated_submission_time': 13937.247032403946, 'accumulated_eval_time': 7005.932266712189, 'accumulated_logging_time': 2.1925864219665527, 'global_step': 43601, 'preemption_count': 0}), (44353, {'train/accuracy': 0.994534432888031, 'train/loss': 0.017085274681448936, 'train/mean_average_precision': 0.7098776166423071, 'validation/accuracy': 0.9860822558403015, 'validation/loss': 0.054855622351169586, 'validation/mean_average_precision': 0.2360641167700365, 'validation/num_examples': 43793, 'test/accuracy': 0.9852712154388428, 'test/loss': 0.058071594685316086, 'test/mean_average_precision': 0.22560777626673703, 'test/num_examples': 43793, 'score': 14177.268211841583, 'total_duration': 21297.09924530983, 'accumulated_submission_time': 14177.268211841583, 'accumulated_eval_time': 7116.383370637894, 'accumulated_logging_time': 2.228288173675537, 'global_step': 44353, 'preemption_count': 0}), (45098, {'train/accuracy': 0.9942643046379089, 'train/loss': 0.017705237492918968, 'train/mean_average_precision': 0.7059773931270805, 'validation/accuracy': 0.9860595464706421, 'validation/loss': 0.055160850286483765, 'validation/mean_average_precision': 0.23142329750694968, 'validation/num_examples': 43793, 'test/accuracy': 0.9852467775344849, 'test/loss': 0.05843737721443176, 'test/mean_average_precision': 0.2243313486320527, 'test/num_examples': 43793, 'score': 14417.231942176819, 'total_duration': 21651.221431732178, 'accumulated_submission_time': 14417.231942176819, 'accumulated_eval_time': 7230.485638141632, 'accumulated_logging_time': 2.2639756202697754, 'global_step': 45098, 'preemption_count': 0}), (45843, {'train/accuracy': 0.9943100214004517, 'train/loss': 0.01757192611694336, 'train/mean_average_precision': 0.711232518020157, 'validation/accuracy': 0.9860436916351318, 'validation/loss': 0.05571623891592026, 'validation/mean_average_precision': 0.23105697250320886, 'validation/num_examples': 43793, 'test/accuracy': 0.9852017164230347, 'test/loss': 0.0595104843378067, 'test/mean_average_precision': 0.2203197279736102, 'test/num_examples': 43793, 'score': 14657.17865562439, 'total_duration': 22004.224217414856, 'accumulated_submission_time': 14657.17865562439, 'accumulated_eval_time': 7343.484463214874, 'accumulated_logging_time': 2.3002424240112305, 'global_step': 45843, 'preemption_count': 0}), (46594, {'train/accuracy': 0.9962734580039978, 'train/loss': 0.012980311177670956, 'train/mean_average_precision': 0.8045735996318846, 'validation/accuracy': 0.9860225915908813, 'validation/loss': 0.05560600012540817, 'validation/mean_average_precision': 0.2332473908672151, 'validation/num_examples': 43793, 'test/accuracy': 0.9852076172828674, 'test/loss': 0.058608949184417725, 'test/mean_average_precision': 0.2262182866703456, 'test/num_examples': 43793, 'score': 14897.215474367142, 'total_duration': 22358.96573448181, 'accumulated_submission_time': 14897.215474367142, 'accumulated_eval_time': 7458.133021593094, 'accumulated_logging_time': 2.3357396125793457, 'global_step': 46594, 'preemption_count': 0}), (47350, {'train/accuracy': 0.9964053630828857, 'train/loss': 0.012584680691361427, 'train/mean_average_precision': 0.8052383302908812, 'validation/accuracy': 0.9860960841178894, 'validation/loss': 0.05634652078151703, 'validation/mean_average_precision': 0.2354068295352333, 'validation/num_examples': 43793, 'test/accuracy': 0.9852636456489563, 'test/loss': 0.0596487857401371, 'test/mean_average_precision': 0.22829275553690145, 'test/num_examples': 43793, 'score': 15137.289443016052, 'total_duration': 22709.003462314606, 'accumulated_submission_time': 15137.289443016052, 'accumulated_eval_time': 7568.040026426315, 'accumulated_logging_time': 2.3717122077941895, 'global_step': 47350, 'preemption_count': 0}), (48094, {'train/accuracy': 0.9965007305145264, 'train/loss': 0.012393580749630928, 'train/mean_average_precision': 0.8195799484038679, 'validation/accuracy': 0.9859645366668701, 'validation/loss': 0.05673098936676979, 'validation/mean_average_precision': 0.22698595600141783, 'validation/num_examples': 43793, 'test/accuracy': 0.9851579070091248, 'test/loss': 0.06013302132487297, 'test/mean_average_precision': 0.22456300586969077, 'test/num_examples': 43793, 'score': 15377.269327640533, 'total_duration': 23060.869948148727, 'accumulated_submission_time': 15377.269327640533, 'accumulated_eval_time': 7679.870763301849, 'accumulated_logging_time': 2.407210350036621, 'global_step': 48094, 'preemption_count': 0}), (48846, {'train/accuracy': 0.9960833191871643, 'train/loss': 0.013001606799662113, 'train/mean_average_precision': 0.8006489196358728, 'validation/accuracy': 0.9861443638801575, 'validation/loss': 0.05728606507182121, 'validation/mean_average_precision': 0.2332671815231344, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.06086369603872299, 'test/mean_average_precision': 0.22346355660484257, 'test/num_examples': 43793, 'score': 15617.50436925888, 'total_duration': 23413.28269124031, 'accumulated_submission_time': 15617.50436925888, 'accumulated_eval_time': 7791.991349935532, 'accumulated_logging_time': 2.443833827972412, 'global_step': 48846, 'preemption_count': 0}), (49595, {'train/accuracy': 0.9956724047660828, 'train/loss': 0.013841000385582447, 'train/mean_average_precision': 0.797884302093334, 'validation/accuracy': 0.9861240983009338, 'validation/loss': 0.05729652941226959, 'validation/mean_average_precision': 0.23156148534243237, 'validation/num_examples': 43793, 'test/accuracy': 0.9852871894836426, 'test/loss': 0.06051979586482048, 'test/mean_average_precision': 0.22366266750840783, 'test/num_examples': 43793, 'score': 15857.626742601395, 'total_duration': 23764.239976644516, 'accumulated_submission_time': 15857.626742601395, 'accumulated_eval_time': 7902.767471790314, 'accumulated_logging_time': 2.482178211212158, 'global_step': 49595, 'preemption_count': 0}), (50347, {'train/accuracy': 0.9952304363250732, 'train/loss': 0.014908693730831146, 'train/mean_average_precision': 0.7654407735773269, 'validation/accuracy': 0.9860441088676453, 'validation/loss': 0.058121465146541595, 'validation/mean_average_precision': 0.231803685346562, 'validation/num_examples': 43793, 'test/accuracy': 0.9851604104042053, 'test/loss': 0.06149018928408623, 'test/mean_average_precision': 0.2223522837272326, 'test/num_examples': 43793, 'score': 16097.812799215317, 'total_duration': 24110.558915138245, 'accumulated_submission_time': 16097.812799215317, 'accumulated_eval_time': 8008.843861818314, 'accumulated_logging_time': 2.518423318862915, 'global_step': 50347, 'preemption_count': 0}), (51099, {'train/accuracy': 0.9944838285446167, 'train/loss': 0.01668316312134266, 'train/mean_average_precision': 0.735212748790653, 'validation/accuracy': 0.9861963391304016, 'validation/loss': 0.058478597551584244, 'validation/mean_average_precision': 0.23081843631601204, 'validation/num_examples': 43793, 'test/accuracy': 0.9852526783943176, 'test/loss': 0.06204073876142502, 'test/mean_average_precision': 0.22100602733148664, 'test/num_examples': 43793, 'score': 16337.775570392609, 'total_duration': 24461.129461288452, 'accumulated_submission_time': 16337.775570392609, 'accumulated_eval_time': 8119.3956298828125, 'accumulated_logging_time': 2.554534435272217, 'global_step': 51099, 'preemption_count': 0}), (51855, {'train/accuracy': 0.9948172569274902, 'train/loss': 0.0157752875238657, 'train/mean_average_precision': 0.7602446975067121, 'validation/accuracy': 0.9861037731170654, 'validation/loss': 0.0588659830391407, 'validation/mean_average_precision': 0.23148958688131108, 'validation/num_examples': 43793, 'test/accuracy': 0.9852792024612427, 'test/loss': 0.062334612011909485, 'test/mean_average_precision': 0.22103803290156743, 'test/num_examples': 43793, 'score': 16577.728811979294, 'total_duration': 24809.496871471405, 'accumulated_submission_time': 16577.728811979294, 'accumulated_eval_time': 8227.75286102295, 'accumulated_logging_time': 2.5908830165863037, 'global_step': 51855, 'preemption_count': 0}), (52612, {'train/accuracy': 0.9952720999717712, 'train/loss': 0.01456244383007288, 'train/mean_average_precision': 0.7853455614625474, 'validation/accuracy': 0.9860274791717529, 'validation/loss': 0.05888041481375694, 'validation/mean_average_precision': 0.23159215014093068, 'validation/num_examples': 43793, 'test/accuracy': 0.9851730465888977, 'test/loss': 0.06262774020433426, 'test/mean_average_precision': 0.21721341671585395, 'test/num_examples': 43793, 'score': 16817.936812877655, 'total_duration': 25159.259149074554, 'accumulated_submission_time': 16817.936812877655, 'accumulated_eval_time': 8337.247876405716, 'accumulated_logging_time': 2.629739284515381, 'global_step': 52612, 'preemption_count': 0}), (53365, {'train/accuracy': 0.9949951767921448, 'train/loss': 0.015158366411924362, 'train/mean_average_precision': 0.7691710098606686, 'validation/accuracy': 0.9860047698020935, 'validation/loss': 0.05945604667067528, 'validation/mean_average_precision': 0.229525978638166, 'validation/num_examples': 43793, 'test/accuracy': 0.9851570725440979, 'test/loss': 0.06314238905906677, 'test/mean_average_precision': 0.2154677620242295, 'test/num_examples': 43793, 'score': 17057.958690166473, 'total_duration': 25511.86580467224, 'accumulated_submission_time': 17057.958690166473, 'accumulated_eval_time': 8449.776660203934, 'accumulated_logging_time': 2.665452480316162, 'global_step': 53365, 'preemption_count': 0}), (54120, {'train/accuracy': 0.9957131147384644, 'train/loss': 0.013677745126187801, 'train/mean_average_precision': 0.805361075076067, 'validation/accuracy': 0.9861322045326233, 'validation/loss': 0.060508742928504944, 'validation/mean_average_precision': 0.23027808961236285, 'validation/num_examples': 43793, 'test/accuracy': 0.9852569103240967, 'test/loss': 0.06417310237884521, 'test/mean_average_precision': 0.21644151179846766, 'test/num_examples': 43793, 'score': 17298.226668834686, 'total_duration': 25861.56529688835, 'accumulated_submission_time': 17298.226668834686, 'accumulated_eval_time': 8559.149673700333, 'accumulated_logging_time': 2.7035751342773438, 'global_step': 54120, 'preemption_count': 0}), (54869, {'train/accuracy': 0.9979147911071777, 'train/loss': 0.009291942231357098, 'train/mean_average_precision': 0.8778368365167077, 'validation/accuracy': 0.9860007166862488, 'validation/loss': 0.0598200224339962, 'validation/mean_average_precision': 0.22958359651974927, 'validation/num_examples': 43793, 'test/accuracy': 0.9850584864616394, 'test/loss': 0.06363627314567566, 'test/mean_average_precision': 0.21293531794694817, 'test/num_examples': 43793, 'score': 17538.3272023201, 'total_duration': 26212.610743045807, 'accumulated_submission_time': 17538.3272023201, 'accumulated_eval_time': 8670.035662651062, 'accumulated_logging_time': 2.741891622543335, 'global_step': 54869, 'preemption_count': 0}), (55620, {'train/accuracy': 0.9977280497550964, 'train/loss': 0.009441524744033813, 'train/mean_average_precision': 0.886361785591973, 'validation/accuracy': 0.9860274791717529, 'validation/loss': 0.05996182933449745, 'validation/mean_average_precision': 0.22747897461906752, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.06353277713060379, 'test/mean_average_precision': 0.21295138216163578, 'test/num_examples': 43793, 'score': 17778.506385326385, 'total_duration': 26565.708988189697, 'accumulated_submission_time': 17778.506385326385, 'accumulated_eval_time': 8782.897083044052, 'accumulated_logging_time': 2.7787158489227295, 'global_step': 55620, 'preemption_count': 0}), (56372, {'train/accuracy': 0.9974233508110046, 'train/loss': 0.009903688915073872, 'train/mean_average_precision': 0.8731948836777758, 'validation/accuracy': 0.9860575199127197, 'validation/loss': 0.060540780425071716, 'validation/mean_average_precision': 0.22406773213074888, 'validation/num_examples': 43793, 'test/accuracy': 0.9851629734039307, 'test/loss': 0.06434593349695206, 'test/mean_average_precision': 0.21004910877619207, 'test/num_examples': 43793, 'score': 18018.62752890587, 'total_duration': 26921.62234067917, 'accumulated_submission_time': 18018.62752890587, 'accumulated_eval_time': 8898.630964279175, 'accumulated_logging_time': 2.816796064376831, 'global_step': 56372, 'preemption_count': 0}), (57128, {'train/accuracy': 0.9969244599342346, 'train/loss': 0.010655700229108334, 'train/mean_average_precision': 0.8580153795451726, 'validation/accuracy': 0.986025869846344, 'validation/loss': 0.06091301143169403, 'validation/mean_average_precision': 0.22415679755386675, 'validation/num_examples': 43793, 'test/accuracy': 0.9850812554359436, 'test/loss': 0.06488525122404099, 'test/mean_average_precision': 0.21352743763209386, 'test/num_examples': 43793, 'score': 18258.657961845398, 'total_duration': 27272.670634508133, 'accumulated_submission_time': 18258.657961845398, 'accumulated_eval_time': 9009.587788581848, 'accumulated_logging_time': 2.856903314590454, 'global_step': 57128, 'preemption_count': 0})], 'global_step': 57807}
I0205 19:20:24.828983 139978932307776 submission_runner.py:586] Timing: 18477.262572288513
I0205 19:20:24.829053 139978932307776 submission_runner.py:588] Total number of evals: 77
I0205 19:20:24.829100 139978932307776 submission_runner.py:589] ====================
I0205 19:20:24.829152 139978932307776 submission_runner.py:542] Using RNG seed 3917441912
I0205 19:20:24.905614 139978932307776 submission_runner.py:551] --- Tuning run 3/5 ---
I0205 19:20:24.905827 139978932307776 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3.
I0205 19:20:24.906090 139978932307776 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3/hparams.json.
I0205 19:20:25.062143 139978932307776 submission_runner.py:206] Initializing dataset.
I0205 19:20:25.163650 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 19:20:25.168483 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 19:20:25.316798 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 19:20:25.358299 139978932307776 submission_runner.py:213] Initializing model.
I0205 19:20:29.925009 139978932307776 submission_runner.py:255] Initializing optimizer.
I0205 19:20:30.543295 139978932307776 submission_runner.py:262] Initializing metrics bundle.
I0205 19:20:30.543502 139978932307776 submission_runner.py:280] Initializing checkpoint and logger.
I0205 19:20:30.544173 139978932307776 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3 with prefix checkpoint_
I0205 19:20:30.544294 139978932307776 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3/meta_data_0.json.
I0205 19:20:30.544501 139978932307776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 19:20:30.544561 139978932307776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 19:20:32.310676 139978932307776 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 19:20:34.057228 139978932307776 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3/flags_0.json.
I0205 19:20:34.061958 139978932307776 submission_runner.py:314] Starting training loop.
I0205 19:20:45.587792 139770764113664 logging_writer.py:48] [0] global_step=0, grad_norm=3.1811904907226562, loss=0.7991228103637695
I0205 19:20:45.598065 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:22:31.736780 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:22:35.018107 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:22:38.002908 139978932307776 submission_runner.py:408] Time since start: 123.94s, 	Step: 1, 	{'train/accuracy': 0.3884694278240204, 'train/loss': 0.7994099855422974, 'train/mean_average_precision': 0.021870049108438757, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02483726344495895, 'validation/num_examples': 43793, 'test/accuracy': 0.3947484791278839, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026385459559166446, 'test/num_examples': 43793, 'score': 11.536062002182007, 'total_duration': 123.94088912010193, 'accumulated_submission_time': 11.536062002182007, 'accumulated_eval_time': 112.40478038787842, 'accumulated_logging_time': 0}
I0205 19:22:38.013261 139770780927744 logging_writer.py:48] [1] accumulated_eval_time=112.404780, accumulated_logging_time=0, accumulated_submission_time=11.536062, global_step=1, preemption_count=0, score=11.536062, test/accuracy=0.394748, test/loss=0.795676, test/mean_average_precision=0.026385, test/num_examples=43793, total_duration=123.940889, train/accuracy=0.388469, train/loss=0.799410, train/mean_average_precision=0.021870, validation/accuracy=0.392645, validation/loss=0.797466, validation/mean_average_precision=0.024837, validation/num_examples=43793
I0205 19:23:10.225231 139789425600256 logging_writer.py:48] [100] global_step=100, grad_norm=0.7057592868804932, loss=0.5042977929115295
I0205 19:23:42.156112 139770780927744 logging_writer.py:48] [200] global_step=200, grad_norm=0.41106924414634705, loss=0.3638734817504883
I0205 19:24:14.407600 139789425600256 logging_writer.py:48] [300] global_step=300, grad_norm=0.3004800081253052, loss=0.26094740629196167
I0205 19:24:46.144034 139770780927744 logging_writer.py:48] [400] global_step=400, grad_norm=0.20156635344028473, loss=0.17559422552585602
I0205 19:25:18.069387 139789425600256 logging_writer.py:48] [500] global_step=500, grad_norm=0.12518756091594696, loss=0.12303497642278671
I0205 19:25:49.858709 139770780927744 logging_writer.py:48] [600] global_step=600, grad_norm=0.08223223686218262, loss=0.09022561460733414
I0205 19:26:21.962277 139789425600256 logging_writer.py:48] [700] global_step=700, grad_norm=0.09540457278490067, loss=0.08624209463596344
I0205 19:26:38.172461 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:28:24.128493 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:28:27.104701 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:28:30.066356 139978932307776 submission_runner.py:408] Time since start: 476.00s, 	Step: 752, 	{'train/accuracy': 0.986751914024353, 'train/loss': 0.07101087272167206, 'train/mean_average_precision': 0.03521220543918937, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07898423075675964, 'validation/mean_average_precision': 0.038268597202817944, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08169642835855484, 'test/mean_average_precision': 0.039718402530105536, 'test/num_examples': 43793, 'score': 251.66132426261902, 'total_duration': 476.00432682037354, 'accumulated_submission_time': 251.66132426261902, 'accumulated_eval_time': 224.29861760139465, 'accumulated_logging_time': 0.024303913116455078}
I0205 19:28:30.081793 139770806105856 logging_writer.py:48] [752] accumulated_eval_time=224.298618, accumulated_logging_time=0.024304, accumulated_submission_time=251.661324, global_step=752, preemption_count=0, score=251.661324, test/accuracy=0.983142, test/loss=0.081696, test/mean_average_precision=0.039718, test/num_examples=43793, total_duration=476.004327, train/accuracy=0.986752, train/loss=0.071011, train/mean_average_precision=0.035212, validation/accuracy=0.984118, validation/loss=0.078984, validation/mean_average_precision=0.038269, validation/num_examples=43793
I0205 19:28:46.148746 139788813231872 logging_writer.py:48] [800] global_step=800, grad_norm=0.14346809685230255, loss=0.061537086963653564
I0205 19:29:18.499242 139770806105856 logging_writer.py:48] [900] global_step=900, grad_norm=0.035710543394088745, loss=0.06264777481555939
I0205 19:29:50.269610 139788813231872 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.10783373564481735, loss=0.061865001916885376
I0205 19:30:22.565831 139770806105856 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.06522165983915329, loss=0.05491487681865692
I0205 19:30:54.402897 139788813231872 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.09450070559978485, loss=0.04817678779363632
I0205 19:31:26.169205 139770806105856 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.07995063811540604, loss=0.045516375452280045
I0205 19:31:58.326444 139788813231872 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.13089513778686523, loss=0.04362360015511513
I0205 19:32:30.177768 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:34:19.777468 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:34:22.845184 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:34:25.822568 139978932307776 submission_runner.py:408] Time since start: 831.76s, 	Step: 1500, 	{'train/accuracy': 0.9871714115142822, 'train/loss': 0.04863082617521286, 'train/mean_average_precision': 0.08834327833907998, 'validation/accuracy': 0.9846265912055969, 'validation/loss': 0.057549361139535904, 'validation/mean_average_precision': 0.09029757624371303, 'validation/num_examples': 43793, 'test/accuracy': 0.9836398959159851, 'test/loss': 0.06086340546607971, 'test/mean_average_precision': 0.08853657797377494, 'test/num_examples': 43793, 'score': 491.7242970466614, 'total_duration': 831.7605443000793, 'accumulated_submission_time': 491.7242970466614, 'accumulated_eval_time': 339.94337153434753, 'accumulated_logging_time': 0.05216574668884277}
I0205 19:34:25.838776 139770797713152 logging_writer.py:48] [1500] accumulated_eval_time=339.943372, accumulated_logging_time=0.052166, accumulated_submission_time=491.724297, global_step=1500, preemption_count=0, score=491.724297, test/accuracy=0.983640, test/loss=0.060863, test/mean_average_precision=0.088537, test/num_examples=43793, total_duration=831.760544, train/accuracy=0.987171, train/loss=0.048631, train/mean_average_precision=0.088343, validation/accuracy=0.984627, validation/loss=0.057549, validation/mean_average_precision=0.090298, validation/num_examples=43793
I0205 19:34:26.175250 139789425600256 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1476263701915741, loss=0.05370939150452614
I0205 19:34:58.051693 139770797713152 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1377248913049698, loss=0.05157916992902756
I0205 19:35:30.346448 139789425600256 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.2320505976676941, loss=0.04844256862998009
I0205 19:36:02.551350 139770797713152 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1459280103445053, loss=0.0463063009083271
I0205 19:36:34.457314 139789425600256 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06450609862804413, loss=0.043713413178920746
I0205 19:37:06.868294 139770797713152 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.21636100113391876, loss=0.04844292998313904
I0205 19:37:39.049853 139789425600256 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.14563128352165222, loss=0.04191479831933975
I0205 19:38:11.134996 139770797713152 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.2289346605539322, loss=0.04375142976641655
I0205 19:38:26.069628 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:40:11.068328 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:40:14.091156 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:40:17.094306 139978932307776 submission_runner.py:408] Time since start: 1183.03s, 	Step: 2248, 	{'train/accuracy': 0.9878830909729004, 'train/loss': 0.04364940524101257, 'train/mean_average_precision': 0.13569135498524446, 'validation/accuracy': 0.9850901365280151, 'validation/loss': 0.052616462111473083, 'validation/mean_average_precision': 0.1338274383866124, 'validation/num_examples': 43793, 'test/accuracy': 0.9841011166572571, 'test/loss': 0.055503834038972855, 'test/mean_average_precision': 0.13269239460812846, 'test/num_examples': 43793, 'score': 731.9238619804382, 'total_duration': 1183.0322844982147, 'accumulated_submission_time': 731.9238619804382, 'accumulated_eval_time': 450.96800208091736, 'accumulated_logging_time': 0.07912993431091309}
I0205 19:40:17.110608 139770806105856 logging_writer.py:48] [2248] accumulated_eval_time=450.968002, accumulated_logging_time=0.079130, accumulated_submission_time=731.923862, global_step=2248, preemption_count=0, score=731.923862, test/accuracy=0.984101, test/loss=0.055504, test/mean_average_precision=0.132692, test/num_examples=43793, total_duration=1183.032284, train/accuracy=0.987883, train/loss=0.043649, train/mean_average_precision=0.135691, validation/accuracy=0.985090, validation/loss=0.052616, validation/mean_average_precision=0.133827, validation/num_examples=43793
I0205 19:40:34.578534 139788813231872 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.23404182493686676, loss=0.04675481468439102
I0205 19:41:06.528652 139770806105856 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.1527654230594635, loss=0.045212168246507645
I0205 19:41:38.739782 139788813231872 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2729344666004181, loss=0.04299600422382355
I0205 19:42:10.601513 139770806105856 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.07858769595623016, loss=0.045054808259010315
I0205 19:42:42.780921 139788813231872 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.11403607577085495, loss=0.0451294407248497
I0205 19:43:15.030966 139770806105856 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.08036104589700699, loss=0.04020066559314728
I0205 19:43:47.089420 139788813231872 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.1136588603258133, loss=0.04380306601524353
I0205 19:44:17.361612 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:45:59.394669 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:46:02.456465 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:46:05.437109 139978932307776 submission_runner.py:408] Time since start: 1531.38s, 	Step: 2995, 	{'train/accuracy': 0.9882432818412781, 'train/loss': 0.04136427119374275, 'train/mean_average_precision': 0.16459796593907855, 'validation/accuracy': 0.9853106141090393, 'validation/loss': 0.051228564232587814, 'validation/mean_average_precision': 0.16076437255622866, 'validation/num_examples': 43793, 'test/accuracy': 0.9844043850898743, 'test/loss': 0.054126448929309845, 'test/mean_average_precision': 0.15506180067662756, 'test/num_examples': 43793, 'score': 972.1416659355164, 'total_duration': 1531.3750817775726, 'accumulated_submission_time': 972.1416659355164, 'accumulated_eval_time': 559.0434498786926, 'accumulated_logging_time': 0.1082615852355957}
I0205 19:46:05.453920 139770797713152 logging_writer.py:48] [2995] accumulated_eval_time=559.043450, accumulated_logging_time=0.108262, accumulated_submission_time=972.141666, global_step=2995, preemption_count=0, score=972.141666, test/accuracy=0.984404, test/loss=0.054126, test/mean_average_precision=0.155062, test/num_examples=43793, total_duration=1531.375082, train/accuracy=0.988243, train/loss=0.041364, train/mean_average_precision=0.164598, validation/accuracy=0.985311, validation/loss=0.051229, validation/mean_average_precision=0.160764, validation/num_examples=43793
I0205 19:46:07.420347 139789425600256 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.1260482370853424, loss=0.044116321951150894
I0205 19:46:39.583884 139770797713152 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.05877033621072769, loss=0.039047520607709885
I0205 19:47:11.508939 139789425600256 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.05175565183162689, loss=0.041194379329681396
I0205 19:47:43.195962 139770797713152 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.10964660346508026, loss=0.0366472564637661
I0205 19:48:15.245714 139789425600256 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.09835980087518692, loss=0.039434172213077545
I0205 19:48:47.121096 139770797713152 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05262034758925438, loss=0.039065808057785034
I0205 19:49:19.151530 139789425600256 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.061288982629776, loss=0.039773713797330856
I0205 19:49:51.328616 139770797713152 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.09182650595903397, loss=0.03756420686841011
I0205 19:50:05.673826 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:51:50.222563 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:51:53.284487 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:51:56.275895 139978932307776 submission_runner.py:408] Time since start: 1882.21s, 	Step: 3746, 	{'train/accuracy': 0.9883965849876404, 'train/loss': 0.04021444916725159, 'train/mean_average_precision': 0.18359365003802192, 'validation/accuracy': 0.9854680895805359, 'validation/loss': 0.05016530677676201, 'validation/mean_average_precision': 0.16619727048449834, 'validation/num_examples': 43793, 'test/accuracy': 0.984574556350708, 'test/loss': 0.052942439913749695, 'test/mean_average_precision': 0.1661460566473754, 'test/num_examples': 43793, 'score': 1212.3301203250885, 'total_duration': 1882.2137567996979, 'accumulated_submission_time': 1212.3301203250885, 'accumulated_eval_time': 669.6453518867493, 'accumulated_logging_time': 0.13631319999694824}
I0205 19:51:56.291646 139770185602816 logging_writer.py:48] [3746] accumulated_eval_time=669.645352, accumulated_logging_time=0.136313, accumulated_submission_time=1212.330120, global_step=3746, preemption_count=0, score=1212.330120, test/accuracy=0.984575, test/loss=0.052942, test/mean_average_precision=0.166146, test/num_examples=43793, total_duration=1882.213757, train/accuracy=0.988397, train/loss=0.040214, train/mean_average_precision=0.183594, validation/accuracy=0.985468, validation/loss=0.050165, validation/mean_average_precision=0.166197, validation/num_examples=43793
I0205 19:52:14.048922 139770806105856 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.08549417555332184, loss=0.042595844715833664
I0205 19:52:46.234394 139770185602816 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.05798884481191635, loss=0.03780405968427658
I0205 19:53:18.214103 139770806105856 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0773799791932106, loss=0.03719524294137955
I0205 19:53:50.072785 139770185602816 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.09317471086978912, loss=0.04409947991371155
I0205 19:54:22.468240 139770806105856 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.09234051406383514, loss=0.04300260916352272
I0205 19:54:54.371606 139770185602816 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.0807127058506012, loss=0.03695915639400482
I0205 19:55:26.447881 139770806105856 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.0880763828754425, loss=0.03399808332324028
I0205 19:55:56.478858 139978932307776 spec.py:321] Evaluating on the training split.
I0205 19:57:44.275882 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 19:57:47.325642 139978932307776 spec.py:349] Evaluating on the test split.
I0205 19:57:50.433112 139978932307776 submission_runner.py:408] Time since start: 2236.37s, 	Step: 4495, 	{'train/accuracy': 0.9884520173072815, 'train/loss': 0.03975112736225128, 'train/mean_average_precision': 0.20363035451384381, 'validation/accuracy': 0.9856345057487488, 'validation/loss': 0.049419745802879333, 'validation/mean_average_precision': 0.18314421182517218, 'validation/num_examples': 43793, 'test/accuracy': 0.9847337603569031, 'test/loss': 0.05198296532034874, 'test/mean_average_precision': 0.18370312799845293, 'test/num_examples': 43793, 'score': 1452.4844810962677, 'total_duration': 2236.3710923194885, 'accumulated_submission_time': 1452.4844810962677, 'accumulated_eval_time': 783.5995593070984, 'accumulated_logging_time': 0.16430401802062988}
I0205 19:57:50.449414 139788813231872 logging_writer.py:48] [4495] accumulated_eval_time=783.599559, accumulated_logging_time=0.164304, accumulated_submission_time=1452.484481, global_step=4495, preemption_count=0, score=1452.484481, test/accuracy=0.984734, test/loss=0.051983, test/mean_average_precision=0.183703, test/num_examples=43793, total_duration=2236.371092, train/accuracy=0.988452, train/loss=0.039751, train/mean_average_precision=0.203630, validation/accuracy=0.985635, validation/loss=0.049420, validation/mean_average_precision=0.183144, validation/num_examples=43793
I0205 19:57:52.605329 139789425600256 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.04749077185988426, loss=0.03918071091175079
I0205 19:58:24.914050 139788813231872 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.11668578535318375, loss=0.04028311371803284
I0205 19:58:57.208949 139789425600256 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.08741746842861176, loss=0.03960416093468666
I0205 19:59:29.489819 139788813231872 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04748459905385971, loss=0.037833333015441895
I0205 20:00:01.795308 139789425600256 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.04686566814780235, loss=0.041523467749357224
I0205 20:00:33.902439 139788813231872 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.08243224024772644, loss=0.04211060702800751
I0205 20:01:06.373064 139789425600256 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.05945264548063278, loss=0.036951854825019836
I0205 20:01:38.741276 139788813231872 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.06714584678411484, loss=0.037238266319036484
I0205 20:01:50.643196 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:03:39.117748 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:03:42.127343 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:03:45.127043 139978932307776 submission_runner.py:408] Time since start: 2591.07s, 	Step: 5238, 	{'train/accuracy': 0.9888370633125305, 'train/loss': 0.038162074983119965, 'train/mean_average_precision': 0.22670998958535876, 'validation/accuracy': 0.9859211444854736, 'validation/loss': 0.04755629971623421, 'validation/mean_average_precision': 0.19909767789009397, 'validation/num_examples': 43793, 'test/accuracy': 0.9850395321846008, 'test/loss': 0.0501854233443737, 'test/mean_average_precision': 0.1966521375545678, 'test/num_examples': 43793, 'score': 1692.6457846164703, 'total_duration': 2591.0650255680084, 'accumulated_submission_time': 1692.6457846164703, 'accumulated_eval_time': 898.0833630561829, 'accumulated_logging_time': 0.19303679466247559}
I0205 20:03:45.143232 139770797713152 logging_writer.py:48] [5238] accumulated_eval_time=898.083363, accumulated_logging_time=0.193037, accumulated_submission_time=1692.645785, global_step=5238, preemption_count=0, score=1692.645785, test/accuracy=0.985040, test/loss=0.050185, test/mean_average_precision=0.196652, test/num_examples=43793, total_duration=2591.065026, train/accuracy=0.988837, train/loss=0.038162, train/mean_average_precision=0.226710, validation/accuracy=0.985921, validation/loss=0.047556, validation/mean_average_precision=0.199098, validation/num_examples=43793
I0205 20:04:05.476963 139770806105856 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.06933977454900742, loss=0.04141896218061447
I0205 20:04:37.863193 139770797713152 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05836321786046028, loss=0.033360641449689865
I0205 20:05:10.121472 139770806105856 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05160008743405342, loss=0.043777164071798325
I0205 20:05:42.460612 139770797713152 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.06314035505056381, loss=0.0363958515226841
I0205 20:06:14.763388 139770806105856 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.05323849990963936, loss=0.03783676028251648
I0205 20:06:47.309874 139770797713152 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.04112597554922104, loss=0.03599831089377403
I0205 20:07:19.190181 139770806105856 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.09778522700071335, loss=0.03523482382297516
I0205 20:07:45.272795 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:09:32.279834 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:09:37.272774 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:09:40.289980 139978932307776 submission_runner.py:408] Time since start: 2946.23s, 	Step: 5982, 	{'train/accuracy': 0.9888089299201965, 'train/loss': 0.03777968883514404, 'train/mean_average_precision': 0.23499009768661672, 'validation/accuracy': 0.9859194755554199, 'validation/loss': 0.047309760004282, 'validation/mean_average_precision': 0.20191103284607342, 'validation/num_examples': 43793, 'test/accuracy': 0.9850606322288513, 'test/loss': 0.049863848835229874, 'test/mean_average_precision': 0.2016183392127283, 'test/num_examples': 43793, 'score': 1932.7427368164062, 'total_duration': 2946.2279620170593, 'accumulated_submission_time': 1932.7427368164062, 'accumulated_eval_time': 1013.1005027294159, 'accumulated_logging_time': 0.22141695022583008}
I0205 20:09:40.306640 139806663034624 logging_writer.py:48] [5982] accumulated_eval_time=1013.100503, accumulated_logging_time=0.221417, accumulated_submission_time=1932.742737, global_step=5982, preemption_count=0, score=1932.742737, test/accuracy=0.985061, test/loss=0.049864, test/mean_average_precision=0.201618, test/num_examples=43793, total_duration=2946.227962, train/accuracy=0.988809, train/loss=0.037780, train/mean_average_precision=0.234990, validation/accuracy=0.985919, validation/loss=0.047310, validation/mean_average_precision=0.201911, validation/num_examples=43793
I0205 20:09:46.550063 139916344293120 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.04035510867834091, loss=0.038758303970098495
I0205 20:10:18.938835 139806663034624 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.04869772866368294, loss=0.039822258055210114
I0205 20:10:51.441907 139916344293120 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04905037581920624, loss=0.034688450396060944
I0205 20:11:23.876963 139806663034624 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.044863905757665634, loss=0.035893265157938004
I0205 20:11:56.603305 139916344293120 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.03495049476623535, loss=0.04267628863453865
I0205 20:12:29.249470 139806663034624 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.04362340271472931, loss=0.03916313126683235
I0205 20:13:01.519669 139916344293120 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.04995483160018921, loss=0.0358801893889904
I0205 20:13:33.681634 139806663034624 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.035456083714962006, loss=0.03699169680476189
I0205 20:13:40.554285 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:15:26.302431 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:15:29.321892 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:15:32.310927 139978932307776 submission_runner.py:408] Time since start: 3298.25s, 	Step: 6722, 	{'train/accuracy': 0.9892280697822571, 'train/loss': 0.03680482134222984, 'train/mean_average_precision': 0.27369930332534403, 'validation/accuracy': 0.9860668778419495, 'validation/loss': 0.046758756041526794, 'validation/mean_average_precision': 0.21270309188905026, 'validation/num_examples': 43793, 'test/accuracy': 0.9852674007415771, 'test/loss': 0.04913114756345749, 'test/mean_average_precision': 0.2183474994479768, 'test/num_examples': 43793, 'score': 2172.955377101898, 'total_duration': 3298.24889087677, 'accumulated_submission_time': 2172.955377101898, 'accumulated_eval_time': 1124.8570773601532, 'accumulated_logging_time': 0.2525761127471924}
I0205 20:15:32.328257 139770806105856 logging_writer.py:48] [6722] accumulated_eval_time=1124.857077, accumulated_logging_time=0.252576, accumulated_submission_time=2172.955377, global_step=6722, preemption_count=0, score=2172.955377, test/accuracy=0.985267, test/loss=0.049131, test/mean_average_precision=0.218347, test/num_examples=43793, total_duration=3298.248891, train/accuracy=0.989228, train/loss=0.036805, train/mean_average_precision=0.273699, validation/accuracy=0.986067, validation/loss=0.046759, validation/mean_average_precision=0.212703, validation/num_examples=43793
I0205 20:15:58.085633 139789425600256 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.037432603538036346, loss=0.03474689647555351
I0205 20:16:30.463699 139770806105856 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.06845246255397797, loss=0.03920472785830498
I0205 20:17:02.573734 139789425600256 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0326969213783741, loss=0.03773993253707886
I0205 20:17:34.632937 139770806105856 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.032403070479631424, loss=0.03356938436627388
I0205 20:18:06.936516 139789425600256 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.04759436473250389, loss=0.034677378833293915
I0205 20:18:38.749101 139770806105856 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.04078272357583046, loss=0.03940340504050255
I0205 20:19:10.737556 139789425600256 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.036172449588775635, loss=0.03745335713028908
I0205 20:19:32.491414 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:21:18.859861 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:21:23.810910 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:21:26.788846 139978932307776 submission_runner.py:408] Time since start: 3652.73s, 	Step: 7469, 	{'train/accuracy': 0.9891047477722168, 'train/loss': 0.036559801548719406, 'train/mean_average_precision': 0.26471269146596876, 'validation/accuracy': 0.9860149025917053, 'validation/loss': 0.047384802252054214, 'validation/mean_average_precision': 0.21523215164863765, 'validation/num_examples': 43793, 'test/accuracy': 0.9851044416427612, 'test/loss': 0.05031104013323784, 'test/mean_average_precision': 0.20823299610528184, 'test/num_examples': 43793, 'score': 2413.086009979248, 'total_duration': 3652.7268018722534, 'accumulated_submission_time': 2413.086009979248, 'accumulated_eval_time': 1239.154440164566, 'accumulated_logging_time': 0.28229618072509766}
I0205 20:21:26.806079 139806663034624 logging_writer.py:48] [7469] accumulated_eval_time=1239.154440, accumulated_logging_time=0.282296, accumulated_submission_time=2413.086010, global_step=7469, preemption_count=0, score=2413.086010, test/accuracy=0.985104, test/loss=0.050311, test/mean_average_precision=0.208233, test/num_examples=43793, total_duration=3652.726802, train/accuracy=0.989105, train/loss=0.036560, train/mean_average_precision=0.264713, validation/accuracy=0.986015, validation/loss=0.047385, validation/mean_average_precision=0.215232, validation/num_examples=43793
I0205 20:21:37.030285 139916344293120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03466261550784111, loss=0.03752100467681885
I0205 20:22:10.365449 139806663034624 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.041340749710798264, loss=0.03736315295100212
I0205 20:22:42.170527 139916344293120 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.032377779483795166, loss=0.036016251891851425
I0205 20:23:14.736033 139806663034624 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.043178681284189224, loss=0.03375917673110962
I0205 20:23:48.255831 139916344293120 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.02941063418984413, loss=0.03620120882987976
I0205 20:24:20.871581 139806663034624 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03238022327423096, loss=0.03657478839159012
I0205 20:24:52.450794 139916344293120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.04488701373338699, loss=0.04148499667644501
I0205 20:25:24.213301 139806663034624 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03831608220934868, loss=0.03378678113222122
I0205 20:25:26.802645 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:27:11.844847 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:27:14.851651 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:27:17.865394 139978932307776 submission_runner.py:408] Time since start: 4003.80s, 	Step: 8209, 	{'train/accuracy': 0.9893330335617065, 'train/loss': 0.035949498414993286, 'train/mean_average_precision': 0.2700580912185201, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.046453479677438736, 'validation/mean_average_precision': 0.22399626963309754, 'validation/num_examples': 43793, 'test/accuracy': 0.9853959083557129, 'test/loss': 0.049026843160390854, 'test/mean_average_precision': 0.21997002438227886, 'test/num_examples': 43793, 'score': 2653.0507991313934, 'total_duration': 4003.8033468723297, 'accumulated_submission_time': 2653.0507991313934, 'accumulated_eval_time': 1350.2171156406403, 'accumulated_logging_time': 0.3105733394622803}
I0205 20:27:17.882569 139770806105856 logging_writer.py:48] [8209] accumulated_eval_time=1350.217116, accumulated_logging_time=0.310573, accumulated_submission_time=2653.050799, global_step=8209, preemption_count=0, score=2653.050799, test/accuracy=0.985396, test/loss=0.049027, test/mean_average_precision=0.219970, test/num_examples=43793, total_duration=4003.803347, train/accuracy=0.989333, train/loss=0.035949, train/mean_average_precision=0.270058, validation/accuracy=0.986201, validation/loss=0.046453, validation/mean_average_precision=0.223996, validation/num_examples=43793
I0205 20:27:47.317029 139788813231872 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.029094871133565903, loss=0.03422669693827629
I0205 20:28:19.511943 139770806105856 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.037420034408569336, loss=0.03594649210572243
I0205 20:28:51.695318 139788813231872 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.04206175357103348, loss=0.03614367917180061
I0205 20:29:23.344063 139770806105856 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.03274979442358017, loss=0.03556005284190178
I0205 20:29:55.184805 139788813231872 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.02629580907523632, loss=0.03688922896981239
I0205 20:30:26.931621 139770806105856 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03467646241188049, loss=0.03772526606917381
I0205 20:30:59.446162 139788813231872 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.03058416210114956, loss=0.03648228943347931
I0205 20:31:18.157871 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:33:05.390421 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:33:10.221659 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:33:13.170921 139978932307776 submission_runner.py:408] Time since start: 4359.11s, 	Step: 8959, 	{'train/accuracy': 0.989347517490387, 'train/loss': 0.03583766147494316, 'train/mean_average_precision': 0.2764391488246033, 'validation/accuracy': 0.9863911867141724, 'validation/loss': 0.04540354013442993, 'validation/mean_average_precision': 0.22955552490979517, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.0479598231613636, 'test/mean_average_precision': 0.23206196939903595, 'test/num_examples': 43793, 'score': 2893.2927191257477, 'total_duration': 4359.108901500702, 'accumulated_submission_time': 2893.2927191257477, 'accumulated_eval_time': 1465.2301201820374, 'accumulated_logging_time': 0.34090471267700195}
I0205 20:33:13.188076 139789425600256 logging_writer.py:48] [8959] accumulated_eval_time=1465.230120, accumulated_logging_time=0.340905, accumulated_submission_time=2893.292719, global_step=8959, preemption_count=0, score=2893.292719, test/accuracy=0.985557, test/loss=0.047960, test/mean_average_precision=0.232062, test/num_examples=43793, total_duration=4359.108902, train/accuracy=0.989348, train/loss=0.035838, train/mean_average_precision=0.276439, validation/accuracy=0.986391, validation/loss=0.045404, validation/mean_average_precision=0.229556, validation/num_examples=43793
I0205 20:33:26.433223 139806663034624 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.047381188720464706, loss=0.03965415060520172
I0205 20:33:58.396539 139789425600256 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03482820466160774, loss=0.03871111944317818
I0205 20:34:29.997494 139806663034624 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03525714948773384, loss=0.033686406910419464
I0205 20:35:01.751969 139789425600256 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.034008488059043884, loss=0.038769759237766266
I0205 20:35:33.490448 139806663034624 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.041282977908849716, loss=0.03772853687405586
I0205 20:36:05.319358 139789425600256 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.030456658452749252, loss=0.03328562155365944
I0205 20:36:37.036700 139806663034624 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03751879185438156, loss=0.03792925179004669
I0205 20:37:08.810924 139789425600256 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.03326989337801933, loss=0.03886640444397926
I0205 20:37:13.279809 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:38:59.275710 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:39:02.387239 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:39:05.428190 139978932307776 submission_runner.py:408] Time since start: 4711.37s, 	Step: 9715, 	{'train/accuracy': 0.9895155429840088, 'train/loss': 0.035268500447273254, 'train/mean_average_precision': 0.2899146334264587, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04599665105342865, 'validation/mean_average_precision': 0.22869382715933165, 'validation/num_examples': 43793, 'test/accuracy': 0.9855496287345886, 'test/loss': 0.04855114594101906, 'test/mean_average_precision': 0.22784956893426864, 'test/num_examples': 43793, 'score': 3133.352974653244, 'total_duration': 4711.366160392761, 'accumulated_submission_time': 3133.352974653244, 'accumulated_eval_time': 1577.3784453868866, 'accumulated_logging_time': 0.36896514892578125}
I0205 20:39:05.446026 139770806105856 logging_writer.py:48] [9715] accumulated_eval_time=1577.378445, accumulated_logging_time=0.368965, accumulated_submission_time=3133.352975, global_step=9715, preemption_count=0, score=3133.352975, test/accuracy=0.985550, test/loss=0.048551, test/mean_average_precision=0.227850, test/num_examples=43793, total_duration=4711.366160, train/accuracy=0.989516, train/loss=0.035269, train/mean_average_precision=0.289915, validation/accuracy=0.986351, validation/loss=0.045997, validation/mean_average_precision=0.228694, validation/num_examples=43793
I0205 20:39:35.395334 139788813231872 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.02864529937505722, loss=0.03284500166773796
I0205 20:40:09.205785 139770806105856 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03812406212091446, loss=0.035214100033044815
I0205 20:40:41.838816 139788813231872 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.023816857486963272, loss=0.03419645503163338
I0205 20:41:13.708972 139770806105856 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.023330477997660637, loss=0.03170471638441086
I0205 20:41:45.505773 139788813231872 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03641130030155182, loss=0.03704143315553665
I0205 20:42:17.372475 139770806105856 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.027191288769245148, loss=0.03153557330369949
I0205 20:42:49.620596 139788813231872 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.028736144304275513, loss=0.03635158762335777
I0205 20:43:05.661741 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:44:53.034159 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:44:57.894301 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:45:00.893116 139978932307776 submission_runner.py:408] Time since start: 5066.83s, 	Step: 10451, 	{'train/accuracy': 0.9897177219390869, 'train/loss': 0.03448662906885147, 'train/mean_average_precision': 0.31379166194335967, 'validation/accuracy': 0.9864187836647034, 'validation/loss': 0.04568442702293396, 'validation/mean_average_precision': 0.2386852578627625, 'validation/num_examples': 43793, 'test/accuracy': 0.9855100512504578, 'test/loss': 0.048339344561100006, 'test/mean_average_precision': 0.23221452658596023, 'test/num_examples': 43793, 'score': 3373.5377974510193, 'total_duration': 5066.831092119217, 'accumulated_submission_time': 3373.5377974510193, 'accumulated_eval_time': 1692.609769821167, 'accumulated_logging_time': 0.397641658782959}
I0205 20:45:00.911477 139770797713152 logging_writer.py:48] [10451] accumulated_eval_time=1692.609770, accumulated_logging_time=0.397642, accumulated_submission_time=3373.537797, global_step=10451, preemption_count=0, score=3373.537797, test/accuracy=0.985510, test/loss=0.048339, test/mean_average_precision=0.232215, test/num_examples=43793, total_duration=5066.831092, train/accuracy=0.989718, train/loss=0.034487, train/mean_average_precision=0.313792, validation/accuracy=0.986419, validation/loss=0.045684, validation/mean_average_precision=0.238685, validation/num_examples=43793
I0205 20:45:16.891126 139789425600256 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.02926955558359623, loss=0.033510275185108185
I0205 20:45:49.107521 139770797713152 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.02436576597392559, loss=0.03197376802563667
I0205 20:46:21.144562 139789425600256 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.05075770616531372, loss=0.031704191118478775
I0205 20:46:53.129548 139770797713152 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03513798490166664, loss=0.034485284239053726
I0205 20:47:25.725466 139789425600256 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.0327240489423275, loss=0.03433876112103462
I0205 20:47:58.058616 139770797713152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.046463675796985626, loss=0.03493819758296013
I0205 20:48:30.353880 139789425600256 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.036751195788383484, loss=0.0348467156291008
I0205 20:49:00.940623 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:50:43.511210 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:50:46.551860 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:50:49.546360 139978932307776 submission_runner.py:408] Time since start: 5415.48s, 	Step: 11196, 	{'train/accuracy': 0.9898854494094849, 'train/loss': 0.03352813422679901, 'train/mean_average_precision': 0.32871810873388474, 'validation/accuracy': 0.9863871335983276, 'validation/loss': 0.04538214951753616, 'validation/mean_average_precision': 0.24170799077011787, 'validation/num_examples': 43793, 'test/accuracy': 0.9856237769126892, 'test/loss': 0.04807046800851822, 'test/mean_average_precision': 0.24482837398800203, 'test/num_examples': 43793, 'score': 3613.5333302021027, 'total_duration': 5415.48432302475, 'accumulated_submission_time': 3613.5333302021027, 'accumulated_eval_time': 1801.215446472168, 'accumulated_logging_time': 0.4285142421722412}
I0205 20:50:49.567178 139770806105856 logging_writer.py:48] [11196] accumulated_eval_time=1801.215446, accumulated_logging_time=0.428514, accumulated_submission_time=3613.533330, global_step=11196, preemption_count=0, score=3613.533330, test/accuracy=0.985624, test/loss=0.048070, test/mean_average_precision=0.244828, test/num_examples=43793, total_duration=5415.484323, train/accuracy=0.989885, train/loss=0.033528, train/mean_average_precision=0.328718, validation/accuracy=0.986387, validation/loss=0.045382, validation/mean_average_precision=0.241708, validation/num_examples=43793
I0205 20:50:51.269946 139788813231872 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03148393705487251, loss=0.03108128346502781
I0205 20:51:23.948580 139770806105856 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.041487183421850204, loss=0.03512776643037796
I0205 20:51:56.461022 139788813231872 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.03153587132692337, loss=0.0343022383749485
I0205 20:52:28.852526 139770806105856 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.04917226731777191, loss=0.03513457998633385
I0205 20:53:01.367895 139788813231872 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.029444711282849312, loss=0.03395255655050278
I0205 20:53:34.094707 139770806105856 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.04760875925421715, loss=0.03609354794025421
I0205 20:54:06.762911 139788813231872 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03405747562646866, loss=0.030656112357974052
I0205 20:54:39.047080 139770806105856 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04050043225288391, loss=0.03475002571940422
I0205 20:54:49.601159 139978932307776 spec.py:321] Evaluating on the training split.
I0205 20:56:34.984615 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 20:56:39.975564 139978932307776 spec.py:349] Evaluating on the test split.
I0205 20:56:42.957626 139978932307776 submission_runner.py:408] Time since start: 5768.90s, 	Step: 11934, 	{'train/accuracy': 0.9900333285331726, 'train/loss': 0.03320500999689102, 'train/mean_average_precision': 0.3330144534538547, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.04528070241212845, 'validation/mean_average_precision': 0.2396226332273137, 'validation/num_examples': 43793, 'test/accuracy': 0.9855976104736328, 'test/loss': 0.04800181835889816, 'test/mean_average_precision': 0.232307527215203, 'test/num_examples': 43793, 'score': 3853.5349090099335, 'total_duration': 5768.89560508728, 'accumulated_submission_time': 3853.5349090099335, 'accumulated_eval_time': 1914.5718655586243, 'accumulated_logging_time': 0.4614851474761963}
I0205 20:56:42.976495 139770797713152 logging_writer.py:48] [11934] accumulated_eval_time=1914.571866, accumulated_logging_time=0.461485, accumulated_submission_time=3853.534909, global_step=11934, preemption_count=0, score=3853.534909, test/accuracy=0.985598, test/loss=0.048002, test/mean_average_precision=0.232308, test/num_examples=43793, total_duration=5768.895605, train/accuracy=0.990033, train/loss=0.033205, train/mean_average_precision=0.333014, validation/accuracy=0.986502, validation/loss=0.045281, validation/mean_average_precision=0.239623, validation/num_examples=43793
I0205 20:57:04.663495 139806663034624 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.04244617000222206, loss=0.03508753329515457
I0205 20:57:36.701833 139770797713152 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.06641275435686111, loss=0.03236459940671921
I0205 20:58:08.961147 139806663034624 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03525061532855034, loss=0.03312276303768158
I0205 20:58:40.972885 139770797713152 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.039176251739263535, loss=0.03357372060418129
I0205 20:59:12.808404 139806663034624 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.027452174574136734, loss=0.03144184127449989
I0205 20:59:44.837107 139770797713152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.05389897897839546, loss=0.0338553711771965
I0205 21:00:16.604832 139806663034624 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.04165559634566307, loss=0.030879460275173187
I0205 21:00:43.230408 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:02:25.410160 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:02:28.544226 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:02:31.492278 139978932307776 submission_runner.py:408] Time since start: 6117.43s, 	Step: 12685, 	{'train/accuracy': 0.9902761578559875, 'train/loss': 0.03202471509575844, 'train/mean_average_precision': 0.37875461384207076, 'validation/accuracy': 0.9864386916160583, 'validation/loss': 0.04516429826617241, 'validation/mean_average_precision': 0.25107617938919036, 'validation/num_examples': 43793, 'test/accuracy': 0.9856582880020142, 'test/loss': 0.04794519394636154, 'test/mean_average_precision': 0.24156832028636196, 'test/num_examples': 43793, 'score': 4093.756762266159, 'total_duration': 6117.430259227753, 'accumulated_submission_time': 4093.756762266159, 'accumulated_eval_time': 2022.8337025642395, 'accumulated_logging_time': 0.4917325973510742}
I0205 21:02:31.510113 139770806105856 logging_writer.py:48] [12685] accumulated_eval_time=2022.833703, accumulated_logging_time=0.491733, accumulated_submission_time=4093.756762, global_step=12685, preemption_count=0, score=4093.756762, test/accuracy=0.985658, test/loss=0.047945, test/mean_average_precision=0.241568, test/num_examples=43793, total_duration=6117.430259, train/accuracy=0.990276, train/loss=0.032025, train/mean_average_precision=0.378755, validation/accuracy=0.986439, validation/loss=0.045164, validation/mean_average_precision=0.251076, validation/num_examples=43793
I0205 21:02:36.576923 139788813231872 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.041734591126441956, loss=0.036246538162231445
I0205 21:03:08.761288 139770806105856 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03531539812684059, loss=0.031120184808969498
I0205 21:03:41.519715 139788813231872 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.029267381876707077, loss=0.030816303566098213
I0205 21:04:13.414750 139770806105856 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04255742207169533, loss=0.03259962424635887
I0205 21:04:45.500079 139788813231872 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.08329403400421143, loss=0.02975531294941902
I0205 21:05:17.550511 139770806105856 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.049558863043785095, loss=0.030424902215600014
I0205 21:05:49.273310 139788813231872 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.04424338415265083, loss=0.034436725080013275
I0205 21:06:21.451835 139770806105856 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04281166195869446, loss=0.032312776893377304
I0205 21:06:31.634723 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:08:15.563990 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:08:18.620135 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:08:23.517406 139978932307776 submission_runner.py:408] Time since start: 6469.46s, 	Step: 13433, 	{'train/accuracy': 0.9904384613037109, 'train/loss': 0.03149892017245293, 'train/mean_average_precision': 0.3717057181802991, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.0446271151304245, 'validation/mean_average_precision': 0.24721268033636373, 'validation/num_examples': 43793, 'test/accuracy': 0.9858389496803284, 'test/loss': 0.04734957590699196, 'test/mean_average_precision': 0.24626384470574914, 'test/num_examples': 43793, 'score': 4333.850111484528, 'total_duration': 6469.4553780555725, 'accumulated_submission_time': 4333.850111484528, 'accumulated_eval_time': 2134.716328382492, 'accumulated_logging_time': 0.5204606056213379}
I0205 21:08:23.535642 139770797713152 logging_writer.py:48] [13433] accumulated_eval_time=2134.716328, accumulated_logging_time=0.520461, accumulated_submission_time=4333.850111, global_step=13433, preemption_count=0, score=4333.850111, test/accuracy=0.985839, test/loss=0.047350, test/mean_average_precision=0.246264, test/num_examples=43793, total_duration=6469.455378, train/accuracy=0.990438, train/loss=0.031499, train/mean_average_precision=0.371706, validation/accuracy=0.986667, validation/loss=0.044627, validation/mean_average_precision=0.247213, validation/num_examples=43793
I0205 21:08:45.351369 139789425600256 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.05946671962738037, loss=0.03407277539372444
I0205 21:09:17.211151 139770797713152 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.0551193505525589, loss=0.037191811949014664
I0205 21:09:49.146386 139789425600256 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.03941451385617256, loss=0.028772074729204178
I0205 21:10:21.260051 139770797713152 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.08962144702672958, loss=0.030920593068003654
I0205 21:10:54.002263 139789425600256 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07577498257160187, loss=0.03812842071056366
I0205 21:11:26.541863 139770797713152 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.08083256334066391, loss=0.03213580697774887
I0205 21:11:58.746742 139789425600256 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04874172806739807, loss=0.03237778693437576
I0205 21:12:23.615802 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:14:08.601587 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:14:11.614505 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:14:14.646944 139978932307776 submission_runner.py:408] Time since start: 6820.58s, 	Step: 14178, 	{'train/accuracy': 0.9904738664627075, 'train/loss': 0.03142843395471573, 'train/mean_average_precision': 0.37346244724380895, 'validation/accuracy': 0.9864821434020996, 'validation/loss': 0.04513578861951828, 'validation/mean_average_precision': 0.2507288477370744, 'validation/num_examples': 43793, 'test/accuracy': 0.9856254458427429, 'test/loss': 0.04799942299723625, 'test/mean_average_precision': 0.25043207856634864, 'test/num_examples': 43793, 'score': 4573.89826631546, 'total_duration': 6820.58492398262, 'accumulated_submission_time': 4573.89826631546, 'accumulated_eval_time': 2245.7474250793457, 'accumulated_logging_time': 0.549689769744873}
I0205 21:14:14.665085 139770806105856 logging_writer.py:48] [14178] accumulated_eval_time=2245.747425, accumulated_logging_time=0.549690, accumulated_submission_time=4573.898266, global_step=14178, preemption_count=0, score=4573.898266, test/accuracy=0.985625, test/loss=0.047999, test/mean_average_precision=0.250432, test/num_examples=43793, total_duration=6820.584924, train/accuracy=0.990474, train/loss=0.031428, train/mean_average_precision=0.373462, validation/accuracy=0.986482, validation/loss=0.045136, validation/mean_average_precision=0.250729, validation/num_examples=43793
I0205 21:14:22.205495 139806663034624 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.049719762057065964, loss=0.03775465115904808
I0205 21:14:54.728176 139770806105856 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.06250892579555511, loss=0.031991515308618546
I0205 21:15:26.818855 139806663034624 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0707932561635971, loss=0.0375150591135025
I0205 21:15:59.199375 139770806105856 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04681258276104927, loss=0.030918562784790993
I0205 21:16:31.482853 139806663034624 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.07120081782341003, loss=0.033405836671590805
I0205 21:17:03.926035 139770806105856 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.056403521448373795, loss=0.033583130687475204
I0205 21:17:35.854976 139806663034624 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04209507256746292, loss=0.027714280411601067
I0205 21:18:08.195950 139770806105856 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.08744815737009048, loss=0.034312471747398376
I0205 21:18:14.726988 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:19:58.555601 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:20:01.663672 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:20:04.625561 139978932307776 submission_runner.py:408] Time since start: 7170.56s, 	Step: 14921, 	{'train/accuracy': 0.9905796051025391, 'train/loss': 0.031162654981017113, 'train/mean_average_precision': 0.3785872396635164, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.0440506637096405, 'validation/mean_average_precision': 0.2621364522054176, 'validation/num_examples': 43793, 'test/accuracy': 0.9858343601226807, 'test/loss': 0.0468188151717186, 'test/mean_average_precision': 0.2576561377273499, 'test/num_examples': 43793, 'score': 4813.926500082016, 'total_duration': 7170.563362598419, 'accumulated_submission_time': 4813.926500082016, 'accumulated_eval_time': 2355.6457736492157, 'accumulated_logging_time': 0.5813260078430176}
I0205 21:20:04.642895 139770797713152 logging_writer.py:48] [14921] accumulated_eval_time=2355.645774, accumulated_logging_time=0.581326, accumulated_submission_time=4813.926500, global_step=14921, preemption_count=0, score=4813.926500, test/accuracy=0.985834, test/loss=0.046819, test/mean_average_precision=0.257656, test/num_examples=43793, total_duration=7170.563363, train/accuracy=0.990580, train/loss=0.031163, train/mean_average_precision=0.378587, validation/accuracy=0.986697, validation/loss=0.044051, validation/mean_average_precision=0.262136, validation/num_examples=43793
I0205 21:20:29.876394 139789425600256 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.08076745271682739, loss=0.033230971544981
I0205 21:21:02.115695 139770797713152 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.058833129703998566, loss=0.029403025284409523
I0205 21:21:33.810830 139789425600256 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05386222153902054, loss=0.031711649149656296
I0205 21:22:05.693415 139770797713152 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06125776842236519, loss=0.036541763693094254
I0205 21:22:37.610524 139789425600256 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.046213142573833466, loss=0.030312420800328255
I0205 21:23:09.530830 139770797713152 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.07883849740028381, loss=0.03277448192238808
I0205 21:23:41.357386 139789425600256 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04510214924812317, loss=0.03294447064399719
I0205 21:24:04.640197 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:25:46.662125 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:25:49.637512 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:25:52.651819 139978932307776 submission_runner.py:408] Time since start: 7518.59s, 	Step: 15673, 	{'train/accuracy': 0.9905015826225281, 'train/loss': 0.031514354050159454, 'train/mean_average_precision': 0.37039730201168186, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.044145770370960236, 'validation/mean_average_precision': 0.26115744996768614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.04671604931354523, 'test/mean_average_precision': 0.25491086610496483, 'test/num_examples': 43793, 'score': 5053.890505075455, 'total_duration': 7518.589793205261, 'accumulated_submission_time': 5053.890505075455, 'accumulated_eval_time': 2463.6573424339294, 'accumulated_logging_time': 0.611168384552002}
I0205 21:25:52.670068 139770806105856 logging_writer.py:48] [15673] accumulated_eval_time=2463.657342, accumulated_logging_time=0.611168, accumulated_submission_time=5053.890505, global_step=15673, preemption_count=0, score=5053.890505, test/accuracy=0.985955, test/loss=0.046716, test/mean_average_precision=0.254911, test/num_examples=43793, total_duration=7518.589793, train/accuracy=0.990502, train/loss=0.031514, train/mean_average_precision=0.370397, validation/accuracy=0.986763, validation/loss=0.044146, validation/mean_average_precision=0.261157, validation/num_examples=43793
I0205 21:26:01.628273 139806663034624 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.07302901148796082, loss=0.035201869904994965
I0205 21:26:33.421233 139770806105856 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.065181665122509, loss=0.031107807531952858
I0205 21:27:05.240243 139806663034624 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.06749477982521057, loss=0.029841206967830658
I0205 21:27:37.215468 139770806105856 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0514686182141304, loss=0.027344895526766777
I0205 21:28:09.083085 139806663034624 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06687942147254944, loss=0.03518938273191452
I0205 21:28:40.960994 139770806105856 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.06682731956243515, loss=0.03001987561583519
I0205 21:29:13.051259 139806663034624 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.08868293464183807, loss=0.02869667299091816
I0205 21:29:44.651302 139770806105856 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04810726270079613, loss=0.03374679386615753
I0205 21:29:52.674613 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:31:35.286319 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:31:40.229033 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:31:43.238732 139978932307776 submission_runner.py:408] Time since start: 7869.18s, 	Step: 16426, 	{'train/accuracy': 0.9906003475189209, 'train/loss': 0.031076526269316673, 'train/mean_average_precision': 0.39021054358474716, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.04423775523900986, 'validation/mean_average_precision': 0.2586473980508823, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.046707622706890106, 'test/mean_average_precision': 0.2628098371482614, 'test/num_examples': 43793, 'score': 5293.863883733749, 'total_duration': 7869.176714420319, 'accumulated_submission_time': 5293.863883733749, 'accumulated_eval_time': 2574.2214167118073, 'accumulated_logging_time': 0.6401748657226562}
I0205 21:31:43.257459 139770797713152 logging_writer.py:48] [16426] accumulated_eval_time=2574.221417, accumulated_logging_time=0.640175, accumulated_submission_time=5293.863884, global_step=16426, preemption_count=0, score=5293.863884, test/accuracy=0.985849, test/loss=0.046708, test/mean_average_precision=0.262810, test/num_examples=43793, total_duration=7869.176714, train/accuracy=0.990600, train/loss=0.031077, train/mean_average_precision=0.390211, validation/accuracy=0.986645, validation/loss=0.044238, validation/mean_average_precision=0.258647, validation/num_examples=43793
I0205 21:32:07.572106 139789425600256 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.05788680911064148, loss=0.0267197173088789
I0205 21:32:39.787621 139770797713152 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.0919778048992157, loss=0.02997257187962532
I0205 21:33:11.991226 139789425600256 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.08042412251234055, loss=0.03314867615699768
I0205 21:33:43.547084 139770797713152 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.05454642325639725, loss=0.030671413987874985
I0205 21:34:15.370381 139789425600256 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.13444477319717407, loss=0.03292655199766159
I0205 21:34:47.351837 139770797713152 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.06380090117454529, loss=0.03418678790330887
I0205 21:35:18.868169 139789425600256 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.06587235629558563, loss=0.030406543985009193
I0205 21:35:43.335477 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:37:30.300825 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:37:33.333490 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:37:36.284307 139978932307776 submission_runner.py:408] Time since start: 8222.22s, 	Step: 17178, 	{'train/accuracy': 0.9905943274497986, 'train/loss': 0.030888676643371582, 'train/mean_average_precision': 0.3930183406771667, 'validation/accuracy': 0.9867305755615234, 'validation/loss': 0.04413682594895363, 'validation/mean_average_precision': 0.2651857368659311, 'validation/num_examples': 43793, 'test/accuracy': 0.9858617186546326, 'test/loss': 0.0468544140458107, 'test/mean_average_precision': 0.25534591552830505, 'test/num_examples': 43793, 'score': 5533.909998893738, 'total_duration': 8222.222283363342, 'accumulated_submission_time': 5533.909998893738, 'accumulated_eval_time': 2687.1701986789703, 'accumulated_logging_time': 0.6697847843170166}
I0205 21:37:36.302922 139788813231872 logging_writer.py:48] [17178] accumulated_eval_time=2687.170199, accumulated_logging_time=0.669785, accumulated_submission_time=5533.909999, global_step=17178, preemption_count=0, score=5533.909999, test/accuracy=0.985862, test/loss=0.046854, test/mean_average_precision=0.255346, test/num_examples=43793, total_duration=8222.222283, train/accuracy=0.990594, train/loss=0.030889, train/mean_average_precision=0.393018, validation/accuracy=0.986731, validation/loss=0.044137, validation/mean_average_precision=0.265186, validation/num_examples=43793
I0205 21:37:43.670367 139806663034624 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.0634247213602066, loss=0.02985510230064392
I0205 21:38:15.582340 139788813231872 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.07128575444221497, loss=0.0316336490213871
I0205 21:38:47.732679 139806663034624 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0629129558801651, loss=0.0300338976085186
I0205 21:39:19.514136 139788813231872 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.09235131740570068, loss=0.030060915276408195
I0205 21:39:51.376572 139806663034624 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.07564277946949005, loss=0.03057883121073246
I0205 21:40:23.190711 139788813231872 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0751480907201767, loss=0.026339231058955193
I0205 21:40:54.957035 139806663034624 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.06425344198942184, loss=0.029418589547276497
I0205 21:41:27.080895 139788813231872 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07792626321315765, loss=0.031061336398124695
I0205 21:41:36.338400 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:43:21.753335 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:43:24.739612 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:43:27.670874 139978932307776 submission_runner.py:408] Time since start: 8573.61s, 	Step: 17930, 	{'train/accuracy': 0.9906186461448669, 'train/loss': 0.030826617032289505, 'train/mean_average_precision': 0.3925631480101053, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.0444091372191906, 'validation/mean_average_precision': 0.2627921188302755, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04698709771037102, 'test/mean_average_precision': 0.2571229912207292, 'test/num_examples': 43793, 'score': 5773.913890838623, 'total_duration': 8573.608848333359, 'accumulated_submission_time': 5773.913890838623, 'accumulated_eval_time': 2798.5026183128357, 'accumulated_logging_time': 0.699364423751831}
I0205 21:43:27.689754 139770806105856 logging_writer.py:48] [17930] accumulated_eval_time=2798.502618, accumulated_logging_time=0.699364, accumulated_submission_time=5773.913891, global_step=17930, preemption_count=0, score=5773.913891, test/accuracy=0.985882, test/loss=0.046987, test/mean_average_precision=0.257123, test/num_examples=43793, total_duration=8573.608848, train/accuracy=0.990619, train/loss=0.030827, train/mean_average_precision=0.392563, validation/accuracy=0.986718, validation/loss=0.044409, validation/mean_average_precision=0.262792, validation/num_examples=43793
I0205 21:43:51.811379 139789425600256 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.10292178392410278, loss=0.03219543397426605
I0205 21:44:23.838008 139770806105856 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.13868941366672516, loss=0.030335940420627594
I0205 21:44:55.511901 139789425600256 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.08096806704998016, loss=0.03448382019996643
I0205 21:45:27.131648 139770806105856 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.08142799139022827, loss=0.032387956976890564
I0205 21:45:58.721160 139789425600256 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.060675568878650665, loss=0.028802093118429184
I0205 21:46:30.339789 139770806105856 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.07673975825309753, loss=0.028483573347330093
I0205 21:47:02.070296 139789425600256 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.08085685223340988, loss=0.03230864182114601
I0205 21:47:27.806050 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:49:07.159593 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:49:10.141009 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:49:13.087248 139978932307776 submission_runner.py:408] Time since start: 8919.03s, 	Step: 18681, 	{'train/accuracy': 0.9908760190010071, 'train/loss': 0.03000824525952339, 'train/mean_average_precision': 0.4102506283303613, 'validation/accuracy': 0.986647367477417, 'validation/loss': 0.044502776116132736, 'validation/mean_average_precision': 0.26556058880960226, 'validation/num_examples': 43793, 'test/accuracy': 0.9858953952789307, 'test/loss': 0.0472092404961586, 'test/mean_average_precision': 0.25721369249162757, 'test/num_examples': 43793, 'score': 6013.996770858765, 'total_duration': 8919.025218486786, 'accumulated_submission_time': 6013.996770858765, 'accumulated_eval_time': 2903.7837584018707, 'accumulated_logging_time': 0.73091721534729}
I0205 21:49:13.106640 139770797713152 logging_writer.py:48] [18681] accumulated_eval_time=2903.783758, accumulated_logging_time=0.730917, accumulated_submission_time=6013.996771, global_step=18681, preemption_count=0, score=6013.996771, test/accuracy=0.985895, test/loss=0.047209, test/mean_average_precision=0.257214, test/num_examples=43793, total_duration=8919.025218, train/accuracy=0.990876, train/loss=0.030008, train/mean_average_precision=0.410251, validation/accuracy=0.986647, validation/loss=0.044503, validation/mean_average_precision=0.265561, validation/num_examples=43793
I0205 21:49:19.490321 139806663034624 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.06772209703922272, loss=0.02710467204451561
I0205 21:49:51.288353 139770797713152 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.10542450100183487, loss=0.03098289854824543
I0205 21:50:23.060704 139806663034624 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.09822011739015579, loss=0.031681906431913376
I0205 21:50:55.058115 139770797713152 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.06487980484962463, loss=0.029496829956769943
I0205 21:51:27.109041 139806663034624 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.07252491265535355, loss=0.03237644582986832
I0205 21:51:59.248737 139770797713152 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.09605951607227325, loss=0.033575866371393204
I0205 21:52:31.449404 139806663034624 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.10691170394420624, loss=0.032206304371356964
I0205 21:53:03.415023 139770797713152 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07989867776632309, loss=0.0322396457195282
I0205 21:53:13.184783 139978932307776 spec.py:321] Evaluating on the training split.
I0205 21:55:00.690593 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 21:55:05.719416 139978932307776 spec.py:349] Evaluating on the test split.
I0205 21:55:08.695276 139978932307776 submission_runner.py:408] Time since start: 9274.63s, 	Step: 19432, 	{'train/accuracy': 0.9908602237701416, 'train/loss': 0.029808955267071724, 'train/mean_average_precision': 0.42696041075451996, 'validation/accuracy': 0.9866859316825867, 'validation/loss': 0.04467317834496498, 'validation/mean_average_precision': 0.2662054523909674, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.047456517815589905, 'test/mean_average_precision': 0.2520375820148166, 'test/num_examples': 43793, 'score': 6254.042366743088, 'total_duration': 9274.63326048851, 'accumulated_submission_time': 6254.042366743088, 'accumulated_eval_time': 3019.294206380844, 'accumulated_logging_time': 0.7621853351593018}
I0205 21:55:08.715296 139788813231872 logging_writer.py:48] [19432] accumulated_eval_time=3019.294206, accumulated_logging_time=0.762185, accumulated_submission_time=6254.042367, global_step=19432, preemption_count=0, score=6254.042367, test/accuracy=0.985889, test/loss=0.047457, test/mean_average_precision=0.252038, test/num_examples=43793, total_duration=9274.633260, train/accuracy=0.990860, train/loss=0.029809, train/mean_average_precision=0.426960, validation/accuracy=0.986686, validation/loss=0.044673, validation/mean_average_precision=0.266205, validation/num_examples=43793
I0205 21:55:30.880988 139789425600256 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.13374750316143036, loss=0.028343597427010536
I0205 21:56:03.219548 139788813231872 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.07658896595239639, loss=0.03085981495678425
I0205 21:56:35.308332 139789425600256 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.09731674194335938, loss=0.027471592649817467
I0205 21:57:07.512247 139788813231872 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.08680582046508789, loss=0.028967848047614098
I0205 21:57:40.137990 139789425600256 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.06969163566827774, loss=0.029947955161333084
I0205 21:58:12.456359 139788813231872 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0993436872959137, loss=0.0282992385327816
I0205 21:58:44.853034 139789425600256 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.07068725675344467, loss=0.027671614661812782
I0205 21:59:08.983629 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:00:52.388648 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:00:55.528570 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:00:58.507766 139978932307776 submission_runner.py:408] Time since start: 9624.45s, 	Step: 20175, 	{'train/accuracy': 0.9910188317298889, 'train/loss': 0.028977325186133385, 'train/mean_average_precision': 0.4231962341096236, 'validation/accuracy': 0.9867512583732605, 'validation/loss': 0.044515322893857956, 'validation/mean_average_precision': 0.26375224926940755, 'validation/num_examples': 43793, 'test/accuracy': 0.9858208894729614, 'test/loss': 0.0472513772547245, 'test/mean_average_precision': 0.2518622757509492, 'test/num_examples': 43793, 'score': 6494.2784950733185, 'total_duration': 9624.445745706558, 'accumulated_submission_time': 6494.2784950733185, 'accumulated_eval_time': 3128.8182978630066, 'accumulated_logging_time': 0.7937026023864746}
I0205 22:00:58.527019 139770797713152 logging_writer.py:48] [20175] accumulated_eval_time=3128.818298, accumulated_logging_time=0.793703, accumulated_submission_time=6494.278495, global_step=20175, preemption_count=0, score=6494.278495, test/accuracy=0.985821, test/loss=0.047251, test/mean_average_precision=0.251862, test/num_examples=43793, total_duration=9624.445746, train/accuracy=0.991019, train/loss=0.028977, train/mean_average_precision=0.423196, validation/accuracy=0.986751, validation/loss=0.044515, validation/mean_average_precision=0.263752, validation/num_examples=43793
I0205 22:01:06.926477 139770806105856 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.09542258083820343, loss=0.025700949132442474
I0205 22:01:39.244150 139770797713152 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07460451126098633, loss=0.030452245846390724
I0205 22:02:11.166335 139770806105856 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.07033244520425797, loss=0.02964465692639351
I0205 22:02:43.680928 139770797713152 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.09301542490720749, loss=0.032545603811740875
I0205 22:03:16.021490 139770806105856 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.08725360035896301, loss=0.02330588921904564
I0205 22:03:47.967810 139770797713152 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.08611451834440231, loss=0.031020043417811394
I0205 22:04:20.286584 139770806105856 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.07869405299425125, loss=0.02773616649210453
I0205 22:04:52.385448 139770797713152 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.10086178779602051, loss=0.03026561811566353
I0205 22:04:58.511977 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:06:42.186245 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:06:45.371562 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:06:50.387156 139978932307776 submission_runner.py:408] Time since start: 9976.33s, 	Step: 20920, 	{'train/accuracy': 0.9911974668502808, 'train/loss': 0.02875051088631153, 'train/mean_average_precision': 0.43925762858152084, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.04468398541212082, 'validation/mean_average_precision': 0.25641154136754146, 'validation/num_examples': 43793, 'test/accuracy': 0.9857374429702759, 'test/loss': 0.047172099351882935, 'test/mean_average_precision': 0.2560707943968566, 'test/num_examples': 43793, 'score': 6734.232186079025, 'total_duration': 9976.325129032135, 'accumulated_submission_time': 6734.232186079025, 'accumulated_eval_time': 3240.6934225559235, 'accumulated_logging_time': 0.8236665725708008}
I0205 22:06:50.406776 139788813231872 logging_writer.py:48] [20920] accumulated_eval_time=3240.693423, accumulated_logging_time=0.823667, accumulated_submission_time=6734.232186, global_step=20920, preemption_count=0, score=6734.232186, test/accuracy=0.985737, test/loss=0.047172, test/mean_average_precision=0.256071, test/num_examples=43793, total_duration=9976.325129, train/accuracy=0.991197, train/loss=0.028751, train/mean_average_precision=0.439258, validation/accuracy=0.986549, validation/loss=0.044684, validation/mean_average_precision=0.256412, validation/num_examples=43793
I0205 22:07:16.735409 139806663034624 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.09074176847934723, loss=0.03153163567185402
I0205 22:07:48.777430 139788813231872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.07389311492443085, loss=0.03070085309445858
I0205 22:08:21.039071 139806663034624 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.11329582333564758, loss=0.02968386374413967
I0205 22:08:52.888105 139788813231872 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.08718299865722656, loss=0.028189973905682564
I0205 22:09:24.956631 139806663034624 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.06781980395317078, loss=0.029111403971910477
I0205 22:09:56.996965 139788813231872 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.16510409116744995, loss=0.03311663866043091
I0205 22:10:29.128381 139806663034624 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.09057626128196716, loss=0.029143527150154114
I0205 22:10:50.564701 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:12:32.140789 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:12:35.187006 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:12:38.161667 139978932307776 submission_runner.py:408] Time since start: 10324.10s, 	Step: 21667, 	{'train/accuracy': 0.9909592270851135, 'train/loss': 0.029395489022135735, 'train/mean_average_precision': 0.4405538692550107, 'validation/accuracy': 0.9868373274803162, 'validation/loss': 0.04413104057312012, 'validation/mean_average_precision': 0.2741253771401372, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.046785593032836914, 'test/mean_average_precision': 0.25911216191366065, 'test/num_examples': 43793, 'score': 6974.358366250992, 'total_duration': 10324.099648237228, 'accumulated_submission_time': 6974.358366250992, 'accumulated_eval_time': 3348.290340423584, 'accumulated_logging_time': 0.854386568069458}
I0205 22:12:38.181274 139770797713152 logging_writer.py:48] [21667] accumulated_eval_time=3348.290340, accumulated_logging_time=0.854387, accumulated_submission_time=6974.358366, global_step=21667, preemption_count=0, score=6974.358366, test/accuracy=0.985966, test/loss=0.046786, test/mean_average_precision=0.259112, test/num_examples=43793, total_duration=10324.099648, train/accuracy=0.990959, train/loss=0.029395, train/mean_average_precision=0.440554, validation/accuracy=0.986837, validation/loss=0.044131, validation/mean_average_precision=0.274125, validation/num_examples=43793
I0205 22:12:49.009762 139770806105856 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08649963140487671, loss=0.02472325973212719
I0205 22:13:21.155946 139770797713152 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.09559772908687592, loss=0.030403973534703255
I0205 22:13:53.160137 139770806105856 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.0686698853969574, loss=0.033178627490997314
I0205 22:14:25.204091 139770797713152 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.11236953735351562, loss=0.03140687942504883
I0205 22:14:57.120410 139770806105856 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.09225621074438095, loss=0.032277774065732956
I0205 22:15:29.240320 139770797713152 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.0899997428059578, loss=0.03083646669983864
I0205 22:16:01.753881 139770806105856 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.07861830294132233, loss=0.028113575652241707
I0205 22:16:34.123985 139770797713152 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.09116973727941513, loss=0.030742548406124115
I0205 22:16:38.257959 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:18:19.387777 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:18:22.392705 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:18:25.457744 139978932307776 submission_runner.py:408] Time since start: 10671.40s, 	Step: 22414, 	{'train/accuracy': 0.9908499121665955, 'train/loss': 0.029780704528093338, 'train/mean_average_precision': 0.4160635084609168, 'validation/accuracy': 0.9868572354316711, 'validation/loss': 0.04488062486052513, 'validation/mean_average_precision': 0.2624296437397281, 'validation/num_examples': 43793, 'test/accuracy': 0.9860116839408875, 'test/loss': 0.04767533391714096, 'test/mean_average_precision': 0.25534959644604904, 'test/num_examples': 43793, 'score': 7214.402206897736, 'total_duration': 10671.395725727081, 'accumulated_submission_time': 7214.402206897736, 'accumulated_eval_time': 3455.4900765419006, 'accumulated_logging_time': 0.8864257335662842}
I0205 22:18:25.477600 139788813231872 logging_writer.py:48] [22414] accumulated_eval_time=3455.490077, accumulated_logging_time=0.886426, accumulated_submission_time=7214.402207, global_step=22414, preemption_count=0, score=7214.402207, test/accuracy=0.986012, test/loss=0.047675, test/mean_average_precision=0.255350, test/num_examples=43793, total_duration=10671.395726, train/accuracy=0.990850, train/loss=0.029781, train/mean_average_precision=0.416064, validation/accuracy=0.986857, validation/loss=0.044881, validation/mean_average_precision=0.262430, validation/num_examples=43793
I0205 22:18:53.354851 139789425600256 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.1679181009531021, loss=0.03164002299308777
I0205 22:19:25.277860 139788813231872 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1453510820865631, loss=0.03161240741610527
I0205 22:19:57.178050 139789425600256 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.08830501139163971, loss=0.02703782171010971
I0205 22:20:29.018063 139788813231872 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.12720109522342682, loss=0.031094228848814964
I0205 22:21:00.932435 139789425600256 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.07327359914779663, loss=0.027316473424434662
I0205 22:21:32.594839 139788813231872 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.11092787981033325, loss=0.028127770870923996
I0205 22:22:04.644540 139789425600256 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08331298828125, loss=0.029003575444221497
I0205 22:22:25.615184 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:24:13.662693 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:24:16.655332 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:24:19.619292 139978932307776 submission_runner.py:408] Time since start: 11025.56s, 	Step: 23167, 	{'train/accuracy': 0.9909346103668213, 'train/loss': 0.029603630304336548, 'train/mean_average_precision': 0.41999051149073074, 'validation/accuracy': 0.9866583347320557, 'validation/loss': 0.04457014426589012, 'validation/mean_average_precision': 0.2730043897985921, 'validation/num_examples': 43793, 'test/accuracy': 0.9858132600784302, 'test/loss': 0.04735338315367699, 'test/mean_average_precision': 0.2534655795057701, 'test/num_examples': 43793, 'score': 7454.508299827576, 'total_duration': 11025.55727314949, 'accumulated_submission_time': 7454.508299827576, 'accumulated_eval_time': 3569.4941403865814, 'accumulated_logging_time': 0.9170982837677002}
I0205 22:24:19.638906 139770806105856 logging_writer.py:48] [23167] accumulated_eval_time=3569.494140, accumulated_logging_time=0.917098, accumulated_submission_time=7454.508300, global_step=23167, preemption_count=0, score=7454.508300, test/accuracy=0.985813, test/loss=0.047353, test/mean_average_precision=0.253466, test/num_examples=43793, total_duration=11025.557273, train/accuracy=0.990935, train/loss=0.029604, train/mean_average_precision=0.419991, validation/accuracy=0.986658, validation/loss=0.044570, validation/mean_average_precision=0.273004, validation/num_examples=43793
I0205 22:24:30.493306 139806663034624 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.08304078876972198, loss=0.02980927936732769
I0205 22:25:02.201731 139770806105856 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.08461350202560425, loss=0.031190937384963036
I0205 22:25:33.936839 139806663034624 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.07146986573934555, loss=0.025852635502815247
I0205 22:26:05.880755 139770806105856 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.09342420101165771, loss=0.03218270465731621
I0205 22:26:37.841775 139806663034624 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.10295961797237396, loss=0.025912165641784668
I0205 22:27:09.762747 139770806105856 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.11236881464719772, loss=0.03072809986770153
I0205 22:27:41.410162 139806663034624 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.09727676212787628, loss=0.03141080588102341
I0205 22:28:13.243606 139770806105856 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.10052309930324554, loss=0.030133085325360298
I0205 22:28:19.646663 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:30:02.260894 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:30:05.297937 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:30:08.252113 139978932307776 submission_runner.py:408] Time since start: 11374.19s, 	Step: 23921, 	{'train/accuracy': 0.9909778833389282, 'train/loss': 0.029523272067308426, 'train/mean_average_precision': 0.4255365867503188, 'validation/accuracy': 0.9867942929267883, 'validation/loss': 0.04464536905288696, 'validation/mean_average_precision': 0.2707242531647493, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04751303046941757, 'test/mean_average_precision': 0.2569773068587559, 'test/num_examples': 43793, 'score': 7694.484705686569, 'total_duration': 11374.190093517303, 'accumulated_submission_time': 7694.484705686569, 'accumulated_eval_time': 3678.099539041519, 'accumulated_logging_time': 0.9474050998687744}
I0205 22:30:08.272598 139770797713152 logging_writer.py:48] [23921] accumulated_eval_time=3678.099539, accumulated_logging_time=0.947405, accumulated_submission_time=7694.484706, global_step=23921, preemption_count=0, score=7694.484706, test/accuracy=0.985951, test/loss=0.047513, test/mean_average_precision=0.256977, test/num_examples=43793, total_duration=11374.190094, train/accuracy=0.990978, train/loss=0.029523, train/mean_average_precision=0.425537, validation/accuracy=0.986794, validation/loss=0.044645, validation/mean_average_precision=0.270724, validation/num_examples=43793
I0205 22:30:33.807751 139789425600256 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.09950608760118484, loss=0.03079916350543499
I0205 22:31:05.911682 139770797713152 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.09293518960475922, loss=0.028157878667116165
I0205 22:31:38.062004 139789425600256 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.09744177758693695, loss=0.02980155684053898
I0205 22:32:09.897462 139770797713152 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09694426506757736, loss=0.02625155821442604
I0205 22:32:41.820977 139789425600256 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.07620736956596375, loss=0.02534341812133789
I0205 22:33:13.446608 139770797713152 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.12423153966665268, loss=0.030585629865527153
I0205 22:33:46.736531 139789425600256 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.15407782793045044, loss=0.029239561408758163
I0205 22:34:08.394777 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:35:48.931673 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:35:51.916705 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:35:54.838355 139978932307776 submission_runner.py:408] Time since start: 11720.78s, 	Step: 24666, 	{'train/accuracy': 0.9910315871238708, 'train/loss': 0.029066704213619232, 'train/mean_average_precision': 0.43930651407631194, 'validation/accuracy': 0.9868904948234558, 'validation/loss': 0.044614922255277634, 'validation/mean_average_precision': 0.2751389474181981, 'validation/num_examples': 43793, 'test/accuracy': 0.9860731363296509, 'test/loss': 0.04757541045546532, 'test/mean_average_precision': 0.2620029472242, 'test/num_examples': 43793, 'score': 7934.573546886444, 'total_duration': 11720.776325702667, 'accumulated_submission_time': 7934.573546886444, 'accumulated_eval_time': 3784.543060064316, 'accumulated_logging_time': 0.9804179668426514}
I0205 22:35:54.858421 139788813231872 logging_writer.py:48] [24666] accumulated_eval_time=3784.543060, accumulated_logging_time=0.980418, accumulated_submission_time=7934.573547, global_step=24666, preemption_count=0, score=7934.573547, test/accuracy=0.986073, test/loss=0.047575, test/mean_average_precision=0.262003, test/num_examples=43793, total_duration=11720.776326, train/accuracy=0.991032, train/loss=0.029067, train/mean_average_precision=0.439307, validation/accuracy=0.986890, validation/loss=0.044615, validation/mean_average_precision=0.275139, validation/num_examples=43793
I0205 22:36:06.116234 139806663034624 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07620035856962204, loss=0.03307965397834778
I0205 22:36:38.191309 139788813231872 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.08441842347383499, loss=0.023320773616433144
I0205 22:37:10.094177 139806663034624 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.10017102211713791, loss=0.029317134991288185
I0205 22:37:42.115923 139788813231872 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.13832470774650574, loss=0.028126414865255356
I0205 22:38:14.555799 139806663034624 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.17101223766803741, loss=0.026795819401741028
I0205 22:38:46.695022 139788813231872 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.0815935730934143, loss=0.029465297237038612
I0205 22:39:19.033755 139806663034624 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.08426978439092636, loss=0.027718938887119293
I0205 22:39:51.210440 139788813231872 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.09319444745779037, loss=0.02759474702179432
I0205 22:39:55.090080 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:41:40.719371 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:41:45.653245 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:41:48.619079 139978932307776 submission_runner.py:408] Time since start: 12074.56s, 	Step: 25413, 	{'train/accuracy': 0.9910230040550232, 'train/loss': 0.02899915911257267, 'train/mean_average_precision': 0.42797345102108686, 'validation/accuracy': 0.986810564994812, 'validation/loss': 0.0445956252515316, 'validation/mean_average_precision': 0.2719177220954323, 'validation/num_examples': 43793, 'test/accuracy': 0.9860171675682068, 'test/loss': 0.04729219898581505, 'test/mean_average_precision': 0.2669060818825145, 'test/num_examples': 43793, 'score': 8174.771550655365, 'total_duration': 12074.557059288025, 'accumulated_submission_time': 8174.771550655365, 'accumulated_eval_time': 3898.0720086097717, 'accumulated_logging_time': 1.0132358074188232}
I0205 22:41:48.639783 139770797713152 logging_writer.py:48] [25413] accumulated_eval_time=3898.072009, accumulated_logging_time=1.013236, accumulated_submission_time=8174.771551, global_step=25413, preemption_count=0, score=8174.771551, test/accuracy=0.986017, test/loss=0.047292, test/mean_average_precision=0.266906, test/num_examples=43793, total_duration=12074.557059, train/accuracy=0.991023, train/loss=0.028999, train/mean_average_precision=0.427973, validation/accuracy=0.986811, validation/loss=0.044596, validation/mean_average_precision=0.271918, validation/num_examples=43793
I0205 22:42:16.909277 139770806105856 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0951220914721489, loss=0.025350529700517654
I0205 22:42:48.829785 139770797713152 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.07474204897880554, loss=0.028905052691698074
I0205 22:43:20.939050 139770806105856 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.10895071178674698, loss=0.031758762896060944
I0205 22:43:52.652318 139770797713152 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.10562929511070251, loss=0.03026217594742775
I0205 22:44:24.483236 139770806105856 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.1131652221083641, loss=0.03269131854176521
I0205 22:44:56.521344 139770797713152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.09344401955604553, loss=0.027354251593351364
I0205 22:45:28.288583 139770806105856 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.09271824359893799, loss=0.030837727710604668
I0205 22:45:48.709288 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:47:32.218732 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:47:35.148638 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:47:38.082960 139978932307776 submission_runner.py:408] Time since start: 12424.02s, 	Step: 26165, 	{'train/accuracy': 0.9912290573120117, 'train/loss': 0.028361449018120766, 'train/mean_average_precision': 0.45259342880654135, 'validation/accuracy': 0.9866153001785278, 'validation/loss': 0.044667817652225494, 'validation/mean_average_precision': 0.2719697859873213, 'validation/num_examples': 43793, 'test/accuracy': 0.9857838153839111, 'test/loss': 0.04710261896252632, 'test/mean_average_precision': 0.25841901698511205, 'test/num_examples': 43793, 'score': 8414.809683322906, 'total_duration': 12424.020931243896, 'accumulated_submission_time': 8414.809683322906, 'accumulated_eval_time': 4007.4456238746643, 'accumulated_logging_time': 1.0445480346679688}
I0205 22:47:38.103724 139789425600256 logging_writer.py:48] [26165] accumulated_eval_time=4007.445624, accumulated_logging_time=1.044548, accumulated_submission_time=8414.809683, global_step=26165, preemption_count=0, score=8414.809683, test/accuracy=0.985784, test/loss=0.047103, test/mean_average_precision=0.258419, test/num_examples=43793, total_duration=12424.020931, train/accuracy=0.991229, train/loss=0.028361, train/mean_average_precision=0.452593, validation/accuracy=0.986615, validation/loss=0.044668, validation/mean_average_precision=0.271970, validation/num_examples=43793
I0205 22:47:49.684424 139806663034624 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.127656489610672, loss=0.032827798277139664
I0205 22:48:21.483096 139789425600256 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.11585445702075958, loss=0.027292676270008087
I0205 22:48:53.361583 139806663034624 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.11101242154836655, loss=0.028334463015198708
I0205 22:49:25.189927 139789425600256 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0935046374797821, loss=0.028102366253733635
I0205 22:49:56.761035 139806663034624 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.08862031996250153, loss=0.029449284076690674
I0205 22:50:28.578382 139789425600256 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.1349642425775528, loss=0.03081257827579975
I0205 22:51:00.273239 139806663034624 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.09802260249853134, loss=0.030497979372739792
I0205 22:51:32.215119 139789425600256 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.11322423070669174, loss=0.033753421157598495
I0205 22:51:38.204496 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:53:19.519203 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:53:22.516857 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:53:27.363819 139978932307776 submission_runner.py:408] Time since start: 12773.30s, 	Step: 26920, 	{'train/accuracy': 0.991524875164032, 'train/loss': 0.027365470305085182, 'train/mean_average_precision': 0.4807769372003801, 'validation/accuracy': 0.9869144558906555, 'validation/loss': 0.044424235820770264, 'validation/mean_average_precision': 0.2759751459481721, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.04718492180109024, 'test/mean_average_precision': 0.2646887047154832, 'test/num_examples': 43793, 'score': 8654.877965211868, 'total_duration': 12773.301801204681, 'accumulated_submission_time': 8654.877965211868, 'accumulated_eval_time': 4116.604898691177, 'accumulated_logging_time': 1.0775237083435059}
I0205 22:53:27.384583 139770797713152 logging_writer.py:48] [26920] accumulated_eval_time=4116.604899, accumulated_logging_time=1.077524, accumulated_submission_time=8654.877965, global_step=26920, preemption_count=0, score=8654.877965, test/accuracy=0.985988, test/loss=0.047185, test/mean_average_precision=0.264689, test/num_examples=43793, total_duration=12773.301801, train/accuracy=0.991525, train/loss=0.027365, train/mean_average_precision=0.480777, validation/accuracy=0.986914, validation/loss=0.044424, validation/mean_average_precision=0.275975, validation/num_examples=43793
I0205 22:53:53.255681 139788813231872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.09959238022565842, loss=0.02925550937652588
I0205 22:54:25.398717 139770797713152 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.09709440916776657, loss=0.029741507023572922
I0205 22:54:57.135831 139788813231872 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.07650338858366013, loss=0.027533432468771935
I0205 22:55:29.194685 139770797713152 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.08569273352622986, loss=0.028576968237757683
I0205 22:56:00.995487 139788813231872 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.10026153177022934, loss=0.03072834573686123
I0205 22:56:32.922387 139770797713152 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10005578398704529, loss=0.030179403722286224
I0205 22:57:05.150371 139788813231872 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.08561185002326965, loss=0.0281433816999197
I0205 22:57:27.624258 139978932307776 spec.py:321] Evaluating on the training split.
I0205 22:59:08.518756 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 22:59:11.516528 139978932307776 spec.py:349] Evaluating on the test split.
I0205 22:59:14.440972 139978932307776 submission_runner.py:408] Time since start: 13120.38s, 	Step: 27672, 	{'train/accuracy': 0.9917396306991577, 'train/loss': 0.026780622079968452, 'train/mean_average_precision': 0.48750520532975306, 'validation/accuracy': 0.9867861866950989, 'validation/loss': 0.0446733795106411, 'validation/mean_average_precision': 0.2727244827693121, 'validation/num_examples': 43793, 'test/accuracy': 0.9859206676483154, 'test/loss': 0.04740463197231293, 'test/mean_average_precision': 0.2627906059002099, 'test/num_examples': 43793, 'score': 8895.08589887619, 'total_duration': 13120.378950834274, 'accumulated_submission_time': 8895.08589887619, 'accumulated_eval_time': 4223.421566724777, 'accumulated_logging_time': 1.1091325283050537}
I0205 22:59:14.461464 139770806105856 logging_writer.py:48] [27672] accumulated_eval_time=4223.421567, accumulated_logging_time=1.109133, accumulated_submission_time=8895.085899, global_step=27672, preemption_count=0, score=8895.085899, test/accuracy=0.985921, test/loss=0.047405, test/mean_average_precision=0.262791, test/num_examples=43793, total_duration=13120.378951, train/accuracy=0.991740, train/loss=0.026781, train/mean_average_precision=0.487505, validation/accuracy=0.986786, validation/loss=0.044673, validation/mean_average_precision=0.272724, validation/num_examples=43793
I0205 22:59:23.912199 139789425600256 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.10604074597358704, loss=0.028746193274855614
I0205 22:59:56.528173 139770806105856 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.1080331802368164, loss=0.03076937608420849
I0205 23:00:28.594122 139789425600256 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.10820472240447998, loss=0.030390214174985886
I0205 23:01:00.396896 139770806105856 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.08821924030780792, loss=0.03036443330347538
I0205 23:01:32.341323 139789425600256 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.10293743759393692, loss=0.0317855104804039
I0205 23:02:04.299170 139770806105856 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.08649276942014694, loss=0.030467670410871506
I0205 23:02:36.753425 139789425600256 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.08477514982223511, loss=0.02781766653060913
I0205 23:03:08.935197 139770806105856 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.11423159390687943, loss=0.027882540598511696
I0205 23:03:14.606930 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:04:52.812158 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:04:55.847632 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:04:58.783016 139978932307776 submission_runner.py:408] Time since start: 13464.72s, 	Step: 28419, 	{'train/accuracy': 0.9916260838508606, 'train/loss': 0.026939881965517998, 'train/mean_average_precision': 0.48579127106100845, 'validation/accuracy': 0.9868592619895935, 'validation/loss': 0.0448070652782917, 'validation/mean_average_precision': 0.27169239613355595, 'validation/num_examples': 43793, 'test/accuracy': 0.9859859943389893, 'test/loss': 0.04767269268631935, 'test/mean_average_precision': 0.2642261563246968, 'test/num_examples': 43793, 'score': 9135.200062274933, 'total_duration': 13464.72099852562, 'accumulated_submission_time': 9135.200062274933, 'accumulated_eval_time': 4327.597607374191, 'accumulated_logging_time': 1.1407244205474854}
I0205 23:04:58.804374 139788813231872 logging_writer.py:48] [28419] accumulated_eval_time=4327.597607, accumulated_logging_time=1.140724, accumulated_submission_time=9135.200062, global_step=28419, preemption_count=0, score=9135.200062, test/accuracy=0.985986, test/loss=0.047673, test/mean_average_precision=0.264226, test/num_examples=43793, total_duration=13464.720999, train/accuracy=0.991626, train/loss=0.026940, train/mean_average_precision=0.485791, validation/accuracy=0.986859, validation/loss=0.044807, validation/mean_average_precision=0.271692, validation/num_examples=43793
I0205 23:05:25.051644 139806663034624 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.08118294179439545, loss=0.02764289081096649
I0205 23:05:56.839450 139788813231872 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.11048800498247147, loss=0.0282468069344759
I0205 23:06:28.904264 139806663034624 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.0974835753440857, loss=0.028573662042617798
I0205 23:07:00.467184 139788813231872 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.09223067760467529, loss=0.02780785784125328
I0205 23:07:32.487791 139806663034624 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.10518268495798111, loss=0.0258085485547781
I0205 23:08:04.370031 139788813231872 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.07489929348230362, loss=0.024962855502963066
I0205 23:08:36.094899 139806663034624 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.12457995861768723, loss=0.027861639857292175
I0205 23:08:59.032270 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:10:36.510479 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:10:39.448730 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:10:42.347378 139978932307776 submission_runner.py:408] Time since start: 13808.29s, 	Step: 29173, 	{'train/accuracy': 0.991509735584259, 'train/loss': 0.027415966615080833, 'train/mean_average_precision': 0.47126790399796437, 'validation/accuracy': 0.9867748022079468, 'validation/loss': 0.04474445432424545, 'validation/mean_average_precision': 0.27036917002655547, 'validation/num_examples': 43793, 'test/accuracy': 0.985869288444519, 'test/loss': 0.047515373677015305, 'test/mean_average_precision': 0.25662169516709116, 'test/num_examples': 43793, 'score': 9375.395035743713, 'total_duration': 13808.285351991653, 'accumulated_submission_time': 9375.395035743713, 'accumulated_eval_time': 4430.912664890289, 'accumulated_logging_time': 1.174621820449829}
I0205 23:10:42.368073 139770806105856 logging_writer.py:48] [29173] accumulated_eval_time=4430.912665, accumulated_logging_time=1.174622, accumulated_submission_time=9375.395036, global_step=29173, preemption_count=0, score=9375.395036, test/accuracy=0.985869, test/loss=0.047515, test/mean_average_precision=0.256622, test/num_examples=43793, total_duration=13808.285352, train/accuracy=0.991510, train/loss=0.027416, train/mean_average_precision=0.471268, validation/accuracy=0.986775, validation/loss=0.044744, validation/mean_average_precision=0.270369, validation/num_examples=43793
I0205 23:10:51.406458 139789425600256 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.09610643237829208, loss=0.02778572030365467
I0205 23:11:23.466319 139770806105856 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.09947094321250916, loss=0.02952936477959156
I0205 23:11:55.697411 139789425600256 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.08630688488483429, loss=0.025955170392990112
I0205 23:12:27.869853 139770806105856 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1133633479475975, loss=0.026361066848039627
I0205 23:12:59.681828 139789425600256 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.10812339186668396, loss=0.026615634560585022
I0205 23:13:31.731801 139770806105856 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.10382257401943207, loss=0.027323704212903976
I0205 23:14:03.319737 139789425600256 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.09435422718524933, loss=0.025936072692275047
I0205 23:14:35.160791 139770806105856 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.10640759766101837, loss=0.030473003163933754
I0205 23:14:42.559979 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:16:22.412454 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:16:25.354410 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:16:28.278782 139978932307776 submission_runner.py:408] Time since start: 14154.22s, 	Step: 29924, 	{'train/accuracy': 0.9912999868392944, 'train/loss': 0.02828758768737316, 'train/mean_average_precision': 0.4548449958025577, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.044981010258197784, 'validation/mean_average_precision': 0.2710515862463466, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04792707785964012, 'test/mean_average_precision': 0.260950514393316, 'test/num_examples': 43793, 'score': 9615.55433011055, 'total_duration': 14154.216762781143, 'accumulated_submission_time': 9615.55433011055, 'accumulated_eval_time': 4536.631418704987, 'accumulated_logging_time': 1.2075426578521729}
I0205 23:16:28.299557 139770797713152 logging_writer.py:48] [29924] accumulated_eval_time=4536.631419, accumulated_logging_time=1.207543, accumulated_submission_time=9615.554330, global_step=29924, preemption_count=0, score=9615.554330, test/accuracy=0.985859, test/loss=0.047927, test/mean_average_precision=0.260951, test/num_examples=43793, total_duration=14154.216763, train/accuracy=0.991300, train/loss=0.028288, train/mean_average_precision=0.454845, validation/accuracy=0.986757, validation/loss=0.044981, validation/mean_average_precision=0.271052, validation/num_examples=43793
I0205 23:16:52.721845 139788813231872 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.1320868581533432, loss=0.03083772584795952
I0205 23:17:24.783982 139770797713152 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.08565215766429901, loss=0.028637731447815895
I0205 23:17:56.878597 139788813231872 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.09642156958580017, loss=0.02763102948665619
I0205 23:18:28.782211 139770797713152 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.11320578306913376, loss=0.027283713221549988
I0205 23:19:00.411518 139788813231872 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.16169004142284393, loss=0.0274213720113039
I0205 23:19:32.323505 139770797713152 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.09638907760381699, loss=0.031113941222429276
I0205 23:20:04.236469 139788813231872 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.09016422927379608, loss=0.025517649948596954
I0205 23:20:28.374138 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:22:09.107270 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:22:12.077895 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:22:14.948642 139978932307776 submission_runner.py:408] Time since start: 14500.89s, 	Step: 30677, 	{'train/accuracy': 0.9913409948348999, 'train/loss': 0.028073610737919807, 'train/mean_average_precision': 0.4493626684822014, 'validation/accuracy': 0.9867821335792542, 'validation/loss': 0.04475332424044609, 'validation/mean_average_precision': 0.27232175876993087, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.047855161130428314, 'test/mean_average_precision': 0.2620535742729729, 'test/num_examples': 43793, 'score': 9855.597105264664, 'total_duration': 14500.886621952057, 'accumulated_submission_time': 9855.597105264664, 'accumulated_eval_time': 4643.205877542496, 'accumulated_logging_time': 1.2392997741699219}
I0205 23:22:14.970128 139770806105856 logging_writer.py:48] [30677] accumulated_eval_time=4643.205878, accumulated_logging_time=1.239300, accumulated_submission_time=9855.597105, global_step=30677, preemption_count=0, score=9855.597105, test/accuracy=0.985881, test/loss=0.047855, test/mean_average_precision=0.262054, test/num_examples=43793, total_duration=14500.886622, train/accuracy=0.991341, train/loss=0.028074, train/mean_average_precision=0.449363, validation/accuracy=0.986782, validation/loss=0.044753, validation/mean_average_precision=0.272322, validation/num_examples=43793
I0205 23:22:22.840480 139806663034624 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.09406373649835587, loss=0.024791816249489784
I0205 23:22:55.253637 139770806105856 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.12404365092515945, loss=0.02840515971183777
I0205 23:23:26.761215 139806663034624 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.12262208014726639, loss=0.029765017330646515
I0205 23:23:58.477858 139770806105856 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.10530894994735718, loss=0.02681289240717888
I0205 23:24:30.050209 139806663034624 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.09678743034601212, loss=0.026996180415153503
I0205 23:25:01.623486 139770806105856 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.13726598024368286, loss=0.030316201969981194
I0205 23:25:33.051319 139806663034624 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0997641310095787, loss=0.026729010045528412
I0205 23:26:04.553133 139770806105856 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.11467312276363373, loss=0.025832217186689377
I0205 23:26:15.035443 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:27:53.855211 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:27:56.955720 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:28:02.009340 139978932307776 submission_runner.py:408] Time since start: 14847.95s, 	Step: 31434, 	{'train/accuracy': 0.9913233518600464, 'train/loss': 0.028088927268981934, 'train/mean_average_precision': 0.45806846877071095, 'validation/accuracy': 0.9867622256278992, 'validation/loss': 0.044960204511880875, 'validation/mean_average_precision': 0.2680808067314692, 'validation/num_examples': 43793, 'test/accuracy': 0.985932469367981, 'test/loss': 0.04747050628066063, 'test/mean_average_precision': 0.26141791565521816, 'test/num_examples': 43793, 'score': 10095.629689216614, 'total_duration': 14847.947264671326, 'accumulated_submission_time': 10095.629689216614, 'accumulated_eval_time': 4750.179671287537, 'accumulated_logging_time': 1.2727985382080078}
I0205 23:28:02.048994 139770797713152 logging_writer.py:48] [31434] accumulated_eval_time=4750.179671, accumulated_logging_time=1.272799, accumulated_submission_time=10095.629689, global_step=31434, preemption_count=0, score=10095.629689, test/accuracy=0.985932, test/loss=0.047471, test/mean_average_precision=0.261418, test/num_examples=43793, total_duration=14847.947265, train/accuracy=0.991323, train/loss=0.028089, train/mean_average_precision=0.458068, validation/accuracy=0.986762, validation/loss=0.044960, validation/mean_average_precision=0.268081, validation/num_examples=43793
I0205 23:28:23.554714 139789425600256 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.07800951600074768, loss=0.023199032992124557
I0205 23:28:55.164893 139770797713152 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.08757803589105606, loss=0.029329495504498482
I0205 23:29:27.174712 139789425600256 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.08719047158956528, loss=0.02715468592941761
I0205 23:29:58.694362 139770797713152 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.1301688700914383, loss=0.029728662222623825
I0205 23:30:30.845659 139789425600256 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.12614215910434723, loss=0.030074575915932655
I0205 23:31:02.757058 139770797713152 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.10409077256917953, loss=0.02859814278781414
I0205 23:31:34.404224 139789425600256 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.10974260419607162, loss=0.024469172582030296
I0205 23:32:02.013880 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:33:45.395833 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:33:48.545596 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:33:51.575426 139978932307776 submission_runner.py:408] Time since start: 15197.51s, 	Step: 32188, 	{'train/accuracy': 0.9914279580116272, 'train/loss': 0.02746138721704483, 'train/mean_average_precision': 0.4688236204403521, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.0450935922563076, 'validation/mean_average_precision': 0.2805715467170202, 'validation/num_examples': 43793, 'test/accuracy': 0.9858309626579285, 'test/loss': 0.04816046357154846, 'test/mean_average_precision': 0.261548027310026, 'test/num_examples': 43793, 'score': 10335.560795545578, 'total_duration': 15197.513407230377, 'accumulated_submission_time': 10335.560795545578, 'accumulated_eval_time': 4859.741172552109, 'accumulated_logging_time': 1.325857400894165}
I0205 23:33:51.597172 139770806105856 logging_writer.py:48] [32188] accumulated_eval_time=4859.741173, accumulated_logging_time=1.325857, accumulated_submission_time=10335.560796, global_step=32188, preemption_count=0, score=10335.560796, test/accuracy=0.985831, test/loss=0.048160, test/mean_average_precision=0.261548, test/num_examples=43793, total_duration=15197.513407, train/accuracy=0.991428, train/loss=0.027461, train/mean_average_precision=0.468824, validation/accuracy=0.986765, validation/loss=0.045094, validation/mean_average_precision=0.280572, validation/num_examples=43793
I0205 23:33:55.804099 139806663034624 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.13059791922569275, loss=0.031309083104133606
I0205 23:34:28.158615 139770806105856 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.09017203003168106, loss=0.02622286044061184
I0205 23:35:00.012320 139806663034624 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.11018702387809753, loss=0.02850436605513096
I0205 23:35:31.923206 139770806105856 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0905875638127327, loss=0.02539815939962864
I0205 23:36:04.245876 139806663034624 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.10338059812784195, loss=0.02790919877588749
I0205 23:36:36.320663 139770806105856 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.10338889062404633, loss=0.02824045717716217
I0205 23:37:08.426169 139806663034624 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10809900611639023, loss=0.02933056280016899
I0205 23:37:40.625013 139770806105856 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.12682050466537476, loss=0.03125655651092529
I0205 23:37:51.837001 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:39:33.043563 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:39:36.136532 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:39:39.182214 139978932307776 submission_runner.py:408] Time since start: 15545.12s, 	Step: 32936, 	{'train/accuracy': 0.9914365410804749, 'train/loss': 0.02748289331793785, 'train/mean_average_precision': 0.4736294815094482, 'validation/accuracy': 0.9866388440132141, 'validation/loss': 0.04488770291209221, 'validation/mean_average_precision': 0.2706954798470259, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.047641586512327194, 'test/mean_average_precision': 0.2580936874518775, 'test/num_examples': 43793, 'score': 10575.767451763153, 'total_duration': 15545.120171785355, 'accumulated_submission_time': 10575.767451763153, 'accumulated_eval_time': 4967.086313724518, 'accumulated_logging_time': 1.359889268875122}
I0205 23:39:39.204115 139770797713152 logging_writer.py:48] [32936] accumulated_eval_time=4967.086314, accumulated_logging_time=1.359889, accumulated_submission_time=10575.767452, global_step=32936, preemption_count=0, score=10575.767452, test/accuracy=0.985933, test/loss=0.047642, test/mean_average_precision=0.258094, test/num_examples=43793, total_duration=15545.120172, train/accuracy=0.991437, train/loss=0.027483, train/mean_average_precision=0.473629, validation/accuracy=0.986639, validation/loss=0.044888, validation/mean_average_precision=0.270695, validation/num_examples=43793
I0205 23:40:00.004739 139789425600256 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.09003497660160065, loss=0.025937268510460854
I0205 23:40:32.155934 139770797713152 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.10631412267684937, loss=0.029375704005360603
I0205 23:41:04.229742 139789425600256 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.09637109190225601, loss=0.025187920778989792
I0205 23:41:36.248674 139770797713152 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.10260995477437973, loss=0.028446879237890244
I0205 23:42:08.196364 139789425600256 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.15059445798397064, loss=0.03074495866894722
I0205 23:42:40.525691 139770797713152 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.09644661843776703, loss=0.02896435745060444
I0205 23:43:12.437951 139789425600256 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.12034761160612106, loss=0.02657410129904747
I0205 23:43:39.274727 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:45:24.653895 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:45:27.662490 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:45:30.582020 139978932307776 submission_runner.py:408] Time since start: 15896.52s, 	Step: 33684, 	{'train/accuracy': 0.9916418790817261, 'train/loss': 0.02673172391951084, 'train/mean_average_precision': 0.48136942091615786, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.044977832585573196, 'validation/mean_average_precision': 0.2782887354386499, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.04794187471270561, 'test/mean_average_precision': 0.2566208483574557, 'test/num_examples': 43793, 'score': 10815.805988311768, 'total_duration': 15896.519999742508, 'accumulated_submission_time': 10815.805988311768, 'accumulated_eval_time': 5078.3935606479645, 'accumulated_logging_time': 1.3929271697998047}
I0205 23:45:30.603756 139788813231872 logging_writer.py:48] [33684] accumulated_eval_time=5078.393561, accumulated_logging_time=1.392927, accumulated_submission_time=10815.805988, global_step=33684, preemption_count=0, score=10815.805988, test/accuracy=0.985954, test/loss=0.047942, test/mean_average_precision=0.256621, test/num_examples=43793, total_duration=15896.520000, train/accuracy=0.991642, train/loss=0.026732, train/mean_average_precision=0.481369, validation/accuracy=0.986828, validation/loss=0.044978, validation/mean_average_precision=0.278289, validation/num_examples=43793
I0205 23:45:36.003985 139806663034624 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.08832040429115295, loss=0.0284216720610857
I0205 23:46:07.796266 139788813231872 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.10329300910234451, loss=0.023418767377734184
I0205 23:46:40.442653 139806663034624 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.12223850190639496, loss=0.02820238657295704
I0205 23:47:11.919264 139788813231872 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.09928198158740997, loss=0.028460994362831116
I0205 23:47:43.700309 139806663034624 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.1026572436094284, loss=0.026166493073105812
I0205 23:48:15.656448 139788813231872 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.12807242572307587, loss=0.02393634431064129
I0205 23:48:47.471065 139806663034624 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.13036513328552246, loss=0.02754644863307476
I0205 23:49:19.545411 139788813231872 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.11608485132455826, loss=0.030173752456903458
I0205 23:49:30.675749 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:51:09.869244 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:51:12.941069 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:51:15.863279 139978932307776 submission_runner.py:408] Time since start: 16241.80s, 	Step: 34436, 	{'train/accuracy': 0.9919087290763855, 'train/loss': 0.02593764290213585, 'train/mean_average_precision': 0.505223532274534, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.04495269060134888, 'validation/mean_average_precision': 0.28174461946087653, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.047770678997039795, 'test/mean_average_precision': 0.267297587830477, 'test/num_examples': 43793, 'score': 11055.845739364624, 'total_duration': 16241.80126285553, 'accumulated_submission_time': 11055.845739364624, 'accumulated_eval_time': 5183.58104300499, 'accumulated_logging_time': 1.426476001739502}
I0205 23:51:15.885432 139770797713152 logging_writer.py:48] [34436] accumulated_eval_time=5183.581043, accumulated_logging_time=1.426476, accumulated_submission_time=11055.845739, global_step=34436, preemption_count=0, score=11055.845739, test/accuracy=0.985964, test/loss=0.047771, test/mean_average_precision=0.267298, test/num_examples=43793, total_duration=16241.801263, train/accuracy=0.991909, train/loss=0.025938, train/mean_average_precision=0.505224, validation/accuracy=0.986828, validation/loss=0.044953, validation/mean_average_precision=0.281745, validation/num_examples=43793
I0205 23:51:36.632194 139770806105856 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.11488397419452667, loss=0.027379965409636497
I0205 23:52:08.569978 139770797713152 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.10203687101602554, loss=0.028920957818627357
I0205 23:52:40.685017 139770806105856 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.11709223687648773, loss=0.027855616062879562
I0205 23:53:12.538995 139770797713152 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.1104549840092659, loss=0.028820335865020752
I0205 23:53:44.320509 139770806105856 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.10447799414396286, loss=0.027425238862633705
I0205 23:54:16.279385 139770797713152 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.10875463485717773, loss=0.027277881279587746
I0205 23:54:47.960943 139770806105856 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.09554021060466766, loss=0.02771402895450592
I0205 23:55:16.107091 139978932307776 spec.py:321] Evaluating on the training split.
I0205 23:57:02.701037 139978932307776 spec.py:333] Evaluating on the validation split.
I0205 23:57:05.695514 139978932307776 spec.py:349] Evaluating on the test split.
I0205 23:57:08.636922 139978932307776 submission_runner.py:408] Time since start: 16594.57s, 	Step: 35189, 	{'train/accuracy': 0.9923018217086792, 'train/loss': 0.02476343885064125, 'train/mean_average_precision': 0.5400334925387037, 'validation/accuracy': 0.9866660237312317, 'validation/loss': 0.04505612328648567, 'validation/mean_average_precision': 0.2804774655869647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858478307723999, 'test/loss': 0.047578711062669754, 'test/mean_average_precision': 0.268802852899132, 'test/num_examples': 43793, 'score': 11296.034619808197, 'total_duration': 16594.574902057648, 'accumulated_submission_time': 11296.034619808197, 'accumulated_eval_time': 5296.110832691193, 'accumulated_logging_time': 1.461381196975708}
I0205 23:57:08.659187 139788813231872 logging_writer.py:48] [35189] accumulated_eval_time=5296.110833, accumulated_logging_time=1.461381, accumulated_submission_time=11296.034620, global_step=35189, preemption_count=0, score=11296.034620, test/accuracy=0.985848, test/loss=0.047579, test/mean_average_precision=0.268803, test/num_examples=43793, total_duration=16594.574902, train/accuracy=0.992302, train/loss=0.024763, train/mean_average_precision=0.540033, validation/accuracy=0.986666, validation/loss=0.045056, validation/mean_average_precision=0.280477, validation/num_examples=43793
I0205 23:57:12.526382 139806663034624 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.09738057106733322, loss=0.024766026064753532
I0205 23:57:44.810178 139788813231872 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.10741805285215378, loss=0.026071244850754738
I0205 23:58:16.699750 139806663034624 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.08850342780351639, loss=0.024369806051254272
I0205 23:58:48.633742 139788813231872 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.11900225281715393, loss=0.02878681942820549
I0205 23:59:20.624271 139806663034624 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.10463239252567291, loss=0.0251576267182827
I0205 23:59:52.386338 139788813231872 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.10096383839845657, loss=0.027127115055918694
I0206 00:00:24.420564 139806663034624 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.11069279909133911, loss=0.030989577993750572
I0206 00:00:56.338304 139788813231872 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.13004928827285767, loss=0.027193397283554077
I0206 00:01:08.752325 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:02:56.439461 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:02:59.435876 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:03:02.502625 139978932307776 submission_runner.py:408] Time since start: 16948.44s, 	Step: 35940, 	{'train/accuracy': 0.9923170804977417, 'train/loss': 0.02455207146704197, 'train/mean_average_precision': 0.5424872716795479, 'validation/accuracy': 0.9867898225784302, 'validation/loss': 0.045494239777326584, 'validation/mean_average_precision': 0.28353401300497966, 'validation/num_examples': 43793, 'test/accuracy': 0.9859143495559692, 'test/loss': 0.04834071546792984, 'test/mean_average_precision': 0.2643073077441462, 'test/num_examples': 43793, 'score': 11536.095036268234, 'total_duration': 16948.44060611725, 'accumulated_submission_time': 11536.095036268234, 'accumulated_eval_time': 5409.861083984375, 'accumulated_logging_time': 1.496187686920166}
I0206 00:03:02.524565 139770797713152 logging_writer.py:48] [35940] accumulated_eval_time=5409.861084, accumulated_logging_time=1.496188, accumulated_submission_time=11536.095036, global_step=35940, preemption_count=0, score=11536.095036, test/accuracy=0.985914, test/loss=0.048341, test/mean_average_precision=0.264307, test/num_examples=43793, total_duration=16948.440606, train/accuracy=0.992317, train/loss=0.024552, train/mean_average_precision=0.542487, validation/accuracy=0.986790, validation/loss=0.045494, validation/mean_average_precision=0.283534, validation/num_examples=43793
I0206 00:03:21.952734 139789425600256 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.10465897619724274, loss=0.02807907573878765
I0206 00:03:53.636789 139770797713152 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.11069537699222565, loss=0.02672327496111393
I0206 00:04:25.756204 139789425600256 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.1095236986875534, loss=0.027290664613246918
I0206 00:04:57.561177 139770797713152 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.09486661851406097, loss=0.025764843448996544
I0206 00:05:29.754505 139789425600256 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.10679077357053757, loss=0.027025729417800903
I0206 00:06:01.558130 139770797713152 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.10211688280105591, loss=0.02792174369096756
I0206 00:06:33.479573 139789425600256 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.12605075538158417, loss=0.026514679193496704
I0206 00:07:02.628199 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:08:44.825449 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:08:47.859144 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:08:50.899042 139978932307776 submission_runner.py:408] Time since start: 17296.84s, 	Step: 36692, 	{'train/accuracy': 0.9918184876441956, 'train/loss': 0.026134414598345757, 'train/mean_average_precision': 0.5122857286942415, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.045166220515966415, 'validation/mean_average_precision': 0.2817615748781584, 'validation/num_examples': 43793, 'test/accuracy': 0.9858882427215576, 'test/loss': 0.04821586608886719, 'test/mean_average_precision': 0.2715438217701766, 'test/num_examples': 43793, 'score': 11776.166133642197, 'total_duration': 17296.837022542953, 'accumulated_submission_time': 11776.166133642197, 'accumulated_eval_time': 5518.131883144379, 'accumulated_logging_time': 1.530383825302124}
I0206 00:08:50.921071 139770806105856 logging_writer.py:48] [36692] accumulated_eval_time=5518.131883, accumulated_logging_time=1.530384, accumulated_submission_time=11776.166134, global_step=36692, preemption_count=0, score=11776.166134, test/accuracy=0.985888, test/loss=0.048216, test/mean_average_precision=0.271544, test/num_examples=43793, total_duration=17296.837023, train/accuracy=0.991818, train/loss=0.026134, train/mean_average_precision=0.512286, validation/accuracy=0.986724, validation/loss=0.045166, validation/mean_average_precision=0.281762, validation/num_examples=43793
I0206 00:08:53.795365 139788813231872 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.12404067069292068, loss=0.02511480450630188
I0206 00:09:25.152260 139770806105856 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.11287263035774231, loss=0.028017915785312653
I0206 00:09:56.619765 139788813231872 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.11306330561637878, loss=0.02945089153945446
I0206 00:10:28.207343 139770806105856 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.11504087597131729, loss=0.025711752474308014
I0206 00:10:59.710431 139788813231872 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1067270040512085, loss=0.026096830144524574
I0206 00:11:31.069521 139770806105856 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.10579425096511841, loss=0.02356972172856331
I0206 00:12:02.690878 139788813231872 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.11074049025774002, loss=0.028270116075873375
I0206 00:12:34.508051 139770806105856 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.116444431245327, loss=0.02836003713309765
I0206 00:12:50.998650 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:14:34.003438 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:14:37.156042 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:14:40.210558 139978932307776 submission_runner.py:408] Time since start: 17646.15s, 	Step: 37454, 	{'train/accuracy': 0.9918424487113953, 'train/loss': 0.026177141815423965, 'train/mean_average_precision': 0.4943024589468207, 'validation/accuracy': 0.9865970015525818, 'validation/loss': 0.04534425958991051, 'validation/mean_average_precision': 0.2697746331850882, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.04839446768164635, 'test/mean_average_precision': 0.26086746319160364, 'test/num_examples': 43793, 'score': 12016.210379838943, 'total_duration': 17646.148535490036, 'accumulated_submission_time': 12016.210379838943, 'accumulated_eval_time': 5627.343741178513, 'accumulated_logging_time': 1.5651028156280518}
I0206 00:14:40.233553 139770797713152 logging_writer.py:48] [37454] accumulated_eval_time=5627.343741, accumulated_logging_time=1.565103, accumulated_submission_time=12016.210380, global_step=37454, preemption_count=0, score=12016.210380, test/accuracy=0.985758, test/loss=0.048394, test/mean_average_precision=0.260867, test/num_examples=43793, total_duration=17646.148535, train/accuracy=0.991842, train/loss=0.026177, train/mean_average_precision=0.494302, validation/accuracy=0.986597, validation/loss=0.045344, validation/mean_average_precision=0.269775, validation/num_examples=43793
I0206 00:14:55.172048 139806663034624 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.11033833026885986, loss=0.02348661608994007
I0206 00:15:26.993987 139770797713152 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.13855396211147308, loss=0.029592348262667656
I0206 00:15:58.598759 139806663034624 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.11651565879583359, loss=0.02673301286995411
I0206 00:16:30.470019 139770797713152 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.10778134316205978, loss=0.026953862980008125
I0206 00:17:02.085043 139806663034624 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.12150134146213531, loss=0.027010533958673477
I0206 00:17:33.758225 139770797713152 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.1112501248717308, loss=0.024512721225619316
I0206 00:18:05.870207 139806663034624 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.11771052330732346, loss=0.028216715902090073
I0206 00:18:37.727838 139770797713152 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.10878944396972656, loss=0.026243703439831734
I0206 00:18:40.330074 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:20:20.732614 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:20:23.788720 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:20:26.812132 139978932307776 submission_runner.py:408] Time since start: 17992.75s, 	Step: 38209, 	{'train/accuracy': 0.9917693138122559, 'train/loss': 0.026469871401786804, 'train/mean_average_precision': 0.49684058513550633, 'validation/accuracy': 0.9866034984588623, 'validation/loss': 0.04540769383311272, 'validation/mean_average_precision': 0.2799366756800923, 'validation/num_examples': 43793, 'test/accuracy': 0.985753059387207, 'test/loss': 0.048491571098566055, 'test/mean_average_precision': 0.25814005772812937, 'test/num_examples': 43793, 'score': 12256.275754451752, 'total_duration': 17992.750111341476, 'accumulated_submission_time': 12256.275754451752, 'accumulated_eval_time': 5733.825747728348, 'accumulated_logging_time': 1.5990040302276611}
I0206 00:20:26.834479 139770806105856 logging_writer.py:48] [38209] accumulated_eval_time=5733.825748, accumulated_logging_time=1.599004, accumulated_submission_time=12256.275754, global_step=38209, preemption_count=0, score=12256.275754, test/accuracy=0.985753, test/loss=0.048492, test/mean_average_precision=0.258140, test/num_examples=43793, total_duration=17992.750111, train/accuracy=0.991769, train/loss=0.026470, train/mean_average_precision=0.496841, validation/accuracy=0.986603, validation/loss=0.045408, validation/mean_average_precision=0.279937, validation/num_examples=43793
I0206 00:20:56.043665 139789425600256 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.11088818311691284, loss=0.026300210505723953
I0206 00:21:27.936877 139770806105856 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.09748700261116028, loss=0.026238679885864258
I0206 00:21:59.697257 139789425600256 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.13507241010665894, loss=0.028759922832250595
I0206 00:22:31.589149 139770806105856 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.12429385632276535, loss=0.028377771377563477
I0206 00:23:03.829483 139789425600256 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.11664924025535583, loss=0.02296024188399315
I0206 00:23:35.495814 139770806105856 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.1342530995607376, loss=0.02570733055472374
I0206 00:24:07.023020 139789425600256 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.1247221902012825, loss=0.02405986748635769
I0206 00:24:26.899896 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:26:06.546525 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:26:09.713939 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:26:12.726554 139978932307776 submission_runner.py:408] Time since start: 18338.66s, 	Step: 38963, 	{'train/accuracy': 0.9920052289962769, 'train/loss': 0.025733111426234245, 'train/mean_average_precision': 0.49616740820467975, 'validation/accuracy': 0.9868503212928772, 'validation/loss': 0.04526519775390625, 'validation/mean_average_precision': 0.27865147177100186, 'validation/num_examples': 43793, 'test/accuracy': 0.9859619736671448, 'test/loss': 0.048397425562143326, 'test/mean_average_precision': 0.2601541778417228, 'test/num_examples': 43793, 'score': 12496.309900045395, 'total_duration': 18338.664535284042, 'accumulated_submission_time': 12496.309900045395, 'accumulated_eval_time': 5839.652360200882, 'accumulated_logging_time': 1.6322979927062988}
I0206 00:26:12.748961 139788813231872 logging_writer.py:48] [38963] accumulated_eval_time=5839.652360, accumulated_logging_time=1.632298, accumulated_submission_time=12496.309900, global_step=38963, preemption_count=0, score=12496.309900, test/accuracy=0.985962, test/loss=0.048397, test/mean_average_precision=0.260154, test/num_examples=43793, total_duration=18338.664535, train/accuracy=0.992005, train/loss=0.025733, train/mean_average_precision=0.496167, validation/accuracy=0.986850, validation/loss=0.045265, validation/mean_average_precision=0.278651, validation/num_examples=43793
I0206 00:26:24.857138 139806663034624 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.12957529723644257, loss=0.028134427964687347
I0206 00:26:56.277186 139788813231872 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.13197121024131775, loss=0.025779087096452713
I0206 00:27:27.955990 139806663034624 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.12122689932584763, loss=0.0239395834505558
I0206 00:27:59.602231 139788813231872 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.09653283655643463, loss=0.024382438510656357
I0206 00:28:31.402730 139806663034624 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.1056225597858429, loss=0.026063883677124977
I0206 00:29:02.778595 139788813231872 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1440473347902298, loss=0.027851535007357597
I0206 00:29:34.365553 139806663034624 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.12633241713047028, loss=0.0259874127805233
I0206 00:30:05.914590 139788813231872 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.10999839752912521, loss=0.023992463946342468
I0206 00:30:13.012057 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:31:54.841934 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:31:57.896694 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:32:00.886797 139978932307776 submission_runner.py:408] Time since start: 18686.82s, 	Step: 39723, 	{'train/accuracy': 0.9920634031295776, 'train/loss': 0.02536453679203987, 'train/mean_average_precision': 0.5136206193556406, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.045312780886888504, 'validation/mean_average_precision': 0.2773486024374958, 'validation/num_examples': 43793, 'test/accuracy': 0.9859986305236816, 'test/loss': 0.04844435304403305, 'test/mean_average_precision': 0.2660761208561459, 'test/num_examples': 43793, 'score': 12736.541811704636, 'total_duration': 18686.824773311615, 'accumulated_submission_time': 12736.541811704636, 'accumulated_eval_time': 5947.527049303055, 'accumulated_logging_time': 1.6655912399291992}
I0206 00:32:00.909607 139770806105856 logging_writer.py:48] [39723] accumulated_eval_time=5947.527049, accumulated_logging_time=1.665591, accumulated_submission_time=12736.541812, global_step=39723, preemption_count=0, score=12736.541812, test/accuracy=0.985999, test/loss=0.048444, test/mean_average_precision=0.266076, test/num_examples=43793, total_duration=18686.824773, train/accuracy=0.992063, train/loss=0.025365, train/mean_average_precision=0.513621, validation/accuracy=0.986785, validation/loss=0.045313, validation/mean_average_precision=0.277349, validation/num_examples=43793
I0206 00:32:26.108079 139789425600256 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.13386087119579315, loss=0.028428100049495697
I0206 00:32:57.865154 139770806105856 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.10211527347564697, loss=0.024325182661414146
I0206 00:33:29.104832 139789425600256 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.14111629128456116, loss=0.029836880043148994
I0206 00:34:01.264369 139770806105856 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.10688255727291107, loss=0.024119477719068527
I0206 00:34:32.763039 139789425600256 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.10597854107618332, loss=0.02532345801591873
I0206 00:35:04.554645 139770806105856 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.10134058445692062, loss=0.023941483348608017
I0206 00:35:36.342249 139789425600256 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.11190850287675858, loss=0.025350192561745644
I0206 00:36:01.168907 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:37:42.552385 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:37:45.597004 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:37:48.504337 139978932307776 submission_runner.py:408] Time since start: 19034.44s, 	Step: 40479, 	{'train/accuracy': 0.9920461177825928, 'train/loss': 0.025316696614027023, 'train/mean_average_precision': 0.527780740298631, 'validation/accuracy': 0.986867368221283, 'validation/loss': 0.04554228484630585, 'validation/mean_average_precision': 0.28026867914384174, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.048717182129621506, 'test/mean_average_precision': 0.26557231991056374, 'test/num_examples': 43793, 'score': 12976.76999258995, 'total_duration': 19034.442306518555, 'accumulated_submission_time': 12976.76999258995, 'accumulated_eval_time': 6054.862420797348, 'accumulated_logging_time': 1.6991665363311768}
I0206 00:37:48.528210 139770797713152 logging_writer.py:48] [40479] accumulated_eval_time=6054.862421, accumulated_logging_time=1.699167, accumulated_submission_time=12976.769993, global_step=40479, preemption_count=0, score=12976.769993, test/accuracy=0.985933, test/loss=0.048717, test/mean_average_precision=0.265572, test/num_examples=43793, total_duration=19034.442307, train/accuracy=0.992046, train/loss=0.025317, train/mean_average_precision=0.527781, validation/accuracy=0.986867, validation/loss=0.045542, validation/mean_average_precision=0.280269, validation/num_examples=43793
I0206 00:37:55.579023 139806663034624 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.13558557629585266, loss=0.024021174758672714
I0206 00:38:27.203594 139770797713152 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.1183881014585495, loss=0.02353125996887684
I0206 00:38:58.609032 139806663034624 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.11435689777135849, loss=0.02377968281507492
I0206 00:39:30.567092 139770797713152 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.14338423311710358, loss=0.024835798889398575
I0206 00:40:01.719119 139806663034624 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.1372530609369278, loss=0.026685986667871475
I0206 00:40:33.530287 139770797713152 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.13369543850421906, loss=0.025704151019454002
I0206 00:41:05.007249 139806663034624 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.12548084557056427, loss=0.025833331048488617
I0206 00:41:36.568762 139770797713152 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.1350887566804886, loss=0.02479652315378189
I0206 00:41:48.769318 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:43:31.529941 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:43:34.549437 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:43:37.461503 139978932307776 submission_runner.py:408] Time since start: 19383.40s, 	Step: 41240, 	{'train/accuracy': 0.9922798275947571, 'train/loss': 0.024545539170503616, 'train/mean_average_precision': 0.524614840843699, 'validation/accuracy': 0.9867496490478516, 'validation/loss': 0.045290201902389526, 'validation/mean_average_precision': 0.283905575901221, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.04830815643072128, 'test/mean_average_precision': 0.2637783218790555, 'test/num_examples': 43793, 'score': 13216.979594230652, 'total_duration': 19383.399483442307, 'accumulated_submission_time': 13216.979594230652, 'accumulated_eval_time': 6163.554557561874, 'accumulated_logging_time': 1.7342724800109863}
I0206 00:43:37.484676 139788813231872 logging_writer.py:48] [41240] accumulated_eval_time=6163.554558, accumulated_logging_time=1.734272, accumulated_submission_time=13216.979594, global_step=41240, preemption_count=0, score=13216.979594, test/accuracy=0.985854, test/loss=0.048308, test/mean_average_precision=0.263778, test/num_examples=43793, total_duration=19383.399483, train/accuracy=0.992280, train/loss=0.024546, train/mean_average_precision=0.524615, validation/accuracy=0.986750, validation/loss=0.045290, validation/mean_average_precision=0.283906, validation/num_examples=43793
I0206 00:43:56.867445 139789425600256 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.12500324845314026, loss=0.022859608754515648
I0206 00:44:28.482665 139788813231872 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.11642859131097794, loss=0.02324097417294979
I0206 00:45:00.249040 139789425600256 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.13618585467338562, loss=0.02915908955037594
I0206 00:45:31.992277 139788813231872 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.12449375540018082, loss=0.02556922286748886
I0206 00:46:03.911674 139789425600256 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.1224217563867569, loss=0.02427896112203598
I0206 00:46:35.419562 139788813231872 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.12485071271657944, loss=0.02406327985227108
I0206 00:47:07.419242 139789425600256 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.11588690429925919, loss=0.023129213601350784
I0206 00:47:37.598934 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:49:14.033472 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:49:17.011866 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:49:19.918962 139978932307776 submission_runner.py:408] Time since start: 19725.86s, 	Step: 41996, 	{'train/accuracy': 0.9924442172050476, 'train/loss': 0.02393968589603901, 'train/mean_average_precision': 0.5495143535314464, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.045334722846746445, 'validation/mean_average_precision': 0.2868209618737811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858916401863098, 'test/loss': 0.048358894884586334, 'test/mean_average_precision': 0.2650313659736344, 'test/num_examples': 43793, 'score': 13457.06114768982, 'total_duration': 19725.856940746307, 'accumulated_submission_time': 13457.06114768982, 'accumulated_eval_time': 6265.874538183212, 'accumulated_logging_time': 1.769547462463379}
I0206 00:49:19.942438 139770806105856 logging_writer.py:48] [41996] accumulated_eval_time=6265.874538, accumulated_logging_time=1.769547, accumulated_submission_time=13457.061148, global_step=41996, preemption_count=0, score=13457.061148, test/accuracy=0.985892, test/loss=0.048359, test/mean_average_precision=0.265031, test/num_examples=43793, total_duration=19725.856941, train/accuracy=0.992444, train/loss=0.023940, train/mean_average_precision=0.549514, validation/accuracy=0.986664, validation/loss=0.045335, validation/mean_average_precision=0.286821, validation/num_examples=43793
I0206 00:49:21.582915 139806663034624 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1314036250114441, loss=0.024067280814051628
I0206 00:49:53.159508 139770806105856 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.11442113667726517, loss=0.02307933010160923
I0206 00:50:24.929182 139806663034624 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.1443026214838028, loss=0.025347057729959488
I0206 00:50:56.555112 139770806105856 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1449243128299713, loss=0.026674004271626472
I0206 00:51:28.310732 139806663034624 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.12389042228460312, loss=0.024783065542578697
I0206 00:51:59.767224 139770806105856 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.13002875447273254, loss=0.02435358241200447
I0206 00:52:31.820027 139806663034624 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.15121479332447052, loss=0.026072170585393906
I0206 00:53:03.571157 139770806105856 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.12358295917510986, loss=0.023889146745204926
I0206 00:53:20.027420 139978932307776 spec.py:321] Evaluating on the training split.
I0206 00:55:04.723033 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 00:55:07.759436 139978932307776 spec.py:349] Evaluating on the test split.
I0206 00:55:10.706993 139978932307776 submission_runner.py:408] Time since start: 20076.64s, 	Step: 42753, 	{'train/accuracy': 0.9926914572715759, 'train/loss': 0.02306647226214409, 'train/mean_average_precision': 0.5733023445192826, 'validation/accuracy': 0.9868162274360657, 'validation/loss': 0.04585631936788559, 'validation/mean_average_precision': 0.2811874228785678, 'validation/num_examples': 43793, 'test/accuracy': 0.9859809279441833, 'test/loss': 0.0491667203605175, 'test/mean_average_precision': 0.2701141743328484, 'test/num_examples': 43793, 'score': 13697.11407327652, 'total_duration': 20076.64497256279, 'accumulated_submission_time': 13697.11407327652, 'accumulated_eval_time': 6376.554070949554, 'accumulated_logging_time': 1.804668664932251}
I0206 00:55:10.730191 139788813231872 logging_writer.py:48] [42753] accumulated_eval_time=6376.554071, accumulated_logging_time=1.804669, accumulated_submission_time=13697.114073, global_step=42753, preemption_count=0, score=13697.114073, test/accuracy=0.985981, test/loss=0.049167, test/mean_average_precision=0.270114, test/num_examples=43793, total_duration=20076.644973, train/accuracy=0.992691, train/loss=0.023066, train/mean_average_precision=0.573302, validation/accuracy=0.986816, validation/loss=0.045856, validation/mean_average_precision=0.281187, validation/num_examples=43793
I0206 00:55:25.965925 139789425600256 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.13272903859615326, loss=0.024059297516942024
I0206 00:55:57.772770 139788813231872 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.13536010682582855, loss=0.023195555433630943
I0206 00:56:29.709526 139789425600256 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.11197306215763092, loss=0.02277560532093048
I0206 00:57:02.128453 139788813231872 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.11769318580627441, loss=0.02791133150458336
I0206 00:57:33.615419 139789425600256 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1264970600605011, loss=0.024536916986107826
I0206 00:58:05.385294 139788813231872 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.12481296807527542, loss=0.025209326297044754
I0206 00:58:37.059614 139789425600256 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.14805839955806732, loss=0.026448115706443787
I0206 00:59:08.684839 139788813231872 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.13984642922878265, loss=0.024833297356963158
I0206 00:59:10.943388 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:00:55.451732 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:00:58.505096 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:01:01.511650 139978932307776 submission_runner.py:408] Time since start: 20427.45s, 	Step: 43508, 	{'train/accuracy': 0.9930054545402527, 'train/loss': 0.022223111242055893, 'train/mean_average_precision': 0.5976667659929961, 'validation/accuracy': 0.9868007898330688, 'validation/loss': 0.04619989171624184, 'validation/mean_average_precision': 0.28058899499968, 'validation/num_examples': 43793, 'test/accuracy': 0.9859379529953003, 'test/loss': 0.04921237379312515, 'test/mean_average_precision': 0.26696407008968237, 'test/num_examples': 43793, 'score': 13937.296043395996, 'total_duration': 20427.44962954521, 'accumulated_submission_time': 13937.296043395996, 'accumulated_eval_time': 6487.12228512764, 'accumulated_logging_time': 1.838754415512085}
I0206 01:01:01.535628 139770797713152 logging_writer.py:48] [43508] accumulated_eval_time=6487.122285, accumulated_logging_time=1.838754, accumulated_submission_time=13937.296043, global_step=43508, preemption_count=0, score=13937.296043, test/accuracy=0.985938, test/loss=0.049212, test/mean_average_precision=0.266964, test/num_examples=43793, total_duration=20427.449630, train/accuracy=0.993005, train/loss=0.022223, train/mean_average_precision=0.597667, validation/accuracy=0.986801, validation/loss=0.046200, validation/mean_average_precision=0.280589, validation/num_examples=43793
I0206 01:01:31.637689 139806663034624 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.15542080998420715, loss=0.027103474363684654
I0206 01:02:03.221166 139770797713152 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1270117610692978, loss=0.025728287175297737
I0206 01:02:35.029941 139806663034624 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.14169949293136597, loss=0.024309108033776283
I0206 01:03:06.862538 139770797713152 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.12633974850177765, loss=0.023743433877825737
I0206 01:03:38.748649 139806663034624 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.14386019110679626, loss=0.023796986788511276
I0206 01:04:10.331076 139770797713152 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.14341720938682556, loss=0.024076087400317192
I0206 01:04:42.207104 139806663034624 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.12603811919689178, loss=0.02454461343586445
I0206 01:05:01.526844 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:06:43.288913 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:06:46.284325 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:06:49.194077 139978932307776 submission_runner.py:408] Time since start: 20775.13s, 	Step: 44262, 	{'train/accuracy': 0.9929794073104858, 'train/loss': 0.022425740957260132, 'train/mean_average_precision': 0.5909267678181348, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.046438559889793396, 'validation/mean_average_precision': 0.2845100129284141, 'validation/num_examples': 43793, 'test/accuracy': 0.985765278339386, 'test/loss': 0.049320854246616364, 'test/mean_average_precision': 0.2665610301084667, 'test/num_examples': 43793, 'score': 14177.254234313965, 'total_duration': 20775.13205766678, 'accumulated_submission_time': 14177.254234313965, 'accumulated_eval_time': 6594.789473056793, 'accumulated_logging_time': 1.8754479885101318}
I0206 01:06:49.218025 139788813231872 logging_writer.py:48] [44262] accumulated_eval_time=6594.789473, accumulated_logging_time=1.875448, accumulated_submission_time=14177.254234, global_step=44262, preemption_count=0, score=14177.254234, test/accuracy=0.985765, test/loss=0.049321, test/mean_average_precision=0.266561, test/num_examples=43793, total_duration=20775.132058, train/accuracy=0.992979, train/loss=0.022426, train/mean_average_precision=0.590927, validation/accuracy=0.986566, validation/loss=0.046439, validation/mean_average_precision=0.284510, validation/num_examples=43793
I0206 01:07:01.991077 139789425600256 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.17364999651908875, loss=0.026924384757876396
I0206 01:07:34.202378 139788813231872 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.17893527448177338, loss=0.021565038710832596
I0206 01:08:06.065832 139789425600256 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.12542910873889923, loss=0.022631805390119553
I0206 01:08:38.008953 139788813231872 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1282690316438675, loss=0.027355976402759552
I0206 01:09:09.973952 139789425600256 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.13814640045166016, loss=0.022724542766809464
I0206 01:09:41.740568 139788813231872 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.1357976347208023, loss=0.025011835619807243
I0206 01:10:13.580049 139789425600256 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.16054166853427887, loss=0.025730645284056664
I0206 01:10:45.775630 139788813231872 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.141132652759552, loss=0.022995127364993095
I0206 01:10:49.516559 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:12:31.695684 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:12:34.745859 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:12:37.653788 139978932307776 submission_runner.py:408] Time since start: 21123.59s, 	Step: 45013, 	{'train/accuracy': 0.9926031827926636, 'train/loss': 0.02358449250459671, 'train/mean_average_precision': 0.553874841307491, 'validation/accuracy': 0.9867179989814758, 'validation/loss': 0.046413395553827286, 'validation/mean_average_precision': 0.27960552758316165, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.049666281789541245, 'test/mean_average_precision': 0.2710117089890043, 'test/num_examples': 43793, 'score': 14417.521105766296, 'total_duration': 21123.5917699337, 'accumulated_submission_time': 14417.521105766296, 'accumulated_eval_time': 6702.926654815674, 'accumulated_logging_time': 1.910888671875}
I0206 01:12:37.677453 139770797713152 logging_writer.py:48] [45013] accumulated_eval_time=6702.926655, accumulated_logging_time=1.910889, accumulated_submission_time=14417.521106, global_step=45013, preemption_count=0, score=14417.521106, test/accuracy=0.985882, test/loss=0.049666, test/mean_average_precision=0.271012, test/num_examples=43793, total_duration=21123.591770, train/accuracy=0.992603, train/loss=0.023584, train/mean_average_precision=0.553875, validation/accuracy=0.986718, validation/loss=0.046413, validation/mean_average_precision=0.279606, validation/num_examples=43793
I0206 01:13:05.638068 139806663034624 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.16105006635189056, loss=0.02528332732617855
I0206 01:13:37.279802 139770797713152 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1358654946088791, loss=0.02439125068485737
I0206 01:14:09.036634 139806663034624 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.1186634823679924, loss=0.02175576612353325
I0206 01:14:40.878984 139770797713152 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.11324339359998703, loss=0.02228918857872486
I0206 01:15:12.402281 139806663034624 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.13974708318710327, loss=0.022488003596663475
I0206 01:15:44.440012 139770797713152 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.15340037643909454, loss=0.024622013792395592
I0206 01:16:16.360786 139806663034624 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.15771491825580597, loss=0.02460695244371891
I0206 01:16:37.806662 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:18:21.928970 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:18:24.898557 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:18:27.815306 139978932307776 submission_runner.py:408] Time since start: 21473.75s, 	Step: 45768, 	{'train/accuracy': 0.9925385117530823, 'train/loss': 0.02367567829787731, 'train/mean_average_precision': 0.5445353539343192, 'validation/accuracy': 0.9865588545799255, 'validation/loss': 0.04667806997895241, 'validation/mean_average_precision': 0.27965255637826175, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.04969778656959534, 'test/mean_average_precision': 0.2654268209684074, 'test/num_examples': 43793, 'score': 14657.61928486824, 'total_duration': 21473.753289222717, 'accumulated_submission_time': 14657.61928486824, 'accumulated_eval_time': 6812.935254096985, 'accumulated_logging_time': 1.94553542137146}
I0206 01:18:27.839198 139788813231872 logging_writer.py:48] [45768] accumulated_eval_time=6812.935254, accumulated_logging_time=1.945535, accumulated_submission_time=14657.619285, global_step=45768, preemption_count=0, score=14657.619285, test/accuracy=0.985740, test/loss=0.049698, test/mean_average_precision=0.265427, test/num_examples=43793, total_duration=21473.753289, train/accuracy=0.992539, train/loss=0.023676, train/mean_average_precision=0.544535, validation/accuracy=0.986559, validation/loss=0.046678, validation/mean_average_precision=0.279653, validation/num_examples=43793
I0206 01:18:38.235798 139789425600256 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.16730406880378723, loss=0.021333880722522736
I0206 01:19:09.615006 139788813231872 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.13555364310741425, loss=0.023292718455195427
I0206 01:19:41.067871 139789425600256 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.1469734162092209, loss=0.025416452437639236
I0206 01:20:12.638244 139788813231872 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.129384845495224, loss=0.02178332395851612
I0206 01:20:43.943146 139789425600256 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.15563936531543732, loss=0.02211005799472332
I0206 01:21:15.939886 139788813231872 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.13512127101421356, loss=0.023628463968634605
I0206 01:21:47.628421 139789425600256 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.18197935819625854, loss=0.022461850196123123
I0206 01:22:19.303363 139788813231872 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.13498198986053467, loss=0.021862095221877098
I0206 01:22:27.966014 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:24:04.509024 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:24:07.481718 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:24:10.394114 139978932307776 submission_runner.py:408] Time since start: 21816.33s, 	Step: 46528, 	{'train/accuracy': 0.9925999045372009, 'train/loss': 0.02357323095202446, 'train/mean_average_precision': 0.5610417006179397, 'validation/accuracy': 0.9867228865623474, 'validation/loss': 0.04652188718318939, 'validation/mean_average_precision': 0.27857221709013974, 'validation/num_examples': 43793, 'test/accuracy': 0.9858840703964233, 'test/loss': 0.04971200227737427, 'test/mean_average_precision': 0.2653429253234877, 'test/num_examples': 43793, 'score': 14897.713246583939, 'total_duration': 21816.33208823204, 'accumulated_submission_time': 14897.713246583939, 'accumulated_eval_time': 6915.363301515579, 'accumulated_logging_time': 1.9816162586212158}
I0206 01:24:10.418543 139770797713152 logging_writer.py:48] [46528] accumulated_eval_time=6915.363302, accumulated_logging_time=1.981616, accumulated_submission_time=14897.713247, global_step=46528, preemption_count=0, score=14897.713247, test/accuracy=0.985884, test/loss=0.049712, test/mean_average_precision=0.265343, test/num_examples=43793, total_duration=21816.332088, train/accuracy=0.992600, train/loss=0.023573, train/mean_average_precision=0.561042, validation/accuracy=0.986723, validation/loss=0.046522, validation/mean_average_precision=0.278572, validation/num_examples=43793
I0206 01:24:33.414184 139806663034624 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.17777682840824127, loss=0.023785177618265152
I0206 01:25:05.528164 139770797713152 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.15247149765491486, loss=0.026409106329083443
I0206 01:25:37.336678 139806663034624 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.1585337072610855, loss=0.023832183331251144
I0206 01:26:09.280078 139770797713152 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.152195543050766, loss=0.02559644728899002
I0206 01:26:40.914993 139806663034624 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.1355060636997223, loss=0.021008815616369247
I0206 01:27:12.720848 139770797713152 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.1290014386177063, loss=0.02329573966562748
I0206 01:27:44.525842 139806663034624 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1517123132944107, loss=0.023885101079940796
I0206 01:28:10.623627 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:29:53.765762 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:29:56.827744 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:29:59.888858 139978932307776 submission_runner.py:408] Time since start: 22165.83s, 	Step: 47283, 	{'train/accuracy': 0.9925141930580139, 'train/loss': 0.023568419739603996, 'train/mean_average_precision': 0.5593789092608922, 'validation/accuracy': 0.9866579174995422, 'validation/loss': 0.04711558297276497, 'validation/mean_average_precision': 0.2736626211202902, 'validation/num_examples': 43793, 'test/accuracy': 0.9857581257820129, 'test/loss': 0.05036972090601921, 'test/mean_average_precision': 0.2608815513531671, 'test/num_examples': 43793, 'score': 15137.886631250381, 'total_duration': 22165.82682275772, 'accumulated_submission_time': 15137.886631250381, 'accumulated_eval_time': 7024.628471374512, 'accumulated_logging_time': 2.017472982406616}
I0206 01:29:59.913510 139770806105856 logging_writer.py:48] [47283] accumulated_eval_time=7024.628471, accumulated_logging_time=2.017473, accumulated_submission_time=15137.886631, global_step=47283, preemption_count=0, score=15137.886631, test/accuracy=0.985758, test/loss=0.050370, test/mean_average_precision=0.260882, test/num_examples=43793, total_duration=22165.826823, train/accuracy=0.992514, train/loss=0.023568, train/mean_average_precision=0.559379, validation/accuracy=0.986658, validation/loss=0.047116, validation/mean_average_precision=0.273663, validation/num_examples=43793
I0206 01:30:05.702695 139789425600256 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.17854829132556915, loss=0.025735588744282722
I0206 01:30:37.396500 139770806105856 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.14856989681720734, loss=0.022536443546414375
I0206 01:31:09.063365 139789425600256 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.14530925452709198, loss=0.022960111498832703
I0206 01:31:40.723657 139770806105856 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.17270267009735107, loss=0.023107022047042847
I0206 01:32:12.560577 139789425600256 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.18713432550430298, loss=0.02316875196993351
I0206 01:32:44.808419 139770806105856 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.15556210279464722, loss=0.02236565761268139
I0206 01:33:16.751796 139789425600256 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1558842658996582, loss=0.022638479247689247
I0206 01:33:48.487580 139770806105856 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.15798762440681458, loss=0.02050134912133217
I0206 01:33:59.911666 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:35:36.611180 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:35:39.653853 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:35:42.611566 139978932307776 submission_runner.py:408] Time since start: 22508.55s, 	Step: 48037, 	{'train/accuracy': 0.992720901966095, 'train/loss': 0.02295967936515808, 'train/mean_average_precision': 0.5675811443116955, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.0468745082616806, 'validation/mean_average_precision': 0.2781233637894734, 'validation/num_examples': 43793, 'test/accuracy': 0.9855588674545288, 'test/loss': 0.050317663699388504, 'test/mean_average_precision': 0.2622498398743667, 'test/num_examples': 43793, 'score': 15377.85105419159, 'total_duration': 22508.549419403076, 'accumulated_submission_time': 15377.85105419159, 'accumulated_eval_time': 7127.328198194504, 'accumulated_logging_time': 2.0556468963623047}
I0206 01:35:42.635503 139770797713152 logging_writer.py:48] [48037] accumulated_eval_time=7127.328198, accumulated_logging_time=2.055647, accumulated_submission_time=15377.851054, global_step=48037, preemption_count=0, score=15377.851054, test/accuracy=0.985559, test/loss=0.050318, test/mean_average_precision=0.262250, test/num_examples=43793, total_duration=22508.549419, train/accuracy=0.992721, train/loss=0.022960, train/mean_average_precision=0.567581, validation/accuracy=0.986549, validation/loss=0.046875, validation/mean_average_precision=0.278123, validation/num_examples=43793
I0206 01:36:03.205213 139788813231872 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.14474329352378845, loss=0.021637963131070137
I0206 01:36:34.883113 139770797713152 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.15293167531490326, loss=0.021303357556462288
I0206 01:37:06.488745 139788813231872 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.1291143298149109, loss=0.019402772188186646
I0206 01:37:38.414033 139770797713152 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.1364668607711792, loss=0.022028634324669838
I0206 01:38:10.066299 139788813231872 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.15309776365756989, loss=0.024459978565573692
I0206 01:38:42.078016 139770797713152 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.13941948115825653, loss=0.02161841280758381
I0206 01:39:14.094759 139788813231872 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.17246507108211517, loss=0.022755112498998642
I0206 01:39:42.853264 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:41:19.612671 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:41:22.610818 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:41:25.520525 139978932307776 submission_runner.py:408] Time since start: 22851.46s, 	Step: 48792, 	{'train/accuracy': 0.9929220080375671, 'train/loss': 0.02197171375155449, 'train/mean_average_precision': 0.5988421261644996, 'validation/accuracy': 0.9868214726448059, 'validation/loss': 0.047487981617450714, 'validation/mean_average_precision': 0.2777354293557749, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.05089396610856056, 'test/mean_average_precision': 0.26474430938656773, 'test/num_examples': 43793, 'score': 15618.037787914276, 'total_duration': 22851.45850634575, 'accumulated_submission_time': 15618.037787914276, 'accumulated_eval_time': 7229.995413780212, 'accumulated_logging_time': 2.0907273292541504}
I0206 01:41:25.545105 139789425600256 logging_writer.py:48] [48792] accumulated_eval_time=7229.995414, accumulated_logging_time=2.090727, accumulated_submission_time=15618.037788, global_step=48792, preemption_count=0, score=15618.037788, test/accuracy=0.985991, test/loss=0.050894, test/mean_average_precision=0.264744, test/num_examples=43793, total_duration=22851.458506, train/accuracy=0.992922, train/loss=0.021972, train/mean_average_precision=0.598842, validation/accuracy=0.986821, validation/loss=0.047488, validation/mean_average_precision=0.277735, validation/num_examples=43793
I0206 01:41:28.731071 139806663034624 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.16362233459949493, loss=0.023846004158258438
I0206 01:42:00.360760 139789425600256 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.16487416625022888, loss=0.021610742434859276
I0206 01:42:32.491228 139806663034624 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.158416286110878, loss=0.02078058198094368
I0206 01:43:04.618368 139789425600256 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.1763972043991089, loss=0.018981005996465683
I0206 01:43:36.431529 139806663034624 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19086752831935883, loss=0.024684995412826538
I0206 01:44:08.275153 139789425600256 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.15176144242286682, loss=0.02068314142525196
I0206 01:44:39.889488 139806663034624 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.15278497338294983, loss=0.022260606288909912
I0206 01:45:11.783772 139789425600256 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.14765511453151703, loss=0.020907143130898476
I0206 01:45:25.797600 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:47:05.330327 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:47:08.334646 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:47:11.302025 139978932307776 submission_runner.py:408] Time since start: 23197.24s, 	Step: 49545, 	{'train/accuracy': 0.9931709170341492, 'train/loss': 0.021309148520231247, 'train/mean_average_precision': 0.6088010839997859, 'validation/accuracy': 0.986845850944519, 'validation/loss': 0.04742836952209473, 'validation/mean_average_precision': 0.2821814441667886, 'validation/num_examples': 43793, 'test/accuracy': 0.9858722686767578, 'test/loss': 0.05089188367128372, 'test/mean_average_precision': 0.26339655352001395, 'test/num_examples': 43793, 'score': 15857.938402414322, 'total_duration': 23197.240000247955, 'accumulated_submission_time': 15857.938402414322, 'accumulated_eval_time': 7335.499788284302, 'accumulated_logging_time': 2.4467310905456543}
I0206 01:47:11.327621 139770806105856 logging_writer.py:48] [49545] accumulated_eval_time=7335.499788, accumulated_logging_time=2.446731, accumulated_submission_time=15857.938402, global_step=49545, preemption_count=0, score=15857.938402, test/accuracy=0.985872, test/loss=0.050892, test/mean_average_precision=0.263397, test/num_examples=43793, total_duration=23197.240000, train/accuracy=0.993171, train/loss=0.021309, train/mean_average_precision=0.608801, validation/accuracy=0.986846, validation/loss=0.047428, validation/mean_average_precision=0.282181, validation/num_examples=43793
I0206 01:47:29.127137 139788813231872 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.1500767022371292, loss=0.022977713495492935
I0206 01:48:01.171449 139770806105856 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.15683889389038086, loss=0.02364390902221203
I0206 01:48:33.003538 139788813231872 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.15375439822673798, loss=0.022072842344641685
I0206 01:49:04.862699 139770806105856 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.18068870902061462, loss=0.02235373854637146
I0206 01:49:36.766141 139788813231872 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.18269513547420502, loss=0.02032562345266342
I0206 01:50:08.362429 139770806105856 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1615293323993683, loss=0.02277568355202675
I0206 01:50:40.403370 139788813231872 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.16311733424663544, loss=0.020047016441822052
I0206 01:51:11.585588 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:52:53.022571 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:52:56.083220 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:52:59.052478 139978932307776 submission_runner.py:408] Time since start: 23544.99s, 	Step: 50299, 	{'train/accuracy': 0.9934626221656799, 'train/loss': 0.02049173228442669, 'train/mean_average_precision': 0.6304601455926362, 'validation/accuracy': 0.9867045879364014, 'validation/loss': 0.047425173223018646, 'validation/mean_average_precision': 0.2834991495572792, 'validation/num_examples': 43793, 'test/accuracy': 0.9857922196388245, 'test/loss': 0.05078952759504318, 'test/mean_average_precision': 0.2697486751280569, 'test/num_examples': 43793, 'score': 16098.165282726288, 'total_duration': 23544.99045753479, 'accumulated_submission_time': 16098.165282726288, 'accumulated_eval_time': 7442.966633319855, 'accumulated_logging_time': 2.483210563659668}
I0206 01:52:59.077485 139770797713152 logging_writer.py:48] [50299] accumulated_eval_time=7442.966633, accumulated_logging_time=2.483211, accumulated_submission_time=16098.165283, global_step=50299, preemption_count=0, score=16098.165283, test/accuracy=0.985792, test/loss=0.050790, test/mean_average_precision=0.269749, test/num_examples=43793, total_duration=23544.990458, train/accuracy=0.993463, train/loss=0.020492, train/mean_average_precision=0.630460, validation/accuracy=0.986705, validation/loss=0.047425, validation/mean_average_precision=0.283499, validation/num_examples=43793
I0206 01:52:59.725044 139806663034624 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.18235911428928375, loss=0.019658315926790237
I0206 01:53:31.944940 139770797713152 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.14669957756996155, loss=0.019688680768013
I0206 01:54:03.770715 139806663034624 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.14905506372451782, loss=0.02090434730052948
I0206 01:54:35.262434 139770797713152 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.18642447888851166, loss=0.022596389055252075
I0206 01:55:07.216789 139806663034624 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.15924331545829773, loss=0.020372258499264717
I0206 01:55:38.701339 139770797713152 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.17022588849067688, loss=0.022130625322461128
I0206 01:56:10.774904 139806663034624 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.17483046650886536, loss=0.02281816676259041
I0206 01:56:42.256705 139770797713152 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.15735109150409698, loss=0.020856281742453575
I0206 01:56:59.139148 139978932307776 spec.py:321] Evaluating on the training split.
I0206 01:58:37.176235 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 01:58:40.167623 139978932307776 spec.py:349] Evaluating on the test split.
I0206 01:58:43.132959 139978932307776 submission_runner.py:408] Time since start: 23889.07s, 	Step: 51055, 	{'train/accuracy': 0.9939581155776978, 'train/loss': 0.01905048079788685, 'train/mean_average_precision': 0.6652305154089658, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.048273272812366486, 'validation/mean_average_precision': 0.2792698083358352, 'validation/num_examples': 43793, 'test/accuracy': 0.9858272075653076, 'test/loss': 0.05148047208786011, 'test/mean_average_precision': 0.26181299020306065, 'test/num_examples': 43793, 'score': 16338.194887399673, 'total_duration': 23889.070935964584, 'accumulated_submission_time': 16338.194887399673, 'accumulated_eval_time': 7546.960394859314, 'accumulated_logging_time': 2.5192384719848633}
I0206 01:58:43.158513 139770806105856 logging_writer.py:48] [51055] accumulated_eval_time=7546.960395, accumulated_logging_time=2.519238, accumulated_submission_time=16338.194887, global_step=51055, preemption_count=0, score=16338.194887, test/accuracy=0.985827, test/loss=0.051480, test/mean_average_precision=0.261813, test/num_examples=43793, total_duration=23889.070936, train/accuracy=0.993958, train/loss=0.019050, train/mean_average_precision=0.665231, validation/accuracy=0.986686, validation/loss=0.048273, validation/mean_average_precision=0.279270, validation/num_examples=43793
I0206 01:58:57.695183 139788813231872 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.16324664652347565, loss=0.021712563931941986
I0206 01:59:29.207090 139770806105856 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.17218376696109772, loss=0.022380735725164413
I0206 02:00:00.843368 139788813231872 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.18231472373008728, loss=0.021911488845944405
I0206 02:00:32.555718 139770806105856 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.1536174714565277, loss=0.018736582249403
I0206 02:01:03.924061 139788813231872 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.16504670679569244, loss=0.02038097009062767
I0206 02:01:35.512331 139770806105856 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.18007276952266693, loss=0.023572344332933426
I0206 02:02:07.220417 139788813231872 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.1730537712574005, loss=0.02088896557688713
I0206 02:02:39.066915 139770806105856 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20673376321792603, loss=0.0203634575009346
I0206 02:02:43.256685 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:04:23.169440 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:04:26.113866 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:04:29.086545 139978932307776 submission_runner.py:408] Time since start: 24235.02s, 	Step: 51814, 	{'train/accuracy': 0.9940596222877502, 'train/loss': 0.018846679478883743, 'train/mean_average_precision': 0.6622694237726308, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04867831990122795, 'validation/mean_average_precision': 0.2745682719653072, 'validation/num_examples': 43793, 'test/accuracy': 0.9856536388397217, 'test/loss': 0.052153799682855606, 'test/mean_average_precision': 0.2611147893670439, 'test/num_examples': 43793, 'score': 16578.26107263565, 'total_duration': 24235.02452325821, 'accumulated_submission_time': 16578.26107263565, 'accumulated_eval_time': 7652.790205001831, 'accumulated_logging_time': 2.5558063983917236}
I0206 02:04:29.112404 139770797713152 logging_writer.py:48] [51814] accumulated_eval_time=7652.790205, accumulated_logging_time=2.555806, accumulated_submission_time=16578.261073, global_step=51814, preemption_count=0, score=16578.261073, test/accuracy=0.985654, test/loss=0.052154, test/mean_average_precision=0.261115, test/num_examples=43793, total_duration=24235.024523, train/accuracy=0.994060, train/loss=0.018847, train/mean_average_precision=0.662269, validation/accuracy=0.986575, validation/loss=0.048678, validation/mean_average_precision=0.274568, validation/num_examples=43793
I0206 02:04:56.871068 139806663034624 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.18098996579647064, loss=0.022537559270858765
I0206 02:05:28.726810 139770797713152 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.1612270176410675, loss=0.02063380740582943
I0206 02:05:59.927843 139806663034624 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.1805545836687088, loss=0.022888464853167534
I0206 02:06:31.537312 139770797713152 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.15902864933013916, loss=0.020223479717969894
I0206 02:07:03.535206 139806663034624 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.15640828013420105, loss=0.019900644198060036
I0206 02:07:35.110515 139770797713152 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.19662420451641083, loss=0.02093961276113987
I0206 02:08:06.726602 139806663034624 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.18143753707408905, loss=0.023823287338018417
I0206 02:08:29.389771 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:10:11.935327 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:10:14.932651 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:10:17.971517 139978932307776 submission_runner.py:408] Time since start: 24583.91s, 	Step: 52572, 	{'train/accuracy': 0.9937030076980591, 'train/loss': 0.019762754440307617, 'train/mean_average_precision': 0.6384974196668561, 'validation/accuracy': 0.9864805340766907, 'validation/loss': 0.04896825924515724, 'validation/mean_average_precision': 0.276152484739644, 'validation/num_examples': 43793, 'test/accuracy': 0.9855180382728577, 'test/loss': 0.052332133054733276, 'test/mean_average_precision': 0.26013065956833903, 'test/num_examples': 43793, 'score': 16818.50644493103, 'total_duration': 24583.90949845314, 'accumulated_submission_time': 16818.50644493103, 'accumulated_eval_time': 7761.37190580368, 'accumulated_logging_time': 2.5926008224487305}
I0206 02:10:17.997458 139788813231872 logging_writer.py:48] [52572] accumulated_eval_time=7761.371906, accumulated_logging_time=2.592601, accumulated_submission_time=16818.506445, global_step=52572, preemption_count=0, score=16818.506445, test/accuracy=0.985518, test/loss=0.052332, test/mean_average_precision=0.260131, test/num_examples=43793, total_duration=24583.909498, train/accuracy=0.993703, train/loss=0.019763, train/mean_average_precision=0.638497, validation/accuracy=0.986481, validation/loss=0.048968, validation/mean_average_precision=0.276152, validation/num_examples=43793
I0206 02:10:27.315503 139789425600256 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.22058998048305511, loss=0.022902607917785645
I0206 02:10:59.209369 139788813231872 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1866830289363861, loss=0.018968217074871063
I0206 02:11:30.982275 139789425600256 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.17204740643501282, loss=0.019489388912916183
I0206 02:12:02.631576 139788813231872 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.20409922301769257, loss=0.022447016090154648
I0206 02:12:34.513571 139789425600256 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.18352410197257996, loss=0.01982179284095764
I0206 02:13:06.475538 139788813231872 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.18659712374210358, loss=0.02057168260216713
I0206 02:13:38.696682 139789425600256 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.18194471299648285, loss=0.019319523125886917
I0206 02:14:10.999143 139788813231872 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2565826177597046, loss=0.025085996836423874
I0206 02:14:18.230883 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:16:00.680778 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:16:03.775023 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:16:06.738116 139978932307776 submission_runner.py:408] Time since start: 24932.68s, 	Step: 53324, 	{'train/accuracy': 0.993492603302002, 'train/loss': 0.020317774266004562, 'train/mean_average_precision': 0.6140696882882921, 'validation/accuracy': 0.9865548014640808, 'validation/loss': 0.048930615186691284, 'validation/mean_average_precision': 0.28487371778432663, 'validation/num_examples': 43793, 'test/accuracy': 0.9857269525527954, 'test/loss': 0.05243641138076782, 'test/mean_average_precision': 0.26731853432521185, 'test/num_examples': 43793, 'score': 17058.707488775253, 'total_duration': 24932.676094055176, 'accumulated_submission_time': 17058.707488775253, 'accumulated_eval_time': 7869.87908744812, 'accumulated_logging_time': 2.6299984455108643}
I0206 02:16:06.764461 139770806105856 logging_writer.py:48] [53324] accumulated_eval_time=7869.879087, accumulated_logging_time=2.629998, accumulated_submission_time=17058.707489, global_step=53324, preemption_count=0, score=17058.707489, test/accuracy=0.985727, test/loss=0.052436, test/mean_average_precision=0.267319, test/num_examples=43793, total_duration=24932.676094, train/accuracy=0.993493, train/loss=0.020318, train/mean_average_precision=0.614070, validation/accuracy=0.986555, validation/loss=0.048931, validation/mean_average_precision=0.284874, validation/num_examples=43793
I0206 02:16:31.239745 139806663034624 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.17803016304969788, loss=0.019008694216609
I0206 02:17:02.933685 139770806105856 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20382268726825714, loss=0.019269512966275215
I0206 02:17:34.725688 139806663034624 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.20606613159179688, loss=0.020211583003401756
I0206 02:18:06.629327 139770806105856 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1945357471704483, loss=0.02056214027106762
I0206 02:18:38.167500 139806663034624 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.17609021067619324, loss=0.018785422667860985
I0206 02:19:10.120613 139770806105856 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.19446593523025513, loss=0.019976653158664703
I0206 02:19:41.859871 139806663034624 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.20090296864509583, loss=0.021649807691574097
I0206 02:20:06.847537 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:21:47.672282 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:21:50.642605 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:21:53.614426 139978932307776 submission_runner.py:408] Time since start: 25279.55s, 	Step: 54079, 	{'train/accuracy': 0.9934467077255249, 'train/loss': 0.020390227437019348, 'train/mean_average_precision': 0.6370068629099701, 'validation/accuracy': 0.9865905046463013, 'validation/loss': 0.04956473410129547, 'validation/mean_average_precision': 0.2744399979478792, 'validation/num_examples': 43793, 'test/accuracy': 0.985632598400116, 'test/loss': 0.053322236984968185, 'test/mean_average_precision': 0.2601269322421878, 'test/num_examples': 43793, 'score': 17298.7577586174, 'total_duration': 25279.552406549454, 'accumulated_submission_time': 17298.7577586174, 'accumulated_eval_time': 7976.645930767059, 'accumulated_logging_time': 2.6677396297454834}
I0206 02:21:53.642735 139770797713152 logging_writer.py:48] [54079] accumulated_eval_time=7976.645931, accumulated_logging_time=2.667740, accumulated_submission_time=17298.757759, global_step=54079, preemption_count=0, score=17298.757759, test/accuracy=0.985633, test/loss=0.053322, test/mean_average_precision=0.260127, test/num_examples=43793, total_duration=25279.552407, train/accuracy=0.993447, train/loss=0.020390, train/mean_average_precision=0.637007, validation/accuracy=0.986591, validation/loss=0.049565, validation/mean_average_precision=0.274440, validation/num_examples=43793
I0206 02:22:00.656366 139788813231872 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1940140426158905, loss=0.019671378657221794
I0206 02:22:32.855123 139770797713152 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.20368386805057526, loss=0.019938409328460693
I0206 02:23:04.779838 139788813231872 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2155505120754242, loss=0.02043125033378601
I0206 02:23:36.139718 139770797713152 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.19378243386745453, loss=0.019483890384435654
I0206 02:24:07.974760 139788813231872 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.1792910397052765, loss=0.019278528168797493
I0206 02:24:39.789457 139770797713152 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.19973307847976685, loss=0.018142633140087128
I0206 02:25:11.368208 139788813231872 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.25223904848098755, loss=0.021703477948904037
I0206 02:25:43.387149 139770797713152 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.19299453496932983, loss=0.018231049180030823
I0206 02:25:53.675121 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:27:34.806954 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:27:37.937998 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:27:40.967458 139978932307776 submission_runner.py:408] Time since start: 25626.91s, 	Step: 54833, 	{'train/accuracy': 0.9934847354888916, 'train/loss': 0.020191708579659462, 'train/mean_average_precision': 0.6408067088189375, 'validation/accuracy': 0.9866063594818115, 'validation/loss': 0.05004741996526718, 'validation/mean_average_precision': 0.2746520933144069, 'validation/num_examples': 43793, 'test/accuracy': 0.9855727553367615, 'test/loss': 0.05376891791820526, 'test/mean_average_precision': 0.26042919967582107, 'test/num_examples': 43793, 'score': 17538.758221387863, 'total_duration': 25626.905437469482, 'accumulated_submission_time': 17538.758221387863, 'accumulated_eval_time': 8083.9382219314575, 'accumulated_logging_time': 2.706973075866699}
I0206 02:27:40.993328 139789425600256 logging_writer.py:48] [54833] accumulated_eval_time=8083.938222, accumulated_logging_time=2.706973, accumulated_submission_time=17538.758221, global_step=54833, preemption_count=0, score=17538.758221, test/accuracy=0.985573, test/loss=0.053769, test/mean_average_precision=0.260429, test/num_examples=43793, total_duration=25626.905437, train/accuracy=0.993485, train/loss=0.020192, train/mean_average_precision=0.640807, validation/accuracy=0.986606, validation/loss=0.050047, validation/mean_average_precision=0.274652, validation/num_examples=43793
I0206 02:28:03.094578 139806663034624 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.22145047783851624, loss=0.020221460610628128
I0206 02:28:35.261172 139789425600256 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.20820549130439758, loss=0.018343593925237656
I0206 02:29:07.087276 139806663034624 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.1977139413356781, loss=0.019904933869838715
I0206 02:29:38.909437 139789425600256 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.1834287792444229, loss=0.01801345683634281
I0206 02:30:10.928284 139806663034624 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.22689592838287354, loss=0.02029784582555294
I0206 02:30:42.700357 139789425600256 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.22260361909866333, loss=0.02066713385283947
I0206 02:31:14.694783 139806663034624 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.18032705783843994, loss=0.01772223971784115
I0206 02:31:41.174206 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:33:22.954405 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:33:26.102278 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:33:29.184912 139978932307776 submission_runner.py:408] Time since start: 25975.12s, 	Step: 55585, 	{'train/accuracy': 0.9934579133987427, 'train/loss': 0.020221708342432976, 'train/mean_average_precision': 0.6336687988840867, 'validation/accuracy': 0.9862795472145081, 'validation/loss': 0.050230368971824646, 'validation/mean_average_precision': 0.2801965927562672, 'validation/num_examples': 43793, 'test/accuracy': 0.9853209257125854, 'test/loss': 0.05393723025918007, 'test/mean_average_precision': 0.257986693851216, 'test/num_examples': 43793, 'score': 17778.90761089325, 'total_duration': 25975.122877836227, 'accumulated_submission_time': 17778.90761089325, 'accumulated_eval_time': 8191.948867797852, 'accumulated_logging_time': 2.743889093399048}
I0206 02:33:29.210734 139770806105856 logging_writer.py:48] [55585] accumulated_eval_time=8191.948868, accumulated_logging_time=2.743889, accumulated_submission_time=17778.907611, global_step=55585, preemption_count=0, score=17778.907611, test/accuracy=0.985321, test/loss=0.053937, test/mean_average_precision=0.257987, test/num_examples=43793, total_duration=25975.122878, train/accuracy=0.993458, train/loss=0.020222, train/mean_average_precision=0.633669, validation/accuracy=0.986280, validation/loss=0.050230, validation/mean_average_precision=0.280197, validation/num_examples=43793
I0206 02:33:34.296082 139788813231872 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.22030700743198395, loss=0.01825757883489132
I0206 02:34:05.925096 139770806105856 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19874873757362366, loss=0.018124505877494812
I0206 02:34:37.394213 139788813231872 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2150854617357254, loss=0.0187537744641304
I0206 02:35:09.116734 139770806105856 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.19191023707389832, loss=0.01790282130241394
I0206 02:35:40.504926 139788813231872 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.17254526913166046, loss=0.016060492023825645
I0206 02:36:12.262218 139770806105856 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.22015385329723358, loss=0.01966754160821438
I0206 02:36:43.957656 139788813231872 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.21396313607692719, loss=0.01944168470799923
I0206 02:37:15.569638 139770806105856 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.23158815503120422, loss=0.01914515160024166
I0206 02:37:29.442376 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:39:08.502769 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:39:11.624912 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:39:14.629844 139978932307776 submission_runner.py:408] Time since start: 26320.57s, 	Step: 56345, 	{'train/accuracy': 0.9935694336891174, 'train/loss': 0.019636856392025948, 'train/mean_average_precision': 0.6439595382453188, 'validation/accuracy': 0.9865819811820984, 'validation/loss': 0.05091746151447296, 'validation/mean_average_precision': 0.27855193216330126, 'validation/num_examples': 43793, 'test/accuracy': 0.9856279492378235, 'test/loss': 0.0548713281750679, 'test/mean_average_precision': 0.26244628027064826, 'test/num_examples': 43793, 'score': 18019.107362270355, 'total_duration': 26320.5678255558, 'accumulated_submission_time': 18019.107362270355, 'accumulated_eval_time': 8297.136292934418, 'accumulated_logging_time': 2.7808399200439453}
I0206 02:39:14.655844 139770797713152 logging_writer.py:48] [56345] accumulated_eval_time=8297.136293, accumulated_logging_time=2.780840, accumulated_submission_time=18019.107362, global_step=56345, preemption_count=0, score=18019.107362, test/accuracy=0.985628, test/loss=0.054871, test/mean_average_precision=0.262446, test/num_examples=43793, total_duration=26320.567826, train/accuracy=0.993569, train/loss=0.019637, train/mean_average_precision=0.643960, validation/accuracy=0.986582, validation/loss=0.050917, validation/mean_average_precision=0.278552, validation/num_examples=43793
I0206 02:39:32.626846 139789425600256 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2106269896030426, loss=0.018449978902935982
I0206 02:40:04.489697 139770797713152 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.22793973982334137, loss=0.01848563179373741
I0206 02:40:36.332889 139789425600256 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.18544305860996246, loss=0.01729474402964115
I0206 02:41:08.157928 139770797713152 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.21010266244411469, loss=0.018243253231048584
I0206 02:41:40.158140 139789425600256 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.22858048975467682, loss=0.018585151061415672
I0206 02:42:12.379014 139770797713152 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.20755262672901154, loss=0.016250303015112877
I0206 02:42:44.329524 139789425600256 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21133355796337128, loss=0.020728081464767456
I0206 02:43:14.637550 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:44:58.619835 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:45:01.744765 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:45:04.727985 139978932307776 submission_runner.py:408] Time since start: 26670.67s, 	Step: 57096, 	{'train/accuracy': 0.9941542148590088, 'train/loss': 0.017990222200751305, 'train/mean_average_precision': 0.6795010598582831, 'validation/accuracy': 0.9863534569740295, 'validation/loss': 0.051240529865026474, 'validation/mean_average_precision': 0.2821614791012339, 'validation/num_examples': 43793, 'test/accuracy': 0.985390841960907, 'test/loss': 0.05514898896217346, 'test/mean_average_precision': 0.25607921974810816, 'test/num_examples': 43793, 'score': 18259.057409524918, 'total_duration': 26670.66596722603, 'accumulated_submission_time': 18259.057409524918, 'accumulated_eval_time': 8407.226685523987, 'accumulated_logging_time': 2.8178892135620117}
I0206 02:45:04.753719 139770806105856 logging_writer.py:48] [57096] accumulated_eval_time=8407.226686, accumulated_logging_time=2.817889, accumulated_submission_time=18259.057410, global_step=57096, preemption_count=0, score=18259.057410, test/accuracy=0.985391, test/loss=0.055149, test/mean_average_precision=0.256079, test/num_examples=43793, total_duration=26670.665967, train/accuracy=0.994154, train/loss=0.017990, train/mean_average_precision=0.679501, validation/accuracy=0.986353, validation/loss=0.051241, validation/mean_average_precision=0.282161, validation/num_examples=43793
I0206 02:45:06.424250 139806663034624 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.23258869349956512, loss=0.019246196374297142
I0206 02:45:37.987997 139770806105856 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2223508059978485, loss=0.019482804462313652
I0206 02:46:10.115449 139806663034624 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.2165786474943161, loss=0.01788577251136303
I0206 02:46:41.657081 139770806105856 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19985833764076233, loss=0.01776624470949173
I0206 02:47:13.700409 139806663034624 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.236248180270195, loss=0.017845788970589638
I0206 02:47:45.358433 139770806105856 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.2463662326335907, loss=0.01902170479297638
I0206 02:48:17.424695 139806663034624 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.24615563452243805, loss=0.020089441910386086
I0206 02:48:42.751776 139770806105856 logging_writer.py:48] [57780] global_step=57780, preemption_count=0, score=18477.008486
I0206 02:48:42.803023 139978932307776 checkpoints.py:490] Saving checkpoint at step: 57780
I0206 02:48:42.967826 139978932307776 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3/checkpoint_57780
I0206 02:48:42.970808 139978932307776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_3/checkpoint_57780.
I0206 02:48:43.155057 139978932307776 submission_runner.py:583] Tuning trial 3/5
I0206 02:48:43.155284 139978932307776 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0206 02:48:43.160434 139978932307776 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.3884694278240204, 'train/loss': 0.7994099855422974, 'train/mean_average_precision': 0.021870049108438757, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02483726344495895, 'validation/num_examples': 43793, 'test/accuracy': 0.3947484791278839, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026385459559166446, 'test/num_examples': 43793, 'score': 11.536062002182007, 'total_duration': 123.94088912010193, 'accumulated_submission_time': 11.536062002182007, 'accumulated_eval_time': 112.40478038787842, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (752, {'train/accuracy': 0.986751914024353, 'train/loss': 0.07101087272167206, 'train/mean_average_precision': 0.03521220543918937, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07898423075675964, 'validation/mean_average_precision': 0.038268597202817944, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08169642835855484, 'test/mean_average_precision': 0.039718402530105536, 'test/num_examples': 43793, 'score': 251.66132426261902, 'total_duration': 476.00432682037354, 'accumulated_submission_time': 251.66132426261902, 'accumulated_eval_time': 224.29861760139465, 'accumulated_logging_time': 0.024303913116455078, 'global_step': 752, 'preemption_count': 0}), (1500, {'train/accuracy': 0.9871714115142822, 'train/loss': 0.04863082617521286, 'train/mean_average_precision': 0.08834327833907998, 'validation/accuracy': 0.9846265912055969, 'validation/loss': 0.057549361139535904, 'validation/mean_average_precision': 0.09029757624371303, 'validation/num_examples': 43793, 'test/accuracy': 0.9836398959159851, 'test/loss': 0.06086340546607971, 'test/mean_average_precision': 0.08853657797377494, 'test/num_examples': 43793, 'score': 491.7242970466614, 'total_duration': 831.7605443000793, 'accumulated_submission_time': 491.7242970466614, 'accumulated_eval_time': 339.94337153434753, 'accumulated_logging_time': 0.05216574668884277, 'global_step': 1500, 'preemption_count': 0}), (2248, {'train/accuracy': 0.9878830909729004, 'train/loss': 0.04364940524101257, 'train/mean_average_precision': 0.13569135498524446, 'validation/accuracy': 0.9850901365280151, 'validation/loss': 0.052616462111473083, 'validation/mean_average_precision': 0.1338274383866124, 'validation/num_examples': 43793, 'test/accuracy': 0.9841011166572571, 'test/loss': 0.055503834038972855, 'test/mean_average_precision': 0.13269239460812846, 'test/num_examples': 43793, 'score': 731.9238619804382, 'total_duration': 1183.0322844982147, 'accumulated_submission_time': 731.9238619804382, 'accumulated_eval_time': 450.96800208091736, 'accumulated_logging_time': 0.07912993431091309, 'global_step': 2248, 'preemption_count': 0}), (2995, {'train/accuracy': 0.9882432818412781, 'train/loss': 0.04136427119374275, 'train/mean_average_precision': 0.16459796593907855, 'validation/accuracy': 0.9853106141090393, 'validation/loss': 0.051228564232587814, 'validation/mean_average_precision': 0.16076437255622866, 'validation/num_examples': 43793, 'test/accuracy': 0.9844043850898743, 'test/loss': 0.054126448929309845, 'test/mean_average_precision': 0.15506180067662756, 'test/num_examples': 43793, 'score': 972.1416659355164, 'total_duration': 1531.3750817775726, 'accumulated_submission_time': 972.1416659355164, 'accumulated_eval_time': 559.0434498786926, 'accumulated_logging_time': 0.1082615852355957, 'global_step': 2995, 'preemption_count': 0}), (3746, {'train/accuracy': 0.9883965849876404, 'train/loss': 0.04021444916725159, 'train/mean_average_precision': 0.18359365003802192, 'validation/accuracy': 0.9854680895805359, 'validation/loss': 0.05016530677676201, 'validation/mean_average_precision': 0.16619727048449834, 'validation/num_examples': 43793, 'test/accuracy': 0.984574556350708, 'test/loss': 0.052942439913749695, 'test/mean_average_precision': 0.1661460566473754, 'test/num_examples': 43793, 'score': 1212.3301203250885, 'total_duration': 1882.2137567996979, 'accumulated_submission_time': 1212.3301203250885, 'accumulated_eval_time': 669.6453518867493, 'accumulated_logging_time': 0.13631319999694824, 'global_step': 3746, 'preemption_count': 0}), (4495, {'train/accuracy': 0.9884520173072815, 'train/loss': 0.03975112736225128, 'train/mean_average_precision': 0.20363035451384381, 'validation/accuracy': 0.9856345057487488, 'validation/loss': 0.049419745802879333, 'validation/mean_average_precision': 0.18314421182517218, 'validation/num_examples': 43793, 'test/accuracy': 0.9847337603569031, 'test/loss': 0.05198296532034874, 'test/mean_average_precision': 0.18370312799845293, 'test/num_examples': 43793, 'score': 1452.4844810962677, 'total_duration': 2236.3710923194885, 'accumulated_submission_time': 1452.4844810962677, 'accumulated_eval_time': 783.5995593070984, 'accumulated_logging_time': 0.16430401802062988, 'global_step': 4495, 'preemption_count': 0}), (5238, {'train/accuracy': 0.9888370633125305, 'train/loss': 0.038162074983119965, 'train/mean_average_precision': 0.22670998958535876, 'validation/accuracy': 0.9859211444854736, 'validation/loss': 0.04755629971623421, 'validation/mean_average_precision': 0.19909767789009397, 'validation/num_examples': 43793, 'test/accuracy': 0.9850395321846008, 'test/loss': 0.0501854233443737, 'test/mean_average_precision': 0.1966521375545678, 'test/num_examples': 43793, 'score': 1692.6457846164703, 'total_duration': 2591.0650255680084, 'accumulated_submission_time': 1692.6457846164703, 'accumulated_eval_time': 898.0833630561829, 'accumulated_logging_time': 0.19303679466247559, 'global_step': 5238, 'preemption_count': 0}), (5982, {'train/accuracy': 0.9888089299201965, 'train/loss': 0.03777968883514404, 'train/mean_average_precision': 0.23499009768661672, 'validation/accuracy': 0.9859194755554199, 'validation/loss': 0.047309760004282, 'validation/mean_average_precision': 0.20191103284607342, 'validation/num_examples': 43793, 'test/accuracy': 0.9850606322288513, 'test/loss': 0.049863848835229874, 'test/mean_average_precision': 0.2016183392127283, 'test/num_examples': 43793, 'score': 1932.7427368164062, 'total_duration': 2946.2279620170593, 'accumulated_submission_time': 1932.7427368164062, 'accumulated_eval_time': 1013.1005027294159, 'accumulated_logging_time': 0.22141695022583008, 'global_step': 5982, 'preemption_count': 0}), (6722, {'train/accuracy': 0.9892280697822571, 'train/loss': 0.03680482134222984, 'train/mean_average_precision': 0.27369930332534403, 'validation/accuracy': 0.9860668778419495, 'validation/loss': 0.046758756041526794, 'validation/mean_average_precision': 0.21270309188905026, 'validation/num_examples': 43793, 'test/accuracy': 0.9852674007415771, 'test/loss': 0.04913114756345749, 'test/mean_average_precision': 0.2183474994479768, 'test/num_examples': 43793, 'score': 2172.955377101898, 'total_duration': 3298.24889087677, 'accumulated_submission_time': 2172.955377101898, 'accumulated_eval_time': 1124.8570773601532, 'accumulated_logging_time': 0.2525761127471924, 'global_step': 6722, 'preemption_count': 0}), (7469, {'train/accuracy': 0.9891047477722168, 'train/loss': 0.036559801548719406, 'train/mean_average_precision': 0.26471269146596876, 'validation/accuracy': 0.9860149025917053, 'validation/loss': 0.047384802252054214, 'validation/mean_average_precision': 0.21523215164863765, 'validation/num_examples': 43793, 'test/accuracy': 0.9851044416427612, 'test/loss': 0.05031104013323784, 'test/mean_average_precision': 0.20823299610528184, 'test/num_examples': 43793, 'score': 2413.086009979248, 'total_duration': 3652.7268018722534, 'accumulated_submission_time': 2413.086009979248, 'accumulated_eval_time': 1239.154440164566, 'accumulated_logging_time': 0.28229618072509766, 'global_step': 7469, 'preemption_count': 0}), (8209, {'train/accuracy': 0.9893330335617065, 'train/loss': 0.035949498414993286, 'train/mean_average_precision': 0.2700580912185201, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.046453479677438736, 'validation/mean_average_precision': 0.22399626963309754, 'validation/num_examples': 43793, 'test/accuracy': 0.9853959083557129, 'test/loss': 0.049026843160390854, 'test/mean_average_precision': 0.21997002438227886, 'test/num_examples': 43793, 'score': 2653.0507991313934, 'total_duration': 4003.8033468723297, 'accumulated_submission_time': 2653.0507991313934, 'accumulated_eval_time': 1350.2171156406403, 'accumulated_logging_time': 0.3105733394622803, 'global_step': 8209, 'preemption_count': 0}), (8959, {'train/accuracy': 0.989347517490387, 'train/loss': 0.03583766147494316, 'train/mean_average_precision': 0.2764391488246033, 'validation/accuracy': 0.9863911867141724, 'validation/loss': 0.04540354013442993, 'validation/mean_average_precision': 0.22955552490979517, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.0479598231613636, 'test/mean_average_precision': 0.23206196939903595, 'test/num_examples': 43793, 'score': 2893.2927191257477, 'total_duration': 4359.108901500702, 'accumulated_submission_time': 2893.2927191257477, 'accumulated_eval_time': 1465.2301201820374, 'accumulated_logging_time': 0.34090471267700195, 'global_step': 8959, 'preemption_count': 0}), (9715, {'train/accuracy': 0.9895155429840088, 'train/loss': 0.035268500447273254, 'train/mean_average_precision': 0.2899146334264587, 'validation/accuracy': 0.9863510131835938, 'validation/loss': 0.04599665105342865, 'validation/mean_average_precision': 0.22869382715933165, 'validation/num_examples': 43793, 'test/accuracy': 0.9855496287345886, 'test/loss': 0.04855114594101906, 'test/mean_average_precision': 0.22784956893426864, 'test/num_examples': 43793, 'score': 3133.352974653244, 'total_duration': 4711.366160392761, 'accumulated_submission_time': 3133.352974653244, 'accumulated_eval_time': 1577.3784453868866, 'accumulated_logging_time': 0.36896514892578125, 'global_step': 9715, 'preemption_count': 0}), (10451, {'train/accuracy': 0.9897177219390869, 'train/loss': 0.03448662906885147, 'train/mean_average_precision': 0.31379166194335967, 'validation/accuracy': 0.9864187836647034, 'validation/loss': 0.04568442702293396, 'validation/mean_average_precision': 0.2386852578627625, 'validation/num_examples': 43793, 'test/accuracy': 0.9855100512504578, 'test/loss': 0.048339344561100006, 'test/mean_average_precision': 0.23221452658596023, 'test/num_examples': 43793, 'score': 3373.5377974510193, 'total_duration': 5066.831092119217, 'accumulated_submission_time': 3373.5377974510193, 'accumulated_eval_time': 1692.609769821167, 'accumulated_logging_time': 0.397641658782959, 'global_step': 10451, 'preemption_count': 0}), (11196, {'train/accuracy': 0.9898854494094849, 'train/loss': 0.03352813422679901, 'train/mean_average_precision': 0.32871810873388474, 'validation/accuracy': 0.9863871335983276, 'validation/loss': 0.04538214951753616, 'validation/mean_average_precision': 0.24170799077011787, 'validation/num_examples': 43793, 'test/accuracy': 0.9856237769126892, 'test/loss': 0.04807046800851822, 'test/mean_average_precision': 0.24482837398800203, 'test/num_examples': 43793, 'score': 3613.5333302021027, 'total_duration': 5415.48432302475, 'accumulated_submission_time': 3613.5333302021027, 'accumulated_eval_time': 1801.215446472168, 'accumulated_logging_time': 0.4285142421722412, 'global_step': 11196, 'preemption_count': 0}), (11934, {'train/accuracy': 0.9900333285331726, 'train/loss': 0.03320500999689102, 'train/mean_average_precision': 0.3330144534538547, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.04528070241212845, 'validation/mean_average_precision': 0.2396226332273137, 'validation/num_examples': 43793, 'test/accuracy': 0.9855976104736328, 'test/loss': 0.04800181835889816, 'test/mean_average_precision': 0.232307527215203, 'test/num_examples': 43793, 'score': 3853.5349090099335, 'total_duration': 5768.89560508728, 'accumulated_submission_time': 3853.5349090099335, 'accumulated_eval_time': 1914.5718655586243, 'accumulated_logging_time': 0.4614851474761963, 'global_step': 11934, 'preemption_count': 0}), (12685, {'train/accuracy': 0.9902761578559875, 'train/loss': 0.03202471509575844, 'train/mean_average_precision': 0.37875461384207076, 'validation/accuracy': 0.9864386916160583, 'validation/loss': 0.04516429826617241, 'validation/mean_average_precision': 0.25107617938919036, 'validation/num_examples': 43793, 'test/accuracy': 0.9856582880020142, 'test/loss': 0.04794519394636154, 'test/mean_average_precision': 0.24156832028636196, 'test/num_examples': 43793, 'score': 4093.756762266159, 'total_duration': 6117.430259227753, 'accumulated_submission_time': 4093.756762266159, 'accumulated_eval_time': 2022.8337025642395, 'accumulated_logging_time': 0.4917325973510742, 'global_step': 12685, 'preemption_count': 0}), (13433, {'train/accuracy': 0.9904384613037109, 'train/loss': 0.03149892017245293, 'train/mean_average_precision': 0.3717057181802991, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.0446271151304245, 'validation/mean_average_precision': 0.24721268033636373, 'validation/num_examples': 43793, 'test/accuracy': 0.9858389496803284, 'test/loss': 0.04734957590699196, 'test/mean_average_precision': 0.24626384470574914, 'test/num_examples': 43793, 'score': 4333.850111484528, 'total_duration': 6469.4553780555725, 'accumulated_submission_time': 4333.850111484528, 'accumulated_eval_time': 2134.716328382492, 'accumulated_logging_time': 0.5204606056213379, 'global_step': 13433, 'preemption_count': 0}), (14178, {'train/accuracy': 0.9904738664627075, 'train/loss': 0.03142843395471573, 'train/mean_average_precision': 0.37346244724380895, 'validation/accuracy': 0.9864821434020996, 'validation/loss': 0.04513578861951828, 'validation/mean_average_precision': 0.2507288477370744, 'validation/num_examples': 43793, 'test/accuracy': 0.9856254458427429, 'test/loss': 0.04799942299723625, 'test/mean_average_precision': 0.25043207856634864, 'test/num_examples': 43793, 'score': 4573.89826631546, 'total_duration': 6820.58492398262, 'accumulated_submission_time': 4573.89826631546, 'accumulated_eval_time': 2245.7474250793457, 'accumulated_logging_time': 0.549689769744873, 'global_step': 14178, 'preemption_count': 0}), (14921, {'train/accuracy': 0.9905796051025391, 'train/loss': 0.031162654981017113, 'train/mean_average_precision': 0.3785872396635164, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.0440506637096405, 'validation/mean_average_precision': 0.2621364522054176, 'validation/num_examples': 43793, 'test/accuracy': 0.9858343601226807, 'test/loss': 0.0468188151717186, 'test/mean_average_precision': 0.2576561377273499, 'test/num_examples': 43793, 'score': 4813.926500082016, 'total_duration': 7170.563362598419, 'accumulated_submission_time': 4813.926500082016, 'accumulated_eval_time': 2355.6457736492157, 'accumulated_logging_time': 0.5813260078430176, 'global_step': 14921, 'preemption_count': 0}), (15673, {'train/accuracy': 0.9905015826225281, 'train/loss': 0.031514354050159454, 'train/mean_average_precision': 0.37039730201168186, 'validation/accuracy': 0.986763060092926, 'validation/loss': 0.044145770370960236, 'validation/mean_average_precision': 0.26115744996768614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.04671604931354523, 'test/mean_average_precision': 0.25491086610496483, 'test/num_examples': 43793, 'score': 5053.890505075455, 'total_duration': 7518.589793205261, 'accumulated_submission_time': 5053.890505075455, 'accumulated_eval_time': 2463.6573424339294, 'accumulated_logging_time': 0.611168384552002, 'global_step': 15673, 'preemption_count': 0}), (16426, {'train/accuracy': 0.9906003475189209, 'train/loss': 0.031076526269316673, 'train/mean_average_precision': 0.39021054358474716, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.04423775523900986, 'validation/mean_average_precision': 0.2586473980508823, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.046707622706890106, 'test/mean_average_precision': 0.2628098371482614, 'test/num_examples': 43793, 'score': 5293.863883733749, 'total_duration': 7869.176714420319, 'accumulated_submission_time': 5293.863883733749, 'accumulated_eval_time': 2574.2214167118073, 'accumulated_logging_time': 0.6401748657226562, 'global_step': 16426, 'preemption_count': 0}), (17178, {'train/accuracy': 0.9905943274497986, 'train/loss': 0.030888676643371582, 'train/mean_average_precision': 0.3930183406771667, 'validation/accuracy': 0.9867305755615234, 'validation/loss': 0.04413682594895363, 'validation/mean_average_precision': 0.2651857368659311, 'validation/num_examples': 43793, 'test/accuracy': 0.9858617186546326, 'test/loss': 0.0468544140458107, 'test/mean_average_precision': 0.25534591552830505, 'test/num_examples': 43793, 'score': 5533.909998893738, 'total_duration': 8222.222283363342, 'accumulated_submission_time': 5533.909998893738, 'accumulated_eval_time': 2687.1701986789703, 'accumulated_logging_time': 0.6697847843170166, 'global_step': 17178, 'preemption_count': 0}), (17930, {'train/accuracy': 0.9906186461448669, 'train/loss': 0.030826617032289505, 'train/mean_average_precision': 0.3925631480101053, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.0444091372191906, 'validation/mean_average_precision': 0.2627921188302755, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.04698709771037102, 'test/mean_average_precision': 0.2571229912207292, 'test/num_examples': 43793, 'score': 5773.913890838623, 'total_duration': 8573.608848333359, 'accumulated_submission_time': 5773.913890838623, 'accumulated_eval_time': 2798.5026183128357, 'accumulated_logging_time': 0.699364423751831, 'global_step': 17930, 'preemption_count': 0}), (18681, {'train/accuracy': 0.9908760190010071, 'train/loss': 0.03000824525952339, 'train/mean_average_precision': 0.4102506283303613, 'validation/accuracy': 0.986647367477417, 'validation/loss': 0.044502776116132736, 'validation/mean_average_precision': 0.26556058880960226, 'validation/num_examples': 43793, 'test/accuracy': 0.9858953952789307, 'test/loss': 0.0472092404961586, 'test/mean_average_precision': 0.25721369249162757, 'test/num_examples': 43793, 'score': 6013.996770858765, 'total_duration': 8919.025218486786, 'accumulated_submission_time': 6013.996770858765, 'accumulated_eval_time': 2903.7837584018707, 'accumulated_logging_time': 0.73091721534729, 'global_step': 18681, 'preemption_count': 0}), (19432, {'train/accuracy': 0.9908602237701416, 'train/loss': 0.029808955267071724, 'train/mean_average_precision': 0.42696041075451996, 'validation/accuracy': 0.9866859316825867, 'validation/loss': 0.04467317834496498, 'validation/mean_average_precision': 0.2662054523909674, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.047456517815589905, 'test/mean_average_precision': 0.2520375820148166, 'test/num_examples': 43793, 'score': 6254.042366743088, 'total_duration': 9274.63326048851, 'accumulated_submission_time': 6254.042366743088, 'accumulated_eval_time': 3019.294206380844, 'accumulated_logging_time': 0.7621853351593018, 'global_step': 19432, 'preemption_count': 0}), (20175, {'train/accuracy': 0.9910188317298889, 'train/loss': 0.028977325186133385, 'train/mean_average_precision': 0.4231962341096236, 'validation/accuracy': 0.9867512583732605, 'validation/loss': 0.044515322893857956, 'validation/mean_average_precision': 0.26375224926940755, 'validation/num_examples': 43793, 'test/accuracy': 0.9858208894729614, 'test/loss': 0.0472513772547245, 'test/mean_average_precision': 0.2518622757509492, 'test/num_examples': 43793, 'score': 6494.2784950733185, 'total_duration': 9624.445745706558, 'accumulated_submission_time': 6494.2784950733185, 'accumulated_eval_time': 3128.8182978630066, 'accumulated_logging_time': 0.7937026023864746, 'global_step': 20175, 'preemption_count': 0}), (20920, {'train/accuracy': 0.9911974668502808, 'train/loss': 0.02875051088631153, 'train/mean_average_precision': 0.43925762858152084, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.04468398541212082, 'validation/mean_average_precision': 0.25641154136754146, 'validation/num_examples': 43793, 'test/accuracy': 0.9857374429702759, 'test/loss': 0.047172099351882935, 'test/mean_average_precision': 0.2560707943968566, 'test/num_examples': 43793, 'score': 6734.232186079025, 'total_duration': 9976.325129032135, 'accumulated_submission_time': 6734.232186079025, 'accumulated_eval_time': 3240.6934225559235, 'accumulated_logging_time': 0.8236665725708008, 'global_step': 20920, 'preemption_count': 0}), (21667, {'train/accuracy': 0.9909592270851135, 'train/loss': 0.029395489022135735, 'train/mean_average_precision': 0.4405538692550107, 'validation/accuracy': 0.9868373274803162, 'validation/loss': 0.04413104057312012, 'validation/mean_average_precision': 0.2741253771401372, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.046785593032836914, 'test/mean_average_precision': 0.25911216191366065, 'test/num_examples': 43793, 'score': 6974.358366250992, 'total_duration': 10324.099648237228, 'accumulated_submission_time': 6974.358366250992, 'accumulated_eval_time': 3348.290340423584, 'accumulated_logging_time': 0.854386568069458, 'global_step': 21667, 'preemption_count': 0}), (22414, {'train/accuracy': 0.9908499121665955, 'train/loss': 0.029780704528093338, 'train/mean_average_precision': 0.4160635084609168, 'validation/accuracy': 0.9868572354316711, 'validation/loss': 0.04488062486052513, 'validation/mean_average_precision': 0.2624296437397281, 'validation/num_examples': 43793, 'test/accuracy': 0.9860116839408875, 'test/loss': 0.04767533391714096, 'test/mean_average_precision': 0.25534959644604904, 'test/num_examples': 43793, 'score': 7214.402206897736, 'total_duration': 10671.395725727081, 'accumulated_submission_time': 7214.402206897736, 'accumulated_eval_time': 3455.4900765419006, 'accumulated_logging_time': 0.8864257335662842, 'global_step': 22414, 'preemption_count': 0}), (23167, {'train/accuracy': 0.9909346103668213, 'train/loss': 0.029603630304336548, 'train/mean_average_precision': 0.41999051149073074, 'validation/accuracy': 0.9866583347320557, 'validation/loss': 0.04457014426589012, 'validation/mean_average_precision': 0.2730043897985921, 'validation/num_examples': 43793, 'test/accuracy': 0.9858132600784302, 'test/loss': 0.04735338315367699, 'test/mean_average_precision': 0.2534655795057701, 'test/num_examples': 43793, 'score': 7454.508299827576, 'total_duration': 11025.55727314949, 'accumulated_submission_time': 7454.508299827576, 'accumulated_eval_time': 3569.4941403865814, 'accumulated_logging_time': 0.9170982837677002, 'global_step': 23167, 'preemption_count': 0}), (23921, {'train/accuracy': 0.9909778833389282, 'train/loss': 0.029523272067308426, 'train/mean_average_precision': 0.4255365867503188, 'validation/accuracy': 0.9867942929267883, 'validation/loss': 0.04464536905288696, 'validation/mean_average_precision': 0.2707242531647493, 'validation/num_examples': 43793, 'test/accuracy': 0.9859514236450195, 'test/loss': 0.04751303046941757, 'test/mean_average_precision': 0.2569773068587559, 'test/num_examples': 43793, 'score': 7694.484705686569, 'total_duration': 11374.190093517303, 'accumulated_submission_time': 7694.484705686569, 'accumulated_eval_time': 3678.099539041519, 'accumulated_logging_time': 0.9474050998687744, 'global_step': 23921, 'preemption_count': 0}), (24666, {'train/accuracy': 0.9910315871238708, 'train/loss': 0.029066704213619232, 'train/mean_average_precision': 0.43930651407631194, 'validation/accuracy': 0.9868904948234558, 'validation/loss': 0.044614922255277634, 'validation/mean_average_precision': 0.2751389474181981, 'validation/num_examples': 43793, 'test/accuracy': 0.9860731363296509, 'test/loss': 0.04757541045546532, 'test/mean_average_precision': 0.2620029472242, 'test/num_examples': 43793, 'score': 7934.573546886444, 'total_duration': 11720.776325702667, 'accumulated_submission_time': 7934.573546886444, 'accumulated_eval_time': 3784.543060064316, 'accumulated_logging_time': 0.9804179668426514, 'global_step': 24666, 'preemption_count': 0}), (25413, {'train/accuracy': 0.9910230040550232, 'train/loss': 0.02899915911257267, 'train/mean_average_precision': 0.42797345102108686, 'validation/accuracy': 0.986810564994812, 'validation/loss': 0.0445956252515316, 'validation/mean_average_precision': 0.2719177220954323, 'validation/num_examples': 43793, 'test/accuracy': 0.9860171675682068, 'test/loss': 0.04729219898581505, 'test/mean_average_precision': 0.2669060818825145, 'test/num_examples': 43793, 'score': 8174.771550655365, 'total_duration': 12074.557059288025, 'accumulated_submission_time': 8174.771550655365, 'accumulated_eval_time': 3898.0720086097717, 'accumulated_logging_time': 1.0132358074188232, 'global_step': 25413, 'preemption_count': 0}), (26165, {'train/accuracy': 0.9912290573120117, 'train/loss': 0.028361449018120766, 'train/mean_average_precision': 0.45259342880654135, 'validation/accuracy': 0.9866153001785278, 'validation/loss': 0.044667817652225494, 'validation/mean_average_precision': 0.2719697859873213, 'validation/num_examples': 43793, 'test/accuracy': 0.9857838153839111, 'test/loss': 0.04710261896252632, 'test/mean_average_precision': 0.25841901698511205, 'test/num_examples': 43793, 'score': 8414.809683322906, 'total_duration': 12424.020931243896, 'accumulated_submission_time': 8414.809683322906, 'accumulated_eval_time': 4007.4456238746643, 'accumulated_logging_time': 1.0445480346679688, 'global_step': 26165, 'preemption_count': 0}), (26920, {'train/accuracy': 0.991524875164032, 'train/loss': 0.027365470305085182, 'train/mean_average_precision': 0.4807769372003801, 'validation/accuracy': 0.9869144558906555, 'validation/loss': 0.044424235820770264, 'validation/mean_average_precision': 0.2759751459481721, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.04718492180109024, 'test/mean_average_precision': 0.2646887047154832, 'test/num_examples': 43793, 'score': 8654.877965211868, 'total_duration': 12773.301801204681, 'accumulated_submission_time': 8654.877965211868, 'accumulated_eval_time': 4116.604898691177, 'accumulated_logging_time': 1.0775237083435059, 'global_step': 26920, 'preemption_count': 0}), (27672, {'train/accuracy': 0.9917396306991577, 'train/loss': 0.026780622079968452, 'train/mean_average_precision': 0.48750520532975306, 'validation/accuracy': 0.9867861866950989, 'validation/loss': 0.0446733795106411, 'validation/mean_average_precision': 0.2727244827693121, 'validation/num_examples': 43793, 'test/accuracy': 0.9859206676483154, 'test/loss': 0.04740463197231293, 'test/mean_average_precision': 0.2627906059002099, 'test/num_examples': 43793, 'score': 8895.08589887619, 'total_duration': 13120.378950834274, 'accumulated_submission_time': 8895.08589887619, 'accumulated_eval_time': 4223.421566724777, 'accumulated_logging_time': 1.1091325283050537, 'global_step': 27672, 'preemption_count': 0}), (28419, {'train/accuracy': 0.9916260838508606, 'train/loss': 0.026939881965517998, 'train/mean_average_precision': 0.48579127106100845, 'validation/accuracy': 0.9868592619895935, 'validation/loss': 0.0448070652782917, 'validation/mean_average_precision': 0.27169239613355595, 'validation/num_examples': 43793, 'test/accuracy': 0.9859859943389893, 'test/loss': 0.04767269268631935, 'test/mean_average_precision': 0.2642261563246968, 'test/num_examples': 43793, 'score': 9135.200062274933, 'total_duration': 13464.72099852562, 'accumulated_submission_time': 9135.200062274933, 'accumulated_eval_time': 4327.597607374191, 'accumulated_logging_time': 1.1407244205474854, 'global_step': 28419, 'preemption_count': 0}), (29173, {'train/accuracy': 0.991509735584259, 'train/loss': 0.027415966615080833, 'train/mean_average_precision': 0.47126790399796437, 'validation/accuracy': 0.9867748022079468, 'validation/loss': 0.04474445432424545, 'validation/mean_average_precision': 0.27036917002655547, 'validation/num_examples': 43793, 'test/accuracy': 0.985869288444519, 'test/loss': 0.047515373677015305, 'test/mean_average_precision': 0.25662169516709116, 'test/num_examples': 43793, 'score': 9375.395035743713, 'total_duration': 13808.285351991653, 'accumulated_submission_time': 9375.395035743713, 'accumulated_eval_time': 4430.912664890289, 'accumulated_logging_time': 1.174621820449829, 'global_step': 29173, 'preemption_count': 0}), (29924, {'train/accuracy': 0.9912999868392944, 'train/loss': 0.02828758768737316, 'train/mean_average_precision': 0.4548449958025577, 'validation/accuracy': 0.9867565631866455, 'validation/loss': 0.044981010258197784, 'validation/mean_average_precision': 0.2710515862463466, 'validation/num_examples': 43793, 'test/accuracy': 0.985859215259552, 'test/loss': 0.04792707785964012, 'test/mean_average_precision': 0.260950514393316, 'test/num_examples': 43793, 'score': 9615.55433011055, 'total_duration': 14154.216762781143, 'accumulated_submission_time': 9615.55433011055, 'accumulated_eval_time': 4536.631418704987, 'accumulated_logging_time': 1.2075426578521729, 'global_step': 29924, 'preemption_count': 0}), (30677, {'train/accuracy': 0.9913409948348999, 'train/loss': 0.028073610737919807, 'train/mean_average_precision': 0.4493626684822014, 'validation/accuracy': 0.9867821335792542, 'validation/loss': 0.04475332424044609, 'validation/mean_average_precision': 0.27232175876993087, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.047855161130428314, 'test/mean_average_precision': 0.2620535742729729, 'test/num_examples': 43793, 'score': 9855.597105264664, 'total_duration': 14500.886621952057, 'accumulated_submission_time': 9855.597105264664, 'accumulated_eval_time': 4643.205877542496, 'accumulated_logging_time': 1.2392997741699219, 'global_step': 30677, 'preemption_count': 0}), (31434, {'train/accuracy': 0.9913233518600464, 'train/loss': 0.028088927268981934, 'train/mean_average_precision': 0.45806846877071095, 'validation/accuracy': 0.9867622256278992, 'validation/loss': 0.044960204511880875, 'validation/mean_average_precision': 0.2680808067314692, 'validation/num_examples': 43793, 'test/accuracy': 0.985932469367981, 'test/loss': 0.04747050628066063, 'test/mean_average_precision': 0.26141791565521816, 'test/num_examples': 43793, 'score': 10095.629689216614, 'total_duration': 14847.947264671326, 'accumulated_submission_time': 10095.629689216614, 'accumulated_eval_time': 4750.179671287537, 'accumulated_logging_time': 1.2727985382080078, 'global_step': 31434, 'preemption_count': 0}), (32188, {'train/accuracy': 0.9914279580116272, 'train/loss': 0.02746138721704483, 'train/mean_average_precision': 0.4688236204403521, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.0450935922563076, 'validation/mean_average_precision': 0.2805715467170202, 'validation/num_examples': 43793, 'test/accuracy': 0.9858309626579285, 'test/loss': 0.04816046357154846, 'test/mean_average_precision': 0.261548027310026, 'test/num_examples': 43793, 'score': 10335.560795545578, 'total_duration': 15197.513407230377, 'accumulated_submission_time': 10335.560795545578, 'accumulated_eval_time': 4859.741172552109, 'accumulated_logging_time': 1.325857400894165, 'global_step': 32188, 'preemption_count': 0}), (32936, {'train/accuracy': 0.9914365410804749, 'train/loss': 0.02748289331793785, 'train/mean_average_precision': 0.4736294815094482, 'validation/accuracy': 0.9866388440132141, 'validation/loss': 0.04488770291209221, 'validation/mean_average_precision': 0.2706954798470259, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.047641586512327194, 'test/mean_average_precision': 0.2580936874518775, 'test/num_examples': 43793, 'score': 10575.767451763153, 'total_duration': 15545.120171785355, 'accumulated_submission_time': 10575.767451763153, 'accumulated_eval_time': 4967.086313724518, 'accumulated_logging_time': 1.359889268875122, 'global_step': 32936, 'preemption_count': 0}), (33684, {'train/accuracy': 0.9916418790817261, 'train/loss': 0.02673172391951084, 'train/mean_average_precision': 0.48136942091615786, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.044977832585573196, 'validation/mean_average_precision': 0.2782887354386499, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.04794187471270561, 'test/mean_average_precision': 0.2566208483574557, 'test/num_examples': 43793, 'score': 10815.805988311768, 'total_duration': 15896.519999742508, 'accumulated_submission_time': 10815.805988311768, 'accumulated_eval_time': 5078.3935606479645, 'accumulated_logging_time': 1.3929271697998047, 'global_step': 33684, 'preemption_count': 0}), (34436, {'train/accuracy': 0.9919087290763855, 'train/loss': 0.02593764290213585, 'train/mean_average_precision': 0.505223532274534, 'validation/accuracy': 0.9868279695510864, 'validation/loss': 0.04495269060134888, 'validation/mean_average_precision': 0.28174461946087653, 'validation/num_examples': 43793, 'test/accuracy': 0.9859640598297119, 'test/loss': 0.047770678997039795, 'test/mean_average_precision': 0.267297587830477, 'test/num_examples': 43793, 'score': 11055.845739364624, 'total_duration': 16241.80126285553, 'accumulated_submission_time': 11055.845739364624, 'accumulated_eval_time': 5183.58104300499, 'accumulated_logging_time': 1.426476001739502, 'global_step': 34436, 'preemption_count': 0}), (35189, {'train/accuracy': 0.9923018217086792, 'train/loss': 0.02476343885064125, 'train/mean_average_precision': 0.5400334925387037, 'validation/accuracy': 0.9866660237312317, 'validation/loss': 0.04505612328648567, 'validation/mean_average_precision': 0.2804774655869647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858478307723999, 'test/loss': 0.047578711062669754, 'test/mean_average_precision': 0.268802852899132, 'test/num_examples': 43793, 'score': 11296.034619808197, 'total_duration': 16594.574902057648, 'accumulated_submission_time': 11296.034619808197, 'accumulated_eval_time': 5296.110832691193, 'accumulated_logging_time': 1.461381196975708, 'global_step': 35189, 'preemption_count': 0}), (35940, {'train/accuracy': 0.9923170804977417, 'train/loss': 0.02455207146704197, 'train/mean_average_precision': 0.5424872716795479, 'validation/accuracy': 0.9867898225784302, 'validation/loss': 0.045494239777326584, 'validation/mean_average_precision': 0.28353401300497966, 'validation/num_examples': 43793, 'test/accuracy': 0.9859143495559692, 'test/loss': 0.04834071546792984, 'test/mean_average_precision': 0.2643073077441462, 'test/num_examples': 43793, 'score': 11536.095036268234, 'total_duration': 16948.44060611725, 'accumulated_submission_time': 11536.095036268234, 'accumulated_eval_time': 5409.861083984375, 'accumulated_logging_time': 1.496187686920166, 'global_step': 35940, 'preemption_count': 0}), (36692, {'train/accuracy': 0.9918184876441956, 'train/loss': 0.026134414598345757, 'train/mean_average_precision': 0.5122857286942415, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.045166220515966415, 'validation/mean_average_precision': 0.2817615748781584, 'validation/num_examples': 43793, 'test/accuracy': 0.9858882427215576, 'test/loss': 0.04821586608886719, 'test/mean_average_precision': 0.2715438217701766, 'test/num_examples': 43793, 'score': 11776.166133642197, 'total_duration': 17296.837022542953, 'accumulated_submission_time': 11776.166133642197, 'accumulated_eval_time': 5518.131883144379, 'accumulated_logging_time': 1.530383825302124, 'global_step': 36692, 'preemption_count': 0}), (37454, {'train/accuracy': 0.9918424487113953, 'train/loss': 0.026177141815423965, 'train/mean_average_precision': 0.4943024589468207, 'validation/accuracy': 0.9865970015525818, 'validation/loss': 0.04534425958991051, 'validation/mean_average_precision': 0.2697746331850882, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.04839446768164635, 'test/mean_average_precision': 0.26086746319160364, 'test/num_examples': 43793, 'score': 12016.210379838943, 'total_duration': 17646.148535490036, 'accumulated_submission_time': 12016.210379838943, 'accumulated_eval_time': 5627.343741178513, 'accumulated_logging_time': 1.5651028156280518, 'global_step': 37454, 'preemption_count': 0}), (38209, {'train/accuracy': 0.9917693138122559, 'train/loss': 0.026469871401786804, 'train/mean_average_precision': 0.49684058513550633, 'validation/accuracy': 0.9866034984588623, 'validation/loss': 0.04540769383311272, 'validation/mean_average_precision': 0.2799366756800923, 'validation/num_examples': 43793, 'test/accuracy': 0.985753059387207, 'test/loss': 0.048491571098566055, 'test/mean_average_precision': 0.25814005772812937, 'test/num_examples': 43793, 'score': 12256.275754451752, 'total_duration': 17992.750111341476, 'accumulated_submission_time': 12256.275754451752, 'accumulated_eval_time': 5733.825747728348, 'accumulated_logging_time': 1.5990040302276611, 'global_step': 38209, 'preemption_count': 0}), (38963, {'train/accuracy': 0.9920052289962769, 'train/loss': 0.025733111426234245, 'train/mean_average_precision': 0.49616740820467975, 'validation/accuracy': 0.9868503212928772, 'validation/loss': 0.04526519775390625, 'validation/mean_average_precision': 0.27865147177100186, 'validation/num_examples': 43793, 'test/accuracy': 0.9859619736671448, 'test/loss': 0.048397425562143326, 'test/mean_average_precision': 0.2601541778417228, 'test/num_examples': 43793, 'score': 12496.309900045395, 'total_duration': 18338.664535284042, 'accumulated_submission_time': 12496.309900045395, 'accumulated_eval_time': 5839.652360200882, 'accumulated_logging_time': 1.6322979927062988, 'global_step': 38963, 'preemption_count': 0}), (39723, {'train/accuracy': 0.9920634031295776, 'train/loss': 0.02536453679203987, 'train/mean_average_precision': 0.5136206193556406, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.045312780886888504, 'validation/mean_average_precision': 0.2773486024374958, 'validation/num_examples': 43793, 'test/accuracy': 0.9859986305236816, 'test/loss': 0.04844435304403305, 'test/mean_average_precision': 0.2660761208561459, 'test/num_examples': 43793, 'score': 12736.541811704636, 'total_duration': 18686.824773311615, 'accumulated_submission_time': 12736.541811704636, 'accumulated_eval_time': 5947.527049303055, 'accumulated_logging_time': 1.6655912399291992, 'global_step': 39723, 'preemption_count': 0}), (40479, {'train/accuracy': 0.9920461177825928, 'train/loss': 0.025316696614027023, 'train/mean_average_precision': 0.527780740298631, 'validation/accuracy': 0.986867368221283, 'validation/loss': 0.04554228484630585, 'validation/mean_average_precision': 0.28026867914384174, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.048717182129621506, 'test/mean_average_precision': 0.26557231991056374, 'test/num_examples': 43793, 'score': 12976.76999258995, 'total_duration': 19034.442306518555, 'accumulated_submission_time': 12976.76999258995, 'accumulated_eval_time': 6054.862420797348, 'accumulated_logging_time': 1.6991665363311768, 'global_step': 40479, 'preemption_count': 0}), (41240, {'train/accuracy': 0.9922798275947571, 'train/loss': 0.024545539170503616, 'train/mean_average_precision': 0.524614840843699, 'validation/accuracy': 0.9867496490478516, 'validation/loss': 0.045290201902389526, 'validation/mean_average_precision': 0.283905575901221, 'validation/num_examples': 43793, 'test/accuracy': 0.9858541488647461, 'test/loss': 0.04830815643072128, 'test/mean_average_precision': 0.2637783218790555, 'test/num_examples': 43793, 'score': 13216.979594230652, 'total_duration': 19383.399483442307, 'accumulated_submission_time': 13216.979594230652, 'accumulated_eval_time': 6163.554557561874, 'accumulated_logging_time': 1.7342724800109863, 'global_step': 41240, 'preemption_count': 0}), (41996, {'train/accuracy': 0.9924442172050476, 'train/loss': 0.02393968589603901, 'train/mean_average_precision': 0.5495143535314464, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.045334722846746445, 'validation/mean_average_precision': 0.2868209618737811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858916401863098, 'test/loss': 0.048358894884586334, 'test/mean_average_precision': 0.2650313659736344, 'test/num_examples': 43793, 'score': 13457.06114768982, 'total_duration': 19725.856940746307, 'accumulated_submission_time': 13457.06114768982, 'accumulated_eval_time': 6265.874538183212, 'accumulated_logging_time': 1.769547462463379, 'global_step': 41996, 'preemption_count': 0}), (42753, {'train/accuracy': 0.9926914572715759, 'train/loss': 0.02306647226214409, 'train/mean_average_precision': 0.5733023445192826, 'validation/accuracy': 0.9868162274360657, 'validation/loss': 0.04585631936788559, 'validation/mean_average_precision': 0.2811874228785678, 'validation/num_examples': 43793, 'test/accuracy': 0.9859809279441833, 'test/loss': 0.0491667203605175, 'test/mean_average_precision': 0.2701141743328484, 'test/num_examples': 43793, 'score': 13697.11407327652, 'total_duration': 20076.64497256279, 'accumulated_submission_time': 13697.11407327652, 'accumulated_eval_time': 6376.554070949554, 'accumulated_logging_time': 1.804668664932251, 'global_step': 42753, 'preemption_count': 0}), (43508, {'train/accuracy': 0.9930054545402527, 'train/loss': 0.022223111242055893, 'train/mean_average_precision': 0.5976667659929961, 'validation/accuracy': 0.9868007898330688, 'validation/loss': 0.04619989171624184, 'validation/mean_average_precision': 0.28058899499968, 'validation/num_examples': 43793, 'test/accuracy': 0.9859379529953003, 'test/loss': 0.04921237379312515, 'test/mean_average_precision': 0.26696407008968237, 'test/num_examples': 43793, 'score': 13937.296043395996, 'total_duration': 20427.44962954521, 'accumulated_submission_time': 13937.296043395996, 'accumulated_eval_time': 6487.12228512764, 'accumulated_logging_time': 1.838754415512085, 'global_step': 43508, 'preemption_count': 0}), (44262, {'train/accuracy': 0.9929794073104858, 'train/loss': 0.022425740957260132, 'train/mean_average_precision': 0.5909267678181348, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.046438559889793396, 'validation/mean_average_precision': 0.2845100129284141, 'validation/num_examples': 43793, 'test/accuracy': 0.985765278339386, 'test/loss': 0.049320854246616364, 'test/mean_average_precision': 0.2665610301084667, 'test/num_examples': 43793, 'score': 14177.254234313965, 'total_duration': 20775.13205766678, 'accumulated_submission_time': 14177.254234313965, 'accumulated_eval_time': 6594.789473056793, 'accumulated_logging_time': 1.8754479885101318, 'global_step': 44262, 'preemption_count': 0}), (45013, {'train/accuracy': 0.9926031827926636, 'train/loss': 0.02358449250459671, 'train/mean_average_precision': 0.553874841307491, 'validation/accuracy': 0.9867179989814758, 'validation/loss': 0.046413395553827286, 'validation/mean_average_precision': 0.27960552758316165, 'validation/num_examples': 43793, 'test/accuracy': 0.9858819246292114, 'test/loss': 0.049666281789541245, 'test/mean_average_precision': 0.2710117089890043, 'test/num_examples': 43793, 'score': 14417.521105766296, 'total_duration': 21123.5917699337, 'accumulated_submission_time': 14417.521105766296, 'accumulated_eval_time': 6702.926654815674, 'accumulated_logging_time': 1.910888671875, 'global_step': 45013, 'preemption_count': 0}), (45768, {'train/accuracy': 0.9925385117530823, 'train/loss': 0.02367567829787731, 'train/mean_average_precision': 0.5445353539343192, 'validation/accuracy': 0.9865588545799255, 'validation/loss': 0.04667806997895241, 'validation/mean_average_precision': 0.27965255637826175, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.04969778656959534, 'test/mean_average_precision': 0.2654268209684074, 'test/num_examples': 43793, 'score': 14657.61928486824, 'total_duration': 21473.753289222717, 'accumulated_submission_time': 14657.61928486824, 'accumulated_eval_time': 6812.935254096985, 'accumulated_logging_time': 1.94553542137146, 'global_step': 45768, 'preemption_count': 0}), (46528, {'train/accuracy': 0.9925999045372009, 'train/loss': 0.02357323095202446, 'train/mean_average_precision': 0.5610417006179397, 'validation/accuracy': 0.9867228865623474, 'validation/loss': 0.04652188718318939, 'validation/mean_average_precision': 0.27857221709013974, 'validation/num_examples': 43793, 'test/accuracy': 0.9858840703964233, 'test/loss': 0.04971200227737427, 'test/mean_average_precision': 0.2653429253234877, 'test/num_examples': 43793, 'score': 14897.713246583939, 'total_duration': 21816.33208823204, 'accumulated_submission_time': 14897.713246583939, 'accumulated_eval_time': 6915.363301515579, 'accumulated_logging_time': 1.9816162586212158, 'global_step': 46528, 'preemption_count': 0}), (47283, {'train/accuracy': 0.9925141930580139, 'train/loss': 0.023568419739603996, 'train/mean_average_precision': 0.5593789092608922, 'validation/accuracy': 0.9866579174995422, 'validation/loss': 0.04711558297276497, 'validation/mean_average_precision': 0.2736626211202902, 'validation/num_examples': 43793, 'test/accuracy': 0.9857581257820129, 'test/loss': 0.05036972090601921, 'test/mean_average_precision': 0.2608815513531671, 'test/num_examples': 43793, 'score': 15137.886631250381, 'total_duration': 22165.82682275772, 'accumulated_submission_time': 15137.886631250381, 'accumulated_eval_time': 7024.628471374512, 'accumulated_logging_time': 2.017472982406616, 'global_step': 47283, 'preemption_count': 0}), (48037, {'train/accuracy': 0.992720901966095, 'train/loss': 0.02295967936515808, 'train/mean_average_precision': 0.5675811443116955, 'validation/accuracy': 0.9865494966506958, 'validation/loss': 0.0468745082616806, 'validation/mean_average_precision': 0.2781233637894734, 'validation/num_examples': 43793, 'test/accuracy': 0.9855588674545288, 'test/loss': 0.050317663699388504, 'test/mean_average_precision': 0.2622498398743667, 'test/num_examples': 43793, 'score': 15377.85105419159, 'total_duration': 22508.549419403076, 'accumulated_submission_time': 15377.85105419159, 'accumulated_eval_time': 7127.328198194504, 'accumulated_logging_time': 2.0556468963623047, 'global_step': 48037, 'preemption_count': 0}), (48792, {'train/accuracy': 0.9929220080375671, 'train/loss': 0.02197171375155449, 'train/mean_average_precision': 0.5988421261644996, 'validation/accuracy': 0.9868214726448059, 'validation/loss': 0.047487981617450714, 'validation/mean_average_precision': 0.2777354293557749, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.05089396610856056, 'test/mean_average_precision': 0.26474430938656773, 'test/num_examples': 43793, 'score': 15618.037787914276, 'total_duration': 22851.45850634575, 'accumulated_submission_time': 15618.037787914276, 'accumulated_eval_time': 7229.995413780212, 'accumulated_logging_time': 2.0907273292541504, 'global_step': 48792, 'preemption_count': 0}), (49545, {'train/accuracy': 0.9931709170341492, 'train/loss': 0.021309148520231247, 'train/mean_average_precision': 0.6088010839997859, 'validation/accuracy': 0.986845850944519, 'validation/loss': 0.04742836952209473, 'validation/mean_average_precision': 0.2821814441667886, 'validation/num_examples': 43793, 'test/accuracy': 0.9858722686767578, 'test/loss': 0.05089188367128372, 'test/mean_average_precision': 0.26339655352001395, 'test/num_examples': 43793, 'score': 15857.938402414322, 'total_duration': 23197.240000247955, 'accumulated_submission_time': 15857.938402414322, 'accumulated_eval_time': 7335.499788284302, 'accumulated_logging_time': 2.4467310905456543, 'global_step': 49545, 'preemption_count': 0}), (50299, {'train/accuracy': 0.9934626221656799, 'train/loss': 0.02049173228442669, 'train/mean_average_precision': 0.6304601455926362, 'validation/accuracy': 0.9867045879364014, 'validation/loss': 0.047425173223018646, 'validation/mean_average_precision': 0.2834991495572792, 'validation/num_examples': 43793, 'test/accuracy': 0.9857922196388245, 'test/loss': 0.05078952759504318, 'test/mean_average_precision': 0.2697486751280569, 'test/num_examples': 43793, 'score': 16098.165282726288, 'total_duration': 23544.99045753479, 'accumulated_submission_time': 16098.165282726288, 'accumulated_eval_time': 7442.966633319855, 'accumulated_logging_time': 2.483210563659668, 'global_step': 50299, 'preemption_count': 0}), (51055, {'train/accuracy': 0.9939581155776978, 'train/loss': 0.01905048079788685, 'train/mean_average_precision': 0.6652305154089658, 'validation/accuracy': 0.9866855144500732, 'validation/loss': 0.048273272812366486, 'validation/mean_average_precision': 0.2792698083358352, 'validation/num_examples': 43793, 'test/accuracy': 0.9858272075653076, 'test/loss': 0.05148047208786011, 'test/mean_average_precision': 0.26181299020306065, 'test/num_examples': 43793, 'score': 16338.194887399673, 'total_duration': 23889.070935964584, 'accumulated_submission_time': 16338.194887399673, 'accumulated_eval_time': 7546.960394859314, 'accumulated_logging_time': 2.5192384719848633, 'global_step': 51055, 'preemption_count': 0}), (51814, {'train/accuracy': 0.9940596222877502, 'train/loss': 0.018846679478883743, 'train/mean_average_precision': 0.6622694237726308, 'validation/accuracy': 0.9865750670433044, 'validation/loss': 0.04867831990122795, 'validation/mean_average_precision': 0.2745682719653072, 'validation/num_examples': 43793, 'test/accuracy': 0.9856536388397217, 'test/loss': 0.052153799682855606, 'test/mean_average_precision': 0.2611147893670439, 'test/num_examples': 43793, 'score': 16578.26107263565, 'total_duration': 24235.02452325821, 'accumulated_submission_time': 16578.26107263565, 'accumulated_eval_time': 7652.790205001831, 'accumulated_logging_time': 2.5558063983917236, 'global_step': 51814, 'preemption_count': 0}), (52572, {'train/accuracy': 0.9937030076980591, 'train/loss': 0.019762754440307617, 'train/mean_average_precision': 0.6384974196668561, 'validation/accuracy': 0.9864805340766907, 'validation/loss': 0.04896825924515724, 'validation/mean_average_precision': 0.276152484739644, 'validation/num_examples': 43793, 'test/accuracy': 0.9855180382728577, 'test/loss': 0.052332133054733276, 'test/mean_average_precision': 0.26013065956833903, 'test/num_examples': 43793, 'score': 16818.50644493103, 'total_duration': 24583.90949845314, 'accumulated_submission_time': 16818.50644493103, 'accumulated_eval_time': 7761.37190580368, 'accumulated_logging_time': 2.5926008224487305, 'global_step': 52572, 'preemption_count': 0}), (53324, {'train/accuracy': 0.993492603302002, 'train/loss': 0.020317774266004562, 'train/mean_average_precision': 0.6140696882882921, 'validation/accuracy': 0.9865548014640808, 'validation/loss': 0.048930615186691284, 'validation/mean_average_precision': 0.28487371778432663, 'validation/num_examples': 43793, 'test/accuracy': 0.9857269525527954, 'test/loss': 0.05243641138076782, 'test/mean_average_precision': 0.26731853432521185, 'test/num_examples': 43793, 'score': 17058.707488775253, 'total_duration': 24932.676094055176, 'accumulated_submission_time': 17058.707488775253, 'accumulated_eval_time': 7869.87908744812, 'accumulated_logging_time': 2.6299984455108643, 'global_step': 53324, 'preemption_count': 0}), (54079, {'train/accuracy': 0.9934467077255249, 'train/loss': 0.020390227437019348, 'train/mean_average_precision': 0.6370068629099701, 'validation/accuracy': 0.9865905046463013, 'validation/loss': 0.04956473410129547, 'validation/mean_average_precision': 0.2744399979478792, 'validation/num_examples': 43793, 'test/accuracy': 0.985632598400116, 'test/loss': 0.053322236984968185, 'test/mean_average_precision': 0.2601269322421878, 'test/num_examples': 43793, 'score': 17298.7577586174, 'total_duration': 25279.552406549454, 'accumulated_submission_time': 17298.7577586174, 'accumulated_eval_time': 7976.645930767059, 'accumulated_logging_time': 2.6677396297454834, 'global_step': 54079, 'preemption_count': 0}), (54833, {'train/accuracy': 0.9934847354888916, 'train/loss': 0.020191708579659462, 'train/mean_average_precision': 0.6408067088189375, 'validation/accuracy': 0.9866063594818115, 'validation/loss': 0.05004741996526718, 'validation/mean_average_precision': 0.2746520933144069, 'validation/num_examples': 43793, 'test/accuracy': 0.9855727553367615, 'test/loss': 0.05376891791820526, 'test/mean_average_precision': 0.26042919967582107, 'test/num_examples': 43793, 'score': 17538.758221387863, 'total_duration': 25626.905437469482, 'accumulated_submission_time': 17538.758221387863, 'accumulated_eval_time': 8083.9382219314575, 'accumulated_logging_time': 2.706973075866699, 'global_step': 54833, 'preemption_count': 0}), (55585, {'train/accuracy': 0.9934579133987427, 'train/loss': 0.020221708342432976, 'train/mean_average_precision': 0.6336687988840867, 'validation/accuracy': 0.9862795472145081, 'validation/loss': 0.050230368971824646, 'validation/mean_average_precision': 0.2801965927562672, 'validation/num_examples': 43793, 'test/accuracy': 0.9853209257125854, 'test/loss': 0.05393723025918007, 'test/mean_average_precision': 0.257986693851216, 'test/num_examples': 43793, 'score': 17778.90761089325, 'total_duration': 25975.122877836227, 'accumulated_submission_time': 17778.90761089325, 'accumulated_eval_time': 8191.948867797852, 'accumulated_logging_time': 2.743889093399048, 'global_step': 55585, 'preemption_count': 0}), (56345, {'train/accuracy': 0.9935694336891174, 'train/loss': 0.019636856392025948, 'train/mean_average_precision': 0.6439595382453188, 'validation/accuracy': 0.9865819811820984, 'validation/loss': 0.05091746151447296, 'validation/mean_average_precision': 0.27855193216330126, 'validation/num_examples': 43793, 'test/accuracy': 0.9856279492378235, 'test/loss': 0.0548713281750679, 'test/mean_average_precision': 0.26244628027064826, 'test/num_examples': 43793, 'score': 18019.107362270355, 'total_duration': 26320.5678255558, 'accumulated_submission_time': 18019.107362270355, 'accumulated_eval_time': 8297.136292934418, 'accumulated_logging_time': 2.7808399200439453, 'global_step': 56345, 'preemption_count': 0}), (57096, {'train/accuracy': 0.9941542148590088, 'train/loss': 0.017990222200751305, 'train/mean_average_precision': 0.6795010598582831, 'validation/accuracy': 0.9863534569740295, 'validation/loss': 0.051240529865026474, 'validation/mean_average_precision': 0.2821614791012339, 'validation/num_examples': 43793, 'test/accuracy': 0.985390841960907, 'test/loss': 0.05514898896217346, 'test/mean_average_precision': 0.25607921974810816, 'test/num_examples': 43793, 'score': 18259.057409524918, 'total_duration': 26670.66596722603, 'accumulated_submission_time': 18259.057409524918, 'accumulated_eval_time': 8407.226685523987, 'accumulated_logging_time': 2.8178892135620117, 'global_step': 57096, 'preemption_count': 0})], 'global_step': 57780}
I0206 02:48:43.160624 139978932307776 submission_runner.py:586] Timing: 18477.00848555565
I0206 02:48:43.160677 139978932307776 submission_runner.py:588] Total number of evals: 77
I0206 02:48:43.160736 139978932307776 submission_runner.py:589] ====================
I0206 02:48:43.160782 139978932307776 submission_runner.py:542] Using RNG seed 3917441912
I0206 02:48:43.224829 139978932307776 submission_runner.py:551] --- Tuning run 4/5 ---
I0206 02:48:43.224993 139978932307776 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4.
I0206 02:48:43.225216 139978932307776 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4/hparams.json.
I0206 02:48:43.357012 139978932307776 submission_runner.py:206] Initializing dataset.
I0206 02:48:43.443655 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0206 02:48:43.447865 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0206 02:48:43.578510 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0206 02:48:43.615557 139978932307776 submission_runner.py:213] Initializing model.
I0206 02:48:46.109376 139978932307776 submission_runner.py:255] Initializing optimizer.
I0206 02:48:46.710740 139978932307776 submission_runner.py:262] Initializing metrics bundle.
I0206 02:48:46.710947 139978932307776 submission_runner.py:280] Initializing checkpoint and logger.
I0206 02:48:46.711704 139978932307776 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4 with prefix checkpoint_
I0206 02:48:46.711843 139978932307776 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4/meta_data_0.json.
I0206 02:48:46.712066 139978932307776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 02:48:46.712135 139978932307776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 02:48:48.810449 139978932307776 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 02:48:50.908710 139978932307776 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4/flags_0.json.
I0206 02:48:50.912916 139978932307776 submission_runner.py:314] Starting training loop.
I0206 02:49:02.798252 139752134514432 logging_writer.py:48] [0] global_step=0, grad_norm=3.1811881065368652, loss=0.7991227507591248
I0206 02:49:02.807845 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:50:41.640410 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:50:44.637204 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:50:47.593926 139978932307776 submission_runner.py:408] Time since start: 116.68s, 	Step: 1, 	{'train/accuracy': 0.38870498538017273, 'train/loss': 0.7994356751441956, 'train/mean_average_precision': 0.023099368034654698, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.024843149604178178, 'validation/num_examples': 43793, 'test/accuracy': 0.3947480618953705, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.02636636073612592, 'test/num_examples': 43793, 'score': 11.89487600326538, 'total_duration': 116.68095016479492, 'accumulated_submission_time': 11.89487600326538, 'accumulated_eval_time': 104.78602743148804, 'accumulated_logging_time': 0}
I0206 02:50:47.606609 139752176375552 logging_writer.py:48] [1] accumulated_eval_time=104.786027, accumulated_logging_time=0, accumulated_submission_time=11.894876, global_step=1, preemption_count=0, score=11.894876, test/accuracy=0.394748, test/loss=0.795676, test/mean_average_precision=0.026366, test/num_examples=43793, total_duration=116.680950, train/accuracy=0.388705, train/loss=0.799436, train/mean_average_precision=0.023099, validation/accuracy=0.392645, validation/loss=0.797466, validation/mean_average_precision=0.024843, validation/num_examples=43793
I0206 02:51:19.751407 139770185602816 logging_writer.py:48] [100] global_step=100, grad_norm=0.11703073233366013, loss=0.12093871831893921
I0206 02:51:52.068414 139752176375552 logging_writer.py:48] [200] global_step=200, grad_norm=0.008400563150644302, loss=0.05690360441803932
I0206 02:52:24.408560 139770185602816 logging_writer.py:48] [300] global_step=300, grad_norm=0.007058307062834501, loss=0.05111575871706009
I0206 02:52:56.386457 139752176375552 logging_writer.py:48] [400] global_step=400, grad_norm=0.009147398173809052, loss=0.04886140301823616
I0206 02:53:28.211046 139770185602816 logging_writer.py:48] [500] global_step=500, grad_norm=0.024820365011692047, loss=0.05428244546055794
I0206 02:54:00.444212 139752176375552 logging_writer.py:48] [600] global_step=600, grad_norm=0.015920186415314674, loss=0.0498712919652462
I0206 02:54:32.839413 139770185602816 logging_writer.py:48] [700] global_step=700, grad_norm=0.02427012473344803, loss=0.0657680407166481
I0206 02:54:47.676236 139978932307776 spec.py:321] Evaluating on the training split.
I0206 02:56:28.760878 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 02:56:31.789936 139978932307776 spec.py:349] Evaluating on the test split.
I0206 02:56:34.757412 139978932307776 submission_runner.py:408] Time since start: 463.84s, 	Step: 748, 	{'train/accuracy': 0.9867178201675415, 'train/loss': 0.05405712500214577, 'train/mean_average_precision': 0.041328803588909446, 'validation/accuracy': 0.984153687953949, 'validation/loss': 0.06424260884523392, 'validation/mean_average_precision': 0.04055070544653996, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06758540123701096, 'test/mean_average_precision': 0.0414079131255289, 'test/num_examples': 43793, 'score': 251.93400812149048, 'total_duration': 463.8443031311035, 'accumulated_submission_time': 251.93400812149048, 'accumulated_eval_time': 211.86702752113342, 'accumulated_logging_time': 0.023287296295166016}
I0206 02:56:34.772321 139752137176832 logging_writer.py:48] [748] accumulated_eval_time=211.867028, accumulated_logging_time=0.023287, accumulated_submission_time=251.934008, global_step=748, preemption_count=0, score=251.934008, test/accuracy=0.983172, test/loss=0.067585, test/mean_average_precision=0.041408, test/num_examples=43793, total_duration=463.844303, train/accuracy=0.986718, train/loss=0.054057, train/mean_average_precision=0.041329, validation/accuracy=0.984154, validation/loss=0.064243, validation/mean_average_precision=0.040551, validation/num_examples=43793
I0206 02:56:51.895504 139770797713152 logging_writer.py:48] [800] global_step=800, grad_norm=0.007781799882650375, loss=0.0460912324488163
I0206 02:57:23.757833 139752137176832 logging_writer.py:48] [900] global_step=900, grad_norm=0.00478987256065011, loss=0.05499229207634926
I0206 02:57:55.859820 139770797713152 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.005576778668910265, loss=0.056869249790906906
I0206 02:58:27.483603 139752137176832 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.009790724143385887, loss=0.05210503190755844
I0206 02:58:59.293937 139770797713152 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.013282200321555138, loss=0.04723861441016197
I0206 02:59:31.309925 139752137176832 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.008645700290799141, loss=0.04663629084825516
I0206 03:00:03.329499 139770797713152 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.007955852895975113, loss=0.047648835927248
I0206 03:00:34.815548 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:02:12.057854 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:02:15.063949 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:02:17.990906 139978932307776 submission_runner.py:408] Time since start: 807.08s, 	Step: 1500, 	{'train/accuracy': 0.9868547916412354, 'train/loss': 0.0534534752368927, 'train/mean_average_precision': 0.04040749192785452, 'validation/accuracy': 0.984169065952301, 'validation/loss': 0.06380725651979446, 'validation/mean_average_precision': 0.04014752498615162, 'validation/num_examples': 43793, 'test/accuracy': 0.9832056760787964, 'test/loss': 0.06706760823726654, 'test/mean_average_precision': 0.04194428881865402, 'test/num_examples': 43793, 'score': 491.94453287124634, 'total_duration': 807.0779266357422, 'accumulated_submission_time': 491.94453287124634, 'accumulated_eval_time': 315.04234194755554, 'accumulated_logging_time': 0.05069994926452637}
I0206 03:02:18.007751 139770185602816 logging_writer.py:48] [1500] accumulated_eval_time=315.042342, accumulated_logging_time=0.050700, accumulated_submission_time=491.944533, global_step=1500, preemption_count=0, score=491.944533, test/accuracy=0.983206, test/loss=0.067068, test/mean_average_precision=0.041944, test/num_examples=43793, total_duration=807.077927, train/accuracy=0.986855, train/loss=0.053453, train/mean_average_precision=0.040407, validation/accuracy=0.984169, validation/loss=0.063807, validation/mean_average_precision=0.040148, validation/num_examples=43793
I0206 03:02:18.340156 139770806105856 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.01461554691195488, loss=0.061812955886125565
I0206 03:02:50.217500 139770185602816 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.009512016549706459, loss=0.057269323617219925
I0206 03:03:22.046607 139770806105856 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.008002428337931633, loss=0.05357475206255913
I0206 03:03:53.972220 139770185602816 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.018469367176294327, loss=0.05676331743597984
I0206 03:04:25.800915 139770806105856 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.006810994818806648, loss=0.05034717172384262
I0206 03:04:57.471025 139770185602816 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.010300093330442905, loss=0.05872681364417076
I0206 03:05:29.339397 139770806105856 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.008668622933328152, loss=0.05091221630573273
I0206 03:06:00.996413 139770185602816 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.013080575503408909, loss=0.04989265650510788
I0206 03:06:18.065135 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:07:59.198109 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:08:02.234518 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:08:05.171805 139978932307776 submission_runner.py:408] Time since start: 1154.26s, 	Step: 2255, 	{'train/accuracy': 0.9868224263191223, 'train/loss': 0.05183904245495796, 'train/mean_average_precision': 0.05263785527638787, 'validation/accuracy': 0.9842417240142822, 'validation/loss': 0.062132131308317184, 'validation/mean_average_precision': 0.04984300730249895, 'validation/num_examples': 43793, 'test/accuracy': 0.9832654595375061, 'test/loss': 0.06538085639476776, 'test/mean_average_precision': 0.05076436693773005, 'test/num_examples': 43793, 'score': 731.971120595932, 'total_duration': 1154.2588243484497, 'accumulated_submission_time': 731.971120595932, 'accumulated_eval_time': 422.14896512031555, 'accumulated_logging_time': 0.07857346534729004}
I0206 03:08:05.187267 139752176375552 logging_writer.py:48] [2255] accumulated_eval_time=422.148965, accumulated_logging_time=0.078573, accumulated_submission_time=731.971121, global_step=2255, preemption_count=0, score=731.971121, test/accuracy=0.983265, test/loss=0.065381, test/mean_average_precision=0.050764, test/num_examples=43793, total_duration=1154.258824, train/accuracy=0.986822, train/loss=0.051839, train/mean_average_precision=0.052638, validation/accuracy=0.984242, validation/loss=0.062132, validation/mean_average_precision=0.049843, validation/num_examples=43793
I0206 03:08:20.132706 139770797713152 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.010056889615952969, loss=0.05697137117385864
I0206 03:08:51.708319 139752176375552 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.008230870589613914, loss=0.05046413093805313
I0206 03:09:23.119400 139770797713152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.007200114894658327, loss=0.048402849584817886
I0206 03:09:54.610111 139752176375552 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.008549281395971775, loss=0.05511635169386864
I0206 03:10:26.759706 139770797713152 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.013401489704847336, loss=0.05335347354412079
I0206 03:10:58.509327 139752176375552 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.004164778161793947, loss=0.04775570333003998
I0206 03:11:30.275344 139770797713152 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.015742259100079536, loss=0.05242513492703438
I0206 03:12:01.881096 139752176375552 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.007182800676673651, loss=0.05206593498587608
I0206 03:12:05.355595 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:13:45.822852 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:13:48.817872 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:13:51.754403 139978932307776 submission_runner.py:408] Time since start: 1500.84s, 	Step: 3012, 	{'train/accuracy': 0.9869569540023804, 'train/loss': 0.049530282616615295, 'train/mean_average_precision': 0.06984231221718884, 'validation/accuracy': 0.98429936170578, 'validation/loss': 0.05969257652759552, 'validation/mean_average_precision': 0.06538215512675465, 'validation/num_examples': 43793, 'test/accuracy': 0.9833332896232605, 'test/loss': 0.06334034353494644, 'test/mean_average_precision': 0.06630883860500196, 'test/num_examples': 43793, 'score': 972.108246088028, 'total_duration': 1500.8414223194122, 'accumulated_submission_time': 972.108246088028, 'accumulated_eval_time': 528.547721862793, 'accumulated_logging_time': 0.10526752471923828}
I0206 03:13:51.769762 139770185602816 logging_writer.py:48] [3012] accumulated_eval_time=528.547722, accumulated_logging_time=0.105268, accumulated_submission_time=972.108246, global_step=3012, preemption_count=0, score=972.108246, test/accuracy=0.983333, test/loss=0.063340, test/mean_average_precision=0.066309, test/num_examples=43793, total_duration=1500.841422, train/accuracy=0.986957, train/loss=0.049530, train/mean_average_precision=0.069842, validation/accuracy=0.984299, validation/loss=0.059693, validation/mean_average_precision=0.065382, validation/num_examples=43793
I0206 03:14:19.905975 139770806105856 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010308713652193546, loss=0.04816851019859314
I0206 03:14:51.370170 139770185602816 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.009694669395685196, loss=0.04928027093410492
I0206 03:15:23.103106 139770806105856 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.02084967866539955, loss=0.041689638048410416
I0206 03:15:54.946513 139770185602816 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01196126826107502, loss=0.04544874280691147
I0206 03:16:26.818029 139770806105856 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.013705426827073097, loss=0.046822477132081985
I0206 03:16:58.709652 139770185602816 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013958786614239216, loss=0.04683799669146538
I0206 03:17:30.612903 139770806105856 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.023697929456830025, loss=0.04352525621652603
I0206 03:17:51.926234 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:19:34.273333 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:19:37.256442 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:19:40.226442 139978932307776 submission_runner.py:408] Time since start: 1849.31s, 	Step: 3768, 	{'train/accuracy': 0.9871721863746643, 'train/loss': 0.04804283380508423, 'train/mean_average_precision': 0.09283866234241844, 'validation/accuracy': 0.9844650030136108, 'validation/loss': 0.05787404999136925, 'validation/mean_average_precision': 0.09066849691709097, 'validation/num_examples': 43793, 'test/accuracy': 0.9835565090179443, 'test/loss': 0.06136487051844597, 'test/mean_average_precision': 0.09212002824112336, 'test/num_examples': 43793, 'score': 1212.2337291240692, 'total_duration': 1849.313452243805, 'accumulated_submission_time': 1212.2337291240692, 'accumulated_eval_time': 636.8478765487671, 'accumulated_logging_time': 0.13160991668701172}
I0206 03:19:40.242100 139752176375552 logging_writer.py:48] [3768] accumulated_eval_time=636.847877, accumulated_logging_time=0.131610, accumulated_submission_time=1212.233729, global_step=3768, preemption_count=0, score=1212.233729, test/accuracy=0.983557, test/loss=0.061365, test/mean_average_precision=0.092120, test/num_examples=43793, total_duration=1849.313452, train/accuracy=0.987172, train/loss=0.048043, train/mean_average_precision=0.092839, validation/accuracy=0.984465, validation/loss=0.057874, validation/mean_average_precision=0.090668, validation/num_examples=43793
I0206 03:19:50.712061 139770797713152 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.006908677518367767, loss=0.048733148723840714
I0206 03:20:22.207113 139752176375552 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.00793228205293417, loss=0.04374290257692337
I0206 03:20:54.112700 139770797713152 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.05788429453969002, loss=0.046446509659290314
I0206 03:21:26.129182 139752176375552 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.02620554342865944, loss=0.05141551047563553
I0206 03:21:57.914323 139770797713152 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.031497225165367126, loss=0.0520784892141819
I0206 03:22:30.035002 139752176375552 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.016387930139899254, loss=0.04223361238837242
I0206 03:23:01.752143 139770797713152 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.03039659932255745, loss=0.038555629551410675
I0206 03:23:33.696570 139752176375552 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.015532243996858597, loss=0.045188259333372116
I0206 03:23:40.386357 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:25:19.033853 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:25:22.050322 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:25:25.045205 139978932307776 submission_runner.py:408] Time since start: 2194.13s, 	Step: 4522, 	{'train/accuracy': 0.9874393939971924, 'train/loss': 0.04534367099404335, 'train/mean_average_precision': 0.12454040697946092, 'validation/accuracy': 0.9847800135612488, 'validation/loss': 0.05480606481432915, 'validation/mean_average_precision': 0.11117304640724955, 'validation/num_examples': 43793, 'test/accuracy': 0.9838467240333557, 'test/loss': 0.057779669761657715, 'test/mean_average_precision': 0.11733046716598139, 'test/num_examples': 43793, 'score': 1452.3454518318176, 'total_duration': 2194.1322169303894, 'accumulated_submission_time': 1452.3454518318176, 'accumulated_eval_time': 741.5066666603088, 'accumulated_logging_time': 0.15953278541564941}
I0206 03:25:25.061124 139752137176832 logging_writer.py:48] [4522] accumulated_eval_time=741.506667, accumulated_logging_time=0.159533, accumulated_submission_time=1452.345452, global_step=4522, preemption_count=0, score=1452.345452, test/accuracy=0.983847, test/loss=0.057780, test/mean_average_precision=0.117330, test/num_examples=43793, total_duration=2194.132217, train/accuracy=0.987439, train/loss=0.045344, train/mean_average_precision=0.124540, validation/accuracy=0.984780, validation/loss=0.054806, validation/mean_average_precision=0.111173, validation/num_examples=43793
I0206 03:25:50.484625 139770806105856 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.032473716884851456, loss=0.046378638595342636
I0206 03:26:22.860108 139752137176832 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.023097030818462372, loss=0.044220972806215286
I0206 03:26:54.667856 139770806105856 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.0170927494764328, loss=0.042405616492033005
I0206 03:27:26.540105 139752137176832 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.011588145978748798, loss=0.047509849071502686
I0206 03:27:58.600413 139770806105856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.056276075541973114, loss=0.04757438972592354
I0206 03:28:30.864922 139752137176832 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.026455050334334373, loss=0.041980914771556854
I0206 03:29:03.217159 139770806105856 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.02026176266372204, loss=0.042771950364112854
I0206 03:29:25.183015 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:31:05.557805 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:31:08.601390 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:31:11.670979 139978932307776 submission_runner.py:408] Time since start: 2540.76s, 	Step: 5269, 	{'train/accuracy': 0.9877853393554688, 'train/loss': 0.04363641142845154, 'train/mean_average_precision': 0.13495412738983195, 'validation/accuracy': 0.9847633838653564, 'validation/loss': 0.05361570790410042, 'validation/mean_average_precision': 0.1228188394742861, 'validation/num_examples': 43793, 'test/accuracy': 0.9838109016418457, 'test/loss': 0.056455958634614944, 'test/mean_average_precision': 0.128097338821601, 'test/num_examples': 43793, 'score': 1692.4349780082703, 'total_duration': 2540.757847547531, 'accumulated_submission_time': 1692.4349780082703, 'accumulated_eval_time': 847.9944367408752, 'accumulated_logging_time': 0.18788361549377441}
I0206 03:31:11.687134 139770185602816 logging_writer.py:48] [5269] accumulated_eval_time=847.994437, accumulated_logging_time=0.187884, accumulated_submission_time=1692.434978, global_step=5269, preemption_count=0, score=1692.434978, test/accuracy=0.983811, test/loss=0.056456, test/mean_average_precision=0.128097, test/num_examples=43793, total_duration=2540.757848, train/accuracy=0.987785, train/loss=0.043636, train/mean_average_precision=0.134954, validation/accuracy=0.984763, validation/loss=0.053616, validation/mean_average_precision=0.122819, validation/num_examples=43793
I0206 03:31:21.953889 139770797713152 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.028921455144882202, loss=0.04609525203704834
I0206 03:31:53.992984 139770185602816 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.053672485053539276, loss=0.03860137239098549
I0206 03:32:26.210886 139770797713152 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.04613567888736725, loss=0.04864611476659775
I0206 03:32:58.054290 139770185602816 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.021845053881406784, loss=0.04150179773569107
I0206 03:33:30.558616 139770797713152 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.03297065570950508, loss=0.04259180650115013
I0206 03:34:02.715940 139770185602816 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.03056749515235424, loss=0.04060470312833786
I0206 03:34:34.554170 139770797713152 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.029096752405166626, loss=0.0379265621304512
I0206 03:35:06.739643 139770185602816 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.014444613829255104, loss=0.0443703755736351
I0206 03:35:11.973660 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:36:55.342170 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:36:58.375098 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:37:01.427645 139978932307776 submission_runner.py:408] Time since start: 2890.51s, 	Step: 6017, 	{'train/accuracy': 0.9877686500549316, 'train/loss': 0.0430554561316967, 'train/mean_average_precision': 0.15247730794115238, 'validation/accuracy': 0.9850386381149292, 'validation/loss': 0.05250886082649231, 'validation/mean_average_precision': 0.13898969018160748, 'validation/num_examples': 43793, 'test/accuracy': 0.9840855598449707, 'test/loss': 0.05529408901929855, 'test/mean_average_precision': 0.14267761147463756, 'test/num_examples': 43793, 'score': 1932.6885635852814, 'total_duration': 2890.5146567821503, 'accumulated_submission_time': 1932.6885635852814, 'accumulated_eval_time': 957.4483652114868, 'accumulated_logging_time': 0.21676349639892578}
I0206 03:37:01.444231 139770806105856 logging_writer.py:48] [6017] accumulated_eval_time=957.448365, accumulated_logging_time=0.216763, accumulated_submission_time=1932.688564, global_step=6017, preemption_count=0, score=1932.688564, test/accuracy=0.984086, test/loss=0.055294, test/mean_average_precision=0.142678, test/num_examples=43793, total_duration=2890.514657, train/accuracy=0.987769, train/loss=0.043055, train/mean_average_precision=0.152477, validation/accuracy=0.985039, validation/loss=0.052509, validation/mean_average_precision=0.138990, validation/num_examples=43793
I0206 03:37:28.058720 139804984182528 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.035535018891096115, loss=0.04504447430372238
I0206 03:37:59.859108 139770806105856 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.028335316106677055, loss=0.03927747905254364
I0206 03:38:31.690361 139804984182528 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03086840733885765, loss=0.041787005960941315
I0206 03:39:03.668882 139770806105856 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.025489462539553642, loss=0.049152251332998276
I0206 03:39:35.642362 139804984182528 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.021848654374480247, loss=0.04452820122241974
I0206 03:40:07.405198 139770806105856 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.05170491710305214, loss=0.04119903966784477
I0206 03:40:39.351552 139804984182528 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.04182823374867439, loss=0.041219428181648254
I0206 03:41:01.638476 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:42:45.021193 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:42:48.107285 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:42:51.084439 139978932307776 submission_runner.py:408] Time since start: 3240.17s, 	Step: 6771, 	{'train/accuracy': 0.9881178140640259, 'train/loss': 0.04193282872438431, 'train/mean_average_precision': 0.14712715805639054, 'validation/accuracy': 0.9850714802742004, 'validation/loss': 0.051934532821178436, 'validation/mean_average_precision': 0.13690095667623864, 'validation/num_examples': 43793, 'test/accuracy': 0.984137773513794, 'test/loss': 0.054795846343040466, 'test/mean_average_precision': 0.13726670168943106, 'test/num_examples': 43793, 'score': 2172.8519797325134, 'total_duration': 3240.1714539527893, 'accumulated_submission_time': 2172.8519797325134, 'accumulated_eval_time': 1066.894276857376, 'accumulated_logging_time': 0.24420785903930664}
I0206 03:42:51.100624 139770797713152 logging_writer.py:48] [6771] accumulated_eval_time=1066.894277, accumulated_logging_time=0.244208, accumulated_submission_time=2172.851980, global_step=6771, preemption_count=0, score=2172.851980, test/accuracy=0.984138, test/loss=0.054796, test/mean_average_precision=0.137267, test/num_examples=43793, total_duration=3240.171454, train/accuracy=0.988118, train/loss=0.041933, train/mean_average_precision=0.147127, validation/accuracy=0.985071, validation/loss=0.051935, validation/mean_average_precision=0.136901, validation/num_examples=43793
I0206 03:43:00.681277 139916335904512 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.029019588604569435, loss=0.04053109884262085
I0206 03:43:32.804463 139770797713152 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.04114754498004913, loss=0.043413855135440826
I0206 03:44:05.006363 139916335904512 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.022471757605671883, loss=0.04423396661877632
I0206 03:44:36.937765 139770797713152 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.044702257961034775, loss=0.038369279354810715
I0206 03:45:08.835249 139916335904512 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0358150415122509, loss=0.03856451436877251
I0206 03:45:40.632016 139770797713152 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.06837282329797745, loss=0.044733911752700806
I0206 03:46:12.422173 139916335904512 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.05024093762040138, loss=0.04409228265285492
I0206 03:46:44.586588 139770797713152 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.027805065736174583, loss=0.04220457747578621
I0206 03:46:51.279530 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:48:31.844062 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:48:34.947960 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:48:37.938421 139978932307776 submission_runner.py:408] Time since start: 3587.03s, 	Step: 7522, 	{'train/accuracy': 0.9878849387168884, 'train/loss': 0.04250209033489227, 'train/mean_average_precision': 0.15903786419567645, 'validation/accuracy': 0.9852147698402405, 'validation/loss': 0.051785580813884735, 'validation/mean_average_precision': 0.13968376306762054, 'validation/num_examples': 43793, 'test/accuracy': 0.9842430949211121, 'test/loss': 0.054584402590990067, 'test/mean_average_precision': 0.13880957022061796, 'test/num_examples': 43793, 'score': 2412.9996078014374, 'total_duration': 3587.0254402160645, 'accumulated_submission_time': 2412.9996078014374, 'accumulated_eval_time': 1173.5531170368195, 'accumulated_logging_time': 0.2712841033935547}
I0206 03:48:37.955339 139770806105856 logging_writer.py:48] [7522] accumulated_eval_time=1173.553117, accumulated_logging_time=0.271284, accumulated_submission_time=2412.999608, global_step=7522, preemption_count=0, score=2412.999608, test/accuracy=0.984243, test/loss=0.054584, test/mean_average_precision=0.138810, test/num_examples=43793, total_duration=3587.025440, train/accuracy=0.987885, train/loss=0.042502, train/mean_average_precision=0.159038, validation/accuracy=0.985215, validation/loss=0.051786, validation/mean_average_precision=0.139684, validation/num_examples=43793
I0206 03:49:03.014610 139804984182528 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.029783163219690323, loss=0.04214004799723625
I0206 03:49:34.578244 139770806105856 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03308023884892464, loss=0.04453393444418907
I0206 03:50:06.148665 139804984182528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.06245231255888939, loss=0.039033278822898865
I0206 03:50:38.139588 139770806105856 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.022281985729932785, loss=0.04027939215302467
I0206 03:51:09.841578 139804984182528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.025801552459597588, loss=0.04126318544149399
I0206 03:51:41.497577 139770806105856 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.03617660328745842, loss=0.046511080116033554
I0206 03:52:13.569879 139804984182528 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.029281247407197952, loss=0.03761730715632439
I0206 03:52:38.135028 139978932307776 spec.py:321] Evaluating on the training split.
I0206 03:54:21.039095 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 03:54:24.118468 139978932307776 spec.py:349] Evaluating on the test split.
I0206 03:54:27.127077 139978932307776 submission_runner.py:408] Time since start: 3936.21s, 	Step: 8278, 	{'train/accuracy': 0.9880563616752625, 'train/loss': 0.041752468794584274, 'train/mean_average_precision': 0.16108226297426947, 'validation/accuracy': 0.9852578043937683, 'validation/loss': 0.05095980688929558, 'validation/mean_average_precision': 0.14882748193841347, 'validation/num_examples': 43793, 'test/accuracy': 0.9843167662620544, 'test/loss': 0.053699295967817307, 'test/mean_average_precision': 0.1497537212633665, 'test/num_examples': 43793, 'score': 2653.148269176483, 'total_duration': 3936.214086532593, 'accumulated_submission_time': 2653.148269176483, 'accumulated_eval_time': 1282.5451092720032, 'accumulated_logging_time': 0.2990598678588867}
I0206 03:54:27.144666 139770185602816 logging_writer.py:48] [8278] accumulated_eval_time=1282.545109, accumulated_logging_time=0.299060, accumulated_submission_time=2653.148269, global_step=8278, preemption_count=0, score=2653.148269, test/accuracy=0.984317, test/loss=0.053699, test/mean_average_precision=0.149754, test/num_examples=43793, total_duration=3936.214087, train/accuracy=0.988056, train/loss=0.041752, train/mean_average_precision=0.161082, validation/accuracy=0.985258, validation/loss=0.050960, validation/mean_average_precision=0.148827, validation/num_examples=43793
I0206 03:54:34.500353 139916335904512 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.08908326923847198, loss=0.03827785328030586
I0206 03:55:06.350768 139770185602816 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.04067113250494003, loss=0.039850179105997086
I0206 03:55:38.048952 139916335904512 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.026590073481202126, loss=0.04134105518460274
I0206 03:56:10.205394 139770185602816 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.0730319544672966, loss=0.04324940964579582
I0206 03:56:42.403470 139916335904512 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.049088701605796814, loss=0.04246466979384422
I0206 03:57:14.796673 139770185602816 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03498866781592369, loss=0.04498990997672081
I0206 03:57:47.096306 139916335904512 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.08398642390966415, loss=0.04252295941114426
I0206 03:58:19.188494 139770185602816 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.041800618171691895, loss=0.04461642727255821
I0206 03:58:27.218548 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:00:06.645570 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:00:09.783370 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:00:12.762486 139978932307776 submission_runner.py:408] Time since start: 4281.85s, 	Step: 9026, 	{'train/accuracy': 0.9882237315177917, 'train/loss': 0.04226386919617653, 'train/mean_average_precision': 0.1666925139943225, 'validation/accuracy': 0.9851611852645874, 'validation/loss': 0.05139685422182083, 'validation/mean_average_precision': 0.1470027461109916, 'validation/num_examples': 43793, 'test/accuracy': 0.9842881560325623, 'test/loss': 0.053785428404808044, 'test/mean_average_precision': 0.14674788369270744, 'test/num_examples': 43793, 'score': 2893.1906435489655, 'total_duration': 4281.849505901337, 'accumulated_submission_time': 2893.1906435489655, 'accumulated_eval_time': 1388.0890011787415, 'accumulated_logging_time': 0.3277096748352051}
I0206 04:00:12.779788 139770806105856 logging_writer.py:48] [9026] accumulated_eval_time=1388.089001, accumulated_logging_time=0.327710, accumulated_submission_time=2893.190644, global_step=9026, preemption_count=0, score=2893.190644, test/accuracy=0.984288, test/loss=0.053785, test/mean_average_precision=0.146748, test/num_examples=43793, total_duration=4281.849506, train/accuracy=0.988224, train/loss=0.042264, train/mean_average_precision=0.166693, validation/accuracy=0.985161, validation/loss=0.051397, validation/mean_average_precision=0.147003, validation/num_examples=43793
I0206 04:00:36.777219 139804984182528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.04108089208602905, loss=0.04385685920715332
I0206 04:01:08.746826 139770806105856 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.031456202268600464, loss=0.03920864686369896
I0206 04:01:40.914445 139804984182528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.036290254443883896, loss=0.045348573476076126
I0206 04:02:12.670767 139770806105856 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.05595150962471962, loss=0.044635698199272156
I0206 04:02:44.634701 139804984182528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.056044407188892365, loss=0.03811604529619217
I0206 04:03:16.597751 139770806105856 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.04487265273928642, loss=0.04417116194963455
I0206 04:03:48.424497 139804984182528 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.05120895057916641, loss=0.04378918558359146
I0206 04:04:12.962395 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:05:50.235006 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:05:53.272251 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:05:56.311913 139978932307776 submission_runner.py:408] Time since start: 4625.40s, 	Step: 9778, 	{'train/accuracy': 0.988153874874115, 'train/loss': 0.04126552864909172, 'train/mean_average_precision': 0.17768094811149476, 'validation/accuracy': 0.9853041172027588, 'validation/loss': 0.05078065022826195, 'validation/mean_average_precision': 0.1524563100207836, 'validation/num_examples': 43793, 'test/accuracy': 0.9843904972076416, 'test/loss': 0.05362384766340256, 'test/mean_average_precision': 0.15504898866218714, 'test/num_examples': 43793, 'score': 3133.3405344486237, 'total_duration': 4625.398817539215, 'accumulated_submission_time': 3133.3405344486237, 'accumulated_eval_time': 1491.4383640289307, 'accumulated_logging_time': 0.35793447494506836}
I0206 04:05:56.329648 139770185602816 logging_writer.py:48] [9778] accumulated_eval_time=1491.438364, accumulated_logging_time=0.357934, accumulated_submission_time=3133.340534, global_step=9778, preemption_count=0, score=3133.340534, test/accuracy=0.984390, test/loss=0.053624, test/mean_average_precision=0.155049, test/num_examples=43793, total_duration=4625.398818, train/accuracy=0.988154, train/loss=0.041266, train/mean_average_precision=0.177681, validation/accuracy=0.985304, validation/loss=0.050781, validation/mean_average_precision=0.152456, validation/num_examples=43793
I0206 04:06:03.963039 139916335904512 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.0414961576461792, loss=0.03816840797662735
I0206 04:06:35.910632 139770185602816 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.034806028008461, loss=0.040213774889707565
I0206 04:07:07.959899 139916335904512 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.06853823363780975, loss=0.04100928083062172
I0206 04:07:40.198045 139770185602816 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.04662127047777176, loss=0.04041651263833046
I0206 04:08:12.238854 139916335904512 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03188696503639221, loss=0.044545624405145645
I0206 04:08:44.215697 139770185602816 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.046712473034858704, loss=0.038849204778671265
I0206 04:09:16.037307 139916335904512 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.06066674739122391, loss=0.043642811477184296
I0206 04:09:48.239306 139770185602816 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.027288805693387985, loss=0.03933165222406387
I0206 04:09:56.391092 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:11:38.244115 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:11:41.237992 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:11:44.225292 139978932307776 submission_runner.py:408] Time since start: 4973.31s, 	Step: 10526, 	{'train/accuracy': 0.988370418548584, 'train/loss': 0.04057848080992699, 'train/mean_average_precision': 0.1764253451645966, 'validation/accuracy': 0.9853061437606812, 'validation/loss': 0.05046161636710167, 'validation/mean_average_precision': 0.15362740943123013, 'validation/num_examples': 43793, 'test/accuracy': 0.9843748807907104, 'test/loss': 0.05324826017022133, 'test/mean_average_precision': 0.16294833313941567, 'test/num_examples': 43793, 'score': 3373.36945104599, 'total_duration': 4973.312300443649, 'accumulated_submission_time': 3373.36945104599, 'accumulated_eval_time': 1599.2725040912628, 'accumulated_logging_time': 0.3884594440460205}
I0206 04:11:44.243260 139770797713152 logging_writer.py:48] [10526] accumulated_eval_time=1599.272504, accumulated_logging_time=0.388459, accumulated_submission_time=3373.369451, global_step=10526, preemption_count=0, score=3373.369451, test/accuracy=0.984375, test/loss=0.053248, test/mean_average_precision=0.162948, test/num_examples=43793, total_duration=4973.312300, train/accuracy=0.988370, train/loss=0.040578, train/mean_average_precision=0.176425, validation/accuracy=0.985306, validation/loss=0.050462, validation/mean_average_precision=0.153627, validation/num_examples=43793
I0206 04:12:08.305264 139804984182528 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.03255791589617729, loss=0.038782596588134766
I0206 04:12:40.679081 139770797713152 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.12085679173469543, loss=0.03957987204194069
I0206 04:13:12.431295 139804984182528 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05860501527786255, loss=0.040776778012514114
I0206 04:13:44.288244 139770797713152 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03882581368088722, loss=0.041867535561323166
I0206 04:14:16.208677 139804984182528 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.08746977895498276, loss=0.043299105018377304
I0206 04:14:47.720170 139770797713152 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.052248597145080566, loss=0.04487000033259392
I0206 04:15:19.275168 139804984182528 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.033262699842453, loss=0.03952091932296753
I0206 04:15:44.365653 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:17:22.399750 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:17:25.529029 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:17:28.663941 139978932307776 submission_runner.py:408] Time since start: 5317.75s, 	Step: 11280, 	{'train/accuracy': 0.9882692694664001, 'train/loss': 0.04072313383221626, 'train/mean_average_precision': 0.18064637046975113, 'validation/accuracy': 0.9853795766830444, 'validation/loss': 0.05063217505812645, 'validation/mean_average_precision': 0.15163249736390821, 'validation/num_examples': 43793, 'test/accuracy': 0.9844081997871399, 'test/loss': 0.053570620715618134, 'test/mean_average_precision': 0.15353083904258258, 'test/num_examples': 43793, 'score': 3613.459751367569, 'total_duration': 5317.750963449478, 'accumulated_submission_time': 3613.459751367569, 'accumulated_eval_time': 1703.5707485675812, 'accumulated_logging_time': 0.41864895820617676}
I0206 04:17:28.681427 139770806105856 logging_writer.py:48] [11280] accumulated_eval_time=1703.570749, accumulated_logging_time=0.418649, accumulated_submission_time=3613.459751, global_step=11280, preemption_count=0, score=3613.459751, test/accuracy=0.984408, test/loss=0.053571, test/mean_average_precision=0.153531, test/num_examples=43793, total_duration=5317.750963, train/accuracy=0.988269, train/loss=0.040723, train/mean_average_precision=0.180646, validation/accuracy=0.985380, validation/loss=0.050632, validation/mean_average_precision=0.151632, validation/num_examples=43793
I0206 04:17:35.522358 139916335904512 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.02973303385078907, loss=0.0406566821038723
I0206 04:18:07.628384 139770806105856 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04591725766658783, loss=0.040561456233263016
I0206 04:18:39.407603 139916335904512 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.04329676553606987, loss=0.040496744215488434
I0206 04:19:11.358317 139770806105856 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.027271775528788567, loss=0.041041869670152664
I0206 04:19:42.988156 139916335904512 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03300097957253456, loss=0.04425298422574997
I0206 04:20:14.718004 139770806105856 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.04022971913218498, loss=0.03655655309557915
I0206 04:20:46.685961 139916335904512 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.0609818734228611, loss=0.04545913264155388
I0206 04:21:18.506722 139770806105856 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.033603519201278687, loss=0.03955579549074173
I0206 04:21:28.990783 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:23:09.084493 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:23:12.176032 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:23:15.204665 139978932307776 submission_runner.py:408] Time since start: 5664.29s, 	Step: 12034, 	{'train/accuracy': 0.9883570671081543, 'train/loss': 0.040610287338495255, 'train/mean_average_precision': 0.18789730580368405, 'validation/accuracy': 0.9853954315185547, 'validation/loss': 0.05130927264690399, 'validation/mean_average_precision': 0.14694854662782258, 'validation/num_examples': 43793, 'test/accuracy': 0.9845050573348999, 'test/loss': 0.05422745645046234, 'test/mean_average_precision': 0.1533889459399407, 'test/num_examples': 43793, 'score': 3853.7374968528748, 'total_duration': 5664.29166841507, 'accumulated_submission_time': 3853.7374968528748, 'accumulated_eval_time': 1809.7845659255981, 'accumulated_logging_time': 0.4482748508453369}
I0206 04:23:15.223704 139770185602816 logging_writer.py:48] [12034] accumulated_eval_time=1809.784566, accumulated_logging_time=0.448275, accumulated_submission_time=3853.737497, global_step=12034, preemption_count=0, score=3853.737497, test/accuracy=0.984505, test/loss=0.054227, test/mean_average_precision=0.153389, test/num_examples=43793, total_duration=5664.291668, train/accuracy=0.988357, train/loss=0.040610, train/mean_average_precision=0.187897, validation/accuracy=0.985395, validation/loss=0.051309, validation/mean_average_precision=0.146949, validation/num_examples=43793
I0206 04:23:36.495721 139804984182528 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.029022542759776115, loss=0.039785951375961304
I0206 04:24:08.000903 139770185602816 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.023050790652632713, loss=0.03863140195608139
I0206 04:24:39.491495 139804984182528 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.06270307302474976, loss=0.04269964247941971
I0206 04:25:11.110318 139770185602816 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.04712213575839996, loss=0.03905470669269562
I0206 04:25:42.842173 139804984182528 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.029173702001571655, loss=0.0412076935172081
I0206 04:26:14.484209 139770185602816 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0332072377204895, loss=0.039256226271390915
I0206 04:26:46.093219 139804984182528 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.06135711446404457, loss=0.04350246116518974
I0206 04:27:15.275659 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:28:52.999538 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:28:56.146835 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:28:59.190950 139978932307776 submission_runner.py:408] Time since start: 6008.28s, 	Step: 12793, 	{'train/accuracy': 0.9884112477302551, 'train/loss': 0.04033699259161949, 'train/mean_average_precision': 0.18814700029006395, 'validation/accuracy': 0.9853706955909729, 'validation/loss': 0.05050535500049591, 'validation/mean_average_precision': 0.15826210688516798, 'validation/num_examples': 43793, 'test/accuracy': 0.9844494462013245, 'test/loss': 0.05334792658686638, 'test/mean_average_precision': 0.16042928879542304, 'test/num_examples': 43793, 'score': 4093.757108926773, 'total_duration': 6008.2779705524445, 'accumulated_submission_time': 4093.757108926773, 'accumulated_eval_time': 1913.6998341083527, 'accumulated_logging_time': 0.4794890880584717}
I0206 04:28:59.209741 139770797713152 logging_writer.py:48] [12793] accumulated_eval_time=1913.699834, accumulated_logging_time=0.479489, accumulated_submission_time=4093.757109, global_step=12793, preemption_count=0, score=4093.757109, test/accuracy=0.984449, test/loss=0.053348, test/mean_average_precision=0.160429, test/num_examples=43793, total_duration=6008.277971, train/accuracy=0.988411, train/loss=0.040337, train/mean_average_precision=0.188147, validation/accuracy=0.985371, validation/loss=0.050505, validation/mean_average_precision=0.158262, validation/num_examples=43793
I0206 04:29:01.854986 139770806105856 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.09451413154602051, loss=0.04081757366657257
I0206 04:29:33.424242 139770797713152 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.022314447909593582, loss=0.03886745497584343
I0206 04:30:05.136921 139770806105856 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.023927489295601845, loss=0.03919724375009537
I0206 04:30:36.805044 139770797713152 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04529033601284027, loss=0.035553254187107086
I0206 04:31:08.410641 139770806105856 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.040849026292562485, loss=0.03824930265545845
I0206 04:31:40.174763 139770797713152 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0696011409163475, loss=0.041478849947452545
I0206 04:32:11.870886 139770806105856 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0380701944231987, loss=0.039747267961502075
I0206 04:32:43.665552 139770797713152 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.041720956563949585, loss=0.04306305944919586
I0206 04:32:59.488035 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:34:38.009181 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:34:41.110713 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:34:44.155353 139978932307776 submission_runner.py:408] Time since start: 6353.24s, 	Step: 13551, 	{'train/accuracy': 0.9883461594581604, 'train/loss': 0.040721870958805084, 'train/mean_average_precision': 0.18812930461424757, 'validation/accuracy': 0.9853450655937195, 'validation/loss': 0.05106170102953911, 'validation/mean_average_precision': 0.16256916073948913, 'validation/num_examples': 43793, 'test/accuracy': 0.9844717979431152, 'test/loss': 0.05389748513698578, 'test/mean_average_precision': 0.15899843356600682, 'test/num_examples': 43793, 'score': 4334.002388477325, 'total_duration': 6353.242366313934, 'accumulated_submission_time': 4334.002388477325, 'accumulated_eval_time': 2018.3670988082886, 'accumulated_logging_time': 0.5114123821258545}
I0206 04:34:44.173435 139770185602816 logging_writer.py:48] [13551] accumulated_eval_time=2018.367099, accumulated_logging_time=0.511412, accumulated_submission_time=4334.002388, global_step=13551, preemption_count=0, score=4334.002388, test/accuracy=0.984472, test/loss=0.053897, test/mean_average_precision=0.158998, test/num_examples=43793, total_duration=6353.242366, train/accuracy=0.988346, train/loss=0.040722, train/mean_average_precision=0.188129, validation/accuracy=0.985345, validation/loss=0.051062, validation/mean_average_precision=0.162569, validation/num_examples=43793
I0206 04:34:59.970542 139916335904512 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03052285499870777, loss=0.043168243020772934
I0206 04:35:31.868987 139770185602816 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.07089634239673615, loss=0.03691131994128227
I0206 04:36:03.587964 139916335904512 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.034982312470674515, loss=0.04067710041999817
I0206 04:36:35.436335 139770185602816 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.08368517458438873, loss=0.04830770567059517
I0206 04:37:07.086522 139916335904512 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.10016646236181259, loss=0.039967410266399384
I0206 04:37:38.543412 139770185602816 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04943990707397461, loss=0.04172077775001526
I0206 04:38:10.200628 139916335904512 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.0568433478474617, loss=0.04623571038246155
I0206 04:38:41.850824 139770185602816 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03748665750026703, loss=0.04204953834414482
I0206 04:38:44.422383 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:40:26.989534 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:40:30.124631 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:40:33.231505 139978932307776 submission_runner.py:408] Time since start: 6702.32s, 	Step: 14309, 	{'train/accuracy': 0.9886543154716492, 'train/loss': 0.03959142789244652, 'train/mean_average_precision': 0.18865576898000522, 'validation/accuracy': 0.9855687618255615, 'validation/loss': 0.04963674396276474, 'validation/mean_average_precision': 0.16727042522727417, 'validation/num_examples': 43793, 'test/accuracy': 0.9845977425575256, 'test/loss': 0.0522046722471714, 'test/mean_average_precision': 0.17068206760017082, 'test/num_examples': 43793, 'score': 4574.218851804733, 'total_duration': 6702.318521976471, 'accumulated_submission_time': 4574.218851804733, 'accumulated_eval_time': 2127.1761713027954, 'accumulated_logging_time': 0.542165994644165}
I0206 04:40:33.250820 139770797713152 logging_writer.py:48] [14309] accumulated_eval_time=2127.176171, accumulated_logging_time=0.542166, accumulated_submission_time=4574.218852, global_step=14309, preemption_count=0, score=4574.218852, test/accuracy=0.984598, test/loss=0.052205, test/mean_average_precision=0.170682, test/num_examples=43793, total_duration=6702.318522, train/accuracy=0.988654, train/loss=0.039591, train/mean_average_precision=0.188656, validation/accuracy=0.985569, validation/loss=0.049637, validation/mean_average_precision=0.167270, validation/num_examples=43793
I0206 04:41:02.521754 139804984182528 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04131905362010002, loss=0.04471642151474953
I0206 04:41:34.026630 139770797713152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.05585794523358345, loss=0.038344498723745346
I0206 04:42:05.798251 139804984182528 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0361601859331131, loss=0.042926352471113205
I0206 04:42:37.731461 139770797713152 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.05883117392659187, loss=0.04299040883779526
I0206 04:43:09.725892 139804984182528 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.043793193995952606, loss=0.036978427320718765
I0206 04:43:41.489208 139770797713152 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.06042413413524628, loss=0.04169609025120735
I0206 04:44:12.892603 139804984182528 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.04798125848174095, loss=0.040702078491449356
I0206 04:44:33.353005 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:46:16.602204 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:46:19.703672 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:46:22.725695 139978932307776 submission_runner.py:408] Time since start: 7051.81s, 	Step: 15065, 	{'train/accuracy': 0.9882400631904602, 'train/loss': 0.04076105356216431, 'train/mean_average_precision': 0.1768932244139741, 'validation/accuracy': 0.98537677526474, 'validation/loss': 0.0505119189620018, 'validation/mean_average_precision': 0.16216779284240665, 'validation/num_examples': 43793, 'test/accuracy': 0.984438955783844, 'test/loss': 0.05335091054439545, 'test/mean_average_precision': 0.15989612033431252, 'test/num_examples': 43793, 'score': 4814.288403272629, 'total_duration': 7051.812611579895, 'accumulated_submission_time': 4814.288403272629, 'accumulated_eval_time': 2236.5487127304077, 'accumulated_logging_time': 0.5745348930358887}
I0206 04:46:22.744036 139752176375552 logging_writer.py:48] [15065] accumulated_eval_time=2236.548713, accumulated_logging_time=0.574535, accumulated_submission_time=4814.288403, global_step=15065, preemption_count=0, score=4814.288403, test/accuracy=0.984439, test/loss=0.053351, test/mean_average_precision=0.159896, test/num_examples=43793, total_duration=7051.812612, train/accuracy=0.988240, train/loss=0.040761, train/mean_average_precision=0.176893, validation/accuracy=0.985377, validation/loss=0.050512, validation/mean_average_precision=0.162168, validation/num_examples=43793
I0206 04:46:34.050733 139770185602816 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.02990904077887535, loss=0.037606190890073776
I0206 04:47:05.759572 139752176375552 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05287518352270126, loss=0.04095115140080452
I0206 04:47:37.600769 139770185602816 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.026403730735182762, loss=0.0441044420003891
I0206 04:48:09.460164 139752176375552 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03333524242043495, loss=0.03896043822169304
I0206 04:48:41.236529 139770185602816 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.058728791773319244, loss=0.04025770351290703
I0206 04:49:13.207613 139752176375552 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.029539939016103745, loss=0.041738275438547134
I0206 04:49:45.225200 139770185602816 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.05376545712351799, loss=0.04302503541111946
I0206 04:50:17.266667 139752176375552 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.039358921349048615, loss=0.04103006049990654
I0206 04:50:22.992704 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:52:02.428476 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:52:05.462994 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:52:08.394709 139978932307776 submission_runner.py:408] Time since start: 7397.48s, 	Step: 15819, 	{'train/accuracy': 0.9884235262870789, 'train/loss': 0.04031607508659363, 'train/mean_average_precision': 0.18005961443945234, 'validation/accuracy': 0.9854745864868164, 'validation/loss': 0.050167400389909744, 'validation/mean_average_precision': 0.15899198403434275, 'validation/num_examples': 43793, 'test/accuracy': 0.9844840168952942, 'test/loss': 0.05308358371257782, 'test/mean_average_precision': 0.16104881745609437, 'test/num_examples': 43793, 'score': 5054.5061230659485, 'total_duration': 7397.481730699539, 'accumulated_submission_time': 5054.5061230659485, 'accumulated_eval_time': 2341.950671672821, 'accumulated_logging_time': 0.6038038730621338}
I0206 04:52:08.413194 139770797713152 logging_writer.py:48] [15819] accumulated_eval_time=2341.950672, accumulated_logging_time=0.603804, accumulated_submission_time=5054.506123, global_step=15819, preemption_count=0, score=5054.506123, test/accuracy=0.984484, test/loss=0.053084, test/mean_average_precision=0.161049, test/num_examples=43793, total_duration=7397.481731, train/accuracy=0.988424, train/loss=0.040316, train/mean_average_precision=0.180060, validation/accuracy=0.985475, validation/loss=0.050167, validation/mean_average_precision=0.158992, validation/num_examples=43793
I0206 04:52:35.036252 139770806105856 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.028563490137457848, loss=0.038382645696401596
I0206 04:53:06.958674 139770797713152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.030290376394987106, loss=0.03494250029325485
I0206 04:53:38.579970 139770806105856 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.0674792006611824, loss=0.04482833668589592
I0206 04:54:10.563143 139770797713152 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.11753549426794052, loss=0.04052373394370079
I0206 04:54:42.015800 139770806105856 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.05219392105937004, loss=0.03742637485265732
I0206 04:55:13.543314 139770797713152 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.02921253629028797, loss=0.04371220991015434
I0206 04:55:45.074034 139770806105856 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.05673661455512047, loss=0.03548227250576019
I0206 04:56:08.570162 139978932307776 spec.py:321] Evaluating on the training split.
I0206 04:57:49.985785 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 04:57:53.023786 139978932307776 spec.py:349] Evaluating on the test split.
I0206 04:57:56.023941 139978932307776 submission_runner.py:408] Time since start: 7745.11s, 	Step: 16575, 	{'train/accuracy': 0.9884944558143616, 'train/loss': 0.040065258741378784, 'train/mean_average_precision': 0.19465300055209425, 'validation/accuracy': 0.9855245351791382, 'validation/loss': 0.04934633895754814, 'validation/mean_average_precision': 0.16485746697257292, 'validation/num_examples': 43793, 'test/accuracy': 0.9845366477966309, 'test/loss': 0.052238624542951584, 'test/mean_average_precision': 0.15859879831130447, 'test/num_examples': 43793, 'score': 5294.6321749687195, 'total_duration': 7745.110958576202, 'accumulated_submission_time': 5294.6321749687195, 'accumulated_eval_time': 2449.4044041633606, 'accumulated_logging_time': 0.6335453987121582}
I0206 04:57:56.043051 139752176375552 logging_writer.py:48] [16575] accumulated_eval_time=2449.404404, accumulated_logging_time=0.633545, accumulated_submission_time=5294.632175, global_step=16575, preemption_count=0, score=5294.632175, test/accuracy=0.984537, test/loss=0.052239, test/mean_average_precision=0.158599, test/num_examples=43793, total_duration=7745.110959, train/accuracy=0.988494, train/loss=0.040065, train/mean_average_precision=0.194653, validation/accuracy=0.985525, validation/loss=0.049346, validation/mean_average_precision=0.164857, validation/num_examples=43793
I0206 04:58:04.305843 139770185602816 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05006902292370796, loss=0.04097883030772209
I0206 04:58:35.704028 139752176375552 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.05460493266582489, loss=0.04531509056687355
I0206 04:59:07.415670 139770185602816 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.04612509533762932, loss=0.037852708250284195
I0206 04:59:38.999850 139752176375552 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.052125368267297745, loss=0.04119502007961273
I0206 05:00:10.392087 139770185602816 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0689723938703537, loss=0.04308787360787392
I0206 05:00:41.792177 139752176375552 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.0411083959043026, loss=0.03927430883049965
I0206 05:01:13.360563 139770185602816 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.055347807705402374, loss=0.038465466350317
I0206 05:01:44.698565 139752176375552 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03091043047606945, loss=0.040265053510665894
I0206 05:01:56.229143 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:03:33.586382 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:03:36.619363 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:03:39.578863 139978932307776 submission_runner.py:408] Time since start: 8088.67s, 	Step: 17338, 	{'train/accuracy': 0.9884665012359619, 'train/loss': 0.04012645035982132, 'train/mean_average_precision': 0.18912319578821066, 'validation/accuracy': 0.9855257272720337, 'validation/loss': 0.04987785592675209, 'validation/mean_average_precision': 0.16157510315203238, 'validation/num_examples': 43793, 'test/accuracy': 0.9845648407936096, 'test/loss': 0.0527358315885067, 'test/mean_average_precision': 0.16286102605605052, 'test/num_examples': 43793, 'score': 5534.787349462509, 'total_duration': 8088.665885448456, 'accumulated_submission_time': 5534.787349462509, 'accumulated_eval_time': 2552.754077911377, 'accumulated_logging_time': 0.6638550758361816}
I0206 05:03:39.598097 139770797713152 logging_writer.py:48] [17338] accumulated_eval_time=2552.754078, accumulated_logging_time=0.663855, accumulated_submission_time=5534.787349, global_step=17338, preemption_count=0, score=5534.787349, test/accuracy=0.984565, test/loss=0.052736, test/mean_average_precision=0.162861, test/num_examples=43793, total_duration=8088.665885, train/accuracy=0.988467, train/loss=0.040126, train/mean_average_precision=0.189123, validation/accuracy=0.985526, validation/loss=0.049878, validation/mean_average_precision=0.161575, validation/num_examples=43793
I0206 05:03:59.037840 139804984182528 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.046137094497680664, loss=0.03800927475094795
I0206 05:04:30.855287 139770797713152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.10306065529584885, loss=0.039427366107702255
I0206 05:05:02.410810 139804984182528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.022250946611166, loss=0.03853139653801918
I0206 05:05:33.624272 139770797713152 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.03764250501990318, loss=0.03501620516180992
I0206 05:06:04.960586 139804984182528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.044331151992082596, loss=0.036842867732048035
I0206 05:06:36.477067 139770797713152 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.05443313345313072, loss=0.03848910704255104
I0206 05:07:08.033879 139804984182528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.10183461755514145, loss=0.04622739553451538
I0206 05:07:39.425656 139770797713152 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08012958616018295, loss=0.039220962673425674
I0206 05:07:39.735482 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:09:16.596130 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:09:19.700436 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:09:24.864837 139978932307776 submission_runner.py:408] Time since start: 8433.95s, 	Step: 18102, 	{'train/accuracy': 0.9886404275894165, 'train/loss': 0.03960519656538963, 'train/mean_average_precision': 0.20165343855178597, 'validation/accuracy': 0.985596776008606, 'validation/loss': 0.04983937740325928, 'validation/mean_average_precision': 0.16603703175047954, 'validation/num_examples': 43793, 'test/accuracy': 0.9846120476722717, 'test/loss': 0.052671339362859726, 'test/mean_average_precision': 0.16606212858015526, 'test/num_examples': 43793, 'score': 5774.893808841705, 'total_duration': 8433.951842308044, 'accumulated_submission_time': 5774.893808841705, 'accumulated_eval_time': 2657.8833651542664, 'accumulated_logging_time': 0.6942009925842285}
I0206 05:09:24.884885 139752176375552 logging_writer.py:48] [18102] accumulated_eval_time=2657.883365, accumulated_logging_time=0.694201, accumulated_submission_time=5774.893809, global_step=18102, preemption_count=0, score=5774.893809, test/accuracy=0.984612, test/loss=0.052671, test/mean_average_precision=0.166062, test/num_examples=43793, total_duration=8433.951842, train/accuracy=0.988640, train/loss=0.039605, train/mean_average_precision=0.201653, validation/accuracy=0.985597, validation/loss=0.049839, validation/mean_average_precision=0.166037, validation/num_examples=43793
I0206 05:09:56.442328 139770806105856 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.03569047525525093, loss=0.04177306592464447
I0206 05:10:28.038149 139752176375552 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.05457016080617905, loss=0.040501631796360016
I0206 05:10:59.794897 139770806105856 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.037163496017456055, loss=0.037930652499198914
I0206 05:11:31.551830 139752176375552 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.028200600296258926, loss=0.03686127811670303
I0206 05:12:03.408933 139770806105856 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.05943765491247177, loss=0.04100105166435242
I0206 05:12:35.262167 139752176375552 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.038889504969120026, loss=0.03649775683879852
I0206 05:13:06.939676 139770806105856 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.05779515579342842, loss=0.03762217238545418
I0206 05:13:25.005518 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:15:05.793288 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:15:08.802975 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:15:11.741829 139978932307776 submission_runner.py:408] Time since start: 8780.83s, 	Step: 18858, 	{'train/accuracy': 0.9884719252586365, 'train/loss': 0.04005434364080429, 'train/mean_average_precision': 0.18568562201382344, 'validation/accuracy': 0.9854221940040588, 'validation/loss': 0.05040871351957321, 'validation/mean_average_precision': 0.15669814211524635, 'validation/num_examples': 43793, 'test/accuracy': 0.9844696521759033, 'test/loss': 0.05345328897237778, 'test/mean_average_precision': 0.15697611739618625, 'test/num_examples': 43793, 'score': 6014.983092546463, 'total_duration': 8780.828848361969, 'accumulated_submission_time': 6014.983092546463, 'accumulated_eval_time': 2764.619640350342, 'accumulated_logging_time': 0.7254533767700195}
I0206 05:15:11.760388 139770185602816 logging_writer.py:48] [18858] accumulated_eval_time=2764.619640, accumulated_logging_time=0.725453, accumulated_submission_time=6014.983093, global_step=18858, preemption_count=0, score=6014.983093, test/accuracy=0.984470, test/loss=0.053453, test/mean_average_precision=0.156976, test/num_examples=43793, total_duration=8780.828848, train/accuracy=0.988472, train/loss=0.040054, train/mean_average_precision=0.185686, validation/accuracy=0.985422, validation/loss=0.050409, validation/mean_average_precision=0.156698, validation/num_examples=43793
I0206 05:15:25.310227 139770797713152 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.06813735514879227, loss=0.04062158986926079
I0206 05:15:57.142392 139770185602816 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04239825904369354, loss=0.04008488357067108
I0206 05:16:28.744568 139770797713152 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04589585214853287, loss=0.04086170718073845
I0206 05:17:00.612017 139770185602816 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.06512637436389923, loss=0.04439496621489525
I0206 05:17:32.212913 139770797713152 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.025654885917901993, loss=0.04348519816994667
I0206 05:18:03.882054 139770185602816 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.031419284641742706, loss=0.03899858891963959
I0206 05:18:35.450766 139770797713152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.035256173461675644, loss=0.04002976417541504
I0206 05:19:06.807211 139770185602816 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.05701829120516777, loss=0.04095657169818878
I0206 05:19:11.916763 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:20:47.575451 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:20:50.638521 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:20:53.597689 139978932307776 submission_runner.py:408] Time since start: 9122.68s, 	Step: 19617, 	{'train/accuracy': 0.9885842204093933, 'train/loss': 0.039523422718048096, 'train/mean_average_precision': 0.19490424779512652, 'validation/accuracy': 0.9855241179466248, 'validation/loss': 0.05002894625067711, 'validation/mean_average_precision': 0.1680505834962828, 'validation/num_examples': 43793, 'test/accuracy': 0.9845610857009888, 'test/loss': 0.052960846573114395, 'test/mean_average_precision': 0.16620601517443098, 'test/num_examples': 43793, 'score': 6255.108852148056, 'total_duration': 9122.684712648392, 'accumulated_submission_time': 6255.108852148056, 'accumulated_eval_time': 2866.3005216121674, 'accumulated_logging_time': 0.7550556659698486}
I0206 05:20:53.619299 139752176375552 logging_writer.py:48] [19617] accumulated_eval_time=2866.300522, accumulated_logging_time=0.755056, accumulated_submission_time=6255.108852, global_step=19617, preemption_count=0, score=6255.108852, test/accuracy=0.984561, test/loss=0.052961, test/mean_average_precision=0.166206, test/num_examples=43793, total_duration=9122.684713, train/accuracy=0.988584, train/loss=0.039523, train/mean_average_precision=0.194904, validation/accuracy=0.985524, validation/loss=0.050029, validation/mean_average_precision=0.168051, validation/num_examples=43793
I0206 05:21:20.152907 139770806105856 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.06391672044992447, loss=0.036653582006692886
I0206 05:21:51.512468 139752176375552 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.029646199196577072, loss=0.037826504558324814
I0206 05:22:23.765595 139770806105856 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.025551754981279373, loss=0.03933127224445343
I0206 05:22:56.018193 139752176375552 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.025012848898768425, loss=0.035805873572826385
I0206 05:23:27.988719 139770806105856 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.043627429753541946, loss=0.03594072535634041
I0206 05:23:59.879427 139752176375552 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.045095447450876236, loss=0.03454574942588806
I0206 05:24:31.873137 139770806105856 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.03528590500354767, loss=0.03933602198958397
I0206 05:24:53.873887 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:26:37.019342 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:26:40.019099 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:26:43.045620 139978932307776 submission_runner.py:408] Time since start: 9472.13s, 	Step: 20370, 	{'train/accuracy': 0.9885866641998291, 'train/loss': 0.04034075513482094, 'train/mean_average_precision': 0.19055236333307302, 'validation/accuracy': 0.985431969165802, 'validation/loss': 0.049722012132406235, 'validation/mean_average_precision': 0.16182988616494326, 'validation/num_examples': 43793, 'test/accuracy': 0.984565258026123, 'test/loss': 0.052569009363651276, 'test/mean_average_precision': 0.16090210738048638, 'test/num_examples': 43793, 'score': 6495.332993984222, 'total_duration': 9472.132626056671, 'accumulated_submission_time': 6495.332993984222, 'accumulated_eval_time': 2975.4721944332123, 'accumulated_logging_time': 0.787273645401001}
I0206 05:26:43.064867 139770185602816 logging_writer.py:48] [20370] accumulated_eval_time=2975.472194, accumulated_logging_time=0.787274, accumulated_submission_time=6495.332994, global_step=20370, preemption_count=0, score=6495.332994, test/accuracy=0.984565, test/loss=0.052569, test/mean_average_precision=0.160902, test/num_examples=43793, total_duration=9472.132626, train/accuracy=0.988587, train/loss=0.040341, train/mean_average_precision=0.190552, validation/accuracy=0.985432, validation/loss=0.049722, validation/mean_average_precision=0.161830, validation/num_examples=43793
I0206 05:26:52.766904 139770797713152 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.09305106103420258, loss=0.04089194908738136
I0206 05:27:24.331963 139770185602816 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.02631993405520916, loss=0.042933203279972076
I0206 05:27:55.953164 139770797713152 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.10213540494441986, loss=0.03262291103601456
I0206 05:28:27.380284 139770185602816 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.03546079993247986, loss=0.042714159935712814
I0206 05:28:58.857391 139770797713152 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.043929096311330795, loss=0.03848619386553764
I0206 05:29:30.662119 139770185602816 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.1280762106180191, loss=0.03913547098636627
I0206 05:30:02.630217 139770797713152 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05428014695644379, loss=0.04136371985077858
I0206 05:30:34.416543 139770185602816 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.03553811460733414, loss=0.03967512771487236
I0206 05:30:43.147242 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:32:18.895367 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:32:21.906562 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:32:24.956001 139978932307776 submission_runner.py:408] Time since start: 9814.04s, 	Step: 21128, 	{'train/accuracy': 0.9888412952423096, 'train/loss': 0.03901582956314087, 'train/mean_average_precision': 0.19844235275635128, 'validation/accuracy': 0.9855196475982666, 'validation/loss': 0.04963627830147743, 'validation/mean_average_precision': 0.1704775937419216, 'validation/num_examples': 43793, 'test/accuracy': 0.9846183657646179, 'test/loss': 0.05252900719642639, 'test/mean_average_precision': 0.16738075430890176, 'test/num_examples': 43793, 'score': 6735.384921073914, 'total_duration': 9814.043020009995, 'accumulated_submission_time': 6735.384921073914, 'accumulated_eval_time': 3077.2809023857117, 'accumulated_logging_time': 0.8171370029449463}
I0206 05:32:24.976777 139770806105856 logging_writer.py:48] [21128] accumulated_eval_time=3077.280902, accumulated_logging_time=0.817137, accumulated_submission_time=6735.384921, global_step=21128, preemption_count=0, score=6735.384921, test/accuracy=0.984618, test/loss=0.052529, test/mean_average_precision=0.167381, test/num_examples=43793, total_duration=9814.043020, train/accuracy=0.988841, train/loss=0.039016, train/mean_average_precision=0.198442, validation/accuracy=0.985520, validation/loss=0.049636, validation/mean_average_precision=0.170478, validation/num_examples=43793
I0206 05:32:48.174832 139804984182528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.05499335378408432, loss=0.039250873029232025
I0206 05:33:19.644051 139770806105856 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.024489127099514008, loss=0.039003413170576096
I0206 05:33:51.185762 139804984182528 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.05124904215335846, loss=0.03923172876238823
I0206 05:34:22.971993 139770806105856 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.10221081972122192, loss=0.04408939182758331
I0206 05:34:54.283298 139804984182528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.06116105616092682, loss=0.04058822989463806
I0206 05:35:25.885138 139770806105856 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.07934924215078354, loss=0.0355570912361145
I0206 05:35:57.673581 139804984182528 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.06851133704185486, loss=0.04166136682033539
I0206 05:36:24.968554 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:38:06.820036 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:38:09.826887 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:38:12.865819 139978932307776 submission_runner.py:408] Time since start: 10161.95s, 	Step: 21888, 	{'train/accuracy': 0.988599419593811, 'train/loss': 0.0396040715277195, 'train/mean_average_precision': 0.1955595368230457, 'validation/accuracy': 0.9856069087982178, 'validation/loss': 0.04932992532849312, 'validation/mean_average_precision': 0.17090716974554493, 'validation/num_examples': 43793, 'test/accuracy': 0.9847084879875183, 'test/loss': 0.05207694321870804, 'test/mean_average_precision': 0.16870305237778235, 'test/num_examples': 43793, 'score': 6975.345520019531, 'total_duration': 10161.952837228775, 'accumulated_submission_time': 6975.345520019531, 'accumulated_eval_time': 3185.1781203746796, 'accumulated_logging_time': 0.8489246368408203}
I0206 05:38:12.885603 139770185602816 logging_writer.py:48] [21888] accumulated_eval_time=3185.178120, accumulated_logging_time=0.848925, accumulated_submission_time=6975.345520, global_step=21888, preemption_count=0, score=6975.345520, test/accuracy=0.984708, test/loss=0.052077, test/mean_average_precision=0.168703, test/num_examples=43793, total_duration=10161.952837, train/accuracy=0.988599, train/loss=0.039604, train/mean_average_precision=0.195560, validation/accuracy=0.985607, validation/loss=0.049330, validation/mean_average_precision=0.170907, validation/num_examples=43793
I0206 05:38:17.138630 139770797713152 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.07820615917444229, loss=0.043005939573049545
I0206 05:38:49.522058 139770185602816 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08259748667478561, loss=0.04042268171906471
I0206 05:39:21.386909 139770797713152 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.11680598556995392, loss=0.042407531291246414
I0206 05:39:53.224705 139770185602816 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03556704521179199, loss=0.041418541222810745
I0206 05:40:25.047056 139770797713152 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04465601593255997, loss=0.041500505059957504
I0206 05:40:56.972195 139770185602816 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.10166703909635544, loss=0.04209863767027855
I0206 05:41:28.880010 139770797713152 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.03910844773054123, loss=0.04025300219655037
I0206 05:42:00.664433 139770185602816 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.03601565957069397, loss=0.040666114538908005
I0206 05:42:12.956640 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:43:53.381089 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:43:56.421807 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:43:59.461891 139978932307776 submission_runner.py:408] Time since start: 10508.55s, 	Step: 22640, 	{'train/accuracy': 0.9885961413383484, 'train/loss': 0.03991701081395149, 'train/mean_average_precision': 0.19312791976085006, 'validation/accuracy': 0.9854997396469116, 'validation/loss': 0.04980744421482086, 'validation/mean_average_precision': 0.16469691309091544, 'validation/num_examples': 43793, 'test/accuracy': 0.9846465587615967, 'test/loss': 0.05239691212773323, 'test/mean_average_precision': 0.16187797766191273, 'test/num_examples': 43793, 'score': 7215.383780956268, 'total_duration': 10508.548904895782, 'accumulated_submission_time': 7215.383780956268, 'accumulated_eval_time': 3291.6833214759827, 'accumulated_logging_time': 0.8813366889953613}
I0206 05:43:59.487286 139752176375552 logging_writer.py:48] [22640] accumulated_eval_time=3291.683321, accumulated_logging_time=0.881337, accumulated_submission_time=7215.383781, global_step=22640, preemption_count=0, score=7215.383781, test/accuracy=0.984647, test/loss=0.052397, test/mean_average_precision=0.161878, test/num_examples=43793, total_duration=10508.548905, train/accuracy=0.988596, train/loss=0.039917, train/mean_average_precision=0.193128, validation/accuracy=0.985500, validation/loss=0.049807, validation/mean_average_precision=0.164697, validation/num_examples=43793
I0206 05:44:18.751529 139804984182528 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04599504545331001, loss=0.03659820556640625
I0206 05:44:50.349622 139752176375552 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.0558684840798378, loss=0.0403602309525013
I0206 05:45:22.003666 139804984182528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.040327925235033035, loss=0.03933292254805565
I0206 05:45:53.872489 139752176375552 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.09162715077400208, loss=0.03860797733068466
I0206 05:46:25.631403 139804984182528 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.047371573746204376, loss=0.03891020640730858
I0206 05:46:57.622471 139752176375552 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.06071579083800316, loss=0.039582837373018265
I0206 05:47:29.411468 139804984182528 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0342443473637104, loss=0.040918655693531036
I0206 05:47:59.600619 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:49:39.091387 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:49:42.169970 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:49:45.118828 139978932307776 submission_runner.py:408] Time since start: 10854.21s, 	Step: 23395, 	{'train/accuracy': 0.9885261058807373, 'train/loss': 0.039845407009124756, 'train/mean_average_precision': 0.20450545013479177, 'validation/accuracy': 0.9855204820632935, 'validation/loss': 0.04962068051099777, 'validation/mean_average_precision': 0.1696779981251334, 'validation/num_examples': 43793, 'test/accuracy': 0.9845758080482483, 'test/loss': 0.05225157365202904, 'test/mean_average_precision': 0.16689131979029878, 'test/num_examples': 43793, 'score': 7455.464512586594, 'total_duration': 10854.205847978592, 'accumulated_submission_time': 7455.464512586594, 'accumulated_eval_time': 3397.2014861106873, 'accumulated_logging_time': 0.9188354015350342}
I0206 05:49:45.138451 139770797713152 logging_writer.py:48] [23395] accumulated_eval_time=3397.201486, accumulated_logging_time=0.918835, accumulated_submission_time=7455.464513, global_step=23395, preemption_count=0, score=7455.464513, test/accuracy=0.984576, test/loss=0.052252, test/mean_average_precision=0.166891, test/num_examples=43793, total_duration=10854.205848, train/accuracy=0.988526, train/loss=0.039845, train/mean_average_precision=0.204505, validation/accuracy=0.985520, validation/loss=0.049621, validation/mean_average_precision=0.169678, validation/num_examples=43793
I0206 05:49:47.039156 139770806105856 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.07604525983333588, loss=0.035814978182315826
I0206 05:50:18.619766 139770797713152 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.05362377688288689, loss=0.04466627538204193
I0206 05:50:50.690042 139770806105856 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.05312288552522659, loss=0.03554492071270943
I0206 05:51:22.430358 139770797713152 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.04675062373280525, loss=0.04134652763605118
I0206 05:51:54.188405 139770806105856 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04286583885550499, loss=0.04001275449991226
I0206 05:52:26.093020 139770797713152 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.06558863073587418, loss=0.0398772731423378
I0206 05:52:57.851273 139770806105856 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.048435233533382416, loss=0.040714021772146225
I0206 05:53:29.704080 139770797713152 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.06699298322200775, loss=0.04141790792346001
I0206 05:53:45.185198 139978932307776 spec.py:321] Evaluating on the training split.
I0206 05:55:22.653481 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 05:55:25.665953 139978932307776 spec.py:349] Evaluating on the test split.
I0206 05:55:28.669314 139978932307776 submission_runner.py:408] Time since start: 11197.76s, 	Step: 24150, 	{'train/accuracy': 0.9885877370834351, 'train/loss': 0.03958648443222046, 'train/mean_average_precision': 0.198987881596415, 'validation/accuracy': 0.9856178760528564, 'validation/loss': 0.04945551976561546, 'validation/mean_average_precision': 0.17012725483797447, 'validation/num_examples': 43793, 'test/accuracy': 0.9847046732902527, 'test/loss': 0.052401501685380936, 'test/mean_average_precision': 0.16773314797260425, 'test/num_examples': 43793, 'score': 7695.480072259903, 'total_duration': 11197.75633430481, 'accumulated_submission_time': 7695.480072259903, 'accumulated_eval_time': 3500.685555458069, 'accumulated_logging_time': 0.9494316577911377}
I0206 05:55:28.689827 139752176375552 logging_writer.py:48] [24150] accumulated_eval_time=3500.685555, accumulated_logging_time=0.949432, accumulated_submission_time=7695.480072, global_step=24150, preemption_count=0, score=7695.480072, test/accuracy=0.984705, test/loss=0.052402, test/mean_average_precision=0.167733, test/num_examples=43793, total_duration=11197.756334, train/accuracy=0.988588, train/loss=0.039586, train/mean_average_precision=0.198988, validation/accuracy=0.985618, validation/loss=0.049456, validation/mean_average_precision=0.170127, validation/num_examples=43793
I0206 05:55:45.143909 139804984182528 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.04464273154735565, loss=0.03979572653770447
I0206 05:56:17.100294 139752176375552 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.057955507189035416, loss=0.034196436405181885
I0206 05:56:49.429240 139804984182528 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.07462209463119507, loss=0.03324040398001671
I0206 05:57:21.399013 139752176375552 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05557824298739433, loss=0.0380328968167305
I0206 05:57:53.745826 139804984182528 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.08114984631538391, loss=0.03809672221541405
I0206 05:58:26.088104 139752176375552 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.05828722193837166, loss=0.044495534151792526
I0206 05:58:58.390145 139804984182528 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.048428863286972046, loss=0.033692866563797
I0206 05:59:28.692051 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:01:06.799729 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:01:12.246062 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:01:15.397058 139978932307776 submission_runner.py:408] Time since start: 11544.48s, 	Step: 24895, 	{'train/accuracy': 0.9885725378990173, 'train/loss': 0.039620619267225266, 'train/mean_average_precision': 0.18849833988280928, 'validation/accuracy': 0.9855935573577881, 'validation/loss': 0.049551356583833694, 'validation/mean_average_precision': 0.16380789915553823, 'validation/num_examples': 43793, 'test/accuracy': 0.9846591949462891, 'test/loss': 0.05243632197380066, 'test/mean_average_precision': 0.16223804286958965, 'test/num_examples': 43793, 'score': 7935.451113462448, 'total_duration': 11544.484077215195, 'accumulated_submission_time': 7935.451113462448, 'accumulated_eval_time': 3607.390516757965, 'accumulated_logging_time': 0.980963945388794}
I0206 06:01:15.417798 139770185602816 logging_writer.py:48] [24895] accumulated_eval_time=3607.390517, accumulated_logging_time=0.980964, accumulated_submission_time=7935.451113, global_step=24895, preemption_count=0, score=7935.451113, test/accuracy=0.984659, test/loss=0.052436, test/mean_average_precision=0.162238, test/num_examples=43793, total_duration=11544.484077, train/accuracy=0.988573, train/loss=0.039621, train/mean_average_precision=0.188498, validation/accuracy=0.985594, validation/loss=0.049551, validation/mean_average_precision=0.163808, validation/num_examples=43793
I0206 06:01:17.367937 139770806105856 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05963468179106712, loss=0.04201890155673027
I0206 06:01:49.547452 139770185602816 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.054537590593099594, loss=0.0381314791738987
I0206 06:02:21.802500 139770806105856 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.04131412133574486, loss=0.03719469532370567
I0206 06:02:53.976087 139770185602816 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.061264097690582275, loss=0.04074287414550781
I0206 06:03:25.976885 139770806105856 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03627358376979828, loss=0.037431519478559494
I0206 06:03:57.597724 139770185602816 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.053970739245414734, loss=0.03789907321333885
I0206 06:04:29.429997 139770806105856 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.038870811462402344, loss=0.034827686846256256
I0206 06:05:01.269422 139770185602816 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.039801761507987976, loss=0.039196886122226715
I0206 06:05:15.515118 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:06:55.497925 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:06:58.593442 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:07:01.598926 139978932307776 submission_runner.py:408] Time since start: 11890.69s, 	Step: 25646, 	{'train/accuracy': 0.9884662628173828, 'train/loss': 0.0406963936984539, 'train/mean_average_precision': 0.18891245798950673, 'validation/accuracy': 0.9852277636528015, 'validation/loss': 0.05177460238337517, 'validation/mean_average_precision': 0.1620684550428439, 'validation/num_examples': 43793, 'test/accuracy': 0.9842435121536255, 'test/loss': 0.05533933266997337, 'test/mean_average_precision': 0.14969740247395008, 'test/num_examples': 43793, 'score': 8175.517338037491, 'total_duration': 11890.685946941376, 'accumulated_submission_time': 8175.517338037491, 'accumulated_eval_time': 3713.474276781082, 'accumulated_logging_time': 1.012589931488037}
I0206 06:07:01.619857 139752176375552 logging_writer.py:48] [25646] accumulated_eval_time=3713.474277, accumulated_logging_time=1.012590, accumulated_submission_time=8175.517338, global_step=25646, preemption_count=0, score=8175.517338, test/accuracy=0.984244, test/loss=0.055339, test/mean_average_precision=0.149697, test/num_examples=43793, total_duration=11890.685947, train/accuracy=0.988466, train/loss=0.040696, train/mean_average_precision=0.188912, validation/accuracy=0.985228, validation/loss=0.051775, validation/mean_average_precision=0.162068, validation/num_examples=43793
I0206 06:07:19.080049 139804984182528 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.06848493963479996, loss=0.044068191200494766
I0206 06:07:51.088328 139752176375552 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.06455940008163452, loss=0.04116518795490265
I0206 06:08:23.213809 139804984182528 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.04957214370369911, loss=0.04283491522073746
I0206 06:08:55.149555 139752176375552 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.04429525136947632, loss=0.03834770992398262
I0206 06:09:27.038439 139804984182528 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.056207749992609024, loss=0.04459235444664955
I0206 06:09:58.658528 139752176375552 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03396564722061157, loss=0.040521975606679916
I0206 06:10:31.419012 139804984182528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.03403138369321823, loss=0.03732816129922867
I0206 06:11:01.684997 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:12:40.643378 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:12:43.643667 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:12:46.575443 139978932307776 submission_runner.py:408] Time since start: 12235.66s, 	Step: 26396, 	{'train/accuracy': 0.9886945486068726, 'train/loss': 0.039120517671108246, 'train/mean_average_precision': 0.20767401350788825, 'validation/accuracy': 0.9855862259864807, 'validation/loss': 0.04942043125629425, 'validation/mean_average_precision': 0.16990612842383535, 'validation/num_examples': 43793, 'test/accuracy': 0.9846705794334412, 'test/loss': 0.0522284172475338, 'test/mean_average_precision': 0.1665207318270333, 'test/num_examples': 43793, 'score': 8415.550181627274, 'total_duration': 12235.662456035614, 'accumulated_submission_time': 8415.550181627274, 'accumulated_eval_time': 3818.3646759986877, 'accumulated_logging_time': 1.0460069179534912}
I0206 06:12:46.595699 139770185602816 logging_writer.py:48] [26396] accumulated_eval_time=3818.364676, accumulated_logging_time=1.046007, accumulated_submission_time=8415.550182, global_step=26396, preemption_count=0, score=8415.550182, test/accuracy=0.984671, test/loss=0.052228, test/mean_average_precision=0.166521, test/num_examples=43793, total_duration=12235.662456, train/accuracy=0.988695, train/loss=0.039121, train/mean_average_precision=0.207674, validation/accuracy=0.985586, validation/loss=0.049420, validation/mean_average_precision=0.169906, validation/num_examples=43793
I0206 06:12:48.191023 139770797713152 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.06898128986358643, loss=0.04067806899547577
I0206 06:13:19.569498 139770185602816 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04341786354780197, loss=0.03772677481174469
I0206 06:13:51.084645 139770797713152 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.03815890848636627, loss=0.03785368427634239
I0206 06:14:22.467048 139770185602816 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.08077079802751541, loss=0.04183467477560043
I0206 06:14:54.041251 139770797713152 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.040659237653017044, loss=0.04202340543270111
I0206 06:15:25.759868 139770185602816 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08843372762203217, loss=0.04559343680739403
I0206 06:15:57.433290 139770797713152 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0533418133854866, loss=0.03847517445683479
I0206 06:16:29.174458 139770185602816 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.057815343141555786, loss=0.041868627071380615
I0206 06:16:46.807678 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:18:26.678780 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:18:29.671457 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:18:32.617100 139978932307776 submission_runner.py:408] Time since start: 12581.70s, 	Step: 27156, 	{'train/accuracy': 0.9885122776031494, 'train/loss': 0.03939487785100937, 'train/mean_average_precision': 0.2045708442516365, 'validation/accuracy': 0.9854699373245239, 'validation/loss': 0.04987451806664467, 'validation/mean_average_precision': 0.17688852857115214, 'validation/num_examples': 43793, 'test/accuracy': 0.9845792055130005, 'test/loss': 0.05277697369456291, 'test/mean_average_precision': 0.17506006175059088, 'test/num_examples': 43793, 'score': 8655.730852127075, 'total_duration': 12581.70412158966, 'accumulated_submission_time': 8655.730852127075, 'accumulated_eval_time': 3924.174050092697, 'accumulated_logging_time': 1.0775668621063232}
I0206 06:18:32.637376 139752176375552 logging_writer.py:48] [27156] accumulated_eval_time=3924.174050, accumulated_logging_time=1.077567, accumulated_submission_time=8655.730852, global_step=27156, preemption_count=0, score=8655.730852, test/accuracy=0.984579, test/loss=0.052777, test/mean_average_precision=0.175060, test/num_examples=43793, total_duration=12581.704122, train/accuracy=0.988512, train/loss=0.039395, train/mean_average_precision=0.204571, validation/accuracy=0.985470, validation/loss=0.049875, validation/mean_average_precision=0.176889, validation/num_examples=43793
I0206 06:18:46.926661 139770806105856 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04832910746335983, loss=0.0372629277408123
I0206 06:19:18.692755 139752176375552 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.03304072842001915, loss=0.03543456643819809
I0206 06:19:50.257184 139770806105856 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.04332299530506134, loss=0.0419088751077652
I0206 06:20:21.825314 139752176375552 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06414057314395905, loss=0.04370503127574921
I0206 06:20:53.379928 139770806105856 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.12024657428264618, loss=0.03768407925963402
I0206 06:21:25.244686 139752176375552 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.046758051961660385, loss=0.04072786867618561
I0206 06:21:57.108233 139770806105856 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.04744367301464081, loss=0.03968893364071846
I0206 06:22:29.122160 139752176375552 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.31409507989883423, loss=0.06294956058263779
I0206 06:22:32.923854 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:24:13.625011 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:24:16.638843 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:24:19.589392 139978932307776 submission_runner.py:408] Time since start: 12928.68s, 	Step: 27913, 	{'train/accuracy': 0.9870010018348694, 'train/loss': 0.04721558466553688, 'train/mean_average_precision': 0.11037968819824376, 'validation/accuracy': 0.9840448498725891, 'validation/loss': 0.05742224305868149, 'validation/mean_average_precision': 0.09850573291118715, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374287605286, 'test/loss': 0.06042836606502533, 'test/mean_average_precision': 0.10381847942793537, 'test/num_examples': 43793, 'score': 8895.984867811203, 'total_duration': 12928.676401853561, 'accumulated_submission_time': 8895.984867811203, 'accumulated_eval_time': 4030.839526414871, 'accumulated_logging_time': 1.110039234161377}
I0206 06:24:19.611209 139770185602816 logging_writer.py:48] [27913] accumulated_eval_time=4030.839526, accumulated_logging_time=1.110039, accumulated_submission_time=8895.984868, global_step=27913, preemption_count=0, score=8895.984868, test/accuracy=0.983137, test/loss=0.060428, test/mean_average_precision=0.103818, test/num_examples=43793, total_duration=12928.676402, train/accuracy=0.987001, train/loss=0.047216, train/mean_average_precision=0.110380, validation/accuracy=0.984045, validation/loss=0.057422, validation/mean_average_precision=0.098506, validation/num_examples=43793
I0206 06:24:47.391563 139770797713152 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.06616246700286865, loss=0.04050513356924057
I0206 06:25:19.040440 139770185602816 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.08419732749462128, loss=0.042615652084350586
I0206 06:25:50.545033 139770797713152 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04575532302260399, loss=0.043243635445833206
I0206 06:26:22.113434 139770185602816 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.05144089087843895, loss=0.04076262190937996
I0206 06:26:53.970705 139770797713152 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.041542600840330124, loss=0.03756536915898323
I0206 06:27:25.839103 139770185602816 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.03458116948604584, loss=0.04067480191588402
I0206 06:27:57.284046 139770797713152 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.03890793398022652, loss=0.03706369176506996
I0206 06:28:19.654848 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:29:59.410760 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:30:02.532247 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:30:05.552555 139978932307776 submission_runner.py:408] Time since start: 13274.64s, 	Step: 28671, 	{'train/accuracy': 0.98862624168396, 'train/loss': 0.03928302228450775, 'train/mean_average_precision': 0.2086000109865182, 'validation/accuracy': 0.9856077432632446, 'validation/loss': 0.04985186085104942, 'validation/mean_average_precision': 0.1624308171851189, 'validation/num_examples': 43793, 'test/accuracy': 0.9846815466880798, 'test/loss': 0.05285181477665901, 'test/mean_average_precision': 0.16642025833074262, 'test/num_examples': 43793, 'score': 9135.996017217636, 'total_duration': 13274.639439105988, 'accumulated_submission_time': 9135.996017217636, 'accumulated_eval_time': 4136.737054347992, 'accumulated_logging_time': 1.1446597576141357}
I0206 06:30:05.573108 139752176375552 logging_writer.py:48] [28671] accumulated_eval_time=4136.737054, accumulated_logging_time=1.144660, accumulated_submission_time=9135.996017, global_step=28671, preemption_count=0, score=9135.996017, test/accuracy=0.984682, test/loss=0.052852, test/mean_average_precision=0.166420, test/num_examples=43793, total_duration=13274.639439, train/accuracy=0.988626, train/loss=0.039283, train/mean_average_precision=0.208600, validation/accuracy=0.985608, validation/loss=0.049852, validation/mean_average_precision=0.162431, validation/num_examples=43793
I0206 06:30:15.241493 139770806105856 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.0668221265077591, loss=0.04009193554520607
I0206 06:30:46.851311 139752176375552 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.06780507415533066, loss=0.039428066462278366
I0206 06:31:18.802568 139770806105856 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.07656164467334747, loss=0.034724101424217224
I0206 06:31:50.625118 139752176375552 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.03994465619325638, loss=0.03533345088362694
I0206 06:32:22.715764 139770806105856 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.04992468282580376, loss=0.037273701280355453
I0206 06:32:54.960422 139752176375552 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.038973618298769, loss=0.035576578229665756
I0206 06:33:26.939811 139770806105856 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05060841888189316, loss=0.042727869004011154
I0206 06:33:58.855358 139752176375552 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.032537851482629776, loss=0.03512493520975113
I0206 06:34:05.830833 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:35:43.024236 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:35:46.081585 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:35:49.104444 139978932307776 submission_runner.py:408] Time since start: 13618.19s, 	Step: 29423, 	{'train/accuracy': 0.9888563752174377, 'train/loss': 0.03860921412706375, 'train/mean_average_precision': 0.1952220106581476, 'validation/accuracy': 0.9856755137443542, 'validation/loss': 0.04896049574017525, 'validation/mean_average_precision': 0.17366474140797325, 'validation/num_examples': 43793, 'test/accuracy': 0.9847674369812012, 'test/loss': 0.05177288129925728, 'test/mean_average_precision': 0.17578625348674065, 'test/num_examples': 43793, 'score': 9376.222169399261, 'total_duration': 13618.191462039948, 'accumulated_submission_time': 9376.222169399261, 'accumulated_eval_time': 4240.01061463356, 'accumulated_logging_time': 1.1763203144073486}
I0206 06:35:49.126839 139770185602816 logging_writer.py:48] [29423] accumulated_eval_time=4240.010615, accumulated_logging_time=1.176320, accumulated_submission_time=9376.222169, global_step=29423, preemption_count=0, score=9376.222169, test/accuracy=0.984767, test/loss=0.051773, test/mean_average_precision=0.175786, test/num_examples=43793, total_duration=13618.191462, train/accuracy=0.988856, train/loss=0.038609, train/mean_average_precision=0.195222, validation/accuracy=0.985676, validation/loss=0.048960, validation/mean_average_precision=0.173665, validation/num_examples=43793
I0206 06:36:13.982980 139804984182528 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09125552326440811, loss=0.03820590302348137
I0206 06:36:45.799005 139770185602816 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.07369225472211838, loss=0.03851398825645447
I0206 06:37:17.716284 139804984182528 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.03688988834619522, loss=0.035950131714344025
I0206 06:37:50.272291 139770185602816 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.061657775193452835, loss=0.03561416640877724
I0206 06:38:22.009345 139804984182528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04987145587801933, loss=0.042207736521959305
I0206 06:38:53.408671 139770185602816 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.042765162885189056, loss=0.04158893600106239
I0206 06:39:25.027600 139804984182528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.07625364512205124, loss=0.041524484753608704
I0206 06:39:49.232952 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:41:28.061533 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:41:31.176392 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:41:34.220732 139978932307776 submission_runner.py:408] Time since start: 13963.31s, 	Step: 30177, 	{'train/accuracy': 0.9887091517448425, 'train/loss': 0.03916693478822708, 'train/mean_average_precision': 0.2039721153390068, 'validation/accuracy': 0.9856755137443542, 'validation/loss': 0.049208447337150574, 'validation/mean_average_precision': 0.17392643637769953, 'validation/num_examples': 43793, 'test/accuracy': 0.9847522974014282, 'test/loss': 0.05200439319014549, 'test/mean_average_precision': 0.17425520445439235, 'test/num_examples': 43793, 'score': 9616.297856807709, 'total_duration': 13963.307752609253, 'accumulated_submission_time': 9616.297856807709, 'accumulated_eval_time': 4344.998346328735, 'accumulated_logging_time': 1.209303617477417}
I0206 06:41:34.241573 139752176375552 logging_writer.py:48] [30177] accumulated_eval_time=4344.998346, accumulated_logging_time=1.209304, accumulated_submission_time=9616.297857, global_step=30177, preemption_count=0, score=9616.297857, test/accuracy=0.984752, test/loss=0.052004, test/mean_average_precision=0.174255, test/num_examples=43793, total_duration=13963.307753, train/accuracy=0.988709, train/loss=0.039167, train/mean_average_precision=0.203972, validation/accuracy=0.985676, validation/loss=0.049208, validation/mean_average_precision=0.173926, validation/num_examples=43793
I0206 06:41:41.927922 139770797713152 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.03026171401143074, loss=0.03764596953988075
I0206 06:42:14.111432 139752176375552 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.09219081699848175, loss=0.037977635860443115
I0206 06:42:46.153648 139770797713152 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.09005537629127502, loss=0.03911951556801796
I0206 06:43:18.253095 139752176375552 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.05681033805012703, loss=0.044292058795690536
I0206 06:43:49.851748 139770797713152 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.058699388056993484, loss=0.034496065229177475
I0206 06:44:21.844407 139752176375552 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.03873181715607643, loss=0.03383156657218933
I0206 06:44:53.627266 139770797713152 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.052409276366233826, loss=0.039652757346630096
I0206 06:45:25.169065 139752176375552 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.07523560523986816, loss=0.04178394004702568
I0206 06:45:34.348138 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:47:09.223690 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:47:12.302065 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:47:15.457960 139978932307776 submission_runner.py:408] Time since start: 14304.54s, 	Step: 30930, 	{'train/accuracy': 0.9886908531188965, 'train/loss': 0.039093196392059326, 'train/mean_average_precision': 0.1994932528086023, 'validation/accuracy': 0.9856739044189453, 'validation/loss': 0.04953419044613838, 'validation/mean_average_precision': 0.1778154397527345, 'validation/num_examples': 43793, 'test/accuracy': 0.9847704172134399, 'test/loss': 0.05254374444484711, 'test/mean_average_precision': 0.1694027176352054, 'test/num_examples': 43793, 'score': 9856.373237371445, 'total_duration': 14304.544981956482, 'accumulated_submission_time': 9856.373237371445, 'accumulated_eval_time': 4446.108120918274, 'accumulated_logging_time': 1.2410881519317627}
I0206 06:47:15.480121 139770185602816 logging_writer.py:48] [30930] accumulated_eval_time=4446.108121, accumulated_logging_time=1.241088, accumulated_submission_time=9856.373237, global_step=30930, preemption_count=0, score=9856.373237, test/accuracy=0.984770, test/loss=0.052544, test/mean_average_precision=0.169403, test/num_examples=43793, total_duration=14304.544982, train/accuracy=0.988691, train/loss=0.039093, train/mean_average_precision=0.199493, validation/accuracy=0.985674, validation/loss=0.049534, validation/mean_average_precision=0.177815, validation/num_examples=43793
I0206 06:47:37.863844 139770806105856 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06056233495473862, loss=0.03892163187265396
I0206 06:48:09.436348 139770185602816 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.05146939679980278, loss=0.03640702739357948
I0206 06:48:40.936274 139770806105856 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.057042915374040604, loss=0.04077181965112686
I0206 06:49:12.365561 139770185602816 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.03184587508440018, loss=0.038340307772159576
I0206 06:49:43.939794 139770806105856 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.043630730360746384, loss=0.03510688990354538
I0206 06:50:15.425384 139770185602816 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04525790736079216, loss=0.03203849121928215
I0206 06:50:46.641327 139770806105856 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.04323771223425865, loss=0.0405186302959919
I0206 06:51:15.560409 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:52:53.677170 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:52:56.866198 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:52:59.971902 139978932307776 submission_runner.py:408] Time since start: 14649.06s, 	Step: 31693, 	{'train/accuracy': 0.9885149598121643, 'train/loss': 0.03962215408682823, 'train/mean_average_precision': 0.19421593475752508, 'validation/accuracy': 0.9857165217399597, 'validation/loss': 0.04926184192299843, 'validation/mean_average_precision': 0.1747485124816603, 'validation/num_examples': 43793, 'test/accuracy': 0.9847164750099182, 'test/loss': 0.05217207968235016, 'test/mean_average_precision': 0.17375916759515642, 'test/num_examples': 43793, 'score': 10096.422625780106, 'total_duration': 14649.05891919136, 'accumulated_submission_time': 10096.422625780106, 'accumulated_eval_time': 4550.519570350647, 'accumulated_logging_time': 1.2745091915130615}
I0206 06:52:59.994685 139752176375552 logging_writer.py:48] [31693] accumulated_eval_time=4550.519570, accumulated_logging_time=1.274509, accumulated_submission_time=10096.422626, global_step=31693, preemption_count=0, score=10096.422626, test/accuracy=0.984716, test/loss=0.052172, test/mean_average_precision=0.173759, test/num_examples=43793, total_duration=14649.058919, train/accuracy=0.988515, train/loss=0.039622, train/mean_average_precision=0.194216, validation/accuracy=0.985717, validation/loss=0.049262, validation/mean_average_precision=0.174749, validation/num_examples=43793
I0206 06:53:02.632350 139804984182528 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.04932851344347, loss=0.036722347140312195
I0206 06:53:34.206109 139752176375552 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06487566977739334, loss=0.0442826971411705
I0206 06:54:06.113666 139804984182528 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.06692086160182953, loss=0.040680501610040665
I0206 06:54:37.688917 139752176375552 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.08715614676475525, loss=0.04386824369430542
I0206 06:55:09.566219 139804984182528 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05708467587828636, loss=0.03394261375069618
I0206 06:55:41.050301 139752176375552 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05576125159859657, loss=0.04119518771767616
I0206 06:56:12.473477 139804984182528 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.056865133345127106, loss=0.035672616213560104
I0206 06:56:44.078880 139752176375552 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.07153116911649704, loss=0.03895531967282295
I0206 06:57:00.040791 139978932307776 spec.py:321] Evaluating on the training split.
I0206 06:58:41.026518 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 06:58:44.250279 139978932307776 spec.py:349] Evaluating on the test split.
I0206 06:58:47.287416 139978932307776 submission_runner.py:408] Time since start: 14996.37s, 	Step: 32452, 	{'train/accuracy': 0.988632082939148, 'train/loss': 0.039258670061826706, 'train/mean_average_precision': 0.20436964645347408, 'validation/accuracy': 0.9855963587760925, 'validation/loss': 0.04927314445376396, 'validation/mean_average_precision': 0.175708370852377, 'validation/num_examples': 43793, 'test/accuracy': 0.9845787882804871, 'test/loss': 0.05223559960722923, 'test/mean_average_precision': 0.1658119645684935, 'test/num_examples': 43793, 'score': 10336.437881469727, 'total_duration': 14996.374430418015, 'accumulated_submission_time': 10336.437881469727, 'accumulated_eval_time': 4657.7661418914795, 'accumulated_logging_time': 1.3084368705749512}
I0206 06:58:47.309922 139770185602816 logging_writer.py:48] [32452] accumulated_eval_time=4657.766142, accumulated_logging_time=1.308437, accumulated_submission_time=10336.437881, global_step=32452, preemption_count=0, score=10336.437881, test/accuracy=0.984579, test/loss=0.052236, test/mean_average_precision=0.165812, test/num_examples=43793, total_duration=14996.374430, train/accuracy=0.988632, train/loss=0.039259, train/mean_average_precision=0.204370, validation/accuracy=0.985596, validation/loss=0.049273, validation/mean_average_precision=0.175708, validation/num_examples=43793
I0206 06:59:02.588937 139770797713152 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06608440726995468, loss=0.035203490406274796
I0206 06:59:34.298830 139770185602816 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.04855946823954582, loss=0.038118306547403336
I0206 07:00:05.515557 139770797713152 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.03848738595843315, loss=0.04065859690308571
I0206 07:00:36.857501 139770185602816 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.050467219203710556, loss=0.0405481792986393
I0206 07:01:08.299480 139770797713152 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.04632267728447914, loss=0.043854210525751114
I0206 07:01:39.489672 139770185602816 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0793423131108284, loss=0.038463715463876724
I0206 07:02:10.951631 139770797713152 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.08714089542627335, loss=0.04133538529276848
I0206 07:02:42.478723 139770185602816 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.042577169835567474, loss=0.03581384941935539
I0206 07:02:47.574545 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:04:24.190594 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:04:27.310298 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:04:30.340648 139978932307776 submission_runner.py:408] Time since start: 15339.43s, 	Step: 33217, 	{'train/accuracy': 0.9888612031936646, 'train/loss': 0.03862139210104942, 'train/mean_average_precision': 0.2074459373809987, 'validation/accuracy': 0.9858188033103943, 'validation/loss': 0.04930514469742775, 'validation/mean_average_precision': 0.18337859176326854, 'validation/num_examples': 43793, 'test/accuracy': 0.9848264455795288, 'test/loss': 0.05229155346751213, 'test/mean_average_precision': 0.1770464495206037, 'test/num_examples': 43793, 'score': 10576.67140340805, 'total_duration': 15339.427662611008, 'accumulated_submission_time': 10576.67140340805, 'accumulated_eval_time': 4760.532192707062, 'accumulated_logging_time': 1.342067003250122}
I0206 07:04:30.363328 139770806105856 logging_writer.py:48] [33217] accumulated_eval_time=4760.532193, accumulated_logging_time=1.342067, accumulated_submission_time=10576.671403, global_step=33217, preemption_count=0, score=10576.671403, test/accuracy=0.984826, test/loss=0.052292, test/mean_average_precision=0.177046, test/num_examples=43793, total_duration=15339.427663, train/accuracy=0.988861, train/loss=0.038621, train/mean_average_precision=0.207446, validation/accuracy=0.985819, validation/loss=0.049305, validation/mean_average_precision=0.183379, validation/num_examples=43793
I0206 07:04:58.035084 139804984182528 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.04688943922519684, loss=0.04136968031525612
I0206 07:05:30.147323 139770806105856 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06613348424434662, loss=0.044951919466257095
I0206 07:06:02.121320 139804984182528 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05721684917807579, loss=0.040432002395391464
I0206 07:06:34.051304 139770806105856 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.07295846939086914, loss=0.03665051981806755
I0206 07:07:05.699208 139804984182528 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05096222087740898, loss=0.041204437613487244
I0206 07:07:37.645634 139770806105856 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.057523421943187714, loss=0.03266647085547447
I0206 07:08:09.430918 139804984182528 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.12334756553173065, loss=0.037318386137485504
I0206 07:08:30.481487 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:10:13.003234 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:10:16.096009 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:10:19.124550 139978932307776 submission_runner.py:408] Time since start: 15688.21s, 	Step: 33968, 	{'train/accuracy': 0.988823652267456, 'train/loss': 0.03878296911716461, 'train/mean_average_precision': 0.20970713652735315, 'validation/accuracy': 0.9857664704322815, 'validation/loss': 0.049136847257614136, 'validation/mean_average_precision': 0.17837035100668272, 'validation/num_examples': 43793, 'test/accuracy': 0.9848175644874573, 'test/loss': 0.052116215229034424, 'test/mean_average_precision': 0.17825656076499521, 'test/num_examples': 43793, 'score': 10816.757288694382, 'total_duration': 15688.211569309235, 'accumulated_submission_time': 10816.757288694382, 'accumulated_eval_time': 4869.175209760666, 'accumulated_logging_time': 1.3774352073669434}
I0206 07:10:19.147251 139752176375552 logging_writer.py:48] [33968] accumulated_eval_time=4869.175210, accumulated_logging_time=1.377435, accumulated_submission_time=10816.757289, global_step=33968, preemption_count=0, score=10816.757289, test/accuracy=0.984818, test/loss=0.052116, test/mean_average_precision=0.178257, test/num_examples=43793, total_duration=15688.211569, train/accuracy=0.988824, train/loss=0.038783, train/mean_average_precision=0.209707, validation/accuracy=0.985766, validation/loss=0.049137, validation/mean_average_precision=0.178370, validation/num_examples=43793
I0206 07:10:29.504832 139770797713152 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06479671597480774, loss=0.042403995990753174
I0206 07:11:01.108450 139752176375552 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05660819262266159, loss=0.0362870953977108
I0206 07:11:32.659708 139770797713152 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.10959188640117645, loss=0.03529597446322441
I0206 07:12:04.303552 139752176375552 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06758780032396317, loss=0.03711726516485214
I0206 07:12:36.216950 139770797713152 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.09761156886816025, loss=0.045113980770111084
I0206 07:13:08.089659 139752176375552 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06421665847301483, loss=0.0392245277762413
I0206 07:13:39.803720 139770797713152 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.04748690873384476, loss=0.03999128192663193
I0206 07:14:11.824591 139752176375552 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.0625462681055069, loss=0.04289279133081436
I0206 07:14:19.407558 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:16:01.159446 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:16:04.240604 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:16:07.195676 139978932307776 submission_runner.py:408] Time since start: 16036.28s, 	Step: 34725, 	{'train/accuracy': 0.9889429807662964, 'train/loss': 0.03803737834095955, 'train/mean_average_precision': 0.21792488734815457, 'validation/accuracy': 0.9857100248336792, 'validation/loss': 0.048944659531116486, 'validation/mean_average_precision': 0.17134611019919604, 'validation/num_examples': 43793, 'test/accuracy': 0.9847207069396973, 'test/loss': 0.051899559795856476, 'test/mean_average_precision': 0.1696407534624433, 'test/num_examples': 43793, 'score': 11056.986756563187, 'total_duration': 16036.28269791603, 'accumulated_submission_time': 11056.986756563187, 'accumulated_eval_time': 4976.963281154633, 'accumulated_logging_time': 1.4110808372497559}
I0206 07:16:07.217288 139770806105856 logging_writer.py:48] [34725] accumulated_eval_time=4976.963281, accumulated_logging_time=1.411081, accumulated_submission_time=11056.986757, global_step=34725, preemption_count=0, score=11056.986757, test/accuracy=0.984721, test/loss=0.051900, test/mean_average_precision=0.169641, test/num_examples=43793, total_duration=16036.282698, train/accuracy=0.988943, train/loss=0.038037, train/mean_average_precision=0.217925, validation/accuracy=0.985710, validation/loss=0.048945, validation/mean_average_precision=0.171346, validation/num_examples=43793
I0206 07:16:31.230430 139804984182528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.043713364750146866, loss=0.04084125906229019
I0206 07:17:02.982465 139770806105856 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.042401496320962906, loss=0.039820823818445206
I0206 07:17:34.752512 139804984182528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06441878527402878, loss=0.03848941624164581
I0206 07:18:06.727081 139770806105856 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.04741235449910164, loss=0.042029332369565964
I0206 07:18:38.257210 139804984182528 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.10212709754705429, loss=0.03643249347805977
I0206 07:19:10.853218 139770806105856 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05903935432434082, loss=0.03848907724022865
I0206 07:19:42.659550 139804984182528 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.04699733853340149, loss=0.03717716410756111
I0206 07:20:07.366158 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:21:47.599173 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:21:50.689032 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:21:53.626866 139978932307776 submission_runner.py:408] Time since start: 16382.71s, 	Step: 35479, 	{'train/accuracy': 0.9887934327125549, 'train/loss': 0.038353241980075836, 'train/mean_average_precision': 0.2175011480776211, 'validation/accuracy': 0.985688328742981, 'validation/loss': 0.0491277314722538, 'validation/mean_average_precision': 0.18221929719318744, 'validation/num_examples': 43793, 'test/accuracy': 0.9847089052200317, 'test/loss': 0.052242908626794815, 'test/mean_average_precision': 0.1731621813997392, 'test/num_examples': 43793, 'score': 11297.105067253113, 'total_duration': 16382.713775157928, 'accumulated_submission_time': 11297.105067253113, 'accumulated_eval_time': 5083.223834276199, 'accumulated_logging_time': 1.4434118270874023}
I0206 07:21:53.648513 139770185602816 logging_writer.py:48] [35479] accumulated_eval_time=5083.223834, accumulated_logging_time=1.443412, accumulated_submission_time=11297.105067, global_step=35479, preemption_count=0, score=11297.105067, test/accuracy=0.984709, test/loss=0.052243, test/mean_average_precision=0.173162, test/num_examples=43793, total_duration=16382.713775, train/accuracy=0.988793, train/loss=0.038353, train/mean_average_precision=0.217501, validation/accuracy=0.985688, validation/loss=0.049128, validation/mean_average_precision=0.182219, validation/num_examples=43793
I0206 07:22:00.883384 139770797713152 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.04279143363237381, loss=0.03990694507956505
I0206 07:22:32.749463 139770185602816 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.07311727851629257, loss=0.03616570308804512
I0206 07:23:04.374038 139770797713152 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.07045590132474899, loss=0.036620236933231354
I0206 07:23:36.026948 139770185602816 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.13432449102401733, loss=0.043021854013204575
I0206 07:24:08.136056 139770797713152 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05540890619158745, loss=0.04140857979655266
I0206 07:24:40.056373 139770185602816 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.11130321770906448, loss=0.038939233869314194
I0206 07:25:12.125843 139770797713152 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06605103611946106, loss=0.03614864870905876
I0206 07:25:43.755903 139770185602816 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06086057424545288, loss=0.040018610656261444
I0206 07:25:53.753111 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:27:32.968874 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:27:36.046089 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:27:39.028320 139978932307776 submission_runner.py:408] Time since start: 16728.12s, 	Step: 36232, 	{'train/accuracy': 0.9889160990715027, 'train/loss': 0.038255613297224045, 'train/mean_average_precision': 0.2174486785456183, 'validation/accuracy': 0.9857396483421326, 'validation/loss': 0.048740580677986145, 'validation/mean_average_precision': 0.1753565414250365, 'validation/num_examples': 43793, 'test/accuracy': 0.984761118888855, 'test/loss': 0.05180809274315834, 'test/mean_average_precision': 0.16818006022956916, 'test/num_examples': 43793, 'score': 11537.17705130577, 'total_duration': 16728.115339756012, 'accumulated_submission_time': 11537.17705130577, 'accumulated_eval_time': 5188.498993873596, 'accumulated_logging_time': 1.47762131690979}
I0206 07:27:39.052055 139752176375552 logging_writer.py:48] [36232] accumulated_eval_time=5188.498994, accumulated_logging_time=1.477621, accumulated_submission_time=11537.177051, global_step=36232, preemption_count=0, score=11537.177051, test/accuracy=0.984761, test/loss=0.051808, test/mean_average_precision=0.168180, test/num_examples=43793, total_duration=16728.115340, train/accuracy=0.988916, train/loss=0.038256, train/mean_average_precision=0.217449, validation/accuracy=0.985740, validation/loss=0.048741, validation/mean_average_precision=0.175357, validation/num_examples=43793
I0206 07:28:01.306270 139804984182528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06460516899824142, loss=0.03948131203651428
I0206 07:28:33.486829 139752176375552 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.045544277876615524, loss=0.03921804204583168
I0206 07:29:05.831368 139804984182528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0646921843290329, loss=0.04074554145336151
I0206 07:29:37.882642 139752176375552 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06964854896068573, loss=0.038002386689186096
I0206 07:30:10.115633 139804984182528 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06256195157766342, loss=0.03720998764038086
I0206 07:30:42.090310 139752176375552 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.04428817331790924, loss=0.0412416085600853
I0206 07:31:14.248711 139804984182528 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.04567396268248558, loss=0.04214897006750107
I0206 07:31:39.115222 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:33:17.304656 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:33:20.359047 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:33:23.326504 139978932307776 submission_runner.py:408] Time since start: 17072.41s, 	Step: 36979, 	{'train/accuracy': 0.9888588786125183, 'train/loss': 0.038587551563978195, 'train/mean_average_precision': 0.21144420240805334, 'validation/accuracy': 0.9856597185134888, 'validation/loss': 0.049098994582891464, 'validation/mean_average_precision': 0.17396346792948608, 'validation/num_examples': 43793, 'test/accuracy': 0.9847059845924377, 'test/loss': 0.05208496376872063, 'test/mean_average_precision': 0.17090511164112587, 'test/num_examples': 43793, 'score': 11777.208164453506, 'total_duration': 17072.413522958755, 'accumulated_submission_time': 11777.208164453506, 'accumulated_eval_time': 5292.71022772789, 'accumulated_logging_time': 1.513559103012085}
I0206 07:33:23.348932 139770797713152 logging_writer.py:48] [36979] accumulated_eval_time=5292.710228, accumulated_logging_time=1.513559, accumulated_submission_time=11777.208164, global_step=36979, preemption_count=0, score=11777.208164, test/accuracy=0.984706, test/loss=0.052085, test/mean_average_precision=0.170905, test/num_examples=43793, total_duration=17072.413523, train/accuracy=0.988859, train/loss=0.038588, train/mean_average_precision=0.211444, validation/accuracy=0.985660, validation/loss=0.049099, validation/mean_average_precision=0.173963, validation/num_examples=43793
I0206 07:33:30.344365 139770806105856 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06735427677631378, loss=0.03875806927680969
I0206 07:34:02.107195 139770797713152 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.040439803153276443, loss=0.03787289559841156
I0206 07:34:34.181461 139770806105856 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.054867055267095566, loss=0.03242996335029602
I0206 07:35:05.875422 139770797713152 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.04761156067252159, loss=0.040695950388908386
I0206 07:35:37.828952 139770806105856 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.08536160737276077, loss=0.04078828543424606
I0206 07:36:09.626132 139770797713152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05358615517616272, loss=0.0354204960167408
I0206 07:36:41.220920 139770806105856 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.05169561505317688, loss=0.0432821661233902
I0206 07:37:12.996556 139770797713152 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07991516590118408, loss=0.03857399895787239
I0206 07:37:23.373517 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:39:04.209791 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:39:07.259460 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:39:10.209979 139978932307776 submission_runner.py:408] Time since start: 17419.30s, 	Step: 37734, 	{'train/accuracy': 0.988965630531311, 'train/loss': 0.03813077136874199, 'train/mean_average_precision': 0.21632119362024058, 'validation/accuracy': 0.9856706857681274, 'validation/loss': 0.049130287021398544, 'validation/mean_average_precision': 0.17558204788664689, 'validation/num_examples': 43793, 'test/accuracy': 0.9846655130386353, 'test/loss': 0.051924485713243484, 'test/mean_average_precision': 0.168482360769061, 'test/num_examples': 43793, 'score': 12017.201822519302, 'total_duration': 17419.296885967255, 'accumulated_submission_time': 12017.201822519302, 'accumulated_eval_time': 5399.546529531479, 'accumulated_logging_time': 1.54679536819458}
I0206 07:39:10.232316 139752176375552 logging_writer.py:48] [37734] accumulated_eval_time=5399.546530, accumulated_logging_time=1.546795, accumulated_submission_time=12017.201823, global_step=37734, preemption_count=0, score=12017.201823, test/accuracy=0.984666, test/loss=0.051924, test/mean_average_precision=0.168482, test/num_examples=43793, total_duration=17419.296886, train/accuracy=0.988966, train/loss=0.038131, train/mean_average_precision=0.216321, validation/accuracy=0.985671, validation/loss=0.049130, validation/mean_average_precision=0.175582, validation/num_examples=43793
I0206 07:39:31.342515 139770185602816 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.043777547776699066, loss=0.039453018456697464
I0206 07:40:03.230749 139752176375552 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.0551222525537014, loss=0.043368224054574966
I0206 07:40:34.840226 139770185602816 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.05605895817279816, loss=0.03660482168197632
I0206 07:41:06.446839 139752176375552 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.08173207938671112, loss=0.043262701481580734
I0206 07:41:37.901482 139770185602816 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.050745122134685516, loss=0.03827737644314766
I0206 07:42:09.408236 139752176375552 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06950404495000839, loss=0.04020661488175392
I0206 07:42:41.023868 139770185602816 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.05196622386574745, loss=0.03844922035932541
I0206 07:43:10.224854 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:44:47.054450 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:44:50.072839 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:44:53.015771 139978932307776 submission_runner.py:408] Time since start: 17762.10s, 	Step: 38494, 	{'train/accuracy': 0.9889072179794312, 'train/loss': 0.038279175758361816, 'train/mean_average_precision': 0.2058152897816714, 'validation/accuracy': 0.9858139753341675, 'validation/loss': 0.04839226230978966, 'validation/mean_average_precision': 0.1763234303719536, 'validation/num_examples': 43793, 'test/accuracy': 0.9848403334617615, 'test/loss': 0.05114014074206352, 'test/mean_average_precision': 0.17291108656222037, 'test/num_examples': 43793, 'score': 12257.161703586578, 'total_duration': 17762.10279250145, 'accumulated_submission_time': 12257.161703586578, 'accumulated_eval_time': 5502.337423086166, 'accumulated_logging_time': 1.5815489292144775}
I0206 07:44:53.038566 139770806105856 logging_writer.py:48] [38494] accumulated_eval_time=5502.337423, accumulated_logging_time=1.581549, accumulated_submission_time=12257.161704, global_step=38494, preemption_count=0, score=12257.161704, test/accuracy=0.984840, test/loss=0.051140, test/mean_average_precision=0.172911, test/num_examples=43793, total_duration=17762.102793, train/accuracy=0.988907, train/loss=0.038279, train/mean_average_precision=0.205815, validation/accuracy=0.985814, validation/loss=0.048392, validation/mean_average_precision=0.176323, validation/num_examples=43793
I0206 07:44:55.250993 139804984182528 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.04942735657095909, loss=0.043384864926338196
I0206 07:45:26.954544 139770806105856 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.04782319813966751, loss=0.04171062633395195
I0206 07:45:58.587212 139804984182528 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.055185478180646896, loss=0.03760816156864166
I0206 07:46:30.308063 139770806105856 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.0623137503862381, loss=0.037473198026418686
I0206 07:47:01.680128 139804984182528 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.056103117763996124, loss=0.036305710673332214
I0206 07:47:33.595367 139770806105856 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07311312854290009, loss=0.04292876645922661
I0206 07:48:05.579186 139804984182528 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.04717808589339256, loss=0.036347899585962296
I0206 07:48:37.502065 139770806105856 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.045192405581474304, loss=0.03642217442393303
I0206 07:48:53.064297 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:50:32.235516 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:50:35.298648 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:50:38.310270 139978932307776 submission_runner.py:408] Time since start: 18107.40s, 	Step: 39250, 	{'train/accuracy': 0.9886656999588013, 'train/loss': 0.0391177199780941, 'train/mean_average_precision': 0.20900084989286502, 'validation/accuracy': 0.9853398203849792, 'validation/loss': 0.049594368785619736, 'validation/mean_average_precision': 0.17641425803007238, 'validation/num_examples': 43793, 'test/accuracy': 0.9843534231185913, 'test/loss': 0.05261526629328728, 'test/mean_average_precision': 0.1698303266528774, 'test/num_examples': 43793, 'score': 12497.155459403992, 'total_duration': 18107.39728331566, 'accumulated_submission_time': 12497.155459403992, 'accumulated_eval_time': 5607.583341121674, 'accumulated_logging_time': 1.6165733337402344}
I0206 07:50:38.333172 139752176375552 logging_writer.py:48] [39250] accumulated_eval_time=5607.583341, accumulated_logging_time=1.616573, accumulated_submission_time=12497.155459, global_step=39250, preemption_count=0, score=12497.155459, test/accuracy=0.984353, test/loss=0.052615, test/mean_average_precision=0.169830, test/num_examples=43793, total_duration=18107.397283, train/accuracy=0.988666, train/loss=0.039118, train/mean_average_precision=0.209001, validation/accuracy=0.985340, validation/loss=0.049594, validation/mean_average_precision=0.176414, validation/num_examples=43793
I0206 07:50:54.533522 139770797713152 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.065401092171669, loss=0.03638540580868721
I0206 07:51:26.203698 139752176375552 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.047122273594141006, loss=0.041063010692596436
I0206 07:51:57.824993 139770797713152 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07171911001205444, loss=0.04059206321835518
I0206 07:52:29.662190 139752176375552 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07025954127311707, loss=0.03892281651496887
I0206 07:53:01.578933 139770797713152 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.049477167427539825, loss=0.03507206216454506
I0206 07:53:33.391189 139752176375552 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.163188636302948, loss=0.04502859339118004
I0206 07:54:04.930223 139770797713152 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.04644201323390007, loss=0.03851646929979324
I0206 07:54:36.252888 139752176375552 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07457590103149414, loss=0.04577438160777092
I0206 07:54:38.438898 139978932307776 spec.py:321] Evaluating on the training split.
I0206 07:56:16.888223 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 07:56:19.931786 139978932307776 spec.py:349] Evaluating on the test split.
I0206 07:56:22.864580 139978932307776 submission_runner.py:408] Time since start: 18451.95s, 	Step: 40008, 	{'train/accuracy': 0.988823652267456, 'train/loss': 0.03839899227023125, 'train/mean_average_precision': 0.21181767514573896, 'validation/accuracy': 0.9857344031333923, 'validation/loss': 0.04862920567393303, 'validation/mean_average_precision': 0.18505388601295117, 'validation/num_examples': 43793, 'test/accuracy': 0.9848032593727112, 'test/loss': 0.05150538682937622, 'test/mean_average_precision': 0.17641508945720805, 'test/num_examples': 43793, 'score': 12737.230195999146, 'total_duration': 18451.95160317421, 'accumulated_submission_time': 12737.230195999146, 'accumulated_eval_time': 5712.008977174759, 'accumulated_logging_time': 1.6503448486328125}
I0206 07:56:22.887312 139770185602816 logging_writer.py:48] [40008] accumulated_eval_time=5712.008977, accumulated_logging_time=1.650345, accumulated_submission_time=12737.230196, global_step=40008, preemption_count=0, score=12737.230196, test/accuracy=0.984803, test/loss=0.051505, test/mean_average_precision=0.176415, test/num_examples=43793, total_duration=18451.951603, train/accuracy=0.988824, train/loss=0.038399, train/mean_average_precision=0.211818, validation/accuracy=0.985734, validation/loss=0.048629, validation/mean_average_precision=0.185054, validation/num_examples=43793
I0206 07:56:52.201233 139770806105856 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.09595881402492523, loss=0.03640744090080261
I0206 07:57:23.921382 139770185602816 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.05177801102399826, loss=0.03771258890628815
I0206 07:57:55.448830 139770806105856 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.059390317648649216, loss=0.036592502146959305
I0206 07:58:26.999325 139770185602816 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07105536758899689, loss=0.04006803035736084
I0206 07:58:58.827760 139770806105856 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.05232524126768112, loss=0.036469344049692154
I0206 07:59:30.494982 139770185602816 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.10484053194522858, loss=0.03429873660206795
I0206 08:00:02.060038 139770806105856 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.05797639489173889, loss=0.03722495213150978
I0206 08:00:23.117719 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:01:59.905726 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:02:02.996970 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:02:05.926093 139978932307776 submission_runner.py:408] Time since start: 18795.01s, 	Step: 40765, 	{'train/accuracy': 0.988957405090332, 'train/loss': 0.03811485692858696, 'train/mean_average_precision': 0.22504035243759737, 'validation/accuracy': 0.9858992099761963, 'validation/loss': 0.0484270341694355, 'validation/mean_average_precision': 0.19021670604537053, 'validation/num_examples': 43793, 'test/accuracy': 0.98490309715271, 'test/loss': 0.05145435035228729, 'test/mean_average_precision': 0.1773463244063912, 'test/num_examples': 43793, 'score': 12977.429183483124, 'total_duration': 18795.01301598549, 'accumulated_submission_time': 12977.429183483124, 'accumulated_eval_time': 5814.817209243774, 'accumulated_logging_time': 1.6850457191467285}
I0206 08:02:05.949357 139752176375552 logging_writer.py:48] [40765] accumulated_eval_time=5814.817209, accumulated_logging_time=1.685046, accumulated_submission_time=12977.429183, global_step=40765, preemption_count=0, score=12977.429183, test/accuracy=0.984903, test/loss=0.051454, test/mean_average_precision=0.177346, test/num_examples=43793, total_duration=18795.013016, train/accuracy=0.988957, train/loss=0.038115, train/mean_average_precision=0.225040, validation/accuracy=0.985899, validation/loss=0.048427, validation/mean_average_precision=0.190217, validation/num_examples=43793
I0206 08:02:17.245968 139770797713152 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07532485574483871, loss=0.03617941960692406
I0206 08:02:49.173223 139752176375552 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.05296647176146507, loss=0.04024980217218399
I0206 08:03:20.851027 139770797713152 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.05646773427724838, loss=0.03879751265048981
I0206 08:03:52.181665 139752176375552 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.06628549844026566, loss=0.03886418789625168
I0206 08:04:23.910088 139770797713152 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.05836913362145424, loss=0.03682998940348625
I0206 08:04:55.231392 139752176375552 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.039970941841602325, loss=0.03401048853993416
I0206 08:05:26.796627 139770797713152 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0665554478764534, loss=0.0372166633605957
I0206 08:05:58.383279 139752176375552 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.06640669703483582, loss=0.04446115717291832
I0206 08:06:05.928090 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:07:43.967782 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:07:47.018766 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:07:49.958013 139978932307776 submission_runner.py:408] Time since start: 19139.05s, 	Step: 41525, 	{'train/accuracy': 0.9889556169509888, 'train/loss': 0.037930309772491455, 'train/mean_average_precision': 0.22477926950095242, 'validation/accuracy': 0.9858963489532471, 'validation/loss': 0.048464491963386536, 'validation/mean_average_precision': 0.18542875821589172, 'validation/num_examples': 43793, 'test/accuracy': 0.9849595427513123, 'test/loss': 0.05132923275232315, 'test/mean_average_precision': 0.17811563395579139, 'test/num_examples': 43793, 'score': 13217.376702070236, 'total_duration': 19139.04503273964, 'accumulated_submission_time': 13217.376702070236, 'accumulated_eval_time': 5918.8470821380615, 'accumulated_logging_time': 1.7193021774291992}
I0206 08:07:49.981854 139770806105856 logging_writer.py:48] [41525] accumulated_eval_time=5918.847082, accumulated_logging_time=1.719302, accumulated_submission_time=13217.376702, global_step=41525, preemption_count=0, score=13217.376702, test/accuracy=0.984960, test/loss=0.051329, test/mean_average_precision=0.178116, test/num_examples=43793, total_duration=19139.045033, train/accuracy=0.988956, train/loss=0.037930, train/mean_average_precision=0.224779, validation/accuracy=0.985896, validation/loss=0.048464, validation/mean_average_precision=0.185429, validation/num_examples=43793
I0206 08:08:14.034084 139804984182528 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.037655264139175415, loss=0.038937538862228394
I0206 08:08:45.396189 139770806105856 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.12052763253450394, loss=0.03761132434010506
I0206 08:09:17.048617 139804984182528 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.10540709644556046, loss=0.03454786166548729
I0206 08:09:48.724054 139770806105856 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06257116794586182, loss=0.034705158323049545
I0206 08:10:20.300268 139804984182528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.052627477794885635, loss=0.03715289756655693
I0206 08:10:52.256754 139770806105856 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.055538833141326904, loss=0.03537783771753311
I0206 08:11:23.829607 139804984182528 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.0607839897274971, loss=0.039022695273160934
I0206 08:11:50.170938 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:13:30.051297 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:13:33.045558 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:13:35.995154 139978932307776 submission_runner.py:408] Time since start: 19485.08s, 	Step: 42284, 	{'train/accuracy': 0.9891242384910583, 'train/loss': 0.03746219351887703, 'train/mean_average_precision': 0.2190261638573074, 'validation/accuracy': 0.9858837723731995, 'validation/loss': 0.04842827469110489, 'validation/mean_average_precision': 0.18823987726990113, 'validation/num_examples': 43793, 'test/accuracy': 0.9849300384521484, 'test/loss': 0.0514775849878788, 'test/mean_average_precision': 0.17285506517278354, 'test/num_examples': 43793, 'score': 13457.534606933594, 'total_duration': 19485.082169294357, 'accumulated_submission_time': 13457.534606933594, 'accumulated_eval_time': 6024.671246051788, 'accumulated_logging_time': 1.7549612522125244}
I0206 08:13:36.018930 139752176375552 logging_writer.py:48] [42284] accumulated_eval_time=6024.671246, accumulated_logging_time=1.754961, accumulated_submission_time=13457.534607, global_step=42284, preemption_count=0, score=13457.534607, test/accuracy=0.984930, test/loss=0.051478, test/mean_average_precision=0.172855, test/num_examples=43793, total_duration=19485.082169, train/accuracy=0.989124, train/loss=0.037462, train/mean_average_precision=0.219026, validation/accuracy=0.985884, validation/loss=0.048428, validation/mean_average_precision=0.188240, validation/num_examples=43793
I0206 08:13:41.472259 139770185602816 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07745964080095291, loss=0.04102962464094162
I0206 08:14:13.372118 139752176375552 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09731043875217438, loss=0.04116492345929146
I0206 08:14:45.002489 139770185602816 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.05361055210232735, loss=0.036494556814432144
I0206 08:15:17.062696 139752176375552 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06804194301366806, loss=0.03811400756239891
I0206 08:15:49.006088 139770185602816 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.06826505810022354, loss=0.037304509431123734
I0206 08:16:20.655782 139752176375552 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.08269555121660233, loss=0.03958810865879059
I0206 08:16:52.568761 139770185602816 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.06561653316020966, loss=0.036135073751211166
I0206 08:17:24.101811 139752176375552 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.11625347286462784, loss=0.035880230367183685
I0206 08:17:36.071148 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:19:17.237531 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:19:20.865316 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:19:24.484668 139978932307776 submission_runner.py:408] Time since start: 19833.57s, 	Step: 43039, 	{'train/accuracy': 0.9891573190689087, 'train/loss': 0.0371311753988266, 'train/mean_average_precision': 0.2342620274547162, 'validation/accuracy': 0.9858505129814148, 'validation/loss': 0.04841018468141556, 'validation/mean_average_precision': 0.18300957679890592, 'validation/num_examples': 43793, 'test/accuracy': 0.9848761558532715, 'test/loss': 0.051237087696790695, 'test/mean_average_precision': 0.17557429269603625, 'test/num_examples': 43793, 'score': 13697.556046247482, 'total_duration': 19833.571689128876, 'accumulated_submission_time': 13697.556046247482, 'accumulated_eval_time': 6133.084717750549, 'accumulated_logging_time': 1.7895762920379639}
I0206 08:19:24.508294 139770797713152 logging_writer.py:48] [43039] accumulated_eval_time=6133.084718, accumulated_logging_time=1.789576, accumulated_submission_time=13697.556046, global_step=43039, preemption_count=0, score=13697.556046, test/accuracy=0.984876, test/loss=0.051237, test/mean_average_precision=0.175574, test/num_examples=43793, total_duration=19833.571689, train/accuracy=0.989157, train/loss=0.037131, train/mean_average_precision=0.234262, validation/accuracy=0.985851, validation/loss=0.048410, validation/mean_average_precision=0.183010, validation/num_examples=43793
I0206 08:19:44.271288 139804984182528 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.1520080864429474, loss=0.04154524952173233
I0206 08:20:16.018229 139770797713152 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1798628717660904, loss=0.04006190598011017
I0206 08:20:47.882422 139804984182528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.06696192920207977, loss=0.037099163979291916
I0206 08:21:19.788876 139770797713152 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.057937514036893845, loss=0.04021356627345085
I0206 08:21:51.516105 139804984182528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07406240701675415, loss=0.037212640047073364
I0206 08:22:23.348731 139770797713152 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08042214065790176, loss=0.04151947796344757
I0206 08:22:55.084746 139804984182528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1373363584280014, loss=0.04077066481113434
I0206 08:23:24.556560 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:25:06.109344 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:25:09.116202 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:25:14.388519 139978932307776 submission_runner.py:408] Time since start: 20183.48s, 	Step: 43794, 	{'train/accuracy': 0.989040195941925, 'train/loss': 0.03743480145931244, 'train/mean_average_precision': 0.2364169550507474, 'validation/accuracy': 0.9858590364456177, 'validation/loss': 0.04835963249206543, 'validation/mean_average_precision': 0.1810546301375452, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05103975161910057, 'test/mean_average_precision': 0.17830234383257554, 'test/num_examples': 43793, 'score': 13937.573343992233, 'total_duration': 20183.47554087639, 'accumulated_submission_time': 13937.573343992233, 'accumulated_eval_time': 6242.9166431427, 'accumulated_logging_time': 1.8242683410644531}
I0206 08:25:14.412876 139752176375552 logging_writer.py:48] [43794] accumulated_eval_time=6242.916643, accumulated_logging_time=1.824268, accumulated_submission_time=13937.573344, global_step=43794, preemption_count=0, score=13937.573344, test/accuracy=0.984913, test/loss=0.051040, test/mean_average_precision=0.178302, test/num_examples=43793, total_duration=20183.475541, train/accuracy=0.989040, train/loss=0.037435, train/mean_average_precision=0.236417, validation/accuracy=0.985859, validation/loss=0.048360, validation/mean_average_precision=0.181055, validation/num_examples=43793
I0206 08:25:16.675867 139770806105856 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.09674201160669327, loss=0.0367351658642292
I0206 08:25:48.750788 139752176375552 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08767714351415634, loss=0.03696667030453682
I0206 08:26:20.918839 139770806105856 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.23112183809280396, loss=0.03894146904349327
I0206 08:26:52.492785 139752176375552 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.061042651534080505, loss=0.038186993449926376
I0206 08:27:24.475100 139770806105856 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.113742895424366, loss=0.038229018449783325
I0206 08:27:56.169333 139752176375552 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.13894234597682953, loss=0.039531707763671875
I0206 08:28:28.244868 139770806105856 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.11957469582557678, loss=0.03460856154561043
I0206 08:29:00.078156 139752176375552 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.10948114842176437, loss=0.035270437598228455
I0206 08:29:14.392367 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:30:50.690392 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:30:53.660503 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:30:56.660399 139978932307776 submission_runner.py:408] Time since start: 20525.75s, 	Step: 44546, 	{'train/accuracy': 0.9892123341560364, 'train/loss': 0.03711209073662758, 'train/mean_average_precision': 0.23362917116675708, 'validation/accuracy': 0.9858176112174988, 'validation/loss': 0.048688534647226334, 'validation/mean_average_precision': 0.18658132410017447, 'validation/num_examples': 43793, 'test/accuracy': 0.9848727583885193, 'test/loss': 0.05150778964161873, 'test/mean_average_precision': 0.17870415084724914, 'test/num_examples': 43793, 'score': 14177.521633148193, 'total_duration': 20525.747420549393, 'accumulated_submission_time': 14177.521633148193, 'accumulated_eval_time': 6345.184635639191, 'accumulated_logging_time': 1.8598694801330566}
I0206 08:30:56.684072 139770185602816 logging_writer.py:48] [44546] accumulated_eval_time=6345.184636, accumulated_logging_time=1.859869, accumulated_submission_time=14177.521633, global_step=44546, preemption_count=0, score=14177.521633, test/accuracy=0.984873, test/loss=0.051508, test/mean_average_precision=0.178704, test/num_examples=43793, total_duration=20525.747421, train/accuracy=0.989212, train/loss=0.037112, train/mean_average_precision=0.233629, validation/accuracy=0.985818, validation/loss=0.048689, validation/mean_average_precision=0.186581, validation/num_examples=43793
I0206 08:31:14.568543 139804984182528 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.06635301560163498, loss=0.042995620518922806
I0206 08:31:46.533785 139770185602816 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08496302366256714, loss=0.03464948385953903
I0206 08:32:18.294267 139804984182528 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07218529284000397, loss=0.037261441349983215
I0206 08:32:50.227248 139770185602816 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.05518408492207527, loss=0.039386965334415436
I0206 08:33:22.056300 139804984182528 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.05650986731052399, loss=0.034696418792009354
I0206 08:33:54.145374 139770185602816 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07235294580459595, loss=0.03893325850367546
I0206 08:34:25.793414 139804984182528 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.09407966583967209, loss=0.03655335307121277
I0206 08:34:56.897062 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:36:34.302141 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:36:37.365453 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:36:40.367095 139978932307776 submission_runner.py:408] Time since start: 20869.45s, 	Step: 45299, 	{'train/accuracy': 0.989071786403656, 'train/loss': 0.0375232920050621, 'train/mean_average_precision': 0.22586479658455655, 'validation/accuracy': 0.9858951568603516, 'validation/loss': 0.04855578392744064, 'validation/mean_average_precision': 0.18478510657217334, 'validation/num_examples': 43793, 'test/accuracy': 0.9849607944488525, 'test/loss': 0.05158171057701111, 'test/mean_average_precision': 0.1814172696895895, 'test/num_examples': 43793, 'score': 14417.702919960022, 'total_duration': 20869.454118013382, 'accumulated_submission_time': 14417.702919960022, 'accumulated_eval_time': 6448.654628753662, 'accumulated_logging_time': 1.8947200775146484}
I0206 08:36:40.391431 139752176375552 logging_writer.py:48] [45299] accumulated_eval_time=6448.654629, accumulated_logging_time=1.894720, accumulated_submission_time=14417.702920, global_step=45299, preemption_count=0, score=14417.702920, test/accuracy=0.984961, test/loss=0.051582, test/mean_average_precision=0.181417, test/num_examples=43793, total_duration=20869.454118, train/accuracy=0.989072, train/loss=0.037523, train/mean_average_precision=0.225865, validation/accuracy=0.985895, validation/loss=0.048556, validation/mean_average_precision=0.184785, validation/num_examples=43793
I0206 08:36:41.106857 139770806105856 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.11544393002986908, loss=0.035656124353408813
I0206 08:37:13.095012 139752176375552 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.06348732858896255, loss=0.0366276279091835
I0206 08:37:44.821953 139770806105856 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.07943854480981827, loss=0.036590538918972015
I0206 08:38:16.781742 139752176375552 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09726119786500931, loss=0.03679461404681206
I0206 08:38:48.586050 139770806105856 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.09187952429056168, loss=0.039137788116931915
I0206 08:39:20.445297 139752176375552 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.08850248903036118, loss=0.03470359742641449
I0206 08:39:51.977435 139770806105856 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08716398477554321, loss=0.03773949295282364
I0206 08:40:23.960493 139752176375552 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.04744415730237961, loss=0.03948616236448288
I0206 08:40:40.629122 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:42:22.788155 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:42:25.903945 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:42:30.197104 139978932307776 submission_runner.py:408] Time since start: 21219.28s, 	Step: 46053, 	{'train/accuracy': 0.989142119884491, 'train/loss': 0.03717942535877228, 'train/mean_average_precision': 0.22769445211762607, 'validation/accuracy': 0.9859349131584167, 'validation/loss': 0.048473209142684937, 'validation/mean_average_precision': 0.18759193675028046, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.05141131207346916, 'test/mean_average_precision': 0.18088843178256075, 'test/num_examples': 43793, 'score': 14657.908961057663, 'total_duration': 21219.284123420715, 'accumulated_submission_time': 14657.908961057663, 'accumulated_eval_time': 6558.222561836243, 'accumulated_logging_time': 1.9303202629089355}
I0206 08:42:30.226347 139770185602816 logging_writer.py:48] [46053] accumulated_eval_time=6558.222562, accumulated_logging_time=1.930320, accumulated_submission_time=14657.908961, global_step=46053, preemption_count=0, score=14657.908961, test/accuracy=0.984958, test/loss=0.051411, test/mean_average_precision=0.180888, test/num_examples=43793, total_duration=21219.284123, train/accuracy=0.989142, train/loss=0.037179, train/mean_average_precision=0.227694, validation/accuracy=0.985935, validation/loss=0.048473, validation/mean_average_precision=0.187592, validation/num_examples=43793
I0206 08:42:45.733130 139770797713152 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.05913173034787178, loss=0.03438028693199158
I0206 08:43:17.724337 139770185602816 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.059579670429229736, loss=0.03647527098655701
I0206 08:43:49.406974 139770797713152 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08622191101312637, loss=0.03833950310945511
I0206 08:44:20.884791 139770185602816 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.11615519225597382, loss=0.032995253801345825
I0206 08:44:52.492131 139770797713152 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.15661367774009705, loss=0.039879899471998215
I0206 08:45:24.072376 139770185602816 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08322495222091675, loss=0.03827539086341858
I0206 08:45:55.738331 139770797713152 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08086958527565002, loss=0.0422695092856884
I0206 08:46:27.589184 139770185602816 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07139503955841064, loss=0.04007888212800026
I0206 08:46:30.449973 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:48:05.526293 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:48:08.635777 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:48:11.766458 139978932307776 submission_runner.py:408] Time since start: 21560.85s, 	Step: 46810, 	{'train/accuracy': 0.989086389541626, 'train/loss': 0.037325117737054825, 'train/mean_average_precision': 0.23047543000247783, 'validation/accuracy': 0.9859126210212708, 'validation/loss': 0.04823572188615799, 'validation/mean_average_precision': 0.18950510700789985, 'validation/num_examples': 43793, 'test/accuracy': 0.9849451780319214, 'test/loss': 0.05100999027490616, 'test/mean_average_precision': 0.18085092998380684, 'test/num_examples': 43793, 'score': 14898.099229335785, 'total_duration': 21560.853466033936, 'accumulated_submission_time': 14898.099229335785, 'accumulated_eval_time': 6659.5389840602875, 'accumulated_logging_time': 1.9728610515594482}
I0206 08:48:11.790601 139752176375552 logging_writer.py:48] [46810] accumulated_eval_time=6659.538984, accumulated_logging_time=1.972861, accumulated_submission_time=14898.099229, global_step=46810, preemption_count=0, score=14898.099229, test/accuracy=0.984945, test/loss=0.051010, test/mean_average_precision=0.180851, test/num_examples=43793, total_duration=21560.853466, train/accuracy=0.989086, train/loss=0.037325, train/mean_average_precision=0.230475, validation/accuracy=0.985913, validation/loss=0.048236, validation/mean_average_precision=0.189505, validation/num_examples=43793
I0206 08:48:40.596620 139804984182528 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07763933390378952, loss=0.0416133850812912
I0206 08:49:12.614371 139752176375552 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08414172381162643, loss=0.033833179622888565
I0206 08:49:44.508646 139804984182528 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.14387010037899017, loss=0.0380670540034771
I0206 08:50:16.107295 139752176375552 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.13935250043869019, loss=0.038684744387865067
I0206 08:50:47.760968 139804984182528 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08210446685552597, loss=0.041113149374723434
I0206 08:51:19.633677 139752176375552 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07066000998020172, loss=0.03450903296470642
I0206 08:51:51.392779 139804984182528 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.12275028973817825, loss=0.038025908172130585
I0206 08:52:12.075902 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:53:54.035635 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:53:57.127393 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:54:00.149918 139978932307776 submission_runner.py:408] Time since start: 21909.24s, 	Step: 47566, 	{'train/accuracy': 0.9891598224639893, 'train/loss': 0.03719243034720421, 'train/mean_average_precision': 0.23410492165608968, 'validation/accuracy': 0.9859057068824768, 'validation/loss': 0.048074521124362946, 'validation/mean_average_precision': 0.19447153549109586, 'validation/num_examples': 43793, 'test/accuracy': 0.9849637150764465, 'test/loss': 0.05089467391371727, 'test/mean_average_precision': 0.18297250589360362, 'test/num_examples': 43793, 'score': 15138.353684902191, 'total_duration': 21909.236936807632, 'accumulated_submission_time': 15138.353684902191, 'accumulated_eval_time': 6767.612953901291, 'accumulated_logging_time': 2.0079030990600586}
I0206 08:54:00.174627 139770185602816 logging_writer.py:48] [47566] accumulated_eval_time=6767.612954, accumulated_logging_time=2.007903, accumulated_submission_time=15138.353685, global_step=47566, preemption_count=0, score=15138.353685, test/accuracy=0.984964, test/loss=0.050895, test/mean_average_precision=0.182973, test/num_examples=43793, total_duration=21909.236937, train/accuracy=0.989160, train/loss=0.037192, train/mean_average_precision=0.234105, validation/accuracy=0.985906, validation/loss=0.048075, validation/mean_average_precision=0.194472, validation/num_examples=43793
I0206 08:54:11.352166 139770797713152 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.05781228095293045, loss=0.038974348455667496
I0206 08:54:42.797576 139770185602816 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.07295887172222137, loss=0.04023021459579468
I0206 08:55:14.291114 139770797713152 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.12374906241893768, loss=0.039914943277835846
I0206 08:55:45.768410 139770185602816 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.07362344115972519, loss=0.038318462669849396
I0206 08:56:17.342545 139770797713152 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07774844765663147, loss=0.034201811999082565
I0206 08:56:48.984059 139770185602816 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.085996113717556, loss=0.034506384283304214
I0206 08:57:20.887993 139770797713152 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09054937958717346, loss=0.03340847045183182
I0206 08:57:52.258869 139770185602816 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.14012262225151062, loss=0.03385656699538231
I0206 08:58:00.151529 139978932307776 spec.py:321] Evaluating on the training split.
I0206 08:59:35.789554 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 08:59:38.913270 139978932307776 spec.py:349] Evaluating on the test split.
I0206 08:59:42.018073 139978932307776 submission_runner.py:408] Time since start: 22251.10s, 	Step: 48326, 	{'train/accuracy': 0.9889408946037292, 'train/loss': 0.03771990165114403, 'train/mean_average_precision': 0.23455893416809936, 'validation/accuracy': 0.985910177230835, 'validation/loss': 0.04861404001712799, 'validation/mean_average_precision': 0.19349217891836945, 'validation/num_examples': 43793, 'test/accuracy': 0.9850214123725891, 'test/loss': 0.05144278705120087, 'test/mean_average_precision': 0.18614619731923882, 'test/num_examples': 43793, 'score': 15378.298068523407, 'total_duration': 22251.104973077774, 'accumulated_submission_time': 15378.298068523407, 'accumulated_eval_time': 6869.479328393936, 'accumulated_logging_time': 2.0448648929595947}
I0206 08:59:42.042186 139752176375552 logging_writer.py:48] [48326] accumulated_eval_time=6869.479328, accumulated_logging_time=2.044865, accumulated_submission_time=15378.298069, global_step=48326, preemption_count=0, score=15378.298069, test/accuracy=0.985021, test/loss=0.051443, test/mean_average_precision=0.186146, test/num_examples=43793, total_duration=22251.104973, train/accuracy=0.988941, train/loss=0.037720, train/mean_average_precision=0.234559, validation/accuracy=0.985910, validation/loss=0.048614, validation/mean_average_precision=0.193492, validation/num_examples=43793
I0206 09:00:05.943513 139804984182528 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.06393995136022568, loss=0.03829037398099899
I0206 09:00:37.711261 139752176375552 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.1041065901517868, loss=0.03962613642215729
I0206 09:01:09.607736 139804984182528 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08070041984319687, loss=0.03699260577559471
I0206 09:01:41.069506 139752176375552 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.07108848541975021, loss=0.035734888166189194
I0206 09:02:12.983457 139804984182528 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.12131288647651672, loss=0.040417879819869995
I0206 09:02:44.757558 139752176375552 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.060316067188978195, loss=0.03350942209362984
I0206 09:03:16.327413 139804984182528 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.10095640271902084, loss=0.0367751307785511
I0206 09:03:42.314446 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:05:23.814142 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:05:26.950556 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:05:29.990926 139978932307776 submission_runner.py:408] Time since start: 22599.08s, 	Step: 49084, 	{'train/accuracy': 0.9892590641975403, 'train/loss': 0.03705105185508728, 'train/mean_average_precision': 0.22758353279423216, 'validation/accuracy': 0.9858817458152771, 'validation/loss': 0.048171184957027435, 'validation/mean_average_precision': 0.18696699713797535, 'validation/num_examples': 43793, 'test/accuracy': 0.9849550724029541, 'test/loss': 0.0509009025990963, 'test/mean_average_precision': 0.18424540330492756, 'test/num_examples': 43793, 'score': 15618.53837943077, 'total_duration': 22599.077934503555, 'accumulated_submission_time': 15618.53837943077, 'accumulated_eval_time': 6977.155750751495, 'accumulated_logging_time': 2.0811920166015625}
I0206 09:05:30.017261 139770185602816 logging_writer.py:48] [49084] accumulated_eval_time=6977.155751, accumulated_logging_time=2.081192, accumulated_submission_time=15618.538379, global_step=49084, preemption_count=0, score=15618.538379, test/accuracy=0.984955, test/loss=0.050901, test/mean_average_precision=0.184245, test/num_examples=43793, total_duration=22599.077935, train/accuracy=0.989259, train/loss=0.037051, train/mean_average_precision=0.227584, validation/accuracy=0.985882, validation/loss=0.048171, validation/mean_average_precision=0.186967, validation/num_examples=43793
I0206 09:05:35.673777 139770806105856 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.06166997179389, loss=0.03389060124754906
I0206 09:06:07.654078 139770185602816 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.10530415177345276, loss=0.041101980954408646
I0206 09:06:39.561081 139770806105856 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.05792034789919853, loss=0.03762655705213547
I0206 09:07:11.394555 139770185602816 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.06374754011631012, loss=0.03742731362581253
I0206 09:07:43.885616 139770806105856 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09462103992700577, loss=0.03590566664934158
I0206 09:08:16.190756 139770185602816 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.0700024962425232, loss=0.03820880129933357
I0206 09:08:47.955210 139770806105856 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.15530551970005035, loss=0.04244934022426605
I0206 09:09:19.484065 139770185602816 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08083387464284897, loss=0.03875047340989113
I0206 09:09:30.194456 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:11:09.796205 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:11:12.907346 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:11:15.936393 139978932307776 submission_runner.py:408] Time since start: 22945.02s, 	Step: 49835, 	{'train/accuracy': 0.9892882704734802, 'train/loss': 0.036769840866327286, 'train/mean_average_precision': 0.23883347998733698, 'validation/accuracy': 0.9859365224838257, 'validation/loss': 0.04837220162153244, 'validation/mean_average_precision': 0.18908768170199722, 'validation/num_examples': 43793, 'test/accuracy': 0.9850075244903564, 'test/loss': 0.05094403773546219, 'test/mean_average_precision': 0.18682860063281817, 'test/num_examples': 43793, 'score': 15858.68499994278, 'total_duration': 22945.02338528633, 'accumulated_submission_time': 15858.68499994278, 'accumulated_eval_time': 7082.8976101875305, 'accumulated_logging_time': 2.118276596069336}
I0206 09:11:15.961455 139770797713152 logging_writer.py:48] [49835] accumulated_eval_time=7082.897610, accumulated_logging_time=2.118277, accumulated_submission_time=15858.685000, global_step=49835, preemption_count=0, score=15858.685000, test/accuracy=0.985008, test/loss=0.050944, test/mean_average_precision=0.186829, test/num_examples=43793, total_duration=22945.023385, train/accuracy=0.989288, train/loss=0.036770, train/mean_average_precision=0.238833, validation/accuracy=0.985937, validation/loss=0.048372, validation/mean_average_precision=0.189088, validation/num_examples=43793
I0206 09:11:36.906464 139804984182528 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1485283523797989, loss=0.0357641726732254
I0206 09:12:08.340297 139770797713152 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.0628235936164856, loss=0.0347297340631485
I0206 09:12:40.044653 139804984182528 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1139310821890831, loss=0.037984851747751236
I0206 09:13:11.871350 139770797713152 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.06235954537987709, loss=0.03339396044611931
I0206 09:13:43.357050 139804984182528 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09664855152368546, loss=0.036243800073862076
I0206 09:14:15.068544 139770797713152 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.0688326433300972, loss=0.035397887229919434
I0206 09:14:46.807799 139804984182528 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08464004099369049, loss=0.03563323989510536
I0206 09:15:16.224137 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:16:56.871950 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:16:59.991330 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:17:03.097411 139978932307776 submission_runner.py:408] Time since start: 23292.18s, 	Step: 50593, 	{'train/accuracy': 0.9893097877502441, 'train/loss': 0.036671921610832214, 'train/mean_average_precision': 0.23933154547429297, 'validation/accuracy': 0.9859462976455688, 'validation/loss': 0.04780295491218567, 'validation/mean_average_precision': 0.18409884967549606, 'validation/num_examples': 43793, 'test/accuracy': 0.9849485754966736, 'test/loss': 0.0503535121679306, 'test/mean_average_precision': 0.18150416841286934, 'test/num_examples': 43793, 'score': 16098.916516304016, 'total_duration': 23292.18442082405, 'accumulated_submission_time': 16098.916516304016, 'accumulated_eval_time': 7189.770831346512, 'accumulated_logging_time': 2.1541895866394043}
I0206 09:17:03.123193 139752176375552 logging_writer.py:48] [50593] accumulated_eval_time=7189.770831, accumulated_logging_time=2.154190, accumulated_submission_time=16098.916516, global_step=50593, preemption_count=0, score=16098.916516, test/accuracy=0.984949, test/loss=0.050354, test/mean_average_precision=0.181504, test/num_examples=43793, total_duration=23292.184421, train/accuracy=0.989310, train/loss=0.036672, train/mean_average_precision=0.239332, validation/accuracy=0.985946, validation/loss=0.047803, validation/mean_average_precision=0.184099, validation/num_examples=43793
I0206 09:17:05.699273 139770185602816 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.0908326655626297, loss=0.03777659684419632
I0206 09:17:37.357626 139752176375552 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.14931286871433258, loss=0.038431134074926376
I0206 09:18:09.031595 139770185602816 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.08716349303722382, loss=0.03884229436516762
I0206 09:18:40.473334 139752176375552 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08609692752361298, loss=0.038503047078847885
I0206 09:19:11.931665 139770185602816 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.052026428282260895, loss=0.03566644713282585
I0206 09:19:43.486832 139752176375552 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.07263332605361938, loss=0.03732819855213165
I0206 09:20:15.198194 139770185602816 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08452510833740234, loss=0.037428345531225204
I0206 09:20:46.934091 139752176375552 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.07470846176147461, loss=0.03727572038769722
I0206 09:21:03.247181 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:22:39.542266 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:22:42.547438 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:22:45.512191 139978932307776 submission_runner.py:408] Time since start: 23634.60s, 	Step: 51352, 	{'train/accuracy': 0.9894444346427917, 'train/loss': 0.0360986664891243, 'train/mean_average_precision': 0.25477903296182586, 'validation/accuracy': 0.9859942197799683, 'validation/loss': 0.04791146144270897, 'validation/mean_average_precision': 0.1914060906010451, 'validation/num_examples': 43793, 'test/accuracy': 0.9850766062736511, 'test/loss': 0.050619255751371384, 'test/mean_average_precision': 0.1871973443523216, 'test/num_examples': 43793, 'score': 16339.007791280746, 'total_duration': 23634.599209308624, 'accumulated_submission_time': 16339.007791280746, 'accumulated_eval_time': 7292.035793304443, 'accumulated_logging_time': 2.192917823791504}
I0206 09:22:45.537861 139770806105856 logging_writer.py:48] [51352] accumulated_eval_time=7292.035793, accumulated_logging_time=2.192918, accumulated_submission_time=16339.007791, global_step=51352, preemption_count=0, score=16339.007791, test/accuracy=0.985077, test/loss=0.050619, test/mean_average_precision=0.187197, test/num_examples=43793, total_duration=23634.599209, train/accuracy=0.989444, train/loss=0.036099, train/mean_average_precision=0.254779, validation/accuracy=0.985994, validation/loss=0.047911, validation/mean_average_precision=0.191406, validation/num_examples=43793
I0206 09:23:01.205514 139804984182528 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.07281806319952011, loss=0.033932991325855255
I0206 09:23:32.626497 139770806105856 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.05716898664832115, loss=0.03786991909146309
I0206 09:24:04.175680 139804984182528 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.06819698214530945, loss=0.03731129318475723
I0206 09:24:35.386893 139770806105856 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09468759596347809, loss=0.035120803862810135
I0206 09:25:06.869439 139804984182528 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.15927208960056305, loss=0.03496743366122246
I0206 09:25:38.266671 139770806105856 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10114619135856628, loss=0.039136409759521484
I0206 09:26:09.709990 139804984182528 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.07373296469449997, loss=0.038862548768520355
I0206 09:26:41.240979 139770806105856 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.15924251079559326, loss=0.03983220458030701
I0206 09:26:45.658070 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:28:22.263623 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:28:25.255930 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:28:28.212001 139978932307776 submission_runner.py:408] Time since start: 23977.30s, 	Step: 52115, 	{'train/accuracy': 0.9893057346343994, 'train/loss': 0.036439668387174606, 'train/mean_average_precision': 0.24819287581792068, 'validation/accuracy': 0.9859434366226196, 'validation/loss': 0.047678787261247635, 'validation/mean_average_precision': 0.1925508469856677, 'validation/num_examples': 43793, 'test/accuracy': 0.9850197434425354, 'test/loss': 0.050336625427007675, 'test/mean_average_precision': 0.18937713188818903, 'test/num_examples': 43793, 'score': 16579.094081163406, 'total_duration': 23977.299023389816, 'accumulated_submission_time': 16579.094081163406, 'accumulated_eval_time': 7394.589677810669, 'accumulated_logging_time': 2.2324140071868896}
I0206 09:28:28.237618 139752176375552 logging_writer.py:48] [52115] accumulated_eval_time=7394.589678, accumulated_logging_time=2.232414, accumulated_submission_time=16579.094081, global_step=52115, preemption_count=0, score=16579.094081, test/accuracy=0.985020, test/loss=0.050337, test/mean_average_precision=0.189377, test/num_examples=43793, total_duration=23977.299023, train/accuracy=0.989306, train/loss=0.036440, train/mean_average_precision=0.248193, validation/accuracy=0.985943, validation/loss=0.047679, validation/mean_average_precision=0.192551, validation/num_examples=43793
I0206 09:28:55.771802 139770797713152 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.13814356923103333, loss=0.034593645483255386
I0206 09:29:27.557277 139752176375552 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.08092926442623138, loss=0.036043256521224976
I0206 09:29:59.197249 139770797713152 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.12312104552984238, loss=0.036154936999082565
I0206 09:30:30.791305 139752176375552 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.07252123206853867, loss=0.04003215208649635
I0206 09:31:02.549472 139770797713152 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.10360471159219742, loss=0.0372302420437336
I0206 09:31:34.670680 139752176375552 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.13020749390125275, loss=0.035946059972047806
I0206 09:32:06.316522 139770797713152 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1436057835817337, loss=0.035803452134132385
I0206 09:32:28.439123 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:34:03.410676 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:34:06.438972 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:34:09.367935 139978932307776 submission_runner.py:408] Time since start: 24318.45s, 	Step: 52871, 	{'train/accuracy': 0.9894201755523682, 'train/loss': 0.03616183623671532, 'train/mean_average_precision': 0.25292693828969454, 'validation/accuracy': 0.985986053943634, 'validation/loss': 0.048090603202581406, 'validation/mean_average_precision': 0.1971760256055278, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972294807434, 'test/loss': 0.05088086426258087, 'test/mean_average_precision': 0.18794931420806782, 'test/num_examples': 43793, 'score': 16819.26360821724, 'total_duration': 24318.454954862595, 'accumulated_submission_time': 16819.26360821724, 'accumulated_eval_time': 7495.518446922302, 'accumulated_logging_time': 2.2704098224639893}
I0206 09:34:09.393932 139770185602816 logging_writer.py:48] [52871] accumulated_eval_time=7495.518447, accumulated_logging_time=2.270410, accumulated_submission_time=16819.263608, global_step=52871, preemption_count=0, score=16819.263608, test/accuracy=0.985097, test/loss=0.050881, test/mean_average_precision=0.187949, test/num_examples=43793, total_duration=24318.454955, train/accuracy=0.989420, train/loss=0.036162, train/mean_average_precision=0.252927, validation/accuracy=0.985986, validation/loss=0.048091, validation/mean_average_precision=0.197176, validation/num_examples=43793
I0206 09:34:18.946377 139804984182528 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.12931160628795624, loss=0.04260561615228653
I0206 09:34:50.282206 139770185602816 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09154660999774933, loss=0.03644098713994026
I0206 09:35:21.909000 139804984182528 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.1725326031446457, loss=0.03701792657375336
I0206 09:35:53.442375 139770185602816 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09942764788866043, loss=0.03814249858260155
I0206 09:36:24.889350 139804984182528 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.12188123911619186, loss=0.04478778690099716
I0206 09:36:56.147415 139770185602816 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.08384047448635101, loss=0.03496444597840309
I0206 09:37:28.001465 139804984182528 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.1256386786699295, loss=0.036319099366664886
I0206 09:37:59.655767 139770185602816 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.1212107315659523, loss=0.03757207840681076
I0206 09:38:09.425044 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:39:46.641333 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:39:49.686712 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:39:52.652676 139978932307776 submission_runner.py:408] Time since start: 24661.74s, 	Step: 53632, 	{'train/accuracy': 0.9894657731056213, 'train/loss': 0.03598015010356903, 'train/mean_average_precision': 0.2576308237827217, 'validation/accuracy': 0.9860092401504517, 'validation/loss': 0.04783957079052925, 'validation/mean_average_precision': 0.19560399905387693, 'validation/num_examples': 43793, 'test/accuracy': 0.9850951433181763, 'test/loss': 0.05029488727450371, 'test/mean_average_precision': 0.18946398776317053, 'test/num_examples': 43793, 'score': 17059.2628159523, 'total_duration': 24661.73969578743, 'accumulated_submission_time': 17059.2628159523, 'accumulated_eval_time': 7598.746030807495, 'accumulated_logging_time': 2.308499336242676}
I0206 09:39:52.679232 139752176375552 logging_writer.py:48] [53632] accumulated_eval_time=7598.746031, accumulated_logging_time=2.308499, accumulated_submission_time=17059.262816, global_step=53632, preemption_count=0, score=17059.262816, test/accuracy=0.985095, test/loss=0.050295, test/mean_average_precision=0.189464, test/num_examples=43793, total_duration=24661.739696, train/accuracy=0.989466, train/loss=0.035980, train/mean_average_precision=0.257631, validation/accuracy=0.986009, validation/loss=0.047840, validation/mean_average_precision=0.195604, validation/num_examples=43793
I0206 09:40:14.779057 139770806105856 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.21709738671779633, loss=0.038017719984054565
I0206 09:40:45.987074 139752176375552 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1358129233121872, loss=0.036691222339868546
I0206 09:41:17.347932 139770806105856 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.13847333192825317, loss=0.03599489852786064
I0206 09:41:49.188047 139752176375552 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.09088435769081116, loss=0.03822892904281616
I0206 09:42:20.527307 139770806105856 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1094384416937828, loss=0.036137763410806656
I0206 09:42:52.053676 139752176375552 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.06747402995824814, loss=0.03649276867508888
I0206 09:43:23.535031 139770806105856 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09567677229642868, loss=0.035700324922800064
I0206 09:43:52.662220 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:45:30.919408 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:45:33.950959 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:45:36.913468 139978932307776 submission_runner.py:408] Time since start: 25006.00s, 	Step: 54393, 	{'train/accuracy': 0.9890407919883728, 'train/loss': 0.03752331808209419, 'train/mean_average_precision': 0.22529144773405752, 'validation/accuracy': 0.9857092499732971, 'validation/loss': 0.04871625825762749, 'validation/mean_average_precision': 0.18467604560021897, 'validation/num_examples': 43793, 'test/accuracy': 0.9847282767295837, 'test/loss': 0.05140816047787666, 'test/mean_average_precision': 0.1770618964335692, 'test/num_examples': 43793, 'score': 17299.21511077881, 'total_duration': 25006.000480890274, 'accumulated_submission_time': 17299.21511077881, 'accumulated_eval_time': 7702.997226715088, 'accumulated_logging_time': 2.3462722301483154}
I0206 09:45:36.939170 139770797713152 logging_writer.py:48] [54393] accumulated_eval_time=7702.997227, accumulated_logging_time=2.346272, accumulated_submission_time=17299.215111, global_step=54393, preemption_count=0, score=17299.215111, test/accuracy=0.984728, test/loss=0.051408, test/mean_average_precision=0.177062, test/num_examples=43793, total_duration=25006.000481, train/accuracy=0.989041, train/loss=0.037523, train/mean_average_precision=0.225291, validation/accuracy=0.985709, validation/loss=0.048716, validation/mean_average_precision=0.184676, validation/num_examples=43793
I0206 09:45:39.491792 139804984182528 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.11707932502031326, loss=0.03693128749728203
I0206 09:46:11.254519 139770797713152 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.10150839388370514, loss=0.03600214421749115
I0206 09:46:42.795836 139804984182528 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.11053069680929184, loss=0.037291545420885086
I0206 09:47:14.460331 139770797713152 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.10756579041481018, loss=0.03915567696094513
I0206 09:47:45.746076 139804984182528 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.08110706508159637, loss=0.03555046766996384
I0206 09:48:17.448937 139770797713152 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10958632081747055, loss=0.0366116501390934
I0206 09:48:49.093319 139804984182528 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.08611550182104111, loss=0.03481193259358406
I0206 09:49:20.567019 139770797713152 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.11830390244722366, loss=0.036943450570106506
I0206 09:49:37.183342 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:51:13.313354 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:51:16.329834 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:51:19.297266 139978932307776 submission_runner.py:408] Time since start: 25348.38s, 	Step: 55154, 	{'train/accuracy': 0.9894230365753174, 'train/loss': 0.036137502640485764, 'train/mean_average_precision': 0.24171861130247058, 'validation/accuracy': 0.9860656261444092, 'validation/loss': 0.047809481620788574, 'validation/mean_average_precision': 0.1957881328081809, 'validation/num_examples': 43793, 'test/accuracy': 0.9851566553115845, 'test/loss': 0.05043136700987816, 'test/mean_average_precision': 0.18510774874874678, 'test/num_examples': 43793, 'score': 17539.427534341812, 'total_duration': 25348.384285211563, 'accumulated_submission_time': 17539.427534341812, 'accumulated_eval_time': 7805.111110925674, 'accumulated_logging_time': 2.3839142322540283}
I0206 09:51:19.323523 139770185602816 logging_writer.py:48] [55154] accumulated_eval_time=7805.111111, accumulated_logging_time=2.383914, accumulated_submission_time=17539.427534, global_step=55154, preemption_count=0, score=17539.427534, test/accuracy=0.985157, test/loss=0.050431, test/mean_average_precision=0.185108, test/num_examples=43793, total_duration=25348.384285, train/accuracy=0.989423, train/loss=0.036138, train/mean_average_precision=0.241719, validation/accuracy=0.986066, validation/loss=0.047809, validation/mean_average_precision=0.195788, validation/num_examples=43793
I0206 09:51:34.131155 139770806105856 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.09721071273088455, loss=0.034920427948236465
I0206 09:52:05.671767 139770185602816 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.11496783047914505, loss=0.035774655640125275
I0206 09:52:37.425091 139770806105856 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10678322613239288, loss=0.03743462637066841
I0206 09:53:09.215202 139770185602816 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.07878411561250687, loss=0.033444326370954514
I0206 09:53:41.063596 139770806105856 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.14808134734630585, loss=0.0351686030626297
I0206 09:54:12.961601 139770185602816 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10916709154844284, loss=0.03686148300766945
I0206 09:54:44.672893 139770806105856 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.08348900079727173, loss=0.0353417806327343
I0206 09:55:16.478942 139770185602816 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.0965745821595192, loss=0.034870777279138565
I0206 09:55:19.320116 139978932307776 spec.py:321] Evaluating on the training split.
I0206 09:56:56.569985 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 09:56:59.619116 139978932307776 spec.py:349] Evaluating on the test split.
I0206 09:57:02.664701 139978932307776 submission_runner.py:408] Time since start: 25691.75s, 	Step: 55910, 	{'train/accuracy': 0.9894699454307556, 'train/loss': 0.036292899399995804, 'train/mean_average_precision': 0.25152001206536745, 'validation/accuracy': 0.9860270619392395, 'validation/loss': 0.0474446602165699, 'validation/mean_average_precision': 0.19686378699877044, 'validation/num_examples': 43793, 'test/accuracy': 0.985159158706665, 'test/loss': 0.04995325952768326, 'test/mean_average_precision': 0.18886979263849543, 'test/num_examples': 43793, 'score': 17779.39391064644, 'total_duration': 25691.751713991165, 'accumulated_submission_time': 17779.39391064644, 'accumulated_eval_time': 7908.455640792847, 'accumulated_logging_time': 2.4211137294769287}
I0206 09:57:02.690585 139752176375552 logging_writer.py:48] [55910] accumulated_eval_time=7908.455641, accumulated_logging_time=2.421114, accumulated_submission_time=17779.393911, global_step=55910, preemption_count=0, score=17779.393911, test/accuracy=0.985159, test/loss=0.049953, test/mean_average_precision=0.188870, test/num_examples=43793, total_duration=25691.751714, train/accuracy=0.989470, train/loss=0.036293, train/mean_average_precision=0.251520, validation/accuracy=0.986027, validation/loss=0.047445, validation/mean_average_precision=0.196864, validation/num_examples=43793
I0206 09:57:31.450520 139804984182528 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.07707203924655914, loss=0.0326094925403595
I0206 09:58:02.798114 139752176375552 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.08352315425872803, loss=0.03969194367527962
I0206 09:58:34.101696 139804984182528 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.08630118519067764, loss=0.03809121996164322
I0206 09:59:05.558273 139752176375552 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.0882662907242775, loss=0.03635545074939728
I0206 09:59:37.095769 139804984182528 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.14586783945560455, loss=0.03460671752691269
I0206 10:00:08.640521 139752176375552 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.0870649591088295, loss=0.03817262873053551
I0206 10:00:40.036615 139804984182528 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.15097332000732422, loss=0.03188101574778557
I0206 10:01:02.855674 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:02:40.333340 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:02:43.317172 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:02:46.282199 139978932307776 submission_runner.py:408] Time since start: 26035.37s, 	Step: 56674, 	{'train/accuracy': 0.9894645810127258, 'train/loss': 0.035822756588459015, 'train/mean_average_precision': 0.25739268458964026, 'validation/accuracy': 0.9861395359039307, 'validation/loss': 0.04740425571799278, 'validation/mean_average_precision': 0.20158968950371123, 'validation/num_examples': 43793, 'test/accuracy': 0.9852097034454346, 'test/loss': 0.0502450168132782, 'test/mean_average_precision': 0.1947143161858327, 'test/num_examples': 43793, 'score': 18019.52805519104, 'total_duration': 26035.369215011597, 'accumulated_submission_time': 18019.52805519104, 'accumulated_eval_time': 8011.882115125656, 'accumulated_logging_time': 2.458162784576416}
I0206 10:02:46.309024 139770185602816 logging_writer.py:48] [56674] accumulated_eval_time=8011.882115, accumulated_logging_time=2.458163, accumulated_submission_time=18019.528055, global_step=56674, preemption_count=0, score=18019.528055, test/accuracy=0.985210, test/loss=0.050245, test/mean_average_precision=0.194714, test/num_examples=43793, total_duration=26035.369215, train/accuracy=0.989465, train/loss=0.035823, train/mean_average_precision=0.257393, validation/accuracy=0.986140, validation/loss=0.047404, validation/mean_average_precision=0.201590, validation/num_examples=43793
I0206 10:02:54.982350 139770806105856 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08355873823165894, loss=0.03567447513341904
I0206 10:03:26.614752 139770185602816 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1344652622938156, loss=0.03772130236029625
I0206 10:03:58.259892 139770806105856 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.12490987032651901, loss=0.03302015736699104
I0206 10:04:30.215710 139770185602816 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.10576630383729935, loss=0.03739018365740776
I0206 10:05:01.785044 139770806105856 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.17854206264019012, loss=0.0354623943567276
I0206 10:05:33.332665 139770185602816 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.08757708966732025, loss=0.03740713745355606
I0206 10:06:05.197368 139770806105856 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.14861135184764862, loss=0.03506733477115631
I0206 10:06:36.702517 139770185602816 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.15805667638778687, loss=0.036424677819013596
I0206 10:06:46.520484 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:08:19.520453 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:08:22.542484 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:08:25.489657 139978932307776 submission_runner.py:408] Time since start: 26374.58s, 	Step: 57432, 	{'train/accuracy': 0.9895554780960083, 'train/loss': 0.03577268868684769, 'train/mean_average_precision': 0.2601344773012853, 'validation/accuracy': 0.9859702587127686, 'validation/loss': 0.04734781011939049, 'validation/mean_average_precision': 0.19831942626624036, 'validation/num_examples': 43793, 'test/accuracy': 0.985026478767395, 'test/loss': 0.050025396049022675, 'test/mean_average_precision': 0.190945310755514, 'test/num_examples': 43793, 'score': 18259.709180355072, 'total_duration': 26374.576673030853, 'accumulated_submission_time': 18259.709180355072, 'accumulated_eval_time': 8110.851234436035, 'accumulated_logging_time': 2.49564528465271}
I0206 10:08:25.516257 139752176375552 logging_writer.py:48] [57432] accumulated_eval_time=8110.851234, accumulated_logging_time=2.495645, accumulated_submission_time=18259.709180, global_step=57432, preemption_count=0, score=18259.709180, test/accuracy=0.985026, test/loss=0.050025, test/mean_average_precision=0.190945, test/num_examples=43793, total_duration=26374.576673, train/accuracy=0.989555, train/loss=0.035773, train/mean_average_precision=0.260134, validation/accuracy=0.985970, validation/loss=0.047348, validation/mean_average_precision=0.198319, validation/num_examples=43793
I0206 10:08:47.258711 139770797713152 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.0827493816614151, loss=0.03556402772665024
I0206 10:09:18.560605 139752176375552 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.07810889929533005, loss=0.03601868450641632
I0206 10:09:50.267674 139770797713152 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.14327052235603333, loss=0.04038669168949127
I0206 10:10:21.810207 139752176375552 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.08737076073884964, loss=0.03349175304174423
I0206 10:10:53.608691 139770797713152 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.11603750288486481, loss=0.031708475202322006
I0206 10:11:25.532194 139752176375552 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.20722319185733795, loss=0.035151299089193344
I0206 10:11:57.161789 139770797713152 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.14265267550945282, loss=0.03443531692028046
I0206 10:12:02.900028 139752176375552 logging_writer.py:48] [58119] global_step=58119, preemption_count=0, score=18477.048362
I0206 10:12:02.969650 139978932307776 checkpoints.py:490] Saving checkpoint at step: 58119
I0206 10:12:03.105383 139978932307776 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4/checkpoint_58119
I0206 10:12:03.107739 139978932307776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_4/checkpoint_58119.
I0206 10:12:03.252108 139978932307776 submission_runner.py:583] Tuning trial 4/5
I0206 10:12:03.252347 139978932307776 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0206 10:12:03.257360 139978932307776 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.38870498538017273, 'train/loss': 0.7994356751441956, 'train/mean_average_precision': 0.023099368034654698, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.024843149604178178, 'validation/num_examples': 43793, 'test/accuracy': 0.3947480618953705, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.02636636073612592, 'test/num_examples': 43793, 'score': 11.89487600326538, 'total_duration': 116.68095016479492, 'accumulated_submission_time': 11.89487600326538, 'accumulated_eval_time': 104.78602743148804, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (748, {'train/accuracy': 0.9867178201675415, 'train/loss': 0.05405712500214577, 'train/mean_average_precision': 0.041328803588909446, 'validation/accuracy': 0.984153687953949, 'validation/loss': 0.06424260884523392, 'validation/mean_average_precision': 0.04055070544653996, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06758540123701096, 'test/mean_average_precision': 0.0414079131255289, 'test/num_examples': 43793, 'score': 251.93400812149048, 'total_duration': 463.8443031311035, 'accumulated_submission_time': 251.93400812149048, 'accumulated_eval_time': 211.86702752113342, 'accumulated_logging_time': 0.023287296295166016, 'global_step': 748, 'preemption_count': 0}), (1500, {'train/accuracy': 0.9868547916412354, 'train/loss': 0.0534534752368927, 'train/mean_average_precision': 0.04040749192785452, 'validation/accuracy': 0.984169065952301, 'validation/loss': 0.06380725651979446, 'validation/mean_average_precision': 0.04014752498615162, 'validation/num_examples': 43793, 'test/accuracy': 0.9832056760787964, 'test/loss': 0.06706760823726654, 'test/mean_average_precision': 0.04194428881865402, 'test/num_examples': 43793, 'score': 491.94453287124634, 'total_duration': 807.0779266357422, 'accumulated_submission_time': 491.94453287124634, 'accumulated_eval_time': 315.04234194755554, 'accumulated_logging_time': 0.05069994926452637, 'global_step': 1500, 'preemption_count': 0}), (2255, {'train/accuracy': 0.9868224263191223, 'train/loss': 0.05183904245495796, 'train/mean_average_precision': 0.05263785527638787, 'validation/accuracy': 0.9842417240142822, 'validation/loss': 0.062132131308317184, 'validation/mean_average_precision': 0.04984300730249895, 'validation/num_examples': 43793, 'test/accuracy': 0.9832654595375061, 'test/loss': 0.06538085639476776, 'test/mean_average_precision': 0.05076436693773005, 'test/num_examples': 43793, 'score': 731.971120595932, 'total_duration': 1154.2588243484497, 'accumulated_submission_time': 731.971120595932, 'accumulated_eval_time': 422.14896512031555, 'accumulated_logging_time': 0.07857346534729004, 'global_step': 2255, 'preemption_count': 0}), (3012, {'train/accuracy': 0.9869569540023804, 'train/loss': 0.049530282616615295, 'train/mean_average_precision': 0.06984231221718884, 'validation/accuracy': 0.98429936170578, 'validation/loss': 0.05969257652759552, 'validation/mean_average_precision': 0.06538215512675465, 'validation/num_examples': 43793, 'test/accuracy': 0.9833332896232605, 'test/loss': 0.06334034353494644, 'test/mean_average_precision': 0.06630883860500196, 'test/num_examples': 43793, 'score': 972.108246088028, 'total_duration': 1500.8414223194122, 'accumulated_submission_time': 972.108246088028, 'accumulated_eval_time': 528.547721862793, 'accumulated_logging_time': 0.10526752471923828, 'global_step': 3012, 'preemption_count': 0}), (3768, {'train/accuracy': 0.9871721863746643, 'train/loss': 0.04804283380508423, 'train/mean_average_precision': 0.09283866234241844, 'validation/accuracy': 0.9844650030136108, 'validation/loss': 0.05787404999136925, 'validation/mean_average_precision': 0.09066849691709097, 'validation/num_examples': 43793, 'test/accuracy': 0.9835565090179443, 'test/loss': 0.06136487051844597, 'test/mean_average_precision': 0.09212002824112336, 'test/num_examples': 43793, 'score': 1212.2337291240692, 'total_duration': 1849.313452243805, 'accumulated_submission_time': 1212.2337291240692, 'accumulated_eval_time': 636.8478765487671, 'accumulated_logging_time': 0.13160991668701172, 'global_step': 3768, 'preemption_count': 0}), (4522, {'train/accuracy': 0.9874393939971924, 'train/loss': 0.04534367099404335, 'train/mean_average_precision': 0.12454040697946092, 'validation/accuracy': 0.9847800135612488, 'validation/loss': 0.05480606481432915, 'validation/mean_average_precision': 0.11117304640724955, 'validation/num_examples': 43793, 'test/accuracy': 0.9838467240333557, 'test/loss': 0.057779669761657715, 'test/mean_average_precision': 0.11733046716598139, 'test/num_examples': 43793, 'score': 1452.3454518318176, 'total_duration': 2194.1322169303894, 'accumulated_submission_time': 1452.3454518318176, 'accumulated_eval_time': 741.5066666603088, 'accumulated_logging_time': 0.15953278541564941, 'global_step': 4522, 'preemption_count': 0}), (5269, {'train/accuracy': 0.9877853393554688, 'train/loss': 0.04363641142845154, 'train/mean_average_precision': 0.13495412738983195, 'validation/accuracy': 0.9847633838653564, 'validation/loss': 0.05361570790410042, 'validation/mean_average_precision': 0.1228188394742861, 'validation/num_examples': 43793, 'test/accuracy': 0.9838109016418457, 'test/loss': 0.056455958634614944, 'test/mean_average_precision': 0.128097338821601, 'test/num_examples': 43793, 'score': 1692.4349780082703, 'total_duration': 2540.757847547531, 'accumulated_submission_time': 1692.4349780082703, 'accumulated_eval_time': 847.9944367408752, 'accumulated_logging_time': 0.18788361549377441, 'global_step': 5269, 'preemption_count': 0}), (6017, {'train/accuracy': 0.9877686500549316, 'train/loss': 0.0430554561316967, 'train/mean_average_precision': 0.15247730794115238, 'validation/accuracy': 0.9850386381149292, 'validation/loss': 0.05250886082649231, 'validation/mean_average_precision': 0.13898969018160748, 'validation/num_examples': 43793, 'test/accuracy': 0.9840855598449707, 'test/loss': 0.05529408901929855, 'test/mean_average_precision': 0.14267761147463756, 'test/num_examples': 43793, 'score': 1932.6885635852814, 'total_duration': 2890.5146567821503, 'accumulated_submission_time': 1932.6885635852814, 'accumulated_eval_time': 957.4483652114868, 'accumulated_logging_time': 0.21676349639892578, 'global_step': 6017, 'preemption_count': 0}), (6771, {'train/accuracy': 0.9881178140640259, 'train/loss': 0.04193282872438431, 'train/mean_average_precision': 0.14712715805639054, 'validation/accuracy': 0.9850714802742004, 'validation/loss': 0.051934532821178436, 'validation/mean_average_precision': 0.13690095667623864, 'validation/num_examples': 43793, 'test/accuracy': 0.984137773513794, 'test/loss': 0.054795846343040466, 'test/mean_average_precision': 0.13726670168943106, 'test/num_examples': 43793, 'score': 2172.8519797325134, 'total_duration': 3240.1714539527893, 'accumulated_submission_time': 2172.8519797325134, 'accumulated_eval_time': 1066.894276857376, 'accumulated_logging_time': 0.24420785903930664, 'global_step': 6771, 'preemption_count': 0}), (7522, {'train/accuracy': 0.9878849387168884, 'train/loss': 0.04250209033489227, 'train/mean_average_precision': 0.15903786419567645, 'validation/accuracy': 0.9852147698402405, 'validation/loss': 0.051785580813884735, 'validation/mean_average_precision': 0.13968376306762054, 'validation/num_examples': 43793, 'test/accuracy': 0.9842430949211121, 'test/loss': 0.054584402590990067, 'test/mean_average_precision': 0.13880957022061796, 'test/num_examples': 43793, 'score': 2412.9996078014374, 'total_duration': 3587.0254402160645, 'accumulated_submission_time': 2412.9996078014374, 'accumulated_eval_time': 1173.5531170368195, 'accumulated_logging_time': 0.2712841033935547, 'global_step': 7522, 'preemption_count': 0}), (8278, {'train/accuracy': 0.9880563616752625, 'train/loss': 0.041752468794584274, 'train/mean_average_precision': 0.16108226297426947, 'validation/accuracy': 0.9852578043937683, 'validation/loss': 0.05095980688929558, 'validation/mean_average_precision': 0.14882748193841347, 'validation/num_examples': 43793, 'test/accuracy': 0.9843167662620544, 'test/loss': 0.053699295967817307, 'test/mean_average_precision': 0.1497537212633665, 'test/num_examples': 43793, 'score': 2653.148269176483, 'total_duration': 3936.214086532593, 'accumulated_submission_time': 2653.148269176483, 'accumulated_eval_time': 1282.5451092720032, 'accumulated_logging_time': 0.2990598678588867, 'global_step': 8278, 'preemption_count': 0}), (9026, {'train/accuracy': 0.9882237315177917, 'train/loss': 0.04226386919617653, 'train/mean_average_precision': 0.1666925139943225, 'validation/accuracy': 0.9851611852645874, 'validation/loss': 0.05139685422182083, 'validation/mean_average_precision': 0.1470027461109916, 'validation/num_examples': 43793, 'test/accuracy': 0.9842881560325623, 'test/loss': 0.053785428404808044, 'test/mean_average_precision': 0.14674788369270744, 'test/num_examples': 43793, 'score': 2893.1906435489655, 'total_duration': 4281.849505901337, 'accumulated_submission_time': 2893.1906435489655, 'accumulated_eval_time': 1388.0890011787415, 'accumulated_logging_time': 0.3277096748352051, 'global_step': 9026, 'preemption_count': 0}), (9778, {'train/accuracy': 0.988153874874115, 'train/loss': 0.04126552864909172, 'train/mean_average_precision': 0.17768094811149476, 'validation/accuracy': 0.9853041172027588, 'validation/loss': 0.05078065022826195, 'validation/mean_average_precision': 0.1524563100207836, 'validation/num_examples': 43793, 'test/accuracy': 0.9843904972076416, 'test/loss': 0.05362384766340256, 'test/mean_average_precision': 0.15504898866218714, 'test/num_examples': 43793, 'score': 3133.3405344486237, 'total_duration': 4625.398817539215, 'accumulated_submission_time': 3133.3405344486237, 'accumulated_eval_time': 1491.4383640289307, 'accumulated_logging_time': 0.35793447494506836, 'global_step': 9778, 'preemption_count': 0}), (10526, {'train/accuracy': 0.988370418548584, 'train/loss': 0.04057848080992699, 'train/mean_average_precision': 0.1764253451645966, 'validation/accuracy': 0.9853061437606812, 'validation/loss': 0.05046161636710167, 'validation/mean_average_precision': 0.15362740943123013, 'validation/num_examples': 43793, 'test/accuracy': 0.9843748807907104, 'test/loss': 0.05324826017022133, 'test/mean_average_precision': 0.16294833313941567, 'test/num_examples': 43793, 'score': 3373.36945104599, 'total_duration': 4973.312300443649, 'accumulated_submission_time': 3373.36945104599, 'accumulated_eval_time': 1599.2725040912628, 'accumulated_logging_time': 0.3884594440460205, 'global_step': 10526, 'preemption_count': 0}), (11280, {'train/accuracy': 0.9882692694664001, 'train/loss': 0.04072313383221626, 'train/mean_average_precision': 0.18064637046975113, 'validation/accuracy': 0.9853795766830444, 'validation/loss': 0.05063217505812645, 'validation/mean_average_precision': 0.15163249736390821, 'validation/num_examples': 43793, 'test/accuracy': 0.9844081997871399, 'test/loss': 0.053570620715618134, 'test/mean_average_precision': 0.15353083904258258, 'test/num_examples': 43793, 'score': 3613.459751367569, 'total_duration': 5317.750963449478, 'accumulated_submission_time': 3613.459751367569, 'accumulated_eval_time': 1703.5707485675812, 'accumulated_logging_time': 0.41864895820617676, 'global_step': 11280, 'preemption_count': 0}), (12034, {'train/accuracy': 0.9883570671081543, 'train/loss': 0.040610287338495255, 'train/mean_average_precision': 0.18789730580368405, 'validation/accuracy': 0.9853954315185547, 'validation/loss': 0.05130927264690399, 'validation/mean_average_precision': 0.14694854662782258, 'validation/num_examples': 43793, 'test/accuracy': 0.9845050573348999, 'test/loss': 0.05422745645046234, 'test/mean_average_precision': 0.1533889459399407, 'test/num_examples': 43793, 'score': 3853.7374968528748, 'total_duration': 5664.29166841507, 'accumulated_submission_time': 3853.7374968528748, 'accumulated_eval_time': 1809.7845659255981, 'accumulated_logging_time': 0.4482748508453369, 'global_step': 12034, 'preemption_count': 0}), (12793, {'train/accuracy': 0.9884112477302551, 'train/loss': 0.04033699259161949, 'train/mean_average_precision': 0.18814700029006395, 'validation/accuracy': 0.9853706955909729, 'validation/loss': 0.05050535500049591, 'validation/mean_average_precision': 0.15826210688516798, 'validation/num_examples': 43793, 'test/accuracy': 0.9844494462013245, 'test/loss': 0.05334792658686638, 'test/mean_average_precision': 0.16042928879542304, 'test/num_examples': 43793, 'score': 4093.757108926773, 'total_duration': 6008.2779705524445, 'accumulated_submission_time': 4093.757108926773, 'accumulated_eval_time': 1913.6998341083527, 'accumulated_logging_time': 0.4794890880584717, 'global_step': 12793, 'preemption_count': 0}), (13551, {'train/accuracy': 0.9883461594581604, 'train/loss': 0.040721870958805084, 'train/mean_average_precision': 0.18812930461424757, 'validation/accuracy': 0.9853450655937195, 'validation/loss': 0.05106170102953911, 'validation/mean_average_precision': 0.16256916073948913, 'validation/num_examples': 43793, 'test/accuracy': 0.9844717979431152, 'test/loss': 0.05389748513698578, 'test/mean_average_precision': 0.15899843356600682, 'test/num_examples': 43793, 'score': 4334.002388477325, 'total_duration': 6353.242366313934, 'accumulated_submission_time': 4334.002388477325, 'accumulated_eval_time': 2018.3670988082886, 'accumulated_logging_time': 0.5114123821258545, 'global_step': 13551, 'preemption_count': 0}), (14309, {'train/accuracy': 0.9886543154716492, 'train/loss': 0.03959142789244652, 'train/mean_average_precision': 0.18865576898000522, 'validation/accuracy': 0.9855687618255615, 'validation/loss': 0.04963674396276474, 'validation/mean_average_precision': 0.16727042522727417, 'validation/num_examples': 43793, 'test/accuracy': 0.9845977425575256, 'test/loss': 0.0522046722471714, 'test/mean_average_precision': 0.17068206760017082, 'test/num_examples': 43793, 'score': 4574.218851804733, 'total_duration': 6702.318521976471, 'accumulated_submission_time': 4574.218851804733, 'accumulated_eval_time': 2127.1761713027954, 'accumulated_logging_time': 0.542165994644165, 'global_step': 14309, 'preemption_count': 0}), (15065, {'train/accuracy': 0.9882400631904602, 'train/loss': 0.04076105356216431, 'train/mean_average_precision': 0.1768932244139741, 'validation/accuracy': 0.98537677526474, 'validation/loss': 0.0505119189620018, 'validation/mean_average_precision': 0.16216779284240665, 'validation/num_examples': 43793, 'test/accuracy': 0.984438955783844, 'test/loss': 0.05335091054439545, 'test/mean_average_precision': 0.15989612033431252, 'test/num_examples': 43793, 'score': 4814.288403272629, 'total_duration': 7051.812611579895, 'accumulated_submission_time': 4814.288403272629, 'accumulated_eval_time': 2236.5487127304077, 'accumulated_logging_time': 0.5745348930358887, 'global_step': 15065, 'preemption_count': 0}), (15819, {'train/accuracy': 0.9884235262870789, 'train/loss': 0.04031607508659363, 'train/mean_average_precision': 0.18005961443945234, 'validation/accuracy': 0.9854745864868164, 'validation/loss': 0.050167400389909744, 'validation/mean_average_precision': 0.15899198403434275, 'validation/num_examples': 43793, 'test/accuracy': 0.9844840168952942, 'test/loss': 0.05308358371257782, 'test/mean_average_precision': 0.16104881745609437, 'test/num_examples': 43793, 'score': 5054.5061230659485, 'total_duration': 7397.481730699539, 'accumulated_submission_time': 5054.5061230659485, 'accumulated_eval_time': 2341.950671672821, 'accumulated_logging_time': 0.6038038730621338, 'global_step': 15819, 'preemption_count': 0}), (16575, {'train/accuracy': 0.9884944558143616, 'train/loss': 0.040065258741378784, 'train/mean_average_precision': 0.19465300055209425, 'validation/accuracy': 0.9855245351791382, 'validation/loss': 0.04934633895754814, 'validation/mean_average_precision': 0.16485746697257292, 'validation/num_examples': 43793, 'test/accuracy': 0.9845366477966309, 'test/loss': 0.052238624542951584, 'test/mean_average_precision': 0.15859879831130447, 'test/num_examples': 43793, 'score': 5294.6321749687195, 'total_duration': 7745.110958576202, 'accumulated_submission_time': 5294.6321749687195, 'accumulated_eval_time': 2449.4044041633606, 'accumulated_logging_time': 0.6335453987121582, 'global_step': 16575, 'preemption_count': 0}), (17338, {'train/accuracy': 0.9884665012359619, 'train/loss': 0.04012645035982132, 'train/mean_average_precision': 0.18912319578821066, 'validation/accuracy': 0.9855257272720337, 'validation/loss': 0.04987785592675209, 'validation/mean_average_precision': 0.16157510315203238, 'validation/num_examples': 43793, 'test/accuracy': 0.9845648407936096, 'test/loss': 0.0527358315885067, 'test/mean_average_precision': 0.16286102605605052, 'test/num_examples': 43793, 'score': 5534.787349462509, 'total_duration': 8088.665885448456, 'accumulated_submission_time': 5534.787349462509, 'accumulated_eval_time': 2552.754077911377, 'accumulated_logging_time': 0.6638550758361816, 'global_step': 17338, 'preemption_count': 0}), (18102, {'train/accuracy': 0.9886404275894165, 'train/loss': 0.03960519656538963, 'train/mean_average_precision': 0.20165343855178597, 'validation/accuracy': 0.985596776008606, 'validation/loss': 0.04983937740325928, 'validation/mean_average_precision': 0.16603703175047954, 'validation/num_examples': 43793, 'test/accuracy': 0.9846120476722717, 'test/loss': 0.052671339362859726, 'test/mean_average_precision': 0.16606212858015526, 'test/num_examples': 43793, 'score': 5774.893808841705, 'total_duration': 8433.951842308044, 'accumulated_submission_time': 5774.893808841705, 'accumulated_eval_time': 2657.8833651542664, 'accumulated_logging_time': 0.6942009925842285, 'global_step': 18102, 'preemption_count': 0}), (18858, {'train/accuracy': 0.9884719252586365, 'train/loss': 0.04005434364080429, 'train/mean_average_precision': 0.18568562201382344, 'validation/accuracy': 0.9854221940040588, 'validation/loss': 0.05040871351957321, 'validation/mean_average_precision': 0.15669814211524635, 'validation/num_examples': 43793, 'test/accuracy': 0.9844696521759033, 'test/loss': 0.05345328897237778, 'test/mean_average_precision': 0.15697611739618625, 'test/num_examples': 43793, 'score': 6014.983092546463, 'total_duration': 8780.828848361969, 'accumulated_submission_time': 6014.983092546463, 'accumulated_eval_time': 2764.619640350342, 'accumulated_logging_time': 0.7254533767700195, 'global_step': 18858, 'preemption_count': 0}), (19617, {'train/accuracy': 0.9885842204093933, 'train/loss': 0.039523422718048096, 'train/mean_average_precision': 0.19490424779512652, 'validation/accuracy': 0.9855241179466248, 'validation/loss': 0.05002894625067711, 'validation/mean_average_precision': 0.1680505834962828, 'validation/num_examples': 43793, 'test/accuracy': 0.9845610857009888, 'test/loss': 0.052960846573114395, 'test/mean_average_precision': 0.16620601517443098, 'test/num_examples': 43793, 'score': 6255.108852148056, 'total_duration': 9122.684712648392, 'accumulated_submission_time': 6255.108852148056, 'accumulated_eval_time': 2866.3005216121674, 'accumulated_logging_time': 0.7550556659698486, 'global_step': 19617, 'preemption_count': 0}), (20370, {'train/accuracy': 0.9885866641998291, 'train/loss': 0.04034075513482094, 'train/mean_average_precision': 0.19055236333307302, 'validation/accuracy': 0.985431969165802, 'validation/loss': 0.049722012132406235, 'validation/mean_average_precision': 0.16182988616494326, 'validation/num_examples': 43793, 'test/accuracy': 0.984565258026123, 'test/loss': 0.052569009363651276, 'test/mean_average_precision': 0.16090210738048638, 'test/num_examples': 43793, 'score': 6495.332993984222, 'total_duration': 9472.132626056671, 'accumulated_submission_time': 6495.332993984222, 'accumulated_eval_time': 2975.4721944332123, 'accumulated_logging_time': 0.787273645401001, 'global_step': 20370, 'preemption_count': 0}), (21128, {'train/accuracy': 0.9888412952423096, 'train/loss': 0.03901582956314087, 'train/mean_average_precision': 0.19844235275635128, 'validation/accuracy': 0.9855196475982666, 'validation/loss': 0.04963627830147743, 'validation/mean_average_precision': 0.1704775937419216, 'validation/num_examples': 43793, 'test/accuracy': 0.9846183657646179, 'test/loss': 0.05252900719642639, 'test/mean_average_precision': 0.16738075430890176, 'test/num_examples': 43793, 'score': 6735.384921073914, 'total_duration': 9814.043020009995, 'accumulated_submission_time': 6735.384921073914, 'accumulated_eval_time': 3077.2809023857117, 'accumulated_logging_time': 0.8171370029449463, 'global_step': 21128, 'preemption_count': 0}), (21888, {'train/accuracy': 0.988599419593811, 'train/loss': 0.0396040715277195, 'train/mean_average_precision': 0.1955595368230457, 'validation/accuracy': 0.9856069087982178, 'validation/loss': 0.04932992532849312, 'validation/mean_average_precision': 0.17090716974554493, 'validation/num_examples': 43793, 'test/accuracy': 0.9847084879875183, 'test/loss': 0.05207694321870804, 'test/mean_average_precision': 0.16870305237778235, 'test/num_examples': 43793, 'score': 6975.345520019531, 'total_duration': 10161.952837228775, 'accumulated_submission_time': 6975.345520019531, 'accumulated_eval_time': 3185.1781203746796, 'accumulated_logging_time': 0.8489246368408203, 'global_step': 21888, 'preemption_count': 0}), (22640, {'train/accuracy': 0.9885961413383484, 'train/loss': 0.03991701081395149, 'train/mean_average_precision': 0.19312791976085006, 'validation/accuracy': 0.9854997396469116, 'validation/loss': 0.04980744421482086, 'validation/mean_average_precision': 0.16469691309091544, 'validation/num_examples': 43793, 'test/accuracy': 0.9846465587615967, 'test/loss': 0.05239691212773323, 'test/mean_average_precision': 0.16187797766191273, 'test/num_examples': 43793, 'score': 7215.383780956268, 'total_duration': 10508.548904895782, 'accumulated_submission_time': 7215.383780956268, 'accumulated_eval_time': 3291.6833214759827, 'accumulated_logging_time': 0.8813366889953613, 'global_step': 22640, 'preemption_count': 0}), (23395, {'train/accuracy': 0.9885261058807373, 'train/loss': 0.039845407009124756, 'train/mean_average_precision': 0.20450545013479177, 'validation/accuracy': 0.9855204820632935, 'validation/loss': 0.04962068051099777, 'validation/mean_average_precision': 0.1696779981251334, 'validation/num_examples': 43793, 'test/accuracy': 0.9845758080482483, 'test/loss': 0.05225157365202904, 'test/mean_average_precision': 0.16689131979029878, 'test/num_examples': 43793, 'score': 7455.464512586594, 'total_duration': 10854.205847978592, 'accumulated_submission_time': 7455.464512586594, 'accumulated_eval_time': 3397.2014861106873, 'accumulated_logging_time': 0.9188354015350342, 'global_step': 23395, 'preemption_count': 0}), (24150, {'train/accuracy': 0.9885877370834351, 'train/loss': 0.03958648443222046, 'train/mean_average_precision': 0.198987881596415, 'validation/accuracy': 0.9856178760528564, 'validation/loss': 0.04945551976561546, 'validation/mean_average_precision': 0.17012725483797447, 'validation/num_examples': 43793, 'test/accuracy': 0.9847046732902527, 'test/loss': 0.052401501685380936, 'test/mean_average_precision': 0.16773314797260425, 'test/num_examples': 43793, 'score': 7695.480072259903, 'total_duration': 11197.75633430481, 'accumulated_submission_time': 7695.480072259903, 'accumulated_eval_time': 3500.685555458069, 'accumulated_logging_time': 0.9494316577911377, 'global_step': 24150, 'preemption_count': 0}), (24895, {'train/accuracy': 0.9885725378990173, 'train/loss': 0.039620619267225266, 'train/mean_average_precision': 0.18849833988280928, 'validation/accuracy': 0.9855935573577881, 'validation/loss': 0.049551356583833694, 'validation/mean_average_precision': 0.16380789915553823, 'validation/num_examples': 43793, 'test/accuracy': 0.9846591949462891, 'test/loss': 0.05243632197380066, 'test/mean_average_precision': 0.16223804286958965, 'test/num_examples': 43793, 'score': 7935.451113462448, 'total_duration': 11544.484077215195, 'accumulated_submission_time': 7935.451113462448, 'accumulated_eval_time': 3607.390516757965, 'accumulated_logging_time': 0.980963945388794, 'global_step': 24895, 'preemption_count': 0}), (25646, {'train/accuracy': 0.9884662628173828, 'train/loss': 0.0406963936984539, 'train/mean_average_precision': 0.18891245798950673, 'validation/accuracy': 0.9852277636528015, 'validation/loss': 0.05177460238337517, 'validation/mean_average_precision': 0.1620684550428439, 'validation/num_examples': 43793, 'test/accuracy': 0.9842435121536255, 'test/loss': 0.05533933266997337, 'test/mean_average_precision': 0.14969740247395008, 'test/num_examples': 43793, 'score': 8175.517338037491, 'total_duration': 11890.685946941376, 'accumulated_submission_time': 8175.517338037491, 'accumulated_eval_time': 3713.474276781082, 'accumulated_logging_time': 1.012589931488037, 'global_step': 25646, 'preemption_count': 0}), (26396, {'train/accuracy': 0.9886945486068726, 'train/loss': 0.039120517671108246, 'train/mean_average_precision': 0.20767401350788825, 'validation/accuracy': 0.9855862259864807, 'validation/loss': 0.04942043125629425, 'validation/mean_average_precision': 0.16990612842383535, 'validation/num_examples': 43793, 'test/accuracy': 0.9846705794334412, 'test/loss': 0.0522284172475338, 'test/mean_average_precision': 0.1665207318270333, 'test/num_examples': 43793, 'score': 8415.550181627274, 'total_duration': 12235.662456035614, 'accumulated_submission_time': 8415.550181627274, 'accumulated_eval_time': 3818.3646759986877, 'accumulated_logging_time': 1.0460069179534912, 'global_step': 26396, 'preemption_count': 0}), (27156, {'train/accuracy': 0.9885122776031494, 'train/loss': 0.03939487785100937, 'train/mean_average_precision': 0.2045708442516365, 'validation/accuracy': 0.9854699373245239, 'validation/loss': 0.04987451806664467, 'validation/mean_average_precision': 0.17688852857115214, 'validation/num_examples': 43793, 'test/accuracy': 0.9845792055130005, 'test/loss': 0.05277697369456291, 'test/mean_average_precision': 0.17506006175059088, 'test/num_examples': 43793, 'score': 8655.730852127075, 'total_duration': 12581.70412158966, 'accumulated_submission_time': 8655.730852127075, 'accumulated_eval_time': 3924.174050092697, 'accumulated_logging_time': 1.0775668621063232, 'global_step': 27156, 'preemption_count': 0}), (27913, {'train/accuracy': 0.9870010018348694, 'train/loss': 0.04721558466553688, 'train/mean_average_precision': 0.11037968819824376, 'validation/accuracy': 0.9840448498725891, 'validation/loss': 0.05742224305868149, 'validation/mean_average_precision': 0.09850573291118715, 'validation/num_examples': 43793, 'test/accuracy': 0.9831374287605286, 'test/loss': 0.06042836606502533, 'test/mean_average_precision': 0.10381847942793537, 'test/num_examples': 43793, 'score': 8895.984867811203, 'total_duration': 12928.676401853561, 'accumulated_submission_time': 8895.984867811203, 'accumulated_eval_time': 4030.839526414871, 'accumulated_logging_time': 1.110039234161377, 'global_step': 27913, 'preemption_count': 0}), (28671, {'train/accuracy': 0.98862624168396, 'train/loss': 0.03928302228450775, 'train/mean_average_precision': 0.2086000109865182, 'validation/accuracy': 0.9856077432632446, 'validation/loss': 0.04985186085104942, 'validation/mean_average_precision': 0.1624308171851189, 'validation/num_examples': 43793, 'test/accuracy': 0.9846815466880798, 'test/loss': 0.05285181477665901, 'test/mean_average_precision': 0.16642025833074262, 'test/num_examples': 43793, 'score': 9135.996017217636, 'total_duration': 13274.639439105988, 'accumulated_submission_time': 9135.996017217636, 'accumulated_eval_time': 4136.737054347992, 'accumulated_logging_time': 1.1446597576141357, 'global_step': 28671, 'preemption_count': 0}), (29423, {'train/accuracy': 0.9888563752174377, 'train/loss': 0.03860921412706375, 'train/mean_average_precision': 0.1952220106581476, 'validation/accuracy': 0.9856755137443542, 'validation/loss': 0.04896049574017525, 'validation/mean_average_precision': 0.17366474140797325, 'validation/num_examples': 43793, 'test/accuracy': 0.9847674369812012, 'test/loss': 0.05177288129925728, 'test/mean_average_precision': 0.17578625348674065, 'test/num_examples': 43793, 'score': 9376.222169399261, 'total_duration': 13618.191462039948, 'accumulated_submission_time': 9376.222169399261, 'accumulated_eval_time': 4240.01061463356, 'accumulated_logging_time': 1.1763203144073486, 'global_step': 29423, 'preemption_count': 0}), (30177, {'train/accuracy': 0.9887091517448425, 'train/loss': 0.03916693478822708, 'train/mean_average_precision': 0.2039721153390068, 'validation/accuracy': 0.9856755137443542, 'validation/loss': 0.049208447337150574, 'validation/mean_average_precision': 0.17392643637769953, 'validation/num_examples': 43793, 'test/accuracy': 0.9847522974014282, 'test/loss': 0.05200439319014549, 'test/mean_average_precision': 0.17425520445439235, 'test/num_examples': 43793, 'score': 9616.297856807709, 'total_duration': 13963.307752609253, 'accumulated_submission_time': 9616.297856807709, 'accumulated_eval_time': 4344.998346328735, 'accumulated_logging_time': 1.209303617477417, 'global_step': 30177, 'preemption_count': 0}), (30930, {'train/accuracy': 0.9886908531188965, 'train/loss': 0.039093196392059326, 'train/mean_average_precision': 0.1994932528086023, 'validation/accuracy': 0.9856739044189453, 'validation/loss': 0.04953419044613838, 'validation/mean_average_precision': 0.1778154397527345, 'validation/num_examples': 43793, 'test/accuracy': 0.9847704172134399, 'test/loss': 0.05254374444484711, 'test/mean_average_precision': 0.1694027176352054, 'test/num_examples': 43793, 'score': 9856.373237371445, 'total_duration': 14304.544981956482, 'accumulated_submission_time': 9856.373237371445, 'accumulated_eval_time': 4446.108120918274, 'accumulated_logging_time': 1.2410881519317627, 'global_step': 30930, 'preemption_count': 0}), (31693, {'train/accuracy': 0.9885149598121643, 'train/loss': 0.03962215408682823, 'train/mean_average_precision': 0.19421593475752508, 'validation/accuracy': 0.9857165217399597, 'validation/loss': 0.04926184192299843, 'validation/mean_average_precision': 0.1747485124816603, 'validation/num_examples': 43793, 'test/accuracy': 0.9847164750099182, 'test/loss': 0.05217207968235016, 'test/mean_average_precision': 0.17375916759515642, 'test/num_examples': 43793, 'score': 10096.422625780106, 'total_duration': 14649.05891919136, 'accumulated_submission_time': 10096.422625780106, 'accumulated_eval_time': 4550.519570350647, 'accumulated_logging_time': 1.2745091915130615, 'global_step': 31693, 'preemption_count': 0}), (32452, {'train/accuracy': 0.988632082939148, 'train/loss': 0.039258670061826706, 'train/mean_average_precision': 0.20436964645347408, 'validation/accuracy': 0.9855963587760925, 'validation/loss': 0.04927314445376396, 'validation/mean_average_precision': 0.175708370852377, 'validation/num_examples': 43793, 'test/accuracy': 0.9845787882804871, 'test/loss': 0.05223559960722923, 'test/mean_average_precision': 0.1658119645684935, 'test/num_examples': 43793, 'score': 10336.437881469727, 'total_duration': 14996.374430418015, 'accumulated_submission_time': 10336.437881469727, 'accumulated_eval_time': 4657.7661418914795, 'accumulated_logging_time': 1.3084368705749512, 'global_step': 32452, 'preemption_count': 0}), (33217, {'train/accuracy': 0.9888612031936646, 'train/loss': 0.03862139210104942, 'train/mean_average_precision': 0.2074459373809987, 'validation/accuracy': 0.9858188033103943, 'validation/loss': 0.04930514469742775, 'validation/mean_average_precision': 0.18337859176326854, 'validation/num_examples': 43793, 'test/accuracy': 0.9848264455795288, 'test/loss': 0.05229155346751213, 'test/mean_average_precision': 0.1770464495206037, 'test/num_examples': 43793, 'score': 10576.67140340805, 'total_duration': 15339.427662611008, 'accumulated_submission_time': 10576.67140340805, 'accumulated_eval_time': 4760.532192707062, 'accumulated_logging_time': 1.342067003250122, 'global_step': 33217, 'preemption_count': 0}), (33968, {'train/accuracy': 0.988823652267456, 'train/loss': 0.03878296911716461, 'train/mean_average_precision': 0.20970713652735315, 'validation/accuracy': 0.9857664704322815, 'validation/loss': 0.049136847257614136, 'validation/mean_average_precision': 0.17837035100668272, 'validation/num_examples': 43793, 'test/accuracy': 0.9848175644874573, 'test/loss': 0.052116215229034424, 'test/mean_average_precision': 0.17825656076499521, 'test/num_examples': 43793, 'score': 10816.757288694382, 'total_duration': 15688.211569309235, 'accumulated_submission_time': 10816.757288694382, 'accumulated_eval_time': 4869.175209760666, 'accumulated_logging_time': 1.3774352073669434, 'global_step': 33968, 'preemption_count': 0}), (34725, {'train/accuracy': 0.9889429807662964, 'train/loss': 0.03803737834095955, 'train/mean_average_precision': 0.21792488734815457, 'validation/accuracy': 0.9857100248336792, 'validation/loss': 0.048944659531116486, 'validation/mean_average_precision': 0.17134611019919604, 'validation/num_examples': 43793, 'test/accuracy': 0.9847207069396973, 'test/loss': 0.051899559795856476, 'test/mean_average_precision': 0.1696407534624433, 'test/num_examples': 43793, 'score': 11056.986756563187, 'total_duration': 16036.28269791603, 'accumulated_submission_time': 11056.986756563187, 'accumulated_eval_time': 4976.963281154633, 'accumulated_logging_time': 1.4110808372497559, 'global_step': 34725, 'preemption_count': 0}), (35479, {'train/accuracy': 0.9887934327125549, 'train/loss': 0.038353241980075836, 'train/mean_average_precision': 0.2175011480776211, 'validation/accuracy': 0.985688328742981, 'validation/loss': 0.0491277314722538, 'validation/mean_average_precision': 0.18221929719318744, 'validation/num_examples': 43793, 'test/accuracy': 0.9847089052200317, 'test/loss': 0.052242908626794815, 'test/mean_average_precision': 0.1731621813997392, 'test/num_examples': 43793, 'score': 11297.105067253113, 'total_duration': 16382.713775157928, 'accumulated_submission_time': 11297.105067253113, 'accumulated_eval_time': 5083.223834276199, 'accumulated_logging_time': 1.4434118270874023, 'global_step': 35479, 'preemption_count': 0}), (36232, {'train/accuracy': 0.9889160990715027, 'train/loss': 0.038255613297224045, 'train/mean_average_precision': 0.2174486785456183, 'validation/accuracy': 0.9857396483421326, 'validation/loss': 0.048740580677986145, 'validation/mean_average_precision': 0.1753565414250365, 'validation/num_examples': 43793, 'test/accuracy': 0.984761118888855, 'test/loss': 0.05180809274315834, 'test/mean_average_precision': 0.16818006022956916, 'test/num_examples': 43793, 'score': 11537.17705130577, 'total_duration': 16728.115339756012, 'accumulated_submission_time': 11537.17705130577, 'accumulated_eval_time': 5188.498993873596, 'accumulated_logging_time': 1.47762131690979, 'global_step': 36232, 'preemption_count': 0}), (36979, {'train/accuracy': 0.9888588786125183, 'train/loss': 0.038587551563978195, 'train/mean_average_precision': 0.21144420240805334, 'validation/accuracy': 0.9856597185134888, 'validation/loss': 0.049098994582891464, 'validation/mean_average_precision': 0.17396346792948608, 'validation/num_examples': 43793, 'test/accuracy': 0.9847059845924377, 'test/loss': 0.05208496376872063, 'test/mean_average_precision': 0.17090511164112587, 'test/num_examples': 43793, 'score': 11777.208164453506, 'total_duration': 17072.413522958755, 'accumulated_submission_time': 11777.208164453506, 'accumulated_eval_time': 5292.71022772789, 'accumulated_logging_time': 1.513559103012085, 'global_step': 36979, 'preemption_count': 0}), (37734, {'train/accuracy': 0.988965630531311, 'train/loss': 0.03813077136874199, 'train/mean_average_precision': 0.21632119362024058, 'validation/accuracy': 0.9856706857681274, 'validation/loss': 0.049130287021398544, 'validation/mean_average_precision': 0.17558204788664689, 'validation/num_examples': 43793, 'test/accuracy': 0.9846655130386353, 'test/loss': 0.051924485713243484, 'test/mean_average_precision': 0.168482360769061, 'test/num_examples': 43793, 'score': 12017.201822519302, 'total_duration': 17419.296885967255, 'accumulated_submission_time': 12017.201822519302, 'accumulated_eval_time': 5399.546529531479, 'accumulated_logging_time': 1.54679536819458, 'global_step': 37734, 'preemption_count': 0}), (38494, {'train/accuracy': 0.9889072179794312, 'train/loss': 0.038279175758361816, 'train/mean_average_precision': 0.2058152897816714, 'validation/accuracy': 0.9858139753341675, 'validation/loss': 0.04839226230978966, 'validation/mean_average_precision': 0.1763234303719536, 'validation/num_examples': 43793, 'test/accuracy': 0.9848403334617615, 'test/loss': 0.05114014074206352, 'test/mean_average_precision': 0.17291108656222037, 'test/num_examples': 43793, 'score': 12257.161703586578, 'total_duration': 17762.10279250145, 'accumulated_submission_time': 12257.161703586578, 'accumulated_eval_time': 5502.337423086166, 'accumulated_logging_time': 1.5815489292144775, 'global_step': 38494, 'preemption_count': 0}), (39250, {'train/accuracy': 0.9886656999588013, 'train/loss': 0.0391177199780941, 'train/mean_average_precision': 0.20900084989286502, 'validation/accuracy': 0.9853398203849792, 'validation/loss': 0.049594368785619736, 'validation/mean_average_precision': 0.17641425803007238, 'validation/num_examples': 43793, 'test/accuracy': 0.9843534231185913, 'test/loss': 0.05261526629328728, 'test/mean_average_precision': 0.1698303266528774, 'test/num_examples': 43793, 'score': 12497.155459403992, 'total_duration': 18107.39728331566, 'accumulated_submission_time': 12497.155459403992, 'accumulated_eval_time': 5607.583341121674, 'accumulated_logging_time': 1.6165733337402344, 'global_step': 39250, 'preemption_count': 0}), (40008, {'train/accuracy': 0.988823652267456, 'train/loss': 0.03839899227023125, 'train/mean_average_precision': 0.21181767514573896, 'validation/accuracy': 0.9857344031333923, 'validation/loss': 0.04862920567393303, 'validation/mean_average_precision': 0.18505388601295117, 'validation/num_examples': 43793, 'test/accuracy': 0.9848032593727112, 'test/loss': 0.05150538682937622, 'test/mean_average_precision': 0.17641508945720805, 'test/num_examples': 43793, 'score': 12737.230195999146, 'total_duration': 18451.95160317421, 'accumulated_submission_time': 12737.230195999146, 'accumulated_eval_time': 5712.008977174759, 'accumulated_logging_time': 1.6503448486328125, 'global_step': 40008, 'preemption_count': 0}), (40765, {'train/accuracy': 0.988957405090332, 'train/loss': 0.03811485692858696, 'train/mean_average_precision': 0.22504035243759737, 'validation/accuracy': 0.9858992099761963, 'validation/loss': 0.0484270341694355, 'validation/mean_average_precision': 0.19021670604537053, 'validation/num_examples': 43793, 'test/accuracy': 0.98490309715271, 'test/loss': 0.05145435035228729, 'test/mean_average_precision': 0.1773463244063912, 'test/num_examples': 43793, 'score': 12977.429183483124, 'total_duration': 18795.01301598549, 'accumulated_submission_time': 12977.429183483124, 'accumulated_eval_time': 5814.817209243774, 'accumulated_logging_time': 1.6850457191467285, 'global_step': 40765, 'preemption_count': 0}), (41525, {'train/accuracy': 0.9889556169509888, 'train/loss': 0.037930309772491455, 'train/mean_average_precision': 0.22477926950095242, 'validation/accuracy': 0.9858963489532471, 'validation/loss': 0.048464491963386536, 'validation/mean_average_precision': 0.18542875821589172, 'validation/num_examples': 43793, 'test/accuracy': 0.9849595427513123, 'test/loss': 0.05132923275232315, 'test/mean_average_precision': 0.17811563395579139, 'test/num_examples': 43793, 'score': 13217.376702070236, 'total_duration': 19139.04503273964, 'accumulated_submission_time': 13217.376702070236, 'accumulated_eval_time': 5918.8470821380615, 'accumulated_logging_time': 1.7193021774291992, 'global_step': 41525, 'preemption_count': 0}), (42284, {'train/accuracy': 0.9891242384910583, 'train/loss': 0.03746219351887703, 'train/mean_average_precision': 0.2190261638573074, 'validation/accuracy': 0.9858837723731995, 'validation/loss': 0.04842827469110489, 'validation/mean_average_precision': 0.18823987726990113, 'validation/num_examples': 43793, 'test/accuracy': 0.9849300384521484, 'test/loss': 0.0514775849878788, 'test/mean_average_precision': 0.17285506517278354, 'test/num_examples': 43793, 'score': 13457.534606933594, 'total_duration': 19485.082169294357, 'accumulated_submission_time': 13457.534606933594, 'accumulated_eval_time': 6024.671246051788, 'accumulated_logging_time': 1.7549612522125244, 'global_step': 42284, 'preemption_count': 0}), (43039, {'train/accuracy': 0.9891573190689087, 'train/loss': 0.0371311753988266, 'train/mean_average_precision': 0.2342620274547162, 'validation/accuracy': 0.9858505129814148, 'validation/loss': 0.04841018468141556, 'validation/mean_average_precision': 0.18300957679890592, 'validation/num_examples': 43793, 'test/accuracy': 0.9848761558532715, 'test/loss': 0.051237087696790695, 'test/mean_average_precision': 0.17557429269603625, 'test/num_examples': 43793, 'score': 13697.556046247482, 'total_duration': 19833.571689128876, 'accumulated_submission_time': 13697.556046247482, 'accumulated_eval_time': 6133.084717750549, 'accumulated_logging_time': 1.7895762920379639, 'global_step': 43039, 'preemption_count': 0}), (43794, {'train/accuracy': 0.989040195941925, 'train/loss': 0.03743480145931244, 'train/mean_average_precision': 0.2364169550507474, 'validation/accuracy': 0.9858590364456177, 'validation/loss': 0.04835963249206543, 'validation/mean_average_precision': 0.1810546301375452, 'validation/num_examples': 43793, 'test/accuracy': 0.984913170337677, 'test/loss': 0.05103975161910057, 'test/mean_average_precision': 0.17830234383257554, 'test/num_examples': 43793, 'score': 13937.573343992233, 'total_duration': 20183.47554087639, 'accumulated_submission_time': 13937.573343992233, 'accumulated_eval_time': 6242.9166431427, 'accumulated_logging_time': 1.8242683410644531, 'global_step': 43794, 'preemption_count': 0}), (44546, {'train/accuracy': 0.9892123341560364, 'train/loss': 0.03711209073662758, 'train/mean_average_precision': 0.23362917116675708, 'validation/accuracy': 0.9858176112174988, 'validation/loss': 0.048688534647226334, 'validation/mean_average_precision': 0.18658132410017447, 'validation/num_examples': 43793, 'test/accuracy': 0.9848727583885193, 'test/loss': 0.05150778964161873, 'test/mean_average_precision': 0.17870415084724914, 'test/num_examples': 43793, 'score': 14177.521633148193, 'total_duration': 20525.747420549393, 'accumulated_submission_time': 14177.521633148193, 'accumulated_eval_time': 6345.184635639191, 'accumulated_logging_time': 1.8598694801330566, 'global_step': 44546, 'preemption_count': 0}), (45299, {'train/accuracy': 0.989071786403656, 'train/loss': 0.0375232920050621, 'train/mean_average_precision': 0.22586479658455655, 'validation/accuracy': 0.9858951568603516, 'validation/loss': 0.04855578392744064, 'validation/mean_average_precision': 0.18478510657217334, 'validation/num_examples': 43793, 'test/accuracy': 0.9849607944488525, 'test/loss': 0.05158171057701111, 'test/mean_average_precision': 0.1814172696895895, 'test/num_examples': 43793, 'score': 14417.702919960022, 'total_duration': 20869.454118013382, 'accumulated_submission_time': 14417.702919960022, 'accumulated_eval_time': 6448.654628753662, 'accumulated_logging_time': 1.8947200775146484, 'global_step': 45299, 'preemption_count': 0}), (46053, {'train/accuracy': 0.989142119884491, 'train/loss': 0.03717942535877228, 'train/mean_average_precision': 0.22769445211762607, 'validation/accuracy': 0.9859349131584167, 'validation/loss': 0.048473209142684937, 'validation/mean_average_precision': 0.18759193675028046, 'validation/num_examples': 43793, 'test/accuracy': 0.9849578142166138, 'test/loss': 0.05141131207346916, 'test/mean_average_precision': 0.18088843178256075, 'test/num_examples': 43793, 'score': 14657.908961057663, 'total_duration': 21219.284123420715, 'accumulated_submission_time': 14657.908961057663, 'accumulated_eval_time': 6558.222561836243, 'accumulated_logging_time': 1.9303202629089355, 'global_step': 46053, 'preemption_count': 0}), (46810, {'train/accuracy': 0.989086389541626, 'train/loss': 0.037325117737054825, 'train/mean_average_precision': 0.23047543000247783, 'validation/accuracy': 0.9859126210212708, 'validation/loss': 0.04823572188615799, 'validation/mean_average_precision': 0.18950510700789985, 'validation/num_examples': 43793, 'test/accuracy': 0.9849451780319214, 'test/loss': 0.05100999027490616, 'test/mean_average_precision': 0.18085092998380684, 'test/num_examples': 43793, 'score': 14898.099229335785, 'total_duration': 21560.853466033936, 'accumulated_submission_time': 14898.099229335785, 'accumulated_eval_time': 6659.5389840602875, 'accumulated_logging_time': 1.9728610515594482, 'global_step': 46810, 'preemption_count': 0}), (47566, {'train/accuracy': 0.9891598224639893, 'train/loss': 0.03719243034720421, 'train/mean_average_precision': 0.23410492165608968, 'validation/accuracy': 0.9859057068824768, 'validation/loss': 0.048074521124362946, 'validation/mean_average_precision': 0.19447153549109586, 'validation/num_examples': 43793, 'test/accuracy': 0.9849637150764465, 'test/loss': 0.05089467391371727, 'test/mean_average_precision': 0.18297250589360362, 'test/num_examples': 43793, 'score': 15138.353684902191, 'total_duration': 21909.236936807632, 'accumulated_submission_time': 15138.353684902191, 'accumulated_eval_time': 6767.612953901291, 'accumulated_logging_time': 2.0079030990600586, 'global_step': 47566, 'preemption_count': 0}), (48326, {'train/accuracy': 0.9889408946037292, 'train/loss': 0.03771990165114403, 'train/mean_average_precision': 0.23455893416809936, 'validation/accuracy': 0.985910177230835, 'validation/loss': 0.04861404001712799, 'validation/mean_average_precision': 0.19349217891836945, 'validation/num_examples': 43793, 'test/accuracy': 0.9850214123725891, 'test/loss': 0.05144278705120087, 'test/mean_average_precision': 0.18614619731923882, 'test/num_examples': 43793, 'score': 15378.298068523407, 'total_duration': 22251.104973077774, 'accumulated_submission_time': 15378.298068523407, 'accumulated_eval_time': 6869.479328393936, 'accumulated_logging_time': 2.0448648929595947, 'global_step': 48326, 'preemption_count': 0}), (49084, {'train/accuracy': 0.9892590641975403, 'train/loss': 0.03705105185508728, 'train/mean_average_precision': 0.22758353279423216, 'validation/accuracy': 0.9858817458152771, 'validation/loss': 0.048171184957027435, 'validation/mean_average_precision': 0.18696699713797535, 'validation/num_examples': 43793, 'test/accuracy': 0.9849550724029541, 'test/loss': 0.0509009025990963, 'test/mean_average_precision': 0.18424540330492756, 'test/num_examples': 43793, 'score': 15618.53837943077, 'total_duration': 22599.077934503555, 'accumulated_submission_time': 15618.53837943077, 'accumulated_eval_time': 6977.155750751495, 'accumulated_logging_time': 2.0811920166015625, 'global_step': 49084, 'preemption_count': 0}), (49835, {'train/accuracy': 0.9892882704734802, 'train/loss': 0.036769840866327286, 'train/mean_average_precision': 0.23883347998733698, 'validation/accuracy': 0.9859365224838257, 'validation/loss': 0.04837220162153244, 'validation/mean_average_precision': 0.18908768170199722, 'validation/num_examples': 43793, 'test/accuracy': 0.9850075244903564, 'test/loss': 0.05094403773546219, 'test/mean_average_precision': 0.18682860063281817, 'test/num_examples': 43793, 'score': 15858.68499994278, 'total_duration': 22945.02338528633, 'accumulated_submission_time': 15858.68499994278, 'accumulated_eval_time': 7082.8976101875305, 'accumulated_logging_time': 2.118276596069336, 'global_step': 49835, 'preemption_count': 0}), (50593, {'train/accuracy': 0.9893097877502441, 'train/loss': 0.036671921610832214, 'train/mean_average_precision': 0.23933154547429297, 'validation/accuracy': 0.9859462976455688, 'validation/loss': 0.04780295491218567, 'validation/mean_average_precision': 0.18409884967549606, 'validation/num_examples': 43793, 'test/accuracy': 0.9849485754966736, 'test/loss': 0.0503535121679306, 'test/mean_average_precision': 0.18150416841286934, 'test/num_examples': 43793, 'score': 16098.916516304016, 'total_duration': 23292.18442082405, 'accumulated_submission_time': 16098.916516304016, 'accumulated_eval_time': 7189.770831346512, 'accumulated_logging_time': 2.1541895866394043, 'global_step': 50593, 'preemption_count': 0}), (51352, {'train/accuracy': 0.9894444346427917, 'train/loss': 0.0360986664891243, 'train/mean_average_precision': 0.25477903296182586, 'validation/accuracy': 0.9859942197799683, 'validation/loss': 0.04791146144270897, 'validation/mean_average_precision': 0.1914060906010451, 'validation/num_examples': 43793, 'test/accuracy': 0.9850766062736511, 'test/loss': 0.050619255751371384, 'test/mean_average_precision': 0.1871973443523216, 'test/num_examples': 43793, 'score': 16339.007791280746, 'total_duration': 23634.599209308624, 'accumulated_submission_time': 16339.007791280746, 'accumulated_eval_time': 7292.035793304443, 'accumulated_logging_time': 2.192917823791504, 'global_step': 51352, 'preemption_count': 0}), (52115, {'train/accuracy': 0.9893057346343994, 'train/loss': 0.036439668387174606, 'train/mean_average_precision': 0.24819287581792068, 'validation/accuracy': 0.9859434366226196, 'validation/loss': 0.047678787261247635, 'validation/mean_average_precision': 0.1925508469856677, 'validation/num_examples': 43793, 'test/accuracy': 0.9850197434425354, 'test/loss': 0.050336625427007675, 'test/mean_average_precision': 0.18937713188818903, 'test/num_examples': 43793, 'score': 16579.094081163406, 'total_duration': 23977.299023389816, 'accumulated_submission_time': 16579.094081163406, 'accumulated_eval_time': 7394.589677810669, 'accumulated_logging_time': 2.2324140071868896, 'global_step': 52115, 'preemption_count': 0}), (52871, {'train/accuracy': 0.9894201755523682, 'train/loss': 0.03616183623671532, 'train/mean_average_precision': 0.25292693828969454, 'validation/accuracy': 0.985986053943634, 'validation/loss': 0.048090603202581406, 'validation/mean_average_precision': 0.1971760256055278, 'validation/num_examples': 43793, 'test/accuracy': 0.9850972294807434, 'test/loss': 0.05088086426258087, 'test/mean_average_precision': 0.18794931420806782, 'test/num_examples': 43793, 'score': 16819.26360821724, 'total_duration': 24318.454954862595, 'accumulated_submission_time': 16819.26360821724, 'accumulated_eval_time': 7495.518446922302, 'accumulated_logging_time': 2.2704098224639893, 'global_step': 52871, 'preemption_count': 0}), (53632, {'train/accuracy': 0.9894657731056213, 'train/loss': 0.03598015010356903, 'train/mean_average_precision': 0.2576308237827217, 'validation/accuracy': 0.9860092401504517, 'validation/loss': 0.04783957079052925, 'validation/mean_average_precision': 0.19560399905387693, 'validation/num_examples': 43793, 'test/accuracy': 0.9850951433181763, 'test/loss': 0.05029488727450371, 'test/mean_average_precision': 0.18946398776317053, 'test/num_examples': 43793, 'score': 17059.2628159523, 'total_duration': 24661.73969578743, 'accumulated_submission_time': 17059.2628159523, 'accumulated_eval_time': 7598.746030807495, 'accumulated_logging_time': 2.308499336242676, 'global_step': 53632, 'preemption_count': 0}), (54393, {'train/accuracy': 0.9890407919883728, 'train/loss': 0.03752331808209419, 'train/mean_average_precision': 0.22529144773405752, 'validation/accuracy': 0.9857092499732971, 'validation/loss': 0.04871625825762749, 'validation/mean_average_precision': 0.18467604560021897, 'validation/num_examples': 43793, 'test/accuracy': 0.9847282767295837, 'test/loss': 0.05140816047787666, 'test/mean_average_precision': 0.1770618964335692, 'test/num_examples': 43793, 'score': 17299.21511077881, 'total_duration': 25006.000480890274, 'accumulated_submission_time': 17299.21511077881, 'accumulated_eval_time': 7702.997226715088, 'accumulated_logging_time': 2.3462722301483154, 'global_step': 54393, 'preemption_count': 0}), (55154, {'train/accuracy': 0.9894230365753174, 'train/loss': 0.036137502640485764, 'train/mean_average_precision': 0.24171861130247058, 'validation/accuracy': 0.9860656261444092, 'validation/loss': 0.047809481620788574, 'validation/mean_average_precision': 0.1957881328081809, 'validation/num_examples': 43793, 'test/accuracy': 0.9851566553115845, 'test/loss': 0.05043136700987816, 'test/mean_average_precision': 0.18510774874874678, 'test/num_examples': 43793, 'score': 17539.427534341812, 'total_duration': 25348.384285211563, 'accumulated_submission_time': 17539.427534341812, 'accumulated_eval_time': 7805.111110925674, 'accumulated_logging_time': 2.3839142322540283, 'global_step': 55154, 'preemption_count': 0}), (55910, {'train/accuracy': 0.9894699454307556, 'train/loss': 0.036292899399995804, 'train/mean_average_precision': 0.25152001206536745, 'validation/accuracy': 0.9860270619392395, 'validation/loss': 0.0474446602165699, 'validation/mean_average_precision': 0.19686378699877044, 'validation/num_examples': 43793, 'test/accuracy': 0.985159158706665, 'test/loss': 0.04995325952768326, 'test/mean_average_precision': 0.18886979263849543, 'test/num_examples': 43793, 'score': 17779.39391064644, 'total_duration': 25691.751713991165, 'accumulated_submission_time': 17779.39391064644, 'accumulated_eval_time': 7908.455640792847, 'accumulated_logging_time': 2.4211137294769287, 'global_step': 55910, 'preemption_count': 0}), (56674, {'train/accuracy': 0.9894645810127258, 'train/loss': 0.035822756588459015, 'train/mean_average_precision': 0.25739268458964026, 'validation/accuracy': 0.9861395359039307, 'validation/loss': 0.04740425571799278, 'validation/mean_average_precision': 0.20158968950371123, 'validation/num_examples': 43793, 'test/accuracy': 0.9852097034454346, 'test/loss': 0.0502450168132782, 'test/mean_average_precision': 0.1947143161858327, 'test/num_examples': 43793, 'score': 18019.52805519104, 'total_duration': 26035.369215011597, 'accumulated_submission_time': 18019.52805519104, 'accumulated_eval_time': 8011.882115125656, 'accumulated_logging_time': 2.458162784576416, 'global_step': 56674, 'preemption_count': 0}), (57432, {'train/accuracy': 0.9895554780960083, 'train/loss': 0.03577268868684769, 'train/mean_average_precision': 0.2601344773012853, 'validation/accuracy': 0.9859702587127686, 'validation/loss': 0.04734781011939049, 'validation/mean_average_precision': 0.19831942626624036, 'validation/num_examples': 43793, 'test/accuracy': 0.985026478767395, 'test/loss': 0.050025396049022675, 'test/mean_average_precision': 0.190945310755514, 'test/num_examples': 43793, 'score': 18259.709180355072, 'total_duration': 26374.576673030853, 'accumulated_submission_time': 18259.709180355072, 'accumulated_eval_time': 8110.851234436035, 'accumulated_logging_time': 2.49564528465271, 'global_step': 57432, 'preemption_count': 0})], 'global_step': 58119}
I0206 10:12:03.257575 139978932307776 submission_runner.py:586] Timing: 18477.048362255096
I0206 10:12:03.257640 139978932307776 submission_runner.py:588] Total number of evals: 77
I0206 10:12:03.257691 139978932307776 submission_runner.py:589] ====================
I0206 10:12:03.257746 139978932307776 submission_runner.py:542] Using RNG seed 3917441912
I0206 10:12:03.323389 139978932307776 submission_runner.py:551] --- Tuning run 5/5 ---
I0206 10:12:03.323559 139978932307776 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5.
I0206 10:12:03.326691 139978932307776 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5/hparams.json.
I0206 10:12:03.459976 139978932307776 submission_runner.py:206] Initializing dataset.
I0206 10:12:03.548288 139978932307776 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0206 10:12:03.552633 139978932307776 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0206 10:12:03.683078 139978932307776 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0206 10:12:03.721366 139978932307776 submission_runner.py:213] Initializing model.
I0206 10:12:06.169123 139978932307776 submission_runner.py:255] Initializing optimizer.
I0206 10:12:06.765745 139978932307776 submission_runner.py:262] Initializing metrics bundle.
I0206 10:12:06.765945 139978932307776 submission_runner.py:280] Initializing checkpoint and logger.
I0206 10:12:06.766659 139978932307776 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5 with prefix checkpoint_
I0206 10:12:06.766798 139978932307776 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5/meta_data_0.json.
I0206 10:12:06.767008 139978932307776 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 10:12:06.767071 139978932307776 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 10:12:09.235069 139978932307776 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 10:12:11.692484 139978932307776 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5/flags_0.json.
I0206 10:12:11.696682 139978932307776 submission_runner.py:314] Starting training loop.
I0206 10:12:24.951346 139733319546624 logging_writer.py:48] [0] global_step=0, grad_norm=2.007039785385132, loss=0.800382673740387
I0206 10:12:24.963511 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:14:06.882479 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:14:09.913937 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:14:12.929713 139978932307776 submission_runner.py:408] Time since start: 121.23s, 	Step: 1, 	{'train/accuracy': 0.38867321610450745, 'train/loss': 0.7993758320808411, 'train/mean_average_precision': 0.024189225297995574, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02483808256432166, 'validation/num_examples': 43793, 'test/accuracy': 0.3947482705116272, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026379578053164793, 'test/num_examples': 43793, 'score': 13.266764402389526, 'total_duration': 121.23296904563904, 'accumulated_submission_time': 13.266764402389526, 'accumulated_eval_time': 107.96615147590637, 'accumulated_logging_time': 0}
I0206 10:14:12.938768 139752137176832 logging_writer.py:48] [1] accumulated_eval_time=107.966151, accumulated_logging_time=0, accumulated_submission_time=13.266764, global_step=1, preemption_count=0, score=13.266764, test/accuracy=0.394748, test/loss=0.795676, test/mean_average_precision=0.026380, test/num_examples=43793, total_duration=121.232969, train/accuracy=0.388673, train/loss=0.799376, train/mean_average_precision=0.024189, validation/accuracy=0.392645, validation/loss=0.797466, validation/mean_average_precision=0.024838, validation/num_examples=43793
I0206 10:14:44.549062 139770185602816 logging_writer.py:48] [100] global_step=100, grad_norm=0.32185128331184387, loss=0.28955021500587463
I0206 10:15:16.135451 139752137176832 logging_writer.py:48] [200] global_step=200, grad_norm=0.10585848987102509, loss=0.11475760489702225
I0206 10:15:47.697421 139770185602816 logging_writer.py:48] [300] global_step=300, grad_norm=0.035833168774843216, loss=0.06658442318439484
I0206 10:16:19.382268 139752137176832 logging_writer.py:48] [400] global_step=400, grad_norm=0.017815064638853073, loss=0.05393092706799507
I0206 10:16:50.759353 139770185602816 logging_writer.py:48] [500] global_step=500, grad_norm=0.016821546480059624, loss=0.055248573422431946
I0206 10:17:22.169484 139752137176832 logging_writer.py:48] [600] global_step=600, grad_norm=0.03469168394804001, loss=0.051456719636917114
I0206 10:17:54.056979 139770185602816 logging_writer.py:48] [700] global_step=700, grad_norm=0.040598463267087936, loss=0.06432847678661346
I0206 10:18:12.945867 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:19:46.239595 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:19:49.329912 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:19:52.378435 139978932307776 submission_runner.py:408] Time since start: 460.68s, 	Step: 760, 	{'train/accuracy': 0.9868772029876709, 'train/loss': 0.05130617320537567, 'train/mean_average_precision': 0.055429095232677444, 'validation/accuracy': 0.9841986894607544, 'validation/loss': 0.060611315071582794, 'validation/mean_average_precision': 0.053341938027725105, 'validation/num_examples': 43793, 'test/accuracy': 0.9831896424293518, 'test/loss': 0.06384647637605667, 'test/mean_average_precision': 0.05541749006743494, 'test/num_examples': 43793, 'score': 253.24348163604736, 'total_duration': 460.68167448043823, 'accumulated_submission_time': 253.24348163604736, 'accumulated_eval_time': 207.39866089820862, 'accumulated_logging_time': 0.01946091651916504}
I0206 10:19:52.393770 139733338965760 logging_writer.py:48] [760] accumulated_eval_time=207.398661, accumulated_logging_time=0.019461, accumulated_submission_time=253.243482, global_step=760, preemption_count=0, score=253.243482, test/accuracy=0.983190, test/loss=0.063846, test/mean_average_precision=0.055417, test/num_examples=43793, total_duration=460.681674, train/accuracy=0.986877, train/loss=0.051306, train/mean_average_precision=0.055429, validation/accuracy=0.984199, validation/loss=0.060611, validation/mean_average_precision=0.053342, validation/num_examples=43793
I0206 10:20:05.601132 139752176375552 logging_writer.py:48] [800] global_step=800, grad_norm=0.02572576142847538, loss=0.04538188502192497
I0206 10:20:37.613947 139733338965760 logging_writer.py:48] [900] global_step=900, grad_norm=0.0253292266279459, loss=0.05310799553990364
I0206 10:21:09.481920 139752176375552 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01847672462463379, loss=0.05153666436672211
I0206 10:21:41.292922 139733338965760 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.019880589097738266, loss=0.04870687797665596
I0206 10:22:13.139628 139752176375552 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02468900755047798, loss=0.04300808906555176
I0206 10:22:44.808766 139733338965760 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.019410932436585426, loss=0.04177024960517883
I0206 10:23:16.801266 139752176375552 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02840091846883297, loss=0.042137809097766876
I0206 10:23:48.652606 139733338965760 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.021620716899633408, loss=0.05336935818195343
I0206 10:23:52.513984 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:25:32.351528 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:25:35.351523 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:25:38.342020 139978932307776 submission_runner.py:408] Time since start: 806.65s, 	Step: 1513, 	{'train/accuracy': 0.9872434139251709, 'train/loss': 0.046811483800411224, 'train/mean_average_precision': 0.10833265647219083, 'validation/accuracy': 0.9846939444541931, 'validation/loss': 0.055397987365722656, 'validation/mean_average_precision': 0.10490476316852082, 'validation/num_examples': 43793, 'test/accuracy': 0.9836639165878296, 'test/loss': 0.05843276530504227, 'test/mean_average_precision': 0.1038389597988644, 'test/num_examples': 43793, 'score': 493.3325746059418, 'total_duration': 806.6452507972717, 'accumulated_submission_time': 493.3325746059418, 'accumulated_eval_time': 313.2266294956207, 'accumulated_logging_time': 0.04572749137878418}
I0206 10:25:38.360329 139733347358464 logging_writer.py:48] [1513] accumulated_eval_time=313.226629, accumulated_logging_time=0.045727, accumulated_submission_time=493.332575, global_step=1513, preemption_count=0, score=493.332575, test/accuracy=0.983664, test/loss=0.058433, test/mean_average_precision=0.103839, test/num_examples=43793, total_duration=806.645251, train/accuracy=0.987243, train/loss=0.046811, train/mean_average_precision=0.108333, validation/accuracy=0.984694, validation/loss=0.055398, validation/mean_average_precision=0.104905, validation/num_examples=43793
I0206 10:26:06.661723 139752137176832 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02044861577451229, loss=0.05108550190925598
I0206 10:26:38.374665 139733347358464 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.02716049738228321, loss=0.04733659699559212
I0206 10:27:10.348310 139752137176832 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.036273516714572906, loss=0.04687981680035591
I0206 10:27:42.038503 139733347358464 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.010626145638525486, loss=0.04337671399116516
I0206 10:28:13.870962 139752137176832 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.016731223091483116, loss=0.04833336919546127
I0206 10:28:46.071394 139733347358464 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01441078819334507, loss=0.0421883761882782
I0206 10:29:17.804962 139752137176832 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.023790357634425163, loss=0.04418863356113434
I0206 10:29:38.586672 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:31:16.045773 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:31:19.086017 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:31:22.003262 139978932307776 submission_runner.py:408] Time since start: 1150.31s, 	Step: 2267, 	{'train/accuracy': 0.9878523945808411, 'train/loss': 0.0432136245071888, 'train/mean_average_precision': 0.1485056292047553, 'validation/accuracy': 0.9850824475288391, 'validation/loss': 0.05285615846514702, 'validation/mean_average_precision': 0.14011133332460388, 'validation/num_examples': 43793, 'test/accuracy': 0.9840859770774841, 'test/loss': 0.055762797594070435, 'test/mean_average_precision': 0.13522063297129633, 'test/num_examples': 43793, 'score': 733.5270512104034, 'total_duration': 1150.3065140247345, 'accumulated_submission_time': 733.5270512104034, 'accumulated_eval_time': 416.6431694030762, 'accumulated_logging_time': 0.07539010047912598}
I0206 10:31:22.018995 139733338965760 logging_writer.py:48] [2267] accumulated_eval_time=416.643169, accumulated_logging_time=0.075390, accumulated_submission_time=733.527051, global_step=2267, preemption_count=0, score=733.527051, test/accuracy=0.984086, test/loss=0.055763, test/mean_average_precision=0.135221, test/num_examples=43793, total_duration=1150.306514, train/accuracy=0.987852, train/loss=0.043214, train/mean_average_precision=0.148506, validation/accuracy=0.985082, validation/loss=0.052856, validation/mean_average_precision=0.140111, validation/num_examples=43793
I0206 10:31:32.711586 139752176375552 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.011218041181564331, loss=0.04586895555257797
I0206 10:32:03.831809 139733338965760 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.014700553379952908, loss=0.04563570395112038
I0206 10:32:35.683364 139752176375552 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.023203423246741295, loss=0.04259796813130379
I0206 10:33:07.210897 139733338965760 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.012210422195494175, loss=0.045819614082574844
I0206 10:33:38.509622 139752176375552 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.0187226515263319, loss=0.046959009021520615
I0206 10:34:10.025647 139733338965760 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.014566554687917233, loss=0.041574522852897644
I0206 10:34:41.989466 139752176375552 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.02020709402859211, loss=0.04420704394578934
I0206 10:35:13.836827 139733338965760 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.014617329463362694, loss=0.045325085520744324
I0206 10:35:22.252484 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:37:01.174490 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:37:04.244735 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:37:07.191550 139978932307776 submission_runner.py:408] Time since start: 1495.49s, 	Step: 3027, 	{'train/accuracy': 0.9883006811141968, 'train/loss': 0.04095477983355522, 'train/mean_average_precision': 0.17294835312469342, 'validation/accuracy': 0.9853901267051697, 'validation/loss': 0.05044339969754219, 'validation/mean_average_precision': 0.16348467636629252, 'validation/num_examples': 43793, 'test/accuracy': 0.9844338893890381, 'test/loss': 0.053324077278375626, 'test/mean_average_precision': 0.15936131435489542, 'test/num_examples': 43793, 'score': 973.7290751934052, 'total_duration': 1495.4947953224182, 'accumulated_submission_time': 973.7290751934052, 'accumulated_eval_time': 521.5821809768677, 'accumulated_logging_time': 0.10217666625976562}
I0206 10:37:07.207228 139733347358464 logging_writer.py:48] [3027] accumulated_eval_time=521.582181, accumulated_logging_time=0.102177, accumulated_submission_time=973.729075, global_step=3027, preemption_count=0, score=973.729075, test/accuracy=0.984434, test/loss=0.053324, test/mean_average_precision=0.159361, test/num_examples=43793, total_duration=1495.494795, train/accuracy=0.988301, train/loss=0.040955, train/mean_average_precision=0.172948, validation/accuracy=0.985390, validation/loss=0.050443, validation/mean_average_precision=0.163485, validation/num_examples=43793
I0206 10:37:31.159360 139752137176832 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.00895377341657877, loss=0.03969375789165497
I0206 10:38:03.074773 139733347358464 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.010514786466956139, loss=0.043016765266656876
I0206 10:38:34.411958 139752137176832 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.019950026646256447, loss=0.0364953838288784
I0206 10:39:06.138415 139733347358464 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.013300648890435696, loss=0.04016876220703125
I0206 10:39:37.863396 139752137176832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.00973779521882534, loss=0.0401308536529541
I0206 10:40:09.629738 139733347358464 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013877414166927338, loss=0.03996703401207924
I0206 10:40:41.334954 139752137176832 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.013605917803943157, loss=0.037647392600774765
I0206 10:41:07.226161 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:42:45.734974 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:42:48.751992 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:42:51.695674 139978932307776 submission_runner.py:408] Time since start: 1840.00s, 	Step: 3783, 	{'train/accuracy': 0.9884099364280701, 'train/loss': 0.040116507560014725, 'train/mean_average_precision': 0.19470530027179817, 'validation/accuracy': 0.9855772852897644, 'validation/loss': 0.049140069633722305, 'validation/mean_average_precision': 0.17634006684516024, 'validation/num_examples': 43793, 'test/accuracy': 0.984683632850647, 'test/loss': 0.05181457847356796, 'test/mean_average_precision': 0.17571994317594894, 'test/num_examples': 43793, 'score': 1213.7164585590363, 'total_duration': 1839.9989259243011, 'accumulated_submission_time': 1213.7164585590363, 'accumulated_eval_time': 626.0516443252563, 'accumulated_logging_time': 0.12928414344787598}
I0206 10:42:51.711373 139752176375552 logging_writer.py:48] [3783] accumulated_eval_time=626.051644, accumulated_logging_time=0.129284, accumulated_submission_time=1213.716459, global_step=3783, preemption_count=0, score=1213.716459, test/accuracy=0.984684, test/loss=0.051815, test/mean_average_precision=0.175720, test/num_examples=43793, total_duration=1839.998926, train/accuracy=0.988410, train/loss=0.040117, train/mean_average_precision=0.194705, validation/accuracy=0.985577, validation/loss=0.049140, validation/mean_average_precision=0.176340, validation/num_examples=43793
I0206 10:42:57.617654 139770185602816 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.01089564524590969, loss=0.04325246810913086
I0206 10:43:29.384964 139752176375552 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.012252886779606342, loss=0.038071200251579285
I0206 10:44:01.067254 139770185602816 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01278839074075222, loss=0.03822595626115799
I0206 10:44:32.909817 139752176375552 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.017598159611225128, loss=0.045537207275629044
I0206 10:45:04.767032 139770185602816 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.013772092759609222, loss=0.04352885112166405
I0206 10:45:36.446130 139752176375552 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.01301948819309473, loss=0.03589455783367157
I0206 10:46:08.409760 139770185602816 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.011109367944300175, loss=0.03372999280691147
I0206 10:46:39.908006 139752176375552 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011329234577715397, loss=0.03985680639743805
I0206 10:46:51.743071 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:48:27.445249 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:48:30.579573 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:48:33.621951 139978932307776 submission_runner.py:408] Time since start: 2181.93s, 	Step: 4538, 	{'train/accuracy': 0.9886505007743835, 'train/loss': 0.03874871879816055, 'train/mean_average_precision': 0.21259982180952808, 'validation/accuracy': 0.9857733845710754, 'validation/loss': 0.04842757806181908, 'validation/mean_average_precision': 0.19539026533259743, 'validation/num_examples': 43793, 'test/accuracy': 0.9848495721817017, 'test/loss': 0.051139019429683685, 'test/mean_average_precision': 0.1969125162851597, 'test/num_examples': 43793, 'score': 1453.7154524326324, 'total_duration': 2181.925208091736, 'accumulated_submission_time': 1453.7154524326324, 'accumulated_eval_time': 727.9304811954498, 'accumulated_logging_time': 0.15792489051818848}
I0206 10:48:33.637638 139733347358464 logging_writer.py:48] [4538] accumulated_eval_time=727.930481, accumulated_logging_time=0.157925, accumulated_submission_time=1453.715452, global_step=4538, preemption_count=0, score=1453.715452, test/accuracy=0.984850, test/loss=0.051139, test/mean_average_precision=0.196913, test/num_examples=43793, total_duration=2181.925208, train/accuracy=0.988651, train/loss=0.038749, train/mean_average_precision=0.212600, validation/accuracy=0.985773, validation/loss=0.048428, validation/mean_average_precision=0.195390, validation/num_examples=43793
I0206 10:48:53.743172 139752137176832 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.017149318009614944, loss=0.040593892335891724
I0206 10:49:25.361341 139733347358464 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.01235156785696745, loss=0.038293804973363876
I0206 10:49:57.218071 139752137176832 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.01036070566624403, loss=0.037946004420518875
I0206 10:50:29.350390 139733347358464 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.010559828020632267, loss=0.041764356195926666
I0206 10:51:01.212559 139752137176832 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.016063716262578964, loss=0.04136859253048897
I0206 10:51:32.976758 139733347358464 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.01252389419823885, loss=0.03618714213371277
I0206 10:52:05.092839 139752137176832 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.015755074098706245, loss=0.036455586552619934
I0206 10:52:33.914796 139978932307776 spec.py:321] Evaluating on the training split.
I0206 10:54:16.707776 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 10:54:19.867861 139978932307776 spec.py:349] Evaluating on the test split.
I0206 10:54:22.925907 139978932307776 submission_runner.py:408] Time since start: 2531.23s, 	Step: 5291, 	{'train/accuracy': 0.9889928698539734, 'train/loss': 0.03743991628289223, 'train/mean_average_precision': 0.24753548398149616, 'validation/accuracy': 0.9859016537666321, 'validation/loss': 0.04728766903281212, 'validation/mean_average_precision': 0.2130693687389712, 'validation/num_examples': 43793, 'test/accuracy': 0.9849818348884583, 'test/loss': 0.04990756884217262, 'test/mean_average_precision': 0.20962896712493773, 'test/num_examples': 43793, 'score': 1693.9603996276855, 'total_duration': 2531.2291519641876, 'accumulated_submission_time': 1693.9603996276855, 'accumulated_eval_time': 836.9415402412415, 'accumulated_logging_time': 0.1859288215637207}
I0206 10:54:22.942164 139733338965760 logging_writer.py:48] [5291] accumulated_eval_time=836.941540, accumulated_logging_time=0.185929, accumulated_submission_time=1693.960400, global_step=5291, preemption_count=0, score=1693.960400, test/accuracy=0.984982, test/loss=0.049908, test/mean_average_precision=0.209629, test/num_examples=43793, total_duration=2531.229152, train/accuracy=0.988993, train/loss=0.037440, train/mean_average_precision=0.247535, validation/accuracy=0.985902, validation/loss=0.047288, validation/mean_average_precision=0.213069, validation/num_examples=43793
I0206 10:54:26.077701 139770185602816 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.014165709726512432, loss=0.04131244495511055
I0206 10:54:57.541448 139733338965760 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01028835866600275, loss=0.03374867141246796
I0206 10:55:29.500577 139770185602816 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014312146231532097, loss=0.0423860140144825
I0206 10:56:01.064054 139733338965760 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.013637286610901356, loss=0.037191540002822876
I0206 10:56:32.765401 139770185602816 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.012403605505824089, loss=0.03765102103352547
I0206 10:57:04.702301 139733338965760 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.012111167423427105, loss=0.03567339479923248
I0206 10:57:36.350714 139770185602816 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.018359970301389694, loss=0.03477287292480469
I0206 10:58:07.915363 139733338965760 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.01205205637961626, loss=0.03930158540606499
I0206 10:58:23.010455 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:00:04.946464 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:00:08.036523 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:00:11.079777 139978932307776 submission_runner.py:408] Time since start: 2879.38s, 	Step: 6049, 	{'train/accuracy': 0.9890961050987244, 'train/loss': 0.03699062764644623, 'train/mean_average_precision': 0.25872194904249346, 'validation/accuracy': 0.9860299229621887, 'validation/loss': 0.047073882073163986, 'validation/mean_average_precision': 0.21408591631613105, 'validation/num_examples': 43793, 'test/accuracy': 0.9852286577224731, 'test/loss': 0.04985300824046135, 'test/mean_average_precision': 0.20940728521010876, 'test/num_examples': 43793, 'score': 1933.9973559379578, 'total_duration': 2879.3830313682556, 'accumulated_submission_time': 1933.9973559379578, 'accumulated_eval_time': 945.0108182430267, 'accumulated_logging_time': 0.21348261833190918}
I0206 11:00:11.096154 139752176375552 logging_writer.py:48] [6049] accumulated_eval_time=945.010818, accumulated_logging_time=0.213483, accumulated_submission_time=1933.997356, global_step=6049, preemption_count=0, score=1933.997356, test/accuracy=0.985229, test/loss=0.049853, test/mean_average_precision=0.209407, test/num_examples=43793, total_duration=2879.383031, train/accuracy=0.989096, train/loss=0.036991, train/mean_average_precision=0.258722, validation/accuracy=0.986030, validation/loss=0.047074, validation/mean_average_precision=0.214086, validation/num_examples=43793
I0206 11:00:27.580549 139916327515904 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01175440102815628, loss=0.03866549953818321
I0206 11:00:59.178315 139752176375552 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.013710401020944118, loss=0.034512102603912354
I0206 11:01:30.853015 139916327515904 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.013644935563206673, loss=0.03536830469965935
I0206 11:02:02.509752 139752176375552 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.014333497732877731, loss=0.042238228023052216
I0206 11:02:34.672121 139916327515904 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.012327499687671661, loss=0.039197295904159546
I0206 11:03:05.935200 139752176375552 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012442290782928467, loss=0.034830741584300995
I0206 11:03:37.467918 139916327515904 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.012027254328131676, loss=0.036723826080560684
I0206 11:04:08.974521 139752176375552 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.014247278682887554, loss=0.035258032381534576
I0206 11:04:11.148710 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:05:48.523309 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:05:51.684633 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:05:54.701763 139978932307776 submission_runner.py:408] Time since start: 3223.01s, 	Step: 6808, 	{'train/accuracy': 0.9892982840538025, 'train/loss': 0.03599486127495766, 'train/mean_average_precision': 0.2853854649351488, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.046015024185180664, 'validation/mean_average_precision': 0.22560966442688724, 'validation/num_examples': 43793, 'test/accuracy': 0.9852871894836426, 'test/loss': 0.048492759466171265, 'test/mean_average_precision': 0.21902900229270952, 'test/num_examples': 43793, 'score': 2174.0190699100494, 'total_duration': 3223.005018234253, 'accumulated_submission_time': 2174.0190699100494, 'accumulated_eval_time': 1048.5638234615326, 'accumulated_logging_time': 0.24115276336669922}
I0206 11:05:54.717224 139752137176832 logging_writer.py:48] [6808] accumulated_eval_time=1048.563823, accumulated_logging_time=0.241153, accumulated_submission_time=2174.019070, global_step=6808, preemption_count=0, score=2174.019070, test/accuracy=0.985287, test/loss=0.048493, test/mean_average_precision=0.219029, test/num_examples=43793, total_duration=3223.005018, train/accuracy=0.989298, train/loss=0.035995, train/mean_average_precision=0.285385, validation/accuracy=0.986221, validation/loss=0.046015, validation/mean_average_precision=0.225610, validation/num_examples=43793
I0206 11:06:24.569785 139770185602816 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.0129895880818367, loss=0.03874789923429489
I0206 11:06:56.026534 139752137176832 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.014511299319565296, loss=0.0370015911757946
I0206 11:07:27.642802 139770185602816 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.013706936500966549, loss=0.0336693599820137
I0206 11:07:59.206336 139752137176832 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.013737444765865803, loss=0.034191716462373734
I0206 11:08:30.801995 139770185602816 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.022691061720252037, loss=0.03954119235277176
I0206 11:09:02.665147 139752137176832 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.013509766198694706, loss=0.03582228720188141
I0206 11:09:33.888497 139770185602816 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.016777269542217255, loss=0.03697195276618004
I0206 11:09:54.989740 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:11:31.566861 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:11:34.741966 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:11:37.753680 139978932307776 submission_runner.py:408] Time since start: 3566.06s, 	Step: 7566, 	{'train/accuracy': 0.9897069931030273, 'train/loss': 0.034704916179180145, 'train/mean_average_precision': 0.30216275499469225, 'validation/accuracy': 0.9863408803939819, 'validation/loss': 0.04567665234208107, 'validation/mean_average_precision': 0.23187020433388755, 'validation/num_examples': 43793, 'test/accuracy': 0.9855091571807861, 'test/loss': 0.048133257776498795, 'test/mean_average_precision': 0.23834373456583896, 'test/num_examples': 43793, 'score': 2414.2607395648956, 'total_duration': 3566.0569252967834, 'accumulated_submission_time': 2414.2607395648956, 'accumulated_eval_time': 1151.3277099132538, 'accumulated_logging_time': 0.2674412727355957}
I0206 11:11:37.770377 139752176375552 logging_writer.py:48] [7566] accumulated_eval_time=1151.327710, accumulated_logging_time=0.267441, accumulated_submission_time=2414.260740, global_step=7566, preemption_count=0, score=2414.260740, test/accuracy=0.985509, test/loss=0.048133, test/mean_average_precision=0.238344, test/num_examples=43793, total_duration=3566.056925, train/accuracy=0.989707, train/loss=0.034705, train/mean_average_precision=0.302163, validation/accuracy=0.986341, validation/loss=0.045677, validation/mean_average_precision=0.231870, validation/num_examples=43793
I0206 11:11:49.039520 139916327515904 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.013820797204971313, loss=0.035680875182151794
I0206 11:12:20.730269 139752176375552 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01382444892078638, loss=0.03589893504977226
I0206 11:12:52.643886 139916327515904 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.01713910698890686, loss=0.03283046931028366
I0206 11:13:24.441861 139752176375552 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01731252111494541, loss=0.03444650024175644
I0206 11:13:55.913456 139916327515904 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.014157700352370739, loss=0.03514532372355461
I0206 11:14:27.707729 139752176375552 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.018419159576296806, loss=0.040396444499492645
I0206 11:14:59.275038 139916327515904 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01378367654979229, loss=0.03213922679424286
I0206 11:15:30.836673 139752176375552 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.01272617094218731, loss=0.03304300084710121
I0206 11:15:38.064709 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:17:18.186748 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:17:21.218813 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:17:24.198537 139978932307776 submission_runner.py:408] Time since start: 3912.50s, 	Step: 8324, 	{'train/accuracy': 0.9899153709411621, 'train/loss': 0.033655136823654175, 'train/mean_average_precision': 0.32028739999883327, 'validation/accuracy': 0.9864200353622437, 'validation/loss': 0.04565197229385376, 'validation/mean_average_precision': 0.23815165667823943, 'validation/num_examples': 43793, 'test/accuracy': 0.9855584502220154, 'test/loss': 0.04834138602018356, 'test/mean_average_precision': 0.2369385751055225, 'test/num_examples': 43793, 'score': 2654.523429632187, 'total_duration': 3912.501790523529, 'accumulated_submission_time': 2654.523429632187, 'accumulated_eval_time': 1257.461489200592, 'accumulated_logging_time': 0.2958061695098877}
I0206 11:17:24.215304 139752137176832 logging_writer.py:48] [8324] accumulated_eval_time=1257.461489, accumulated_logging_time=0.295806, accumulated_submission_time=2654.523430, global_step=8324, preemption_count=0, score=2654.523430, test/accuracy=0.985558, test/loss=0.048341, test/mean_average_precision=0.236939, test/num_examples=43793, total_duration=3912.501791, train/accuracy=0.989915, train/loss=0.033655, train/mean_average_precision=0.320287, validation/accuracy=0.986420, validation/loss=0.045652, validation/mean_average_precision=0.238152, validation/num_examples=43793
I0206 11:17:48.429807 139770185602816 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.020110689103603363, loss=0.034904979169368744
I0206 11:18:20.016551 139752137176832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.016771458089351654, loss=0.03562020882964134
I0206 11:18:51.608033 139770185602816 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.024257536977529526, loss=0.036022916436195374
I0206 11:19:23.192350 139752137176832 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.016539854928851128, loss=0.035484183579683304
I0206 11:19:54.754096 139770185602816 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01531223114579916, loss=0.036134012043476105
I0206 11:20:26.607966 139752137176832 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.023861899971961975, loss=0.03671903535723686
I0206 11:20:58.225527 139770185602816 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.029242249205708504, loss=0.038756392896175385
I0206 11:21:24.406672 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:22:59.761210 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:23:02.903319 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:23:05.870720 139978932307776 submission_runner.py:408] Time since start: 4254.17s, 	Step: 9083, 	{'train/accuracy': 0.9900816082954407, 'train/loss': 0.03320526331663132, 'train/mean_average_precision': 0.3405124811740702, 'validation/accuracy': 0.9864683151245117, 'validation/loss': 0.045106906443834305, 'validation/mean_average_precision': 0.23899023678908335, 'validation/num_examples': 43793, 'test/accuracy': 0.985533595085144, 'test/loss': 0.04767252877354622, 'test/mean_average_precision': 0.24344503418705318, 'test/num_examples': 43793, 'score': 2894.683405160904, 'total_duration': 4254.173964262009, 'accumulated_submission_time': 2894.683405160904, 'accumulated_eval_time': 1358.925484418869, 'accumulated_logging_time': 0.32367920875549316}
I0206 11:23:05.891716 139804644198144 logging_writer.py:48] [9083] accumulated_eval_time=1358.925484, accumulated_logging_time=0.323679, accumulated_submission_time=2894.683405, global_step=9083, preemption_count=0, score=2894.683405, test/accuracy=0.985534, test/loss=0.047673, test/mean_average_precision=0.243445, test/num_examples=43793, total_duration=4254.173964, train/accuracy=0.990082, train/loss=0.033205, train/mean_average_precision=0.340512, validation/accuracy=0.986468, validation/loss=0.045107, validation/mean_average_precision=0.238990, validation/num_examples=43793
I0206 11:23:11.610435 139916327515904 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.016849752515554428, loss=0.03756000101566315
I0206 11:23:43.391496 139804644198144 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.018382582813501358, loss=0.033330515027046204
I0206 11:24:15.333647 139916327515904 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.018933283165097237, loss=0.03762368857860565
I0206 11:24:46.933678 139804644198144 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.020353183150291443, loss=0.03670521453022957
I0206 11:25:18.796191 139916327515904 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02135218121111393, loss=0.03210749849677086
I0206 11:25:50.633103 139804644198144 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.021765682846307755, loss=0.03686624392867088
I0206 11:26:22.229055 139916327515904 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.025593852624297142, loss=0.03739149123430252
I0206 11:26:54.018090 139804644198144 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.020615212619304657, loss=0.03261462599039078
I0206 11:27:05.991602 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:28:42.570024 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:28:45.656847 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:28:48.649807 139978932307776 submission_runner.py:408] Time since start: 4596.95s, 	Step: 9838, 	{'train/accuracy': 0.9902423024177551, 'train/loss': 0.03240682929754257, 'train/mean_average_precision': 0.3589596793839536, 'validation/accuracy': 0.9864655137062073, 'validation/loss': 0.044942017644643784, 'validation/mean_average_precision': 0.24561560730822854, 'validation/num_examples': 43793, 'test/accuracy': 0.9856464862823486, 'test/loss': 0.0476103201508522, 'test/mean_average_precision': 0.2401196061095888, 'test/num_examples': 43793, 'score': 3134.751321554184, 'total_duration': 4596.953059434891, 'accumulated_submission_time': 3134.751321554184, 'accumulated_eval_time': 1461.5836379528046, 'accumulated_logging_time': 0.35663437843322754}
I0206 11:28:48.667887 139752137176832 logging_writer.py:48] [9838] accumulated_eval_time=1461.583638, accumulated_logging_time=0.356634, accumulated_submission_time=3134.751322, global_step=9838, preemption_count=0, score=3134.751322, test/accuracy=0.985646, test/loss=0.047610, test/mean_average_precision=0.240120, test/num_examples=43793, total_duration=4596.953059, train/accuracy=0.990242, train/loss=0.032407, train/mean_average_precision=0.358960, validation/accuracy=0.986466, validation/loss=0.044942, validation/mean_average_precision=0.245616, validation/num_examples=43793
I0206 11:29:08.797536 139770185602816 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02519489824771881, loss=0.03429991379380226
I0206 11:29:40.745638 139752137176832 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01838388666510582, loss=0.03428979963064194
I0206 11:30:12.801340 139770185602816 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02070271223783493, loss=0.033255960792303085
I0206 11:30:44.698701 139752137176832 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02358771674335003, loss=0.03631531074643135
I0206 11:31:16.450601 139770185602816 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.018870821222662926, loss=0.032221294939517975
I0206 11:31:48.202430 139752137176832 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.02075590193271637, loss=0.03641096130013466
I0206 11:32:20.275849 139770185602816 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.022899655625224113, loss=0.03280660882592201
I0206 11:32:48.885232 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:34:25.886405 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:34:28.915872 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:34:31.900123 139978932307776 submission_runner.py:408] Time since start: 4940.20s, 	Step: 10591, 	{'train/accuracy': 0.9902772903442383, 'train/loss': 0.03225903585553169, 'train/mean_average_precision': 0.36103109120477755, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.04454413428902626, 'validation/mean_average_precision': 0.25543941493454586, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.047201115638017654, 'test/mean_average_precision': 0.24936071927245262, 'test/num_examples': 43793, 'score': 3374.937530517578, 'total_duration': 4940.203378915787, 'accumulated_submission_time': 3374.937530517578, 'accumulated_eval_time': 1564.5984835624695, 'accumulated_logging_time': 0.3857431411743164}
I0206 11:34:31.917190 139752176375552 logging_writer.py:48] [10591] accumulated_eval_time=1564.598484, accumulated_logging_time=0.385743, accumulated_submission_time=3374.937531, global_step=10591, preemption_count=0, score=3374.937531, test/accuracy=0.985820, test/loss=0.047201, test/mean_average_precision=0.249361, test/num_examples=43793, total_duration=4940.203379, train/accuracy=0.990277, train/loss=0.032259, train/mean_average_precision=0.361031, validation/accuracy=0.986683, validation/loss=0.044544, validation/mean_average_precision=0.255439, validation/num_examples=43793
I0206 11:34:35.160582 139916327515904 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.022045113146305084, loss=0.03240913525223732
I0206 11:35:07.078153 139752176375552 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03879266604781151, loss=0.031769219785928726
I0206 11:35:38.492953 139916327515904 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.02096329815685749, loss=0.03357166051864624
I0206 11:36:10.460080 139752176375552 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.022332321852445602, loss=0.034090202301740646
I0206 11:36:42.263724 139916327515904 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.026463666930794716, loss=0.033328693360090256
I0206 11:37:13.963466 139752176375552 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.027727840468287468, loss=0.0348484106361866
I0206 11:37:45.847145 139916327515904 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.01904020458459854, loss=0.03092866949737072
I0206 11:38:17.573605 139752176375552 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.02313011698424816, loss=0.034685563296079636
I0206 11:38:32.088596 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:40:07.750309 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:40:10.822756 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:40:13.776270 139978932307776 submission_runner.py:408] Time since start: 5282.08s, 	Step: 11347, 	{'train/accuracy': 0.9904393553733826, 'train/loss': 0.03198949620127678, 'train/mean_average_precision': 0.35538638490244756, 'validation/accuracy': 0.9865807890892029, 'validation/loss': 0.04452678933739662, 'validation/mean_average_precision': 0.24939574652448415, 'validation/num_examples': 43793, 'test/accuracy': 0.9857147336006165, 'test/loss': 0.04725104570388794, 'test/mean_average_precision': 0.25355996776683765, 'test/num_examples': 43793, 'score': 3615.0766611099243, 'total_duration': 5282.0795221328735, 'accumulated_submission_time': 3615.0766611099243, 'accumulated_eval_time': 1666.2861070632935, 'accumulated_logging_time': 0.4149281978607178}
I0206 11:40:13.793694 139770185602816 logging_writer.py:48] [11347] accumulated_eval_time=1666.286107, accumulated_logging_time=0.414928, accumulated_submission_time=3615.076661, global_step=11347, preemption_count=0, score=3615.076661, test/accuracy=0.985715, test/loss=0.047251, test/mean_average_precision=0.253560, test/num_examples=43793, total_duration=5282.079522, train/accuracy=0.990439, train/loss=0.031989, train/mean_average_precision=0.355386, validation/accuracy=0.986581, validation/loss=0.044527, validation/mean_average_precision=0.249396, validation/num_examples=43793
I0206 11:40:30.945267 139804644198144 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.023497197777032852, loss=0.033986058086156845
I0206 11:41:02.841725 139770185602816 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.024077758193016052, loss=0.034891728311777115
I0206 11:41:34.704584 139804644198144 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.020526932552456856, loss=0.03400230407714844
I0206 11:42:06.493467 139770185602816 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0306955985724926, loss=0.035670455545186996
I0206 11:42:38.705701 139804644198144 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.027651887387037277, loss=0.030951913446187973
I0206 11:43:10.766031 139770185602816 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.029106954112648964, loss=0.03506943956017494
I0206 11:43:42.722646 139804644198144 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02698376402258873, loss=0.034860752522945404
I0206 11:44:14.066366 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:45:54.538429 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:45:57.609821 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:46:00.581302 139978932307776 submission_runner.py:408] Time since start: 5628.88s, 	Step: 12099, 	{'train/accuracy': 0.9903896450996399, 'train/loss': 0.03196679428219795, 'train/mean_average_precision': 0.36273471093644405, 'validation/accuracy': 0.9866493940353394, 'validation/loss': 0.044371090829372406, 'validation/mean_average_precision': 0.2522052206234418, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.046992335468530655, 'test/mean_average_precision': 0.2538715301046184, 'test/num_examples': 43793, 'score': 3855.316538333893, 'total_duration': 5628.884557723999, 'accumulated_submission_time': 3855.316538333893, 'accumulated_eval_time': 1772.8010022640228, 'accumulated_logging_time': 0.44490718841552734}
I0206 11:46:00.598647 139752176375552 logging_writer.py:48] [12099] accumulated_eval_time=1772.801002, accumulated_logging_time=0.444907, accumulated_submission_time=3855.316538, global_step=12099, preemption_count=0, score=3855.316538, test/accuracy=0.985866, test/loss=0.046992, test/mean_average_precision=0.253872, test/num_examples=43793, total_duration=5628.884558, train/accuracy=0.990390, train/loss=0.031967, train/mean_average_precision=0.362735, validation/accuracy=0.986649, validation/loss=0.044371, validation/mean_average_precision=0.252205, validation/num_examples=43793
I0206 11:46:01.247266 139916327515904 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.038388941437006, loss=0.032454442232847214
I0206 11:46:32.788119 139752176375552 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.02603231370449066, loss=0.0340365506708622
I0206 11:47:04.255336 139916327515904 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.026555700227618217, loss=0.0334908664226532
I0206 11:47:35.536106 139752176375552 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.023563774302601814, loss=0.030890773981809616
I0206 11:48:07.900202 139916327515904 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.027637416496872902, loss=0.032413236796855927
I0206 11:48:39.423687 139752176375552 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.0269829872995615, loss=0.031572092324495316
I0206 11:49:10.978316 139916327515904 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03536850959062576, loss=0.03675370290875435
I0206 11:49:42.533285 139752176375552 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.022718502208590508, loss=0.03148386627435684
I0206 11:50:00.684892 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:51:37.866221 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:51:40.905957 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:51:43.851951 139978932307776 submission_runner.py:408] Time since start: 5972.16s, 	Step: 12859, 	{'train/accuracy': 0.9905506372451782, 'train/loss': 0.0315115861594677, 'train/mean_average_precision': 0.3814257408336702, 'validation/accuracy': 0.9867147207260132, 'validation/loss': 0.04444871470332146, 'validation/mean_average_precision': 0.26060915477594093, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04690170660614967, 'test/mean_average_precision': 0.2577900264790435, 'test/num_examples': 43793, 'score': 4095.371396303177, 'total_duration': 5972.1552057266235, 'accumulated_submission_time': 4095.371396303177, 'accumulated_eval_time': 1875.968015909195, 'accumulated_logging_time': 0.4730713367462158}
I0206 11:51:43.869933 139770185602816 logging_writer.py:48] [12859] accumulated_eval_time=1875.968016, accumulated_logging_time=0.473071, accumulated_submission_time=4095.371396, global_step=12859, preemption_count=0, score=4095.371396, test/accuracy=0.985922, test/loss=0.046902, test/mean_average_precision=0.257790, test/num_examples=43793, total_duration=5972.155206, train/accuracy=0.990551, train/loss=0.031512, train/mean_average_precision=0.381426, validation/accuracy=0.986715, validation/loss=0.044449, validation/mean_average_precision=0.260609, validation/num_examples=43793
I0206 11:51:57.253286 139804644198144 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.024320097640156746, loss=0.03095778077840805
I0206 11:52:29.351828 139770185602816 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.032802339643239975, loss=0.03274765983223915
I0206 11:53:01.510953 139804644198144 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.030311673879623413, loss=0.029705140739679337
I0206 11:53:33.400293 139770185602816 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03644711896777153, loss=0.030555322766304016
I0206 11:54:04.993960 139804644198144 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03519225865602493, loss=0.034624792635440826
I0206 11:54:36.781411 139770185602816 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03432723507285118, loss=0.032808609306812286
I0206 11:55:08.802039 139804644198144 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.03234599158167839, loss=0.03356482461094856
I0206 11:55:40.504946 139770185602816 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03039950132369995, loss=0.03589374199509621
I0206 11:55:44.083207 139978932307776 spec.py:321] Evaluating on the training split.
I0206 11:57:22.732378 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 11:57:25.772270 139978932307776 spec.py:349] Evaluating on the test split.
I0206 11:57:31.198020 139978932307776 submission_runner.py:408] Time since start: 6319.50s, 	Step: 13612, 	{'train/accuracy': 0.9904999732971191, 'train/loss': 0.031200099736452103, 'train/mean_average_precision': 0.39747005719247963, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.044112421572208405, 'validation/mean_average_precision': 0.26093377510968474, 'validation/num_examples': 43793, 'test/accuracy': 0.9859269857406616, 'test/loss': 0.04675736278295517, 'test/mean_average_precision': 0.2620023400737273, 'test/num_examples': 43793, 'score': 4335.5524117946625, 'total_duration': 6319.501272678375, 'accumulated_submission_time': 4335.5524117946625, 'accumulated_eval_time': 1983.0827827453613, 'accumulated_logging_time': 0.5029690265655518}
I0206 11:57:31.216745 139752176375552 logging_writer.py:48] [13612] accumulated_eval_time=1983.082783, accumulated_logging_time=0.502969, accumulated_submission_time=4335.552412, global_step=13612, preemption_count=0, score=4335.552412, test/accuracy=0.985927, test/loss=0.046757, test/mean_average_precision=0.262002, test/num_examples=43793, total_duration=6319.501273, train/accuracy=0.990500, train/loss=0.031200, train/mean_average_precision=0.397470, validation/accuracy=0.986765, validation/loss=0.044112, validation/mean_average_precision=0.260934, validation/num_examples=43793
I0206 11:57:59.831035 139916327515904 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.031626876443624496, loss=0.029040731489658356
I0206 11:58:31.865232 139752176375552 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0284279678016901, loss=0.031189972534775734
I0206 11:59:04.000255 139916327515904 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03384752199053764, loss=0.03759700804948807
I0206 11:59:35.823385 139752176375552 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03140433877706528, loss=0.030611753463745117
I0206 12:00:07.572895 139916327515904 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03575951233506203, loss=0.03349192440509796
I0206 12:00:39.580960 139752176375552 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03687933832406998, loss=0.03826070576906204
I0206 12:01:11.552685 139916327515904 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.04042375832796097, loss=0.032440148293972015
I0206 12:01:31.474231 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:03:09.083196 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:03:12.118933 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:03:15.118508 139978932307776 submission_runner.py:408] Time since start: 6663.42s, 	Step: 14362, 	{'train/accuracy': 0.9905785322189331, 'train/loss': 0.030962450429797173, 'train/mean_average_precision': 0.3904840151507778, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.04408657178282738, 'validation/mean_average_precision': 0.2646759096238124, 'validation/num_examples': 43793, 'test/accuracy': 0.9859316349029541, 'test/loss': 0.0466216616332531, 'test/mean_average_precision': 0.25938697563630925, 'test/num_examples': 43793, 'score': 4575.777185678482, 'total_duration': 6663.42175078392, 'accumulated_submission_time': 4575.777185678482, 'accumulated_eval_time': 2086.7270028591156, 'accumulated_logging_time': 0.5339128971099854}
I0206 12:03:15.137764 139752137176832 logging_writer.py:48] [14362] accumulated_eval_time=2086.727003, accumulated_logging_time=0.533913, accumulated_submission_time=4575.777186, global_step=14362, preemption_count=0, score=4575.777186, test/accuracy=0.985932, test/loss=0.046622, test/mean_average_precision=0.259387, test/num_examples=43793, total_duration=6663.421751, train/accuracy=0.990579, train/loss=0.030962, train/mean_average_precision=0.390484, validation/accuracy=0.986754, validation/loss=0.044087, validation/mean_average_precision=0.264676, validation/num_examples=43793
I0206 12:03:27.800604 139770185602816 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0438150130212307, loss=0.03898561745882034
I0206 12:03:59.780757 139752137176832 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03437865525484085, loss=0.03160811960697174
I0206 12:04:31.847001 139770185602816 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.037771616131067276, loss=0.033703792840242386
I0206 12:05:03.892497 139752137176832 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.031322333961725235, loss=0.03340280055999756
I0206 12:05:35.647648 139770185602816 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.032286424189805984, loss=0.02805154211819172
I0206 12:06:07.234697 139752137176832 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04220554977655411, loss=0.035498589277267456
I0206 12:06:38.657893 139770185602816 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.044324543327093124, loss=0.03397563472390175
I0206 12:07:10.909870 139752137176832 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03421147167682648, loss=0.030111316591501236
I0206 12:07:15.411114 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:08:50.581362 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:08:53.600008 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:08:56.716311 139978932307776 submission_runner.py:408] Time since start: 7005.02s, 	Step: 15115, 	{'train/accuracy': 0.9908715486526489, 'train/loss': 0.029975270852446556, 'train/mean_average_precision': 0.4078574356020376, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.043870147317647934, 'validation/mean_average_precision': 0.2704771739922652, 'validation/num_examples': 43793, 'test/accuracy': 0.9859648942947388, 'test/loss': 0.04653972387313843, 'test/mean_average_precision': 0.26243861631050347, 'test/num_examples': 43793, 'score': 4816.017210483551, 'total_duration': 7005.01956653595, 'accumulated_submission_time': 4816.017210483551, 'accumulated_eval_time': 2188.0321526527405, 'accumulated_logging_time': 0.5664629936218262}
I0206 12:08:56.734903 139804644198144 logging_writer.py:48] [15115] accumulated_eval_time=2188.032153, accumulated_logging_time=0.566463, accumulated_submission_time=4816.017210, global_step=15115, preemption_count=0, score=4816.017210, test/accuracy=0.985965, test/loss=0.046540, test/mean_average_precision=0.262439, test/num_examples=43793, total_duration=7005.019567, train/accuracy=0.990872, train/loss=0.029975, train/mean_average_precision=0.407857, validation/accuracy=0.986753, validation/loss=0.043870, validation/mean_average_precision=0.270477, validation/num_examples=43793
I0206 12:09:24.289241 139916327515904 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03203422576189041, loss=0.032459650188684464
I0206 12:09:56.027788 139804644198144 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.036789458245038986, loss=0.03621457889676094
I0206 12:10:27.975492 139916327515904 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03260423615574837, loss=0.030253397300839424
I0206 12:10:59.748733 139804644198144 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.04516899958252907, loss=0.03236747905611992
I0206 12:11:31.751581 139916327515904 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.03397615998983383, loss=0.03369415923953056
I0206 12:12:03.363374 139804644198144 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.041682422161102295, loss=0.03614256903529167
I0206 12:12:35.431308 139916327515904 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.036640461534261703, loss=0.03165796771645546
I0206 12:12:56.969282 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:14:35.228040 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:14:38.205842 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:14:41.185091 139978932307776 submission_runner.py:408] Time since start: 7349.49s, 	Step: 15868, 	{'train/accuracy': 0.990837574005127, 'train/loss': 0.029973115772008896, 'train/mean_average_precision': 0.4219413948334143, 'validation/accuracy': 0.9869092106819153, 'validation/loss': 0.04432278871536255, 'validation/mean_average_precision': 0.2641223882343574, 'validation/num_examples': 43793, 'test/accuracy': 0.9860209226608276, 'test/loss': 0.04720812290906906, 'test/mean_average_precision': 0.25567677099933905, 'test/num_examples': 43793, 'score': 5056.221124887466, 'total_duration': 7349.488345146179, 'accumulated_submission_time': 5056.221124887466, 'accumulated_eval_time': 2292.2479150295258, 'accumulated_logging_time': 0.595801830291748}
I0206 12:14:41.203703 139752137176832 logging_writer.py:48] [15868] accumulated_eval_time=2292.247915, accumulated_logging_time=0.595802, accumulated_submission_time=5056.221125, global_step=15868, preemption_count=0, score=5056.221125, test/accuracy=0.986021, test/loss=0.047208, test/mean_average_precision=0.255677, test/num_examples=43793, total_duration=7349.488345, train/accuracy=0.990838, train/loss=0.029973, train/mean_average_precision=0.421941, validation/accuracy=0.986909, validation/loss=0.044323, validation/mean_average_precision=0.264122, validation/num_examples=43793
I0206 12:14:51.713238 139770185602816 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.03679073229432106, loss=0.03009733557701111
I0206 12:15:23.402462 139752137176832 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.036240480840206146, loss=0.028176050633192062
I0206 12:15:54.874879 139770185602816 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.040590159595012665, loss=0.035901349037885666
I0206 12:16:26.392595 139752137176832 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.042221780866384506, loss=0.02993529662489891
I0206 12:16:57.702653 139770185602816 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.03346988558769226, loss=0.028673773631453514
I0206 12:17:28.944748 139752137176832 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0314083993434906, loss=0.033836729824543
I0206 12:18:00.320967 139770185602816 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.036359257996082306, loss=0.029125796630978584
I0206 12:18:31.823823 139752137176832 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05101347342133522, loss=0.0314142145216465
I0206 12:18:41.304702 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:20:17.140155 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:20:20.115566 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:20:23.071891 139978932307776 submission_runner.py:408] Time since start: 7691.38s, 	Step: 16631, 	{'train/accuracy': 0.9911161065101624, 'train/loss': 0.029060611501336098, 'train/mean_average_precision': 0.44426435509248297, 'validation/accuracy': 0.9867061972618103, 'validation/loss': 0.04393668472766876, 'validation/mean_average_precision': 0.2679191421744507, 'validation/num_examples': 43793, 'test/accuracy': 0.9859219789505005, 'test/loss': 0.04660208523273468, 'test/mean_average_precision': 0.27087723820100235, 'test/num_examples': 43793, 'score': 5296.290515899658, 'total_duration': 7691.375034809113, 'accumulated_submission_time': 5296.290515899658, 'accumulated_eval_time': 2394.0149455070496, 'accumulated_logging_time': 0.6254878044128418}
I0206 12:20:23.091277 139752176375552 logging_writer.py:48] [16631] accumulated_eval_time=2394.014946, accumulated_logging_time=0.625488, accumulated_submission_time=5296.290516, global_step=16631, preemption_count=0, score=5296.290516, test/accuracy=0.985922, test/loss=0.046602, test/mean_average_precision=0.270877, test/num_examples=43793, total_duration=7691.375035, train/accuracy=0.991116, train/loss=0.029061, train/mean_average_precision=0.444264, validation/accuracy=0.986706, validation/loss=0.043937, validation/mean_average_precision=0.267919, validation/num_examples=43793
I0206 12:20:45.269212 139804644198144 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.03793756663799286, loss=0.0330469124019146
I0206 12:21:16.761082 139752176375552 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.032027121633291245, loss=0.030743597075343132
I0206 12:21:48.091254 139804644198144 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.037257175892591476, loss=0.03149690106511116
I0206 12:22:19.515260 139752176375552 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.04162668064236641, loss=0.03456880897283554
I0206 12:22:51.104176 139804644198144 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.03726110979914665, loss=0.03017490915954113
I0206 12:23:22.734476 139752176375552 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03139660879969597, loss=0.029077759012579918
I0206 12:23:54.063079 139804644198144 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.04825053736567497, loss=0.0328703410923481
I0206 12:24:23.344445 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:25:59.808173 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:26:02.870174 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:26:05.859985 139978932307776 submission_runner.py:408] Time since start: 8034.16s, 	Step: 17393, 	{'train/accuracy': 0.9911894798278809, 'train/loss': 0.028837032616138458, 'train/mean_average_precision': 0.4573207657369297, 'validation/accuracy': 0.9868093132972717, 'validation/loss': 0.044242873787879944, 'validation/mean_average_precision': 0.268753044165262, 'validation/num_examples': 43793, 'test/accuracy': 0.9859758615493774, 'test/loss': 0.04703646898269653, 'test/mean_average_precision': 0.26876617914154183, 'test/num_examples': 43793, 'score': 5536.512343883514, 'total_duration': 8034.163238525391, 'accumulated_submission_time': 5536.512343883514, 'accumulated_eval_time': 2496.5304474830627, 'accumulated_logging_time': 0.655855655670166}
I0206 12:26:05.878650 139752137176832 logging_writer.py:48] [17393] accumulated_eval_time=2496.530447, accumulated_logging_time=0.655856, accumulated_submission_time=5536.512344, global_step=17393, preemption_count=0, score=5536.512344, test/accuracy=0.985976, test/loss=0.047036, test/mean_average_precision=0.268766, test/num_examples=43793, total_duration=8034.163239, train/accuracy=0.991189, train/loss=0.028837, train/mean_average_precision=0.457321, validation/accuracy=0.986809, validation/loss=0.044243, validation/mean_average_precision=0.268753, validation/num_examples=43793
I0206 12:26:08.404525 139770185602816 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.032886095345020294, loss=0.028974980115890503
I0206 12:26:39.948364 139752137176832 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03912634402513504, loss=0.029590189456939697
I0206 12:27:11.531608 139770185602816 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.051938727498054504, loss=0.030901234596967697
I0206 12:27:42.857164 139752137176832 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.03931853547692299, loss=0.02627137489616871
I0206 12:28:14.292733 139770185602816 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03468988835811615, loss=0.029115835204720497
I0206 12:28:45.972971 139752137176832 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04346024990081787, loss=0.030543480068445206
I0206 12:29:17.861608 139770185602816 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.08766651153564453, loss=0.03409440815448761
I0206 12:29:49.420644 139752137176832 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.05613967403769493, loss=0.03140972554683685
I0206 12:30:05.928779 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:31:42.865110 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:31:45.915923 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:31:48.906247 139978932307776 submission_runner.py:408] Time since start: 8377.21s, 	Step: 18153, 	{'train/accuracy': 0.9911827445030212, 'train/loss': 0.02886652573943138, 'train/mean_average_precision': 0.4415226656518888, 'validation/accuracy': 0.9869375824928284, 'validation/loss': 0.04401398450136185, 'validation/mean_average_precision': 0.27571217023098854, 'validation/num_examples': 43793, 'test/accuracy': 0.9860365390777588, 'test/loss': 0.0470629520714283, 'test/mean_average_precision': 0.2600734428996599, 'test/num_examples': 43793, 'score': 5776.531605005264, 'total_duration': 8377.209501743317, 'accumulated_submission_time': 5776.531605005264, 'accumulated_eval_time': 2599.5078728199005, 'accumulated_logging_time': 0.6851916313171387}
I0206 12:31:48.925467 139804644198144 logging_writer.py:48] [18153] accumulated_eval_time=2599.507873, accumulated_logging_time=0.685192, accumulated_submission_time=5776.531605, global_step=18153, preemption_count=0, score=5776.531605, test/accuracy=0.986037, test/loss=0.047063, test/mean_average_precision=0.260073, test/num_examples=43793, total_duration=8377.209502, train/accuracy=0.991183, train/loss=0.028867, train/mean_average_precision=0.441523, validation/accuracy=0.986938, validation/loss=0.044014, validation/mean_average_precision=0.275712, validation/num_examples=43793
I0206 12:32:04.138117 139916327515904 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04018223285675049, loss=0.033029474318027496
I0206 12:32:36.045918 139804644198144 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.03879385441541672, loss=0.03192581981420517
I0206 12:33:07.851590 139916327515904 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.04304010793566704, loss=0.029924986883997917
I0206 12:33:39.772022 139804644198144 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.03711928054690361, loss=0.02783682569861412
I0206 12:34:11.758336 139916327515904 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04339246824383736, loss=0.03290503844618797
I0206 12:34:44.420868 139804644198144 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.03118644468486309, loss=0.027366185560822487
I0206 12:35:16.028227 139916327515904 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04131307080388069, loss=0.0298471562564373
I0206 12:35:47.726650 139804644198144 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04156123101711273, loss=0.031650811433792114
I0206 12:35:49.011084 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:37:25.984077 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:37:28.976878 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:37:32.006624 139978932307776 submission_runner.py:408] Time since start: 8720.31s, 	Step: 18905, 	{'train/accuracy': 0.9912071824073792, 'train/loss': 0.02895207516849041, 'train/mean_average_precision': 0.42595162934474834, 'validation/accuracy': 0.9868218898773193, 'validation/loss': 0.04381611943244934, 'validation/mean_average_precision': 0.2703069049199251, 'validation/num_examples': 43793, 'test/accuracy': 0.9859219789505005, 'test/loss': 0.04655629023909569, 'test/mean_average_precision': 0.26244538041953613, 'test/num_examples': 43793, 'score': 6016.586377620697, 'total_duration': 8720.30987906456, 'accumulated_submission_time': 6016.586377620697, 'accumulated_eval_time': 2702.5033643245697, 'accumulated_logging_time': 0.7152974605560303}
I0206 12:37:32.026324 139752176375552 logging_writer.py:48] [18905] accumulated_eval_time=2702.503364, accumulated_logging_time=0.715297, accumulated_submission_time=6016.586378, global_step=18905, preemption_count=0, score=6016.586378, test/accuracy=0.985922, test/loss=0.046556, test/mean_average_precision=0.262445, test/num_examples=43793, total_duration=8720.309879, train/accuracy=0.991207, train/loss=0.028952, train/mean_average_precision=0.425952, validation/accuracy=0.986822, validation/loss=0.043816, validation/mean_average_precision=0.270307, validation/num_examples=43793
I0206 12:38:02.487285 139770185602816 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.045083802193403244, loss=0.03141183778643608
I0206 12:38:34.405625 139752176375552 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03686077892780304, loss=0.03294496610760689
I0206 12:39:06.333778 139770185602816 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04318198189139366, loss=0.03444445878267288
I0206 12:39:38.100678 139752176375552 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.0509372353553772, loss=0.03273950517177582
I0206 12:40:09.744701 139770185602816 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.03798327222466469, loss=0.031094098463654518
I0206 12:40:41.619366 139752176375552 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05695516988635063, loss=0.03015931136906147
I0206 12:41:13.638859 139770185602816 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.0513908714056015, loss=0.031182609498500824
I0206 12:41:32.013818 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:43:09.122677 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:43:12.252634 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:43:15.257925 139978932307776 submission_runner.py:408] Time since start: 9063.56s, 	Step: 19659, 	{'train/accuracy': 0.9910314083099365, 'train/loss': 0.029263755306601524, 'train/mean_average_precision': 0.4337188049806543, 'validation/accuracy': 0.9868438243865967, 'validation/loss': 0.044348929077386856, 'validation/mean_average_precision': 0.26577527529149897, 'validation/num_examples': 43793, 'test/accuracy': 0.9860196709632874, 'test/loss': 0.04709337279200554, 'test/mean_average_precision': 0.2598258010263444, 'test/num_examples': 43793, 'score': 6256.5426633358, 'total_duration': 9063.561178445816, 'accumulated_submission_time': 6256.5426633358, 'accumulated_eval_time': 2805.7474246025085, 'accumulated_logging_time': 0.7460854053497314}
I0206 12:43:15.277181 139733347358464 logging_writer.py:48] [19659] accumulated_eval_time=2805.747425, accumulated_logging_time=0.746085, accumulated_submission_time=6256.542663, global_step=19659, preemption_count=0, score=6256.542663, test/accuracy=0.986020, test/loss=0.047093, test/mean_average_precision=0.259826, test/num_examples=43793, total_duration=9063.561178, train/accuracy=0.991031, train/loss=0.029264, train/mean_average_precision=0.433719, validation/accuracy=0.986844, validation/loss=0.044349, validation/mean_average_precision=0.265775, validation/num_examples=43793
I0206 12:43:28.914228 139752137176832 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.03869723901152611, loss=0.027476662769913673
I0206 12:44:00.666753 139733347358464 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.04539036005735397, loss=0.029202530160546303
I0206 12:44:33.230462 139752137176832 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04463297501206398, loss=0.03146904334425926
I0206 12:45:05.732795 139733347358464 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04478611797094345, loss=0.028411943465471268
I0206 12:45:37.821486 139752137176832 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04936404898762703, loss=0.029732590541243553
I0206 12:46:09.868204 139733347358464 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04705037549138069, loss=0.026101889088749886
I0206 12:46:41.630805 139752137176832 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.04016884043812752, loss=0.03164055943489075
I0206 12:47:13.696090 139733347358464 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.04195229709148407, loss=0.030150992795825005
I0206 12:47:15.306741 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:48:50.728408 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:48:53.787486 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:48:56.804674 139978932307776 submission_runner.py:408] Time since start: 9405.11s, 	Step: 20406, 	{'train/accuracy': 0.9910750985145569, 'train/loss': 0.02913183718919754, 'train/mean_average_precision': 0.4227976599470897, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04403292387723923, 'validation/mean_average_precision': 0.26792202286689754, 'validation/num_examples': 43793, 'test/accuracy': 0.9860963225364685, 'test/loss': 0.04649520292878151, 'test/mean_average_precision': 0.2645376394413712, 'test/num_examples': 43793, 'score': 6496.541145086288, 'total_duration': 9405.10793018341, 'accumulated_submission_time': 6496.541145086288, 'accumulated_eval_time': 2907.2453095912933, 'accumulated_logging_time': 0.7763607501983643}
I0206 12:48:56.824129 139770185602816 logging_writer.py:48] [20406] accumulated_eval_time=2907.245310, accumulated_logging_time=0.776361, accumulated_submission_time=6496.541145, global_step=20406, preemption_count=0, score=6496.541145, test/accuracy=0.986096, test/loss=0.046495, test/mean_average_precision=0.264538, test/num_examples=43793, total_duration=9405.107930, train/accuracy=0.991075, train/loss=0.029132, train/mean_average_precision=0.422798, validation/accuracy=0.986786, validation/loss=0.044033, validation/mean_average_precision=0.267922, validation/num_examples=43793
I0206 12:49:27.008648 139804644198144 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.05140037089586258, loss=0.034190576523542404
I0206 12:49:59.379612 139770185602816 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.044877272099256516, loss=0.02298366092145443
I0206 12:50:31.545264 139804644198144 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.0416509248316288, loss=0.0316108874976635
I0206 12:51:04.767042 139770185602816 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.05055671185255051, loss=0.029293512925505638
I0206 12:51:36.868976 139804644198144 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.07467783987522125, loss=0.030556023120880127
I0206 12:52:08.680458 139770185602816 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.048218488693237305, loss=0.03161226958036423
I0206 12:52:40.356039 139804644198144 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.05143529921770096, loss=0.032033152878284454
I0206 12:52:56.875308 139978932307776 spec.py:321] Evaluating on the training split.
I0206 12:54:34.271920 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 12:54:37.409031 139978932307776 spec.py:349] Evaluating on the test split.
I0206 12:54:40.568863 139978932307776 submission_runner.py:408] Time since start: 9748.87s, 	Step: 21153, 	{'train/accuracy': 0.9912204146385193, 'train/loss': 0.02879350446164608, 'train/mean_average_precision': 0.4551327226887302, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.043685078620910645, 'validation/mean_average_precision': 0.27464628654591416, 'validation/num_examples': 43793, 'test/accuracy': 0.9861363172531128, 'test/loss': 0.04666018486022949, 'test/mean_average_precision': 0.2698105443588578, 'test/num_examples': 43793, 'score': 6736.561691999435, 'total_duration': 9748.872096776962, 'accumulated_submission_time': 6736.561691999435, 'accumulated_eval_time': 3010.9387967586517, 'accumulated_logging_time': 0.8066954612731934}
I0206 12:54:40.589457 139733347358464 logging_writer.py:48] [21153] accumulated_eval_time=3010.938797, accumulated_logging_time=0.806695, accumulated_submission_time=6736.561692, global_step=21153, preemption_count=0, score=6736.561692, test/accuracy=0.986136, test/loss=0.046660, test/mean_average_precision=0.269811, test/num_examples=43793, total_duration=9748.872097, train/accuracy=0.991220, train/loss=0.028794, train/mean_average_precision=0.455133, validation/accuracy=0.986974, validation/loss=0.043685, validation/mean_average_precision=0.274646, validation/num_examples=43793
I0206 12:54:55.848629 139752176375552 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.06162154674530029, loss=0.029898278415203094
I0206 12:55:28.044993 139733347358464 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0428047701716423, loss=0.028828158974647522
I0206 12:55:59.862477 139752176375552 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.049445170909166336, loss=0.03010771982371807
I0206 12:56:31.613401 139733347358464 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.06670042872428894, loss=0.03522565960884094
I0206 12:57:03.562747 139752176375552 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.06991282105445862, loss=0.0313585065305233
I0206 12:57:35.205782 139733347358464 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.0579625703394413, loss=0.025013698264956474
I0206 12:58:06.884927 139752176375552 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.05416882783174515, loss=0.03096473030745983
I0206 12:58:38.592611 139733347358464 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04893749579787254, loss=0.03345179185271263
I0206 12:58:40.812975 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:00:17.130143 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:00:20.259878 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:00:23.361020 139978932307776 submission_runner.py:408] Time since start: 10091.66s, 	Step: 21908, 	{'train/accuracy': 0.9911985397338867, 'train/loss': 0.028594762086868286, 'train/mean_average_precision': 0.4477774106470991, 'validation/accuracy': 0.9868621230125427, 'validation/loss': 0.0438196025788784, 'validation/mean_average_precision': 0.2761807715640725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861161112785339, 'test/loss': 0.046595923602581024, 'test/mean_average_precision': 0.2598133222072068, 'test/num_examples': 43793, 'score': 6976.753581285477, 'total_duration': 10091.66427564621, 'accumulated_submission_time': 6976.753581285477, 'accumulated_eval_time': 3113.4867935180664, 'accumulated_logging_time': 0.838770866394043}
I0206 13:00:23.380623 139752137176832 logging_writer.py:48] [21908] accumulated_eval_time=3113.486794, accumulated_logging_time=0.838771, accumulated_submission_time=6976.753581, global_step=21908, preemption_count=0, score=6976.753581, test/accuracy=0.986116, test/loss=0.046596, test/mean_average_precision=0.259813, test/num_examples=43793, total_duration=10091.664276, train/accuracy=0.991199, train/loss=0.028595, train/mean_average_precision=0.447777, validation/accuracy=0.986862, validation/loss=0.043820, validation/mean_average_precision=0.276181, validation/num_examples=43793
I0206 13:00:52.893856 139770185602816 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.060946397483348846, loss=0.03174542263150215
I0206 13:01:24.760839 139752137176832 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.06296740472316742, loss=0.032880254089832306
I0206 13:01:56.539456 139770185602816 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.05109240487217903, loss=0.03145222365856171
I0206 13:02:28.867456 139752137176832 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04715961590409279, loss=0.028637072071433067
I0206 13:03:00.387332 139770185602816 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0499366857111454, loss=0.031523846089839935
I0206 13:03:32.413817 139752137176832 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06437870860099792, loss=0.03167300671339035
I0206 13:04:04.294970 139770185602816 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.04883863776922226, loss=0.032221462577581406
I0206 13:04:23.512904 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:06:00.326062 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:06:03.556096 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:06:06.629011 139978932307776 submission_runner.py:408] Time since start: 10434.93s, 	Step: 22662, 	{'train/accuracy': 0.9913204908370972, 'train/loss': 0.028243862092494965, 'train/mean_average_precision': 0.4488545594683917, 'validation/accuracy': 0.9869660139083862, 'validation/loss': 0.044106561690568924, 'validation/mean_average_precision': 0.27705950686903197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861489534378052, 'test/loss': 0.04689888656139374, 'test/mean_average_precision': 0.26680178288940487, 'test/num_examples': 43793, 'score': 7216.855234861374, 'total_duration': 10434.932267189026, 'accumulated_submission_time': 7216.855234861374, 'accumulated_eval_time': 3216.6028592586517, 'accumulated_logging_time': 0.8691599369049072}
I0206 13:06:06.649302 139733347358464 logging_writer.py:48] [22662] accumulated_eval_time=3216.602859, accumulated_logging_time=0.869160, accumulated_submission_time=7216.855235, global_step=22662, preemption_count=0, score=7216.855235, test/accuracy=0.986149, test/loss=0.046899, test/mean_average_precision=0.266802, test/num_examples=43793, total_duration=10434.932267, train/accuracy=0.991320, train/loss=0.028244, train/mean_average_precision=0.448855, validation/accuracy=0.986966, validation/loss=0.044107, validation/mean_average_precision=0.277060, validation/num_examples=43793
I0206 13:06:19.291511 139752176375552 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.052362989634275436, loss=0.02805912122130394
I0206 13:06:51.216838 139733347358464 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.05648178979754448, loss=0.03215469792485237
I0206 13:07:23.365957 139752176375552 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.05492350086569786, loss=0.02905050478875637
I0206 13:07:55.685526 139733347358464 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06831002235412598, loss=0.02949276752769947
I0206 13:08:27.655708 139752176375552 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.050497639924287796, loss=0.028295136988162994
I0206 13:08:59.883831 139733347358464 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.04847705364227295, loss=0.029264146462082863
I0206 13:09:32.031556 139752176375552 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.0474381297826767, loss=0.03138591721653938
I0206 13:10:04.115103 139733347358464 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04274770990014076, loss=0.0262601301074028
I0206 13:10:06.660664 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:11:45.417877 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:11:48.589510 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:11:51.675681 139978932307776 submission_runner.py:408] Time since start: 10779.98s, 	Step: 23409, 	{'train/accuracy': 0.991339385509491, 'train/loss': 0.02800729125738144, 'train/mean_average_precision': 0.4768710540761837, 'validation/accuracy': 0.9870260953903198, 'validation/loss': 0.043588072061538696, 'validation/mean_average_precision': 0.27544556176136864, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.04647839441895485, 'test/mean_average_precision': 0.2654733454371574, 'test/num_examples': 43793, 'score': 7456.835754871368, 'total_duration': 10779.978934049606, 'accumulated_submission_time': 7456.835754871368, 'accumulated_eval_time': 3321.6178319454193, 'accumulated_logging_time': 0.9006702899932861}
I0206 13:11:51.696430 139770185602816 logging_writer.py:48] [23409] accumulated_eval_time=3321.617832, accumulated_logging_time=0.900670, accumulated_submission_time=7456.835755, global_step=23409, preemption_count=0, score=7456.835755, test/accuracy=0.986184, test/loss=0.046478, test/mean_average_precision=0.265473, test/num_examples=43793, total_duration=10779.978934, train/accuracy=0.991339, train/loss=0.028007, train/mean_average_precision=0.476871, validation/accuracy=0.987026, validation/loss=0.043588, validation/mean_average_precision=0.275446, validation/num_examples=43793
I0206 13:12:20.808476 139804644198144 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.04717680439352989, loss=0.03285877779126167
I0206 13:12:52.335216 139770185602816 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.056059639900922775, loss=0.02660815790295601
I0206 13:13:24.063328 139804644198144 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.060278765857219696, loss=0.030939744785428047
I0206 13:13:55.323496 139770185602816 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.046966537833213806, loss=0.03163973614573479
I0206 13:14:26.772987 139804644198144 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.05437774583697319, loss=0.0311694648116827
I0206 13:14:58.275924 139770185602816 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.04610373452305794, loss=0.030879078432917595
I0206 13:15:29.896834 139804644198144 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.054528504610061646, loss=0.030460728332400322
I0206 13:15:51.841239 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:17:29.307931 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:17:32.460767 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:17:35.524029 139978932307776 submission_runner.py:408] Time since start: 11123.83s, 	Step: 24169, 	{'train/accuracy': 0.9917247891426086, 'train/loss': 0.026840627193450928, 'train/mean_average_precision': 0.48928305066369693, 'validation/accuracy': 0.9871340990066528, 'validation/loss': 0.04346122220158577, 'validation/mean_average_precision': 0.28249201456963385, 'validation/num_examples': 43793, 'test/accuracy': 0.9862883687019348, 'test/loss': 0.04660702869296074, 'test/mean_average_precision': 0.2689184754578295, 'test/num_examples': 43793, 'score': 7696.949980020523, 'total_duration': 11123.827285289764, 'accumulated_submission_time': 7696.949980020523, 'accumulated_eval_time': 3425.3005759716034, 'accumulated_logging_time': 0.932265043258667}
I0206 13:17:35.544357 139733347358464 logging_writer.py:48] [24169] accumulated_eval_time=3425.300576, accumulated_logging_time=0.932265, accumulated_submission_time=7696.949980, global_step=24169, preemption_count=0, score=7696.949980, test/accuracy=0.986288, test/loss=0.046607, test/mean_average_precision=0.268918, test/num_examples=43793, total_duration=11123.827285, train/accuracy=0.991725, train/loss=0.026841, train/mean_average_precision=0.489283, validation/accuracy=0.987134, validation/loss=0.043461, validation/mean_average_precision=0.282492, validation/num_examples=43793
I0206 13:17:45.727733 139752176375552 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.05475011095404625, loss=0.03081999532878399
I0206 13:18:17.899086 139733347358464 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.05360710620880127, loss=0.025631271302700043
I0206 13:18:49.840516 139752176375552 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05460876226425171, loss=0.0259283147752285
I0206 13:19:21.524919 139733347358464 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05203661322593689, loss=0.030224544927477837
I0206 13:19:53.026007 139752176375552 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.07012182474136353, loss=0.029749449342489243
I0206 13:20:25.130808 139733347358464 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.05521925538778305, loss=0.03405028209090233
I0206 13:20:58.226807 139752176375552 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.043205294758081436, loss=0.024617375805974007
I0206 13:21:30.364494 139733347358464 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.061195965856313705, loss=0.030322715640068054
I0206 13:21:35.829011 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:23:17.760044 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:23:20.881181 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:23:23.958626 139978932307776 submission_runner.py:408] Time since start: 11472.26s, 	Step: 24918, 	{'train/accuracy': 0.9918337464332581, 'train/loss': 0.02654416300356388, 'train/mean_average_precision': 0.5018239814929434, 'validation/accuracy': 0.9869444966316223, 'validation/loss': 0.0440145805478096, 'validation/mean_average_precision': 0.28537577870853154, 'validation/num_examples': 43793, 'test/accuracy': 0.9860310554504395, 'test/loss': 0.046867869794368744, 'test/mean_average_precision': 0.2691691968252736, 'test/num_examples': 43793, 'score': 7937.204426765442, 'total_duration': 11472.261877059937, 'accumulated_submission_time': 7937.204426765442, 'accumulated_eval_time': 3533.4301381111145, 'accumulated_logging_time': 0.9634485244750977}
I0206 13:23:23.979230 139752137176832 logging_writer.py:48] [24918] accumulated_eval_time=3533.430138, accumulated_logging_time=0.963449, accumulated_submission_time=7937.204427, global_step=24918, preemption_count=0, score=7937.204427, test/accuracy=0.986031, test/loss=0.046868, test/mean_average_precision=0.269169, test/num_examples=43793, total_duration=11472.261877, train/accuracy=0.991834, train/loss=0.026544, train/mean_average_precision=0.501824, validation/accuracy=0.986944, validation/loss=0.044015, validation/mean_average_precision=0.285376, validation/num_examples=43793
I0206 13:23:51.332218 139770185602816 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0539357028901577, loss=0.028662117198109627
I0206 13:24:23.227518 139752137176832 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.05697249993681908, loss=0.02718896046280861
I0206 13:24:54.674323 139770185602816 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.04563529044389725, loss=0.029816847294569016
I0206 13:25:26.474169 139752137176832 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.05245126783847809, loss=0.029281487688422203
I0206 13:25:57.967338 139770185602816 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.05300572142004967, loss=0.028591303154826164
I0206 13:26:29.647347 139752137176832 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04571977257728577, loss=0.026037175208330154
I0206 13:27:01.672700 139770185602816 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.05067313089966774, loss=0.029995357617735863
I0206 13:27:24.177016 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:29:00.840291 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:29:03.896404 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:29:06.860623 139978932307776 submission_runner.py:408] Time since start: 11815.16s, 	Step: 25672, 	{'train/accuracy': 0.9916606545448303, 'train/loss': 0.02706550620496273, 'train/mean_average_precision': 0.4931265444328698, 'validation/accuracy': 0.9869298934936523, 'validation/loss': 0.04404542222619057, 'validation/mean_average_precision': 0.2736783751099223, 'validation/num_examples': 43793, 'test/accuracy': 0.9861308932304382, 'test/loss': 0.046751171350479126, 'test/mean_average_precision': 0.26793754461580116, 'test/num_examples': 43793, 'score': 8177.371206998825, 'total_duration': 11815.163880109787, 'accumulated_submission_time': 8177.371206998825, 'accumulated_eval_time': 3636.113703250885, 'accumulated_logging_time': 0.9951300621032715}
I0206 13:29:06.881016 139733347358464 logging_writer.py:48] [25672] accumulated_eval_time=3636.113703, accumulated_logging_time=0.995130, accumulated_submission_time=8177.371207, global_step=25672, preemption_count=0, score=8177.371207, test/accuracy=0.986131, test/loss=0.046751, test/mean_average_precision=0.267938, test/num_examples=43793, total_duration=11815.163880, train/accuracy=0.991661, train/loss=0.027066, train/mean_average_precision=0.493127, validation/accuracy=0.986930, validation/loss=0.044045, validation/mean_average_precision=0.273678, validation/num_examples=43793
I0206 13:29:16.118048 139752176375552 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.0571868009865284, loss=0.03214385733008385
I0206 13:29:47.689249 139733347358464 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.0561693049967289, loss=0.03048628196120262
I0206 13:30:19.031977 139752176375552 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.05675376579165459, loss=0.0341571681201458
I0206 13:30:50.649200 139733347358464 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.04760684445500374, loss=0.02834301069378853
I0206 13:31:22.442684 139752176375552 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.06011094152927399, loss=0.03266660124063492
I0206 13:31:54.195626 139733347358464 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.05855453014373779, loss=0.03251020610332489
I0206 13:32:26.005131 139752176375552 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04793136566877365, loss=0.02759724296629429
I0206 13:32:57.681229 139733347358464 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05812544375658035, loss=0.029812848195433617
I0206 13:33:06.950978 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:34:49.058300 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:34:52.103140 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:34:55.092774 139978932307776 submission_runner.py:408] Time since start: 12163.40s, 	Step: 26430, 	{'train/accuracy': 0.9915685653686523, 'train/loss': 0.027531573548913002, 'train/mean_average_precision': 0.47640785936763286, 'validation/accuracy': 0.9869379997253418, 'validation/loss': 0.043456874787807465, 'validation/mean_average_precision': 0.280425913360974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861363172531128, 'test/loss': 0.04630114138126373, 'test/mean_average_precision': 0.2708964711546916, 'test/num_examples': 43793, 'score': 8417.410165786743, 'total_duration': 12163.396028280258, 'accumulated_submission_time': 8417.410165786743, 'accumulated_eval_time': 3744.255452156067, 'accumulated_logging_time': 1.0263991355895996}
I0206 13:34:55.113446 139770185602816 logging_writer.py:48] [26430] accumulated_eval_time=3744.255452, accumulated_logging_time=1.026399, accumulated_submission_time=8417.410166, global_step=26430, preemption_count=0, score=8417.410166, test/accuracy=0.986136, test/loss=0.046301, test/mean_average_precision=0.270896, test/num_examples=43793, total_duration=12163.396028, train/accuracy=0.991569, train/loss=0.027532, train/mean_average_precision=0.476408, validation/accuracy=0.986938, validation/loss=0.043457, validation/mean_average_precision=0.280426, validation/num_examples=43793
I0206 13:35:17.605441 139804644198144 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.05430400371551514, loss=0.028979038819670677
I0206 13:35:48.997564 139770185602816 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.0535774789750576, loss=0.030374040827155113
I0206 13:36:21.005269 139804644198144 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.06643084436655045, loss=0.031564876437187195
I0206 13:36:52.491137 139770185602816 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.054657597094774246, loss=0.031233953312039375
I0206 13:37:24.316210 139804644198144 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05472554266452789, loss=0.03281247615814209
I0206 13:37:55.954038 139770185602816 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05766560137271881, loss=0.030315980315208435
I0206 13:38:27.618272 139804644198144 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.05155876278877258, loss=0.03108922950923443
I0206 13:38:55.296334 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:40:32.489997 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:40:35.580183 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:40:38.571281 139978932307776 submission_runner.py:408] Time since start: 12506.87s, 	Step: 27188, 	{'train/accuracy': 0.9914869666099548, 'train/loss': 0.027469003573060036, 'train/mean_average_precision': 0.469126213426749, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.044153884053230286, 'validation/mean_average_precision': 0.28014835494886564, 'validation/num_examples': 43793, 'test/accuracy': 0.9861030578613281, 'test/loss': 0.047083813697099686, 'test/mean_average_precision': 0.26727003029691904, 'test/num_examples': 43793, 'score': 8657.560030698776, 'total_duration': 12506.874527454376, 'accumulated_submission_time': 8657.560030698776, 'accumulated_eval_time': 3847.5303523540497, 'accumulated_logging_time': 1.059520959854126}
I0206 13:40:38.592589 139733347358464 logging_writer.py:48] [27188] accumulated_eval_time=3847.530352, accumulated_logging_time=1.059521, accumulated_submission_time=8657.560031, global_step=27188, preemption_count=0, score=8657.560031, test/accuracy=0.986103, test/loss=0.047084, test/mean_average_precision=0.267270, test/num_examples=43793, total_duration=12506.874527, train/accuracy=0.991487, train/loss=0.027469, train/mean_average_precision=0.469126, validation/accuracy=0.986975, validation/loss=0.044154, validation/mean_average_precision=0.280148, validation/num_examples=43793
I0206 13:40:42.734187 139752137176832 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.046982474625110626, loss=0.02842045947909355
I0206 13:41:14.328314 139733347358464 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.053971223533153534, loss=0.028281481936573982
I0206 13:41:46.183491 139752137176832 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.053970836102962494, loss=0.030575867742300034
I0206 13:42:17.683694 139733347358464 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0564831867814064, loss=0.031244879588484764
I0206 13:42:49.504561 139752137176832 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.05513593927025795, loss=0.028962718322873116
I0206 13:43:20.984051 139733347358464 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.056857265532016754, loss=0.030246760696172714
I0206 13:43:52.149846 139752137176832 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.0595780573785305, loss=0.029648516327142715
I0206 13:44:23.927996 139733347358464 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05949896574020386, loss=0.029355397447943687
I0206 13:44:38.691822 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:46:13.443067 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:46:16.471217 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:46:19.482253 139978932307776 submission_runner.py:408] Time since start: 12847.79s, 	Step: 27948, 	{'train/accuracy': 0.9915177226066589, 'train/loss': 0.027623098343610764, 'train/mean_average_precision': 0.46357976633261544, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.04409085214138031, 'validation/mean_average_precision': 0.27683687410500485, 'validation/num_examples': 43793, 'test/accuracy': 0.9861097931861877, 'test/loss': 0.04702117294073105, 'test/mean_average_precision': 0.2677642311999752, 'test/num_examples': 43793, 'score': 8897.627020835876, 'total_duration': 12847.785507917404, 'accumulated_submission_time': 8897.627020835876, 'accumulated_eval_time': 3948.3207392692566, 'accumulated_logging_time': 1.0930004119873047}
I0206 13:46:19.502688 139770185602816 logging_writer.py:48] [27948] accumulated_eval_time=3948.320739, accumulated_logging_time=1.093000, accumulated_submission_time=8897.627021, global_step=27948, preemption_count=0, score=8897.627021, test/accuracy=0.986110, test/loss=0.047021, test/mean_average_precision=0.267764, test/num_examples=43793, total_duration=12847.785508, train/accuracy=0.991518, train/loss=0.027623, train/mean_average_precision=0.463580, validation/accuracy=0.986879, validation/loss=0.044091, validation/mean_average_precision=0.276837, validation/num_examples=43793
I0206 13:46:36.637534 139804644198144 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.05726472660899162, loss=0.030419563874602318
I0206 13:47:08.886935 139770185602816 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05177421122789383, loss=0.029934337362647057
I0206 13:47:40.180977 139804644198144 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.050157416611909866, loss=0.03111284039914608
I0206 13:48:12.031706 139770185602816 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.06645317375659943, loss=0.02828330360352993
I0206 13:48:43.567106 139804644198144 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.06192472577095032, loss=0.027897238731384277
I0206 13:49:15.134496 139770185602816 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.04986492544412613, loss=0.029363567009568214
I0206 13:49:47.416703 139804644198144 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.05358821898698807, loss=0.028290754184126854
I0206 13:50:18.975869 139770185602816 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.06583122909069061, loss=0.029722025617957115
I0206 13:50:19.600585 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:51:58.191641 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:52:01.233959 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:52:04.222343 139978932307776 submission_runner.py:408] Time since start: 13192.53s, 	Step: 28703, 	{'train/accuracy': 0.9916020035743713, 'train/loss': 0.027359729632735252, 'train/mean_average_precision': 0.47975518086924696, 'validation/accuracy': 0.9869339466094971, 'validation/loss': 0.044039491564035416, 'validation/mean_average_precision': 0.27764830198249624, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.04705201834440231, 'test/mean_average_precision': 0.26992850144159103, 'test/num_examples': 43793, 'score': 9137.693308830261, 'total_duration': 13192.525599956512, 'accumulated_submission_time': 9137.693308830261, 'accumulated_eval_time': 4052.9424483776093, 'accumulated_logging_time': 1.1257007122039795}
I0206 13:52:04.243041 139752137176832 logging_writer.py:48] [28703] accumulated_eval_time=4052.942448, accumulated_logging_time=1.125701, accumulated_submission_time=9137.693309, global_step=28703, preemption_count=0, score=9137.693309, test/accuracy=0.986113, test/loss=0.047052, test/mean_average_precision=0.269929, test/num_examples=43793, total_duration=13192.525600, train/accuracy=0.991602, train/loss=0.027360, train/mean_average_precision=0.479755, validation/accuracy=0.986934, validation/loss=0.044039, validation/mean_average_precision=0.277648, validation/num_examples=43793
I0206 13:52:35.965243 139752176375552 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.053473398089408875, loss=0.029223298653960228
I0206 13:53:08.126955 139752137176832 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.062063444405794144, loss=0.02770133875310421
I0206 13:53:39.798556 139752176375552 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.048762623220682144, loss=0.026030052453279495
I0206 13:54:11.872339 139752137176832 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05707898363471031, loss=0.027673350647091866
I0206 13:54:43.515045 139752176375552 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04850197210907936, loss=0.027557382360100746
I0206 13:55:15.161290 139752137176832 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.057374633848667145, loss=0.030797868967056274
I0206 13:55:46.707126 139752176375552 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05773923173546791, loss=0.027100328356027603
I0206 13:56:04.426011 139978932307776 spec.py:321] Evaluating on the training split.
I0206 13:57:40.411089 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 13:57:43.498653 139978932307776 spec.py:349] Evaluating on the test split.
I0206 13:57:46.463264 139978932307776 submission_runner.py:408] Time since start: 13534.77s, 	Step: 29457, 	{'train/accuracy': 0.9915141463279724, 'train/loss': 0.027219338342547417, 'train/mean_average_precision': 0.481494203847978, 'validation/accuracy': 0.9870768189430237, 'validation/loss': 0.043897371739149094, 'validation/mean_average_precision': 0.2825347877673383, 'validation/num_examples': 43793, 'test/accuracy': 0.9862100481987, 'test/loss': 0.04682281240820885, 'test/mean_average_precision': 0.27629275772368966, 'test/num_examples': 43793, 'score': 9377.844159126282, 'total_duration': 13534.766516923904, 'accumulated_submission_time': 9377.844159126282, 'accumulated_eval_time': 4154.9796550273895, 'accumulated_logging_time': 1.1584973335266113}
I0206 13:57:46.485231 139733347358464 logging_writer.py:48] [29457] accumulated_eval_time=4154.979655, accumulated_logging_time=1.158497, accumulated_submission_time=9377.844159, global_step=29457, preemption_count=0, score=9377.844159, test/accuracy=0.986210, test/loss=0.046823, test/mean_average_precision=0.276293, test/num_examples=43793, total_duration=13534.766517, train/accuracy=0.991514, train/loss=0.027219, train/mean_average_precision=0.481494, validation/accuracy=0.987077, validation/loss=0.043897, validation/mean_average_precision=0.282535, validation/num_examples=43793
I0206 13:58:00.502445 139770185602816 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.062303658574819565, loss=0.026944471523165703
I0206 13:58:32.362612 139733347358464 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.05280197039246559, loss=0.02786441706120968
I0206 13:59:04.110792 139770185602816 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.0599113367497921, loss=0.028820525854825974
I0206 13:59:35.716228 139733347358464 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.04961865767836571, loss=0.027129249647259712
I0206 14:00:07.395298 139770185602816 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05119815468788147, loss=0.031074756756424904
I0206 14:00:39.048394 139733347358464 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0682147666811943, loss=0.03229657933115959
I0206 14:01:10.507439 139770185602816 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.06337825208902359, loss=0.030363179743289948
I0206 14:01:42.509656 139733347358464 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.05169976130127907, loss=0.028021037578582764
I0206 14:01:46.582682 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:03:24.727854 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:03:27.781342 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:03:30.726400 139978932307776 submission_runner.py:408] Time since start: 13879.03s, 	Step: 30214, 	{'train/accuracy': 0.9916245937347412, 'train/loss': 0.02697494812309742, 'train/mean_average_precision': 0.4861085832401384, 'validation/accuracy': 0.9869063496589661, 'validation/loss': 0.04395526275038719, 'validation/mean_average_precision': 0.27401784640488724, 'validation/num_examples': 43793, 'test/accuracy': 0.9861536026000977, 'test/loss': 0.04662062227725983, 'test/mean_average_precision': 0.26275205954140707, 'test/num_examples': 43793, 'score': 9617.910662651062, 'total_duration': 13879.029645681381, 'accumulated_submission_time': 9617.910662651062, 'accumulated_eval_time': 4259.123318433762, 'accumulated_logging_time': 1.1916203498840332}
I0206 14:03:30.747827 139752137176832 logging_writer.py:48] [30214] accumulated_eval_time=4259.123318, accumulated_logging_time=1.191620, accumulated_submission_time=9617.910663, global_step=30214, preemption_count=0, score=9617.910663, test/accuracy=0.986154, test/loss=0.046621, test/mean_average_precision=0.262752, test/num_examples=43793, total_duration=13879.029646, train/accuracy=0.991625, train/loss=0.026975, train/mean_average_precision=0.486109, validation/accuracy=0.986906, validation/loss=0.043955, validation/mean_average_precision=0.274018, validation/num_examples=43793
I0206 14:03:58.049904 139804644198144 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05690029263496399, loss=0.026931872591376305
I0206 14:04:30.143351 139752137176832 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.056048281490802765, loss=0.02691105380654335
I0206 14:05:02.070375 139804644198144 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.07407475262880325, loss=0.031945373862981796
I0206 14:05:33.483309 139752137176832 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05604933947324753, loss=0.02628757432103157
I0206 14:06:04.998593 139804644198144 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.05994943901896477, loss=0.025947803631424904
I0206 14:06:36.613609 139752137176832 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05275469645857811, loss=0.028958410024642944
I0206 14:07:09.030036 139804644198144 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.079877108335495, loss=0.029903531074523926
I0206 14:07:30.916172 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:09:07.850834 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:09:10.834261 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:09:13.798542 139978932307776 submission_runner.py:408] Time since start: 14222.10s, 	Step: 30970, 	{'train/accuracy': 0.9916879534721375, 'train/loss': 0.026880500838160515, 'train/mean_average_precision': 0.48313094939134127, 'validation/accuracy': 0.9869319200515747, 'validation/loss': 0.04390327259898186, 'validation/mean_average_precision': 0.27804992863153566, 'validation/num_examples': 43793, 'test/accuracy': 0.9861034750938416, 'test/loss': 0.046760544180870056, 'test/mean_average_precision': 0.26753037311622335, 'test/num_examples': 43793, 'score': 9858.047510623932, 'total_duration': 14222.101789474487, 'accumulated_submission_time': 9858.047510623932, 'accumulated_eval_time': 4362.0056364536285, 'accumulated_logging_time': 1.2251360416412354}
I0206 14:09:13.819893 139752176375552 logging_writer.py:48] [30970] accumulated_eval_time=4362.005636, accumulated_logging_time=1.225136, accumulated_submission_time=9858.047511, global_step=30970, preemption_count=0, score=9858.047511, test/accuracy=0.986103, test/loss=0.046761, test/mean_average_precision=0.267530, test/num_examples=43793, total_duration=14222.101789, train/accuracy=0.991688, train/loss=0.026881, train/mean_average_precision=0.483131, validation/accuracy=0.986932, validation/loss=0.043903, validation/mean_average_precision=0.278050, validation/num_examples=43793
I0206 14:09:23.520134 139770185602816 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.05890283361077309, loss=0.029076622799038887
I0206 14:09:54.917436 139752176375552 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06577424705028534, loss=0.02741180919110775
I0206 14:10:26.379922 139770185602816 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06651479005813599, loss=0.02923544868826866
I0206 14:10:57.894628 139752176375552 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.053775884211063385, loss=0.027516819536685944
I0206 14:11:29.474211 139770185602816 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.054924726486206055, loss=0.02622065320611
I0206 14:12:00.593858 139752176375552 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04650511220097542, loss=0.02347485162317753
I0206 14:12:32.414883 139770185602816 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06341829895973206, loss=0.03242599964141846
I0206 14:13:04.021638 139752176375552 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.056159473955631256, loss=0.027299603447318077
I0206 14:13:14.018483 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:14:52.683536 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:14:55.747360 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:14:58.706198 139978932307776 submission_runner.py:408] Time since start: 14567.01s, 	Step: 31733, 	{'train/accuracy': 0.9920236468315125, 'train/loss': 0.025705374777317047, 'train/mean_average_precision': 0.5056896381051835, 'validation/accuracy': 0.9871158003807068, 'validation/loss': 0.043632812798023224, 'validation/mean_average_precision': 0.279755651589898, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.04668949544429779, 'test/mean_average_precision': 0.26906986467590865, 'test/num_examples': 43793, 'score': 10098.214116334915, 'total_duration': 14567.009452581406, 'accumulated_submission_time': 10098.214116334915, 'accumulated_eval_time': 4466.693303823471, 'accumulated_logging_time': 1.2588024139404297}
I0206 14:14:58.728675 139733347358464 logging_writer.py:48] [31733] accumulated_eval_time=4466.693304, accumulated_logging_time=1.258802, accumulated_submission_time=10098.214116, global_step=31733, preemption_count=0, score=10098.214116, test/accuracy=0.986184, test/loss=0.046689, test/mean_average_precision=0.269070, test/num_examples=43793, total_duration=14567.009453, train/accuracy=0.992024, train/loss=0.025705, train/mean_average_precision=0.505690, validation/accuracy=0.987116, validation/loss=0.043633, validation/mean_average_precision=0.279756, validation/num_examples=43793
I0206 14:15:20.319357 139752137176832 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06759822368621826, loss=0.030382035300135612
I0206 14:15:52.059302 139733347358464 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.05627845227718353, loss=0.029515113681554794
I0206 14:16:23.478398 139752137176832 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.07460211962461472, loss=0.030072102323174477
I0206 14:16:55.059433 139733347358464 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05944977328181267, loss=0.02546081505715847
I0206 14:17:26.564119 139752137176832 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0648031011223793, loss=0.03127850219607353
I0206 14:17:58.534283 139733347358464 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.046833109110593796, loss=0.02710230089724064
I0206 14:18:30.752970 139752137176832 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.06567700207233429, loss=0.02861207164824009
I0206 14:18:58.904524 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:20:34.416181 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:20:37.458566 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:20:40.433998 139978932307776 submission_runner.py:408] Time since start: 14908.74s, 	Step: 32490, 	{'train/accuracy': 0.9920137524604797, 'train/loss': 0.02571156434714794, 'train/mean_average_precision': 0.5341408908884115, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04462163522839546, 'validation/mean_average_precision': 0.2860193035317245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861034750938416, 'test/loss': 0.04776688665151596, 'test/mean_average_precision': 0.2708168201569263, 'test/num_examples': 43793, 'score': 10338.35930466652, 'total_duration': 14908.73725104332, 'accumulated_submission_time': 10338.35930466652, 'accumulated_eval_time': 4568.222731590271, 'accumulated_logging_time': 1.2922701835632324}
I0206 14:20:40.456020 139752176375552 logging_writer.py:48] [32490] accumulated_eval_time=4568.222732, accumulated_logging_time=1.292270, accumulated_submission_time=10338.359305, global_step=32490, preemption_count=0, score=10338.359305, test/accuracy=0.986103, test/loss=0.047767, test/mean_average_precision=0.270817, test/num_examples=43793, total_duration=14908.737251, train/accuracy=0.992014, train/loss=0.025712, train/mean_average_precision=0.534141, validation/accuracy=0.987014, validation/loss=0.044622, validation/mean_average_precision=0.286019, validation/num_examples=43793
I0206 14:20:43.895591 139770185602816 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06395547091960907, loss=0.025257553905248642
I0206 14:21:15.433926 139752176375552 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06402880698442459, loss=0.02884761057794094
I0206 14:21:47.017050 139770185602816 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.060738228261470795, loss=0.028953654691576958
I0206 14:22:18.599755 139752176375552 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.056491147726774216, loss=0.03032783977687359
I0206 14:22:50.183464 139770185602816 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05466935411095619, loss=0.0316116027534008
I0206 14:23:21.983750 139752176375552 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05370170995593071, loss=0.02675280161201954
I0206 14:23:53.307931 139770185602816 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05710117518901825, loss=0.02890089713037014
I0206 14:24:24.779886 139752176375552 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.05859040468931198, loss=0.026082467287778854
I0206 14:24:40.446521 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:26:20.589936 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:26:23.647519 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:26:26.655912 139978932307776 submission_runner.py:408] Time since start: 15254.96s, 	Step: 33250, 	{'train/accuracy': 0.9923450946807861, 'train/loss': 0.024707453325390816, 'train/mean_average_precision': 0.542667942251531, 'validation/accuracy': 0.9870890378952026, 'validation/loss': 0.0436905212700367, 'validation/mean_average_precision': 0.2875374195248217, 'validation/num_examples': 43793, 'test/accuracy': 0.9861953258514404, 'test/loss': 0.046789951622486115, 'test/mean_average_precision': 0.2757306207445348, 'test/num_examples': 43793, 'score': 10578.31761264801, 'total_duration': 15254.959168434143, 'accumulated_submission_time': 10578.31761264801, 'accumulated_eval_time': 4674.432077407837, 'accumulated_logging_time': 1.3265063762664795}
I0206 14:26:26.677618 139752137176832 logging_writer.py:48] [33250] accumulated_eval_time=4674.432077, accumulated_logging_time=1.326506, accumulated_submission_time=10578.317613, global_step=33250, preemption_count=0, score=10578.317613, test/accuracy=0.986195, test/loss=0.046790, test/mean_average_precision=0.275731, test/num_examples=43793, total_duration=15254.959168, train/accuracy=0.992345, train/loss=0.024707, train/mean_average_precision=0.542668, validation/accuracy=0.987089, validation/loss=0.043691, validation/mean_average_precision=0.287537, validation/num_examples=43793
I0206 14:26:43.040432 139804644198144 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05738304555416107, loss=0.029794689267873764
I0206 14:27:15.029389 139752137176832 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.08027797192335129, loss=0.0318080373108387
I0206 14:27:46.770002 139804644198144 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.059064801782369614, loss=0.029589707031846046
I0206 14:28:18.662553 139752137176832 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05582413077354431, loss=0.026195909827947617
I0206 14:28:50.401144 139804644198144 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.062422219663858414, loss=0.030442193150520325
I0206 14:29:22.163527 139752137176832 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05903824418783188, loss=0.0231622364372015
I0206 14:29:53.839451 139804644198144 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.07107498496770859, loss=0.02902650646865368
I0206 14:30:25.626212 139752137176832 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.07568530738353729, loss=0.030011923983693123
I0206 14:30:26.868402 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:32:05.368081 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:32:08.391898 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:32:11.411705 139978932307776 submission_runner.py:408] Time since start: 15599.71s, 	Step: 34005, 	{'train/accuracy': 0.9921961426734924, 'train/loss': 0.025209462270140648, 'train/mean_average_precision': 0.5336781380529731, 'validation/accuracy': 0.9870187640190125, 'validation/loss': 0.04386290907859802, 'validation/mean_average_precision': 0.2830934933826157, 'validation/num_examples': 43793, 'test/accuracy': 0.9861144423484802, 'test/loss': 0.04689648747444153, 'test/mean_average_precision': 0.2706710636528843, 'test/num_examples': 43793, 'score': 10818.476083278656, 'total_duration': 15599.71495604515, 'accumulated_submission_time': 10818.476083278656, 'accumulated_eval_time': 4778.975328207016, 'accumulated_logging_time': 1.3605234622955322}
I0206 14:32:11.434311 139733347358464 logging_writer.py:48] [34005] accumulated_eval_time=4778.975328, accumulated_logging_time=1.360523, accumulated_submission_time=10818.476083, global_step=34005, preemption_count=0, score=10818.476083, test/accuracy=0.986114, test/loss=0.046896, test/mean_average_precision=0.270671, test/num_examples=43793, total_duration=15599.714956, train/accuracy=0.992196, train/loss=0.025209, train/mean_average_precision=0.533678, validation/accuracy=0.987019, validation/loss=0.043863, validation/mean_average_precision=0.283093, validation/num_examples=43793
I0206 14:32:42.825105 139770185602816 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06442111730575562, loss=0.02807549573481083
I0206 14:33:14.955701 139733347358464 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06552660465240479, loss=0.0249329786747694
I0206 14:33:47.098156 139770185602816 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06783929467201233, loss=0.02859659120440483
I0206 14:34:19.403771 139733347358464 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.07064793258905411, loss=0.031281013041734695
I0206 14:34:51.440275 139770185602816 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06397150456905365, loss=0.028909876942634583
I0206 14:35:23.425311 139733347358464 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05990134924650192, loss=0.030688751488924026
I0206 14:35:55.083104 139770185602816 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.059673938900232315, loss=0.029069967567920685
I0206 14:36:11.604151 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:37:48.086171 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:37:51.111684 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:37:54.099938 139978932307776 submission_runner.py:408] Time since start: 15942.40s, 	Step: 34753, 	{'train/accuracy': 0.9918820858001709, 'train/loss': 0.02606547810137272, 'train/mean_average_precision': 0.4910209539627918, 'validation/accuracy': 0.9870484471321106, 'validation/loss': 0.04438941925764084, 'validation/mean_average_precision': 0.28277370317285005, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.04756646230816841, 'test/mean_average_precision': 0.26872718959765507, 'test/num_examples': 43793, 'score': 11058.613788604736, 'total_duration': 15942.403191328049, 'accumulated_submission_time': 11058.613788604736, 'accumulated_eval_time': 4881.47107052803, 'accumulated_logging_time': 1.3952465057373047}
I0206 14:37:54.122274 139752137176832 logging_writer.py:48] [34753] accumulated_eval_time=4881.471071, accumulated_logging_time=1.395247, accumulated_submission_time=11058.613789, global_step=34753, preemption_count=0, score=11058.613789, test/accuracy=0.986145, test/loss=0.047566, test/mean_average_precision=0.268727, test/num_examples=43793, total_duration=15942.403191, train/accuracy=0.991882, train/loss=0.026065, train/mean_average_precision=0.491021, validation/accuracy=0.987048, validation/loss=0.044389, validation/mean_average_precision=0.282774, validation/num_examples=43793
I0206 14:38:09.464658 139752176375552 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06261248141527176, loss=0.028687158599495888
I0206 14:38:41.375213 139752137176832 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.05636944994330406, loss=0.027966277673840523
I0206 14:39:13.190543 139752176375552 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06099961698055267, loss=0.028455354273319244
I0206 14:39:45.011189 139752137176832 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06308750808238983, loss=0.030328458175063133
I0206 14:40:16.984774 139752176375552 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05857781693339348, loss=0.02677609771490097
I0206 14:40:49.089990 139752137176832 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05588926374912262, loss=0.02682650461792946
I0206 14:41:21.057407 139752176375552 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.06337357312440872, loss=0.027043061330914497
I0206 14:41:52.761492 139752137176832 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06201452761888504, loss=0.02848089672625065
I0206 14:41:54.326991 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:43:34.034137 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:43:37.089395 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:43:40.168220 139978932307776 submission_runner.py:408] Time since start: 16288.47s, 	Step: 35506, 	{'train/accuracy': 0.991861879825592, 'train/loss': 0.02618979476392269, 'train/mean_average_precision': 0.5024073601537873, 'validation/accuracy': 0.9869396090507507, 'validation/loss': 0.044023074209690094, 'validation/mean_average_precision': 0.2845682260674991, 'validation/num_examples': 43793, 'test/accuracy': 0.9860979914665222, 'test/loss': 0.046877551823854446, 'test/mean_average_precision': 0.2738745127717128, 'test/num_examples': 43793, 'score': 11298.787636518478, 'total_duration': 16288.471473455429, 'accumulated_submission_time': 11298.787636518478, 'accumulated_eval_time': 4987.312251329422, 'accumulated_logging_time': 1.4282660484313965}
I0206 14:43:40.191056 139770185602816 logging_writer.py:48] [35506] accumulated_eval_time=4987.312251, accumulated_logging_time=1.428266, accumulated_submission_time=11298.787637, global_step=35506, preemption_count=0, score=11298.787637, test/accuracy=0.986098, test/loss=0.046878, test/mean_average_precision=0.273875, test/num_examples=43793, total_duration=16288.471473, train/accuracy=0.991862, train/loss=0.026190, train/mean_average_precision=0.502407, validation/accuracy=0.986940, validation/loss=0.044023, validation/mean_average_precision=0.284568, validation/num_examples=43793
I0206 14:44:10.232663 139804644198144 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05529018118977547, loss=0.0265433881431818
I0206 14:44:41.838135 139770185602816 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06598728150129318, loss=0.028936931863427162
I0206 14:45:13.696558 139804644198144 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06331518292427063, loss=0.030606186017394066
I0206 14:45:45.305487 139770185602816 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.08056741952896118, loss=0.029077688232064247
I0206 14:46:17.416157 139804644198144 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06727331131696701, loss=0.02982351928949356
I0206 14:46:49.197137 139770185602816 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06286865472793579, loss=0.027041975408792496
I0206 14:47:20.515315 139804644198144 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07010061293840408, loss=0.03021630272269249
I0206 14:47:40.237697 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:49:18.724733 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:49:21.752391 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:49:24.851782 139978932307776 submission_runner.py:408] Time since start: 16633.16s, 	Step: 36263, 	{'train/accuracy': 0.9919314980506897, 'train/loss': 0.025890670716762543, 'train/mean_average_precision': 0.5145215114867434, 'validation/accuracy': 0.9870853424072266, 'validation/loss': 0.04400112107396126, 'validation/mean_average_precision': 0.2819405539208721, 'validation/num_examples': 43793, 'test/accuracy': 0.9861472845077515, 'test/loss': 0.04721315577626228, 'test/mean_average_precision': 0.2717603088414615, 'test/num_examples': 43793, 'score': 11538.803719043732, 'total_duration': 16633.155037403107, 'accumulated_submission_time': 11538.803719043732, 'accumulated_eval_time': 5091.926290988922, 'accumulated_logging_time': 1.4618382453918457}
I0206 14:49:24.874777 139733347358464 logging_writer.py:48] [36263] accumulated_eval_time=5091.926291, accumulated_logging_time=1.461838, accumulated_submission_time=11538.803719, global_step=36263, preemption_count=0, score=11538.803719, test/accuracy=0.986147, test/loss=0.047213, test/mean_average_precision=0.271760, test/num_examples=43793, total_duration=16633.155037, train/accuracy=0.991931, train/loss=0.025891, train/mean_average_precision=0.514522, validation/accuracy=0.987085, validation/loss=0.044001, validation/mean_average_precision=0.281941, validation/num_examples=43793
I0206 14:49:36.917584 139752137176832 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.05581727251410484, loss=0.02707151137292385
I0206 14:50:09.086487 139733347358464 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.07615253329277039, loss=0.02894161082804203
I0206 14:50:40.708943 139752137176832 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06011134386062622, loss=0.029603105038404465
I0206 14:51:12.328780 139733347358464 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.07111852616071701, loss=0.028013790026307106
I0206 14:51:44.200304 139752137176832 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.07059385627508163, loss=0.02630489692091942
I0206 14:52:15.979144 139733347358464 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.07344301044940948, loss=0.03046974539756775
I0206 14:52:47.638571 139752137176832 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06077494099736214, loss=0.029646268114447594
I0206 14:53:19.133391 139733347358464 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.058889854699373245, loss=0.027437398210167885
I0206 14:53:25.158949 139978932307776 spec.py:321] Evaluating on the training split.
I0206 14:55:02.088250 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 14:55:05.102400 139978932307776 spec.py:349] Evaluating on the test split.
I0206 14:55:08.047641 139978932307776 submission_runner.py:408] Time since start: 16976.35s, 	Step: 37020, 	{'train/accuracy': 0.9920294880867004, 'train/loss': 0.025662560015916824, 'train/mean_average_precision': 0.525321302551629, 'validation/accuracy': 0.9870873689651489, 'validation/loss': 0.04402170330286026, 'validation/mean_average_precision': 0.2841475760462093, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.047062020748853683, 'test/mean_average_precision': 0.2745681328480498, 'test/num_examples': 43793, 'score': 11779.05526304245, 'total_duration': 16976.350893974304, 'accumulated_submission_time': 11779.05526304245, 'accumulated_eval_time': 5194.814932107925, 'accumulated_logging_time': 1.4971117973327637}
I0206 14:55:08.071175 139752176375552 logging_writer.py:48] [37020] accumulated_eval_time=5194.814932, accumulated_logging_time=1.497112, accumulated_submission_time=11779.055263, global_step=37020, preemption_count=0, score=11779.055263, test/accuracy=0.986192, test/loss=0.047062, test/mean_average_precision=0.274568, test/num_examples=43793, total_duration=16976.350894, train/accuracy=0.992029, train/loss=0.025663, train/mean_average_precision=0.525321, validation/accuracy=0.987087, validation/loss=0.044022, validation/mean_average_precision=0.284148, validation/num_examples=43793
I0206 14:55:33.952476 139770185602816 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06695520132780075, loss=0.02640039287507534
I0206 14:56:05.309602 139752176375552 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.05712788552045822, loss=0.025760361924767494
I0206 14:56:36.839158 139770185602816 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06344159692525864, loss=0.03003285452723503
I0206 14:57:08.293837 139752176375552 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06162748858332634, loss=0.02922823466360569
I0206 14:57:40.057866 139770185602816 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05639425292611122, loss=0.024506671354174614
I0206 14:58:11.668389 139752176375552 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06635643541812897, loss=0.030295874923467636
I0206 14:58:43.211390 139770185602816 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07695384323596954, loss=0.027803581207990646
I0206 14:59:08.352648 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:00:47.925938 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:00:51.009109 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:00:53.956206 139978932307776 submission_runner.py:408] Time since start: 17322.26s, 	Step: 37780, 	{'train/accuracy': 0.992198646068573, 'train/loss': 0.025036245584487915, 'train/mean_average_precision': 0.5411420089665825, 'validation/accuracy': 0.9870870113372803, 'validation/loss': 0.04411032795906067, 'validation/mean_average_precision': 0.2871141885306458, 'validation/num_examples': 43793, 'test/accuracy': 0.9862509369850159, 'test/loss': 0.047302741557359695, 'test/mean_average_precision': 0.2781049767731259, 'test/num_examples': 43793, 'score': 12018.916049003601, 'total_duration': 17322.259452581406, 'accumulated_submission_time': 12018.916049003601, 'accumulated_eval_time': 5300.418438196182, 'accumulated_logging_time': 1.9214563369750977}
I0206 15:00:53.979797 139733347358464 logging_writer.py:48] [37780] accumulated_eval_time=5300.418438, accumulated_logging_time=1.921456, accumulated_submission_time=12018.916049, global_step=37780, preemption_count=0, score=12018.916049, test/accuracy=0.986251, test/loss=0.047303, test/mean_average_precision=0.278105, test/num_examples=43793, total_duration=17322.259453, train/accuracy=0.992199, train/loss=0.025036, train/mean_average_precision=0.541142, validation/accuracy=0.987087, validation/loss=0.044110, validation/mean_average_precision=0.287114, validation/num_examples=43793
I0206 15:01:00.803521 139752137176832 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06261535733938217, loss=0.028393840417265892
I0206 15:01:32.089258 139733347358464 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.0751933827996254, loss=0.0287320613861084
I0206 15:02:03.528578 139752137176832 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0661718100309372, loss=0.026716429740190506
I0206 15:02:35.024485 139733347358464 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06525525450706482, loss=0.029039841145277023
I0206 15:03:06.442912 139752137176832 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06635869294404984, loss=0.027393095195293427
I0206 15:03:38.544986 139733347358464 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.0703597292304039, loss=0.028199421241879463
I0206 15:04:10.490421 139752137176832 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.07125832140445709, loss=0.027747083455324173
I0206 15:04:42.155118 139733347358464 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07230716198682785, loss=0.02975372038781643
I0206 15:04:54.213408 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:06:32.104307 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:06:35.160980 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:06:38.137222 139978932307776 submission_runner.py:408] Time since start: 17666.44s, 	Step: 38539, 	{'train/accuracy': 0.9921022057533264, 'train/loss': 0.025213586166501045, 'train/mean_average_precision': 0.5186278352257488, 'validation/accuracy': 0.9870630502700806, 'validation/loss': 0.04458058997988701, 'validation/mean_average_precision': 0.28446345557888714, 'validation/num_examples': 43793, 'test/accuracy': 0.9861801266670227, 'test/loss': 0.04781052842736244, 'test/mean_average_precision': 0.2695032759656423, 'test/num_examples': 43793, 'score': 12259.118973731995, 'total_duration': 17666.440479516983, 'accumulated_submission_time': 12259.118973731995, 'accumulated_eval_time': 5404.342231273651, 'accumulated_logging_time': 1.9558711051940918}
I0206 15:06:38.160103 139770185602816 logging_writer.py:48] [38539] accumulated_eval_time=5404.342231, accumulated_logging_time=1.955871, accumulated_submission_time=12259.118974, global_step=38539, preemption_count=0, score=12259.118974, test/accuracy=0.986180, test/loss=0.047811, test/mean_average_precision=0.269503, test/num_examples=43793, total_duration=17666.440480, train/accuracy=0.992102, train/loss=0.025214, train/mean_average_precision=0.518628, validation/accuracy=0.987063, validation/loss=0.044581, validation/mean_average_precision=0.284463, validation/num_examples=43793
I0206 15:06:58.082591 139804644198144 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06567786633968353, loss=0.0292825885117054
I0206 15:07:30.129372 139770185602816 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07404324412345886, loss=0.025935865938663483
I0206 15:08:01.963705 139804644198144 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.07686663419008255, loss=0.027782395482063293
I0206 15:08:33.849341 139770185602816 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06394878774881363, loss=0.026083562523126602
I0206 15:09:05.641959 139804644198144 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.06315171718597412, loss=0.028338901698589325
I0206 15:09:37.394905 139770185602816 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.06464618444442749, loss=0.026191741228103638
I0206 15:10:08.984839 139804644198144 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.061667218804359436, loss=0.02611367404460907
I0206 15:10:38.265950 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:12:12.954208 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:12:16.055943 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:12:19.151082 139978932307776 submission_runner.py:408] Time since start: 18007.45s, 	Step: 39293, 	{'train/accuracy': 0.9923055171966553, 'train/loss': 0.02453293837606907, 'train/mean_average_precision': 0.5386582770107501, 'validation/accuracy': 0.9871178269386292, 'validation/loss': 0.04436669871211052, 'validation/mean_average_precision': 0.28731616174521757, 'validation/num_examples': 43793, 'test/accuracy': 0.9863056540489197, 'test/loss': 0.04755987226963043, 'test/mean_average_precision': 0.2716499638558299, 'test/num_examples': 43793, 'score': 12499.194529294968, 'total_duration': 18007.454338550568, 'accumulated_submission_time': 12499.194529294968, 'accumulated_eval_time': 5505.22732257843, 'accumulated_logging_time': 1.9891891479492188}
I0206 15:12:19.173920 139733347358464 logging_writer.py:48] [39293] accumulated_eval_time=5505.227323, accumulated_logging_time=1.989189, accumulated_submission_time=12499.194529, global_step=39293, preemption_count=0, score=12499.194529, test/accuracy=0.986306, test/loss=0.047560, test/mean_average_precision=0.271650, test/num_examples=43793, total_duration=18007.454339, train/accuracy=0.992306, train/loss=0.024533, train/mean_average_precision=0.538658, validation/accuracy=0.987118, validation/loss=0.044367, validation/mean_average_precision=0.287316, validation/num_examples=43793
I0206 15:12:21.726047 139752137176832 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.060057297348976135, loss=0.0259071234613657
I0206 15:12:53.599639 139733347358464 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06450437754392624, loss=0.029311396181583405
I0206 15:13:25.746608 139752137176832 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07194827497005463, loss=0.0285445936024189
I0206 15:13:57.463322 139733347358464 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06593411415815353, loss=0.026599841192364693
I0206 15:14:29.489721 139752137176832 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.0633898600935936, loss=0.025858862325549126
I0206 15:15:01.592080 139733347358464 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.08640341460704803, loss=0.030751852318644524
I0206 15:15:33.461652 139752137176832 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07594909518957138, loss=0.026556437835097313
I0206 15:16:05.339197 139733347358464 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07496462017297745, loss=0.030708948150277138
I0206 15:16:19.179998 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:17:54.746399 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:17:57.882298 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:18:01.037880 139978932307776 submission_runner.py:408] Time since start: 18349.34s, 	Step: 40044, 	{'train/accuracy': 0.9925379157066345, 'train/loss': 0.02376587875187397, 'train/mean_average_precision': 0.5654487645215553, 'validation/accuracy': 0.9872254133224487, 'validation/loss': 0.044390469789505005, 'validation/mean_average_precision': 0.2868401826016203, 'validation/num_examples': 43793, 'test/accuracy': 0.9863005876541138, 'test/loss': 0.04770269617438316, 'test/mean_average_precision': 0.27590861828424906, 'test/num_examples': 43793, 'score': 12739.169730901718, 'total_duration': 18349.34113383293, 'accumulated_submission_time': 12739.169730901718, 'accumulated_eval_time': 5607.085157632828, 'accumulated_logging_time': 2.0231266021728516}
I0206 15:18:01.061025 139770185602816 logging_writer.py:48] [40044] accumulated_eval_time=5607.085158, accumulated_logging_time=2.023127, accumulated_submission_time=12739.169731, global_step=40044, preemption_count=0, score=12739.169731, test/accuracy=0.986301, test/loss=0.047703, test/mean_average_precision=0.275909, test/num_examples=43793, total_duration=18349.341134, train/accuracy=0.992538, train/loss=0.023766, train/mean_average_precision=0.565449, validation/accuracy=0.987225, validation/loss=0.044390, validation/mean_average_precision=0.286840, validation/num_examples=43793
I0206 15:18:19.398293 139804644198144 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.055569496005773544, loss=0.025636302307248116
I0206 15:18:51.265469 139770185602816 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06676899641752243, loss=0.027466220781207085
I0206 15:19:22.698986 139804644198144 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.05992508307099342, loss=0.02546636573970318
I0206 15:19:54.356127 139770185602816 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.062461066991090775, loss=0.026507042348384857
I0206 15:20:25.689251 139804644198144 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06936463713645935, loss=0.025231169536709785
I0206 15:20:57.214649 139770185602816 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.07945073395967484, loss=0.025134997442364693
I0206 15:21:28.969590 139804644198144 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.07495676726102829, loss=0.025915000587701797
I0206 15:22:00.289008 139770185602816 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.0821465328335762, loss=0.025865541771054268
I0206 15:22:01.236448 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:23:36.305984 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:23:39.498754 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:23:42.586600 139978932307776 submission_runner.py:408] Time since start: 18690.89s, 	Step: 40804, 	{'train/accuracy': 0.9927852749824524, 'train/loss': 0.023151222616434097, 'train/mean_average_precision': 0.5775893719350957, 'validation/accuracy': 0.9870390892028809, 'validation/loss': 0.044513948261737823, 'validation/mean_average_precision': 0.2907892343854625, 'validation/num_examples': 43793, 'test/accuracy': 0.9861422181129456, 'test/loss': 0.04752732813358307, 'test/mean_average_precision': 0.276028532270375, 'test/num_examples': 43793, 'score': 12979.314190387726, 'total_duration': 18690.889854192734, 'accumulated_submission_time': 12979.314190387726, 'accumulated_eval_time': 5708.435259819031, 'accumulated_logging_time': 2.0572030544281006}
I0206 15:23:42.609852 139752137176832 logging_writer.py:48] [40804] accumulated_eval_time=5708.435260, accumulated_logging_time=2.057203, accumulated_submission_time=12979.314190, global_step=40804, preemption_count=0, score=12979.314190, test/accuracy=0.986142, test/loss=0.047527, test/mean_average_precision=0.276029, test/num_examples=43793, total_duration=18690.889854, train/accuracy=0.992785, train/loss=0.023151, train/mean_average_precision=0.577589, validation/accuracy=0.987039, validation/loss=0.044514, validation/mean_average_precision=0.290789, validation/num_examples=43793
I0206 15:24:13.665593 139752176375552 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.13037993013858795, loss=0.027642162516713142
I0206 15:24:44.903950 139752137176832 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.08015654236078262, loss=0.028803827241063118
I0206 15:25:16.564594 139752176375552 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.08000656962394714, loss=0.028929971158504486
I0206 15:25:47.935758 139752137176832 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07662941515445709, loss=0.02636074647307396
I0206 15:26:19.463837 139752176375552 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.05966552346944809, loss=0.0243628341704607
I0206 15:26:50.966686 139752137176832 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.06702768057584763, loss=0.024779530242085457
I0206 15:27:22.436223 139752176375552 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07893630117177963, loss=0.031027622520923615
I0206 15:27:42.827322 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:29:19.411280 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:29:22.529670 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:29:25.599563 139978932307776 submission_runner.py:408] Time since start: 19033.90s, 	Step: 41566, 	{'train/accuracy': 0.9926801323890686, 'train/loss': 0.023235918954014778, 'train/mean_average_precision': 0.5827686459930178, 'validation/accuracy': 0.9871527552604675, 'validation/loss': 0.044966548681259155, 'validation/mean_average_precision': 0.2888831560468628, 'validation/num_examples': 43793, 'test/accuracy': 0.9863119721412659, 'test/loss': 0.047933343797922134, 'test/mean_average_precision': 0.27867468106709653, 'test/num_examples': 43793, 'score': 13219.500958919525, 'total_duration': 19033.902818202972, 'accumulated_submission_time': 13219.500958919525, 'accumulated_eval_time': 5811.207458734512, 'accumulated_logging_time': 2.0911972522735596}
I0206 15:29:25.623703 139770185602816 logging_writer.py:48] [41566] accumulated_eval_time=5811.207459, accumulated_logging_time=2.091197, accumulated_submission_time=13219.500959, global_step=41566, preemption_count=0, score=13219.500959, test/accuracy=0.986312, test/loss=0.047933, test/mean_average_precision=0.278675, test/num_examples=43793, total_duration=19033.902818, train/accuracy=0.992680, train/loss=0.023236, train/mean_average_precision=0.582769, validation/accuracy=0.987153, validation/loss=0.044967, validation/mean_average_precision=0.288883, validation/num_examples=43793
I0206 15:29:36.694844 139804644198144 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.07575979828834534, loss=0.02810489945113659
I0206 15:30:08.103504 139770185602816 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.0666273981332779, loss=0.02541021816432476
I0206 15:30:39.863793 139804644198144 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.0780976340174675, loss=0.024840960279107094
I0206 15:31:11.304688 139770185602816 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06799410283565521, loss=0.024257350713014603
I0206 15:31:42.799309 139804644198144 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.061662234365940094, loss=0.025242747738957405
I0206 15:32:14.307400 139770185602816 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07257700711488724, loss=0.02373427525162697
I0206 15:32:45.961779 139804644198144 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07786504924297333, loss=0.027130186557769775
I0206 15:33:17.652944 139770185602816 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.08961629867553711, loss=0.02812916785478592
I0206 15:33:25.621471 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:35:04.261414 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:35:07.407538 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:35:10.480054 139978932307776 submission_runner.py:408] Time since start: 19378.78s, 	Step: 42326, 	{'train/accuracy': 0.9926807880401611, 'train/loss': 0.023445546627044678, 'train/mean_average_precision': 0.5660779500427957, 'validation/accuracy': 0.9870723485946655, 'validation/loss': 0.044642020016908646, 'validation/mean_average_precision': 0.2907271142531775, 'validation/num_examples': 43793, 'test/accuracy': 0.9861717224121094, 'test/loss': 0.04774622619152069, 'test/mean_average_precision': 0.2741958646714575, 'test/num_examples': 43793, 'score': 13459.468119859695, 'total_duration': 19378.783309936523, 'accumulated_submission_time': 13459.468119859695, 'accumulated_eval_time': 5916.065999746323, 'accumulated_logging_time': 2.1258957386016846}
I0206 15:35:10.528450 139733347358464 logging_writer.py:48] [42326] accumulated_eval_time=5916.066000, accumulated_logging_time=2.125896, accumulated_submission_time=13459.468120, global_step=42326, preemption_count=0, score=13459.468120, test/accuracy=0.986172, test/loss=0.047746, test/mean_average_precision=0.274196, test/num_examples=43793, total_duration=19378.783310, train/accuracy=0.992681, train/loss=0.023446, train/mean_average_precision=0.566078, validation/accuracy=0.987072, validation/loss=0.044642, validation/mean_average_precision=0.290727, validation/num_examples=43793
I0206 15:35:34.835318 139752137176832 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08231069892644882, loss=0.02720944583415985
I0206 15:36:07.038696 139733347358464 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07209789007902145, loss=0.025419417768716812
I0206 15:36:38.783388 139752137176832 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.07512907683849335, loss=0.026680486276745796
I0206 15:37:10.777845 139733347358464 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.06942877918481827, loss=0.02609246037900448
I0206 15:37:43.108233 139752137176832 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07113482803106308, loss=0.0265825092792511
I0206 15:38:14.902259 139733347358464 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08492759615182877, loss=0.02625853195786476
I0206 15:38:46.654750 139752137176832 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.07361955940723419, loss=0.02527555078268051
I0206 15:39:10.535188 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:40:44.760306 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:40:47.897758 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:40:51.083457 139978932307776 submission_runner.py:408] Time since start: 19719.39s, 	Step: 43076, 	{'train/accuracy': 0.992499589920044, 'train/loss': 0.02394149824976921, 'train/mean_average_precision': 0.5477740813822252, 'validation/accuracy': 0.9870922565460205, 'validation/loss': 0.04479354992508888, 'validation/mean_average_precision': 0.28924994940451176, 'validation/num_examples': 43793, 'test/accuracy': 0.9863384962081909, 'test/loss': 0.04778079316020012, 'test/mean_average_precision': 0.27807957057728816, 'test/num_examples': 43793, 'score': 13699.443043470383, 'total_duration': 19719.386711359024, 'accumulated_submission_time': 13699.443043470383, 'accumulated_eval_time': 6016.614222764969, 'accumulated_logging_time': 2.186445474624634}
I0206 15:40:51.107112 139770185602816 logging_writer.py:48] [43076] accumulated_eval_time=6016.614223, accumulated_logging_time=2.186445, accumulated_submission_time=13699.443043, global_step=43076, preemption_count=0, score=13699.443043, test/accuracy=0.986338, test/loss=0.047781, test/mean_average_precision=0.278080, test/num_examples=43793, total_duration=19719.386711, train/accuracy=0.992500, train/loss=0.023941, train/mean_average_precision=0.547774, validation/accuracy=0.987092, validation/loss=0.044794, validation/mean_average_precision=0.289250, validation/num_examples=43793
I0206 15:40:59.040064 139804644198144 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07977934181690216, loss=0.030116980895400047
I0206 15:41:31.139085 139770185602816 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07521950453519821, loss=0.02629191055893898
I0206 15:42:02.992834 139804644198144 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.06468240171670914, loss=0.025829827412962914
I0206 15:42:34.927965 139770185602816 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.07810502499341965, loss=0.028222113847732544
I0206 15:43:07.269482 139804644198144 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07313329726457596, loss=0.02584034949541092
I0206 15:43:39.276401 139770185602816 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08904542773962021, loss=0.026988936588168144
I0206 15:44:11.161724 139804644198144 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.06651719659566879, loss=0.026911141350865364
I0206 15:44:43.172505 139770185602816 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08305807411670685, loss=0.02575148269534111
I0206 15:44:51.112242 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:46:28.698920 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:46:31.798543 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:46:34.862154 139978932307776 submission_runner.py:408] Time since start: 20063.17s, 	Step: 43826, 	{'train/accuracy': 0.9925187826156616, 'train/loss': 0.023853471502661705, 'train/mean_average_precision': 0.5623201160024388, 'validation/accuracy': 0.9870967268943787, 'validation/loss': 0.04477299377322197, 'validation/mean_average_precision': 0.29060484401217707, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.047920502722263336, 'test/mean_average_precision': 0.2761384758753186, 'test/num_examples': 43793, 'score': 13939.41558265686, 'total_duration': 20063.16539645195, 'accumulated_submission_time': 13939.41558265686, 'accumulated_eval_time': 6120.3640768527985, 'accumulated_logging_time': 2.2230827808380127}
I0206 15:46:34.886364 139733347358464 logging_writer.py:48] [43826] accumulated_eval_time=6120.364077, accumulated_logging_time=2.223083, accumulated_submission_time=13939.415583, global_step=43826, preemption_count=0, score=13939.415583, test/accuracy=0.986235, test/loss=0.047921, test/mean_average_precision=0.276138, test/num_examples=43793, total_duration=20063.165396, train/accuracy=0.992519, train/loss=0.023853, train/mean_average_precision=0.562320, validation/accuracy=0.987097, validation/loss=0.044773, validation/mean_average_precision=0.290605, validation/num_examples=43793
I0206 15:46:59.614074 139752176375552 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.06565801799297333, loss=0.02500176429748535
I0206 15:47:31.723408 139733347358464 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.07152748852968216, loss=0.02505381405353546
I0206 15:48:04.117140 139752176375552 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.08757191896438599, loss=0.026471102610230446
I0206 15:48:35.980230 139733347358464 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.0798506811261177, loss=0.027515189722180367
I0206 15:49:07.688882 139752176375552 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09033820033073425, loss=0.02770598791539669
I0206 15:49:39.842385 139733347358464 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.07765311002731323, loss=0.022163107991218567
I0206 15:50:11.818276 139752176375552 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07868833839893341, loss=0.025329360738396645
I0206 15:50:35.004843 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:52:12.409133 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:52:15.486482 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:52:18.457319 139978932307776 submission_runner.py:408] Time since start: 20406.76s, 	Step: 44573, 	{'train/accuracy': 0.9926663041114807, 'train/loss': 0.023493686690926552, 'train/mean_average_precision': 0.5641044492073488, 'validation/accuracy': 0.9869808554649353, 'validation/loss': 0.04480111226439476, 'validation/mean_average_precision': 0.28610250989785263, 'validation/num_examples': 43793, 'test/accuracy': 0.986177384853363, 'test/loss': 0.04789437726140022, 'test/mean_average_precision': 0.2796849265893141, 'test/num_examples': 43793, 'score': 14179.501737833023, 'total_duration': 20406.76056933403, 'accumulated_submission_time': 14179.501737833023, 'accumulated_eval_time': 6223.816519021988, 'accumulated_logging_time': 2.2597038745880127}
I0206 15:52:18.481870 139752137176832 logging_writer.py:48] [44573] accumulated_eval_time=6223.816519, accumulated_logging_time=2.259704, accumulated_submission_time=14179.501738, global_step=44573, preemption_count=0, score=14179.501738, test/accuracy=0.986177, test/loss=0.047894, test/mean_average_precision=0.279685, test/num_examples=43793, total_duration=20406.760569, train/accuracy=0.992666, train/loss=0.023494, train/mean_average_precision=0.564104, validation/accuracy=0.986981, validation/loss=0.044801, validation/mean_average_precision=0.286103, validation/num_examples=43793
I0206 15:52:27.581561 139770185602816 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08356774598360062, loss=0.029559727758169174
I0206 15:52:59.291429 139752137176832 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09108620882034302, loss=0.024465130642056465
I0206 15:53:31.176030 139770185602816 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07490740716457367, loss=0.026297539472579956
I0206 15:54:02.979773 139752137176832 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.08192949742078781, loss=0.02725147269666195
I0206 15:54:34.429218 139770185602816 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07776876538991928, loss=0.024733955040574074
I0206 15:55:06.539911 139752137176832 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.09146418422460556, loss=0.02729157544672489
I0206 15:55:37.979746 139770185602816 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.07223808765411377, loss=0.025921210646629333
I0206 15:56:09.901386 139752137176832 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07086151838302612, loss=0.023523718118667603
I0206 15:56:18.773941 139978932307776 spec.py:321] Evaluating on the training split.
I0206 15:57:59.148057 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 15:58:02.264621 139978932307776 spec.py:349] Evaluating on the test split.
I0206 15:58:05.263425 139978932307776 submission_runner.py:408] Time since start: 20753.57s, 	Step: 45329, 	{'train/accuracy': 0.9926155209541321, 'train/loss': 0.023533791303634644, 'train/mean_average_precision': 0.5735633061987906, 'validation/accuracy': 0.9870926737785339, 'validation/loss': 0.04458260536193848, 'validation/mean_average_precision': 0.2924033260479522, 'validation/num_examples': 43793, 'test/accuracy': 0.9862361550331116, 'test/loss': 0.04773207753896713, 'test/mean_average_precision': 0.2748628905795507, 'test/num_examples': 43793, 'score': 14419.762070655823, 'total_duration': 20753.566682338715, 'accumulated_submission_time': 14419.762070655823, 'accumulated_eval_time': 6330.305959939957, 'accumulated_logging_time': 2.296353340148926}
I0206 15:58:05.287723 139733347358464 logging_writer.py:48] [45329] accumulated_eval_time=6330.305960, accumulated_logging_time=2.296353, accumulated_submission_time=14419.762071, global_step=45329, preemption_count=0, score=14419.762071, test/accuracy=0.986236, test/loss=0.047732, test/mean_average_precision=0.274863, test/num_examples=43793, total_duration=20753.566682, train/accuracy=0.992616, train/loss=0.023534, train/mean_average_precision=0.573563, validation/accuracy=0.987093, validation/loss=0.044583, validation/mean_average_precision=0.292403, validation/num_examples=43793
I0206 15:58:28.109695 139804644198144 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.07172711938619614, loss=0.02410924807190895
I0206 15:59:00.046567 139733347358464 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.08904165774583817, loss=0.025313179939985275
I0206 15:59:31.588232 139804644198144 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09010900557041168, loss=0.027100153267383575
I0206 16:00:03.347810 139733347358464 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.10110574215650558, loss=0.027000604197382927
I0206 16:00:34.912051 139804644198144 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07019804418087006, loss=0.02395346201956272
I0206 16:01:06.604767 139733347358464 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08360090851783752, loss=0.0254715196788311
I0206 16:01:38.509234 139804644198144 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.07826702296733856, loss=0.027340427041053772
I0206 16:02:05.385936 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:03:44.112672 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:03:47.140240 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:03:50.116440 139978932307776 submission_runner.py:408] Time since start: 21098.42s, 	Step: 46086, 	{'train/accuracy': 0.9928174614906311, 'train/loss': 0.022799771279096603, 'train/mean_average_precision': 0.5843304260900297, 'validation/accuracy': 0.9871162176132202, 'validation/loss': 0.045017652213573456, 'validation/mean_average_precision': 0.29081624087472746, 'validation/num_examples': 43793, 'test/accuracy': 0.9862534403800964, 'test/loss': 0.0483238622546196, 'test/mean_average_precision': 0.2770070234684541, 'test/num_examples': 43793, 'score': 14659.82816028595, 'total_duration': 21098.419695854187, 'accumulated_submission_time': 14659.82816028595, 'accumulated_eval_time': 6435.036421775818, 'accumulated_logging_time': 2.332881212234497}
I0206 16:03:50.140573 139752137176832 logging_writer.py:48] [46086] accumulated_eval_time=6435.036422, accumulated_logging_time=2.332881, accumulated_submission_time=14659.828160, global_step=46086, preemption_count=0, score=14659.828160, test/accuracy=0.986253, test/loss=0.048324, test/mean_average_precision=0.277007, test/num_examples=43793, total_duration=21098.419696, train/accuracy=0.992817, train/loss=0.022800, train/mean_average_precision=0.584330, validation/accuracy=0.987116, validation/loss=0.045018, validation/mean_average_precision=0.290816, validation/num_examples=43793
I0206 16:03:54.891199 139752176375552 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.07515925914049149, loss=0.023935925215482712
I0206 16:04:26.898812 139752137176832 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07900252938270569, loss=0.023827578872442245
I0206 16:04:58.763314 139752176375552 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.07247596234083176, loss=0.026051728054881096
I0206 16:05:30.405669 139752137176832 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.07996580004692078, loss=0.023743676021695137
I0206 16:06:02.284531 139752176375552 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.09056981652975082, loss=0.02539633773267269
I0206 16:06:34.160698 139752137176832 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08981313556432724, loss=0.026662489399313927
I0206 16:07:05.932603 139752176375552 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.0878588929772377, loss=0.0288985725492239
I0206 16:07:37.822935 139752137176832 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08515427261590958, loss=0.025613922625780106
I0206 16:07:50.237683 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:09:27.770087 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:09:30.789347 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:09:33.749447 139978932307776 submission_runner.py:408] Time since start: 21442.05s, 	Step: 46840, 	{'train/accuracy': 0.9929105043411255, 'train/loss': 0.022443510591983795, 'train/mean_average_precision': 0.583947243919866, 'validation/accuracy': 0.9870622158050537, 'validation/loss': 0.04508613795042038, 'validation/mean_average_precision': 0.2884127098754348, 'validation/num_examples': 43793, 'test/accuracy': 0.9863351583480835, 'test/loss': 0.04808259755373001, 'test/mean_average_precision': 0.2740103134893802, 'test/num_examples': 43793, 'score': 14899.894470214844, 'total_duration': 21442.052701473236, 'accumulated_submission_time': 14899.894470214844, 'accumulated_eval_time': 6538.548140287399, 'accumulated_logging_time': 2.3681788444519043}
I0206 16:09:33.774040 139733347358464 logging_writer.py:48] [46840] accumulated_eval_time=6538.548140, accumulated_logging_time=2.368179, accumulated_submission_time=14899.894470, global_step=46840, preemption_count=0, score=14899.894470, test/accuracy=0.986335, test/loss=0.048083, test/mean_average_precision=0.274010, test/num_examples=43793, total_duration=21442.052701, train/accuracy=0.992911, train/loss=0.022444, train/mean_average_precision=0.583947, validation/accuracy=0.987062, validation/loss=0.045086, validation/mean_average_precision=0.288413, validation/num_examples=43793
I0206 16:09:53.309926 139770185602816 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.10274232923984528, loss=0.028067873790860176
I0206 16:10:24.746063 139733347358464 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08011287450790405, loss=0.02448338270187378
I0206 16:10:56.360589 139770185602816 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.07921958714723587, loss=0.02553296834230423
I0206 16:11:28.247175 139733347358464 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.10611946880817413, loss=0.026016762480139732
I0206 16:12:00.284778 139770185602816 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08584018051624298, loss=0.027717342600226402
I0206 16:12:32.501129 139733347358464 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07616165280342102, loss=0.024578647688031197
I0206 16:13:04.062444 139770185602816 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09483765810728073, loss=0.025892462581396103
I0206 16:13:33.795814 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:15:14.932489 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:15:17.951862 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:15:20.940946 139978932307776 submission_runner.py:408] Time since start: 21789.24s, 	Step: 47594, 	{'train/accuracy': 0.9930605888366699, 'train/loss': 0.02211160585284233, 'train/mean_average_precision': 0.6052337144115114, 'validation/accuracy': 0.9869980812072754, 'validation/loss': 0.04506611451506615, 'validation/mean_average_precision': 0.2931761841290524, 'validation/num_examples': 43793, 'test/accuracy': 0.9861367344856262, 'test/loss': 0.04800629988312721, 'test/mean_average_precision': 0.2781953903528778, 'test/num_examples': 43793, 'score': 15139.885927677155, 'total_duration': 21789.244199752808, 'accumulated_submission_time': 15139.885927677155, 'accumulated_eval_time': 6645.693230390549, 'accumulated_logging_time': 2.4038455486297607}
I0206 16:15:20.965674 139752137176832 logging_writer.py:48] [47594] accumulated_eval_time=6645.693230, accumulated_logging_time=2.403846, accumulated_submission_time=15139.885928, global_step=47594, preemption_count=0, score=15139.885928, test/accuracy=0.986137, test/loss=0.048006, test/mean_average_precision=0.278195, test/num_examples=43793, total_duration=21789.244200, train/accuracy=0.993061, train/loss=0.022112, train/mean_average_precision=0.605234, validation/accuracy=0.986998, validation/loss=0.045066, validation/mean_average_precision=0.293176, validation/num_examples=43793
I0206 16:15:23.165529 139804644198144 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08129989355802536, loss=0.025245599448680878
I0206 16:15:54.553017 139752137176832 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.08224772661924362, loss=0.02621428482234478
I0206 16:16:26.099044 139804644198144 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.09852436929941177, loss=0.02612784132361412
I0206 16:16:57.771399 139752137176832 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.08885682374238968, loss=0.026075759902596474
I0206 16:17:29.424365 139804644198144 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.0767025426030159, loss=0.023125072941184044
I0206 16:18:01.101901 139752137176832 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07555811107158661, loss=0.024339748546481133
I0206 16:18:32.720842 139804644198144 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07931499183177948, loss=0.023776600137352943
I0206 16:19:04.469428 139752137176832 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.07952816039323807, loss=0.021392354741692543
I0206 16:19:21.198944 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:20:56.713218 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:20:59.720557 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:21:02.760028 139978932307776 submission_runner.py:408] Time since start: 22131.06s, 	Step: 48354, 	{'train/accuracy': 0.9933203458786011, 'train/loss': 0.021325798705220222, 'train/mean_average_precision': 0.6125192019839938, 'validation/accuracy': 0.9870272874832153, 'validation/loss': 0.044993653893470764, 'validation/mean_average_precision': 0.2895411568214211, 'validation/num_examples': 43793, 'test/accuracy': 0.9861510992050171, 'test/loss': 0.04813129082322121, 'test/mean_average_precision': 0.27331465592941967, 'test/num_examples': 43793, 'score': 15380.088567256927, 'total_duration': 22131.06327676773, 'accumulated_submission_time': 15380.088567256927, 'accumulated_eval_time': 6747.254262685776, 'accumulated_logging_time': 2.439406394958496}
I0206 16:21:02.784800 139733347358464 logging_writer.py:48] [48354] accumulated_eval_time=6747.254263, accumulated_logging_time=2.439406, accumulated_submission_time=15380.088567, global_step=48354, preemption_count=0, score=15380.088567, test/accuracy=0.986151, test/loss=0.048131, test/mean_average_precision=0.273315, test/num_examples=43793, total_duration=22131.063277, train/accuracy=0.993320, train/loss=0.021326, train/mean_average_precision=0.612519, validation/accuracy=0.987027, validation/loss=0.044994, validation/mean_average_precision=0.289541, validation/num_examples=43793
I0206 16:21:17.716245 139752176375552 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.08750572055578232, loss=0.024295855313539505
I0206 16:21:49.321830 139733347358464 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.10084422677755356, loss=0.02788812294602394
I0206 16:22:21.020259 139752176375552 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08366788923740387, loss=0.02495180442929268
I0206 16:22:53.571021 139733347358464 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.09000764787197113, loss=0.02549370937049389
I0206 16:23:24.902011 139752176375552 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.09296531975269318, loss=0.026797249913215637
I0206 16:23:56.107798 139733347358464 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.08312481641769409, loss=0.02311042696237564
I0206 16:24:27.740563 139752176375552 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08488232642412186, loss=0.024455511942505836
I0206 16:24:59.235733 139733347358464 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.08955977112054825, loss=0.021919414401054382
I0206 16:25:03.040430 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:26:38.826251 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:26:41.824039 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:26:44.754953 139978932307776 submission_runner.py:408] Time since start: 22473.06s, 	Step: 49113, 	{'train/accuracy': 0.9934682250022888, 'train/loss': 0.02071463316679001, 'train/mean_average_precision': 0.6426092112092957, 'validation/accuracy': 0.9870119094848633, 'validation/loss': 0.045634426176548004, 'validation/mean_average_precision': 0.2912810678611326, 'validation/num_examples': 43793, 'test/accuracy': 0.9861595034599304, 'test/loss': 0.04884245619177818, 'test/mean_average_precision': 0.27329671056341903, 'test/num_examples': 43793, 'score': 15620.312088727951, 'total_duration': 22473.058209180832, 'accumulated_submission_time': 15620.312088727951, 'accumulated_eval_time': 6848.968739748001, 'accumulated_logging_time': 2.4766972064971924}
I0206 16:26:44.785758 139770185602816 logging_writer.py:48] [49113] accumulated_eval_time=6848.968740, accumulated_logging_time=2.476697, accumulated_submission_time=15620.312089, global_step=49113, preemption_count=0, score=15620.312089, test/accuracy=0.986160, test/loss=0.048842, test/mean_average_precision=0.273297, test/num_examples=43793, total_duration=22473.058209, train/accuracy=0.993468, train/loss=0.020715, train/mean_average_precision=0.642609, validation/accuracy=0.987012, validation/loss=0.045634, validation/mean_average_precision=0.291281, validation/num_examples=43793
I0206 16:27:12.841831 139804644198144 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.09761612862348557, loss=0.027538971975445747
I0206 16:27:44.683915 139770185602816 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.09021008759737015, loss=0.02486526407301426
I0206 16:28:16.087302 139804644198144 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.09735661000013351, loss=0.025410713627934456
I0206 16:28:48.111574 139770185602816 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.07976918667554855, loss=0.022598804906010628
I0206 16:29:19.706406 139804644198144 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.09345383197069168, loss=0.0253493282943964
I0206 16:29:51.564721 139770185602816 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09432316571474075, loss=0.027476932853460312
I0206 16:30:23.287375 139804644198144 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08948339521884918, loss=0.025786209851503372
I0206 16:30:44.967822 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:32:19.190043 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:32:22.229797 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:32:25.344259 139978932307776 submission_runner.py:408] Time since start: 22813.65s, 	Step: 49870, 	{'train/accuracy': 0.993198812007904, 'train/loss': 0.021317660808563232, 'train/mean_average_precision': 0.6175284713675935, 'validation/accuracy': 0.9872095584869385, 'validation/loss': 0.04620447754859924, 'validation/mean_average_precision': 0.2915582478001256, 'validation/num_examples': 43793, 'test/accuracy': 0.9863507151603699, 'test/loss': 0.0493396520614624, 'test/mean_average_precision': 0.27417384906227316, 'test/num_examples': 43793, 'score': 15860.463735103607, 'total_duration': 22813.647511959076, 'accumulated_submission_time': 15860.463735103607, 'accumulated_eval_time': 6949.345129728317, 'accumulated_logging_time': 2.5182931423187256}
I0206 16:32:25.370120 139752137176832 logging_writer.py:48] [49870] accumulated_eval_time=6949.345130, accumulated_logging_time=2.518293, accumulated_submission_time=15860.463735, global_step=49870, preemption_count=0, score=15860.463735, test/accuracy=0.986351, test/loss=0.049340, test/mean_average_precision=0.274174, test/num_examples=43793, total_duration=22813.647512, train/accuracy=0.993199, train/loss=0.021318, train/mean_average_precision=0.617528, validation/accuracy=0.987210, validation/loss=0.046204, validation/mean_average_precision=0.291558, validation/num_examples=43793
I0206 16:32:35.358114 139752176375552 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.08839648216962814, loss=0.02284928783774376
I0206 16:33:06.838633 139752137176832 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.10039381682872772, loss=0.023951290175318718
I0206 16:33:38.278666 139752176375552 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09398212283849716, loss=0.025908393785357475
I0206 16:34:10.123866 139752137176832 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.07833171635866165, loss=0.021518342196941376
I0206 16:34:41.481593 139752176375552 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.08844641596078873, loss=0.02269751764833927
I0206 16:35:12.908529 139752137176832 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.10647886246442795, loss=0.024069223552942276
I0206 16:35:44.784373 139752176375552 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08502409607172012, loss=0.024249067530035973
I0206 16:36:16.482694 139752137176832 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09725207835435867, loss=0.024287600070238113
I0206 16:36:25.646799 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:37:59.428541 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:38:02.583650 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:38:07.993503 139978932307776 submission_runner.py:408] Time since start: 23156.30s, 	Step: 50630, 	{'train/accuracy': 0.9932072758674622, 'train/loss': 0.021530773490667343, 'train/mean_average_precision': 0.6065806372072025, 'validation/accuracy': 0.9869790077209473, 'validation/loss': 0.045623790472745895, 'validation/mean_average_precision': 0.2889365308084413, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.04872185364365578, 'test/mean_average_precision': 0.2741141156405496, 'test/num_examples': 43793, 'score': 16100.710295438766, 'total_duration': 23156.296748638153, 'accumulated_submission_time': 16100.710295438766, 'accumulated_eval_time': 7051.691777467728, 'accumulated_logging_time': 2.554926872253418}
I0206 16:38:08.019966 139733347358464 logging_writer.py:48] [50630] accumulated_eval_time=7051.691777, accumulated_logging_time=2.554927, accumulated_submission_time=16100.710295, global_step=50630, preemption_count=0, score=16100.710295, test/accuracy=0.986174, test/loss=0.048722, test/mean_average_precision=0.274114, test/num_examples=43793, total_duration=23156.296749, train/accuracy=0.993207, train/loss=0.021531, train/mean_average_precision=0.606581, validation/accuracy=0.986979, validation/loss=0.045624, validation/mean_average_precision=0.288937, validation/num_examples=43793
I0206 16:38:30.308692 139770185602816 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.09684336930513382, loss=0.024116424843668938
I0206 16:39:01.802999 139733347358464 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.08785360306501389, loss=0.02485089935362339
I0206 16:39:33.005929 139770185602816 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08267153054475784, loss=0.024376608431339264
I0206 16:40:04.498601 139733347358464 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08312669396400452, loss=0.02461833506822586
I0206 16:40:36.002272 139770185602816 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.114651158452034, loss=0.025792371481657028
I0206 16:41:07.329004 139733347358464 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08859090507030487, loss=0.025115424767136574
I0206 16:41:38.561510 139770185602816 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.09378345310688019, loss=0.02347063086926937
I0206 16:42:08.040958 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:43:42.769907 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:43:45.789115 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:43:48.746146 139978932307776 submission_runner.py:408] Time since start: 23497.05s, 	Step: 51395, 	{'train/accuracy': 0.9932363629341125, 'train/loss': 0.021373797208070755, 'train/mean_average_precision': 0.6189733847194379, 'validation/accuracy': 0.9871328473091125, 'validation/loss': 0.04566166549921036, 'validation/mean_average_precision': 0.28872892109869364, 'validation/num_examples': 43793, 'test/accuracy': 0.9862711429595947, 'test/loss': 0.04888831079006195, 'test/mean_average_precision': 0.2745948013890646, 'test/num_examples': 43793, 'score': 16340.700924158096, 'total_duration': 23497.049400806427, 'accumulated_submission_time': 16340.700924158096, 'accumulated_eval_time': 7152.396923303604, 'accumulated_logging_time': 2.5921742916107178}
I0206 16:43:48.770323 139752137176832 logging_writer.py:48] [51395] accumulated_eval_time=7152.396923, accumulated_logging_time=2.592174, accumulated_submission_time=16340.700924, global_step=51395, preemption_count=0, score=16340.700924, test/accuracy=0.986271, test/loss=0.048888, test/mean_average_precision=0.274595, test/num_examples=43793, total_duration=23497.049401, train/accuracy=0.993236, train/loss=0.021374, train/mean_average_precision=0.618973, validation/accuracy=0.987133, validation/loss=0.045662, validation/mean_average_precision=0.288729, validation/num_examples=43793
I0206 16:43:50.734309 139752176375552 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.07417147606611252, loss=0.0221059899777174
I0206 16:44:22.280631 139752137176832 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09837263822555542, loss=0.023531658574938774
I0206 16:44:53.832600 139752176375552 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.10275895893573761, loss=0.026203038170933723
I0206 16:45:25.149194 139752137176832 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.08089340478181839, loss=0.022684145718812943
I0206 16:45:56.227756 139752176375552 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.10232165455818176, loss=0.023430151864886284
I0206 16:46:27.885101 139752137176832 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10206987708806992, loss=0.026211056858301163
I0206 16:46:59.105388 139752176375552 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09768880903720856, loss=0.024263108149170876
I0206 16:47:30.545796 139752137176832 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.09563310444355011, loss=0.026313215494155884
I0206 16:47:48.817662 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:49:24.249731 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:49:27.237530 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:49:30.292582 139978932307776 submission_runner.py:408] Time since start: 23838.60s, 	Step: 52159, 	{'train/accuracy': 0.9932331442832947, 'train/loss': 0.02135111205279827, 'train/mean_average_precision': 0.6257244482281129, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.045979227870702744, 'validation/mean_average_precision': 0.28716131986331844, 'validation/num_examples': 43793, 'test/accuracy': 0.9862285852432251, 'test/loss': 0.049049995839595795, 'test/mean_average_precision': 0.2820580224329422, 'test/num_examples': 43793, 'score': 16580.71767473221, 'total_duration': 23838.59583926201, 'accumulated_submission_time': 16580.71767473221, 'accumulated_eval_time': 7253.871799230576, 'accumulated_logging_time': 2.6272754669189453}
I0206 16:49:30.317867 139770185602816 logging_writer.py:48] [52159] accumulated_eval_time=7253.871799, accumulated_logging_time=2.627275, accumulated_submission_time=16580.717675, global_step=52159, preemption_count=0, score=16580.717675, test/accuracy=0.986229, test/loss=0.049050, test/mean_average_precision=0.282058, test/num_examples=43793, total_duration=23838.595839, train/accuracy=0.993233, train/loss=0.021351, train/mean_average_precision=0.625724, validation/accuracy=0.986973, validation/loss=0.045979, validation/mean_average_precision=0.287161, validation/num_examples=43793
I0206 16:49:43.613281 139804644198144 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08645384013652802, loss=0.022944528609514236
I0206 16:50:15.138244 139770185602816 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.0874556452035904, loss=0.022429076954722404
I0206 16:50:46.594876 139804644198144 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09459072351455688, loss=0.02414928935468197
I0206 16:51:18.461240 139770185602816 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08875873684883118, loss=0.027219010517001152
I0206 16:51:50.024295 139804644198144 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.11248671263456345, loss=0.025443540886044502
I0206 16:52:21.564941 139770185602816 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.10565702617168427, loss=0.021873794496059418
I0206 16:52:53.361156 139804644198144 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1003553494811058, loss=0.024584222584962845
I0206 16:53:25.109472 139770185602816 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.10144590586423874, loss=0.026263359934091568
I0206 16:53:30.490165 139978932307776 spec.py:321] Evaluating on the training split.
I0206 16:55:05.748259 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 16:55:08.752906 139978932307776 spec.py:349] Evaluating on the test split.
I0206 16:55:11.739419 139978932307776 submission_runner.py:408] Time since start: 24180.04s, 	Step: 52918, 	{'train/accuracy': 0.9933499693870544, 'train/loss': 0.020969053730368614, 'train/mean_average_precision': 0.6198161926220507, 'validation/accuracy': 0.9870723485946655, 'validation/loss': 0.04600640386343002, 'validation/mean_average_precision': 0.2898691017552575, 'validation/num_examples': 43793, 'test/accuracy': 0.9862694144248962, 'test/loss': 0.04908306896686554, 'test/mean_average_precision': 0.2786305604471064, 'test/num_examples': 43793, 'score': 16820.859786748886, 'total_duration': 24180.04267501831, 'accumulated_submission_time': 16820.859786748886, 'accumulated_eval_time': 7355.1210062503815, 'accumulated_logging_time': 2.6630918979644775}
I0206 16:55:11.764312 139733347358464 logging_writer.py:48] [52918] accumulated_eval_time=7355.121006, accumulated_logging_time=2.663092, accumulated_submission_time=16820.859787, global_step=52918, preemption_count=0, score=16820.859787, test/accuracy=0.986269, test/loss=0.049083, test/mean_average_precision=0.278631, test/num_examples=43793, total_duration=24180.042675, train/accuracy=0.993350, train/loss=0.020969, train/mean_average_precision=0.619816, validation/accuracy=0.987072, validation/loss=0.046006, validation/mean_average_precision=0.289869, validation/num_examples=43793
I0206 16:55:38.180510 139752137176832 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.10610035061836243, loss=0.024802787229418755
I0206 16:56:09.588626 139733347358464 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.10738878697156906, loss=0.024901947006583214
I0206 16:56:40.972230 139752137176832 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09616360813379288, loss=0.022731458768248558
I0206 16:57:12.582675 139733347358464 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.11057662963867188, loss=0.02834196574985981
I0206 16:57:43.968440 139752137176832 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.08630694448947906, loss=0.022257542237639427
I0206 16:58:17.143865 139733347358464 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09789564460515976, loss=0.024086643010377884
I0206 16:58:48.856116 139752137176832 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09897124767303467, loss=0.02392455004155636
I0206 16:59:12.015808 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:00:48.746748 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:00:51.832611 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:00:54.810541 139978932307776 submission_runner.py:408] Time since start: 24523.11s, 	Step: 53674, 	{'train/accuracy': 0.9933534264564514, 'train/loss': 0.02080860733985901, 'train/mean_average_precision': 0.629301270151079, 'validation/accuracy': 0.9871153831481934, 'validation/loss': 0.046308934688568115, 'validation/mean_average_precision': 0.28908989832738635, 'validation/num_examples': 43793, 'test/accuracy': 0.9862861037254333, 'test/loss': 0.04954114183783531, 'test/mean_average_precision': 0.27942670271488634, 'test/num_examples': 43793, 'score': 17061.08062529564, 'total_duration': 24523.113798379898, 'accumulated_submission_time': 17061.08062529564, 'accumulated_eval_time': 7457.915698289871, 'accumulated_logging_time': 2.698948621749878}
I0206 17:00:54.836195 139752176375552 logging_writer.py:48] [53674] accumulated_eval_time=7457.915698, accumulated_logging_time=2.698949, accumulated_submission_time=17061.080625, global_step=53674, preemption_count=0, score=17061.080625, test/accuracy=0.986286, test/loss=0.049541, test/mean_average_precision=0.279427, test/num_examples=43793, total_duration=24523.113798, train/accuracy=0.993353, train/loss=0.020809, train/mean_average_precision=0.629301, validation/accuracy=0.987115, validation/loss=0.046309, validation/mean_average_precision=0.289090, validation/num_examples=43793
I0206 17:01:03.376555 139770185602816 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.11104071140289307, loss=0.023816168308258057
I0206 17:01:35.037826 139752176375552 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.09456628561019897, loss=0.02231893502175808
I0206 17:02:06.666915 139770185602816 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.0892917588353157, loss=0.02285381220281124
I0206 17:02:38.430948 139752176375552 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10489701479673386, loss=0.02504158578813076
I0206 17:03:09.945295 139770185602816 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.09904856979846954, loss=0.022135069593787193
I0206 17:03:41.919616 139752176375552 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.11468904465436935, loss=0.024098707363009453
I0206 17:04:13.419614 139770185602816 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.1013282835483551, loss=0.02366078644990921
I0206 17:04:45.041791 139752176375552 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.12659095227718353, loss=0.023139221593737602
I0206 17:04:54.908996 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:06:27.853604 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:06:30.877120 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:06:33.832972 139978932307776 submission_runner.py:408] Time since start: 24862.14s, 	Step: 54432, 	{'train/accuracy': 0.9934797883033752, 'train/loss': 0.020473549142479897, 'train/mean_average_precision': 0.62993081766115, 'validation/accuracy': 0.9871438145637512, 'validation/loss': 0.046290863305330276, 'validation/mean_average_precision': 0.2877251097946155, 'validation/num_examples': 43793, 'test/accuracy': 0.9863119721412659, 'test/loss': 0.049489088356494904, 'test/mean_average_precision': 0.27639562463520206, 'test/num_examples': 43793, 'score': 17301.122370243073, 'total_duration': 24862.13622522354, 'accumulated_submission_time': 17301.122370243073, 'accumulated_eval_time': 7556.839626789093, 'accumulated_logging_time': 2.735714912414551}
I0206 17:06:33.859832 139752137176832 logging_writer.py:48] [54432] accumulated_eval_time=7556.839627, accumulated_logging_time=2.735715, accumulated_submission_time=17301.122370, global_step=54432, preemption_count=0, score=17301.122370, test/accuracy=0.986312, test/loss=0.049489, test/mean_average_precision=0.276396, test/num_examples=43793, total_duration=24862.136225, train/accuracy=0.993480, train/loss=0.020474, train/mean_average_precision=0.629931, validation/accuracy=0.987144, validation/loss=0.046291, validation/mean_average_precision=0.287725, validation/num_examples=43793
I0206 17:06:55.889419 139804644198144 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09684514254331589, loss=0.023954885080456734
I0206 17:07:27.789607 139752137176832 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09538790583610535, loss=0.02123321406543255
I0206 17:07:59.592813 139804644198144 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.11293122917413712, loss=0.02501589059829712
I0206 17:08:31.413060 139752137176832 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.10385654121637344, loss=0.021705390885472298
I0206 17:09:03.744658 139804644198144 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10046606510877609, loss=0.023522110655903816
I0206 17:09:35.735748 139752137176832 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09615886956453323, loss=0.02089885249733925
I0206 17:10:07.509873 139804644198144 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.10223402082920074, loss=0.024477194994688034
I0206 17:10:34.083572 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:12:12.719422 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:12:15.728447 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:12:18.699965 139978932307776 submission_runner.py:408] Time since start: 25207.00s, 	Step: 55184, 	{'train/accuracy': 0.9935911297798157, 'train/loss': 0.020000223070383072, 'train/mean_average_precision': 0.6547790093724063, 'validation/accuracy': 0.9871222972869873, 'validation/loss': 0.0461614653468132, 'validation/mean_average_precision': 0.2900952606281319, 'validation/num_examples': 43793, 'test/accuracy': 0.986243724822998, 'test/loss': 0.04939693585038185, 'test/mean_average_precision': 0.2795634855130245, 'test/num_examples': 43793, 'score': 17541.313578367233, 'total_duration': 25207.003221273422, 'accumulated_submission_time': 17541.313578367233, 'accumulated_eval_time': 7661.45597743988, 'accumulated_logging_time': 2.774885654449463}
I0206 17:12:18.726517 139752176375552 logging_writer.py:48] [55184] accumulated_eval_time=7661.455977, accumulated_logging_time=2.774886, accumulated_submission_time=17541.313578, global_step=55184, preemption_count=0, score=17541.313578, test/accuracy=0.986244, test/loss=0.049397, test/mean_average_precision=0.279563, test/num_examples=43793, total_duration=25207.003221, train/accuracy=0.993591, train/loss=0.020000, train/mean_average_precision=0.654779, validation/accuracy=0.987122, validation/loss=0.046161, validation/mean_average_precision=0.290095, validation/num_examples=43793
I0206 17:12:24.237076 139770185602816 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.11018653213977814, loss=0.022187715396285057
I0206 17:12:55.964219 139752176375552 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1020423024892807, loss=0.023580657318234444
I0206 17:13:27.634530 139770185602816 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10353704541921616, loss=0.02445020154118538
I0206 17:13:59.303494 139752176375552 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09787756204605103, loss=0.021460499614477158
I0206 17:14:30.994693 139770185602816 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.10866319388151169, loss=0.022950369864702225
I0206 17:15:02.311097 139752176375552 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10085634887218475, loss=0.02140517346560955
I0206 17:15:33.700867 139770185602816 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09766624122858047, loss=0.02187415584921837
I0206 17:16:05.276064 139752176375552 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.09712295234203339, loss=0.02089954912662506
I0206 17:16:18.740288 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:17:51.876712 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:17:54.902832 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:17:57.835525 139978932307776 submission_runner.py:408] Time since start: 25546.14s, 	Step: 55944, 	{'train/accuracy': 0.9937967658042908, 'train/loss': 0.01947147771716118, 'train/mean_average_precision': 0.6527825708898463, 'validation/accuracy': 0.9870926737785339, 'validation/loss': 0.04674072191119194, 'validation/mean_average_precision': 0.2911061819818859, 'validation/num_examples': 43793, 'test/accuracy': 0.9862310886383057, 'test/loss': 0.05005291849374771, 'test/mean_average_precision': 0.2778119291983832, 'test/num_examples': 43793, 'score': 17781.294590473175, 'total_duration': 25546.138780355453, 'accumulated_submission_time': 17781.294590473175, 'accumulated_eval_time': 7760.551167964935, 'accumulated_logging_time': 2.8143293857574463}
I0206 17:17:57.861695 139752137176832 logging_writer.py:48] [55944] accumulated_eval_time=7760.551168, accumulated_logging_time=2.814329, accumulated_submission_time=17781.294590, global_step=55944, preemption_count=0, score=17781.294590, test/accuracy=0.986231, test/loss=0.050053, test/mean_average_precision=0.277812, test/num_examples=43793, total_duration=25546.138780, train/accuracy=0.993797, train/loss=0.019471, train/mean_average_precision=0.652783, validation/accuracy=0.987093, validation/loss=0.046741, validation/mean_average_precision=0.291106, validation/num_examples=43793
I0206 17:18:15.925074 139804644198144 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09704723954200745, loss=0.020178526639938354
I0206 17:18:47.528837 139752137176832 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.10626347362995148, loss=0.023137293756008148
I0206 17:19:18.898294 139804644198144 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.10062609612941742, loss=0.023490728810429573
I0206 17:19:50.975334 139752137176832 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10983499139547348, loss=0.02213488705456257
I0206 17:20:22.676676 139804644198144 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1113676056265831, loss=0.02219691127538681
I0206 17:20:54.234075 139752137176832 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.11275341361761093, loss=0.024606216698884964
I0206 17:21:25.922995 139804644198144 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.09759249538183212, loss=0.0217695664614439
I0206 17:21:57.853850 139752137176832 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.11516928672790527, loss=0.022974608466029167
I0206 17:21:57.858936 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:23:36.426344 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:23:39.482241 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:23:42.476048 139978932307776 submission_runner.py:408] Time since start: 25890.78s, 	Step: 56701, 	{'train/accuracy': 0.9940793514251709, 'train/loss': 0.018616680055856705, 'train/mean_average_precision': 0.6898328240343279, 'validation/accuracy': 0.987098753452301, 'validation/loss': 0.04666012153029442, 'validation/mean_average_precision': 0.28995529375292517, 'validation/num_examples': 43793, 'test/accuracy': 0.9862062335014343, 'test/loss': 0.049819622188806534, 'test/mean_average_precision': 0.282412806094966, 'test/num_examples': 43793, 'score': 18021.25959467888, 'total_duration': 25890.779302597046, 'accumulated_submission_time': 18021.25959467888, 'accumulated_eval_time': 7865.168210506439, 'accumulated_logging_time': 2.853222131729126}
I0206 17:23:42.501804 139752176375552 logging_writer.py:48] [56701] accumulated_eval_time=7865.168211, accumulated_logging_time=2.853222, accumulated_submission_time=18021.259595, global_step=56701, preemption_count=0, score=18021.259595, test/accuracy=0.986206, test/loss=0.049820, test/mean_average_precision=0.282413, test/num_examples=43793, total_duration=25890.779303, train/accuracy=0.994079, train/loss=0.018617, train/mean_average_precision=0.689833, validation/accuracy=0.987099, validation/loss=0.046660, validation/mean_average_precision=0.289955, validation/num_examples=43793
I0206 17:24:14.723770 139770185602816 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.09850670397281647, loss=0.021994836628437042
I0206 17:24:46.249051 139752176375552 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.10381998121738434, loss=0.020360464230179787
I0206 17:25:18.118585 139770185602816 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.09975174814462662, loss=0.02452121302485466
I0206 17:25:49.819404 139752176375552 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10583008825778961, loss=0.022818734869360924
I0206 17:26:21.769202 139770185602816 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.12673039734363556, loss=0.023893186822533607
I0206 17:26:53.593755 139752176375552 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.12116043269634247, loss=0.023203251883387566
I0206 17:27:25.302412 139770185602816 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.10581497848033905, loss=0.022278547286987305
I0206 17:27:42.764693 139978932307776 spec.py:321] Evaluating on the training split.
I0206 17:29:16.679940 139978932307776 spec.py:333] Evaluating on the validation split.
I0206 17:29:19.840152 139978932307776 spec.py:349] Evaluating on the test split.
I0206 17:29:22.912671 139978932307776 submission_runner.py:408] Time since start: 26231.22s, 	Step: 57456, 	{'train/accuracy': 0.9942800998687744, 'train/loss': 0.018083401024341583, 'train/mean_average_precision': 0.6976413701881876, 'validation/accuracy': 0.9871283769607544, 'validation/loss': 0.04675629362463951, 'validation/mean_average_precision': 0.2962188183781732, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.05016188323497772, 'test/mean_average_precision': 0.2802577179914768, 'test/num_examples': 43793, 'score': 18261.490270614624, 'total_duration': 26231.215923547745, 'accumulated_submission_time': 18261.490270614624, 'accumulated_eval_time': 7965.316137075424, 'accumulated_logging_time': 2.891075849533081}
I0206 17:29:22.938817 139752137176832 logging_writer.py:48] [57456] accumulated_eval_time=7965.316137, accumulated_logging_time=2.891076, accumulated_submission_time=18261.490271, global_step=57456, preemption_count=0, score=18261.490271, test/accuracy=0.986218, test/loss=0.050162, test/mean_average_precision=0.280258, test/num_examples=43793, total_duration=26231.215924, train/accuracy=0.994280, train/loss=0.018083, train/mean_average_precision=0.697641, validation/accuracy=0.987128, validation/loss=0.046756, validation/mean_average_precision=0.296219, validation/num_examples=43793
I0206 17:29:37.163085 139804644198144 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10700099170207977, loss=0.0215140413492918
I0206 17:30:08.676762 139752137176832 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.11345944553613663, loss=0.023134715855121613
I0206 17:30:40.434103 139804644198144 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.11505164206027985, loss=0.02555708773434162
I0206 17:31:12.073131 139752137176832 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10034347325563431, loss=0.019657272845506668
I0206 17:31:43.657313 139804644198144 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.11361880600452423, loss=0.019872693344950676
I0206 17:32:15.255981 139752137176832 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.10889889299869537, loss=0.022283172234892845
I0206 17:32:47.162013 139804644198144 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.10144251585006714, loss=0.02329564467072487
I0206 17:32:58.732161 139752137176832 logging_writer.py:48] [58138] global_step=58138, preemption_count=0, score=18477.238426
I0206 17:32:58.828673 139978932307776 checkpoints.py:490] Saving checkpoint at step: 58138
I0206 17:32:58.988068 139978932307776 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5/checkpoint_58138
I0206 17:32:58.990782 139978932307776 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_4/ogbg_jax/trial_5/checkpoint_58138.
I0206 17:32:59.127862 139978932307776 submission_runner.py:583] Tuning trial 5/5
I0206 17:32:59.128088 139978932307776 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0206 17:32:59.132299 139978932307776 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.38867321610450745, 'train/loss': 0.7993758320808411, 'train/mean_average_precision': 0.024189225297995574, 'validation/accuracy': 0.39264529943466187, 'validation/loss': 0.7974664568901062, 'validation/mean_average_precision': 0.02483808256432166, 'validation/num_examples': 43793, 'test/accuracy': 0.3947482705116272, 'test/loss': 0.7956756353378296, 'test/mean_average_precision': 0.026379578053164793, 'test/num_examples': 43793, 'score': 13.266764402389526, 'total_duration': 121.23296904563904, 'accumulated_submission_time': 13.266764402389526, 'accumulated_eval_time': 107.96615147590637, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (760, {'train/accuracy': 0.9868772029876709, 'train/loss': 0.05130617320537567, 'train/mean_average_precision': 0.055429095232677444, 'validation/accuracy': 0.9841986894607544, 'validation/loss': 0.060611315071582794, 'validation/mean_average_precision': 0.053341938027725105, 'validation/num_examples': 43793, 'test/accuracy': 0.9831896424293518, 'test/loss': 0.06384647637605667, 'test/mean_average_precision': 0.05541749006743494, 'test/num_examples': 43793, 'score': 253.24348163604736, 'total_duration': 460.68167448043823, 'accumulated_submission_time': 253.24348163604736, 'accumulated_eval_time': 207.39866089820862, 'accumulated_logging_time': 0.01946091651916504, 'global_step': 760, 'preemption_count': 0}), (1513, {'train/accuracy': 0.9872434139251709, 'train/loss': 0.046811483800411224, 'train/mean_average_precision': 0.10833265647219083, 'validation/accuracy': 0.9846939444541931, 'validation/loss': 0.055397987365722656, 'validation/mean_average_precision': 0.10490476316852082, 'validation/num_examples': 43793, 'test/accuracy': 0.9836639165878296, 'test/loss': 0.05843276530504227, 'test/mean_average_precision': 0.1038389597988644, 'test/num_examples': 43793, 'score': 493.3325746059418, 'total_duration': 806.6452507972717, 'accumulated_submission_time': 493.3325746059418, 'accumulated_eval_time': 313.2266294956207, 'accumulated_logging_time': 0.04572749137878418, 'global_step': 1513, 'preemption_count': 0}), (2267, {'train/accuracy': 0.9878523945808411, 'train/loss': 0.0432136245071888, 'train/mean_average_precision': 0.1485056292047553, 'validation/accuracy': 0.9850824475288391, 'validation/loss': 0.05285615846514702, 'validation/mean_average_precision': 0.14011133332460388, 'validation/num_examples': 43793, 'test/accuracy': 0.9840859770774841, 'test/loss': 0.055762797594070435, 'test/mean_average_precision': 0.13522063297129633, 'test/num_examples': 43793, 'score': 733.5270512104034, 'total_duration': 1150.3065140247345, 'accumulated_submission_time': 733.5270512104034, 'accumulated_eval_time': 416.6431694030762, 'accumulated_logging_time': 0.07539010047912598, 'global_step': 2267, 'preemption_count': 0}), (3027, {'train/accuracy': 0.9883006811141968, 'train/loss': 0.04095477983355522, 'train/mean_average_precision': 0.17294835312469342, 'validation/accuracy': 0.9853901267051697, 'validation/loss': 0.05044339969754219, 'validation/mean_average_precision': 0.16348467636629252, 'validation/num_examples': 43793, 'test/accuracy': 0.9844338893890381, 'test/loss': 0.053324077278375626, 'test/mean_average_precision': 0.15936131435489542, 'test/num_examples': 43793, 'score': 973.7290751934052, 'total_duration': 1495.4947953224182, 'accumulated_submission_time': 973.7290751934052, 'accumulated_eval_time': 521.5821809768677, 'accumulated_logging_time': 0.10217666625976562, 'global_step': 3027, 'preemption_count': 0}), (3783, {'train/accuracy': 0.9884099364280701, 'train/loss': 0.040116507560014725, 'train/mean_average_precision': 0.19470530027179817, 'validation/accuracy': 0.9855772852897644, 'validation/loss': 0.049140069633722305, 'validation/mean_average_precision': 0.17634006684516024, 'validation/num_examples': 43793, 'test/accuracy': 0.984683632850647, 'test/loss': 0.05181457847356796, 'test/mean_average_precision': 0.17571994317594894, 'test/num_examples': 43793, 'score': 1213.7164585590363, 'total_duration': 1839.9989259243011, 'accumulated_submission_time': 1213.7164585590363, 'accumulated_eval_time': 626.0516443252563, 'accumulated_logging_time': 0.12928414344787598, 'global_step': 3783, 'preemption_count': 0}), (4538, {'train/accuracy': 0.9886505007743835, 'train/loss': 0.03874871879816055, 'train/mean_average_precision': 0.21259982180952808, 'validation/accuracy': 0.9857733845710754, 'validation/loss': 0.04842757806181908, 'validation/mean_average_precision': 0.19539026533259743, 'validation/num_examples': 43793, 'test/accuracy': 0.9848495721817017, 'test/loss': 0.051139019429683685, 'test/mean_average_precision': 0.1969125162851597, 'test/num_examples': 43793, 'score': 1453.7154524326324, 'total_duration': 2181.925208091736, 'accumulated_submission_time': 1453.7154524326324, 'accumulated_eval_time': 727.9304811954498, 'accumulated_logging_time': 0.15792489051818848, 'global_step': 4538, 'preemption_count': 0}), (5291, {'train/accuracy': 0.9889928698539734, 'train/loss': 0.03743991628289223, 'train/mean_average_precision': 0.24753548398149616, 'validation/accuracy': 0.9859016537666321, 'validation/loss': 0.04728766903281212, 'validation/mean_average_precision': 0.2130693687389712, 'validation/num_examples': 43793, 'test/accuracy': 0.9849818348884583, 'test/loss': 0.04990756884217262, 'test/mean_average_precision': 0.20962896712493773, 'test/num_examples': 43793, 'score': 1693.9603996276855, 'total_duration': 2531.2291519641876, 'accumulated_submission_time': 1693.9603996276855, 'accumulated_eval_time': 836.9415402412415, 'accumulated_logging_time': 0.1859288215637207, 'global_step': 5291, 'preemption_count': 0}), (6049, {'train/accuracy': 0.9890961050987244, 'train/loss': 0.03699062764644623, 'train/mean_average_precision': 0.25872194904249346, 'validation/accuracy': 0.9860299229621887, 'validation/loss': 0.047073882073163986, 'validation/mean_average_precision': 0.21408591631613105, 'validation/num_examples': 43793, 'test/accuracy': 0.9852286577224731, 'test/loss': 0.04985300824046135, 'test/mean_average_precision': 0.20940728521010876, 'test/num_examples': 43793, 'score': 1933.9973559379578, 'total_duration': 2879.3830313682556, 'accumulated_submission_time': 1933.9973559379578, 'accumulated_eval_time': 945.0108182430267, 'accumulated_logging_time': 0.21348261833190918, 'global_step': 6049, 'preemption_count': 0}), (6808, {'train/accuracy': 0.9892982840538025, 'train/loss': 0.03599486127495766, 'train/mean_average_precision': 0.2853854649351488, 'validation/accuracy': 0.9862214922904968, 'validation/loss': 0.046015024185180664, 'validation/mean_average_precision': 0.22560966442688724, 'validation/num_examples': 43793, 'test/accuracy': 0.9852871894836426, 'test/loss': 0.048492759466171265, 'test/mean_average_precision': 0.21902900229270952, 'test/num_examples': 43793, 'score': 2174.0190699100494, 'total_duration': 3223.005018234253, 'accumulated_submission_time': 2174.0190699100494, 'accumulated_eval_time': 1048.5638234615326, 'accumulated_logging_time': 0.24115276336669922, 'global_step': 6808, 'preemption_count': 0}), (7566, {'train/accuracy': 0.9897069931030273, 'train/loss': 0.034704916179180145, 'train/mean_average_precision': 0.30216275499469225, 'validation/accuracy': 0.9863408803939819, 'validation/loss': 0.04567665234208107, 'validation/mean_average_precision': 0.23187020433388755, 'validation/num_examples': 43793, 'test/accuracy': 0.9855091571807861, 'test/loss': 0.048133257776498795, 'test/mean_average_precision': 0.23834373456583896, 'test/num_examples': 43793, 'score': 2414.2607395648956, 'total_duration': 3566.0569252967834, 'accumulated_submission_time': 2414.2607395648956, 'accumulated_eval_time': 1151.3277099132538, 'accumulated_logging_time': 0.2674412727355957, 'global_step': 7566, 'preemption_count': 0}), (8324, {'train/accuracy': 0.9899153709411621, 'train/loss': 0.033655136823654175, 'train/mean_average_precision': 0.32028739999883327, 'validation/accuracy': 0.9864200353622437, 'validation/loss': 0.04565197229385376, 'validation/mean_average_precision': 0.23815165667823943, 'validation/num_examples': 43793, 'test/accuracy': 0.9855584502220154, 'test/loss': 0.04834138602018356, 'test/mean_average_precision': 0.2369385751055225, 'test/num_examples': 43793, 'score': 2654.523429632187, 'total_duration': 3912.501790523529, 'accumulated_submission_time': 2654.523429632187, 'accumulated_eval_time': 1257.461489200592, 'accumulated_logging_time': 0.2958061695098877, 'global_step': 8324, 'preemption_count': 0}), (9083, {'train/accuracy': 0.9900816082954407, 'train/loss': 0.03320526331663132, 'train/mean_average_precision': 0.3405124811740702, 'validation/accuracy': 0.9864683151245117, 'validation/loss': 0.045106906443834305, 'validation/mean_average_precision': 0.23899023678908335, 'validation/num_examples': 43793, 'test/accuracy': 0.985533595085144, 'test/loss': 0.04767252877354622, 'test/mean_average_precision': 0.24344503418705318, 'test/num_examples': 43793, 'score': 2894.683405160904, 'total_duration': 4254.173964262009, 'accumulated_submission_time': 2894.683405160904, 'accumulated_eval_time': 1358.925484418869, 'accumulated_logging_time': 0.32367920875549316, 'global_step': 9083, 'preemption_count': 0}), (9838, {'train/accuracy': 0.9902423024177551, 'train/loss': 0.03240682929754257, 'train/mean_average_precision': 0.3589596793839536, 'validation/accuracy': 0.9864655137062073, 'validation/loss': 0.044942017644643784, 'validation/mean_average_precision': 0.24561560730822854, 'validation/num_examples': 43793, 'test/accuracy': 0.9856464862823486, 'test/loss': 0.0476103201508522, 'test/mean_average_precision': 0.2401196061095888, 'test/num_examples': 43793, 'score': 3134.751321554184, 'total_duration': 4596.953059434891, 'accumulated_submission_time': 3134.751321554184, 'accumulated_eval_time': 1461.5836379528046, 'accumulated_logging_time': 0.35663437843322754, 'global_step': 9838, 'preemption_count': 0}), (10591, {'train/accuracy': 0.9902772903442383, 'train/loss': 0.03225903585553169, 'train/mean_average_precision': 0.36103109120477755, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.04454413428902626, 'validation/mean_average_precision': 0.25543941493454586, 'validation/num_examples': 43793, 'test/accuracy': 0.9858199954032898, 'test/loss': 0.047201115638017654, 'test/mean_average_precision': 0.24936071927245262, 'test/num_examples': 43793, 'score': 3374.937530517578, 'total_duration': 4940.203378915787, 'accumulated_submission_time': 3374.937530517578, 'accumulated_eval_time': 1564.5984835624695, 'accumulated_logging_time': 0.3857431411743164, 'global_step': 10591, 'preemption_count': 0}), (11347, {'train/accuracy': 0.9904393553733826, 'train/loss': 0.03198949620127678, 'train/mean_average_precision': 0.35538638490244756, 'validation/accuracy': 0.9865807890892029, 'validation/loss': 0.04452678933739662, 'validation/mean_average_precision': 0.24939574652448415, 'validation/num_examples': 43793, 'test/accuracy': 0.9857147336006165, 'test/loss': 0.04725104570388794, 'test/mean_average_precision': 0.25355996776683765, 'test/num_examples': 43793, 'score': 3615.0766611099243, 'total_duration': 5282.0795221328735, 'accumulated_submission_time': 3615.0766611099243, 'accumulated_eval_time': 1666.2861070632935, 'accumulated_logging_time': 0.4149281978607178, 'global_step': 11347, 'preemption_count': 0}), (12099, {'train/accuracy': 0.9903896450996399, 'train/loss': 0.03196679428219795, 'train/mean_average_precision': 0.36273471093644405, 'validation/accuracy': 0.9866493940353394, 'validation/loss': 0.044371090829372406, 'validation/mean_average_precision': 0.2522052206234418, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.046992335468530655, 'test/mean_average_precision': 0.2538715301046184, 'test/num_examples': 43793, 'score': 3855.316538333893, 'total_duration': 5628.884557723999, 'accumulated_submission_time': 3855.316538333893, 'accumulated_eval_time': 1772.8010022640228, 'accumulated_logging_time': 0.44490718841552734, 'global_step': 12099, 'preemption_count': 0}), (12859, {'train/accuracy': 0.9905506372451782, 'train/loss': 0.0315115861594677, 'train/mean_average_precision': 0.3814257408336702, 'validation/accuracy': 0.9867147207260132, 'validation/loss': 0.04444871470332146, 'validation/mean_average_precision': 0.26060915477594093, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04690170660614967, 'test/mean_average_precision': 0.2577900264790435, 'test/num_examples': 43793, 'score': 4095.371396303177, 'total_duration': 5972.1552057266235, 'accumulated_submission_time': 4095.371396303177, 'accumulated_eval_time': 1875.968015909195, 'accumulated_logging_time': 0.4730713367462158, 'global_step': 12859, 'preemption_count': 0}), (13612, {'train/accuracy': 0.9904999732971191, 'train/loss': 0.031200099736452103, 'train/mean_average_precision': 0.39747005719247963, 'validation/accuracy': 0.986764669418335, 'validation/loss': 0.044112421572208405, 'validation/mean_average_precision': 0.26093377510968474, 'validation/num_examples': 43793, 'test/accuracy': 0.9859269857406616, 'test/loss': 0.04675736278295517, 'test/mean_average_precision': 0.2620023400737273, 'test/num_examples': 43793, 'score': 4335.5524117946625, 'total_duration': 6319.501272678375, 'accumulated_submission_time': 4335.5524117946625, 'accumulated_eval_time': 1983.0827827453613, 'accumulated_logging_time': 0.5029690265655518, 'global_step': 13612, 'preemption_count': 0}), (14362, {'train/accuracy': 0.9905785322189331, 'train/loss': 0.030962450429797173, 'train/mean_average_precision': 0.3904840151507778, 'validation/accuracy': 0.9867537021636963, 'validation/loss': 0.04408657178282738, 'validation/mean_average_precision': 0.2646759096238124, 'validation/num_examples': 43793, 'test/accuracy': 0.9859316349029541, 'test/loss': 0.0466216616332531, 'test/mean_average_precision': 0.25938697563630925, 'test/num_examples': 43793, 'score': 4575.777185678482, 'total_duration': 6663.42175078392, 'accumulated_submission_time': 4575.777185678482, 'accumulated_eval_time': 2086.7270028591156, 'accumulated_logging_time': 0.5339128971099854, 'global_step': 14362, 'preemption_count': 0}), (15115, {'train/accuracy': 0.9908715486526489, 'train/loss': 0.029975270852446556, 'train/mean_average_precision': 0.4078574356020376, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.043870147317647934, 'validation/mean_average_precision': 0.2704771739922652, 'validation/num_examples': 43793, 'test/accuracy': 0.9859648942947388, 'test/loss': 0.04653972387313843, 'test/mean_average_precision': 0.26243861631050347, 'test/num_examples': 43793, 'score': 4816.017210483551, 'total_duration': 7005.01956653595, 'accumulated_submission_time': 4816.017210483551, 'accumulated_eval_time': 2188.0321526527405, 'accumulated_logging_time': 0.5664629936218262, 'global_step': 15115, 'preemption_count': 0}), (15868, {'train/accuracy': 0.990837574005127, 'train/loss': 0.029973115772008896, 'train/mean_average_precision': 0.4219413948334143, 'validation/accuracy': 0.9869092106819153, 'validation/loss': 0.04432278871536255, 'validation/mean_average_precision': 0.2641223882343574, 'validation/num_examples': 43793, 'test/accuracy': 0.9860209226608276, 'test/loss': 0.04720812290906906, 'test/mean_average_precision': 0.25567677099933905, 'test/num_examples': 43793, 'score': 5056.221124887466, 'total_duration': 7349.488345146179, 'accumulated_submission_time': 5056.221124887466, 'accumulated_eval_time': 2292.2479150295258, 'accumulated_logging_time': 0.595801830291748, 'global_step': 15868, 'preemption_count': 0}), (16631, {'train/accuracy': 0.9911161065101624, 'train/loss': 0.029060611501336098, 'train/mean_average_precision': 0.44426435509248297, 'validation/accuracy': 0.9867061972618103, 'validation/loss': 0.04393668472766876, 'validation/mean_average_precision': 0.2679191421744507, 'validation/num_examples': 43793, 'test/accuracy': 0.9859219789505005, 'test/loss': 0.04660208523273468, 'test/mean_average_precision': 0.27087723820100235, 'test/num_examples': 43793, 'score': 5296.290515899658, 'total_duration': 7691.375034809113, 'accumulated_submission_time': 5296.290515899658, 'accumulated_eval_time': 2394.0149455070496, 'accumulated_logging_time': 0.6254878044128418, 'global_step': 16631, 'preemption_count': 0}), (17393, {'train/accuracy': 0.9911894798278809, 'train/loss': 0.028837032616138458, 'train/mean_average_precision': 0.4573207657369297, 'validation/accuracy': 0.9868093132972717, 'validation/loss': 0.044242873787879944, 'validation/mean_average_precision': 0.268753044165262, 'validation/num_examples': 43793, 'test/accuracy': 0.9859758615493774, 'test/loss': 0.04703646898269653, 'test/mean_average_precision': 0.26876617914154183, 'test/num_examples': 43793, 'score': 5536.512343883514, 'total_duration': 8034.163238525391, 'accumulated_submission_time': 5536.512343883514, 'accumulated_eval_time': 2496.5304474830627, 'accumulated_logging_time': 0.655855655670166, 'global_step': 17393, 'preemption_count': 0}), (18153, {'train/accuracy': 0.9911827445030212, 'train/loss': 0.02886652573943138, 'train/mean_average_precision': 0.4415226656518888, 'validation/accuracy': 0.9869375824928284, 'validation/loss': 0.04401398450136185, 'validation/mean_average_precision': 0.27571217023098854, 'validation/num_examples': 43793, 'test/accuracy': 0.9860365390777588, 'test/loss': 0.0470629520714283, 'test/mean_average_precision': 0.2600734428996599, 'test/num_examples': 43793, 'score': 5776.531605005264, 'total_duration': 8377.209501743317, 'accumulated_submission_time': 5776.531605005264, 'accumulated_eval_time': 2599.5078728199005, 'accumulated_logging_time': 0.6851916313171387, 'global_step': 18153, 'preemption_count': 0}), (18905, {'train/accuracy': 0.9912071824073792, 'train/loss': 0.02895207516849041, 'train/mean_average_precision': 0.42595162934474834, 'validation/accuracy': 0.9868218898773193, 'validation/loss': 0.04381611943244934, 'validation/mean_average_precision': 0.2703069049199251, 'validation/num_examples': 43793, 'test/accuracy': 0.9859219789505005, 'test/loss': 0.04655629023909569, 'test/mean_average_precision': 0.26244538041953613, 'test/num_examples': 43793, 'score': 6016.586377620697, 'total_duration': 8720.30987906456, 'accumulated_submission_time': 6016.586377620697, 'accumulated_eval_time': 2702.5033643245697, 'accumulated_logging_time': 0.7152974605560303, 'global_step': 18905, 'preemption_count': 0}), (19659, {'train/accuracy': 0.9910314083099365, 'train/loss': 0.029263755306601524, 'train/mean_average_precision': 0.4337188049806543, 'validation/accuracy': 0.9868438243865967, 'validation/loss': 0.044348929077386856, 'validation/mean_average_precision': 0.26577527529149897, 'validation/num_examples': 43793, 'test/accuracy': 0.9860196709632874, 'test/loss': 0.04709337279200554, 'test/mean_average_precision': 0.2598258010263444, 'test/num_examples': 43793, 'score': 6256.5426633358, 'total_duration': 9063.561178445816, 'accumulated_submission_time': 6256.5426633358, 'accumulated_eval_time': 2805.7474246025085, 'accumulated_logging_time': 0.7460854053497314, 'global_step': 19659, 'preemption_count': 0}), (20406, {'train/accuracy': 0.9910750985145569, 'train/loss': 0.02913183718919754, 'train/mean_average_precision': 0.4227976599470897, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04403292387723923, 'validation/mean_average_precision': 0.26792202286689754, 'validation/num_examples': 43793, 'test/accuracy': 0.9860963225364685, 'test/loss': 0.04649520292878151, 'test/mean_average_precision': 0.2645376394413712, 'test/num_examples': 43793, 'score': 6496.541145086288, 'total_duration': 9405.10793018341, 'accumulated_submission_time': 6496.541145086288, 'accumulated_eval_time': 2907.2453095912933, 'accumulated_logging_time': 0.7763607501983643, 'global_step': 20406, 'preemption_count': 0}), (21153, {'train/accuracy': 0.9912204146385193, 'train/loss': 0.02879350446164608, 'train/mean_average_precision': 0.4551327226887302, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.043685078620910645, 'validation/mean_average_precision': 0.27464628654591416, 'validation/num_examples': 43793, 'test/accuracy': 0.9861363172531128, 'test/loss': 0.04666018486022949, 'test/mean_average_precision': 0.2698105443588578, 'test/num_examples': 43793, 'score': 6736.561691999435, 'total_duration': 9748.872096776962, 'accumulated_submission_time': 6736.561691999435, 'accumulated_eval_time': 3010.9387967586517, 'accumulated_logging_time': 0.8066954612731934, 'global_step': 21153, 'preemption_count': 0}), (21908, {'train/accuracy': 0.9911985397338867, 'train/loss': 0.028594762086868286, 'train/mean_average_precision': 0.4477774106470991, 'validation/accuracy': 0.9868621230125427, 'validation/loss': 0.0438196025788784, 'validation/mean_average_precision': 0.2761807715640725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861161112785339, 'test/loss': 0.046595923602581024, 'test/mean_average_precision': 0.2598133222072068, 'test/num_examples': 43793, 'score': 6976.753581285477, 'total_duration': 10091.66427564621, 'accumulated_submission_time': 6976.753581285477, 'accumulated_eval_time': 3113.4867935180664, 'accumulated_logging_time': 0.838770866394043, 'global_step': 21908, 'preemption_count': 0}), (22662, {'train/accuracy': 0.9913204908370972, 'train/loss': 0.028243862092494965, 'train/mean_average_precision': 0.4488545594683917, 'validation/accuracy': 0.9869660139083862, 'validation/loss': 0.044106561690568924, 'validation/mean_average_precision': 0.27705950686903197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861489534378052, 'test/loss': 0.04689888656139374, 'test/mean_average_precision': 0.26680178288940487, 'test/num_examples': 43793, 'score': 7216.855234861374, 'total_duration': 10434.932267189026, 'accumulated_submission_time': 7216.855234861374, 'accumulated_eval_time': 3216.6028592586517, 'accumulated_logging_time': 0.8691599369049072, 'global_step': 22662, 'preemption_count': 0}), (23409, {'train/accuracy': 0.991339385509491, 'train/loss': 0.02800729125738144, 'train/mean_average_precision': 0.4768710540761837, 'validation/accuracy': 0.9870260953903198, 'validation/loss': 0.043588072061538696, 'validation/mean_average_precision': 0.27544556176136864, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.04647839441895485, 'test/mean_average_precision': 0.2654733454371574, 'test/num_examples': 43793, 'score': 7456.835754871368, 'total_duration': 10779.978934049606, 'accumulated_submission_time': 7456.835754871368, 'accumulated_eval_time': 3321.6178319454193, 'accumulated_logging_time': 0.9006702899932861, 'global_step': 23409, 'preemption_count': 0}), (24169, {'train/accuracy': 0.9917247891426086, 'train/loss': 0.026840627193450928, 'train/mean_average_precision': 0.48928305066369693, 'validation/accuracy': 0.9871340990066528, 'validation/loss': 0.04346122220158577, 'validation/mean_average_precision': 0.28249201456963385, 'validation/num_examples': 43793, 'test/accuracy': 0.9862883687019348, 'test/loss': 0.04660702869296074, 'test/mean_average_precision': 0.2689184754578295, 'test/num_examples': 43793, 'score': 7696.949980020523, 'total_duration': 11123.827285289764, 'accumulated_submission_time': 7696.949980020523, 'accumulated_eval_time': 3425.3005759716034, 'accumulated_logging_time': 0.932265043258667, 'global_step': 24169, 'preemption_count': 0}), (24918, {'train/accuracy': 0.9918337464332581, 'train/loss': 0.02654416300356388, 'train/mean_average_precision': 0.5018239814929434, 'validation/accuracy': 0.9869444966316223, 'validation/loss': 0.0440145805478096, 'validation/mean_average_precision': 0.28537577870853154, 'validation/num_examples': 43793, 'test/accuracy': 0.9860310554504395, 'test/loss': 0.046867869794368744, 'test/mean_average_precision': 0.2691691968252736, 'test/num_examples': 43793, 'score': 7937.204426765442, 'total_duration': 11472.261877059937, 'accumulated_submission_time': 7937.204426765442, 'accumulated_eval_time': 3533.4301381111145, 'accumulated_logging_time': 0.9634485244750977, 'global_step': 24918, 'preemption_count': 0}), (25672, {'train/accuracy': 0.9916606545448303, 'train/loss': 0.02706550620496273, 'train/mean_average_precision': 0.4931265444328698, 'validation/accuracy': 0.9869298934936523, 'validation/loss': 0.04404542222619057, 'validation/mean_average_precision': 0.2736783751099223, 'validation/num_examples': 43793, 'test/accuracy': 0.9861308932304382, 'test/loss': 0.046751171350479126, 'test/mean_average_precision': 0.26793754461580116, 'test/num_examples': 43793, 'score': 8177.371206998825, 'total_duration': 11815.163880109787, 'accumulated_submission_time': 8177.371206998825, 'accumulated_eval_time': 3636.113703250885, 'accumulated_logging_time': 0.9951300621032715, 'global_step': 25672, 'preemption_count': 0}), (26430, {'train/accuracy': 0.9915685653686523, 'train/loss': 0.027531573548913002, 'train/mean_average_precision': 0.47640785936763286, 'validation/accuracy': 0.9869379997253418, 'validation/loss': 0.043456874787807465, 'validation/mean_average_precision': 0.280425913360974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861363172531128, 'test/loss': 0.04630114138126373, 'test/mean_average_precision': 0.2708964711546916, 'test/num_examples': 43793, 'score': 8417.410165786743, 'total_duration': 12163.396028280258, 'accumulated_submission_time': 8417.410165786743, 'accumulated_eval_time': 3744.255452156067, 'accumulated_logging_time': 1.0263991355895996, 'global_step': 26430, 'preemption_count': 0}), (27188, {'train/accuracy': 0.9914869666099548, 'train/loss': 0.027469003573060036, 'train/mean_average_precision': 0.469126213426749, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.044153884053230286, 'validation/mean_average_precision': 0.28014835494886564, 'validation/num_examples': 43793, 'test/accuracy': 0.9861030578613281, 'test/loss': 0.047083813697099686, 'test/mean_average_precision': 0.26727003029691904, 'test/num_examples': 43793, 'score': 8657.560030698776, 'total_duration': 12506.874527454376, 'accumulated_submission_time': 8657.560030698776, 'accumulated_eval_time': 3847.5303523540497, 'accumulated_logging_time': 1.059520959854126, 'global_step': 27188, 'preemption_count': 0}), (27948, {'train/accuracy': 0.9915177226066589, 'train/loss': 0.027623098343610764, 'train/mean_average_precision': 0.46357976633261544, 'validation/accuracy': 0.9868791699409485, 'validation/loss': 0.04409085214138031, 'validation/mean_average_precision': 0.27683687410500485, 'validation/num_examples': 43793, 'test/accuracy': 0.9861097931861877, 'test/loss': 0.04702117294073105, 'test/mean_average_precision': 0.2677642311999752, 'test/num_examples': 43793, 'score': 8897.627020835876, 'total_duration': 12847.785507917404, 'accumulated_submission_time': 8897.627020835876, 'accumulated_eval_time': 3948.3207392692566, 'accumulated_logging_time': 1.0930004119873047, 'global_step': 27948, 'preemption_count': 0}), (28703, {'train/accuracy': 0.9916020035743713, 'train/loss': 0.027359729632735252, 'train/mean_average_precision': 0.47975518086924696, 'validation/accuracy': 0.9869339466094971, 'validation/loss': 0.044039491564035416, 'validation/mean_average_precision': 0.27764830198249624, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.04705201834440231, 'test/mean_average_precision': 0.26992850144159103, 'test/num_examples': 43793, 'score': 9137.693308830261, 'total_duration': 13192.525599956512, 'accumulated_submission_time': 9137.693308830261, 'accumulated_eval_time': 4052.9424483776093, 'accumulated_logging_time': 1.1257007122039795, 'global_step': 28703, 'preemption_count': 0}), (29457, {'train/accuracy': 0.9915141463279724, 'train/loss': 0.027219338342547417, 'train/mean_average_precision': 0.481494203847978, 'validation/accuracy': 0.9870768189430237, 'validation/loss': 0.043897371739149094, 'validation/mean_average_precision': 0.2825347877673383, 'validation/num_examples': 43793, 'test/accuracy': 0.9862100481987, 'test/loss': 0.04682281240820885, 'test/mean_average_precision': 0.27629275772368966, 'test/num_examples': 43793, 'score': 9377.844159126282, 'total_duration': 13534.766516923904, 'accumulated_submission_time': 9377.844159126282, 'accumulated_eval_time': 4154.9796550273895, 'accumulated_logging_time': 1.1584973335266113, 'global_step': 29457, 'preemption_count': 0}), (30214, {'train/accuracy': 0.9916245937347412, 'train/loss': 0.02697494812309742, 'train/mean_average_precision': 0.4861085832401384, 'validation/accuracy': 0.9869063496589661, 'validation/loss': 0.04395526275038719, 'validation/mean_average_precision': 0.27401784640488724, 'validation/num_examples': 43793, 'test/accuracy': 0.9861536026000977, 'test/loss': 0.04662062227725983, 'test/mean_average_precision': 0.26275205954140707, 'test/num_examples': 43793, 'score': 9617.910662651062, 'total_duration': 13879.029645681381, 'accumulated_submission_time': 9617.910662651062, 'accumulated_eval_time': 4259.123318433762, 'accumulated_logging_time': 1.1916203498840332, 'global_step': 30214, 'preemption_count': 0}), (30970, {'train/accuracy': 0.9916879534721375, 'train/loss': 0.026880500838160515, 'train/mean_average_precision': 0.48313094939134127, 'validation/accuracy': 0.9869319200515747, 'validation/loss': 0.04390327259898186, 'validation/mean_average_precision': 0.27804992863153566, 'validation/num_examples': 43793, 'test/accuracy': 0.9861034750938416, 'test/loss': 0.046760544180870056, 'test/mean_average_precision': 0.26753037311622335, 'test/num_examples': 43793, 'score': 9858.047510623932, 'total_duration': 14222.101789474487, 'accumulated_submission_time': 9858.047510623932, 'accumulated_eval_time': 4362.0056364536285, 'accumulated_logging_time': 1.2251360416412354, 'global_step': 30970, 'preemption_count': 0}), (31733, {'train/accuracy': 0.9920236468315125, 'train/loss': 0.025705374777317047, 'train/mean_average_precision': 0.5056896381051835, 'validation/accuracy': 0.9871158003807068, 'validation/loss': 0.043632812798023224, 'validation/mean_average_precision': 0.279755651589898, 'validation/num_examples': 43793, 'test/accuracy': 0.9861843585968018, 'test/loss': 0.04668949544429779, 'test/mean_average_precision': 0.26906986467590865, 'test/num_examples': 43793, 'score': 10098.214116334915, 'total_duration': 14567.009452581406, 'accumulated_submission_time': 10098.214116334915, 'accumulated_eval_time': 4466.693303823471, 'accumulated_logging_time': 1.2588024139404297, 'global_step': 31733, 'preemption_count': 0}), (32490, {'train/accuracy': 0.9920137524604797, 'train/loss': 0.02571156434714794, 'train/mean_average_precision': 0.5341408908884115, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04462163522839546, 'validation/mean_average_precision': 0.2860193035317245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861034750938416, 'test/loss': 0.04776688665151596, 'test/mean_average_precision': 0.2708168201569263, 'test/num_examples': 43793, 'score': 10338.35930466652, 'total_duration': 14908.73725104332, 'accumulated_submission_time': 10338.35930466652, 'accumulated_eval_time': 4568.222731590271, 'accumulated_logging_time': 1.2922701835632324, 'global_step': 32490, 'preemption_count': 0}), (33250, {'train/accuracy': 0.9923450946807861, 'train/loss': 0.024707453325390816, 'train/mean_average_precision': 0.542667942251531, 'validation/accuracy': 0.9870890378952026, 'validation/loss': 0.0436905212700367, 'validation/mean_average_precision': 0.2875374195248217, 'validation/num_examples': 43793, 'test/accuracy': 0.9861953258514404, 'test/loss': 0.046789951622486115, 'test/mean_average_precision': 0.2757306207445348, 'test/num_examples': 43793, 'score': 10578.31761264801, 'total_duration': 15254.959168434143, 'accumulated_submission_time': 10578.31761264801, 'accumulated_eval_time': 4674.432077407837, 'accumulated_logging_time': 1.3265063762664795, 'global_step': 33250, 'preemption_count': 0}), (34005, {'train/accuracy': 0.9921961426734924, 'train/loss': 0.025209462270140648, 'train/mean_average_precision': 0.5336781380529731, 'validation/accuracy': 0.9870187640190125, 'validation/loss': 0.04386290907859802, 'validation/mean_average_precision': 0.2830934933826157, 'validation/num_examples': 43793, 'test/accuracy': 0.9861144423484802, 'test/loss': 0.04689648747444153, 'test/mean_average_precision': 0.2706710636528843, 'test/num_examples': 43793, 'score': 10818.476083278656, 'total_duration': 15599.71495604515, 'accumulated_submission_time': 10818.476083278656, 'accumulated_eval_time': 4778.975328207016, 'accumulated_logging_time': 1.3605234622955322, 'global_step': 34005, 'preemption_count': 0}), (34753, {'train/accuracy': 0.9918820858001709, 'train/loss': 0.02606547810137272, 'train/mean_average_precision': 0.4910209539627918, 'validation/accuracy': 0.9870484471321106, 'validation/loss': 0.04438941925764084, 'validation/mean_average_precision': 0.28277370317285005, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.04756646230816841, 'test/mean_average_precision': 0.26872718959765507, 'test/num_examples': 43793, 'score': 11058.613788604736, 'total_duration': 15942.403191328049, 'accumulated_submission_time': 11058.613788604736, 'accumulated_eval_time': 4881.47107052803, 'accumulated_logging_time': 1.3952465057373047, 'global_step': 34753, 'preemption_count': 0}), (35506, {'train/accuracy': 0.991861879825592, 'train/loss': 0.02618979476392269, 'train/mean_average_precision': 0.5024073601537873, 'validation/accuracy': 0.9869396090507507, 'validation/loss': 0.044023074209690094, 'validation/mean_average_precision': 0.2845682260674991, 'validation/num_examples': 43793, 'test/accuracy': 0.9860979914665222, 'test/loss': 0.046877551823854446, 'test/mean_average_precision': 0.2738745127717128, 'test/num_examples': 43793, 'score': 11298.787636518478, 'total_duration': 16288.471473455429, 'accumulated_submission_time': 11298.787636518478, 'accumulated_eval_time': 4987.312251329422, 'accumulated_logging_time': 1.4282660484313965, 'global_step': 35506, 'preemption_count': 0}), (36263, {'train/accuracy': 0.9919314980506897, 'train/loss': 0.025890670716762543, 'train/mean_average_precision': 0.5145215114867434, 'validation/accuracy': 0.9870853424072266, 'validation/loss': 0.04400112107396126, 'validation/mean_average_precision': 0.2819405539208721, 'validation/num_examples': 43793, 'test/accuracy': 0.9861472845077515, 'test/loss': 0.04721315577626228, 'test/mean_average_precision': 0.2717603088414615, 'test/num_examples': 43793, 'score': 11538.803719043732, 'total_duration': 16633.155037403107, 'accumulated_submission_time': 11538.803719043732, 'accumulated_eval_time': 5091.926290988922, 'accumulated_logging_time': 1.4618382453918457, 'global_step': 36263, 'preemption_count': 0}), (37020, {'train/accuracy': 0.9920294880867004, 'train/loss': 0.025662560015916824, 'train/mean_average_precision': 0.525321302551629, 'validation/accuracy': 0.9870873689651489, 'validation/loss': 0.04402170330286026, 'validation/mean_average_precision': 0.2841475760462093, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.047062020748853683, 'test/mean_average_precision': 0.2745681328480498, 'test/num_examples': 43793, 'score': 11779.05526304245, 'total_duration': 16976.350893974304, 'accumulated_submission_time': 11779.05526304245, 'accumulated_eval_time': 5194.814932107925, 'accumulated_logging_time': 1.4971117973327637, 'global_step': 37020, 'preemption_count': 0}), (37780, {'train/accuracy': 0.992198646068573, 'train/loss': 0.025036245584487915, 'train/mean_average_precision': 0.5411420089665825, 'validation/accuracy': 0.9870870113372803, 'validation/loss': 0.04411032795906067, 'validation/mean_average_precision': 0.2871141885306458, 'validation/num_examples': 43793, 'test/accuracy': 0.9862509369850159, 'test/loss': 0.047302741557359695, 'test/mean_average_precision': 0.2781049767731259, 'test/num_examples': 43793, 'score': 12018.916049003601, 'total_duration': 17322.259452581406, 'accumulated_submission_time': 12018.916049003601, 'accumulated_eval_time': 5300.418438196182, 'accumulated_logging_time': 1.9214563369750977, 'global_step': 37780, 'preemption_count': 0}), (38539, {'train/accuracy': 0.9921022057533264, 'train/loss': 0.025213586166501045, 'train/mean_average_precision': 0.5186278352257488, 'validation/accuracy': 0.9870630502700806, 'validation/loss': 0.04458058997988701, 'validation/mean_average_precision': 0.28446345557888714, 'validation/num_examples': 43793, 'test/accuracy': 0.9861801266670227, 'test/loss': 0.04781052842736244, 'test/mean_average_precision': 0.2695032759656423, 'test/num_examples': 43793, 'score': 12259.118973731995, 'total_duration': 17666.440479516983, 'accumulated_submission_time': 12259.118973731995, 'accumulated_eval_time': 5404.342231273651, 'accumulated_logging_time': 1.9558711051940918, 'global_step': 38539, 'preemption_count': 0}), (39293, {'train/accuracy': 0.9923055171966553, 'train/loss': 0.02453293837606907, 'train/mean_average_precision': 0.5386582770107501, 'validation/accuracy': 0.9871178269386292, 'validation/loss': 0.04436669871211052, 'validation/mean_average_precision': 0.28731616174521757, 'validation/num_examples': 43793, 'test/accuracy': 0.9863056540489197, 'test/loss': 0.04755987226963043, 'test/mean_average_precision': 0.2716499638558299, 'test/num_examples': 43793, 'score': 12499.194529294968, 'total_duration': 18007.454338550568, 'accumulated_submission_time': 12499.194529294968, 'accumulated_eval_time': 5505.22732257843, 'accumulated_logging_time': 1.9891891479492188, 'global_step': 39293, 'preemption_count': 0}), (40044, {'train/accuracy': 0.9925379157066345, 'train/loss': 0.02376587875187397, 'train/mean_average_precision': 0.5654487645215553, 'validation/accuracy': 0.9872254133224487, 'validation/loss': 0.044390469789505005, 'validation/mean_average_precision': 0.2868401826016203, 'validation/num_examples': 43793, 'test/accuracy': 0.9863005876541138, 'test/loss': 0.04770269617438316, 'test/mean_average_precision': 0.27590861828424906, 'test/num_examples': 43793, 'score': 12739.169730901718, 'total_duration': 18349.34113383293, 'accumulated_submission_time': 12739.169730901718, 'accumulated_eval_time': 5607.085157632828, 'accumulated_logging_time': 2.0231266021728516, 'global_step': 40044, 'preemption_count': 0}), (40804, {'train/accuracy': 0.9927852749824524, 'train/loss': 0.023151222616434097, 'train/mean_average_precision': 0.5775893719350957, 'validation/accuracy': 0.9870390892028809, 'validation/loss': 0.044513948261737823, 'validation/mean_average_precision': 0.2907892343854625, 'validation/num_examples': 43793, 'test/accuracy': 0.9861422181129456, 'test/loss': 0.04752732813358307, 'test/mean_average_precision': 0.276028532270375, 'test/num_examples': 43793, 'score': 12979.314190387726, 'total_duration': 18690.889854192734, 'accumulated_submission_time': 12979.314190387726, 'accumulated_eval_time': 5708.435259819031, 'accumulated_logging_time': 2.0572030544281006, 'global_step': 40804, 'preemption_count': 0}), (41566, {'train/accuracy': 0.9926801323890686, 'train/loss': 0.023235918954014778, 'train/mean_average_precision': 0.5827686459930178, 'validation/accuracy': 0.9871527552604675, 'validation/loss': 0.044966548681259155, 'validation/mean_average_precision': 0.2888831560468628, 'validation/num_examples': 43793, 'test/accuracy': 0.9863119721412659, 'test/loss': 0.047933343797922134, 'test/mean_average_precision': 0.27867468106709653, 'test/num_examples': 43793, 'score': 13219.500958919525, 'total_duration': 19033.902818202972, 'accumulated_submission_time': 13219.500958919525, 'accumulated_eval_time': 5811.207458734512, 'accumulated_logging_time': 2.0911972522735596, 'global_step': 41566, 'preemption_count': 0}), (42326, {'train/accuracy': 0.9926807880401611, 'train/loss': 0.023445546627044678, 'train/mean_average_precision': 0.5660779500427957, 'validation/accuracy': 0.9870723485946655, 'validation/loss': 0.044642020016908646, 'validation/mean_average_precision': 0.2907271142531775, 'validation/num_examples': 43793, 'test/accuracy': 0.9861717224121094, 'test/loss': 0.04774622619152069, 'test/mean_average_precision': 0.2741958646714575, 'test/num_examples': 43793, 'score': 13459.468119859695, 'total_duration': 19378.783309936523, 'accumulated_submission_time': 13459.468119859695, 'accumulated_eval_time': 5916.065999746323, 'accumulated_logging_time': 2.1258957386016846, 'global_step': 42326, 'preemption_count': 0}), (43076, {'train/accuracy': 0.992499589920044, 'train/loss': 0.02394149824976921, 'train/mean_average_precision': 0.5477740813822252, 'validation/accuracy': 0.9870922565460205, 'validation/loss': 0.04479354992508888, 'validation/mean_average_precision': 0.28924994940451176, 'validation/num_examples': 43793, 'test/accuracy': 0.9863384962081909, 'test/loss': 0.04778079316020012, 'test/mean_average_precision': 0.27807957057728816, 'test/num_examples': 43793, 'score': 13699.443043470383, 'total_duration': 19719.386711359024, 'accumulated_submission_time': 13699.443043470383, 'accumulated_eval_time': 6016.614222764969, 'accumulated_logging_time': 2.186445474624634, 'global_step': 43076, 'preemption_count': 0}), (43826, {'train/accuracy': 0.9925187826156616, 'train/loss': 0.023853471502661705, 'train/mean_average_precision': 0.5623201160024388, 'validation/accuracy': 0.9870967268943787, 'validation/loss': 0.04477299377322197, 'validation/mean_average_precision': 0.29060484401217707, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.047920502722263336, 'test/mean_average_precision': 0.2761384758753186, 'test/num_examples': 43793, 'score': 13939.41558265686, 'total_duration': 20063.16539645195, 'accumulated_submission_time': 13939.41558265686, 'accumulated_eval_time': 6120.3640768527985, 'accumulated_logging_time': 2.2230827808380127, 'global_step': 43826, 'preemption_count': 0}), (44573, {'train/accuracy': 0.9926663041114807, 'train/loss': 0.023493686690926552, 'train/mean_average_precision': 0.5641044492073488, 'validation/accuracy': 0.9869808554649353, 'validation/loss': 0.04480111226439476, 'validation/mean_average_precision': 0.28610250989785263, 'validation/num_examples': 43793, 'test/accuracy': 0.986177384853363, 'test/loss': 0.04789437726140022, 'test/mean_average_precision': 0.2796849265893141, 'test/num_examples': 43793, 'score': 14179.501737833023, 'total_duration': 20406.76056933403, 'accumulated_submission_time': 14179.501737833023, 'accumulated_eval_time': 6223.816519021988, 'accumulated_logging_time': 2.2597038745880127, 'global_step': 44573, 'preemption_count': 0}), (45329, {'train/accuracy': 0.9926155209541321, 'train/loss': 0.023533791303634644, 'train/mean_average_precision': 0.5735633061987906, 'validation/accuracy': 0.9870926737785339, 'validation/loss': 0.04458260536193848, 'validation/mean_average_precision': 0.2924033260479522, 'validation/num_examples': 43793, 'test/accuracy': 0.9862361550331116, 'test/loss': 0.04773207753896713, 'test/mean_average_precision': 0.2748628905795507, 'test/num_examples': 43793, 'score': 14419.762070655823, 'total_duration': 20753.566682338715, 'accumulated_submission_time': 14419.762070655823, 'accumulated_eval_time': 6330.305959939957, 'accumulated_logging_time': 2.296353340148926, 'global_step': 45329, 'preemption_count': 0}), (46086, {'train/accuracy': 0.9928174614906311, 'train/loss': 0.022799771279096603, 'train/mean_average_precision': 0.5843304260900297, 'validation/accuracy': 0.9871162176132202, 'validation/loss': 0.045017652213573456, 'validation/mean_average_precision': 0.29081624087472746, 'validation/num_examples': 43793, 'test/accuracy': 0.9862534403800964, 'test/loss': 0.0483238622546196, 'test/mean_average_precision': 0.2770070234684541, 'test/num_examples': 43793, 'score': 14659.82816028595, 'total_duration': 21098.419695854187, 'accumulated_submission_time': 14659.82816028595, 'accumulated_eval_time': 6435.036421775818, 'accumulated_logging_time': 2.332881212234497, 'global_step': 46086, 'preemption_count': 0}), (46840, {'train/accuracy': 0.9929105043411255, 'train/loss': 0.022443510591983795, 'train/mean_average_precision': 0.583947243919866, 'validation/accuracy': 0.9870622158050537, 'validation/loss': 0.04508613795042038, 'validation/mean_average_precision': 0.2884127098754348, 'validation/num_examples': 43793, 'test/accuracy': 0.9863351583480835, 'test/loss': 0.04808259755373001, 'test/mean_average_precision': 0.2740103134893802, 'test/num_examples': 43793, 'score': 14899.894470214844, 'total_duration': 21442.052701473236, 'accumulated_submission_time': 14899.894470214844, 'accumulated_eval_time': 6538.548140287399, 'accumulated_logging_time': 2.3681788444519043, 'global_step': 46840, 'preemption_count': 0}), (47594, {'train/accuracy': 0.9930605888366699, 'train/loss': 0.02211160585284233, 'train/mean_average_precision': 0.6052337144115114, 'validation/accuracy': 0.9869980812072754, 'validation/loss': 0.04506611451506615, 'validation/mean_average_precision': 0.2931761841290524, 'validation/num_examples': 43793, 'test/accuracy': 0.9861367344856262, 'test/loss': 0.04800629988312721, 'test/mean_average_precision': 0.2781953903528778, 'test/num_examples': 43793, 'score': 15139.885927677155, 'total_duration': 21789.244199752808, 'accumulated_submission_time': 15139.885927677155, 'accumulated_eval_time': 6645.693230390549, 'accumulated_logging_time': 2.4038455486297607, 'global_step': 47594, 'preemption_count': 0}), (48354, {'train/accuracy': 0.9933203458786011, 'train/loss': 0.021325798705220222, 'train/mean_average_precision': 0.6125192019839938, 'validation/accuracy': 0.9870272874832153, 'validation/loss': 0.044993653893470764, 'validation/mean_average_precision': 0.2895411568214211, 'validation/num_examples': 43793, 'test/accuracy': 0.9861510992050171, 'test/loss': 0.04813129082322121, 'test/mean_average_precision': 0.27331465592941967, 'test/num_examples': 43793, 'score': 15380.088567256927, 'total_duration': 22131.06327676773, 'accumulated_submission_time': 15380.088567256927, 'accumulated_eval_time': 6747.254262685776, 'accumulated_logging_time': 2.439406394958496, 'global_step': 48354, 'preemption_count': 0}), (49113, {'train/accuracy': 0.9934682250022888, 'train/loss': 0.02071463316679001, 'train/mean_average_precision': 0.6426092112092957, 'validation/accuracy': 0.9870119094848633, 'validation/loss': 0.045634426176548004, 'validation/mean_average_precision': 0.2912810678611326, 'validation/num_examples': 43793, 'test/accuracy': 0.9861595034599304, 'test/loss': 0.04884245619177818, 'test/mean_average_precision': 0.27329671056341903, 'test/num_examples': 43793, 'score': 15620.312088727951, 'total_duration': 22473.058209180832, 'accumulated_submission_time': 15620.312088727951, 'accumulated_eval_time': 6848.968739748001, 'accumulated_logging_time': 2.4766972064971924, 'global_step': 49113, 'preemption_count': 0}), (49870, {'train/accuracy': 0.993198812007904, 'train/loss': 0.021317660808563232, 'train/mean_average_precision': 0.6175284713675935, 'validation/accuracy': 0.9872095584869385, 'validation/loss': 0.04620447754859924, 'validation/mean_average_precision': 0.2915582478001256, 'validation/num_examples': 43793, 'test/accuracy': 0.9863507151603699, 'test/loss': 0.0493396520614624, 'test/mean_average_precision': 0.27417384906227316, 'test/num_examples': 43793, 'score': 15860.463735103607, 'total_duration': 22813.647511959076, 'accumulated_submission_time': 15860.463735103607, 'accumulated_eval_time': 6949.345129728317, 'accumulated_logging_time': 2.5182931423187256, 'global_step': 49870, 'preemption_count': 0}), (50630, {'train/accuracy': 0.9932072758674622, 'train/loss': 0.021530773490667343, 'train/mean_average_precision': 0.6065806372072025, 'validation/accuracy': 0.9869790077209473, 'validation/loss': 0.045623790472745895, 'validation/mean_average_precision': 0.2889365308084413, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.04872185364365578, 'test/mean_average_precision': 0.2741141156405496, 'test/num_examples': 43793, 'score': 16100.710295438766, 'total_duration': 23156.296748638153, 'accumulated_submission_time': 16100.710295438766, 'accumulated_eval_time': 7051.691777467728, 'accumulated_logging_time': 2.554926872253418, 'global_step': 50630, 'preemption_count': 0}), (51395, {'train/accuracy': 0.9932363629341125, 'train/loss': 0.021373797208070755, 'train/mean_average_precision': 0.6189733847194379, 'validation/accuracy': 0.9871328473091125, 'validation/loss': 0.04566166549921036, 'validation/mean_average_precision': 0.28872892109869364, 'validation/num_examples': 43793, 'test/accuracy': 0.9862711429595947, 'test/loss': 0.04888831079006195, 'test/mean_average_precision': 0.2745948013890646, 'test/num_examples': 43793, 'score': 16340.700924158096, 'total_duration': 23497.049400806427, 'accumulated_submission_time': 16340.700924158096, 'accumulated_eval_time': 7152.396923303604, 'accumulated_logging_time': 2.5921742916107178, 'global_step': 51395, 'preemption_count': 0}), (52159, {'train/accuracy': 0.9932331442832947, 'train/loss': 0.02135111205279827, 'train/mean_average_precision': 0.6257244482281129, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.045979227870702744, 'validation/mean_average_precision': 0.28716131986331844, 'validation/num_examples': 43793, 'test/accuracy': 0.9862285852432251, 'test/loss': 0.049049995839595795, 'test/mean_average_precision': 0.2820580224329422, 'test/num_examples': 43793, 'score': 16580.71767473221, 'total_duration': 23838.59583926201, 'accumulated_submission_time': 16580.71767473221, 'accumulated_eval_time': 7253.871799230576, 'accumulated_logging_time': 2.6272754669189453, 'global_step': 52159, 'preemption_count': 0}), (52918, {'train/accuracy': 0.9933499693870544, 'train/loss': 0.020969053730368614, 'train/mean_average_precision': 0.6198161926220507, 'validation/accuracy': 0.9870723485946655, 'validation/loss': 0.04600640386343002, 'validation/mean_average_precision': 0.2898691017552575, 'validation/num_examples': 43793, 'test/accuracy': 0.9862694144248962, 'test/loss': 0.04908306896686554, 'test/mean_average_precision': 0.2786305604471064, 'test/num_examples': 43793, 'score': 16820.859786748886, 'total_duration': 24180.04267501831, 'accumulated_submission_time': 16820.859786748886, 'accumulated_eval_time': 7355.1210062503815, 'accumulated_logging_time': 2.6630918979644775, 'global_step': 52918, 'preemption_count': 0}), (53674, {'train/accuracy': 0.9933534264564514, 'train/loss': 0.02080860733985901, 'train/mean_average_precision': 0.629301270151079, 'validation/accuracy': 0.9871153831481934, 'validation/loss': 0.046308934688568115, 'validation/mean_average_precision': 0.28908989832738635, 'validation/num_examples': 43793, 'test/accuracy': 0.9862861037254333, 'test/loss': 0.04954114183783531, 'test/mean_average_precision': 0.27942670271488634, 'test/num_examples': 43793, 'score': 17061.08062529564, 'total_duration': 24523.113798379898, 'accumulated_submission_time': 17061.08062529564, 'accumulated_eval_time': 7457.915698289871, 'accumulated_logging_time': 2.698948621749878, 'global_step': 53674, 'preemption_count': 0}), (54432, {'train/accuracy': 0.9934797883033752, 'train/loss': 0.020473549142479897, 'train/mean_average_precision': 0.62993081766115, 'validation/accuracy': 0.9871438145637512, 'validation/loss': 0.046290863305330276, 'validation/mean_average_precision': 0.2877251097946155, 'validation/num_examples': 43793, 'test/accuracy': 0.9863119721412659, 'test/loss': 0.049489088356494904, 'test/mean_average_precision': 0.27639562463520206, 'test/num_examples': 43793, 'score': 17301.122370243073, 'total_duration': 24862.13622522354, 'accumulated_submission_time': 17301.122370243073, 'accumulated_eval_time': 7556.839626789093, 'accumulated_logging_time': 2.735714912414551, 'global_step': 54432, 'preemption_count': 0}), (55184, {'train/accuracy': 0.9935911297798157, 'train/loss': 0.020000223070383072, 'train/mean_average_precision': 0.6547790093724063, 'validation/accuracy': 0.9871222972869873, 'validation/loss': 0.0461614653468132, 'validation/mean_average_precision': 0.2900952606281319, 'validation/num_examples': 43793, 'test/accuracy': 0.986243724822998, 'test/loss': 0.04939693585038185, 'test/mean_average_precision': 0.2795634855130245, 'test/num_examples': 43793, 'score': 17541.313578367233, 'total_duration': 25207.003221273422, 'accumulated_submission_time': 17541.313578367233, 'accumulated_eval_time': 7661.45597743988, 'accumulated_logging_time': 2.774885654449463, 'global_step': 55184, 'preemption_count': 0}), (55944, {'train/accuracy': 0.9937967658042908, 'train/loss': 0.01947147771716118, 'train/mean_average_precision': 0.6527825708898463, 'validation/accuracy': 0.9870926737785339, 'validation/loss': 0.04674072191119194, 'validation/mean_average_precision': 0.2911061819818859, 'validation/num_examples': 43793, 'test/accuracy': 0.9862310886383057, 'test/loss': 0.05005291849374771, 'test/mean_average_precision': 0.2778119291983832, 'test/num_examples': 43793, 'score': 17781.294590473175, 'total_duration': 25546.138780355453, 'accumulated_submission_time': 17781.294590473175, 'accumulated_eval_time': 7760.551167964935, 'accumulated_logging_time': 2.8143293857574463, 'global_step': 55944, 'preemption_count': 0}), (56701, {'train/accuracy': 0.9940793514251709, 'train/loss': 0.018616680055856705, 'train/mean_average_precision': 0.6898328240343279, 'validation/accuracy': 0.987098753452301, 'validation/loss': 0.04666012153029442, 'validation/mean_average_precision': 0.28995529375292517, 'validation/num_examples': 43793, 'test/accuracy': 0.9862062335014343, 'test/loss': 0.049819622188806534, 'test/mean_average_precision': 0.282412806094966, 'test/num_examples': 43793, 'score': 18021.25959467888, 'total_duration': 25890.779302597046, 'accumulated_submission_time': 18021.25959467888, 'accumulated_eval_time': 7865.168210506439, 'accumulated_logging_time': 2.853222131729126, 'global_step': 56701, 'preemption_count': 0}), (57456, {'train/accuracy': 0.9942800998687744, 'train/loss': 0.018083401024341583, 'train/mean_average_precision': 0.6976413701881876, 'validation/accuracy': 0.9871283769607544, 'validation/loss': 0.04675629362463951, 'validation/mean_average_precision': 0.2962188183781732, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.05016188323497772, 'test/mean_average_precision': 0.2802577179914768, 'test/num_examples': 43793, 'score': 18261.490270614624, 'total_duration': 26231.215923547745, 'accumulated_submission_time': 18261.490270614624, 'accumulated_eval_time': 7965.316137075424, 'accumulated_logging_time': 2.891075849533081, 'global_step': 57456, 'preemption_count': 0})], 'global_step': 58138}
I0206 17:32:59.132517 139978932307776 submission_runner.py:586] Timing: 18477.238425970078
I0206 17:32:59.132572 139978932307776 submission_runner.py:588] Total number of evals: 77
I0206 17:32:59.132614 139978932307776 submission_runner.py:589] ====================
I0206 17:32:59.135005 139978932307776 submission_runner.py:673] Final ogbg score: 18477.00848555565
